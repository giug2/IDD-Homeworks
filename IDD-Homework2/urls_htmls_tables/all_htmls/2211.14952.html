<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.14952] Federated Learning Attacks and Defenses: A Survey</title><meta property="og:description" content="In terms of artificial intelligence, there are several security and privacy deficiencies in the traditional centralized training methods of machine learning models by a server. To address this limitation, federated lea…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning Attacks and Defenses: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning Attacks and Defenses: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.14952">

<!--Generated on Thu Mar 14 07:09:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
federated learning,  attacks,  defenses,  challenges,  opportunities
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning Attacks and Defenses: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yao Chen<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Yijie Gui<sup id="id9.9.id2" class="ltx_sup"><span id="id9.9.id2.1" class="ltx_text ltx_font_italic">1</span></sup>, Hong Lin<sup id="id10.10.id3" class="ltx_sup"><span id="id10.10.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Wensheng Gan<sup id="id11.11.id4" class="ltx_sup"><span id="id11.11.id4.1" class="ltx_text ltx_font_italic">1,2∗</span></sup>, Yongdong Wu<sup id="id12.12.id5" class="ltx_sup"><span id="id12.12.id5.1" class="ltx_text ltx_font_italic">1</span></sup> 
<br class="ltx_break">
<br class="ltx_break"><sup id="id13.13.id6" class="ltx_sup"><span id="id13.13.id6.1" class="ltx_text ltx_font_italic">1</span></sup>Jinan University, Guangzhou 510632, China
<br class="ltx_break"><sup id="id14.14.id7" class="ltx_sup"><span id="id14.14.id7.1" class="ltx_text ltx_font_italic">2</span></sup>Pazhou Lab, Guangzhou 510330, China
<br class="ltx_break">Email: {csyaochen, y.j.gui123, lhed9eh0g, wsgan001, wuyd175}@gmail.com
</span><span class="ltx_author_notes">1Corresponding author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">In terms of artificial intelligence, there are several security and privacy deficiencies in the traditional centralized training methods of machine learning models by a server. To address this limitation, federated learning (FL) has been proposed and is known for breaking down “data silos” and protecting the privacy of users. However, FL has not yet gained popularity in the industry, mainly due to its security, privacy, and high cost of communication. For the purpose of advancing the research in this field, building a robust FL system, and realizing the wide application of FL, this paper sorts out the possible attacks and corresponding defenses of the current FL system systematically. Firstly, this paper briefly introduces the basic workflow of FL and related knowledge of attacks and defenses. It reviews a great deal of research about privacy theft and malicious attacks that have been studied in recent years. Most importantly, in view of the current three classification criteria, namely the three stages of machine learning, the three different roles in federated learning, and the CIA (Confidentiality, Integrity, and Availability) guidelines on privacy protection, we divide attack approaches into two categories according to the training stage and the prediction stage in machine learning. Furthermore, we also identify the CIA property violated for each attack method and potential attack role. Various defense mechanisms are then analyzed separately from the level of privacy and security. Finally, we summarize the possible challenges in the application of FL from the aspect of attacks and defenses and discuss the future development direction of FL systems. In this way, the designed FL system has the ability to resist different attacks and is more secure and stable.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
federated learning, attacks, defenses, challenges, opportunities

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the vigorous development of big data and artificial intelligence, large amounts of data and models have been generated. The process and the transfer of data have become much more frequent in the meantime. As we all know, in many industries, data often exists in the form of silos, and the most straightforward way to solve the silo problem is to ensemble data into one party for further processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, this practice will certainly cause data privacy leakage problems. Nowadays, many countries have made efforts to strengthen the protection of citizens’ private data security. Take the European Union’s General Data Protection Regulation (GDPR) as an example, which came into force on May 25, 2018. Protecting users’ personal privacy and data security is the goal of this regulation. Similarly, since 2017, the Cybersecurity Law of the People’s Republic of China has guaranteed cybersecurity, protected the legitimate rights and interests of citizens, and promoted the healthy development of economic and social information technology<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://www.gov.cn/xinwen/2016-11/07/content_5129723.htm" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.gov.cn/xinwen/2016-11/07/content_5129723.htm</a></span></span></span>. Naturally, with the emphasis on data privacy and security becoming a worldwide trend, breaking down data silos and making full use of data have become hot topics today. To address this limit, federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is proposed as a data integration method that complies with data privacy and security laws.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Google proposed FL in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to address the limitation of updating models locally by the user using an Android mobile phone. This technique has been used in many other areas in combination with other expertise, such as enterprise data alliance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, smart finance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, smart healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, etc. FL is actually an encrypted distributed machine learning technique that can effectively assist multiple organizations and the usage of data while meeting the requirements of privacy protection, data security, and government regulations. It is characterized by the following four parts: multi-party collaboration; equality of all parties; data privacy protection; and data encryption. A concept that is similar to FL is joint learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The difference between them is that joint learning has no requirements for aggregated methods or privacy about data. In addition, there is still a difference between FL and privacy protection theories commonly used in data mining <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, such as differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, k-anonymity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and l-diversity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Specifically, FL protects users’ data privacy through the exchange of parameters under specific encryption mechanisms. However, differential privacy, k-anonymity, and l-diversity methods protect the users’ privacy information by adding noise to data or obfuscating sensitive attributes in the databases.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2211.14952/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="321" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overall flow of FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The workflow of FL can be generally divided into three steps: the server generates the initial global model, the participants train the model locally, and the server updates the global model. This is shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. On the basis of the distribution of data sources among all participants, FL can be divided into three categories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>: horizontally federated learning (HFL), vertically federated learning (VFL), and federated transfer learning (FTL). The essence of HFL is the union of samples. When users’ features of two datasets overlap more and users overlap less, we divide the dataset horizontally and take out the part of the data where the users’ features of both datasets are the same, but the users are not exactly the same for training (that is the user dimension). The nature of VFL is the union of features. When two datasets have more overlap in users but less overlap in users’ features, we divide the dataset vertically and take out the part of the data where the users on both sides are the same, but the user features are not exactly the same for training (that is the feature dimension). However, in those cases where both datasets have less overlap in users and users’ features, we do not slice the data and use the transfer learning method to overcome the shortfall of data or label. The transfer learning method can be used by alliances between banks and e-commerce companies in different regions. FTL is applicable to some scenarios based on deep neural networks.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Although FL does not directly exchange data and has a higher privacy guarantee than traditional machine learning, there are still some insecure problems in specific practical applications that need to be further studied and solved <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. It faces four challenges in general: high communication costs, systemic heterogeneity, statistical heterogeneity, and data security <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The first three challenges are problems that FL may encounter in practical applications. The last challenge ensures that federal learning meets privacy, security, and various legal regulations. In naive FL, the confidentiality of data mainly depends on the use of cryptographic algorithms, such as AES <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, SM4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, RSA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, etc. These algorithms enable the plaintext data to be encrypted to obtain irreversible ciphertext data, ensuring that the privacy of the data is not leaked. However, there are some research works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proving that some private data can be inferred from the transmission of data. The member inference attack was first proposed by Shokri <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and aims to use the trained model to determine whether a sample belongs to the corresponding training set. This can reveal private information in certain situations, such as disease classification models in the medical field. As attackers continue to find out vulnerabilities in the network, the model inversion attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, the adversarial attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, the backdoor attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, the Denial of Service attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and other attacks are successively discovered by them. In order to cope with various attacks, differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, secure multi-party computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, and other strategies for defense have also been proposed to ensure security in FL.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Until now, most of the existing literature reviews about FL attacks and defenses mainly focused on specific attack types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and listed each attack type for analysis and explanation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Some more specific literature classifies attacks according to different criteria. For example, Liu <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> divided threat models, attacks, and defenses through three stages of FL (data and behavior auditing phase, training phase, and prediction phase). Chen <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> classified attacks according to three different principles of information security (i.e., confidentiality, integrity, and availability). Wu <span id="S1.p5.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> enumerated types of attacks according to the potential attackers in the FL system. One of the highlights of this paper is to look for the intersection of the above three criteria and combine them reasonably.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Contributions</span>: This paper aims to summarizing the development of attack and defense technologies in the frameworks of different FL systems in recent years. The classification of existing attacks, which combines the current three classification criteria, is highlighted in this paper. In addition, we categorize and discuss various attack strategies in detail, including poisoning attack, inference attack, reconstruction attack, etc. Specifically, existing defenses are discussed in detail from the perspective of privacy and security, respectively. Finally, we put forward six research directions for the future development of attack and defense in various FL systems.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">Outline</span>: The remaining content of this paper is organized and presented as follows: we provide and describe some fundamental knowledge about FL in Section <a href="#S2" title="II Background ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Most importantly, we categorize attacks and defenses and give a detailed analysis in Section <a href="#S3" title="III Attacks in FL ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> and Section <a href="#S4" title="IV Defenses in FL ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. Furthermore, in Section <a href="#S5" title="V Challenges and Promising Directions ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, we present a comprehensive review of challenges and potential future research directions for FL. Finally, we make a conclusion for this review in Section <a href="#S6" title="VI Conclusion ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Federated learning is a machine learning paradigm that involves multiple participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. In this paradigm, the participants can build the model jointly without disclosing the underlying data or its cryptographic form. In short, FL implements training and builds a shared model without letting data leave the local area. This section provides background knowledge on FL attacks and defenses, including different types of FL and some threats to security and privacy that FL encounters in each process.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Types of Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Based on the data characteristics of the participants and the distribution of the data samples, FL can be classified into the following three types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Horizontally federated learning (HFL).</span> The characteristic of HFL is that participants have different data samples, but the data features in the samples overlap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. For example, given two hospitals in different locations, the intersection of the data samples from these two hospitals is small. However, the business of hospitals is very similar. Therefore, there are many overlaps in the data characteristics of the data samples. In this case, we can use HFL to build a joint model.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Vertically federated learning (VFL).</span> The characteristic of VFL is that data samples owned by participants overlap, but the data features in the samples are different <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. For instance, a bank and an e-commerce company are in the same area. Their user groups include some local residents, so the intersection of the data samples from these two institutions is large. The characteristics of the data collected by the bank are related to the credit level of the user. And the characteristics of the data collected by the e-commerce company are related to consumer behaviors. Therefore, the characteristics of the data collected by these two institutions are not the same. This situation is suitable for the VFL.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Federated transfer learning (FTL).</span> FTL is characterized by less overlap of data characteristics in different data samples and less overlap of data samples owned by the participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. FTL combines the ideas of FL and transfer learning, where transfer learning is a machine learning method that transfers knowledge learned in one domain or task to another domain or task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. FTL uses transfer learning to overcome the problem of non-overlapping samples and non-overlapping features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Security and Privacy Threats in FL</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Security and Privacy Threats in FL ‣ II Background ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the classic workflow of FL training models and the privacy and security threats that exist.
The workflow of FL training models is generally divided into three steps: generate the initial model, train the local model, and update the global model.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2211.14952/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="300" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Threats in a federated learning environment.</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Generating the initial model.</span> FL usually starts with an initial global model generated by the server. This global model is then broadcast to those participants who trained the model together <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. In this step, a malicious server may inject malicious data into the global model. This behavior affects the training and prediction of the model. All participants download the global model from the malicious server and upload the local model to the malicious server. This attack behavior may threaten the privacy and security of the participants by attacking the models uploaded by the participants. Poisoning attacks often occur during this step.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Training the local model.</span> Each participant downloads a global model from the server. Then, the participant trains this global model with local data. This training process is done locally. As a result, raw data from participants does not leave the local area. Finally, each participant uploads the trained local model to the server. In this step, the attacker may be a participant who injects malicious behavior into the local model or an eavesdropper who steals information from a communication channel. In other words, the attacker, as a participant, influences the training of the FL by sending local models with malicious behaviors to the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Besides, this attacker can also infer some sensitive information from the global model that the server sends each time. The attacker can also use the information stolen from the communication channel to carry out the attack. The communication channel transmits the local models uploaded by the participants and the global model broadcast by the server. During this uploading and downloading process, an attacker may tamper with or steal the models, thus affecting the training of the global model and compromising the privacy of the participants. Poisoning attacks and inference attacks are common here.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Updating the global model.</span> The server collects the local models uploaded by all participants and then performs a model aggregation operation. This step’s goal is to recombine all of the collected local models into a global model. The aggregation algorithm is involved here. FedAvg is the classical aggregation algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which obtains a new common model by weighting the parameters of all the models and averaging them. In this step, the server may be injected with malicious behavior by the attacker, which can affect the training of the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. In addition to this, the malicious server receives local models from all participants. The attacker can use the local models uploaded by the participants to perform inference attacks, or he can use the generated models to reconstruct the training set of the participants. These actions may infer sensitive information about the participants and thus threaten the privacy and security of the participants.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Attacks in FL</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">There are many types of attacks in FL, with different standards for the classification of attacks in previous studies. For instance, Chen <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> classified attacks based on three different principles of information security named CIA (confidentiality, integrity, and availability). Liu <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> listed the corresponding possible attacks based on the three phases of the FL (including data auditing, training, and predicting). Wu <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> enumerated types of attacks according to the potential attackers of the FL system (including local workers, the central server, and the eavesdropper). In this section, we categorize and introduce the major attacks using the above criteria. As shown in Table <a href="#S3.T1" title="TABLE I ‣ III Attacks in FL ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we retain two classification standards: potential attack roles and three principles of information security. Since most attacks exist in the latter two phases, we remove the data auditing phase <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. In addition, FL is usually in the training phase in most cases. However, we believe that for attackers, when the model is trained to a certain extent, they can enter the prediction stage and carry out some attacks through this stage, even though the model is not fully trained. Therefore, in our opinion, the prediction stage is also an important part of federated learning.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Classification of attacks in FL</figcaption><img src="/html/2211.14952/assets/x3.png" id="S3.T1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="196" height="85" alt="[Uncaptioned image]">
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Poisoning Attacks</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">There are some different classification standards for poisoning attacks. Specifically, they can be divided into data poisoning and model poisoning according to the poisoning objects, and can be divided into random attacks and backdoor attacks (target attacks) according to whether the attack is targeted. It violates the integrity principle of FL by making the global model unusable under certain conditions.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Data poisoning refers to an attacker tampering with or adding data to the training set maliciously, which eventually leads to the destruction or hijacking of the model. This attack is usually implemented by the local worker, who is the owner of the data. Later, the idea of clean-label poisoning comes up <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. This is a backdoor attack that adds malicious data instead of changing it. Conversely, dirty data poisoning usually involves changing the original dataset through label-flipping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> or the generation of toxic samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Model poisoning means that the attacker changes the parameters of the target model directly, causing errors in the global model or leaving a backdoor. It has been shown to be more effective than data poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Perturbing the weights of convolutional neural networks in a targeted manner can be used to insert backdoors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. By using available bit-flipping techniques, the target model can be converted into the Trojan infection model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. Since the model needs to be transmitted repeatedly between the local worker and the center server, either one of them could be an attacker.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Inference Attacks</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Although the data transmitted from the client to the server is not the original data, there is still a risk of leakage. Inference attacks indicate that attackers can use the eavesdropped information to infer useful information to a certain extent, which obviously destroys the confidentiality of the model. The member inference attack was first proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which trains a model with similar functions on a specified sample and then judges whether the sample is used for the target model’s training. This training method is called the Shadow training technique. What’s more, the attribute inference attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> aims to determine whether a participant’s data is relevant to a certain attribute. There have been studies to prove that this attack can infer the accent information of the training set in FL of speech recognition. According to a recent study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, sensitive information represented by labels can potentially be inferred from user-uploaded gradients.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Reconstruction Attacks</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Unlike inference attacks that cannot reveal raw data, reconstruction attacks can obtain the original information of the training dataset by collecting some information such as predicted confidence values, model parameters, and gradients. Therefore, this type of attack is also a confidentiality attack. The model inversion attack is one of the reconstruction attacks and was first proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. This model shows that when the adversary has white-box access rights, the adversary can use the linear model to estimate the attribute information of the original data. Another attack method is generative adversarial networks (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and it points out that attackers can obtain samples of other participants, and this process only requires black-box access. And Deep Leakage from Gradients (DLG) shows that the attacker could recover the original data by analyzing the gradient information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. Then, the Improved Deep Leakage from Gradients (IDLG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> improves the accuracy of data recovery. Inverting gradients based on DLG were proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> and it broadened the attack scenario to include the actual industrialized scenario rather than being limited to the strong assumption of low-resolution recovery and a shallow network. Recently, Generative Regression Neural Network (GRNN) based on GAN was proposed to restore the original training data without the need for additional information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. It indicated that GRNN has stronger robustness and higher accuracy than previous methods.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Model Extraction Attack</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">When FL has finished training the model, the global model will serve outsiders in the form of an API. At this time, the user may query the relevant information of the target model through the API loop and finally achieve the effect of extracting the model. Tramèr <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> first demonstrated that this attack will be effective when the attacker has the same distribution of data and model-related information as the model. The attack proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> can obtain hyperparameters located at the bottom layer of the model. Orekondy <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> proposed the Knockoff Net, through which attackers can steal based on the confidence value output by the API, and the stealing effect is positively correlated with the complexity of the target model.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Evasion Attacks</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Evasion attack is a type of attack in which an attacker deceives the target machine learning system by constructing specific input samples without changing the target machine learning system. It usually occurs during the prediction phase, when the model has finished training. The effect of this kind of attack can be summarized as the model extrapolates the original answer “A” to be the wrong answer “B”. Main feature of it is a wide spread of hazards, including road sign recognition for autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, voice recognition system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, etc. Evasion attack is an integrity attack due to the spoofing of the model.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.5.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.6.2" class="ltx_text ltx_font_italic">Byzantine Attacks</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Byzantine attack is a type of attack in which an attacker hides among the participants of FL and arbitrarily uploads malicious data, aiming to destroy the global model (e.g., model availability). To deal with this attack, it is common to combine stochastic gradient descent (SGD) with different robust aggregation rules (e.g., Krum, Median). However, the stochastic gradient noise induced by SGD makes it impossible for the server to judge whether it is malicious information or the noise of real information, which becomes an exploit point for attackers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. Xie <span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> use a method called “inner product manipulation” to make the aggregated vector direction in the server inconsistent with the true gradient, thereby causing SGD to fail. Similar to this idea, it has been proven that by poisoning the local model, the global model has a large test error rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p"><span id="S3.SS6.p2.1.1" class="ltx_text ltx_font_bold">Discussion:</span> Whether an attack is significant depends on the depth of its damage and the scope of its damage. We can see that each of the above attacks makes assumptions about the attacker’s privileges or attack capabilities. Obviously, an excellent attack should have as few assumptions as possible, which mean that it is more applicable to a wider range of scenarios and more difficult to be detected by defense strategies. Therefore, making fewer assumptions should be a principle for designing future attack strategies, and the corresponding defense strategy should also detect the difference between malicious participants and attacking participants as much as possible. In conclusion, comprehensive and in-depth attack research can promote the development of defense.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Defenses in FL</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we describe several defense methods in the FL environment at the privacy level and the security level, respectively.
The goal of the security-based defense policy is different from that of the privacy-based defense policy. More specifically, privacy refers to private information that a person does not want others to know and invade, focusing on sensitive personal information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>; security focuses on confidential data and information assets, not just personal information. As we all know, the CIA follows three core security principles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. The attack on FL
is considered at the privacy level, when the attacker tries to infer private information about the participant. Privacy protection methods are used to defend against privacy attacks and to ensure that sensitive data is not leaked to others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. The security attack is a malicious action performed by hackers with specialized knowledge to compromise the confidentiality, integrity, and availability of data and models. And the security defense aims to improve the FL framework’s CIA.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Defenses at the Privacy Level</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this section, some privacy protection methods are discussed for defending against attacks at the privacy level. Table <a href="#S4.T2" title="TABLE II ‣ IV-A Defenses at the Privacy Level ‣ IV Defenses in FL ‣ Federated Learning Attacks and Defenses: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> describes these approaches, including the types of attacks they defend against and their shortcomings.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Privacy protection methods in FL.</figcaption><img src="/html/2211.14952/assets/x4.png" id="S4.T2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="196" height="103" alt="[Uncaptioned image]">
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Data anonymization.</span> In order to defend against privacy attacks, we can use anonymization techniques to hide or remove sensitive personal attributes, such as personally identifiable information (PII) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, so that the attacker cannot identify a specific individual through the data. K-anonymity, L-diversity, and T-closeness are three common anonymization techniques. Anonymization techniques have been used to improve the privacy of FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. This type of approach improves privacy by hiding or removing sensitive information, but may reduce the usability of the dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. In addition, much anonymized data can be easily “de-anonymized”. Thus, data anonymization is often used in conjunction with other ways to protect privacy.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Differential privacy (DP)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. This is a common approach for protecting privacy in FL frameworks by adding noise to the uploaded data, making it impossible for an attacker to access the original data or model. It hinders the reverse data retrieval from the attacker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>. DP defends against attacks during the training phase and the prediction phase of FL. Those types of attacks that DP defends against are poisoning attack, inference attack, evasion attack, reconstruction attack, and model inversion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. Although DP can improve the privacy of the FL framework, the utility of the models can be seriously affected if too much noise is added. Therefore, how to balance privacy and utility is an important issue when using DP in the FL framework.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Secure multi-party computation (SMC)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. SMC is a generic cryptographic primitive for solving privacy-preserving collaborative computation problems between a set of mutually distrusting participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. SMC does not leak the input and output of the participant to other members participating in the computation. In the FL framework, SMC is able to defend against inference attacks, reconstruction attacks, model inversion, and leaks from malicious center servers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. SMC contains complex cryptographic operations, which leads to a large computational overhead and high performance loss. This may reduce the participants’ interest in cooperating, so SMC is not suitable for large-scale FL scenarios.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Homomorphic encryption (HE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>.</span> The principle that HE can protect data privacy is that it does not touch the original data. HE first encrypts the data, then processes it, and finally decrypts it. HE defends against attacks during the training phase and the prediction phase of FL. Although HE provides strict privacy guarantees, HE incurs a large computational overhead <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. Therefore, this approach is not suitable for FL scenarios with numerous participants and devices with limited computational resources.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_bold">Trusted execution environment (TEE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>.</span> TEE is a tamper-proof and trusted ecosystem for executing authenticated and verified code. In the FL framework, TEE establishes digital trust by protecting devices in FL, which effectively prevents attackers from attacking private information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. TEE can defend against attacks such as MIA, PIA, mode inversion, and malicious server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. However, TEE has limited execution space, which prevents complex transaction logic from being executed.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p"><span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_bold">Blockchain</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>. It is a distributed ledger technology that uses the blockchain to verify and store data, generates and updates data through a consensus mechanism, and involves an intelligent contract and an incentive mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. What’s more, blockchain technology is decentralized, tamper-proof, unforgeable, auditable, and accountable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. Blockchain is a preferred solution to the problem of implementing data security and data validation in a non-centralized FL scenario <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In addition, the verifiability of blockchain will reduce the impact of poisoning attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>. This method has the drawback that it can’t be used in FL situations with a lot of people and devices with limited computing power.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Defenses at the Security Level</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Vulnerabilities in FL provide an opportunity for curious attackers or malicious attackers to gain unauthorized access. To ensure the security of the FL framework, we want to scan for all vulnerabilities as much as possible. There are three main sources of vulnerabilities: insecure communication channels, malicious clients, and central parameter servers that are not robust or secure enough. The defense methods for security breaches can be divided into active and passive defenses. The purpose of active defense is to detect and mitigate the risk to the FL framework in advance, before it has an impact on the framework. The purpose of passive defense is to remediate and mitigate the impact when an attack has already occurred. The security defense approach is closely related to the CIA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, which is the core of information security and lays the foundation for all security-based frameworks. The CIA refers to the three qualities of information security—confidentiality, integrity, and availability—that are usually used to analyze security defense methods in the FL framework.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Confidentiality</span>. The types of attacks that compromise confidentiality are inference attack, reconstruction attack, and model extraction attack. Nasr <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> showed that data can be inferred by considering model weights in the FL environment. It has also been demonstrated that local datasets can be reconstructed by gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. In addition to the privacy-preserving approaches, many defensive approaches have been proposed to ensure the confidentiality of data. For example, VerifyNet, a verifiable FL framework, can guarantee the confidentiality of the gradients uploaded by participants using the proposed double masking protocols <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Integrality</span>. Poisoning attack and Evasion attack are types of attacks that compromise integrity. The primary purpose of ensuring data integrity is to ensure that once data is collected, it cannot be tampered with. Common methods to ensure data integrity are TEE and blockchain. TEE enables end-to-end security and authentication. People have applied TEE to the FL framework to detect participants who violate training integrity protocols <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>. Blockchain is tamper-evident, decentralized, and protected against single points of failure, and these properties meet the requirements of integrity protection. In addition, there are several methods to ensure integrity by screening malicious clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Availability</span>. The attacks on availability in the FL framework are related to Byzantine attacks. Until now, many kinds of defense methods against Byzantine attacks have been proposed. For example, the Krum algorithm is an aggregation rule with resilience properties that can be used to defend against Byzantine attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>. Implementing incentives in the FL framework is a good way to improve data availability by rewarding or penalizing participants based on the value of their contributions. This reduces the possibility of participants sending useless or harmful data, and also improves the usability of the training model.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Discussion:</span> It is clear from the above analysis that each defense method focuses on addressing one or more types of attack methods. There is no single defense method that can address all types of attacks. That is, if we want to keep the FL framework as secure and private as possible, we need to combine multiple defense methods into the FL framework in a harmonious way. How to choose the right defense method is an important issue. In addition to considering the defense capability, the utility of the data or model and cost are also important considerations. The defense method should not sacrifice the utility of the data or model; otherwise, it has little application value. In addition, defense methods require expensive equipment or cost, which greatly limits the application scenarios.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Challenges and Promising Directions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">From the various attacks and defenses mentioned above, it can be seen that although the FL framework and the corresponding techniques can protect data to a certain extent, there are still many security issues to be solved. After sorting through and analyzing the relevant work from the past few years, we’ve come up with a list of several ways that could be improved and deserve more research:</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Trade-off on Security, Communication, and Computing</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">While measures such as homomorphic encryption and differential privacy protect the security of privacy, they can also reduce FL performance, including increased communication delay and increased computing load. In future research, applying cloud computing to FL may be a way to optimize performance with the help of edge computing technology. In addition, recent related research considers choosing an appropriate security strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, with the expectation that it will maximize communication efficiency and computational efficiency as much as possible while ensuring system security.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Emphasis on Robustness or Emphasis on Privacy?</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Current research on robustness and privacy is inherently conflicted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>. On the one hand, since robustness pursues the universality of defenses against adversarial attacks, it is necessary to find commonalities between different attacks, which requires greater access to data and models. However, FL does not comply with the principles in GDPR<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/General_Data_Protection_Regulation</a></span></span></span>, a regulation designed to protect data, and the pursuit of robustness may exacerbate this problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. On the other hand, privacy pursues a comprehensive defense against a certain type of attack with encryption and other means, but may overlook opportunities for other types of attacks (e.g., a lack of robustness). Exploring how to strike a balance between them is a direction worthy of future research.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Contribution Measurements and Incentive Strategies</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We know that it is not easy to guarantee that all clients are honest and well-intentioned. In the FL system, we define individuals who benefit from collective resources or public goods but do not pay anything as free-riders. In general, each client receives an incentive or reward for making a contribution within the FL system. In this way, those free-riders also get incentives for free by sending fake updates to the global model. Previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> extensively discussed and analyzed this phenomenon in peer-to-peer (P2P) networks. A defensive method called Standard deviation-Deep Autoencoding Gaussian Mixture Model (STD-DAGMM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> was proposed to identify free-riders and prevent them from receiving updated models and rewards. As more and more workers emerge into the FL system, certain defensive strategies need to be adopted to deal with this malicious behavior for the purpose of improving the accuracy and fairness of the FL system. Therefore, it is necessary to explore other incentive strategies to detect fake model updates and assess the actual contribution of each client.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Attacks and Defenses in the New Form of FL</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">As mentioned above, the current research on attacks and defenses of FL mainly focuses on the most basic form, e.g., HFL with a homogeneous model architecture. For FL with heterogeneous model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, VFL, and FTL, the applicability of existing privacy-preserving techniques and attack defense mechanisms has not been studied fully and empirically. In addition, based on the innovative idea that each participant can become a server, decentralized FL has received relevant research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>. Does the rotation of server roles and permissions aggravate or distract from security risks? Security research in this area is also a future research direction.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS5.5.1.1" class="ltx_text">V-E</span> </span><span id="S5.SS5.6.2" class="ltx_text ltx_font_italic">Another Interpretation of the Means of Attacks</span>
</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">Note that attacks and defenses are inseparable. Only if the attack keeps developing can the defense keep improving. Thereafter, a thorough study of attacks can promote the development of defenses to a certain extent, thus promoting the development of FL. Apart from threatening models, some attack methods can also be used to study good technical applications. For example, Adi <span id="S5.SS5.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> found that the production, embedding, and verification in the backdoor attack were consistent with the three processes of the watermarking mechanism. Therefore, backdoor technology is used to watermark deep neural networks in a black-box way for intellectual property protection. With the in-depth study of attack and defense problems in FL, a promising research direction is how to use these technologies to address some of the limits of privacy protection in real life, in addition to focusing on attack and defense technology itself.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS6.5.1.1" class="ltx_text">V-F</span> </span><span id="S5.SS6.6.2" class="ltx_text ltx_font_italic">Applications of the FL Systems</span>
</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">It is key to strengthen the defensive strategy in the FL system, integrate privacy protection measures such as secure multi-party computing or differential privacy, and build an enhanced privacy protection FL system framework to simulate the attack and the defense in reality. There are a few open-source systems that provide FL frameworks for researchers and developers to continuously improve and upgrade them, including Google’s TFF<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.tensorflow.org/federated/federated_learning" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated/federated_learning</a></span></span></span>, PySyft<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://blog.openmined.org/tag/pysyft/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://blog.openmined.org/tag/pysyft/</a></span></span></span> based on the PyTorch framework, and FATE<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://fate.fedai.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fate.fedai.org/</a></span></span></span> developed by WeBank. However, it should be noted that, besides PySyft, there is currently no framework or system that can incorporate and execute differential privacy or secure multi-party computation in real-world applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Thereafter, integrating constantly updated privacy protection technologies into FL systems is a challenging direction. It will also be important to use FL systems in more areas of life.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Federated learning, as a new breed of artificial intelligence, is currently in a baby state. It follows local computing and model transmission, two concepts that reduce the cost and risk of privacy leakage brought by traditional centralized machine learning approaches. Although FL can solve some real-world problems, there are still many potential threats. Aiming at providing a comprehensive survey and giving readers a clear view and understanding of attacks and defenses in FL, we introduce and describe the existing work and research of the FL framework in five parts: background, characteristics, classification, systematic attack approaches, and systematic defense strategies. Then, we classify the possible attacks and threats according to the current three classification criteria, list the attack methods for each category, and introduce the corresponding principle of the specific attack. Later on, against these attacks and threats, the specific defense measures are summarized in two parts: privacy and security, respectively. Finally, we discuss six challenges in FL from the perspective of attacks and defenses. We also highlight several promising directions for future work in this quite active research area.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was supported in part by Key-Area Research and Development Program of Guangdong Province (No. 2020B0101090004), National Natural Science Foundation of China (Nos. 62002136, 62272196, 61932011), Natural Science Foundation of Guangdong Province (No. 2022A1515011861), Guangzhou Basic and Applied Basic Research Foundation (Nos. 202102020277, 2019B1515120010), Guangdong Key R&amp;D Plan2020 (No. 2020B0101090002), Fundamental Research Funds for the Central Universities of Jinan University (No. 21622416), the Young Scholar Program of Pazhou Lab (No. PZL2021KF0023), National Key Research and Development Plan of China (No. 2020YFB1005600), and National Joint Engineering Research Center for Network Security Detection and Protection Technology, Guangdong Key Laboratory for Data Security and Privacy Preserving. Dr. Wensheng Gan is the corresponding author of this paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology</em>, vol. 10, no. 2, pp. 1–19, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C. Zhang, Y. Xie, H. Bai, B. Yu, W. Li, and Y. Gao, “A survey on federated
learning,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, vol. 216, p. 106775, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing
Magazine</em>, vol. 37, no. 3, pp. 50–60, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advances
and open problems in federated learning,” <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">Foundations and
Trends® in Machine Learning</em>, vol. 14, no. 1–2, pp. 1–210,
2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>.   PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Yang, Y. Duan, T. Qiao, H. Zhou, J. Wang, and W. Zhao, “Prototyping
federated learning on edge computing systems.” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Frontiers Comput.
Sci.</em>, vol. 14, no. 6, p. 146318, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain and federated
learning for privacy-preserved data sharing in industrial iot,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Industrial Informatics</em>, vol. 16, no. 6, pp. 4177–4186,
2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
G. Long, Y. Tan, J. Jiang, and C. Zhang, “Federated learning for open
banking,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Federated Learning</em>.   Springer, 2020, pp. 240–254.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Kumar and R. Singla, “Federated learning systems for healthcare:
perspective and recent progress,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Federated Learning Systems</em>, pp.
141–156, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. R. Finkel and C. D. Manning, “Hierarchical joint learning: Improving joint
parsing and named entity recognition with non-jointly labeled data,” in
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">48th Annual Meeting of the Association for Computational Linguistics</em>,
2010, pp. 720–728.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
W. Gan, J. C. W. Lin, H. C. Chao, and J. Zhan, “Data mining in distributed
environment: a survey,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Wiley Interdisciplinary Reviews: Data Mining
and Knowledge Discovery</em>, vol. 7, no. 6, p. e1216, 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W. Gan, J. C. W. Lin, P. Fournier-Viger, H. C. Chao, V. S. Tseng, and P. S. Yu,
“A survey of utility-oriented pattern mining,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Knowledge and Data Engineering</em>, vol. 33, no. 4, pp. 1306–1327, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Dwork, “Differential privacy: A survey of results,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International
Conference on Theory and Applications of Models of Computation</em>.   Springer, 2008, pp. 1–19.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ACM SIGSAC
Conference on Computer and Communications Security</em>, 2016, pp. 308–318.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Li, W. Gan, Y. Gui, Y. Wu, and P. S. Yu, “Frequent itemset mining with
local differential privacy,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">the 31st ACM International Conference
on Information and Knowledge Management</em>, 2022, pp. 1146–1155.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
L. Sweeney, “k-anonymity: A model for protecting privacy,”
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Journal of Uncertainty, Fuzziness and Knowledge-based
Systems</em>, vol. 10, no. 05, pp. 557–570, 2002.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. LeFevre, D. J. DeWitt, and R. Ramakrishnan, “Mondrian multidimensional
k-anonymity,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE 22nd International Conference on Data
Engineering</em>.   IEEE, 2006, pp. 25–25.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam,
“l-diversity: Privacy beyond k-anonymity,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on
Knowledge Discovery from Data</em>, vol. 1, no. 1, pp. 3–es, 2007.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, vol. 115, pp. 619–640, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Liu, Y. Kang, C. Xing, T. Chen, and Q. Yang, “A secure federated transfer
learning framework,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em>, vol. 35, no. 4, pp.
70–82, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Daemen and V. Rijmen, “AES proposal: Rijndael,” 1999.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Zhang, W. Wu, and Y. Zheng, “Security of SM4 against (related-key)
differential cryptanalysis,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">International Conference on
Information Security Practice and Experience</em>.   Springer, 2016, pp. 65–78.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. J. Wiener, “Cryptanalysis of short RSA secret exponents,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Information Theory</em>, vol. 36, no. 3, pp. 553–558, 1990.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, “Property inference
attacks on fully connected neural networks using permutation invariant
representations,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ACM SIGSAC Conference on Computer and
Communications Security</em>, 2018, pp. 619–633.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and G. Felici,
“Hacking smart machines with smarter ones: How to extract meaningful data
from machine learning classifiers,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Journal of Security
and Networks</em>, vol. 10, no. 3, pp. 137–150, 2015.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting unintended
feature leakage in collaborative learning,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE Symposium on
Security and Privacy</em>.   IEEE, 2019, pp.
691–706.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference
attacks against machine learning models,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Symposium on
Security and Privacy</em>.   IEEE, 2017, pp.
3–18.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that
exploit confidence information and basic countermeasures,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">the 22nd
ACM SIGSAC Conference on Computer and Communications Security</em>, 2015, pp.
1322–1333.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>, 2014.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y. Li, Y. Jiang, Z. Li, and S. T. Xia, “Backdoor learning: A survey,”
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J.-y. Sohn,
K. Lee, and D. Papailiopoulos, “Attack of the tails: Yes, you really can
backdoor federated learning,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, vol. 33, pp. 16 070–16 084, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
L. Garber, “Denial-of-service attacks rip the internet,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">computer</em>,
vol. 33, no. 04, pp. 12–17, 2000.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
C. Dwork, A. Roth <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The algorithmic foundations of differential
privacy,” <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">Foundations and Trends® in Theoretical
Computer Science</em>, vol. 9, no. 3–4, pp. 211–407, 2014.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
O. Goldreich, “Secure multi-party computation,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Manuscript. Preliminary
version</em>, vol. 78, p. 110, 1998.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
C. Zhao, S. Zhao, M. Zhao, Z. Chen, C. Z. Gao, H. Li, and Y. a. Tan, “Secure
multi-party computation: theory, practice and applications,”
<em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Information Sciences</em>, vol. 476, pp. 357–372, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
M. Naehrig, K. Lauter, and V. Vaikuntanathan, “Can homomorphic encryption be
practical?” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">3rd ACM Workshop on Cloud Computing Security Workshop</em>,
2011, pp. 113–124.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
P. Martins, L. Sousa, and A. Mariano, “A survey on fully homomorphic
encryption: An engineering perspective,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>,
vol. 50, no. 6, pp. 1–33, 2017.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to backdoor
federated learning,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial
Intelligence and Statistics</em>.   PMLR,
2020, pp. 2938–2948.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
X. Luo, Y. Wu, X. Xiao, and B. C. Ooi, “Feature inference attack on model
predictions in vertical federated learning,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE 37th
International Conference on Data Engineering</em>.   IEEE, 2021, pp. 181–192.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M. S. Jere, T. Farnan, and F. Koushanfar, “A taxonomy of attacks on federated
learning,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Security &amp; Privacy</em>, vol. 19, no. 2, pp. 20–28,
2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
L. Lyu, H. Yu, and Q. Yang, “Threats to federated learning: A survey,”
<em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.02133</em>, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
P. Liu, X. Xu, and W. Wang, “Threats, attacks and defenses to federated
learning: issues, taxonomy and perspectives,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Cybersecurity</em>, vol. 5,
no. 1, pp. 1–19, 2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M. Chen, J. Zhang, and T. Li, “Threats and defenses of federated learning: a
survey,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Computer Science</em>, vol. 49, no. 7, p. 310, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J. Wu, S. Si, J. Wang, and J. Xiao, “Threats and defenses of federated
learning: a survey,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Big Data Research</em>, vol. 8, no. 5, p. 12, 2022.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A. Qammar, J. Ding, and H. Ning, “Federated learning attack surface: taxonomy,
cyber defences, challenges, and future directions,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence Review</em>, vol. 55, no. 5, pp. 3569–3606, 2022.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. J. Pan and Q. Yang, “A survey on transfer learning,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Knowledge and Data Engineering</em>, vol. 22, no. 10, pp.
1345–1359, 2009.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Q. Jing, W. Wang, J. Zhang, H. Tian, and K. Chen, “Quantifying the performance
of federated transfer learning,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1912.12795, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
T. Liu, Q. Yang, and D. Tao, “Understanding how feature structure transfers in
transfer learning.” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">the 26th International Joint Conference on
Artificial Intelligence</em>, 2017, pp. 2365–2371.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, and Y.-G. Jiang, “Clean-label
backdoor attacks on video recognition models,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference
on Computer Vision and Pattern Recognition</em>, 2020, pp. 14 443–14 452.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
C. Fung, C. J. Yoon, and I. Beschastnikh, “Mitigating sybils in federated
learning poisoning,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.04866</em>, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks
against federated learning systems,” in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">European Symposium on Research
in Computer Security</em>.   Springer, 2020,
pp. 480–501.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks on deep
learning systems using data poisoning,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1712.05526</em>, 2017.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “BadNets: Evaluating
backdooring attacks on deep neural networks,” <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 7,
pp. 47 230–47 244, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing federated
learning through an adversarial lens,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>.   PMLR, 2019, pp.
634–643.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
J. Dumford and W. Scheirer, “Backdooring convolutional neural networks via
targeted weight perturbations,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">IEEE International Joint Conference
on Biometrics</em>.   IEEE, 2020, pp. 1–9.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
A. S. Rakin, Z. He, and D. Fan, “TBT: Targeted neural network attack with
bit trojan,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2020, pp. 13 198–13 207.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
A. Wainakh, F. Ventola, T. Müßig, J. Keim, C. G. Cordero, E. Zimmer,
T. Grube, K. Kersting, and M. Mühlhäuser, “User label leakage from
gradients in federated learning,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.09369</em>,
2021.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart, “Privacy
in pharmacogenetics: An End-to-End case study of personalized warfarin
dosing,” in <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">23rd USENIX Security Symposium</em>, 2014, pp. 17–32.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
B. Hitaj, G. Ateniese, and F. Perez-Cruz, “Deep models under the gan:
information leakage from collaborative deep learning,” in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">ACM SIGSAC
Conference on Computer and Communications Security</em>, 2017, pp. 603–618.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
L. Zhu, Z. Liu, and S. Han, “Deep leakage from gradients,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>, vol. 32, 2019.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
B. Zhao, K. R. Mopuri, and H. Bilen, “iDLG: Improved deep leakage from
gradients,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.02610</em>, 2020.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
J. Geiping, H. Bauermeister, H. Dröge, and M. Moeller, “Inverting
gradients-how easy is it to break privacy in federated learning?”
<em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, pp.
16 937–16 947, 2020.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
H. Ren, J. Deng, and X. Xie, “GRNN: Generative regression neural network—a
data leakage attack for federated learning,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on
Intelligent Systems and Technology</em>, vol. 13, no. 4, pp. 1–24, 2022.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction APIs,” in <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">25th USENIX
security symposium</em>, 2016, pp. 601–618.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
B. Wang and N. Z. Gong, “Stealing hyperparameters in machine learning,” in
<em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">IEEE Symposium on Security and Privacy</em>.   IEEE, 2018, pp. 36–52.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
T. Orekondy, B. Schiele, and M. Fritz, “Knockoff nets: Stealing functionality
of black-box models,” in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and
Pattern Recognition</em>, 2019, pp. 4954–4963.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J. Lu, H. Sibai, and E. Fabry, “Adversarial examples that fool detectors,”
<em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.02494</em>, 2017.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to a
crime: Real and stealthy attacks on state-of-the-art face recognition,” in
<em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">ACM Sigsac Conference on Computer and Communications Security</em>, 2016,
pp. 1528–1540.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner,
and W. Zhou, “Hidden voice commands,” in <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">25th USENIX Security
Symposium</em>, 2016, pp. 513–530.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Z. Wu, Q. Ling, T. Chen, and G. B. Giannakis, “Federated variance-reduced
stochastic gradient descent with robustness to byzantine attacks,”
<em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal Processing</em>, vol. 68, pp. 4583–4596, 2020.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
C. Xie, O. Koyejo, and I. Gupta, “Fall of empires: Breaking byzantine-tolerant
sgd by inner product manipulation,” in <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Uncertainty in Artificial
Intelligence</em>.   PMLR, 2020, pp.
261–270.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
M. Fang, X. Cao, J. Jia, and N. Gong, “Local model poisoning attacks to
Byzantine-Robust federated learning,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">29th USENIX Security
Symposium</em>, 2020, pp. 1605–1622.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
L. Lyu, H. Yu, X. Ma, L. Sun, J. Zhao, Q. Yang, and P. S. Yu, “Privacy and
robustness in federated learning: Attacks and defenses,” <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2012.06337</em>, 2020.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
M. Alazab, S. P. RM, M. Parimala, P. K. R. Maddikunta, T. R. Gadekallu, and
Q. V. Pham, “Federated learning for cybersecurity: Concepts, challenges, and
future directions,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial Informatics</em>,
vol. 18, no. 5, pp. 3501–3509, 2021.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
X. Yin, Y. Zhu, and J. Hu, “A comprehensive survey of privacy-preserving
federated learning: A taxonomy, review, and future directions,” <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">ACM
Computing Surveys</em>, vol. 54, no. 6, pp. 1–36, 2021.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
P. M. Schwartz and D. J. Solove, “The PII problem: Privacy and a new concept
of personally identifiable information,” <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">New York University Law
Review</em>, vol. 86, p. 1814, 2011.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
M. Song, Z. Wang, Z. Zhang, Y. Song, Q. Wang, J. Ren, and H. Qi, “Analyzing
user-level privacy attack against federated learning,” <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on
Selected Areas in Communications</em>, vol. 38, no. 10, pp. 2430–2444, 2020.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
O. Choudhury, A. Gkoulalas-Divanis, T. Salonidis, I. Sylla, Y. Park, G. Hsu,
and A. Das, “A syntactic approach for privacy-preserving federated
learning,” in <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">European Conference on Artificial Intelligence</em>.   IOS Press, 2020, pp. 1762–1769.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
M. M. Merener, “Theoretical results on de-anonymization via linkage attacks.”
<em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Transactions on Data Privacy</em>, vol. 5, no. 2, pp. 377–402, 2012.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
R. Gosselin, L. Vieu, F. Loukil, and A. Benoit, “Privacy and security in
federated learning: A survey,” <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 12, no. 19, p.
9901, 2022.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
N. Truong, K. Sun, S. Wang, F. Guitton, and Y. Guo, “Privacy preservation in
federated learning: An insightful survey from the gdpr perspective,”
<em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Security</em>, vol. 110, p. 102402, 2021.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
N. Bouacida and P. Mohapatra, “Vulnerabilities in federated learning,”
<em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 9, pp. 63 229–63 249, 2021.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
J. Zhang, H. Zhu, F. Wang, J. Zhao, Q. Xu, H. Li <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Security and
privacy threats to federated learning: Issues, methods, and challenges,”
<em id="bib.bib83.2.2" class="ltx_emph ltx_font_italic">Security and Communication Networks</em>, vol. 2022, 2022.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
P. Paillier, “Public-key cryptosystems based on composite degree residuosity
classes,” in <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">International Conference on the Theory and Applications
of Cryptographic Techniques</em>.   Springer, 1999, pp. 223–238.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
M. Sabt, M. Achemlal, and A. Bouabdallah, “Trusted execution environment: what
it is, and what it is not,” in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">IEEE Trustcom/BigDataSE/ISPA</em>,
vol. 1.   IEEE, 2015, pp. 57–64.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Z. Zheng, S. Xie, H. Dai, X. Chen, and H. Wang, “An overview of blockchain
technology: Architecture, consensus, and future trends,” in <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">IEEE
international congress on big data</em>.   Ieee, 2017, pp. 557–564.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
W. Issa, N. Moustafa, B. Turnbull, N. Sohrabi, and Z. Tari, “Blockchain-based
federated learning for securing internet of things: A comprehensive survey,”
<em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 2022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
M. Nasr, R. Shokri, and A. Houmansadr, “Comprehensive privacy analysis of deep
learning: Passive and active white-box inference attacks against centralized
and federated learning,” in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">IEEE symposium on security and
privacy</em>.   IEEE, 2019, pp. 739–753.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
G. Xu, H. Li, S. Liu, K. Yang, and X. Lin, “Verifynet: Secure and verifiable
federated learning,” <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and
Security</em>, vol. 15, pp. 911–926, 2019.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Y. Chen, F. Luo, T. Li, T. Xiang, Z. Liu, and J. Li, “A training-integrity
privacy-preserving federated learning scheme with trusted execution
environment,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Information Sciences</em>, vol. 522, pp. 69–79, 2020.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
N. Rodríguez-Barroso, E. Martínez-Cámara, M. Luzón, G. G.
Seco, M. Á. Veganzones, and F. Herrera, “Dynamic federated learning
model for identifying adversarial clients,” <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2007.15030</em>, 2020.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
P. Blanchard, E. M. El Mhamdi, R. Guerraoui, and J. Stainer, “Machine learning
with adversaries: Byzantine tolerant gradient descent,” <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
M. Hao, H. Li, G. Xu, S. Liu, and H. Yang, “Towards efficient and
privacy-preserving federated deep learning,” in <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">International
Conference on Communications</em>.   IEEE,
2019, pp. 1–6.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
C. A. Choquette-Choo, N. Dullerud, A. Dziedzic, Y. Zhang, S. Jha, N. Papernot,
and X. Wang, “CaPC Learning: Confidential and private collaborative
learning,” in <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>.   OpenReview.net,
2021.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Y. M. Tseng and F. G. Chen, “A free-rider aware reputation system for
peer-to-peer file-sharing networks,” <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Expert Systems with
Applications</em>, vol. 38, no. 3, pp. 2432–2440, 2011.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
J. Lin, M. Du, and J. Liu, “Free-riders in federated learning: Attacks and
defenses,” <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.12560</em>, 2019.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
O. Litany, H. Maron, D. Acuna, J. Kautz, G. Chechik, and S. Fidler, “Federated
learning with heterogeneous architectures using graph hypernetworks,”
<em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08459</em>, 2022.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
L. Qu, Y. Zhou, P. P. Liang, Y. Xia, F. Wang, E. Adeli, L. Fei-Fei, and
D. Rubin, “Rethinking architecture design for tackling data heterogeneity in
federated learning,” in <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and
Pattern Recognition</em>, 2022, pp. 10 061–10 071.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
C. Korkmaz, H. E. Kocas, A. Uysal, A. Masry, O. Ozkasap, and B. Akgun, “Chain
FL: Decentralized federated machine learning via blockchain,” in
<em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">the 2nd International Conference on Blockchain Computing and
Applications</em>.   IEEE, 2020, pp.
140–146.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Y. Hu, Y. Zhou, J. Xiao, and C. Wu, “GFL: A decentralized federated learning
framework based on blockchain,” <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.10996</em>,
2020.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
J. Li, Y. Shao, K. Wei, M. Ding, C. Ma, L. Shi, Z. Han, and H. V. Poor,
“Blockchain assisted decentralized federated learning (BLADE-FL):
Performance analysis and resource allocation,” <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Parallel and Distributed Systems</em>, vol. 33, no. 10, pp. 2401–2415, 2021.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning your weakness
into a strength: Watermarking deep neural networks by backdooring,” in
<em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">27th USENIX Security Symposium</em>, 2018, pp. 1615–1631.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.14951" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.14952" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.14952">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.14952" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.14953" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 07:09:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
