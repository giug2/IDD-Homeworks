<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images</title>
<!--Generated on Mon Sep 30 18:58:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Texture transfer,  BRDF material,  diffusion model,  synthetic data,  3D garments reconstruction" lang="en" name="keywords"/>
<base href="/html/2410.01801v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S1" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS1" title="In 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Image-based 3D Garment Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS1.SSS1" title="In 2.1. Image-based 3D Garment Modeling ‣ 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Image-to-mesh texture transfer.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS1.SSS2" title="In 2.1. Image-based 3D Garment Modeling ‣ 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Image-based sewing pattern generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS1.SSS3" title="In 2.1. Image-based 3D Garment Modeling ‣ 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>3D garment generation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS2" title="In 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Exemplar-based Texture and Material Extraction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS2.SSS1" title="In 2.2. Exemplar-based Texture and Material Extraction ‣ 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Texture map extraction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS2.SSS2" title="In 2.2. Exemplar-based Texture and Material Extraction ‣ 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Tileable texture synthesis.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS2.SSS3" title="In 2.2. Exemplar-based Texture and Material Extraction ‣ 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>BRDF material estimation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2.SS3" title="In 2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Diffusion-based Image Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS1" title="In 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Problem Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS2" title="In 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Synthetic Paired Training Data Construction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS2.SSS1" title="In 3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Paired training examples construction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS2.SSS2" title="In 3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Paired prints (e.g., logos) construction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS2.SSS3" title="In 3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Scaling up training data with Pseudo-BRDF materials.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS3" title="In 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Normalized Texture Generation via FabricDiffusion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS3.SSS1" title="In 3.3. Normalized Texture Generation via FabricDiffusion ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Training objective of conditional diffusion model.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS3.SSS2" title="In 3.3. Normalized Texture Generation via FabricDiffusion ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Model architecture and training.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS3.SSS3" title="In 3.3. Normalized Texture Generation via FabricDiffusion ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Circular padding for seamless texture generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS3.SSS4" title="In 3.3. Normalized Texture Generation via FabricDiffusion ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Transparent prints generation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS4" title="In 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>PBR Materials Generation and Garment Rendering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS1" title="In 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS1.SSS1" title="In 4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Dataset.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS1.SSS2" title="In 4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Evaluation protocols and tasks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS1.SSS3" title="In 4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Evaluation metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS1.SSS4" title="In 4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Baseline methods.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2" title="In 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2.SSS1" title="In 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>FabricDiffusion on real-world clothing images.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2.SSS2" title="In 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>FabricDiffusion on detailed prints and logos.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2.SSS3" title="In 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Image-to-garment texture transfer.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2.SSS4" title="In 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>PBR materials extraction.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS3" title="In 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablations, Analyses, and Applications</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS3.SSS1" title="In 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Ablation on circular padding and tileability analysis.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS3.SSS2" title="In 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Ablation on pseudo-BRDF data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS3.SSS3" title="In 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Effect of the capture location.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS3.SSS4" title="In 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span>Effect of the capture scale.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS3.SSS5" title="In 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Multi-material texture transfer.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS3.SSS6" title="In 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.6 </span>Compatibility with AI-Generated Images.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S5" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion, Limitation, and Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A1" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Key Advantages of FabricDiffusion</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A1.SS0.SSS0.Px1" title="In Supplementary Material A A Key Advantages of FabricDiffusion ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title">Normalized texture representation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A1.SS0.SSS0.Px2" title="In Supplementary Material A A Key Advantages of FabricDiffusion ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title">Sim-to-real generalizability.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A1.SS0.SSS0.Px3" title="In Supplementary Material A A Key Advantages of FabricDiffusion ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title">Data and computational efficiency.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A2" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Details on Dataset Construction</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A2.SS0.SSS0.Px1" title="In Supplementary Material B B Details on Dataset Construction ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title">Fabric BRDF and textile dataset.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A2.SS0.SSS0.Px2" title="In Supplementary Material B B Details on Dataset Construction ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title">3D garment mesh dataset.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A2.SS0.SSS0.Px3" title="In Supplementary Material B B Details on Dataset Construction ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title">Logos and prints dataset.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A3" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Details of Our Method</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A3.SS1" title="In Supplementary Material C C Additional Details of Our Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Details on physics-based rendering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A3.SS2" title="In Supplementary Material C C Additional Details of Our Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Classifier-free guidance for conditional image generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A3.SS3" title="In Supplementary Material C C Additional Details of Our Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Strategy for determining tiling scales</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A3.SS4" title="In Supplementary Material C C Additional Details of Our Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Implementation details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A4" title="In FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Additional Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A4.SS1" title="In Supplementary Material D D Additional Results ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Additional results on textures extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A4.SS2" title="In Supplementary Material D D Additional Results ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Texture transfer on synthetic data</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
<p class="ltx_p ltx_align_center" id="S0.F1.1"><span class="ltx_text" id="S0.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="427" id="S0.F1.1.1.g1" src="x1.png" width="830"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span><span class="ltx_text" id="S0.F1.3.1" style="font-size:90%;">Given a real-world 2D clothing image and a raw 3D garment mesh, we propose FabricDiffusion to automatically extract high-quality texture maps and prints from the reference image and transfer them to the target 3D garment surface. Our method can handle different types textures, patterns, and materials. Moreover, FabricDiffusion is capable of generating not only diffuse albedo but also roughness, normal, and metallic texture maps, allowing for accurate relighting and rendering of the produced 3D garment across various lighting conditions.
</span></figcaption>
</figure>
<h1 class="ltx_title ltx_title_document">FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cheng Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0003-6255-7648" title="ORCID identifier">0009-0003-6255-7648</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_country" id="id2.2.id2">United States of America</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chzhang@tamu.edu">chzhang@tamu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuanhao Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0002-2737-4689" title="ORCID identifier">0009-0002-2737-4689</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id3.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_country" id="id4.2.id2">United States of America</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yuanhao4@andrew.cmu.edu">yuanhao4@andrew.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francisco Vicente Carrasco
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0005-3680-3029" title="ORCID identifier">0009-0005-3680-3029</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_country" id="id6.2.id2">United States of America</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:fvicente@andrew.cmu.edu">fvicente@andrew.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenglei Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-7307-9480" title="ORCID identifier">0000-0002-7307-9480</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Google Inc.</span><span class="ltx_text ltx_affiliation_country" id="id8.2.id2">United States of America</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chengleiwu@gmail.com">chengleiwu@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinlong Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0003-7605-8209" title="ORCID identifier">0009-0003-7605-8209</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">Google Inc.</span><span class="ltx_text ltx_affiliation_country" id="id10.2.id2">Switzerland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jinlongy@google.com">jinlongy@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thabo Beeler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-8077-1205" title="ORCID identifier">0000-0002-8077-1205</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.1.id1">Google Inc.</span><span class="ltx_text ltx_affiliation_country" id="id12.2.id2">Switzerland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:thabo.beeler@gmail.com">thabo.beeler@gmail.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fernando De la Torre
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-7086-8572" title="ORCID identifier">0000-0002-7086-8572</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_country" id="id14.2.id2">United States of America</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ftorre@cs.cmu.edu">ftorre@cs.cmu.edu</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id15.id1">We introduce FabricDiffusion, a method for transferring fabric textures from a single clothing image to 3D garments of arbitrary shapes. Existing approaches typically synthesize textures on the garment surface through 2D-to-3D texture mapping or depth-aware inpainting via generative models. Unfortunately, these methods often struggle to capture and preserve texture details, particularly due to challenging occlusions, distortions, or poses in the input image. Inspired by the observation that in the fashion industry, most garments are constructed by stitching sewing patterns with flat, repeatable textures, we cast the task of clothing texture transfer as extracting distortion-free, tileable texture materials that are subsequently mapped onto the UV space of the garment. Building upon this insight, we train a denoising diffusion model with a large-scale synthetic dataset to rectify distortions in the input texture image. This process yields a flat texture map that enables a tight coupling with existing Physically-Based Rendering (PBR) material generation pipelines, allowing for realistic relighting of the garment under various lighting conditions. We show that FabricDiffusion can transfer various features from a single clothing image including texture patterns, material properties, and detailed prints and logos. Extensive experiments demonstrate that our model significantly outperforms state-to-the-art methods on both synthetic data and real-world, in-the-wild clothing images while generalizing to unseen textures and garment shapes.</p>
</div>
<div class="ltx_keywords">Texture transfer, BRDF material, diffusion model, synthetic data, 3D garments reconstruction
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Appearance and texture representations</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">There is an increasing interest to experience apparel in 3D for virtual try-on applications and e-commerce as well as an increasing demand for 3D clothing assets for games, virtual reality and augmented reality applications. While there is an abundance of 2D images of fashion items online, and recent generative AI algorithms democratize the creative generation of such images, the creation of high-quality 3D clothing assets remains a significant challenge. In this work we explore how to transfer the appearance of clothing items from 2D images onto 3D assets, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S0.F1" title="Figure 1 ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Extracting the fabric material and prints from such imagery is a challenging task, since the clothing items in the images exhibit strong distortion and shading variation due to wrinkling and the underlying body shape, in addition to general illumination variation and occlusions. To overcome these challenges, we propose a generative approach capable of extracting high-quality physically-based fabric materials and prints from a single input image and transfer them to 3D garment meshes of arbitrary shapes. The result may be rendered using Physically Based Rendering (PBR) to realistically reproduce the garments, for example, in a game engine under novel environment illumination and cloth deformation.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Existing methods for example-based 3D garments texturing primarily focus on direct texture synthesis onto 3D meshes using techniques such as 2D-to-3D texture mapping <cite class="ltx_cite ltx_citemacro_citep">(Mir et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib31" title="">2020</a>; Majithia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib29" title="">2022</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib13" title="">2024</a>)</cite> or multi-view depth-aware inpainting by distilling a pre-trained 2D generative model <cite class="ltx_cite ltx_citemacro_citep">(Richardson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib34" title="">2023</a>; Zeng, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib56" title="">2023</a>; Yeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib54" title="">2024</a>)</cite>. However, these approaches often lead to irregular and low-quality textures due to the inherent inaccuracies of 2D-to-3D registration and the stochastic nature of generative processes. Moreover, they struggle to faithfully represent texture details or disentangle garment distortions, resulting in significant degradation in texture continuity and quality.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we seek to overcome these limitations by drawing inspiration from the real-world garment creation process in the fashion industry <cite class="ltx_cite ltx_citemacro_citep">(Korosteleva and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib22" title="">2021</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib27" title="">2023</a>)</cite>: most 3D garments are typically modeled from 2D sewing patterns with normalized<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We define “normalized” as a canonical texture space devoid of geometric distortions, illumination variations, shadows, and other inconsistencies present in the real-life input images. Terms such as “undistored”, “distortion-free”, “unwarped”, and “flat” are used interchangeably in this paper to describe the textures free from geometric distortions.</span></span></span> and tileable texture maps. This allows us to approach the texturing process from a novel angle, where obtaining such texture maps enables more accurate and realistic garment rendering across various poses and environments. Interestingly, if we take the 3D mesh away from our task of texture transfer, there has been a long history of development in 2D exemplar-based texture map extraction and synthesis <cite class="ltx_cite ltx_citemacro_citep">(Efros and Leung, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib11" title="">1999</a>; Efros and Freeman, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib10" title="">2023</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib52" title="">2009</a>; Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>; Tu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib45" title="">2022</a>; Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib15" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib25" title="">2022</a>; Diamanti et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib8" title="">2015</a>; Cazenavette et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib4" title="">2022</a>; Rodriguez-Pardo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib36" title="">2023</a>; Yeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib55" title="">2022</a>; Schröder et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib42" title="">2014</a>; Guarnera et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib14" title="">2017</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib53" title="">2019</a>; Rodriguez-Pardo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib38" title="">2019</a>)</cite>. Nevertheless, there remains a significant gap in effectively correcting the geometric distortion or calibrating the appearance (e.g., lighting) of the fabric present in the input reference images.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">How can we translate a clothing image to a normalized and tileable texture map? At first glance, solving this ill-posed inverse problem is challenging, and may require developing sophisticated frameworks to model the explicit mapping. Instead, we investigate a feed-forward pathway to simulate the texture distortion and lighting conditions from its normalized form to that on a 3D garment mesh.
Then, we propose to train a denoising diffusion model <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib18" title="">2020</a>; Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib39" title="">2022</a>)</cite> using paired texture images (i.e., both the distorted and normalized) to generate normalized and tileable texture images. Such an objective makes the training procedure fairly straightforward, which we see as a key strength. As a result, generating normalized texture images becomes solving a supervised distribution mapping problem of translating distorted texture patches back to a unified normalized space.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.4">However, acquiring such paired training data from real clothing at scale is infeasible.
To address this issue, we develop a large-scale synthetic dataset comprising over <math alttext="100" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mn id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><cn id="S1.p6.1.m1.1.1.cmml" type="integer" xref="S1.p6.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">100</annotation></semantics></math>k textile color images, <math alttext="3.8" class="ltx_Math" display="inline" id="S1.p6.2.m2.1"><semantics id="S1.p6.2.m2.1a"><mn id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml">3.8</mn><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><cn id="S1.p6.2.m2.1.1.cmml" type="float" xref="S1.p6.2.m2.1.1">3.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">3.8</annotation><annotation encoding="application/x-llamapun" id="S1.p6.2.m2.1d">3.8</annotation></semantics></math>k material PBR texture maps, <math alttext="7" class="ltx_Math" display="inline" id="S1.p6.3.m3.1"><semantics id="S1.p6.3.m3.1a"><mn id="S1.p6.3.m3.1.1" xref="S1.p6.3.m3.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S1.p6.3.m3.1b"><cn id="S1.p6.3.m3.1.1.cmml" type="integer" xref="S1.p6.3.m3.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">7</annotation><annotation encoding="application/x-llamapun" id="S1.p6.3.m3.1d">7</annotation></semantics></math>k prints (e.g., logos), and <math alttext="22" class="ltx_Math" display="inline" id="S1.p6.4.m4.1"><semantics id="S1.p6.4.m4.1a"><mn id="S1.p6.4.m4.1.1" xref="S1.p6.4.m4.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S1.p6.4.m4.1b"><cn id="S1.p6.4.m4.1.1.cmml" type="integer" xref="S1.p6.4.m4.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.4.m4.1c">22</annotation><annotation encoding="application/x-llamapun" id="S1.p6.4.m4.1d">22</annotation></semantics></math> raw 3D garment meshes.
These PBR textures and prints are carefully applied to the raw 3D garment meshes and then rendered using PBR techniques under diverse lighting and environmental conditions, simulating real-world scenarios. For each fabric captures from the textured 3D garment, we render a corresponding image using ground-truth PBR textures, which are applied to a flat mesh under a controlled illumination condition, i.e., orthogonal close-up views with a pointed lighting from above. The captured texture inputs along with their ground-truth flat mesh render are used to train our diffusion model. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.F3" title="Figure 3 ‣ 3.2.1. Paired training examples construction. ‣ 3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the pipeline of training data construction.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We name our method FabricDiffusion and systematically study the performance on both synthetic data and real-world scenarios. Despite being trained entirely on synthetic rendered examples, FabricDiffusion achieves zero-shot generalization to in-the-wild images with complex textures and prints. Furthermore, the outputs of FabricDiffusion seamlessly integrate with existing PBR material estimation pipelines <cite class="ltx_cite ltx_citemacro_citep">(Sartor and Peers, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib41" title="">2023</a>)</cite>, allowing for accurate relighting of the garment under different lighting conditions.
In summary, FabricDiffusion represents a state-of-the-art approach capable of extracting undistorted texture maps from real-world clothing images to produce realistic 3D garments.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our method built upon recent and seminar work on image-based 3D garment modeling, exemplar-based texture and material extraction, and diffusion-based image generation.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Image-based 3D Garment Modeling</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1. </span>Image-to-mesh texture transfer.</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Existing methods on 2D-to-3D texture transfer typically involve (1) learning a 2D-to-3D registration <cite class="ltx_cite ltx_citemacro_citep">(Mir et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib31" title="">2020</a>; Majithia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib29" title="">2022</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib13" title="">2024</a>)</cite> and (2) conducting depth-aware inpainting supervised by a pre-trained image generative model <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib39" title="">2022</a>)</cite> to guarantee multi-view consistency <cite class="ltx_cite ltx_citemacro_citep">(Richardson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib34" title="">2023</a>; Zeng, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib56" title="">2023</a>; Yeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib54" title="">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib59" title="">2024</a>)</cite>. However, these methods often fail to capture the high frequency details of the texture or leads to irregular textures. In this work, we tackle the problem of texturing 3D garments from a drastically different angle, aiming to extract normalized texture maps from a single real-life clothing image so that we can easily apply them to the 2D UV space (i.e., sewing pattern <cite class="ltx_cite ltx_citemacro_citep">(Korosteleva and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib22" title="">2021</a>)</cite>) of the 3D garment mesh for realistic rendering.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2. </span>Image-based sewing pattern generation.</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">We argue that a major cause of the quality gap observed in generated textures is not the capacity of the generation networks, but rather from a suboptimal choice of representations for the texture generation operating from the reference image to the 3D mesh. Unfortunately, there has been little progress in leveraging the idea of generating texture maps that can be used in the 2D UV space, despite the availability of sewing patterns for 3D garments as the sewing pattern can either be manually created by technical artists <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib27" title="">2023</a>)</cite> or automatically reconstructed from reference images <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib27" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib26" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib5" title="">2022</a>)</cite>. Concurrently, DeepIron <cite class="ltx_cite ltx_citemacro_citep">(Kwon and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib23" title="">2024</a>)</cite> is the only work that leverages the similar idea of transferring the texture using sewing pattern representation. Unlike our method, they aim to transfer entire garments without PBR texture maps and exhibits subpar performance in real-world scenarios for practical usages.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3. </span>3D garment generation.</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">Recently, there has been growing interest in 3D garment generation using generative models. For instance, GarmentDreamer <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib24" title="">2024</a>)</cite> and WordRobe <cite class="ltx_cite ltx_citemacro_citep">(Srivastava et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib44" title="">2024</a>)</cite> are recent work that focus on text-based garment generation, whereas our approach transfers textures using image guidance. Another relevant work, Garment3DGen <cite class="ltx_cite ltx_citemacro_citep">(Sarafianos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib40" title="">2024</a>)</cite>, can reconstruct both textures and geometry from a single input image. However, unlike Garment3DGen, our work focuses on generating distortion-free texture and prints and has the additional capability of generating standard PBR materials.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Exemplar-based Texture and Material Extraction</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The literature on exemplar-based texture and material extraction is vast. We focus on representative works that are related to ours.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Texture map extraction.</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">We recast the task of image-to-3D garment texture transfer as generating texture maps from reference clothing image patches. <cite class="ltx_cite ltx_citemacro_citet">Hao et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib15" title="">2023</a>)</cite> trained a diffusion model to rectify distortions and occlusions in natural texture images. However, it does not extract tileable texture patches or PBR materials for fabrics. More recently, Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> addressed a similar problem by using a diffusion-based generative model to extract PBR materials. Their approach relies on personalization methods such as textual inversion <cite class="ltx_cite ltx_citemacro_citep">(Gal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib12" title="">2022</a>)</cite> to represent the exemplar patch without normalizing the patch into a canonical space, i.e., distortion-free with unified lighting.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Tileable texture synthesis.</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Previous work have attempted to synthesize tileable textures with a variety of methods, such as by maximizing perceived texture stationary <cite class="ltx_cite ltx_citemacro_citep">(Moritz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib32" title="">2017</a>)</cite>, by using Guided Correspondence <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib63" title="">2023a</a>)</cite>, by finding repeated patterns in images using pre-trained CNN features <cite class="ltx_cite ltx_citemacro_citep">(Rodriguez-Pardo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib38" title="">2019</a>)</cite>, by manipulating the latent space of pre-trained GANs <cite class="ltx_cite ltx_citemacro_citep">(Rodriguez-Pardo and Garces, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib37" title="">2022</a>)</cite>, or by modifying the noise sampling process of a diffusion model, i.e., rolled-diffusion <cite class="ltx_cite ltx_citemacro_citep">(Vecchio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib47" title="">2023</a>)</cite>. We found that a simple circular padding strategy following  <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib61" title="">2022</a>)</cite> performs well with our model architecture for addressing tileable texture generation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3. </span>BRDF material estimation.</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">A significant body of research exists on BRDF material estimation from a single image <cite class="ltx_cite ltx_citemacro_citep">(Deschaintre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib6" title="">2018</a>; Henzler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib16" title="">2021</a>; Vecchio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib48" title="">2021</a>; Casas and Comino-Trinidad, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib3" title="">2023</a>; Vecchio and Deschaintre, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib46" title="">2024</a>; Vecchio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib49" title="">2024</a>)</cite>.
Our model produces normalized texture maps in a canonical space, enabling compatibility with existing Bidirectional Reflective Distribution Function (BRDF) material estimation pipelines such as MatFusion <cite class="ltx_cite ltx_citemacro_citep">(Sartor and Peers, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib41" title="">2023</a>)</cite>, which can be integrated seamlessly with our output normalized textures. By fine-tuning the pre-trained MatFusion model with fabric PBR texture data and incorporate it into our pipeline, our model generates high-quality material maps for realistic 3D garment rendering.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Diffusion-based Image Generation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Our model architecture is inspired by the recent advancements in diffusion-based image generation models <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib18" title="">2020</a>; Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib39" title="">2022</a>; Sohl-Dickstein et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib43" title="">2015</a>)</cite>. In this work, we fine-tune the pre-trained image generative model using carefully created synthetic data, enabling texture normalization, which includes distortion removal, lighting calibration, and shadow elimination.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>
<figure class="ltx_figure" id="S3.F2">
<p class="ltx_p ltx_align_center" id="S3.F2.1"><span class="ltx_text" id="S3.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="281" id="S3.F2.1.1.g1" src="x2.png" width="814"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span><span class="ltx_text" id="S3.F2.3.1" style="font-size:90%;">Overview of FabricDiffusion. Given a real-life clothing image and region captures of its fabric materials and prints, (a) our model extracts normalized textures and prints, and (b) then generates high-quality Physically-Based Rendering (PBR) materials and transparent prints. (c) These materials and prints can be applied to the target 3D garment meshes of arbitrary shapes (d) for realistic relighting. Our model is trained purely with synthetic data and achieves zero-shot generalization to real-world images.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We propose FabricDiffusion to extract normalized, tileable texture images and materials from a real-world clothing image, and then apply them to the target 3D garment. The overall framework is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.F2" title="Figure 2 ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">2</span></a>. We first introduce the problem statement in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS1" title="3.1. Problem Statement ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.1</span></a>, followed by procedures for constructing synthetic training examples in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS2" title="3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.2</span></a>. In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS3" title="3.3. Normalized Texture Generation via FabricDiffusion ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we detail our specific approach of texture map generation. Finally, we describe PBR materials generation and garment rendering in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS4" title="3.4. PBR Materials Generation and Garment Rendering ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Problem Statement</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.6">Given an input clothing image <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_I</annotation></semantics></math> and a captured texture region <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_x</annotation></semantics></math>, which may exhibit various distortions and illuminations due to occlusion and poses present in the input image, our goal is learn a mapping function <math alttext="g" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">g</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_g</annotation></semantics></math> that takes the captured patch <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_x</annotation></semantics></math> and outputs the corresponding normalized texture map <math alttext="\tilde{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mover accent="true" id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">x</mi><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><ci id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1">~</ci><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\tilde{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">over~ start_ARG italic_x end_ARG</annotation></semantics></math>, effectively correcting the distortions. The texture map <math alttext="\tilde{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mover accent="true" id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">x</mi><mo id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><ci id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1">~</ci><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\tilde{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">over~ start_ARG italic_x end_ARG</annotation></semantics></math> needs to retain the intrinsic properties of the original captured region, such as color, texture pattern, and material characteristics.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">As mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S1" title="1. Introduction ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">1</span></a>, we formulate the generation of normalized texture maps from a real-life clothing patch as a distribution mapping problem. Specifically, the mapping function <math alttext="g" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_g</annotation></semantics></math> can be modeled by a generative process:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{x}\sim G_{\theta}(x,\epsilon),\epsilon\sim\mathcal{N}(0,\mathbf{I})," class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1"><mrow id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.3.cmml"><mrow id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.2.2" xref="S3.E1.m1.5.5.1.1.1.1.2.2.cmml">x</mi><mo id="S3.E1.m1.5.5.1.1.1.1.2.1" xref="S3.E1.m1.5.5.1.1.1.1.2.1.cmml">~</mo></mover><mo id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml">∼</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.3.cmml"><msub id="S3.E1.m1.5.5.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.3.2.2" xref="S3.E1.m1.5.5.1.1.1.1.3.2.2.cmml">G</mi><mi id="S3.E1.m1.5.5.1.1.1.1.3.2.3" xref="S3.E1.m1.5.5.1.1.1.1.3.2.3.cmml">θ</mi></msub><mo id="S3.E1.m1.5.5.1.1.1.1.3.1" xref="S3.E1.m1.5.5.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.3.3.2" xref="S3.E1.m1.5.5.1.1.1.1.3.3.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.3.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.3.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo id="S3.E1.m1.5.5.1.1.1.1.3.3.2.2" xref="S3.E1.m1.5.5.1.1.1.1.3.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">ϵ</mi><mo id="S3.E1.m1.5.5.1.1.1.1.3.3.2.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.2.3" xref="S3.E1.m1.5.5.1.1.3a.cmml">,</mo><mrow id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.cmml">ϵ</mi><mo id="S3.E1.m1.5.5.1.1.2.2.1" xref="S3.E1.m1.5.5.1.1.2.2.1.cmml">∼</mo><mrow id="S3.E1.m1.5.5.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.1.1.2.2.3.2" xref="S3.E1.m1.5.5.1.1.2.2.3.2.cmml">𝒩</mi><mo id="S3.E1.m1.5.5.1.1.2.2.3.1" xref="S3.E1.m1.5.5.1.1.2.2.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.2.2.3.3.2" xref="S3.E1.m1.5.5.1.1.2.2.3.3.1.cmml"><mo id="S3.E1.m1.5.5.1.1.2.2.3.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.3.3.1.cmml">(</mo><mn id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">0</mn><mo id="S3.E1.m1.5.5.1.1.2.2.3.3.2.2" xref="S3.E1.m1.5.5.1.1.2.2.3.3.1.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">𝐈</mi><mo id="S3.E1.m1.5.5.1.1.2.2.3.3.2.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3a.cmml" xref="S3.E1.m1.5.5.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1">similar-to</csymbol><apply id="S3.E1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2"><ci id="S3.E1.m1.5.5.1.1.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.1">~</ci><ci id="S3.E1.m1.5.5.1.1.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2.2">𝑥</ci></apply><apply id="S3.E1.m1.5.5.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3"><times id="S3.E1.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.1"></times><apply id="S3.E1.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.2.2">𝐺</ci><ci id="S3.E1.m1.5.5.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.2.3">𝜃</ci></apply><interval closure="open" id="S3.E1.m1.5.5.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">italic-ϵ</ci></interval></apply></apply><apply id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><csymbol cd="latexml" id="S3.E1.m1.5.5.1.1.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2.1">similar-to</csymbol><ci id="S3.E1.m1.5.5.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2">italic-ϵ</ci><apply id="S3.E1.m1.5.5.1.1.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3"><times id="S3.E1.m1.5.5.1.1.2.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3.1"></times><ci id="S3.E1.m1.5.5.1.1.2.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3.2">𝒩</ci><interval closure="open" id="S3.E1.m1.5.5.1.1.2.2.3.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3.3.2"><cn id="S3.E1.m1.3.3.cmml" type="integer" xref="S3.E1.m1.3.3">0</cn><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝐈</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\tilde{x}\sim G_{\theta}(x,\epsilon),\epsilon\sim\mathcal{N}(0,\mathbf{I}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">over~ start_ARG italic_x end_ARG ∼ italic_G start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x , italic_ϵ ) , italic_ϵ ∼ caligraphic_N ( 0 , bold_I ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.10">where the generative model <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m1.1"><semantics id="S3.SS1.p2.2.m1.1a"><msub id="S3.SS1.p2.2.m1.1.1" xref="S3.SS1.p2.2.m1.1.1.cmml"><mi id="S3.SS1.p2.2.m1.1.1.2" xref="S3.SS1.p2.2.m1.1.1.2.cmml">G</mi><mi id="S3.SS1.p2.2.m1.1.1.3" xref="S3.SS1.p2.2.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m1.1b"><apply id="S3.SS1.p2.2.m1.1.1.cmml" xref="S3.SS1.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m1.1.1.1.cmml" xref="S3.SS1.p2.2.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m1.1.1.2.cmml" xref="S3.SS1.p2.2.m1.1.1.2">𝐺</ci><ci id="S3.SS1.p2.2.m1.1.1.3.cmml" xref="S3.SS1.p2.2.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m1.1c">G_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m1.1d">italic_G start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, parameterized by <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m2.1"><semantics id="S3.SS1.p2.3.m2.1a"><mi id="S3.SS1.p2.3.m2.1.1" xref="S3.SS1.p2.3.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m2.1b"><ci id="S3.SS1.p2.3.m2.1.1.cmml" xref="S3.SS1.p2.3.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m2.1d">italic_θ</annotation></semantics></math>, takes the input patch <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m3.1"><semantics id="S3.SS1.p2.4.m3.1a"><mi id="S3.SS1.p2.4.m3.1.1" xref="S3.SS1.p2.4.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m3.1b"><ci id="S3.SS1.p2.4.m3.1.1.cmml" xref="S3.SS1.p2.4.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m3.1d">italic_x</annotation></semantics></math> as a condition and samples from Gaussian noise to generate the distortion-free texture map <math alttext="\tilde{x}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m4.1"><semantics id="S3.SS1.p2.5.m4.1a"><mover accent="true" id="S3.SS1.p2.5.m4.1.1" xref="S3.SS1.p2.5.m4.1.1.cmml"><mi id="S3.SS1.p2.5.m4.1.1.2" xref="S3.SS1.p2.5.m4.1.1.2.cmml">x</mi><mo id="S3.SS1.p2.5.m4.1.1.1" xref="S3.SS1.p2.5.m4.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m4.1b"><apply id="S3.SS1.p2.5.m4.1.1.cmml" xref="S3.SS1.p2.5.m4.1.1"><ci id="S3.SS1.p2.5.m4.1.1.1.cmml" xref="S3.SS1.p2.5.m4.1.1.1">~</ci><ci id="S3.SS1.p2.5.m4.1.1.2.cmml" xref="S3.SS1.p2.5.m4.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m4.1c">\tilde{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m4.1d">over~ start_ARG italic_x end_ARG</annotation></semantics></math> in a canonical space. To train the generator <math alttext="G" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m5.1"><semantics id="S3.SS1.p2.6.m5.1a"><mi id="S3.SS1.p2.6.m5.1.1" xref="S3.SS1.p2.6.m5.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m5.1b"><ci id="S3.SS1.p2.6.m5.1.1.cmml" xref="S3.SS1.p2.6.m5.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m5.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m5.1d">italic_G</annotation></semantics></math>, we must create a large number of paired training examples <math alttext="(x,x_{0})" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m6.2"><semantics id="S3.SS1.p2.7.m6.2a"><mrow id="S3.SS1.p2.7.m6.2.2.1" xref="S3.SS1.p2.7.m6.2.2.2.cmml"><mo id="S3.SS1.p2.7.m6.2.2.1.2" stretchy="false" xref="S3.SS1.p2.7.m6.2.2.2.cmml">(</mo><mi id="S3.SS1.p2.7.m6.1.1" xref="S3.SS1.p2.7.m6.1.1.cmml">x</mi><mo id="S3.SS1.p2.7.m6.2.2.1.3" xref="S3.SS1.p2.7.m6.2.2.2.cmml">,</mo><msub id="S3.SS1.p2.7.m6.2.2.1.1" xref="S3.SS1.p2.7.m6.2.2.1.1.cmml"><mi id="S3.SS1.p2.7.m6.2.2.1.1.2" xref="S3.SS1.p2.7.m6.2.2.1.1.2.cmml">x</mi><mn id="S3.SS1.p2.7.m6.2.2.1.1.3" xref="S3.SS1.p2.7.m6.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.SS1.p2.7.m6.2.2.1.4" stretchy="false" xref="S3.SS1.p2.7.m6.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m6.2b"><interval closure="open" id="S3.SS1.p2.7.m6.2.2.2.cmml" xref="S3.SS1.p2.7.m6.2.2.1"><ci id="S3.SS1.p2.7.m6.1.1.cmml" xref="S3.SS1.p2.7.m6.1.1">𝑥</ci><apply id="S3.SS1.p2.7.m6.2.2.1.1.cmml" xref="S3.SS1.p2.7.m6.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m6.2.2.1.1.1.cmml" xref="S3.SS1.p2.7.m6.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m6.2.2.1.1.2.cmml" xref="S3.SS1.p2.7.m6.2.2.1.1.2">𝑥</ci><cn id="S3.SS1.p2.7.m6.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.7.m6.2.2.1.1.3">0</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m6.2c">(x,x_{0})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m6.2d">( italic_x , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math> across various types of textures. Here <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m7.1"><semantics id="S3.SS1.p2.8.m7.1a"><mi id="S3.SS1.p2.8.m7.1.1" xref="S3.SS1.p2.8.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m7.1b"><ci id="S3.SS1.p2.8.m7.1.1.cmml" xref="S3.SS1.p2.8.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m7.1d">italic_x</annotation></semantics></math> is the input capture and <math alttext="x_{o}" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m8.1"><semantics id="S3.SS1.p2.9.m8.1a"><msub id="S3.SS1.p2.9.m8.1.1" xref="S3.SS1.p2.9.m8.1.1.cmml"><mi id="S3.SS1.p2.9.m8.1.1.2" xref="S3.SS1.p2.9.m8.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.9.m8.1.1.3" xref="S3.SS1.p2.9.m8.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m8.1b"><apply id="S3.SS1.p2.9.m8.1.1.cmml" xref="S3.SS1.p2.9.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m8.1.1.1.cmml" xref="S3.SS1.p2.9.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.9.m8.1.1.2.cmml" xref="S3.SS1.p2.9.m8.1.1.2">𝑥</ci><ci id="S3.SS1.p2.9.m8.1.1.3.cmml" xref="S3.SS1.p2.9.m8.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m8.1c">x_{o}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m8.1d">italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> is the corresponding ground-truth normalized texture. After the model training, we expect to align the sampled output <math alttext="\tilde{x}" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m9.1"><semantics id="S3.SS1.p2.10.m9.1a"><mover accent="true" id="S3.SS1.p2.10.m9.1.1" xref="S3.SS1.p2.10.m9.1.1.cmml"><mi id="S3.SS1.p2.10.m9.1.1.2" xref="S3.SS1.p2.10.m9.1.1.2.cmml">x</mi><mo id="S3.SS1.p2.10.m9.1.1.1" xref="S3.SS1.p2.10.m9.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m9.1b"><apply id="S3.SS1.p2.10.m9.1.1.cmml" xref="S3.SS1.p2.10.m9.1.1"><ci id="S3.SS1.p2.10.m9.1.1.1.cmml" xref="S3.SS1.p2.10.m9.1.1.1">~</ci><ci id="S3.SS1.p2.10.m9.1.1.2.cmml" xref="S3.SS1.p2.10.m9.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m9.1c">\tilde{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m9.1d">over~ start_ARG italic_x end_ARG</annotation></semantics></math> with the distribution of normalized textures.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Synthetic Paired Training Data Construction</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Collecting paired training examples with real clothing poses significant challenges.
In contrast, we found that PBR textures — the fundamental unit for appearance modeling in 3D apparel creation — are much more accessible from public sources (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS1" title="4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4.1</span></a> for details on dataset collection).
Given these observations, we propose to build synthetic environments for constructing distorted and flat rendered training pairs using the PBR material model <cite class="ltx_cite ltx_citemacro_citep">(McAuley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib30" title="">2012</a>)</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.F3" title="Figure 3 ‣ 3.2.1. Paired training examples construction. ‣ 3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the overall pipeline.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Paired training examples construction.</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.4">For each material, we collect the ground-truth diffuse albedo (<math alttext="k_{d}\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><msub id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.2.cmml">k</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.cmml">d</mi></msub><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><in id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></in><apply id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.2">𝑘</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3">𝑑</ci></apply><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2">ℝ</ci><cn id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">k_{d}\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_k start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>), normal (<math alttext="k_{n}\in\mathbb{R}^{3}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mrow id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml"><msub id="S3.SS2.SSS1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.2.cmml">k</mi><mi id="S3.SS2.SSS1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.3.cmml">n</mi></msub><mo id="S3.SS2.SSS1.p1.2.m2.1.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"><in id="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.1"></in><apply id="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.2">𝑘</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.3">𝑛</ci></apply><apply id="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.2">ℝ</ci><cn id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">k_{n}\in\mathbb{R}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m2.1d">italic_k start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>), roughness (<math alttext="k_{r}\in\mathbb{R}^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3.1"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mrow id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><msub id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.2.cmml">k</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.3.cmml">r</mi></msub><mo id="S3.SS2.SSS1.p1.3.m3.1.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.SSS1.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><in id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1"></in><apply id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.2">𝑘</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.3">𝑟</ci></apply><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2">ℝ</ci><cn id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">k_{r}\in\mathbb{R}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.3.m3.1d">italic_k start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>), and metallic (<math alttext="k_{m}\in\mathbb{R}^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.4.m4.1"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mrow id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml"><msub id="S3.SS2.SSS1.p1.4.m4.1.1.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.2.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.2.cmml">k</mi><mi id="S3.SS2.SSS1.p1.4.m4.1.1.2.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.3.cmml">m</mi></msub><mo id="S3.SS2.SSS1.p1.4.m4.1.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.4.m4.1.1.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.SSS1.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><apply id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1"><in id="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.1"></in><apply id="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.2">𝑘</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.3">𝑚</ci></apply><apply id="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2">ℝ</ci><cn id="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">k_{m}\in\mathbb{R}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.4.m4.1d">italic_k start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>) material maps. To create distorted rendered images that mimic real-world surface deformation and lighting, we map these material maps onto a raw garment mesh sampled from 22 common garment types. The PBR textures are tiled appropriately and illuminated using four environment maps with white lights to avoid color biases. During rendering, we capture frontal views of the garment and randomly crop patches from the rendered images to match the original fabric texture size.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">Separately, we render the same texture material on a plane mesh to create flat rendered images as ground-truths (image <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.1.m1.1"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><msub id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p2.1.m1.1.1.2" xref="S3.SS2.SSS1.p2.1.m1.1.1.2.cmml">x</mi><mn id="S3.SS2.SSS1.p2.1.m1.1.1.3" xref="S3.SS2.SSS1.p2.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><apply id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1.2">𝑥</ci><cn id="S3.SS2.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS1.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p2.1.m1.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.F3" title="Figure 3 ‣ 3.2.1. Paired training examples construction. ‣ 3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3</span></a>). For illumination, we use a fixed point light above the surface center and a fixed orthogonal camera for rendering. This method is highly beneficial as it provides supervision to align the distorted rendered images on the 3D garment to a canonical space of normalized, flat images with a unified lighting condition.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">In fact, our flat image rendering and capturing approach may be reminiscent of the input format used in well-known SVBRDF material estimation methods <cite class="ltx_cite ltx_citemacro_citep">(Sartor and Peers, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib41" title="">2023</a>; Zhou and Kalantari, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib62" title="">2021</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib61" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib60" title="">2023b</a>)</cite>, which require orthogonal close-up views of the materials and/or a flashing image as input. As will be described in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS4" title="3.4. PBR Materials Generation and Garment Rendering ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.4</span></a>, the output normalized textures from our method can be effectively integrated with SVBRDF material estimation models to generate high-quality PBR material maps.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<p class="ltx_p ltx_align_center" id="S3.F3.1"><span class="ltx_text" id="S3.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="415" id="S3.F3.1.1.g1" src="x3.png" width="814"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span><span class="ltx_text" id="S3.F3.3.1" style="font-size:90%;">Pipeline of paired training data construction. Given the textures of a PBR material, we apply them to both the target raw 3D garment mesh and the plain mesh. The 3D garment is rendered using an environment map, while the plain mesh is illuminated using a point light from above. The resulting rendered images <math alttext="(x,x_{0})" class="ltx_Math" display="inline" id="S3.F3.3.1.m1.2"><semantics id="S3.F3.3.1.m1.2b"><mrow id="S3.F3.3.1.m1.2.2.1" xref="S3.F3.3.1.m1.2.2.2.cmml"><mo id="S3.F3.3.1.m1.2.2.1.2" stretchy="false" xref="S3.F3.3.1.m1.2.2.2.cmml">(</mo><mi id="S3.F3.3.1.m1.1.1" xref="S3.F3.3.1.m1.1.1.cmml">x</mi><mo id="S3.F3.3.1.m1.2.2.1.3" xref="S3.F3.3.1.m1.2.2.2.cmml">,</mo><msub id="S3.F3.3.1.m1.2.2.1.1" xref="S3.F3.3.1.m1.2.2.1.1.cmml"><mi id="S3.F3.3.1.m1.2.2.1.1.2" xref="S3.F3.3.1.m1.2.2.1.1.2.cmml">x</mi><mn id="S3.F3.3.1.m1.2.2.1.1.3" xref="S3.F3.3.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.F3.3.1.m1.2.2.1.4" stretchy="false" xref="S3.F3.3.1.m1.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.3.1.m1.2c"><interval closure="open" id="S3.F3.3.1.m1.2.2.2.cmml" xref="S3.F3.3.1.m1.2.2.1"><ci id="S3.F3.3.1.m1.1.1.cmml" xref="S3.F3.3.1.m1.1.1">𝑥</ci><apply id="S3.F3.3.1.m1.2.2.1.1.cmml" xref="S3.F3.3.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.F3.3.1.m1.2.2.1.1.1.cmml" xref="S3.F3.3.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.F3.3.1.m1.2.2.1.1.2.cmml" xref="S3.F3.3.1.m1.2.2.1.1.2">𝑥</ci><cn id="S3.F3.3.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.F3.3.1.m1.2.2.1.1.3">0</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.1.m1.2d">(x,x_{0})</annotation><annotation encoding="application/x-llamapun" id="S3.F3.3.1.m1.2e">( italic_x , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math> from both meshes serve as the paired training examples for training our texture generative model (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS2" title="3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Paired prints (e.g., logos) construction.</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">In additional to general textures, we aim to transfer clothing details by creating warped and flat pairs of print images. We map the print to a random location on the garment mesh and blend it with a uniformly colored background texture. Unlike flat texture generation on a plane mesh, we use the original print image with a transparent background as the flat image.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>Scaling up training data with Pseudo-BRDF materials.</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">While the texture material maps are easier to acquire than real clothing, we raise the question: Do we really need a large amount of real BRDF material maps for paired training data construction, and what if we cannot obtain enough data?</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.4">In this work, we are able to collect a BRDF dataset comprises 3.8k assets in total (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS1" title="4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4.1</span></a> for details), covering a broad spectrum of fabric materials. However, the texture patterns in this dataset exhibit limited diversity because it is not large enough to model the appearance of fabric textures in our real life, given the vast range of colors, patterns, and materials. To address this, we augmented the dataset by gathering 100k textile color images featuring a wide array of patterns and designs, which are then used to generate pseudo-BRDF<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Since the normal, roughness, and metallic maps of the 100k textile images are sampled instead of ground truth, they are referred to as pseudo-BRDF data.</span></span></span> materials. Specifically, the color image served as the albedo map, while the roughness map was assigned a uniform value <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.1.m1.1"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mi id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><ci id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.1.m1.1d">italic_α</annotation></semantics></math> sampled from the distribution <math alttext="\mathcal{N}(0.708,0.193^{2})" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.2.m2.2"><semantics id="S3.SS2.SSS3.p2.2.m2.2a"><mrow id="S3.SS2.SSS3.p2.2.m2.2.2" xref="S3.SS2.SSS3.p2.2.m2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS3.p2.2.m2.2.2.3" xref="S3.SS2.SSS3.p2.2.m2.2.2.3.cmml">𝒩</mi><mo id="S3.SS2.SSS3.p2.2.m2.2.2.2" xref="S3.SS2.SSS3.p2.2.m2.2.2.2.cmml">⁢</mo><mrow id="S3.SS2.SSS3.p2.2.m2.2.2.1.1" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.2.cmml"><mo id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.2" stretchy="false" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.2.cmml">(</mo><mn id="S3.SS2.SSS3.p2.2.m2.1.1" xref="S3.SS2.SSS3.p2.2.m2.1.1.cmml">0.708</mn><mo id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.3" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.2.cmml">,</mo><msup id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.cmml"><mn id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.2" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.2.cmml">0.193</mn><mn id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.3" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.3.cmml">2</mn></msup><mo id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.4" stretchy="false" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.2.m2.2b"><apply id="S3.SS2.SSS3.p2.2.m2.2.2.cmml" xref="S3.SS2.SSS3.p2.2.m2.2.2"><times id="S3.SS2.SSS3.p2.2.m2.2.2.2.cmml" xref="S3.SS2.SSS3.p2.2.m2.2.2.2"></times><ci id="S3.SS2.SSS3.p2.2.m2.2.2.3.cmml" xref="S3.SS2.SSS3.p2.2.m2.2.2.3">𝒩</ci><interval closure="open" id="S3.SS2.SSS3.p2.2.m2.2.2.1.2.cmml" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.1"><cn id="S3.SS2.SSS3.p2.2.m2.1.1.cmml" type="float" xref="S3.SS2.SSS3.p2.2.m2.1.1">0.708</cn><apply id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1">superscript</csymbol><cn id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.2.cmml" type="float" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.2">0.193</cn><cn id="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS3.p2.2.m2.2.2.1.1.1.3">2</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.2.m2.2c">\mathcal{N}(0.708,0.193^{2})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.2.m2.2d">caligraphic_N ( 0.708 , 0.193 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math>, with 0.708 and 0.193 representing the population mean and standard deviation of the mean roughness values of the real BRDF dataset, respectively. The metallic map was assigned a uniform value <math alttext="\max(\beta,0)" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.3.m3.3"><semantics id="S3.SS2.SSS3.p2.3.m3.3a"><mrow id="S3.SS2.SSS3.p2.3.m3.3.4.2" xref="S3.SS2.SSS3.p2.3.m3.3.4.1.cmml"><mi id="S3.SS2.SSS3.p2.3.m3.1.1" xref="S3.SS2.SSS3.p2.3.m3.1.1.cmml">max</mi><mo id="S3.SS2.SSS3.p2.3.m3.3.4.2a" xref="S3.SS2.SSS3.p2.3.m3.3.4.1.cmml">⁡</mo><mrow id="S3.SS2.SSS3.p2.3.m3.3.4.2.1" xref="S3.SS2.SSS3.p2.3.m3.3.4.1.cmml"><mo id="S3.SS2.SSS3.p2.3.m3.3.4.2.1.1" stretchy="false" xref="S3.SS2.SSS3.p2.3.m3.3.4.1.cmml">(</mo><mi id="S3.SS2.SSS3.p2.3.m3.2.2" xref="S3.SS2.SSS3.p2.3.m3.2.2.cmml">β</mi><mo id="S3.SS2.SSS3.p2.3.m3.3.4.2.1.2" xref="S3.SS2.SSS3.p2.3.m3.3.4.1.cmml">,</mo><mn id="S3.SS2.SSS3.p2.3.m3.3.3" xref="S3.SS2.SSS3.p2.3.m3.3.3.cmml">0</mn><mo id="S3.SS2.SSS3.p2.3.m3.3.4.2.1.3" stretchy="false" xref="S3.SS2.SSS3.p2.3.m3.3.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.3.m3.3b"><apply id="S3.SS2.SSS3.p2.3.m3.3.4.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.3.4.2"><max id="S3.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1"></max><ci id="S3.SS2.SSS3.p2.3.m3.2.2.cmml" xref="S3.SS2.SSS3.p2.3.m3.2.2">𝛽</ci><cn id="S3.SS2.SSS3.p2.3.m3.3.3.cmml" type="integer" xref="S3.SS2.SSS3.p2.3.m3.3.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.3.m3.3c">\max(\beta,0)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.3.m3.3d">roman_max ( italic_β , 0 )</annotation></semantics></math>, where <math alttext="\beta\sim\mathcal{U}(-0.05,0.05)" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.4.m4.2"><semantics id="S3.SS2.SSS3.p2.4.m4.2a"><mrow id="S3.SS2.SSS3.p2.4.m4.2.2" xref="S3.SS2.SSS3.p2.4.m4.2.2.cmml"><mi id="S3.SS2.SSS3.p2.4.m4.2.2.3" xref="S3.SS2.SSS3.p2.4.m4.2.2.3.cmml">β</mi><mo id="S3.SS2.SSS3.p2.4.m4.2.2.2" xref="S3.SS2.SSS3.p2.4.m4.2.2.2.cmml">∼</mo><mrow id="S3.SS2.SSS3.p2.4.m4.2.2.1" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS3.p2.4.m4.2.2.1.3" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.3.cmml">𝒰</mi><mo id="S3.SS2.SSS3.p2.4.m4.2.2.1.2" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.2.cmml">⁢</mo><mrow id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.2.cmml"><mo id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.2" stretchy="false" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.2.cmml">(</mo><mrow id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1.cmml"><mo id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1a" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1.cmml">−</mo><mn id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1.2" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1.2.cmml">0.05</mn></mrow><mo id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.3" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.2.cmml">,</mo><mn id="S3.SS2.SSS3.p2.4.m4.1.1" xref="S3.SS2.SSS3.p2.4.m4.1.1.cmml">0.05</mn><mo id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.4" stretchy="false" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.4.m4.2b"><apply id="S3.SS2.SSS3.p2.4.m4.2.2.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2"><csymbol cd="latexml" id="S3.SS2.SSS3.p2.4.m4.2.2.2.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2.2">similar-to</csymbol><ci id="S3.SS2.SSS3.p2.4.m4.2.2.3.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2.3">𝛽</ci><apply id="S3.SS2.SSS3.p2.4.m4.2.2.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2.1"><times id="S3.SS2.SSS3.p2.4.m4.2.2.1.2.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.2"></times><ci id="S3.SS2.SSS3.p2.4.m4.2.2.1.3.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.3">𝒰</ci><interval closure="open" id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1"><apply id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1"><minus id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1"></minus><cn id="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1.2.cmml" type="float" xref="S3.SS2.SSS3.p2.4.m4.2.2.1.1.1.1.2">0.05</cn></apply><cn id="S3.SS2.SSS3.p2.4.m4.1.1.cmml" type="float" xref="S3.SS2.SSS3.p2.4.m4.1.1">0.05</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.4.m4.2c">\beta\sim\mathcal{U}(-0.05,0.05)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.4.m4.2d">italic_β ∼ caligraphic_U ( - 0.05 , 0.05 )</annotation></semantics></math>, and the normal map was kept flat.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.2">We use a combination of real (3.8k) and pseudo-BRDF (100k) materials to create paired rendered images for training our texture generation model. During paired training examples construction, both real and pseudo-BRDF have <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.1.m1.1"><semantics id="S3.SS2.SSS3.p3.1.m1.1a"><mi id="S3.SS2.SSS3.p3.1.m1.1.1" xref="S3.SS2.SSS3.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.1.m1.1b"><ci id="S3.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.1.m1.1d">italic_x</annotation></semantics></math> and <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p3.2.m2.1"><semantics id="S3.SS2.SSS3.p3.2.m2.1a"><msub id="S3.SS2.SSS3.p3.2.m2.1.1" xref="S3.SS2.SSS3.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p3.2.m2.1.1.2" xref="S3.SS2.SSS3.p3.2.m2.1.1.2.cmml">x</mi><mn id="S3.SS2.SSS3.p3.2.m2.1.1.3" xref="S3.SS2.SSS3.p3.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.2.m2.1b"><apply id="S3.SS2.SSS3.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1.2">𝑥</ci><cn id="S3.SS2.SSS3.p3.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.SSS3.p3.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.2.m2.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p3.2.m2.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> (as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.F3" title="Figure 3 ‣ 3.2.1. Paired training examples construction. ‣ 3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3</span></a>), representing distorted and flat textures, respectively. Intuitively, the primary goal of our texture generator is to eliminate geometric distortions, and our generated pseudo rendered images, serve this purpose effectively.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Normalized Texture Generation via FabricDiffusion</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Given the paired training images, we build a denoising diffusion model to learn the distribution mapping from the input capture to the normalized texture map. Next, we detail our training objective, model architecture and training, and the design for tileable texture generation and alpha-channel-enabled<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Alpha-channel-enabled prints are images with transparency that can be overlaid onto existing images for realistic composition and rendering.</span></span></span> prints generation.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Training objective of conditional diffusion model.</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.6">Diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Sohl-Dickstein et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib43" title="">2015</a>; Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib18" title="">2020</a>)</cite> are trained to capture the distribution of training images through a sequential Markov chains of adding random noise into clean images and denoising pure noise to clean images. We leverage Latent Diffusion Model (LDM) <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib39" title="">2022</a>)</cite> to improve the efficiency and quality of diffusion models by operating in the latent space of a pre-trained variational autoencoder <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Welling, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib21" title="">2013</a>)</cite> with encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.1"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><ci id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.1d">caligraphic_E</annotation></semantics></math> and decoder <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.2.m2.1"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><ci id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.2.m2.1d">caligraphic_D</annotation></semantics></math>. In our case, given the paired training data <math alttext="(x,x_{0})" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.3.m3.2"><semantics id="S3.SS3.SSS1.p1.3.m3.2a"><mrow id="S3.SS3.SSS1.p1.3.m3.2.2.1" xref="S3.SS3.SSS1.p1.3.m3.2.2.2.cmml"><mo id="S3.SS3.SSS1.p1.3.m3.2.2.1.2" stretchy="false" xref="S3.SS3.SSS1.p1.3.m3.2.2.2.cmml">(</mo><mi id="S3.SS3.SSS1.p1.3.m3.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.cmml">x</mi><mo id="S3.SS3.SSS1.p1.3.m3.2.2.1.3" xref="S3.SS3.SSS1.p1.3.m3.2.2.2.cmml">,</mo><msub id="S3.SS3.SSS1.p1.3.m3.2.2.1.1" xref="S3.SS3.SSS1.p1.3.m3.2.2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.3.m3.2.2.1.1.2" xref="S3.SS3.SSS1.p1.3.m3.2.2.1.1.2.cmml">x</mi><mn id="S3.SS3.SSS1.p1.3.m3.2.2.1.1.3" xref="S3.SS3.SSS1.p1.3.m3.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.SS3.SSS1.p1.3.m3.2.2.1.4" stretchy="false" xref="S3.SS3.SSS1.p1.3.m3.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.3.m3.2b"><interval closure="open" id="S3.SS3.SSS1.p1.3.m3.2.2.2.cmml" xref="S3.SS3.SSS1.p1.3.m3.2.2.1"><ci id="S3.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1">𝑥</ci><apply id="S3.SS3.SSS1.p1.3.m3.2.2.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.3.m3.2.2.1.1.2">𝑥</ci><cn id="S3.SS3.SSS1.p1.3.m3.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.3.m3.2.2.1.1.3">0</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.3.m3.2c">(x,x_{0})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.3.m3.2d">( italic_x , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.4.m4.1"><semantics id="S3.SS3.SSS1.p1.4.m4.1a"><mi id="S3.SS3.SSS1.p1.4.m4.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.4.m4.1b"><ci id="S3.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.4.m4.1d">italic_x</annotation></semantics></math> is the distorted patch and <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.5.m5.1"><semantics id="S3.SS3.SSS1.p1.5.m5.1a"><msub id="S3.SS3.SSS1.p1.5.m5.1.1" xref="S3.SS3.SSS1.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS1.p1.5.m5.1.1.2" xref="S3.SS3.SSS1.p1.5.m5.1.1.2.cmml">x</mi><mn id="S3.SS3.SSS1.p1.5.m5.1.1.3" xref="S3.SS3.SSS1.p1.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.5.m5.1b"><apply id="S3.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1.2">𝑥</ci><cn id="S3.SS3.SSS1.p1.5.m5.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.5.m5.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.5.m5.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is the normalized texture, the feed-forward process is formulated by adding random Gaussian noise into the latent space of image <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.6.m6.1"><semantics id="S3.SS3.SSS1.p1.6.m6.1a"><msub id="S3.SS3.SSS1.p1.6.m6.1.1" xref="S3.SS3.SSS1.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS1.p1.6.m6.1.1.2" xref="S3.SS3.SSS1.p1.6.m6.1.1.2.cmml">x</mi><mn id="S3.SS3.SSS1.p1.6.m6.1.1.3" xref="S3.SS3.SSS1.p1.6.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.6.m6.1b"><apply id="S3.SS3.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS1.p1.6.m6.1.1.2">𝑥</ci><cn id="S3.SS3.SSS1.p1.6.m6.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.6.m6.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.6.m6.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{t}=\sqrt{\gamma(t)}\mathcal{E}(x_{0})+\sqrt{1-\gamma(t)}\epsilon," class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml">x</mi><mi id="S3.E2.m1.3.3.1.1.3.3" xref="S3.E2.m1.3.3.1.1.3.3.cmml">t</mi></msub><mo id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><msqrt id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">γ</mi><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.4.2.1" stretchy="false" xref="S3.E2.m1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">t</mi><mo id="S3.E2.m1.1.1.1.4.2.2" stretchy="false" xref="S3.E2.m1.1.1.1.cmml">)</mo></mrow></mrow></msqrt><mo id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.2.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.3.cmml">ℰ</mi><mo id="S3.E2.m1.3.3.1.1.1.1.2a" xref="S3.E2.m1.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml"><msqrt id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mn id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml">1</mn><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">−</mo><mrow id="S3.E2.m1.2.2.1.4" xref="S3.E2.m1.2.2.1.4.cmml"><mi id="S3.E2.m1.2.2.1.4.2" xref="S3.E2.m1.2.2.1.4.2.cmml">γ</mi><mo id="S3.E2.m1.2.2.1.4.1" xref="S3.E2.m1.2.2.1.4.1.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.4.3.2" xref="S3.E2.m1.2.2.1.4.cmml"><mo id="S3.E2.m1.2.2.1.4.3.2.1" stretchy="false" xref="S3.E2.m1.2.2.1.4.cmml">(</mo><mi id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml">t</mi><mo id="S3.E2.m1.2.2.1.4.3.2.2" stretchy="false" xref="S3.E2.m1.2.2.1.4.cmml">)</mo></mrow></mrow></mrow></msqrt><mo id="S3.E2.m1.3.3.1.1.1.3.1" xref="S3.E2.m1.3.3.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2.m1.3.3.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.3.2.cmml">ϵ</mi></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"></eq><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2">𝑥</ci><ci id="S3.E2.m1.3.3.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3">𝑡</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><plus id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></plus><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><root id="S3.E2.m1.1.1a.cmml" xref="S3.E2.m1.1.1"></root><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">𝛾</ci><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑡</ci></apply></apply><ci id="S3.E2.m1.3.3.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.3">ℰ</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2">𝑥</ci><cn id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3">0</cn></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3"><times id="S3.E2.m1.3.3.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3.1"></times><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><root id="S3.E2.m1.2.2a.cmml" xref="S3.E2.m1.2.2"></root><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><minus id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></minus><cn id="S3.E2.m1.2.2.1.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.3">1</cn><apply id="S3.E2.m1.2.2.1.4.cmml" xref="S3.E2.m1.2.2.1.4"><times id="S3.E2.m1.2.2.1.4.1.cmml" xref="S3.E2.m1.2.2.1.4.1"></times><ci id="S3.E2.m1.2.2.1.4.2.cmml" xref="S3.E2.m1.2.2.1.4.2">𝛾</ci><ci id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1">𝑡</ci></apply></apply></apply><ci id="S3.E2.m1.3.3.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.2">italic-ϵ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">x_{t}=\sqrt{\gamma(t)}\mathcal{E}(x_{0})+\sqrt{1-\gamma(t)}\epsilon,</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root start_ARG italic_γ ( italic_t ) end_ARG caligraphic_E ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + square-root start_ARG 1 - italic_γ ( italic_t ) end_ARG italic_ϵ ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p1.12">where <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.7.m1.1"><semantics id="S3.SS3.SSS1.p1.7.m1.1a"><msub id="S3.SS3.SSS1.p1.7.m1.1.1" xref="S3.SS3.SSS1.p1.7.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p1.7.m1.1.1.2" xref="S3.SS3.SSS1.p1.7.m1.1.1.2.cmml">x</mi><mi id="S3.SS3.SSS1.p1.7.m1.1.1.3" xref="S3.SS3.SSS1.p1.7.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.7.m1.1b"><apply id="S3.SS3.SSS1.p1.7.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.7.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.7.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.7.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.7.m1.1.1.2">𝑥</ci><ci id="S3.SS3.SSS1.p1.7.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p1.7.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.7.m1.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.7.m1.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is a noisy latent of the original clean input <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.8.m2.1"><semantics id="S3.SS3.SSS1.p1.8.m2.1a"><msub id="S3.SS3.SSS1.p1.8.m2.1.1" xref="S3.SS3.SSS1.p1.8.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.8.m2.1.1.2" xref="S3.SS3.SSS1.p1.8.m2.1.1.2.cmml">x</mi><mn id="S3.SS3.SSS1.p1.8.m2.1.1.3" xref="S3.SS3.SSS1.p1.8.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.8.m2.1b"><apply id="S3.SS3.SSS1.p1.8.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.8.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.8.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.8.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.8.m2.1.1.2">𝑥</ci><cn id="S3.SS3.SSS1.p1.8.m2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.8.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.8.m2.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.8.m2.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\epsilon\sim\mathcal{N}(0,\mathbf{I})" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.9.m3.2"><semantics id="S3.SS3.SSS1.p1.9.m3.2a"><mrow id="S3.SS3.SSS1.p1.9.m3.2.3" xref="S3.SS3.SSS1.p1.9.m3.2.3.cmml"><mi id="S3.SS3.SSS1.p1.9.m3.2.3.2" xref="S3.SS3.SSS1.p1.9.m3.2.3.2.cmml">ϵ</mi><mo id="S3.SS3.SSS1.p1.9.m3.2.3.1" xref="S3.SS3.SSS1.p1.9.m3.2.3.1.cmml">∼</mo><mrow id="S3.SS3.SSS1.p1.9.m3.2.3.3" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.9.m3.2.3.3.2" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.2.cmml">𝒩</mi><mo id="S3.SS3.SSS1.p1.9.m3.2.3.3.1" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p1.9.m3.2.3.3.3.2" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.3.1.cmml"><mo id="S3.SS3.SSS1.p1.9.m3.2.3.3.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.3.1.cmml">(</mo><mn id="S3.SS3.SSS1.p1.9.m3.1.1" xref="S3.SS3.SSS1.p1.9.m3.1.1.cmml">0</mn><mo id="S3.SS3.SSS1.p1.9.m3.2.3.3.3.2.2" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p1.9.m3.2.2" xref="S3.SS3.SSS1.p1.9.m3.2.2.cmml">𝐈</mi><mo id="S3.SS3.SSS1.p1.9.m3.2.3.3.3.2.3" stretchy="false" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.9.m3.2b"><apply id="S3.SS3.SSS1.p1.9.m3.2.3.cmml" xref="S3.SS3.SSS1.p1.9.m3.2.3"><csymbol cd="latexml" id="S3.SS3.SSS1.p1.9.m3.2.3.1.cmml" xref="S3.SS3.SSS1.p1.9.m3.2.3.1">similar-to</csymbol><ci id="S3.SS3.SSS1.p1.9.m3.2.3.2.cmml" xref="S3.SS3.SSS1.p1.9.m3.2.3.2">italic-ϵ</ci><apply id="S3.SS3.SSS1.p1.9.m3.2.3.3.cmml" xref="S3.SS3.SSS1.p1.9.m3.2.3.3"><times id="S3.SS3.SSS1.p1.9.m3.2.3.3.1.cmml" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.1"></times><ci id="S3.SS3.SSS1.p1.9.m3.2.3.3.2.cmml" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS3.SSS1.p1.9.m3.2.3.3.3.1.cmml" xref="S3.SS3.SSS1.p1.9.m3.2.3.3.3.2"><cn id="S3.SS3.SSS1.p1.9.m3.1.1.cmml" type="integer" xref="S3.SS3.SSS1.p1.9.m3.1.1">0</cn><ci id="S3.SS3.SSS1.p1.9.m3.2.2.cmml" xref="S3.SS3.SSS1.p1.9.m3.2.2">𝐈</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.9.m3.2c">\epsilon\sim\mathcal{N}(0,\mathbf{I})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.9.m3.2d">italic_ϵ ∼ caligraphic_N ( 0 , bold_I )</annotation></semantics></math>, <math alttext="t\in[0,1]" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.10.m4.2"><semantics id="S3.SS3.SSS1.p1.10.m4.2a"><mrow id="S3.SS3.SSS1.p1.10.m4.2.3" xref="S3.SS3.SSS1.p1.10.m4.2.3.cmml"><mi id="S3.SS3.SSS1.p1.10.m4.2.3.2" xref="S3.SS3.SSS1.p1.10.m4.2.3.2.cmml">t</mi><mo id="S3.SS3.SSS1.p1.10.m4.2.3.1" xref="S3.SS3.SSS1.p1.10.m4.2.3.1.cmml">∈</mo><mrow id="S3.SS3.SSS1.p1.10.m4.2.3.3.2" xref="S3.SS3.SSS1.p1.10.m4.2.3.3.1.cmml"><mo id="S3.SS3.SSS1.p1.10.m4.2.3.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p1.10.m4.2.3.3.1.cmml">[</mo><mn id="S3.SS3.SSS1.p1.10.m4.1.1" xref="S3.SS3.SSS1.p1.10.m4.1.1.cmml">0</mn><mo id="S3.SS3.SSS1.p1.10.m4.2.3.3.2.2" xref="S3.SS3.SSS1.p1.10.m4.2.3.3.1.cmml">,</mo><mn id="S3.SS3.SSS1.p1.10.m4.2.2" xref="S3.SS3.SSS1.p1.10.m4.2.2.cmml">1</mn><mo id="S3.SS3.SSS1.p1.10.m4.2.3.3.2.3" stretchy="false" xref="S3.SS3.SSS1.p1.10.m4.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.10.m4.2b"><apply id="S3.SS3.SSS1.p1.10.m4.2.3.cmml" xref="S3.SS3.SSS1.p1.10.m4.2.3"><in id="S3.SS3.SSS1.p1.10.m4.2.3.1.cmml" xref="S3.SS3.SSS1.p1.10.m4.2.3.1"></in><ci id="S3.SS3.SSS1.p1.10.m4.2.3.2.cmml" xref="S3.SS3.SSS1.p1.10.m4.2.3.2">𝑡</ci><interval closure="closed" id="S3.SS3.SSS1.p1.10.m4.2.3.3.1.cmml" xref="S3.SS3.SSS1.p1.10.m4.2.3.3.2"><cn id="S3.SS3.SSS1.p1.10.m4.1.1.cmml" type="integer" xref="S3.SS3.SSS1.p1.10.m4.1.1">0</cn><cn id="S3.SS3.SSS1.p1.10.m4.2.2.cmml" type="integer" xref="S3.SS3.SSS1.p1.10.m4.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.10.m4.2c">t\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.10.m4.2d">italic_t ∈ [ 0 , 1 ]</annotation></semantics></math>, and <math alttext="\gamma(t)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.11.m5.1"><semantics id="S3.SS3.SSS1.p1.11.m5.1a"><mrow id="S3.SS3.SSS1.p1.11.m5.1.2" xref="S3.SS3.SSS1.p1.11.m5.1.2.cmml"><mi id="S3.SS3.SSS1.p1.11.m5.1.2.2" xref="S3.SS3.SSS1.p1.11.m5.1.2.2.cmml">γ</mi><mo id="S3.SS3.SSS1.p1.11.m5.1.2.1" xref="S3.SS3.SSS1.p1.11.m5.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p1.11.m5.1.2.3.2" xref="S3.SS3.SSS1.p1.11.m5.1.2.cmml"><mo id="S3.SS3.SSS1.p1.11.m5.1.2.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p1.11.m5.1.2.cmml">(</mo><mi id="S3.SS3.SSS1.p1.11.m5.1.1" xref="S3.SS3.SSS1.p1.11.m5.1.1.cmml">t</mi><mo id="S3.SS3.SSS1.p1.11.m5.1.2.3.2.2" stretchy="false" xref="S3.SS3.SSS1.p1.11.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.11.m5.1b"><apply id="S3.SS3.SSS1.p1.11.m5.1.2.cmml" xref="S3.SS3.SSS1.p1.11.m5.1.2"><times id="S3.SS3.SSS1.p1.11.m5.1.2.1.cmml" xref="S3.SS3.SSS1.p1.11.m5.1.2.1"></times><ci id="S3.SS3.SSS1.p1.11.m5.1.2.2.cmml" xref="S3.SS3.SSS1.p1.11.m5.1.2.2">𝛾</ci><ci id="S3.SS3.SSS1.p1.11.m5.1.1.cmml" xref="S3.SS3.SSS1.p1.11.m5.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.11.m5.1c">\gamma(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.11.m5.1d">italic_γ ( italic_t )</annotation></semantics></math> is defined as a noise scheduler that monotonically descends from 1 to 0.
By adding the distorted image <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.12.m6.1"><semantics id="S3.SS3.SSS1.p1.12.m6.1a"><mi id="S3.SS3.SSS1.p1.12.m6.1.1" xref="S3.SS3.SSS1.p1.12.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.12.m6.1b"><ci id="S3.SS3.SSS1.p1.12.m6.1.1.cmml" xref="S3.SS3.SSS1.p1.12.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.12.m6.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.12.m6.1d">italic_x</annotation></semantics></math> as the condition, the reverse process aims to denoise Gaussian noises back to clean images by iteratively predicting the added noises at each reverse step. We minimize the following latent diffusion objective:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L(\theta)=\mathbb{E}_{\mathcal{E}(x),\epsilon\sim\mathcal{N}(0,\mathbf{I}),t}%
\left[\left\|\epsilon-\epsilon_{\theta}({x}_{t},t,\mathcal{E}(x))\right\|^{2}%
\right]," class="ltx_Math" display="block" id="S3.E3.m1.10"><semantics id="S3.E3.m1.10a"><mrow id="S3.E3.m1.10.10.1" xref="S3.E3.m1.10.10.1.1.cmml"><mrow id="S3.E3.m1.10.10.1.1" xref="S3.E3.m1.10.10.1.1.cmml"><mrow id="S3.E3.m1.10.10.1.1.3" xref="S3.E3.m1.10.10.1.1.3.cmml"><mi id="S3.E3.m1.10.10.1.1.3.2" xref="S3.E3.m1.10.10.1.1.3.2.cmml">L</mi><mo id="S3.E3.m1.10.10.1.1.3.1" xref="S3.E3.m1.10.10.1.1.3.1.cmml">⁢</mo><mrow id="S3.E3.m1.10.10.1.1.3.3.2" xref="S3.E3.m1.10.10.1.1.3.cmml"><mo id="S3.E3.m1.10.10.1.1.3.3.2.1" stretchy="false" xref="S3.E3.m1.10.10.1.1.3.cmml">(</mo><mi id="S3.E3.m1.7.7" xref="S3.E3.m1.7.7.cmml">θ</mi><mo id="S3.E3.m1.10.10.1.1.3.3.2.2" stretchy="false" xref="S3.E3.m1.10.10.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.10.10.1.1.2" xref="S3.E3.m1.10.10.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.10.10.1.1.1" xref="S3.E3.m1.10.10.1.1.1.cmml"><msub id="S3.E3.m1.10.10.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.3.cmml"><mi id="S3.E3.m1.10.10.1.1.1.3.2" xref="S3.E3.m1.10.10.1.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E3.m1.6.6.6.6" xref="S3.E3.m1.6.6.6.7.cmml"><mrow id="S3.E3.m1.6.6.6.6.1" xref="S3.E3.m1.6.6.6.6.1.cmml"><mrow id="S3.E3.m1.6.6.6.6.1.1.1" xref="S3.E3.m1.6.6.6.6.1.1.2.cmml"><mrow id="S3.E3.m1.6.6.6.6.1.1.1.1" xref="S3.E3.m1.6.6.6.6.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.6.6.6.6.1.1.1.1.2" xref="S3.E3.m1.6.6.6.6.1.1.1.1.2.cmml">ℰ</mi><mo id="S3.E3.m1.6.6.6.6.1.1.1.1.1" xref="S3.E3.m1.6.6.6.6.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E3.m1.6.6.6.6.1.1.1.1.3.2" xref="S3.E3.m1.6.6.6.6.1.1.1.1.cmml"><mo id="S3.E3.m1.6.6.6.6.1.1.1.1.3.2.1" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.1.1.1.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">x</mi><mo id="S3.E3.m1.6.6.6.6.1.1.1.1.3.2.2" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.6.6.6.6.1.1.1.2" xref="S3.E3.m1.6.6.6.6.1.1.2.cmml">,</mo><mi id="S3.E3.m1.4.4.4.4" xref="S3.E3.m1.4.4.4.4.cmml">ϵ</mi></mrow><mo id="S3.E3.m1.6.6.6.6.1.2" xref="S3.E3.m1.6.6.6.6.1.2.cmml">∼</mo><mrow id="S3.E3.m1.6.6.6.6.1.3" xref="S3.E3.m1.6.6.6.6.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.6.6.6.6.1.3.2" xref="S3.E3.m1.6.6.6.6.1.3.2.cmml">𝒩</mi><mo id="S3.E3.m1.6.6.6.6.1.3.1" xref="S3.E3.m1.6.6.6.6.1.3.1.cmml">⁢</mo><mrow id="S3.E3.m1.6.6.6.6.1.3.3.2" xref="S3.E3.m1.6.6.6.6.1.3.3.1.cmml"><mo id="S3.E3.m1.6.6.6.6.1.3.3.2.1" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.3.3.1.cmml">(</mo><mn id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">0</mn><mo id="S3.E3.m1.6.6.6.6.1.3.3.2.2" xref="S3.E3.m1.6.6.6.6.1.3.3.1.cmml">,</mo><mi id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">𝐈</mi><mo id="S3.E3.m1.6.6.6.6.1.3.3.2.3" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.6.6.6.6.2" xref="S3.E3.m1.6.6.6.7a.cmml">,</mo><mi id="S3.E3.m1.5.5.5.5" xref="S3.E3.m1.5.5.5.5.cmml">t</mi></mrow></msub><mo id="S3.E3.m1.10.10.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.2.cmml"><mo id="S3.E3.m1.10.10.1.1.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.1.2.1.cmml">[</mo><msup id="S3.E3.m1.10.10.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.4.cmml">ϵ</mi><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.cmml"><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.2.cmml">ϵ</mi><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.3.cmml">θ</mi></msub><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><mi id="S3.E3.m1.9.9" xref="S3.E3.m1.9.9.cmml">t</mi><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.5" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">ℰ</mi><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml">⁢</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.3.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.3.2.1" stretchy="false" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">(</mo><mi id="S3.E3.m1.8.8" xref="S3.E3.m1.8.8.cmml">x</mi><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.3.2.2" stretchy="false" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.6" stretchy="false" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E3.m1.10.10.1.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.E3.m1.10.10.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E3.m1.10.10.1.2" xref="S3.E3.m1.10.10.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.10b"><apply id="S3.E3.m1.10.10.1.1.cmml" xref="S3.E3.m1.10.10.1"><eq id="S3.E3.m1.10.10.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.2"></eq><apply id="S3.E3.m1.10.10.1.1.3.cmml" xref="S3.E3.m1.10.10.1.1.3"><times id="S3.E3.m1.10.10.1.1.3.1.cmml" xref="S3.E3.m1.10.10.1.1.3.1"></times><ci id="S3.E3.m1.10.10.1.1.3.2.cmml" xref="S3.E3.m1.10.10.1.1.3.2">𝐿</ci><ci id="S3.E3.m1.7.7.cmml" xref="S3.E3.m1.7.7">𝜃</ci></apply><apply id="S3.E3.m1.10.10.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1"><times id="S3.E3.m1.10.10.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.2"></times><apply id="S3.E3.m1.10.10.1.1.1.3.cmml" xref="S3.E3.m1.10.10.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.3.1.cmml" xref="S3.E3.m1.10.10.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.10.10.1.1.1.3.2.cmml" xref="S3.E3.m1.10.10.1.1.1.3.2">𝔼</ci><apply id="S3.E3.m1.6.6.6.7.cmml" xref="S3.E3.m1.6.6.6.6"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.7a.cmml" xref="S3.E3.m1.6.6.6.6.2">formulae-sequence</csymbol><apply id="S3.E3.m1.6.6.6.6.1.cmml" xref="S3.E3.m1.6.6.6.6.1"><csymbol cd="latexml" id="S3.E3.m1.6.6.6.6.1.2.cmml" xref="S3.E3.m1.6.6.6.6.1.2">similar-to</csymbol><list id="S3.E3.m1.6.6.6.6.1.1.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1"><apply id="S3.E3.m1.6.6.6.6.1.1.1.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1.1"><times id="S3.E3.m1.6.6.6.6.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1.1.1"></times><ci id="S3.E3.m1.6.6.6.6.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1.1.2">ℰ</ci><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝑥</ci></apply><ci id="S3.E3.m1.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4">italic-ϵ</ci></list><apply id="S3.E3.m1.6.6.6.6.1.3.cmml" xref="S3.E3.m1.6.6.6.6.1.3"><times id="S3.E3.m1.6.6.6.6.1.3.1.cmml" xref="S3.E3.m1.6.6.6.6.1.3.1"></times><ci id="S3.E3.m1.6.6.6.6.1.3.2.cmml" xref="S3.E3.m1.6.6.6.6.1.3.2">𝒩</ci><interval closure="open" id="S3.E3.m1.6.6.6.6.1.3.3.1.cmml" xref="S3.E3.m1.6.6.6.6.1.3.3.2"><cn id="S3.E3.m1.2.2.2.2.cmml" type="integer" xref="S3.E3.m1.2.2.2.2">0</cn><ci id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3">𝐈</ci></interval></apply></apply><ci id="S3.E3.m1.5.5.5.5.cmml" xref="S3.E3.m1.5.5.5.5">𝑡</ci></apply></apply><apply id="S3.E3.m1.10.10.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.10.10.1.1.1.1.2.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.10.10.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3"></minus><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.4">italic-ϵ</ci><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2"><times id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.3"></times><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.2">italic-ϵ</ci><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.4.3">𝜃</ci></apply><vector id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2"><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.E3.m1.9.9.cmml" xref="S3.E3.m1.9.9">𝑡</ci><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2"><times id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.1"></times><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.2.2.2.2">ℰ</ci><ci id="S3.E3.m1.8.8.cmml" xref="S3.E3.m1.8.8">𝑥</ci></apply></vector></apply></apply></apply><cn id="S3.E3.m1.10.10.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.10.10.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.10c">L(\theta)=\mathbb{E}_{\mathcal{E}(x),\epsilon\sim\mathcal{N}(0,\mathbf{I}),t}%
\left[\left\|\epsilon-\epsilon_{\theta}({x}_{t},t,\mathcal{E}(x))\right\|^{2}%
\right],</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.10d">italic_L ( italic_θ ) = blackboard_E start_POSTSUBSCRIPT caligraphic_E ( italic_x ) , italic_ϵ ∼ caligraphic_N ( 0 , bold_I ) , italic_t end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , caligraphic_E ( italic_x ) ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p1.16">where <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.13.m1.1"><semantics id="S3.SS3.SSS1.p1.13.m1.1a"><msub id="S3.SS3.SSS1.p1.13.m1.1.1" xref="S3.SS3.SSS1.p1.13.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p1.13.m1.1.1.2" xref="S3.SS3.SSS1.p1.13.m1.1.1.2.cmml">ϵ</mi><mi id="S3.SS3.SSS1.p1.13.m1.1.1.3" xref="S3.SS3.SSS1.p1.13.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.13.m1.1b"><apply id="S3.SS3.SSS1.p1.13.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.13.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.13.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.13.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.13.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.13.m1.1.1.2">italic-ϵ</ci><ci id="S3.SS3.SSS1.p1.13.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p1.13.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.13.m1.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.13.m1.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> denotes model parameterized by a neural network, <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.14.m2.1"><semantics id="S3.SS3.SSS1.p1.14.m2.1a"><msub id="S3.SS3.SSS1.p1.14.m2.1.1" xref="S3.SS3.SSS1.p1.14.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.14.m2.1.1.2" xref="S3.SS3.SSS1.p1.14.m2.1.1.2.cmml">x</mi><mi id="S3.SS3.SSS1.p1.14.m2.1.1.3" xref="S3.SS3.SSS1.p1.14.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.14.m2.1b"><apply id="S3.SS3.SSS1.p1.14.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.14.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.14.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.14.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.14.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.14.m2.1.1.2">𝑥</ci><ci id="S3.SS3.SSS1.p1.14.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p1.14.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.14.m2.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.14.m2.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the noisy latent for each timestep <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.15.m3.1"><semantics id="S3.SS3.SSS1.p1.15.m3.1a"><mi id="S3.SS3.SSS1.p1.15.m3.1.1" xref="S3.SS3.SSS1.p1.15.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.15.m3.1b"><ci id="S3.SS3.SSS1.p1.15.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.15.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.15.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.15.m3.1d">italic_t</annotation></semantics></math>, and <math alttext="\mathcal{E}(x)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.16.m4.1"><semantics id="S3.SS3.SSS1.p1.16.m4.1a"><mrow id="S3.SS3.SSS1.p1.16.m4.1.2" xref="S3.SS3.SSS1.p1.16.m4.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.16.m4.1.2.2" xref="S3.SS3.SSS1.p1.16.m4.1.2.2.cmml">ℰ</mi><mo id="S3.SS3.SSS1.p1.16.m4.1.2.1" xref="S3.SS3.SSS1.p1.16.m4.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p1.16.m4.1.2.3.2" xref="S3.SS3.SSS1.p1.16.m4.1.2.cmml"><mo id="S3.SS3.SSS1.p1.16.m4.1.2.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p1.16.m4.1.2.cmml">(</mo><mi id="S3.SS3.SSS1.p1.16.m4.1.1" xref="S3.SS3.SSS1.p1.16.m4.1.1.cmml">x</mi><mo id="S3.SS3.SSS1.p1.16.m4.1.2.3.2.2" stretchy="false" xref="S3.SS3.SSS1.p1.16.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.16.m4.1b"><apply id="S3.SS3.SSS1.p1.16.m4.1.2.cmml" xref="S3.SS3.SSS1.p1.16.m4.1.2"><times id="S3.SS3.SSS1.p1.16.m4.1.2.1.cmml" xref="S3.SS3.SSS1.p1.16.m4.1.2.1"></times><ci id="S3.SS3.SSS1.p1.16.m4.1.2.2.cmml" xref="S3.SS3.SSS1.p1.16.m4.1.2.2">ℰ</ci><ci id="S3.SS3.SSS1.p1.16.m4.1.1.cmml" xref="S3.SS3.SSS1.p1.16.m4.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.16.m4.1c">\mathcal{E}(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.16.m4.1d">caligraphic_E ( italic_x )</annotation></semantics></math> is the condition.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">Recalling Equation <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.E1" title="In 3.1. Problem Statement ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">1</span></a>, the above formulation incorporates input-specific information (i.e., the captured patch <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.1.m1.1"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><mi id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><ci id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.1.m1.1d">italic_x</annotation></semantics></math>) into the training process for generating normalized textures. As will be shown in the experimental results in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2" title="4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4.2</span></a>, this design is the key to producing faithful texture maps that differs from existing per-example optimization-based texture extraction approaches <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>; Richardson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib34" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Model architecture and training.</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.2">Any diffusion-based architecture for conditional image generation can realize Equation <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.E3" title="In 3.3.1. Training objective of conditional diffusion model. ‣ 3.3. Normalized Texture Generation via FabricDiffusion ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3</span></a>. Specifically, we use Stable Diffusion <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib39" title="">2022</a>)</cite>, a popular open-source text-conditioned image generative model pre-trained on large-scale text and image pairs. To support image conditioning, we use additional input channels to the first convolutional layer, where the latent noise <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.1.m1.1"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><msub id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.1.1.2" xref="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS3.SSS2.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><apply id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.2">𝑥</ci><ci id="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is concatenated with the conditioned image latent <math alttext="\mathcal{E}(x)" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.2.m2.1"><semantics id="S3.SS3.SSS2.p1.2.m2.1a"><mrow id="S3.SS3.SSS2.p1.2.m2.1.2" xref="S3.SS3.SSS2.p1.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS2.p1.2.m2.1.2.2" xref="S3.SS3.SSS2.p1.2.m2.1.2.2.cmml">ℰ</mi><mo id="S3.SS3.SSS2.p1.2.m2.1.2.1" xref="S3.SS3.SSS2.p1.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.SSS2.p1.2.m2.1.2.3.2" xref="S3.SS3.SSS2.p1.2.m2.1.2.cmml"><mo id="S3.SS3.SSS2.p1.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS3.SSS2.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS3.SSS2.p1.2.m2.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.cmml">x</mi><mo id="S3.SS3.SSS2.p1.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS3.SSS2.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m2.1b"><apply id="S3.SS3.SSS2.p1.2.m2.1.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.2"><times id="S3.SS3.SSS2.p1.2.m2.1.2.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.2.1"></times><ci id="S3.SS3.SSS2.p1.2.m2.1.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.2.2">ℰ</ci><ci id="S3.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m2.1c">\mathcal{E}(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.2.m2.1d">caligraphic_E ( italic_x )</annotation></semantics></math>. The model’s initial weights come from the pre-trained Stable Diffusion v1.5, while the newly added channels are initialized to zero, speeding up training and convergence. We eliminate text conditioning, focusing solely on using a single image as the prompt. This approach addresses the challenge of generating normalized texture maps, which text prompts struggle to describe accurately <cite class="ltx_cite ltx_citemacro_citep">(Deschaintre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib7" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3. </span>Circular padding for seamless texture generation.</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">To ensure the generated texture maps are tileable, we employ a simple yet effective circular padding strategy inspired by TileGen <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib61" title="">2022</a>)</cite>. Unlike TileGen, which uses a StyleGAN-like architecture <cite class="ltx_cite ltx_citemacro_citep">(Karras et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib20" title="">2020</a>)</cite> and needs to replace both regular and transposed (e.g., upsampling or downsampling) convolutions, we only apply circular padding to all regular convolutional layers, thanks to the flexibility of diffusion models.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4. </span>Transparent prints generation.</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">The vanilla Stable Diffusion model can only output RGB images, lacking the capability to generate layered or transparent images, which is in stark contrast to our demand for prints transfer. Instead of redesigning the existing generative model <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Agrawala, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib57" title="">2024</a>)</cite>, we propose a simple and effective recipe to post-process the generated RGB print images for computing an additional alpha channel. We hypothesize that the alpha map for prints can be approximated as binary – either fully transparent or fully opaque. Based on this assumption, we assign a new RGB value for each pixel <math alttext="(i,j)" class="ltx_Math" display="inline" id="S3.SS3.SSS4.p1.1.m1.2"><semantics id="S3.SS3.SSS4.p1.1.m1.2a"><mrow id="S3.SS3.SSS4.p1.1.m1.2.3.2" xref="S3.SS3.SSS4.p1.1.m1.2.3.1.cmml"><mo id="S3.SS3.SSS4.p1.1.m1.2.3.2.1" stretchy="false" xref="S3.SS3.SSS4.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS3.SSS4.p1.1.m1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.cmml">i</mi><mo id="S3.SS3.SSS4.p1.1.m1.2.3.2.2" xref="S3.SS3.SSS4.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS3.SSS4.p1.1.m1.2.2" xref="S3.SS3.SSS4.p1.1.m1.2.2.cmml">j</mi><mo id="S3.SS3.SSS4.p1.1.m1.2.3.2.3" stretchy="false" xref="S3.SS3.SSS4.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.1.m1.2b"><interval closure="open" id="S3.SS3.SSS4.p1.1.m1.2.3.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.2.3.2"><ci id="S3.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1">𝑖</ci><ci id="S3.SS3.SSS4.p1.1.m1.2.2.cmml" xref="S3.SS3.SSS4.p1.1.m1.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.1.m1.2c">(i,j)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS4.p1.1.m1.2d">( italic_i , italic_j )</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{RGB}(i,j)=\max\Bigl{[}0,\frac{\tilde{x}(i,j)-0.1}{0.9}\Bigr{]}," class="ltx_Math" display="block" id="S3.E4.m1.7"><semantics id="S3.E4.m1.7a"><mrow id="S3.E4.m1.7.7.1" xref="S3.E4.m1.7.7.1.1.cmml"><mrow id="S3.E4.m1.7.7.1.1" xref="S3.E4.m1.7.7.1.1.cmml"><mrow id="S3.E4.m1.7.7.1.1.2" xref="S3.E4.m1.7.7.1.1.2.cmml"><mtext id="S3.E4.m1.7.7.1.1.2.2" xref="S3.E4.m1.7.7.1.1.2.2a.cmml">RGB</mtext><mo id="S3.E4.m1.7.7.1.1.2.1" xref="S3.E4.m1.7.7.1.1.2.1.cmml">⁢</mo><mrow id="S3.E4.m1.7.7.1.1.2.3.2" xref="S3.E4.m1.7.7.1.1.2.3.1.cmml"><mo id="S3.E4.m1.7.7.1.1.2.3.2.1" stretchy="false" xref="S3.E4.m1.7.7.1.1.2.3.1.cmml">(</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">i</mi><mo id="S3.E4.m1.7.7.1.1.2.3.2.2" xref="S3.E4.m1.7.7.1.1.2.3.1.cmml">,</mo><mi id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">j</mi><mo id="S3.E4.m1.7.7.1.1.2.3.2.3" stretchy="false" xref="S3.E4.m1.7.7.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.7.7.1.1.1" xref="S3.E4.m1.7.7.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.7.7.1.1.3.2" xref="S3.E4.m1.7.7.1.1.3.1.cmml"><mi id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">max</mi><mo id="S3.E4.m1.7.7.1.1.3.2a" xref="S3.E4.m1.7.7.1.1.3.1.cmml">⁡</mo><mrow id="S3.E4.m1.7.7.1.1.3.2.1" xref="S3.E4.m1.7.7.1.1.3.1.cmml"><mo id="S3.E4.m1.7.7.1.1.3.2.1.1" maxsize="160%" minsize="160%" xref="S3.E4.m1.7.7.1.1.3.1.cmml">[</mo><mn id="S3.E4.m1.6.6" xref="S3.E4.m1.6.6.cmml">0</mn><mo id="S3.E4.m1.7.7.1.1.3.2.1.2" xref="S3.E4.m1.7.7.1.1.3.1.cmml">,</mo><mfrac id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml"><mrow id="S3.E4.m1.2.2.2.4" xref="S3.E4.m1.2.2.2.4.cmml"><mover accent="true" id="S3.E4.m1.2.2.2.4.2" xref="S3.E4.m1.2.2.2.4.2.cmml"><mi id="S3.E4.m1.2.2.2.4.2.2" xref="S3.E4.m1.2.2.2.4.2.2.cmml">x</mi><mo id="S3.E4.m1.2.2.2.4.2.1" xref="S3.E4.m1.2.2.2.4.2.1.cmml">~</mo></mover><mo id="S3.E4.m1.2.2.2.4.1" xref="S3.E4.m1.2.2.2.4.1.cmml">⁢</mo><mrow id="S3.E4.m1.2.2.2.4.3.2" xref="S3.E4.m1.2.2.2.4.3.1.cmml"><mo id="S3.E4.m1.2.2.2.4.3.2.1" stretchy="false" xref="S3.E4.m1.2.2.2.4.3.1.cmml">(</mo><mi id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">i</mi><mo id="S3.E4.m1.2.2.2.4.3.2.2" xref="S3.E4.m1.2.2.2.4.3.1.cmml">,</mo><mi id="S3.E4.m1.2.2.2.2" xref="S3.E4.m1.2.2.2.2.cmml">j</mi><mo id="S3.E4.m1.2.2.2.4.3.2.3" stretchy="false" xref="S3.E4.m1.2.2.2.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.2.2.2.3" xref="S3.E4.m1.2.2.2.3.cmml">−</mo><mn id="S3.E4.m1.2.2.2.5" xref="S3.E4.m1.2.2.2.5.cmml">0.1</mn></mrow><mn id="S3.E4.m1.2.2.4" xref="S3.E4.m1.2.2.4.cmml">0.9</mn></mfrac><mo id="S3.E4.m1.7.7.1.1.3.2.1.3" maxsize="160%" minsize="160%" xref="S3.E4.m1.7.7.1.1.3.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E4.m1.7.7.1.2" xref="S3.E4.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.7b"><apply id="S3.E4.m1.7.7.1.1.cmml" xref="S3.E4.m1.7.7.1"><eq id="S3.E4.m1.7.7.1.1.1.cmml" xref="S3.E4.m1.7.7.1.1.1"></eq><apply id="S3.E4.m1.7.7.1.1.2.cmml" xref="S3.E4.m1.7.7.1.1.2"><times id="S3.E4.m1.7.7.1.1.2.1.cmml" xref="S3.E4.m1.7.7.1.1.2.1"></times><ci id="S3.E4.m1.7.7.1.1.2.2a.cmml" xref="S3.E4.m1.7.7.1.1.2.2"><mtext id="S3.E4.m1.7.7.1.1.2.2.cmml" xref="S3.E4.m1.7.7.1.1.2.2">RGB</mtext></ci><interval closure="open" id="S3.E4.m1.7.7.1.1.2.3.1.cmml" xref="S3.E4.m1.7.7.1.1.2.3.2"><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">𝑖</ci><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">𝑗</ci></interval></apply><apply id="S3.E4.m1.7.7.1.1.3.1.cmml" xref="S3.E4.m1.7.7.1.1.3.2"><max id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5"></max><cn id="S3.E4.m1.6.6.cmml" type="integer" xref="S3.E4.m1.6.6">0</cn><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><divide id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2"></divide><apply id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"><minus id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.3"></minus><apply id="S3.E4.m1.2.2.2.4.cmml" xref="S3.E4.m1.2.2.2.4"><times id="S3.E4.m1.2.2.2.4.1.cmml" xref="S3.E4.m1.2.2.2.4.1"></times><apply id="S3.E4.m1.2.2.2.4.2.cmml" xref="S3.E4.m1.2.2.2.4.2"><ci id="S3.E4.m1.2.2.2.4.2.1.cmml" xref="S3.E4.m1.2.2.2.4.2.1">~</ci><ci id="S3.E4.m1.2.2.2.4.2.2.cmml" xref="S3.E4.m1.2.2.2.4.2.2">𝑥</ci></apply><interval closure="open" id="S3.E4.m1.2.2.2.4.3.1.cmml" xref="S3.E4.m1.2.2.2.4.3.2"><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">𝑖</ci><ci id="S3.E4.m1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2">𝑗</ci></interval></apply><cn id="S3.E4.m1.2.2.2.5.cmml" type="float" xref="S3.E4.m1.2.2.2.5">0.1</cn></apply><cn id="S3.E4.m1.2.2.4.cmml" type="float" xref="S3.E4.m1.2.2.4">0.9</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.7c">\text{RGB}(i,j)=\max\Bigl{[}0,\frac{\tilde{x}(i,j)-0.1}{0.9}\Bigr{]},</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.7d">RGB ( italic_i , italic_j ) = roman_max [ 0 , divide start_ARG over~ start_ARG italic_x end_ARG ( italic_i , italic_j ) - 0.1 end_ARG start_ARG 0.9 end_ARG ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS4.p1.3">where <math alttext="\tilde{x}" class="ltx_Math" display="inline" id="S3.SS3.SSS4.p1.2.m1.1"><semantics id="S3.SS3.SSS4.p1.2.m1.1a"><mover accent="true" id="S3.SS3.SSS4.p1.2.m1.1.1" xref="S3.SS3.SSS4.p1.2.m1.1.1.cmml"><mi id="S3.SS3.SSS4.p1.2.m1.1.1.2" xref="S3.SS3.SSS4.p1.2.m1.1.1.2.cmml">x</mi><mo id="S3.SS3.SSS4.p1.2.m1.1.1.1" xref="S3.SS3.SSS4.p1.2.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.2.m1.1b"><apply id="S3.SS3.SSS4.p1.2.m1.1.1.cmml" xref="S3.SS3.SSS4.p1.2.m1.1.1"><ci id="S3.SS3.SSS4.p1.2.m1.1.1.1.cmml" xref="S3.SS3.SSS4.p1.2.m1.1.1.1">~</ci><ci id="S3.SS3.SSS4.p1.2.m1.1.1.2.cmml" xref="S3.SS3.SSS4.p1.2.m1.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.2.m1.1c">\tilde{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS4.p1.2.m1.1d">over~ start_ARG italic_x end_ARG</annotation></semantics></math> is the generated texture (Equation <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.E1" title="In 3.1. Problem Statement ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">1</span></a>). The alpha channel value at each pixel <math alttext="(i,j)" class="ltx_Math" display="inline" id="S3.SS3.SSS4.p1.3.m2.2"><semantics id="S3.SS3.SSS4.p1.3.m2.2a"><mrow id="S3.SS3.SSS4.p1.3.m2.2.3.2" xref="S3.SS3.SSS4.p1.3.m2.2.3.1.cmml"><mo id="S3.SS3.SSS4.p1.3.m2.2.3.2.1" stretchy="false" xref="S3.SS3.SSS4.p1.3.m2.2.3.1.cmml">(</mo><mi id="S3.SS3.SSS4.p1.3.m2.1.1" xref="S3.SS3.SSS4.p1.3.m2.1.1.cmml">i</mi><mo id="S3.SS3.SSS4.p1.3.m2.2.3.2.2" xref="S3.SS3.SSS4.p1.3.m2.2.3.1.cmml">,</mo><mi id="S3.SS3.SSS4.p1.3.m2.2.2" xref="S3.SS3.SSS4.p1.3.m2.2.2.cmml">j</mi><mo id="S3.SS3.SSS4.p1.3.m2.2.3.2.3" stretchy="false" xref="S3.SS3.SSS4.p1.3.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.3.m2.2b"><interval closure="open" id="S3.SS3.SSS4.p1.3.m2.2.3.1.cmml" xref="S3.SS3.SSS4.p1.3.m2.2.3.2"><ci id="S3.SS3.SSS4.p1.3.m2.1.1.cmml" xref="S3.SS3.SSS4.p1.3.m2.1.1">𝑖</ci><ci id="S3.SS3.SSS4.p1.3.m2.2.2.cmml" xref="S3.SS3.SSS4.p1.3.m2.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.3.m2.2c">(i,j)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS4.p1.3.m2.2d">( italic_i , italic_j )</annotation></semantics></math> is thus determined by the following criteria:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{A}(i,j)=\begin{cases}\qquad 1&amp;\text{if}~{}\tilde{x}(i,j)\geq 0.1,\\
\tilde{x}(i,j)/0.1&amp;\text{otherwise}.\end{cases}" class="ltx_Math" display="block" id="S3.E5.m1.6"><semantics id="S3.E5.m1.6a"><mrow id="S3.E5.m1.6.7" xref="S3.E5.m1.6.7.cmml"><mrow id="S3.E5.m1.6.7.2" xref="S3.E5.m1.6.7.2.cmml"><mtext id="S3.E5.m1.6.7.2.2" xref="S3.E5.m1.6.7.2.2a.cmml">A</mtext><mo id="S3.E5.m1.6.7.2.1" xref="S3.E5.m1.6.7.2.1.cmml">⁢</mo><mrow id="S3.E5.m1.6.7.2.3.2" xref="S3.E5.m1.6.7.2.3.1.cmml"><mo id="S3.E5.m1.6.7.2.3.2.1" stretchy="false" xref="S3.E5.m1.6.7.2.3.1.cmml">(</mo><mi id="S3.E5.m1.5.5" xref="S3.E5.m1.5.5.cmml">i</mi><mo id="S3.E5.m1.6.7.2.3.2.2" xref="S3.E5.m1.6.7.2.3.1.cmml">,</mo><mi id="S3.E5.m1.6.6" xref="S3.E5.m1.6.6.cmml">j</mi><mo id="S3.E5.m1.6.7.2.3.2.3" stretchy="false" xref="S3.E5.m1.6.7.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.6.7.1" xref="S3.E5.m1.6.7.1.cmml">=</mo><mrow id="S3.E5.m1.4.4" xref="S3.E5.m1.6.7.3.1.cmml"><mo id="S3.E5.m1.4.4.5" xref="S3.E5.m1.6.7.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E5.m1.4.4.4" rowspacing="0pt" xref="S3.E5.m1.6.7.3.1.cmml"><mtr id="S3.E5.m1.4.4.4a" xref="S3.E5.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.4.4.4b" xref="S3.E5.m1.6.7.3.1.cmml"><mn id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.4.4.4c" xref="S3.E5.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.2.2.2.2.2.1.3" xref="S3.E5.m1.2.2.2.2.2.1.3.1.cmml"><mrow id="S3.E5.m1.2.2.2.2.2.1.3.1" xref="S3.E5.m1.2.2.2.2.2.1.3.1.cmml"><mrow id="S3.E5.m1.2.2.2.2.2.1.3.1.2" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.cmml"><mtext id="S3.E5.m1.2.2.2.2.2.1.3.1.2.2" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.2a.cmml">if</mtext><mo id="S3.E5.m1.2.2.2.2.2.1.3.1.2.1" lspace="0.330em" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.1.cmml">⁢</mo><mover accent="true" id="S3.E5.m1.2.2.2.2.2.1.3.1.2.3" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.cmml"><mi id="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.2" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.2.cmml">x</mi><mo id="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.1" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.1.cmml">~</mo></mover><mo id="S3.E5.m1.2.2.2.2.2.1.3.1.2.1a" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.1.cmml">⁢</mo><mrow id="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.2" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.1.cmml"><mo id="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.2.1" stretchy="false" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.1.cmml">(</mo><mi id="S3.E5.m1.2.2.2.2.2.1.1" xref="S3.E5.m1.2.2.2.2.2.1.1.cmml">i</mi><mo id="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.2.2" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.1.cmml">,</mo><mi id="S3.E5.m1.2.2.2.2.2.1.2" xref="S3.E5.m1.2.2.2.2.2.1.2.cmml">j</mi><mo id="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.2.3" stretchy="false" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.2.2.2.2.2.1.3.1.1" xref="S3.E5.m1.2.2.2.2.2.1.3.1.1.cmml">≥</mo><mn id="S3.E5.m1.2.2.2.2.2.1.3.1.3" xref="S3.E5.m1.2.2.2.2.2.1.3.1.3.cmml">0.1</mn></mrow><mo id="S3.E5.m1.2.2.2.2.2.1.3.2" xref="S3.E5.m1.2.2.2.2.2.1.3.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E5.m1.4.4.4d" xref="S3.E5.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.4.4.4e" xref="S3.E5.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.3.3.3.3.1.1" xref="S3.E5.m1.3.3.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.3.3.1.1.4" xref="S3.E5.m1.3.3.3.3.1.1.4.cmml"><mover accent="true" id="S3.E5.m1.3.3.3.3.1.1.4.2" xref="S3.E5.m1.3.3.3.3.1.1.4.2.cmml"><mi id="S3.E5.m1.3.3.3.3.1.1.4.2.2" xref="S3.E5.m1.3.3.3.3.1.1.4.2.2.cmml">x</mi><mo id="S3.E5.m1.3.3.3.3.1.1.4.2.1" xref="S3.E5.m1.3.3.3.3.1.1.4.2.1.cmml">~</mo></mover><mo id="S3.E5.m1.3.3.3.3.1.1.4.1" xref="S3.E5.m1.3.3.3.3.1.1.4.1.cmml">⁢</mo><mrow id="S3.E5.m1.3.3.3.3.1.1.4.3.2" xref="S3.E5.m1.3.3.3.3.1.1.4.3.1.cmml"><mo id="S3.E5.m1.3.3.3.3.1.1.4.3.2.1" stretchy="false" xref="S3.E5.m1.3.3.3.3.1.1.4.3.1.cmml">(</mo><mi id="S3.E5.m1.3.3.3.3.1.1.1" xref="S3.E5.m1.3.3.3.3.1.1.1.cmml">i</mi><mo id="S3.E5.m1.3.3.3.3.1.1.4.3.2.2" xref="S3.E5.m1.3.3.3.3.1.1.4.3.1.cmml">,</mo><mi id="S3.E5.m1.3.3.3.3.1.1.2" xref="S3.E5.m1.3.3.3.3.1.1.2.cmml">j</mi><mo id="S3.E5.m1.3.3.3.3.1.1.4.3.2.3" stretchy="false" xref="S3.E5.m1.3.3.3.3.1.1.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.3.3.3.3.1.1.3" xref="S3.E5.m1.3.3.3.3.1.1.3.cmml">/</mo><mn id="S3.E5.m1.3.3.3.3.1.1.5" xref="S3.E5.m1.3.3.3.3.1.1.5.cmml">0.1</mn></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E5.m1.4.4.4f" xref="S3.E5.m1.6.7.3.1.cmml"><mrow id="S3.E5.m1.4.4.4.4.2.1.3" xref="S3.E5.m1.4.4.4.4.2.1.1a.cmml"><mtext id="S3.E5.m1.4.4.4.4.2.1.1" xref="S3.E5.m1.4.4.4.4.2.1.1.cmml">otherwise</mtext><mo id="S3.E5.m1.4.4.4.4.2.1.3.1" lspace="0em" xref="S3.E5.m1.4.4.4.4.2.1.1a.cmml">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.6b"><apply id="S3.E5.m1.6.7.cmml" xref="S3.E5.m1.6.7"><eq id="S3.E5.m1.6.7.1.cmml" xref="S3.E5.m1.6.7.1"></eq><apply id="S3.E5.m1.6.7.2.cmml" xref="S3.E5.m1.6.7.2"><times id="S3.E5.m1.6.7.2.1.cmml" xref="S3.E5.m1.6.7.2.1"></times><ci id="S3.E5.m1.6.7.2.2a.cmml" xref="S3.E5.m1.6.7.2.2"><mtext id="S3.E5.m1.6.7.2.2.cmml" xref="S3.E5.m1.6.7.2.2">A</mtext></ci><interval closure="open" id="S3.E5.m1.6.7.2.3.1.cmml" xref="S3.E5.m1.6.7.2.3.2"><ci id="S3.E5.m1.5.5.cmml" xref="S3.E5.m1.5.5">𝑖</ci><ci id="S3.E5.m1.6.6.cmml" xref="S3.E5.m1.6.6">𝑗</ci></interval></apply><apply id="S3.E5.m1.6.7.3.1.cmml" xref="S3.E5.m1.4.4"><csymbol cd="latexml" id="S3.E5.m1.6.7.3.1.1.cmml" xref="S3.E5.m1.4.4.5">cases</csymbol><cn id="S3.E5.m1.1.1.1.1.1.1.cmml" type="integer" xref="S3.E5.m1.1.1.1.1.1.1">1</cn><apply id="S3.E5.m1.2.2.2.2.2.1.3.1.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3"><geq id="S3.E5.m1.2.2.2.2.2.1.3.1.1.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.1"></geq><apply id="S3.E5.m1.2.2.2.2.2.1.3.1.2.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2"><times id="S3.E5.m1.2.2.2.2.2.1.3.1.2.1.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.1"></times><ci id="S3.E5.m1.2.2.2.2.2.1.3.1.2.2a.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.2"><mtext id="S3.E5.m1.2.2.2.2.2.1.3.1.2.2.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.2">if</mtext></ci><apply id="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.3"><ci id="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.1.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.1">~</ci><ci id="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.2.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.3.2">𝑥</ci></apply><interval closure="open" id="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.1.cmml" xref="S3.E5.m1.2.2.2.2.2.1.3.1.2.4.2"><ci id="S3.E5.m1.2.2.2.2.2.1.1.cmml" xref="S3.E5.m1.2.2.2.2.2.1.1">𝑖</ci><ci id="S3.E5.m1.2.2.2.2.2.1.2.cmml" xref="S3.E5.m1.2.2.2.2.2.1.2">𝑗</ci></interval></apply><cn id="S3.E5.m1.2.2.2.2.2.1.3.1.3.cmml" type="float" xref="S3.E5.m1.2.2.2.2.2.1.3.1.3">0.1</cn></apply><apply id="S3.E5.m1.3.3.3.3.1.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1"><divide id="S3.E5.m1.3.3.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.3.3.1.1.3"></divide><apply id="S3.E5.m1.3.3.3.3.1.1.4.cmml" xref="S3.E5.m1.3.3.3.3.1.1.4"><times id="S3.E5.m1.3.3.3.3.1.1.4.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1.4.1"></times><apply id="S3.E5.m1.3.3.3.3.1.1.4.2.cmml" xref="S3.E5.m1.3.3.3.3.1.1.4.2"><ci id="S3.E5.m1.3.3.3.3.1.1.4.2.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1.4.2.1">~</ci><ci id="S3.E5.m1.3.3.3.3.1.1.4.2.2.cmml" xref="S3.E5.m1.3.3.3.3.1.1.4.2.2">𝑥</ci></apply><interval closure="open" id="S3.E5.m1.3.3.3.3.1.1.4.3.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1.4.3.2"><ci id="S3.E5.m1.3.3.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.3.3.1.1.1">𝑖</ci><ci id="S3.E5.m1.3.3.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.3.3.1.1.2">𝑗</ci></interval></apply><cn id="S3.E5.m1.3.3.3.3.1.1.5.cmml" type="float" xref="S3.E5.m1.3.3.3.3.1.1.5">0.1</cn></apply><ci id="S3.E5.m1.4.4.4.4.2.1.1a.cmml" xref="S3.E5.m1.4.4.4.4.2.1.3"><mtext id="S3.E5.m1.4.4.4.4.2.1.1.cmml" xref="S3.E5.m1.4.4.4.4.2.1.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.6c">\text{A}(i,j)=\begin{cases}\qquad 1&amp;\text{if}~{}\tilde{x}(i,j)\geq 0.1,\\
\tilde{x}(i,j)/0.1&amp;\text{otherwise}.\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.6d">A ( italic_i , italic_j ) = { start_ROW start_CELL 1 end_CELL start_CELL if over~ start_ARG italic_x end_ARG ( italic_i , italic_j ) ≥ 0.1 , end_CELL end_ROW start_ROW start_CELL over~ start_ARG italic_x end_ARG ( italic_i , italic_j ) / 0.1 end_CELL start_CELL otherwise . end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS4.p1.4">This approach assigns full opacity (alpha value of 1) to pixels where the initial value exceeds a certain threshold, and scales down the alpha value for other pixels, designating them as transparent background. As will be shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2" title="4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4.2</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F5" title="Figure 5 ‣ 4.1.4. Baseline methods. ‣ 4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">5</span></a>, our method can handle complex prints and logos and output RGBA print images that can be overlaid onto the fabric texture.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>PBR Materials Generation and Garment Rendering</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Our FabricDiffusion model is able to generate a normalized texture map that is tileable, flat, and under a unified lighting, ensuring compatibility with the SVBRDF material estimation method. The goal of this work is not to develop a new material estimation method but to demonstrate the compatibility of our approach with existing methods. MatFusion <cite class="ltx_cite ltx_citemacro_citep">(Sartor and Peers, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib41" title="">2023</a>)</cite> is a state-of-the-art model trained on approximately 312k SVBRDF maps, most of which are non-fabric or non-clothing materials. We fine-tune this model using our dataset of real fabric BRDF materials. Specifically, we use our normalized textures as inputs, with the material maps <math alttext="(k_{d},k_{n},k_{r},k_{m})" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.4"><semantics id="S3.SS4.p1.1.m1.4a"><mrow id="S3.SS4.p1.1.m1.4.4.4" xref="S3.SS4.p1.1.m1.4.4.5.cmml"><mo id="S3.SS4.p1.1.m1.4.4.4.5" stretchy="false" xref="S3.SS4.p1.1.m1.4.4.5.cmml">(</mo><msub id="S3.SS4.p1.1.m1.1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.1.1.2.cmml">k</mi><mi id="S3.SS4.p1.1.m1.1.1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.1.1.3.cmml">d</mi></msub><mo id="S3.SS4.p1.1.m1.4.4.4.6" xref="S3.SS4.p1.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS4.p1.1.m1.2.2.2.2" xref="S3.SS4.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS4.p1.1.m1.2.2.2.2.2" xref="S3.SS4.p1.1.m1.2.2.2.2.2.cmml">k</mi><mi id="S3.SS4.p1.1.m1.2.2.2.2.3" xref="S3.SS4.p1.1.m1.2.2.2.2.3.cmml">n</mi></msub><mo id="S3.SS4.p1.1.m1.4.4.4.7" xref="S3.SS4.p1.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS4.p1.1.m1.3.3.3.3" xref="S3.SS4.p1.1.m1.3.3.3.3.cmml"><mi id="S3.SS4.p1.1.m1.3.3.3.3.2" xref="S3.SS4.p1.1.m1.3.3.3.3.2.cmml">k</mi><mi id="S3.SS4.p1.1.m1.3.3.3.3.3" xref="S3.SS4.p1.1.m1.3.3.3.3.3.cmml">r</mi></msub><mo id="S3.SS4.p1.1.m1.4.4.4.8" xref="S3.SS4.p1.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS4.p1.1.m1.4.4.4.4" xref="S3.SS4.p1.1.m1.4.4.4.4.cmml"><mi id="S3.SS4.p1.1.m1.4.4.4.4.2" xref="S3.SS4.p1.1.m1.4.4.4.4.2.cmml">k</mi><mi id="S3.SS4.p1.1.m1.4.4.4.4.3" xref="S3.SS4.p1.1.m1.4.4.4.4.3.cmml">m</mi></msub><mo id="S3.SS4.p1.1.m1.4.4.4.9" stretchy="false" xref="S3.SS4.p1.1.m1.4.4.5.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.4b"><vector id="S3.SS4.p1.1.m1.4.4.5.cmml" xref="S3.SS4.p1.1.m1.4.4.4"><apply id="S3.SS4.p1.1.m1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.2">𝑘</ci><ci id="S3.SS4.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.3">𝑑</ci></apply><apply id="S3.SS4.p1.1.m1.2.2.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2">𝑘</ci><ci id="S3.SS4.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.3">𝑛</ci></apply><apply id="S3.SS4.p1.1.m1.3.3.3.3.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.3.3.3.3.1.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS4.p1.1.m1.3.3.3.3.2.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3.2">𝑘</ci><ci id="S3.SS4.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3.3">𝑟</ci></apply><apply id="S3.SS4.p1.1.m1.4.4.4.4.cmml" xref="S3.SS4.p1.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.4.4.4.4.1.cmml" xref="S3.SS4.p1.1.m1.4.4.4.4">subscript</csymbol><ci id="S3.SS4.p1.1.m1.4.4.4.4.2.cmml" xref="S3.SS4.p1.1.m1.4.4.4.4.2">𝑘</ci><ci id="S3.SS4.p1.1.m1.4.4.4.4.3.cmml" xref="S3.SS4.p1.1.m1.4.4.4.4.3">𝑚</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.4c">(k_{d},k_{n},k_{r},k_{m})</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.4d">( italic_k start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT )</annotation></semantics></math> as ground-truths for model fine-tuning.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The generated PBR material maps can be used for tiling in the garment sewing pattern. The remaining question is how to determine the scale for tiling? We consider two specific strategies: (1) Proportion-aware tiling. We use image segmentation to calculate the proportion of the caputured region relative to the segmented clothing, maintaining a similar ratio when tiling the generated texture onto the sewing pattern. (2) User-guided tiling. We emphasize that an end-to-end automatic tilling method may not be optimal, as user involvement is often necessary to resolve ambiguities and provide flexibility in fashion industries.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We validate FabricDiffusion with both synthetic data and real-world images across various scenarios. We begin by introducing the experimental setup in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS1" title="4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4.1</span></a>, followed by detailing the experimental results in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2" title="4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4.2</span></a>. Finally, we conduct ablation studies and show several real-world applications in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS3" title="4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Setup</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Dataset.</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">We detail the process of collecting BRDF texture, print, and garment datasets.
(1) Fabric BRDF dataset. This dataset includes 3.8k real fabric materials and 100k pseudo-BRDF textures (RGB only). We reserved 200 real BRDF materials for testing the PBR generator and 800 pseudo-BRDF materials (combined with the 200 real materials) for testing the texture generator.
(2) 3D garment dataset. We collected 22 3D garment meshes for training and 5 for testing. Using the method in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS2" title="3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we created 220k flat and distorted rendered image pairs for training and 5k pairs for testing.
(3) Logos and prints dataset. This dataset contains 7k prints and logos in PNG format. We generated pseudo-BRDF materials with specific roughness and metallic values and a flat normal map. Dark prints were converted to white if necessary. By compositing these onto 3D garments, we produced 82k warped print images.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Evaluation protocols and tasks.</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">We compare FabricDiffusion to state-of-the-art methods on two tasks:
(1) Image-to-garment texture transfer. Our ultimate goal is to transfer the textures and prints from the reference image to the target garment. We evaluate FabricDiffusion and compare it to baseline methods using both synthetic and real-world test examples.
(2) PBR materials extraction. We provide both quantitative and qualitative results on PBR materials estimation using our testing BRDF materials dataset.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Evaluation metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">We evaluate the quality of generated textures and garments using commonly used metrics: LPIPS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib58" title="">2018</a>)</cite> , SSIM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib50" title="">2004</a>)</cite>, MS-SSIM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib51" title="">2003</a>)</cite>, DISTS <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib9" title="">2020</a>)</cite>, and FLIP <cite class="ltx_cite ltx_citemacro_citep">(Andersson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib2" title="">2020</a>)</cite>. To evaluate the tileability of the generated textures, we adopt the metric proposed by TexTile
<cite class="ltx_cite ltx_citemacro_citep">(Rodriguez-Pardo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib35" title="">2024</a>)</cite>.
For the image-to-garment texture transfer task, we additionally report FID <cite class="ltx_cite ltx_citemacro_citep">(Heusel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib17" title="">2017</a>)</cite> and CLIP-score in CLIP image feature space <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib33" title="">2021</a>; Gal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib12" title="">2022</a>)</cite> to evaluate the visual similarity of the textured garment with the original input clothing.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4. </span>Baseline methods.</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">We compare with state-of-the-art methods that support image-to-mesh texture transfer, including: (1) TEXTure <cite class="ltx_cite ltx_citemacro_citep">(Richardson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib34" title="">2023</a>)</cite>, the most representative method for texturing a 3D mesh based on a small set of sample images through per-subject optimization (i.e., textual inversion <cite class="ltx_cite ltx_citemacro_citep">(Gal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib12" title="">2022</a>)</cite> for personalization). (2) Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite>, which focuses on texture extraction and PBR materials estimation from a single image using generative models. (3) MatFusion <cite class="ltx_cite ltx_citemacro_citep">(Sartor and Peers, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib41" title="">2023</a>)</cite>, for PBR materials estimation for general materials, not specifically fabric or clothing. We fine-tuned the pre-trained MatFusion model with our curated fabric BRDF training examples, resulting in improved performance.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<p class="ltx_p ltx_align_center" id="S4.F4.1"><span class="ltx_text" id="S4.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="919" id="S4.F4.1.1.g1" src="x4.png" width="747"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span><span class="ltx_text" id="S4.F4.3.1" style="font-size:90%;">Results on texture transfer on real-world clothing images. Our method can handle real-world garment images to generate normalized texture maps, along with the corresponding PBR materials. The PBR maps can be applied to the 3D garment for realistic relighting and rendering.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5">
<p class="ltx_p ltx_align_center" id="S4.F5.1"><span class="ltx_text" id="S4.F5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="159" id="S4.F5.1.1.g1" src="x5.png" width="814"/></span></p>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span><span class="ltx_text" id="S4.F5.3.1" style="font-size:90%;">Results on prints and logos transfer on real-world images. Given a real-life garment image with prints and/or logos, and the cropped patch of the region where the print is located. Our method generates a distortion-free and transparent print element, which can be applied to the target 3D garment for realistic rendering. Note that the background texture is transferred using our method as well.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6">
<p class="ltx_p ltx_align_center" id="S4.F6.1"><span class="ltx_text" id="S4.F6.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="910" id="S4.F6.1.1.g1" src="x6.png" width="814"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span><span class="ltx_text" id="S4.F6.3.1" style="font-size:90%;">Comparison on image-to-garment texture transfer. FabricDiffusion faithfully captures and preserves the texture pattern from the input clothing. We observe texture irregularities and artifacts for Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> and TEXTure <cite class="ltx_cite ltx_citemacro_citep">(Richardson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib34" title="">2023</a>)</cite>.
</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Experimental Results</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>FabricDiffusion on real-world clothing images.</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We first show the results of our method on real-world images in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F4" title="Figure 4 ‣ 4.1.4. Baseline methods. ‣ 4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4</span></a>. Our method effectively transfers both texture patterns and material properties from various types of clothing to the target 3D garment. Notably, our method is capable of recovering challenging materials such as knit, translucent fabric, and leather.
We attribute this success to our construction of paired training examples that seamlessly couples the PBR generator with the upstream texture generator.
Since we focus on non-metallic fabrics, the metallic map is omitted in the visualizations in the section. Please be referred to Appendix for more details and results.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>FabricDiffusion on detailed prints and logos.</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In addition to texture patterns and material properties, our FabricDiffusion model can transfer detailed prints and logos. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F5" title="Figure 5 ‣ 4.1.4. Baseline methods. ‣ 4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">5</span></a> shows some examples. We highlight two key advantages of our design that benefit the recovery of prints and logos. First, our conditional generative model corrects geometry distortion caused by human pose or camera perspective. Second, as detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS3" title="3.3. Normalized Texture Generation via FabricDiffusion ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.3</span></a>, our method can generate prints with a transparent background, enabling practical usage in garment appearance modeling.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Image-to-garment texture transfer.</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F6" title="Figure 6 ‣ 4.1.4. Baseline methods. ‣ 4.1. Setup ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">6</span></a>, we compare our method with Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> and TEXTure <cite class="ltx_cite ltx_citemacro_citep">(Richardson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib34" title="">2023</a>)</cite> for image-to-garment texture transfer. We present the results on real-world clothing images featuring a variety of textures, ranging from micro to macro patterns and prints. Our observations indicate that FabricDiffusion not only recovers repetitive patterns, such as scattered stars or camouflage, but also maintains the regularity of structured patterns, like the plaid on a skirt. Please refer to Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.T1" title="Table 1 ‣ 4.2.3. Image-to-garment texture transfer. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">1</span></a> for quantitative results.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>Quantitative comparison on image-to-garment clothing texture transfer. Performances evaluated on synthetic testing data. Our method succeeds at faithfully extracting and transferring textures from images, whereas Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> exhibits significant artifacts, resulting in suboptimal performance, particularly on FID.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.6.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_tt" id="S4.T1.6.6.7" style="padding-left:1.5pt;padding-right:1.5pt;"></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S4.T1.1.1.1.1" style="font-size:90%;">FID </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.2.2" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S4.T1.2.2.2.1" style="font-size:90%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S4.T1.3.3.3.1" style="font-size:90%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.4.4" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S4.T1.4.4.4.1" style="font-size:90%;">MS-SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.4.4.4.m1.1"><semantics id="S4.T1.4.4.4.m1.1a"><mo id="S4.T1.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.5.5.5" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S4.T1.5.5.5.1" style="font-size:90%;">DISTS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.5.5.5.m1.1"><semantics id="S4.T1.5.5.5.m1.1a"><mo id="S4.T1.5.5.5.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.5.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.6.6.6" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text" id="S4.T1.6.6.6.1" style="font-size:90%;">CLIP-s</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.6.6.6.m1.1"><semantics id="S4.T1.6.6.6.m1.1a"><mo id="S4.T1.6.6.6.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T1.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.6.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.6.7.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.6.7.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S4.T1.6.7.1.1.1" style="font-size:90%;">Material Palette</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.6.7.1.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S4.T1.6.7.1.2.1" style="font-size:90%;">34.39</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.6.7.1.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S4.T1.6.7.1.3.1" style="font-size:90%;">0.20</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.6.7.1.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S4.T1.6.7.1.4.1" style="font-size:90%;">0.75</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.6.7.1.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S4.T1.6.7.1.5.1" style="font-size:90%;">0.73</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.6.7.1.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S4.T1.6.7.1.6.1" style="font-size:90%;">0.28</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.6.7.1.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S4.T1.6.7.1.7.1" style="font-size:90%;">0.94</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.8.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.6.8.2.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S4.T1.6.8.2.1.1" style="font-size:90%;">FabricDiffusion (ours)</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.6.8.2.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.2.1" style="font-size:90%;">12.44</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.6.8.2.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.3.1" style="font-size:90%;">0.16</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.6.8.2.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.4.1" style="font-size:90%;">0.79</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.6.8.2.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.5.1" style="font-size:90%;">0.77</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.6.8.2.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.6.1" style="font-size:90%;">0.19</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.6.8.2.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.8.2.7.1" style="font-size:90%;">0.97</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Quantitative comparison with state-of-the-art methods on PBR material extraction. Results are evaluated on the real PBR test examples. By fine-tuning MatFusion with additional fabric PBR training data, our method achieves superior performance across most material maps. Material Palette performs subpar, particularly in estimating the diffuse and roughness maps, due the differences in physical properties between fabric materials and general objects. Please see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.T3" title="Table 3 ‣ 4.2.4. PBR materials extraction. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3</span></a> for quantitative evaluation on rendered images and Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F7" title="Figure 7 ‣ 4.2.3. Image-to-garment texture transfer. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">7</span></a> for a qualitative comparison between FabricDiffusion and Material Palette.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.2.2.3" style="padding-left:2.5pt;padding-right:2.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T2.1.1.1" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text" id="S4.T2.1.1.1.1" style="font-size:90%;">MSE</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="3" id="S4.T2.2.2.2" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text" id="S4.T2.2.2.2.1" style="font-size:90%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T2.2.3.1.1" style="padding-left:2.5pt;padding-right:2.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.3.1.2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.3.1.2.1" style="font-size:90%;">Diff.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.3.1.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.3.1.3.1" style="font-size:90%;">Norm.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T2.2.3.1.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.3.1.4.1" style="font-size:90%;">Rough.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.3.1.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.3.1.5.1" style="font-size:90%;">Diff.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.3.1.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.3.1.6.1" style="font-size:90%;">Norm.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.3.1.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.3.1.7.1" style="font-size:90%;">Rough.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.4.1.1" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.4.1.1.1" style="font-size:90%;">Material Palette</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.1.2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.4.1.2.1" style="font-size:90%;">0.0515</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.1.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.4.1.3.1" style="font-size:90%;">0.0136</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.4.1.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.4.1.4.1" style="font-size:90%;">0.1287</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.1.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.4.1.5.1" style="font-size:90%;">0.2213</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.1.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.4.1.6.1" style="font-size:90%;">0.3028</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.1.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.4.1.7.1" style="font-size:90%;">0.2920</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.5.2.1" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.5.2.1.1" style="font-size:90%;">MatFusion</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.2.2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.5.2.2.1" style="font-size:90%;">0.0896</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.2.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.5.2.3.1" style="font-size:90%;">0.0127</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.5.2.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.5.2.4.1" style="font-size:90%;">0.0806</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.2.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.5.2.5.1" style="font-size:90%;">0.2190</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.2.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.5.2.6.1" style="font-size:90%;">0.3902</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.5.2.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.5.2.7.1" style="font-size:90%;">0.4922</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.2.6.3.1" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text" id="S4.T2.2.6.3.1.1" style="font-size:90%;">FabricDiffusion (ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.6.3.2" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.6.3.2.1" style="font-size:90%;">0.0287</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.6.3.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.6.3.3.1" style="font-size:90%;">0.0094</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.6.3.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.6.3.4.1" style="font-size:90%;">0.0559</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.6.3.5" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.6.3.5.1" style="font-size:90%;">0.3157</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.6.3.6" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.6.3.6.1" style="font-size:90%;">0.3827</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.6.3.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.6.3.7.1" style="font-size:90%;">0.5039</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F7">
<p class="ltx_p ltx_align_center" id="S4.F7.1"><span class="ltx_text" id="S4.F7.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="534" id="S4.F7.1.1.g1" src="x7.png" width="814"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7. </span><span class="ltx_text" id="S4.F7.3.1" style="font-size:90%;">Qualitative comparison on PBR materials extraction. Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> can hardly capture fabric materials while our FabricDiffusion model is capable of recovering physical properties for fabric textures especially on roughness and diffuse maps.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span>PBR materials extraction.</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">We compare our method to Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> and MatFusion <cite class="ltx_cite ltx_citemacro_citep">(Sartor and Peers, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib41" title="">2023</a>)</cite> on PBR materials extraction. In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.T2" title="Table 2 ‣ 4.2.3. Image-to-garment texture transfer. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">2</span></a>, we present a comparison of pixel-level MSE and SSIM between the generated material maps and the ground-truths. Our FabricDiffusion material generator, fine-tuned from the base MatFusion model with additional fabric BRDF training examples, demonstrates superior performance. Additionally, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F7" title="Figure 7 ‣ 4.2.3. Image-to-garment texture transfer. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">7</span></a> shows visual comparisons between FabricDiffusion and Material Palette. While Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> struggles to accurately capture fabric materials, our FabricDiffusion model excels in recovering the physical properties for fabric textures, particularly in roughness and diffuse maps.
We also evaluate different methods on the rendered images and show the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.T3" title="Table 3 ‣ 4.2.4. PBR materials extraction. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3</span></a>. Particularly, we use render-aware metrics like FLIP <cite class="ltx_cite ltx_citemacro_citep">(Andersson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib2" title="">2020</a>)</cite> and perceptual metrics like LPIPS and DISTS. FabricDiffusion consistently achieve better performance over other approaches.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Quantitative comparison on rendered materials. We adopt render-aware and perceptual metrics and compare the quality of rendered generated texture. FabricDiffusion outperforms other methods.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.5.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T3.5.5.6" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text" id="S4.T3.1.1.1.1" style="font-size:90%;">MSE</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.2" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text" id="S4.T3.2.2.2.1" style="font-size:90%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.3.3" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text" id="S4.T3.3.3.3.1" style="font-size:90%;">DISTS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.4.4.4" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text" id="S4.T3.4.4.4.1" style="font-size:90%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.m1.1"><semantics id="S4.T3.4.4.4.m1.1a"><mo id="S4.T3.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.5.5.5" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text" id="S4.T3.5.5.5.1" style="font-size:90%;">FLIP</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.5.5.5.m1.1"><semantics id="S4.T3.5.5.5.m1.1a"><mo id="S4.T3.5.5.5.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.m1.1b"><ci id="S4.T3.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.6.1.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T3.5.6.1.1.1" style="font-size:90%;">Material Palette</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.6.1.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.5.6.1.2.1" style="font-size:90%;">0.0531</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.6.1.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T3.5.6.1.3.1" style="font-size:90%;">0.2838</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.6.1.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.5.6.1.4.1" style="font-size:90%;">0.3388</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.6.1.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.5.6.1.5.1" style="font-size:90%;">0.4463</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.6.1.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.5.6.1.6.1" style="font-size:90%;">0.5812</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.5.7.2.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T3.5.7.2.1.1" style="font-size:90%;">MatFusion</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.2.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T3.5.7.2.2.1" style="font-size:90%;">0.1032</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.2.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.5.7.2.3.1" style="font-size:90%;">0.3233</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.2.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T3.5.7.2.4.1" style="font-size:90%;">0.3790</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.2.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T3.5.7.2.5.1" style="font-size:90%;">0.5697</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.7.2.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T3.5.7.2.6.1" style="font-size:90%;">0.7009</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.5.8.3.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T3.5.8.3.1.1" style="font-size:90%;">FabricDiffusion (ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.8.3.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.8.3.2.1" style="font-size:90%;">0.0284</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.8.3.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.8.3.3.1" style="font-size:90%;">0.4102</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.8.3.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.8.3.4.1" style="font-size:90%;">0.3035</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.8.3.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.8.3.5.1" style="font-size:90%;">0.3836</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.8.3.6" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.8.3.6.1" style="font-size:90%;">0.4411</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Ablations, Analyses, and Applications</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Ablation on circular padding and tileability analysis.</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We conduct an ablation study to evaluate the impact of circular padding using the TexTile metric <cite class="ltx_cite ltx_citemacro_citep">(Rodriguez-Pardo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib35" title="">2024</a>)</cite>, where higher values indicate better tileability. The results show that the MaterialPalette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> achieves a score of 0.54. Our method without circular padding scores 0.47, while with circular padding, our method improves significantly, reaching a score of 0.62.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Ablation on pseudo-BRDF data.</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">We compare the performance of using combined real-BRDF and pseudo-BRDF data versus using only real-BRDF data. The results, summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.T4" title="Table 4 ‣ 4.3.4. Effect of the capture scale. ‣ 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4</span></a>, demonstrate that the inclusion of pseudo-BRDF data alongside real-BRDF data improves performance across all metrics.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>Effect of the capture location.</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS4" title="3.4. PBR Materials Generation and Garment Rendering ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.4</span></a>, we explored how FabricDiffusion can be integrated into an end-to-end framework for 3D garment design. To assess whether the generated texture remains consistent with the input, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F8" title="Figure 8 ‣ 4.3.6. Compatibility with AI-Generated Images. ‣ 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">8</span></a>-(a) shows the results of varying the location of a fixed-size capture region. The results indicate that FabricDiffusion consistently produces similar texture patterns, regardless of the location of the captured region.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4. </span>Effect of the capture scale.</h4>
<div class="ltx_para" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F8" title="Figure 8 ‣ 4.3.6. Compatibility with AI-Generated Images. ‣ 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">8</span></a>-(b), we further study the effect of the size of the captured region. By varying the scale of the captured region, FabricDiffusion recovers the texture pattern from the input patch, demonstrating robustness to changes in resolution.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Ablation study on pseudo-BRDF data. We compare the performance of using combined versus only real-BRDF data. Combined data effectively improve the performance.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.4.4.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.4.5.1" style="font-size:90%;">Real-BRDF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.4.4.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.4.6.1" style="font-size:90%;">Pseudo-BRDF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1" style="padding-left:5.5pt;padding-right:5.5pt;">
<span class="ltx_text" id="S4.T4.1.1.1.1" style="font-size:90%;">FID</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T4.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.2.2" style="padding-left:5.5pt;padding-right:5.5pt;">
<span class="ltx_text" id="S4.T4.2.2.2.1" style="font-size:90%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T4.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.3.3.3" style="padding-left:5.5pt;padding-right:5.5pt;">
<span class="ltx_text" id="S4.T4.3.3.3.1" style="font-size:90%;">DISTS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.m1.1"><semantics id="S4.T4.3.3.3.m1.1a"><mo id="S4.T4.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T4.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.4.4.4" style="padding-left:5.5pt;padding-right:5.5pt;">
<span class="ltx_text" id="S4.T4.4.4.4.1" style="font-size:90%;">CLIP-s</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.4.4.4.m1.1"><semantics id="S4.T4.4.4.4.m1.1a"><mo id="S4.T4.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.m1.1b"><ci id="S4.T4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.4.5.1.1" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.5.1.1.1" style="font-size:90%;">✓</span></th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.4.5.1.2" style="padding-left:5.5pt;padding-right:5.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.5.1.3" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.5.1.3.1" style="font-size:90%;">19.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.5.1.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.5.1.4.1" style="font-size:90%;">0.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.5.1.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.5.1.5.1" style="font-size:90%;">0.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.5.1.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.5.1.6.1" style="font-size:90%;">0.96</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T4.4.6.2.1" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.6.2.1.1" style="font-size:90%;">✓</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T4.4.6.2.2" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text" id="S4.T4.4.6.2.2.1" style="font-size:90%;">✓</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.6.2.3" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.6.2.3.1" style="font-size:90%;">12.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.6.2.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.6.2.4.1" style="font-size:90%;">0.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.6.2.5" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.6.2.5.1" style="font-size:90%;">0.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.6.2.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.6.2.6.1" style="font-size:90%;">0.97</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5. </span>Multi-material texture transfer.</h4>
<div class="ltx_para" id="S4.SS3.SSS5.p1">
<p class="ltx_p" id="S4.SS3.SSS5.p1.1">Since FabricDiffusion works on patches, it can be applied to multi-material garments as well as evidenced in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F10" title="Figure 10 ‣ 4.3.6. Compatibility with AI-Generated Images. ‣ 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">10</span></a>. This suggests that FabricDiffusion can serve as a basic building block for multi-material garment texture transfer.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.6. </span>Compatibility with AI-Generated Images.</h4>
<div class="ltx_para" id="S4.SS3.SSS6.p1">
<p class="ltx_p" id="S4.SS3.SSS6.p1.1">We explore the possibility of enhancing FabricDiffusion with AI-generated images and demonstrate the results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F9" title="Figure 9 ‣ 4.3.6. Compatibility with AI-Generated Images. ‣ 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">9</span></a>. In addition to real-life clothing, we use an advanced text-to-image model to create apparel images and the apply FabricDiffusion to transfer their textures to the target 3D garments.
This opens up new creative possibilities for designers, allowing them to envision and materialize entirely novel textures and patterns through simple text descriptions.</p>
</div>
<figure class="ltx_figure" id="S4.F8">
<p class="ltx_p ltx_align_center" id="S4.F8.1"><span class="ltx_text" id="S4.F8.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="194" id="S4.F8.1.1.g1" src="x8.png" width="813"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8. </span><span class="ltx_text" id="S4.F8.3.1" style="font-size:90%;">Ablation study on varying the position and scale of the captured texture. Given an input clothing image, we evaluate (a) varying the position with a fixed capture size and (b) varying the scale for texture extraction. Our method successfully recovers the input texture despite variation in the location or resolution of the captured image. Since we care about distributions, none of the generated images are cherry- or lemon-pick.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F9">
<p class="ltx_p ltx_align_center" id="S4.F9.1"><span class="ltx_text" id="S4.F9.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="143" id="S4.F9.1.1.g1" src="x9.png" width="789"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9. </span><span class="ltx_text" id="S4.F9.3.1" style="font-size:90%;">Compatibility with generative apparel. FabricDiffusion can extract the textures from the output image of a text-to-image generative model and apply them to a target 3D garment of arbitrary shapes. We highlight that our method can handle imperfect textures, such as the broken black stripes in the first example. For each example, we show the input text prompt (bottom-left), the generated 2D image by Stable Diffusion XL (top-left), and the textured 3D garment (right) created by our FabricDiffusion method.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="182" id="S4.F10.g1" src="x10.png" width="814"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span><span class="ltx_text" id="S4.F10.2.1" style="font-size:90%;">Multi-material textures transfer. Given a clothing image containing multiple texture patterns, materials, and prints, FabricDiffusion can transfer each distinct element to separate regions of the target 3D garment.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="126" id="S4.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span><span class="ltx_text" id="S4.F11.2.1" style="font-size:90%;">Limitations of FabricDiffusion. Our method may struggle to reconstruct specific inputs such as complex (e.g., non-repetitive) patterns (left), fine details in complex prints (middle), and prints over non uniform fabric (right).
</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion, Limitation, and Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduce FabricDiffusion, a new method for transferring fabric textures and prints from a single real-world clothing image onto 3D garments with arbitrary shapes.
Our method, trained entirely using synthetic rendered images, is able to generate undistorted texture and prints from in-the-wild clothing images.
While our method demonstrates strong generalization abilities with real photos and diverse texture patterns, it faces challenges with certain inputs, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F11" title="Figure 11 ‣ 4.3.6. Compatibility with AI-Generated Images. ‣ 4.3. Ablations, Analyses, and Applications ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">11</span></a>. Specifically, FabricDiffusion may produce errors when reconstructing non-repetitive patterns and struggles to accurately capture fine details in complex prints or logos, especially since our focus is on prints with uniform backgrounds, moderate complexity, and moderate distortion.
In the future, we plan to address these challenges by enhancing texture transfer for more complex scenarios and improving performance on difficult fabric categories, such as leather. Additionally, we plan to expand our method to handle a broader range of material maps, including transmittance, to further extend its applicability.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andersson et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Pontus Andersson, Jim
Nilsson, Tomas Akenine-Möller, Magnus
Oskarsson, Kalle Åström, and
Mark D Fairchild. 2020.

</span>
<span class="ltx_bibblock">FLIP: A Difference Evaluator for Alternating
Images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proc. ACM Comput. Graph. Interact. Tech.</em>
3, 2 (2020),
15–1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casas and Comino-Trinidad (2023)</span>
<span class="ltx_bibblock">
Dan Casas and Marc
Comino-Trinidad. 2023.

</span>
<span class="ltx_bibblock">Smplitex: A generative model and dataset for 3d
human texture estimation from single image.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2309.01855</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cazenavette et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
George Cazenavette,
Tongzhou Wang, Antonio Torralba,
Alexei A Efros, and Jun-Yan Zhu.
2022.

</span>
<span class="ltx_bibblock">Wearable imagenet: Synthesizing tileable textures
via dataset distillation. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.
2278–2282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xipeng Chen, Guangrun
Wang, Dizhong Zhu, Xiaodan Liang,
Philip Torr, and Liang Lin.
2022.

</span>
<span class="ltx_bibblock">Structure-preserving 3D garment modeling with
neural sewing machines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Advances in Neural Information Processing
Systems</em> 35 (2022),
15147–15159.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deschaintre et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Valentin Deschaintre,
Miika Aittala, Fredo Durand,
George Drettakis, and Adrien Bousseau.
2018.

</span>
<span class="ltx_bibblock">Single-image svbrdf capture with a rendering-aware
deep network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">ACM Transactions on Graphics (ToG)</em>
37, 4 (2018),
1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deschaintre et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Valentin Deschaintre,
Diego Gutierrez, Tamy Boubekeur,
Julia Guerrero-Viu, and Belen Masia.
2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">The visual language of fabrics</em>.

</span>
<span class="ltx_bibblock">Technical Report.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diamanti et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Olga Diamanti, Connelly
Barnes, Sylvain Paris, Eli Shechtman,
and Olga Sorkine-Hornung.
2015.

</span>
<span class="ltx_bibblock">Synthesis of complex image appearance from limited
exemplars.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">ACM Transactions on Graphics (TOG)</em>
34, 2 (2015),
1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Keyan Ding, Kede Ma,
Shiqi Wang, and Eero P Simoncelli.
2020.

</span>
<span class="ltx_bibblock">Image quality assessment: Unifying structure and
texture similarity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">IEEE transactions on pattern analysis and
machine intelligence</em> 44, 5
(2020), 2567–2581.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Efros and Freeman (2023)</span>
<span class="ltx_bibblock">
Alexei A Efros and
William T Freeman. 2023.

</span>
<span class="ltx_bibblock">Image quilting for texture synthesis and transfer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Seminal Graphics Papers: Pushing the
Boundaries, Volume 2</em>. 571–576.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Efros and Leung (1999)</span>
<span class="ltx_bibblock">
Alexei A Efros and
Thomas K Leung. 1999.

</span>
<span class="ltx_bibblock">Texture synthesis by non-parametric sampling. In
<em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the seventh IEEE international
conference on computer vision</em>, Vol. 2.
1033–1038.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Rinon Gal, Yuval Alaluf,
Yuval Atzmon, Or Patashnik,
Amit H Bermano, Gal Chechik, and
Daniel Cohen-Or. 2022.

</span>
<span class="ltx_bibblock">An image is worth one word: Personalizing
text-to-image generation using textual inversion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:2208.01618</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Daiheng Gao, Xu Chen,
Xindi Zhang, Qi Wang, Ke
Sun, Bang Zhang, Liefeng Bo, and
Qixing Huang. 2024.

</span>
<span class="ltx_bibblock">Cloth2Tex: A Customized Cloth Texture Generation
Pipeline for 3D Virtual Try-On. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guarnera et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Giuseppe Claudio Guarnera,
Peter Hall, Alain Chesnais, and
Mashhuda Glencross. 2017.

</span>
<span class="ltx_bibblock">Woven fabric model creation from a single image.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">ACM Transactions on Graphics (TOG)</em>
36, 5 (2017),
1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Guoqing Hao, Satoshi
Iizuka, Kensho Hara, Edgar Simo-Serra,
Hirokatsu Kataoka, and Kazuhiro Fukui.
2023.

</span>
<span class="ltx_bibblock">Diffusion-based Holistic Texture Rectification and
Synthesis. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">SIGGRAPH Asia 2023 Conference
Papers</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Henzler et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Philipp Henzler, Valentin
Deschaintre, Niloy J Mitra, and Tobias
Ritschel. 2021.

</span>
<span class="ltx_bibblock">Generative modelling of BRDF textures from flash
images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2102.11861</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Martin Heusel, Hubert
Ramsauer, Thomas Unterthiner, Bernhard
Nessler, and Sepp Hochreiter.
2017.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule
converge to a local nash equilibrium.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Advances in neural information processing
systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain,
and Pieter Abbeel. 2020.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Advances in neural information processing
systems</em> 33 (2020),
6840–6851.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho and Salimans (2022)</span>
<span class="ltx_bibblock">
Jonathan Ho and Tim
Salimans. 2022.

</span>
<span class="ltx_bibblock">Classifier-free diffusion guidance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2207.12598</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tero Karras, Samuli
Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila.
2020.

</span>
<span class="ltx_bibblock">Analyzing and improving the image quality of
stylegan. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>.
8110–8119.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Welling (2013)</span>
<span class="ltx_bibblock">
Diederik P Kingma and
Max Welling. 2013.

</span>
<span class="ltx_bibblock">Auto-encoding variational bayes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:1312.6114</em>
(2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korosteleva and Lee (2021)</span>
<span class="ltx_bibblock">
Maria Korosteleva and
Sung-Hee Lee. 2021.

</span>
<span class="ltx_bibblock">Generating datasets of 3d garments with sewing
patterns.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2109.05633</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon and Lee (2024)</span>
<span class="ltx_bibblock">
Hyun-Song Kwon and
Sung-Hee Lee. 2024.

</span>
<span class="ltx_bibblock">DeepIron: Predicting Unwarped Garment Texture from
a Single Image. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Eurographics</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Boqian Li, Xuan Li,
Ying Jiang, Tianyi Xie,
Feng Gao, Huamin Wang,
Yin Yang, and Chenfanfu Jiang.
2024.

</span>
<span class="ltx_bibblock">GarmentDreamer: 3DGS Guided Garment Synthesis with
Diverse Geometry and Texture Details.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">arXiv preprint arXiv:2405.12420</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xueting Li, Xiaolong
Wang, Ming-Hsuan Yang, Alexei A Efros,
and Sifei Liu. 2022.

</span>
<span class="ltx_bibblock">Scraping Textures from Natural Images for Synthesis
and Editing. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">European Conference on Computer
Vision</em>. Springer, 391–408.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yifei Li, Hsiao-yu Chen,
Egor Larionov, Nikolaos Sarafianos,
Wojciech Matusik, and Tuur Stuyck.
2023.

</span>
<span class="ltx_bibblock">DiffAvatar: Simulation-Ready Garment Optimization
with Differentiable Simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">arXiv preprint arXiv:2311.12194</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lijuan Liu, Xiangyu Xu,
Zhijie Lin, Jiabin Liang, and
Shuicheng Yan. 2023.

</span>
<span class="ltx_bibblock">Towards garment sewing pattern reconstruction from
a single image.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">ACM Transactions on Graphics (TOG)</em>
42, 6 (2023),
1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopes et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ivan Lopes, Fabio
Pizzati, and Raoul de Charette.
2024.

</span>
<span class="ltx_bibblock">Material Palette: Extraction of Materials from a
Single Image. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majithia et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sahib Majithia, Sandeep N
Parameswaran, Sadbhavana Babar, Vikram
Garg, Astitva Srivastava, and Avinash
Sharma. 2022.

</span>
<span class="ltx_bibblock">Robust 3d garment digitization from monocular 2d
images for 3d virtual try-on systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision</em>. 3428–3438.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McAuley et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Stephen McAuley, Stephen
Hill, Naty Hoffman, Yoshiharu Gotanda,
Brian Smits, Brent Burley, and
Adam Martinez. 2012.

</span>
<span class="ltx_bibblock">Practical physically-based shading in film and game
production.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">ACM SIGGRAPH 2012 Courses</em>.
1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mir et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Aymen Mir, Thiemo
Alldieck, and Gerard Pons-Moll.
2020.

</span>
<span class="ltx_bibblock">Learning to transfer texture from clothing images
to 3d humans. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
7023–7034.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moritz et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Joep Moritz, Stuart
James, Tom SF Haines, Tobias Ritschel,
and Tim Weyrich. 2017.

</span>
<span class="ltx_bibblock">Texture stationarization: Turning photos into
tileable textures. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Computer graphics forum</em>,
Vol. 36. Wiley Online Library, 177–188.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook
Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark,
et al<span class="ltx_text" id="bib.bib33.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural
language supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.4.1">International conference
on machine learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richardson et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Elad Richardson, Gal
Metzer, Yuval Alaluf, Raja Giryes, and
Daniel Cohen-Or. 2023.

</span>
<span class="ltx_bibblock">Texture: Text-guided texturing of 3d shapes. In
<em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">ACM SIGGRAPH 2023 Conference Proceedings</em>.
1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodriguez-Pardo et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Carlos Rodriguez-Pardo,
Dan Casas, Elena Garces, and
Jorge Lopez-Moreno. 2024.

</span>
<span class="ltx_bibblock">TexTile: A Differentiable Metric for Texture
Tileability. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
4439–4449.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodriguez-Pardo et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Carlos Rodriguez-Pardo,
Henar Dominguez-Elvira, David
Pascual-Hernandez, and Elena Garces.
2023.

</span>
<span class="ltx_bibblock">Umat: Uncertainty-aware single image high
resolution material capture. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.
5764–5774.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodriguez-Pardo and Garces (2022)</span>
<span class="ltx_bibblock">
Carlos Rodriguez-Pardo and
Elena Garces. 2022.

</span>
<span class="ltx_bibblock">Seamlessgan: Self-supervised synthesis of tileable
texture maps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">IEEE Transactions on Visualization and
Computer Graphics</em> 29, 6
(2022), 2914–2925.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodriguez-Pardo et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Carlos Rodriguez-Pardo,
Sergio Suja, David Pascual,
Jorge Lopez-Moreno, and Elena Garces.
2019.

</span>
<span class="ltx_bibblock">Automatic extraction and synthesis of regular
repeatable patterns.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Computers &amp; Graphics</em> 83
(2019), 33–41.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas
Blattmann, Dominik Lorenz, Patrick
Esser, and Björn Ommer.
2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent
diffusion models. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>.
10684–10695.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarafianos et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Nikolaos Sarafianos, Tuur
Stuyck, Xiaoyu Xiang, Yilei Li,
Jovan Popovic, and Rakesh Ranjan.
2024.

</span>
<span class="ltx_bibblock">Garment3DGen: 3D Garment Stylization and Texture
Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:2403.18816</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sartor and Peers (2023)</span>
<span class="ltx_bibblock">
Sam Sartor and Pieter
Peers. 2023.

</span>
<span class="ltx_bibblock">Matfusion: a generative diffusion model for svbrdf
capture. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">SIGGRAPH Asia 2023 Conference
Papers</em>. 1–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schröder et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Kai Schröder, Arno
Zinke, and Reinhard Klein.
2014.

</span>
<span class="ltx_bibblock">Image-based reverse engineering and visual
prototyping of woven cloth.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">IEEE transactions on visualization and
computer graphics</em> 21, 2
(2014), 188–200.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohl-Dickstein et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Jascha Sohl-Dickstein,
Eric Weiss, Niru Maheswaranathan, and
Surya Ganguli. 2015.

</span>
<span class="ltx_bibblock">Deep unsupervised learning using nonequilibrium
thermodynamics. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">International conference on
machine learning</em>. PMLR, 2256–2265.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Astitva Srivastava, Pranav
Manu, Amit Raj, Varun Jampani, and
Avinash Sharma. 2024.

</span>
<span class="ltx_bibblock">WordRobe: Text-Guided Generation of Textured 3D
Garments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">arXiv preprint arXiv:2403.17541</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Peihan Tu, Li-Yi Wei,
and Matthias Zwicker. 2022.

</span>
<span class="ltx_bibblock">Clustered vector textures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">ACM Transactions on Graphics (TOG)</em>
41, 4 (2022),
1–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vecchio and Deschaintre (2024)</span>
<span class="ltx_bibblock">
Giuseppe Vecchio and
Valentin Deschaintre. 2024.

</span>
<span class="ltx_bibblock">MatSynth: A Modern PBR Materials Dataset. In
<em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>. 22109–22118.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vecchio et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Giuseppe Vecchio, Rosalie
Martin, Arthur Roullier, Adrien Kaiser,
Romain Rouffet, Valentin Deschaintre,
and Tamy Boubekeur. 2023.

</span>
<span class="ltx_bibblock">Controlmat: a controlled generative approach to
material capture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">ACM Transactions on Graphics</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vecchio et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Giuseppe Vecchio, Simone
Palazzo, and Concetto Spampinato.
2021.

</span>
<span class="ltx_bibblock">Surfacenet: Adversarial svbrdf estimation from a
single image. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>. 12840–12848.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vecchio et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Giuseppe Vecchio, Renato
Sortino, Simone Palazzo, and Concetto
Spampinato. 2024.

</span>
<span class="ltx_bibblock">Matfuse: controllable material generation with
diffusion models. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
4429–4438.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Zhou Wang, Alan C Bovik,
Hamid R Sheikh, and Eero P Simoncelli.
2004.

</span>
<span class="ltx_bibblock">Image quality assessment: from error visibility to
structural similarity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">IEEE transactions on image processing</em>
13, 4 (2004),
600–612.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2003)</span>
<span class="ltx_bibblock">
Zhou Wang, Eero P
Simoncelli, and Alan C Bovik.
2003.

</span>
<span class="ltx_bibblock">Multiscale structural similarity for image quality
assessment. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">The Thrity-Seventh Asilomar
Conference on Signals, Systems &amp; Computers, 2003</em>,
Vol. 2. Ieee, 1398–1402.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Li-Yi Wei, Sylvain
Lefebvre, Vivek Kwatra, and Greg
Turk. 2009.

</span>
<span class="ltx_bibblock">State of the art in example-based texture
synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Eurographics 2009, State of the Art Report,
EG-STAR</em> (2009), 93–117.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Hong-yu Wu, Xiao-wu Chen,
Chen-xu Zhang, Bin Zhou, and
Qin-ping Zhao. 2019.

</span>
<span class="ltx_bibblock">Modeling yarn-level geometry from a single
micro-image.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Frontiers of Information Technology &amp;
Electronic Engineering</em> 20, 9
(2019), 1165–1174.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yeh et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yu-Ying Yeh, Jia-Bin
Huang, Changil Kim, Lei Xiao,
Thu Nguyen-Phuoc, Numair Khan,
Cheng Zhang, Manmohan Chandraker,
Carl S Marshall, Zhao Dong,
et al<span class="ltx_text" id="bib.bib54.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">TextureDreamer: Image-guided Texture Synthesis
through Geometry-aware Diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.4.1">arXiv preprint arXiv:2401.09416</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yeh et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yu-Ying Yeh, Zhengqin Li,
Yannick Hold-Geoffroy, Rui Zhu,
Zexiang Xu, Miloš Hašan,
Kalyan Sunkavalli, and Manmohan
Chandraker. 2022.

</span>
<span class="ltx_bibblock">Photoscene: Photorealistic material and lighting
transfer for indoor scenes. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.
18562–18571.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng (2023)</span>
<span class="ltx_bibblock">
Xianfang Zeng.
2023.

</span>
<span class="ltx_bibblock">Paint3D: Paint Anything 3D with Lighting-Less
Texture Diffusion Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2312.13913</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Agrawala (2024)</span>
<span class="ltx_bibblock">
Lvmin Zhang and Maneesh
Agrawala. 2024.

</span>
<span class="ltx_bibblock">Transparent Image Layer Diffusion using Latent
Transparency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2402.17113</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip
Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. 2018.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as
a perceptual metric. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>.
586–595.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shangzhan Zhang, Sida
Peng, Tao Xu, Yuanbo Yang,
Tianrun Chen, Nan Xue,
Yujun Shen, Hujun Bao,
Ruizhen Hu, and Xiaowei Zhou.
2024.

</span>
<span class="ltx_bibblock">MaPa: Text-driven Photorealistic Material Painting
for 3D Shapes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">arXiv preprint arXiv:2404.17569</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xilong Zhou, Milos Hasan,
Valentin Deschaintre, Paul Guerrero,
Yannick Hold-Geoffroy, Kalyan Sunkavalli,
and Nima Khademi Kalantari.
2023b.

</span>
<span class="ltx_bibblock">Photomat: A material generator learned from single
flash photos. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">ACM SIGGRAPH 2023 Conference
Proceedings</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xilong Zhou, Milos Hasan,
Valentin Deschaintre, Paul Guerrero,
Kalyan Sunkavalli, and Nima Khademi
Kalantari. 2022.

</span>
<span class="ltx_bibblock">Tilegen: Tileable, controllable material generation
and capture. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">SIGGRAPH Asia 2022 conference
papers</em>. 1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou and Kalantari (2021)</span>
<span class="ltx_bibblock">
Xilong Zhou and
Nima Khademi Kalantari. 2021.

</span>
<span class="ltx_bibblock">Adversarial Single-Image SVBRDF Estimation with
Hybrid Training. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Computer Graphics Forum</em>,
Vol. 40. Wiley Online Library, 315–325.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yang Zhou, Kaijian Chen,
Rongjun Xiao, and Hui Huang.
2023a.

</span>
<span class="ltx_bibblock">Neural texture synthesis with guided
correspondence. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
18095–18104.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p1">
<p class="ltx_p ltx_align_center" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1" style="font-size:173%;">Supplementary Material</span></p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">We provide details and results omitted in the main text.</p>
<ul class="ltx_itemize" id="A0.I1">
<li class="ltx_item" id="A0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A0.I1.i1.p1">
<p class="ltx_p" id="A0.I1.i1.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A1" title="Supplementary Material A A Key Advantages of FabricDiffusion ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">A</span></a>: Key advantages of FabricDiffusion.</p>
</div>
</li>
<li class="ltx_item" id="A0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A0.I1.i2.p1">
<p class="ltx_p" id="A0.I1.i2.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A2" title="Supplementary Material B B Details on Dataset Construction ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">B</span></a>: Additional details on dataset construction.</p>
</div>
</li>
<li class="ltx_item" id="A0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A0.I1.i3.p1">
<p class="ltx_p" id="A0.I1.i3.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A3" title="Supplementary Material C C Additional Details of Our Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">C</span></a>: Additional implementation details.</p>
</div>
</li>
<li class="ltx_item" id="A0.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A0.I1.i4.p1">
<p class="ltx_p" id="A0.I1.i4.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A4" title="Supplementary Material D D Additional Results ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">D</span></a>: Additional results and analyses.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Supplementary Material A A </span>Key Advantages of FabricDiffusion</h2>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Normalized texture representation.</h5>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.1">Unlike existing image-to-3D texture transfer methods, FabricDiffusion generates normalized textures that can be used in the 2D UV space. We highlight two outputs: (1) High-quality, distortion-free, and tileable texture maps from a non-rigid garment surface. (2) Seamless integration with SVBRDF material estimation pipelines, which usually build upon the first output — standard close-up views of the materials as input.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Sim-to-real generalizability.</h5>
<div class="ltx_para" id="A1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1">The conditional diffusion model, trained entirely using <em class="ltx_emph ltx_font_italic" id="A1.SS0.SSS0.Px2.p1.1.1">synthetic</em> rendering images, proves highly effective in generating normalized texture maps from <em class="ltx_emph ltx_font_italic" id="A1.SS0.SSS0.Px2.p1.1.2">real-world</em> images. We attribute this success to: (1) Our model bridging the domain gap between real and rendered textures by conditioning on the real input texture. (2) Synthetic data offering controllable supervision and diverse geometric, illumination, and occlusion variations.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Data and computational efficiency.</h5>
<div class="ltx_para" id="A1.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px3.p1.1">During training, our method of creating pseudo-BRDF material is effective in scaling up the training examples. During inference, our model performs feed-forward sampling from Gaussian noise, which takes approximately less than 5 seconds on a single NVIDIA A6000 GPU. In contrast, existing texture transfer methods often rely on costly per-example optimization.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Supplementary Material B B </span>Details on Dataset Construction</h2>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Fabric BRDF and textile dataset.</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1">To curate textures and their BRDF materials, we use several public libraries (AmbientCG<span class="ltx_note ltx_role_footnote" id="footnote1a"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://ambientcg.com/</span></span></span>, ShareTextures<span class="ltx_note ltx_role_footnote" id="footnote2a"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.sharetextures.com/</span></span></span>, 3D Textures<span class="ltx_note ltx_role_footnote" id="footnote3a"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://3dtextures.me/</span></span></span>) under the CC0 license and supplement them with additional assets purchased from artists. The real BRDF dataset we collected comprises 3.8k assets, encompassing a broad spectrum of fabric materials. The pseudo-BRDF dataset contain 100k fabric textures with only RGB color images. We reserved 200 materials from the real BRDF dataset for testing our BRDF generator, and 800 materials from the pseudo BRDF dataset (combined with the previous 200 materials) for testing the texture flattening module.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p2.1">Our textile images are collected from online sources including Openverse<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://openverse.org/</span></span></span>, PublicDomainPictures<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://publicdomainpictures.net/en/</span></span></span>, and ARTX<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://architextures.org/</span></span></span> under CC0 or royalty-free license.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">3D garment mesh dataset.</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">We collect 22 raw 3D garment meshes for training and 5 garment meshes for testing. That is, during the testing with synthetic data, the model has not seen the geometry from the 5 testing meshes. With the method described in Section 3.2 of the main paper, we construct approximately 220k flat and warped texture pairs for training and 5k pairs for testing.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Logos and prints dataset.</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.2">We collect a dataset of 7k prints and logos in PNG format with CC0 license. Their corresponding pseudo-BRDF materials are generated by assigning a uniform roughness value sampled from <math alttext="\mathcal{U}(0.4,0.7)" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p1.1.m1.2"><semantics id="A2.SS0.SSS0.Px3.p1.1.m1.2a"><mrow id="A2.SS0.SSS0.Px3.p1.1.m1.2.3" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.2" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.2.cmml">𝒰</mi><mo id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.1" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.1.cmml">⁢</mo><mrow id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.2" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml"><mo id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.2.1" stretchy="false" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml">(</mo><mn id="A2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">0.4</mn><mo id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.2.2" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px3.p1.1.m1.2.2" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.2.cmml">0.7</mn><mo id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.2.3" stretchy="false" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.1.m1.2b"><apply id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.cmml" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3"><times id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.1.cmml" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.1"></times><ci id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.2.cmml" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.2">𝒰</ci><interval closure="open" id="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.1.cmml" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.3.3.2"><cn id="A2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" type="float" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1">0.4</cn><cn id="A2.SS0.SSS0.Px3.p1.1.m1.2.2.cmml" type="float" xref="A2.SS0.SSS0.Px3.p1.1.m1.2.2">0.7</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.1.m1.2c">\mathcal{U}(0.4,0.7)</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.1.m1.2d">caligraphic_U ( 0.4 , 0.7 )</annotation></semantics></math>, a uniform metallic value sampled from <math alttext="\mathcal{U}(0,0.3)" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p1.2.m2.2"><semantics id="A2.SS0.SSS0.Px3.p1.2.m2.2a"><mrow id="A2.SS0.SSS0.Px3.p1.2.m2.2.3" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.2" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.2.cmml">𝒰</mi><mo id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.1" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.1.cmml">⁢</mo><mrow id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.2" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.1.cmml"><mo id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.2.1" stretchy="false" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.1.cmml">(</mo><mn id="A2.SS0.SSS0.Px3.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">0</mn><mo id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.2.2" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="A2.SS0.SSS0.Px3.p1.2.m2.2.2" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.2.cmml">0.3</mn><mo id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.2.3" stretchy="false" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.2.m2.2b"><apply id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3"><times id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.1.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.1"></times><ci id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.2.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.2">𝒰</ci><interval closure="open" id="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.1.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.3.3.2"><cn id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1">0</cn><cn id="A2.SS0.SSS0.Px3.p1.2.m2.2.2.cmml" type="float" xref="A2.SS0.SSS0.Px3.p1.2.m2.2.2">0.3</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.2.m2.2c">\mathcal{U}(0,0.3)</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.2.m2.2d">caligraphic_U ( 0 , 0.3 )</annotation></semantics></math>, and a default flat normal map. In cases where a print was uniformly black, we converted it to white if the background texture was also dark. By compositing the logo prints onto the 3D garments, we obtain a total of 82k warped print images, following the method outlined in Section  <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.SS2" title="3.2. Synthetic Paired Training Data Construction ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">3.2</span></a> of the main paper.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Supplementary Material C C </span>Additional Details of Our Method</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1. </span>Details on physics-based rendering</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.13">During rendering, each image pixel value at a specific viewing direction can be computed using the following reflectance equation:</p>
<table class="ltx_equation ltx_eqn_table" id="A3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L(p,\omega_{o})=\int_{\Omega}f_{r}(p,\omega_{i},\omega_{o})L_{i}(p,\omega_{i})%
(\omega_{i}\cdot n_{p})\mathrm{d}\omega_{i}," class="ltx_Math" display="block" id="A3.E6.m1.4"><semantics id="A3.E6.m1.4a"><mrow id="A3.E6.m1.4.4.1" xref="A3.E6.m1.4.4.1.1.cmml"><mrow id="A3.E6.m1.4.4.1.1" xref="A3.E6.m1.4.4.1.1.cmml"><mrow id="A3.E6.m1.4.4.1.1.1" xref="A3.E6.m1.4.4.1.1.1.cmml"><mi id="A3.E6.m1.4.4.1.1.1.3" xref="A3.E6.m1.4.4.1.1.1.3.cmml">L</mi><mo id="A3.E6.m1.4.4.1.1.1.2" xref="A3.E6.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="A3.E6.m1.4.4.1.1.1.1.1" xref="A3.E6.m1.4.4.1.1.1.1.2.cmml"><mo id="A3.E6.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="A3.E6.m1.4.4.1.1.1.1.2.cmml">(</mo><mi id="A3.E6.m1.1.1" xref="A3.E6.m1.1.1.cmml">p</mi><mo id="A3.E6.m1.4.4.1.1.1.1.1.3" xref="A3.E6.m1.4.4.1.1.1.1.2.cmml">,</mo><msub id="A3.E6.m1.4.4.1.1.1.1.1.1" xref="A3.E6.m1.4.4.1.1.1.1.1.1.cmml"><mi id="A3.E6.m1.4.4.1.1.1.1.1.1.2" xref="A3.E6.m1.4.4.1.1.1.1.1.1.2.cmml">ω</mi><mi id="A3.E6.m1.4.4.1.1.1.1.1.1.3" xref="A3.E6.m1.4.4.1.1.1.1.1.1.3.cmml">o</mi></msub><mo id="A3.E6.m1.4.4.1.1.1.1.1.4" stretchy="false" xref="A3.E6.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="A3.E6.m1.4.4.1.1.6" rspace="0.111em" xref="A3.E6.m1.4.4.1.1.6.cmml">=</mo><mrow id="A3.E6.m1.4.4.1.1.5" xref="A3.E6.m1.4.4.1.1.5.cmml"><msub id="A3.E6.m1.4.4.1.1.5.5" xref="A3.E6.m1.4.4.1.1.5.5.cmml"><mo id="A3.E6.m1.4.4.1.1.5.5.2" xref="A3.E6.m1.4.4.1.1.5.5.2.cmml">∫</mo><mi id="A3.E6.m1.4.4.1.1.5.5.3" mathvariant="normal" xref="A3.E6.m1.4.4.1.1.5.5.3.cmml">Ω</mi></msub><mrow id="A3.E6.m1.4.4.1.1.5.4" xref="A3.E6.m1.4.4.1.1.5.4.cmml"><msub id="A3.E6.m1.4.4.1.1.5.4.6" xref="A3.E6.m1.4.4.1.1.5.4.6.cmml"><mi id="A3.E6.m1.4.4.1.1.5.4.6.2" xref="A3.E6.m1.4.4.1.1.5.4.6.2.cmml">f</mi><mi id="A3.E6.m1.4.4.1.1.5.4.6.3" xref="A3.E6.m1.4.4.1.1.5.4.6.3.cmml">r</mi></msub><mo id="A3.E6.m1.4.4.1.1.5.4.5" xref="A3.E6.m1.4.4.1.1.5.4.5.cmml">⁢</mo><mrow id="A3.E6.m1.4.4.1.1.3.2.2.2" xref="A3.E6.m1.4.4.1.1.3.2.2.3.cmml"><mo id="A3.E6.m1.4.4.1.1.3.2.2.2.3" stretchy="false" xref="A3.E6.m1.4.4.1.1.3.2.2.3.cmml">(</mo><mi id="A3.E6.m1.2.2" xref="A3.E6.m1.2.2.cmml">p</mi><mo id="A3.E6.m1.4.4.1.1.3.2.2.2.4" xref="A3.E6.m1.4.4.1.1.3.2.2.3.cmml">,</mo><msub id="A3.E6.m1.4.4.1.1.2.1.1.1.1" xref="A3.E6.m1.4.4.1.1.2.1.1.1.1.cmml"><mi id="A3.E6.m1.4.4.1.1.2.1.1.1.1.2" xref="A3.E6.m1.4.4.1.1.2.1.1.1.1.2.cmml">ω</mi><mi id="A3.E6.m1.4.4.1.1.2.1.1.1.1.3" xref="A3.E6.m1.4.4.1.1.2.1.1.1.1.3.cmml">i</mi></msub><mo id="A3.E6.m1.4.4.1.1.3.2.2.2.5" xref="A3.E6.m1.4.4.1.1.3.2.2.3.cmml">,</mo><msub id="A3.E6.m1.4.4.1.1.3.2.2.2.2" xref="A3.E6.m1.4.4.1.1.3.2.2.2.2.cmml"><mi id="A3.E6.m1.4.4.1.1.3.2.2.2.2.2" xref="A3.E6.m1.4.4.1.1.3.2.2.2.2.2.cmml">ω</mi><mi id="A3.E6.m1.4.4.1.1.3.2.2.2.2.3" xref="A3.E6.m1.4.4.1.1.3.2.2.2.2.3.cmml">o</mi></msub><mo id="A3.E6.m1.4.4.1.1.3.2.2.2.6" stretchy="false" xref="A3.E6.m1.4.4.1.1.3.2.2.3.cmml">)</mo></mrow><mo id="A3.E6.m1.4.4.1.1.5.4.5a" xref="A3.E6.m1.4.4.1.1.5.4.5.cmml">⁢</mo><msub id="A3.E6.m1.4.4.1.1.5.4.7" xref="A3.E6.m1.4.4.1.1.5.4.7.cmml"><mi id="A3.E6.m1.4.4.1.1.5.4.7.2" xref="A3.E6.m1.4.4.1.1.5.4.7.2.cmml">L</mi><mi id="A3.E6.m1.4.4.1.1.5.4.7.3" xref="A3.E6.m1.4.4.1.1.5.4.7.3.cmml">i</mi></msub><mo id="A3.E6.m1.4.4.1.1.5.4.5b" xref="A3.E6.m1.4.4.1.1.5.4.5.cmml">⁢</mo><mrow id="A3.E6.m1.4.4.1.1.4.3.3.1" xref="A3.E6.m1.4.4.1.1.4.3.3.2.cmml"><mo id="A3.E6.m1.4.4.1.1.4.3.3.1.2" stretchy="false" xref="A3.E6.m1.4.4.1.1.4.3.3.2.cmml">(</mo><mi id="A3.E6.m1.3.3" xref="A3.E6.m1.3.3.cmml">p</mi><mo id="A3.E6.m1.4.4.1.1.4.3.3.1.3" xref="A3.E6.m1.4.4.1.1.4.3.3.2.cmml">,</mo><msub id="A3.E6.m1.4.4.1.1.4.3.3.1.1" xref="A3.E6.m1.4.4.1.1.4.3.3.1.1.cmml"><mi id="A3.E6.m1.4.4.1.1.4.3.3.1.1.2" xref="A3.E6.m1.4.4.1.1.4.3.3.1.1.2.cmml">ω</mi><mi id="A3.E6.m1.4.4.1.1.4.3.3.1.1.3" xref="A3.E6.m1.4.4.1.1.4.3.3.1.1.3.cmml">i</mi></msub><mo id="A3.E6.m1.4.4.1.1.4.3.3.1.4" stretchy="false" xref="A3.E6.m1.4.4.1.1.4.3.3.2.cmml">)</mo></mrow><mo id="A3.E6.m1.4.4.1.1.5.4.5c" xref="A3.E6.m1.4.4.1.1.5.4.5.cmml">⁢</mo><mrow id="A3.E6.m1.4.4.1.1.5.4.4.1" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.cmml"><mo id="A3.E6.m1.4.4.1.1.5.4.4.1.2" stretchy="false" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.cmml">(</mo><mrow id="A3.E6.m1.4.4.1.1.5.4.4.1.1" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.cmml"><msub id="A3.E6.m1.4.4.1.1.5.4.4.1.1.2" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.cmml"><mi id="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.2" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.2.cmml">ω</mi><mi id="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.3" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.3.cmml">i</mi></msub><mo id="A3.E6.m1.4.4.1.1.5.4.4.1.1.1" lspace="0.222em" rspace="0.222em" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.1.cmml">⋅</mo><msub id="A3.E6.m1.4.4.1.1.5.4.4.1.1.3" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.cmml"><mi id="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.2" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.2.cmml">n</mi><mi id="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.3" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.3.cmml">p</mi></msub></mrow><mo id="A3.E6.m1.4.4.1.1.5.4.4.1.3" stretchy="false" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.cmml">)</mo></mrow><mo id="A3.E6.m1.4.4.1.1.5.4.5d" lspace="0em" xref="A3.E6.m1.4.4.1.1.5.4.5.cmml">⁢</mo><mrow id="A3.E6.m1.4.4.1.1.5.4.8" xref="A3.E6.m1.4.4.1.1.5.4.8.cmml"><mo id="A3.E6.m1.4.4.1.1.5.4.8.1" rspace="0em" xref="A3.E6.m1.4.4.1.1.5.4.8.1.cmml">d</mo><msub id="A3.E6.m1.4.4.1.1.5.4.8.2" xref="A3.E6.m1.4.4.1.1.5.4.8.2.cmml"><mi id="A3.E6.m1.4.4.1.1.5.4.8.2.2" xref="A3.E6.m1.4.4.1.1.5.4.8.2.2.cmml">ω</mi><mi id="A3.E6.m1.4.4.1.1.5.4.8.2.3" xref="A3.E6.m1.4.4.1.1.5.4.8.2.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow><mo id="A3.E6.m1.4.4.1.2" xref="A3.E6.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.E6.m1.4b"><apply id="A3.E6.m1.4.4.1.1.cmml" xref="A3.E6.m1.4.4.1"><eq id="A3.E6.m1.4.4.1.1.6.cmml" xref="A3.E6.m1.4.4.1.1.6"></eq><apply id="A3.E6.m1.4.4.1.1.1.cmml" xref="A3.E6.m1.4.4.1.1.1"><times id="A3.E6.m1.4.4.1.1.1.2.cmml" xref="A3.E6.m1.4.4.1.1.1.2"></times><ci id="A3.E6.m1.4.4.1.1.1.3.cmml" xref="A3.E6.m1.4.4.1.1.1.3">𝐿</ci><interval closure="open" id="A3.E6.m1.4.4.1.1.1.1.2.cmml" xref="A3.E6.m1.4.4.1.1.1.1.1"><ci id="A3.E6.m1.1.1.cmml" xref="A3.E6.m1.1.1">𝑝</ci><apply id="A3.E6.m1.4.4.1.1.1.1.1.1.cmml" xref="A3.E6.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.1.1.1.1.1.cmml" xref="A3.E6.m1.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.1.1.1.1.2.cmml" xref="A3.E6.m1.4.4.1.1.1.1.1.1.2">𝜔</ci><ci id="A3.E6.m1.4.4.1.1.1.1.1.1.3.cmml" xref="A3.E6.m1.4.4.1.1.1.1.1.1.3">𝑜</ci></apply></interval></apply><apply id="A3.E6.m1.4.4.1.1.5.cmml" xref="A3.E6.m1.4.4.1.1.5"><apply id="A3.E6.m1.4.4.1.1.5.5.cmml" xref="A3.E6.m1.4.4.1.1.5.5"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.5.5.1.cmml" xref="A3.E6.m1.4.4.1.1.5.5">subscript</csymbol><int id="A3.E6.m1.4.4.1.1.5.5.2.cmml" xref="A3.E6.m1.4.4.1.1.5.5.2"></int><ci id="A3.E6.m1.4.4.1.1.5.5.3.cmml" xref="A3.E6.m1.4.4.1.1.5.5.3">Ω</ci></apply><apply id="A3.E6.m1.4.4.1.1.5.4.cmml" xref="A3.E6.m1.4.4.1.1.5.4"><times id="A3.E6.m1.4.4.1.1.5.4.5.cmml" xref="A3.E6.m1.4.4.1.1.5.4.5"></times><apply id="A3.E6.m1.4.4.1.1.5.4.6.cmml" xref="A3.E6.m1.4.4.1.1.5.4.6"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.5.4.6.1.cmml" xref="A3.E6.m1.4.4.1.1.5.4.6">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.5.4.6.2.cmml" xref="A3.E6.m1.4.4.1.1.5.4.6.2">𝑓</ci><ci id="A3.E6.m1.4.4.1.1.5.4.6.3.cmml" xref="A3.E6.m1.4.4.1.1.5.4.6.3">𝑟</ci></apply><vector id="A3.E6.m1.4.4.1.1.3.2.2.3.cmml" xref="A3.E6.m1.4.4.1.1.3.2.2.2"><ci id="A3.E6.m1.2.2.cmml" xref="A3.E6.m1.2.2">𝑝</ci><apply id="A3.E6.m1.4.4.1.1.2.1.1.1.1.cmml" xref="A3.E6.m1.4.4.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.2.1.1.1.1.1.cmml" xref="A3.E6.m1.4.4.1.1.2.1.1.1.1">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.2.1.1.1.1.2.cmml" xref="A3.E6.m1.4.4.1.1.2.1.1.1.1.2">𝜔</ci><ci id="A3.E6.m1.4.4.1.1.2.1.1.1.1.3.cmml" xref="A3.E6.m1.4.4.1.1.2.1.1.1.1.3">𝑖</ci></apply><apply id="A3.E6.m1.4.4.1.1.3.2.2.2.2.cmml" xref="A3.E6.m1.4.4.1.1.3.2.2.2.2"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.3.2.2.2.2.1.cmml" xref="A3.E6.m1.4.4.1.1.3.2.2.2.2">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.3.2.2.2.2.2.cmml" xref="A3.E6.m1.4.4.1.1.3.2.2.2.2.2">𝜔</ci><ci id="A3.E6.m1.4.4.1.1.3.2.2.2.2.3.cmml" xref="A3.E6.m1.4.4.1.1.3.2.2.2.2.3">𝑜</ci></apply></vector><apply id="A3.E6.m1.4.4.1.1.5.4.7.cmml" xref="A3.E6.m1.4.4.1.1.5.4.7"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.5.4.7.1.cmml" xref="A3.E6.m1.4.4.1.1.5.4.7">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.5.4.7.2.cmml" xref="A3.E6.m1.4.4.1.1.5.4.7.2">𝐿</ci><ci id="A3.E6.m1.4.4.1.1.5.4.7.3.cmml" xref="A3.E6.m1.4.4.1.1.5.4.7.3">𝑖</ci></apply><interval closure="open" id="A3.E6.m1.4.4.1.1.4.3.3.2.cmml" xref="A3.E6.m1.4.4.1.1.4.3.3.1"><ci id="A3.E6.m1.3.3.cmml" xref="A3.E6.m1.3.3">𝑝</ci><apply id="A3.E6.m1.4.4.1.1.4.3.3.1.1.cmml" xref="A3.E6.m1.4.4.1.1.4.3.3.1.1"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.4.3.3.1.1.1.cmml" xref="A3.E6.m1.4.4.1.1.4.3.3.1.1">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.4.3.3.1.1.2.cmml" xref="A3.E6.m1.4.4.1.1.4.3.3.1.1.2">𝜔</ci><ci id="A3.E6.m1.4.4.1.1.4.3.3.1.1.3.cmml" xref="A3.E6.m1.4.4.1.1.4.3.3.1.1.3">𝑖</ci></apply></interval><apply id="A3.E6.m1.4.4.1.1.5.4.4.1.1.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1"><ci id="A3.E6.m1.4.4.1.1.5.4.4.1.1.1.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.1">⋅</ci><apply id="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.2"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.1.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.2">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.2.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.2">𝜔</ci><ci id="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.3.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.2.3">𝑖</ci></apply><apply id="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.3"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.1.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.3">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.2.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.2">𝑛</ci><ci id="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.3.cmml" xref="A3.E6.m1.4.4.1.1.5.4.4.1.1.3.3">𝑝</ci></apply></apply><apply id="A3.E6.m1.4.4.1.1.5.4.8.cmml" xref="A3.E6.m1.4.4.1.1.5.4.8"><csymbol cd="latexml" id="A3.E6.m1.4.4.1.1.5.4.8.1.cmml" xref="A3.E6.m1.4.4.1.1.5.4.8.1">differential-d</csymbol><apply id="A3.E6.m1.4.4.1.1.5.4.8.2.cmml" xref="A3.E6.m1.4.4.1.1.5.4.8.2"><csymbol cd="ambiguous" id="A3.E6.m1.4.4.1.1.5.4.8.2.1.cmml" xref="A3.E6.m1.4.4.1.1.5.4.8.2">subscript</csymbol><ci id="A3.E6.m1.4.4.1.1.5.4.8.2.2.cmml" xref="A3.E6.m1.4.4.1.1.5.4.8.2.2">𝜔</ci><ci id="A3.E6.m1.4.4.1.1.5.4.8.2.3.cmml" xref="A3.E6.m1.4.4.1.1.5.4.8.2.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.E6.m1.4c">L(p,\omega_{o})=\int_{\Omega}f_{r}(p,\omega_{i},\omega_{o})L_{i}(p,\omega_{i})%
(\omega_{i}\cdot n_{p})\mathrm{d}\omega_{i},</annotation><annotation encoding="application/x-llamapun" id="A3.E6.m1.4d">italic_L ( italic_p , italic_ω start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) = ∫ start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_p , italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ω start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_p , italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ( italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) roman_d italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A3.SS1.p1.12">where <math alttext="L" class="ltx_Math" display="inline" id="A3.SS1.p1.1.m1.1"><semantics id="A3.SS1.p1.1.m1.1a"><mi id="A3.SS1.p1.1.m1.1.1" xref="A3.SS1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.1.m1.1b"><ci id="A3.SS1.p1.1.m1.1.1.cmml" xref="A3.SS1.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.1.m1.1d">italic_L</annotation></semantics></math> is the rendered pixel color along the direction <math alttext="\omega_{o}" class="ltx_Math" display="inline" id="A3.SS1.p1.2.m2.1"><semantics id="A3.SS1.p1.2.m2.1a"><msub id="A3.SS1.p1.2.m2.1.1" xref="A3.SS1.p1.2.m2.1.1.cmml"><mi id="A3.SS1.p1.2.m2.1.1.2" xref="A3.SS1.p1.2.m2.1.1.2.cmml">ω</mi><mi id="A3.SS1.p1.2.m2.1.1.3" xref="A3.SS1.p1.2.m2.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.2.m2.1b"><apply id="A3.SS1.p1.2.m2.1.1.cmml" xref="A3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.2.m2.1.1.1.cmml" xref="A3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="A3.SS1.p1.2.m2.1.1.2.cmml" xref="A3.SS1.p1.2.m2.1.1.2">𝜔</ci><ci id="A3.SS1.p1.2.m2.1.1.3.cmml" xref="A3.SS1.p1.2.m2.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.2.m2.1c">\omega_{o}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.2.m2.1d">italic_ω start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> from the surface point <math alttext="p" class="ltx_Math" display="inline" id="A3.SS1.p1.3.m3.1"><semantics id="A3.SS1.p1.3.m3.1a"><mi id="A3.SS1.p1.3.m3.1.1" xref="A3.SS1.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.3.m3.1b"><ci id="A3.SS1.p1.3.m3.1.1.cmml" xref="A3.SS1.p1.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.3.m3.1d">italic_p</annotation></semantics></math>, <math alttext="\Omega=\{\omega_{i}:\omega_{i}\cdot n_{p}\geq 0\}" class="ltx_Math" display="inline" id="A3.SS1.p1.4.m4.2"><semantics id="A3.SS1.p1.4.m4.2a"><mrow id="A3.SS1.p1.4.m4.2.2" xref="A3.SS1.p1.4.m4.2.2.cmml"><mi id="A3.SS1.p1.4.m4.2.2.4" mathvariant="normal" xref="A3.SS1.p1.4.m4.2.2.4.cmml">Ω</mi><mo id="A3.SS1.p1.4.m4.2.2.3" xref="A3.SS1.p1.4.m4.2.2.3.cmml">=</mo><mrow id="A3.SS1.p1.4.m4.2.2.2.2" xref="A3.SS1.p1.4.m4.2.2.2.3.cmml"><mo id="A3.SS1.p1.4.m4.2.2.2.2.3" stretchy="false" xref="A3.SS1.p1.4.m4.2.2.2.3.1.cmml">{</mo><msub id="A3.SS1.p1.4.m4.1.1.1.1.1" xref="A3.SS1.p1.4.m4.1.1.1.1.1.cmml"><mi id="A3.SS1.p1.4.m4.1.1.1.1.1.2" xref="A3.SS1.p1.4.m4.1.1.1.1.1.2.cmml">ω</mi><mi id="A3.SS1.p1.4.m4.1.1.1.1.1.3" xref="A3.SS1.p1.4.m4.1.1.1.1.1.3.cmml">i</mi></msub><mo id="A3.SS1.p1.4.m4.2.2.2.2.4" lspace="0.278em" rspace="0.278em" xref="A3.SS1.p1.4.m4.2.2.2.3.1.cmml">:</mo><mrow id="A3.SS1.p1.4.m4.2.2.2.2.2" xref="A3.SS1.p1.4.m4.2.2.2.2.2.cmml"><mrow id="A3.SS1.p1.4.m4.2.2.2.2.2.2" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.cmml"><msub id="A3.SS1.p1.4.m4.2.2.2.2.2.2.2" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.cmml"><mi id="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.2" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.2.cmml">ω</mi><mi id="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.3" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="A3.SS1.p1.4.m4.2.2.2.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.1.cmml">⋅</mo><msub id="A3.SS1.p1.4.m4.2.2.2.2.2.2.3" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.cmml"><mi id="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.2" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.2.cmml">n</mi><mi id="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.3" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.3.cmml">p</mi></msub></mrow><mo id="A3.SS1.p1.4.m4.2.2.2.2.2.1" xref="A3.SS1.p1.4.m4.2.2.2.2.2.1.cmml">≥</mo><mn id="A3.SS1.p1.4.m4.2.2.2.2.2.3" xref="A3.SS1.p1.4.m4.2.2.2.2.2.3.cmml">0</mn></mrow><mo id="A3.SS1.p1.4.m4.2.2.2.2.5" stretchy="false" xref="A3.SS1.p1.4.m4.2.2.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.4.m4.2b"><apply id="A3.SS1.p1.4.m4.2.2.cmml" xref="A3.SS1.p1.4.m4.2.2"><eq id="A3.SS1.p1.4.m4.2.2.3.cmml" xref="A3.SS1.p1.4.m4.2.2.3"></eq><ci id="A3.SS1.p1.4.m4.2.2.4.cmml" xref="A3.SS1.p1.4.m4.2.2.4">Ω</ci><apply id="A3.SS1.p1.4.m4.2.2.2.3.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2"><csymbol cd="latexml" id="A3.SS1.p1.4.m4.2.2.2.3.1.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.3">conditional-set</csymbol><apply id="A3.SS1.p1.4.m4.1.1.1.1.1.cmml" xref="A3.SS1.p1.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.4.m4.1.1.1.1.1.1.cmml" xref="A3.SS1.p1.4.m4.1.1.1.1.1">subscript</csymbol><ci id="A3.SS1.p1.4.m4.1.1.1.1.1.2.cmml" xref="A3.SS1.p1.4.m4.1.1.1.1.1.2">𝜔</ci><ci id="A3.SS1.p1.4.m4.1.1.1.1.1.3.cmml" xref="A3.SS1.p1.4.m4.1.1.1.1.1.3">𝑖</ci></apply><apply id="A3.SS1.p1.4.m4.2.2.2.2.2.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2"><geq id="A3.SS1.p1.4.m4.2.2.2.2.2.1.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.1"></geq><apply id="A3.SS1.p1.4.m4.2.2.2.2.2.2.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2"><ci id="A3.SS1.p1.4.m4.2.2.2.2.2.2.1.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.1">⋅</ci><apply id="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.1.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.2">subscript</csymbol><ci id="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.2.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.2">𝜔</ci><ci id="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.3.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.2.3">𝑖</ci></apply><apply id="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.1.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.3">subscript</csymbol><ci id="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.2.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.2">𝑛</ci><ci id="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.3.cmml" xref="A3.SS1.p1.4.m4.2.2.2.2.2.2.3.3">𝑝</ci></apply></apply><cn id="A3.SS1.p1.4.m4.2.2.2.2.2.3.cmml" type="integer" xref="A3.SS1.p1.4.m4.2.2.2.2.2.3">0</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.4.m4.2c">\Omega=\{\omega_{i}:\omega_{i}\cdot n_{p}\geq 0\}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.4.m4.2d">roman_Ω = { italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ≥ 0 }</annotation></semantics></math> denotes a hemisphere with the incident direction <math alttext="\omega_{i}" class="ltx_Math" display="inline" id="A3.SS1.p1.5.m5.1"><semantics id="A3.SS1.p1.5.m5.1a"><msub id="A3.SS1.p1.5.m5.1.1" xref="A3.SS1.p1.5.m5.1.1.cmml"><mi id="A3.SS1.p1.5.m5.1.1.2" xref="A3.SS1.p1.5.m5.1.1.2.cmml">ω</mi><mi id="A3.SS1.p1.5.m5.1.1.3" xref="A3.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.5.m5.1b"><apply id="A3.SS1.p1.5.m5.1.1.cmml" xref="A3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.5.m5.1.1.1.cmml" xref="A3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="A3.SS1.p1.5.m5.1.1.2.cmml" xref="A3.SS1.p1.5.m5.1.1.2">𝜔</ci><ci id="A3.SS1.p1.5.m5.1.1.3.cmml" xref="A3.SS1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.5.m5.1c">\omega_{i}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.5.m5.1d">italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and surface normal <math alttext="n_{p}" class="ltx_Math" display="inline" id="A3.SS1.p1.6.m6.1"><semantics id="A3.SS1.p1.6.m6.1a"><msub id="A3.SS1.p1.6.m6.1.1" xref="A3.SS1.p1.6.m6.1.1.cmml"><mi id="A3.SS1.p1.6.m6.1.1.2" xref="A3.SS1.p1.6.m6.1.1.2.cmml">n</mi><mi id="A3.SS1.p1.6.m6.1.1.3" xref="A3.SS1.p1.6.m6.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.6.m6.1b"><apply id="A3.SS1.p1.6.m6.1.1.cmml" xref="A3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.6.m6.1.1.1.cmml" xref="A3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="A3.SS1.p1.6.m6.1.1.2.cmml" xref="A3.SS1.p1.6.m6.1.1.2">𝑛</ci><ci id="A3.SS1.p1.6.m6.1.1.3.cmml" xref="A3.SS1.p1.6.m6.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.6.m6.1c">n_{p}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.6.m6.1d">italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> at point <math alttext="p" class="ltx_Math" display="inline" id="A3.SS1.p1.7.m7.1"><semantics id="A3.SS1.p1.7.m7.1a"><mi id="A3.SS1.p1.7.m7.1.1" xref="A3.SS1.p1.7.m7.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.7.m7.1b"><ci id="A3.SS1.p1.7.m7.1.1.cmml" xref="A3.SS1.p1.7.m7.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.7.m7.1c">p</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.7.m7.1d">italic_p</annotation></semantics></math>, <math alttext="L_{i}" class="ltx_Math" display="inline" id="A3.SS1.p1.8.m8.1"><semantics id="A3.SS1.p1.8.m8.1a"><msub id="A3.SS1.p1.8.m8.1.1" xref="A3.SS1.p1.8.m8.1.1.cmml"><mi id="A3.SS1.p1.8.m8.1.1.2" xref="A3.SS1.p1.8.m8.1.1.2.cmml">L</mi><mi id="A3.SS1.p1.8.m8.1.1.3" xref="A3.SS1.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.8.m8.1b"><apply id="A3.SS1.p1.8.m8.1.1.cmml" xref="A3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.8.m8.1.1.1.cmml" xref="A3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="A3.SS1.p1.8.m8.1.1.2.cmml" xref="A3.SS1.p1.8.m8.1.1.2">𝐿</ci><ci id="A3.SS1.p1.8.m8.1.1.3.cmml" xref="A3.SS1.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.8.m8.1c">L_{i}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.8.m8.1d">italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the incident light that is represented by the environment map, and <math alttext="f_{r}" class="ltx_Math" display="inline" id="A3.SS1.p1.9.m9.1"><semantics id="A3.SS1.p1.9.m9.1a"><msub id="A3.SS1.p1.9.m9.1.1" xref="A3.SS1.p1.9.m9.1.1.cmml"><mi id="A3.SS1.p1.9.m9.1.1.2" xref="A3.SS1.p1.9.m9.1.1.2.cmml">f</mi><mi id="A3.SS1.p1.9.m9.1.1.3" xref="A3.SS1.p1.9.m9.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.9.m9.1b"><apply id="A3.SS1.p1.9.m9.1.1.cmml" xref="A3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.9.m9.1.1.1.cmml" xref="A3.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="A3.SS1.p1.9.m9.1.1.2.cmml" xref="A3.SS1.p1.9.m9.1.1.2">𝑓</ci><ci id="A3.SS1.p1.9.m9.1.1.3.cmml" xref="A3.SS1.p1.9.m9.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.9.m9.1c">f_{r}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.9.m9.1d">italic_f start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> is known as the BRDF that scales or weighs the incoming radiance based the material parameters <math alttext="(k_{d},k_{n},k_{r},k_{m})" class="ltx_Math" display="inline" id="A3.SS1.p1.10.m10.4"><semantics id="A3.SS1.p1.10.m10.4a"><mrow id="A3.SS1.p1.10.m10.4.4.4" xref="A3.SS1.p1.10.m10.4.4.5.cmml"><mo id="A3.SS1.p1.10.m10.4.4.4.5" stretchy="false" xref="A3.SS1.p1.10.m10.4.4.5.cmml">(</mo><msub id="A3.SS1.p1.10.m10.1.1.1.1" xref="A3.SS1.p1.10.m10.1.1.1.1.cmml"><mi id="A3.SS1.p1.10.m10.1.1.1.1.2" xref="A3.SS1.p1.10.m10.1.1.1.1.2.cmml">k</mi><mi id="A3.SS1.p1.10.m10.1.1.1.1.3" xref="A3.SS1.p1.10.m10.1.1.1.1.3.cmml">d</mi></msub><mo id="A3.SS1.p1.10.m10.4.4.4.6" xref="A3.SS1.p1.10.m10.4.4.5.cmml">,</mo><msub id="A3.SS1.p1.10.m10.2.2.2.2" xref="A3.SS1.p1.10.m10.2.2.2.2.cmml"><mi id="A3.SS1.p1.10.m10.2.2.2.2.2" xref="A3.SS1.p1.10.m10.2.2.2.2.2.cmml">k</mi><mi id="A3.SS1.p1.10.m10.2.2.2.2.3" xref="A3.SS1.p1.10.m10.2.2.2.2.3.cmml">n</mi></msub><mo id="A3.SS1.p1.10.m10.4.4.4.7" xref="A3.SS1.p1.10.m10.4.4.5.cmml">,</mo><msub id="A3.SS1.p1.10.m10.3.3.3.3" xref="A3.SS1.p1.10.m10.3.3.3.3.cmml"><mi id="A3.SS1.p1.10.m10.3.3.3.3.2" xref="A3.SS1.p1.10.m10.3.3.3.3.2.cmml">k</mi><mi id="A3.SS1.p1.10.m10.3.3.3.3.3" xref="A3.SS1.p1.10.m10.3.3.3.3.3.cmml">r</mi></msub><mo id="A3.SS1.p1.10.m10.4.4.4.8" xref="A3.SS1.p1.10.m10.4.4.5.cmml">,</mo><msub id="A3.SS1.p1.10.m10.4.4.4.4" xref="A3.SS1.p1.10.m10.4.4.4.4.cmml"><mi id="A3.SS1.p1.10.m10.4.4.4.4.2" xref="A3.SS1.p1.10.m10.4.4.4.4.2.cmml">k</mi><mi id="A3.SS1.p1.10.m10.4.4.4.4.3" xref="A3.SS1.p1.10.m10.4.4.4.4.3.cmml">m</mi></msub><mo id="A3.SS1.p1.10.m10.4.4.4.9" stretchy="false" xref="A3.SS1.p1.10.m10.4.4.5.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.10.m10.4b"><vector id="A3.SS1.p1.10.m10.4.4.5.cmml" xref="A3.SS1.p1.10.m10.4.4.4"><apply id="A3.SS1.p1.10.m10.1.1.1.1.cmml" xref="A3.SS1.p1.10.m10.1.1.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.10.m10.1.1.1.1.1.cmml" xref="A3.SS1.p1.10.m10.1.1.1.1">subscript</csymbol><ci id="A3.SS1.p1.10.m10.1.1.1.1.2.cmml" xref="A3.SS1.p1.10.m10.1.1.1.1.2">𝑘</ci><ci id="A3.SS1.p1.10.m10.1.1.1.1.3.cmml" xref="A3.SS1.p1.10.m10.1.1.1.1.3">𝑑</ci></apply><apply id="A3.SS1.p1.10.m10.2.2.2.2.cmml" xref="A3.SS1.p1.10.m10.2.2.2.2"><csymbol cd="ambiguous" id="A3.SS1.p1.10.m10.2.2.2.2.1.cmml" xref="A3.SS1.p1.10.m10.2.2.2.2">subscript</csymbol><ci id="A3.SS1.p1.10.m10.2.2.2.2.2.cmml" xref="A3.SS1.p1.10.m10.2.2.2.2.2">𝑘</ci><ci id="A3.SS1.p1.10.m10.2.2.2.2.3.cmml" xref="A3.SS1.p1.10.m10.2.2.2.2.3">𝑛</ci></apply><apply id="A3.SS1.p1.10.m10.3.3.3.3.cmml" xref="A3.SS1.p1.10.m10.3.3.3.3"><csymbol cd="ambiguous" id="A3.SS1.p1.10.m10.3.3.3.3.1.cmml" xref="A3.SS1.p1.10.m10.3.3.3.3">subscript</csymbol><ci id="A3.SS1.p1.10.m10.3.3.3.3.2.cmml" xref="A3.SS1.p1.10.m10.3.3.3.3.2">𝑘</ci><ci id="A3.SS1.p1.10.m10.3.3.3.3.3.cmml" xref="A3.SS1.p1.10.m10.3.3.3.3.3">𝑟</ci></apply><apply id="A3.SS1.p1.10.m10.4.4.4.4.cmml" xref="A3.SS1.p1.10.m10.4.4.4.4"><csymbol cd="ambiguous" id="A3.SS1.p1.10.m10.4.4.4.4.1.cmml" xref="A3.SS1.p1.10.m10.4.4.4.4">subscript</csymbol><ci id="A3.SS1.p1.10.m10.4.4.4.4.2.cmml" xref="A3.SS1.p1.10.m10.4.4.4.4.2">𝑘</ci><ci id="A3.SS1.p1.10.m10.4.4.4.4.3.cmml" xref="A3.SS1.p1.10.m10.4.4.4.4.3">𝑚</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.10.m10.4c">(k_{d},k_{n},k_{r},k_{m})</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.10.m10.4d">( italic_k start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT )</annotation></semantics></math> of the garment surface. By aggregating the rendered pixel colors along the direction <math alttext="\omega_{o}" class="ltx_Math" display="inline" id="A3.SS1.p1.11.m11.1"><semantics id="A3.SS1.p1.11.m11.1a"><msub id="A3.SS1.p1.11.m11.1.1" xref="A3.SS1.p1.11.m11.1.1.cmml"><mi id="A3.SS1.p1.11.m11.1.1.2" xref="A3.SS1.p1.11.m11.1.1.2.cmml">ω</mi><mi id="A3.SS1.p1.11.m11.1.1.3" xref="A3.SS1.p1.11.m11.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.11.m11.1b"><apply id="A3.SS1.p1.11.m11.1.1.cmml" xref="A3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="A3.SS1.p1.11.m11.1.1.1.cmml" xref="A3.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="A3.SS1.p1.11.m11.1.1.2.cmml" xref="A3.SS1.p1.11.m11.1.1.2">𝜔</ci><ci id="A3.SS1.p1.11.m11.1.1.3.cmml" xref="A3.SS1.p1.11.m11.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.11.m11.1c">\omega_{o}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.11.m11.1d">italic_ω start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> (i.e., camera pose), we are able to obtain the rendered image of the input patch (image <math alttext="x" class="ltx_Math" display="inline" id="A3.SS1.p1.12.m12.1"><semantics id="A3.SS1.p1.12.m12.1a"><mi id="A3.SS1.p1.12.m12.1.1" xref="A3.SS1.p1.12.m12.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.12.m12.1b"><ci id="A3.SS1.p1.12.m12.1.1.cmml" xref="A3.SS1.p1.12.m12.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.12.m12.1c">x</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.12.m12.1d">italic_x</annotation></semantics></math> in Equation  <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S3.E1" title="In 3.1. Problem Statement ‣ 3. Method ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">1</span></a> of the main paper).</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2. </span>Classifier-free guidance for conditional image generation</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.6">We leverage Classifier-Free Guidance (CFG) <cite class="ltx_cite ltx_citemacro_citep">(Ho and Salimans, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib19" title="">2022</a>)</cite> during the training for trading off the quality and diversity of samples generated by our FabricDiffusion model. The implementation of CFG involves jointly training the diffusion model for conditional and unconditional denoising, and combining the two score estimates (the <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="A3.SS2.p1.1.m1.1"><semantics id="A3.SS2.p1.1.m1.1a"><msub id="A3.SS2.p1.1.m1.1.1" xref="A3.SS2.p1.1.m1.1.1.cmml"><mi id="A3.SS2.p1.1.m1.1.1.2" mathvariant="normal" xref="A3.SS2.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="A3.SS2.p1.1.m1.1.1.3" xref="A3.SS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.SS2.p1.1.m1.1b"><apply id="A3.SS2.p1.1.m1.1.1.cmml" xref="A3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A3.SS2.p1.1.m1.1.1.1.cmml" xref="A3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="A3.SS2.p1.1.m1.1.1.2.cmml" xref="A3.SS2.p1.1.m1.1.1.2">ℓ</ci><cn id="A3.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="A3.SS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p1.1.m1.1c">\ell_{2}</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p1.1.m1.1d">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> loss of the noise term in Equation (3) of the main paper) at inference time. Training for unconditional denoising is done by simply setting the conditioning to a fixed null value <math alttext="\mathcal{E}(x)~{}{=}~{}\varnothing" class="ltx_Math" display="inline" id="A3.SS2.p1.2.m2.1"><semantics id="A3.SS2.p1.2.m2.1a"><mrow id="A3.SS2.p1.2.m2.1.2" xref="A3.SS2.p1.2.m2.1.2.cmml"><mrow id="A3.SS2.p1.2.m2.1.2.2" xref="A3.SS2.p1.2.m2.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.SS2.p1.2.m2.1.2.2.2" xref="A3.SS2.p1.2.m2.1.2.2.2.cmml">ℰ</mi><mo id="A3.SS2.p1.2.m2.1.2.2.1" xref="A3.SS2.p1.2.m2.1.2.2.1.cmml">⁢</mo><mrow id="A3.SS2.p1.2.m2.1.2.2.3.2" xref="A3.SS2.p1.2.m2.1.2.2.cmml"><mo id="A3.SS2.p1.2.m2.1.2.2.3.2.1" stretchy="false" xref="A3.SS2.p1.2.m2.1.2.2.cmml">(</mo><mi id="A3.SS2.p1.2.m2.1.1" xref="A3.SS2.p1.2.m2.1.1.cmml">x</mi><mo id="A3.SS2.p1.2.m2.1.2.2.3.2.2" rspace="0.330em" stretchy="false" xref="A3.SS2.p1.2.m2.1.2.2.cmml">)</mo></mrow></mrow><mo id="A3.SS2.p1.2.m2.1.2.1" rspace="0.608em" xref="A3.SS2.p1.2.m2.1.2.1.cmml">=</mo><mi id="A3.SS2.p1.2.m2.1.2.3" mathvariant="normal" xref="A3.SS2.p1.2.m2.1.2.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.SS2.p1.2.m2.1b"><apply id="A3.SS2.p1.2.m2.1.2.cmml" xref="A3.SS2.p1.2.m2.1.2"><eq id="A3.SS2.p1.2.m2.1.2.1.cmml" xref="A3.SS2.p1.2.m2.1.2.1"></eq><apply id="A3.SS2.p1.2.m2.1.2.2.cmml" xref="A3.SS2.p1.2.m2.1.2.2"><times id="A3.SS2.p1.2.m2.1.2.2.1.cmml" xref="A3.SS2.p1.2.m2.1.2.2.1"></times><ci id="A3.SS2.p1.2.m2.1.2.2.2.cmml" xref="A3.SS2.p1.2.m2.1.2.2.2">ℰ</ci><ci id="A3.SS2.p1.2.m2.1.1.cmml" xref="A3.SS2.p1.2.m2.1.1">𝑥</ci></apply><emptyset id="A3.SS2.p1.2.m2.1.2.3.cmml" xref="A3.SS2.p1.2.m2.1.2.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p1.2.m2.1c">\mathcal{E}(x)~{}{=}~{}\varnothing</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p1.2.m2.1d">caligraphic_E ( italic_x ) = ∅</annotation></semantics></math> at some frequency during training. At inference time, with a guidance scale <math alttext="s\geq 1" class="ltx_Math" display="inline" id="A3.SS2.p1.3.m3.1"><semantics id="A3.SS2.p1.3.m3.1a"><mrow id="A3.SS2.p1.3.m3.1.1" xref="A3.SS2.p1.3.m3.1.1.cmml"><mi id="A3.SS2.p1.3.m3.1.1.2" xref="A3.SS2.p1.3.m3.1.1.2.cmml">s</mi><mo id="A3.SS2.p1.3.m3.1.1.1" xref="A3.SS2.p1.3.m3.1.1.1.cmml">≥</mo><mn id="A3.SS2.p1.3.m3.1.1.3" xref="A3.SS2.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS2.p1.3.m3.1b"><apply id="A3.SS2.p1.3.m3.1.1.cmml" xref="A3.SS2.p1.3.m3.1.1"><geq id="A3.SS2.p1.3.m3.1.1.1.cmml" xref="A3.SS2.p1.3.m3.1.1.1"></geq><ci id="A3.SS2.p1.3.m3.1.1.2.cmml" xref="A3.SS2.p1.3.m3.1.1.2">𝑠</ci><cn id="A3.SS2.p1.3.m3.1.1.3.cmml" type="integer" xref="A3.SS2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p1.3.m3.1c">s\geq 1</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p1.3.m3.1d">italic_s ≥ 1</annotation></semantics></math>, the modified score estimate <math alttext="\tilde{e_{\theta}}(x_{t},\mathcal{E}(x))" class="ltx_Math" display="inline" id="A3.SS2.p1.4.m4.3"><semantics id="A3.SS2.p1.4.m4.3a"><mrow id="A3.SS2.p1.4.m4.3.3" xref="A3.SS2.p1.4.m4.3.3.cmml"><mover accent="true" id="A3.SS2.p1.4.m4.3.3.4" xref="A3.SS2.p1.4.m4.3.3.4.cmml"><msub id="A3.SS2.p1.4.m4.3.3.4.2" xref="A3.SS2.p1.4.m4.3.3.4.2.cmml"><mi id="A3.SS2.p1.4.m4.3.3.4.2.2" xref="A3.SS2.p1.4.m4.3.3.4.2.2.cmml">e</mi><mi id="A3.SS2.p1.4.m4.3.3.4.2.3" xref="A3.SS2.p1.4.m4.3.3.4.2.3.cmml">θ</mi></msub><mo id="A3.SS2.p1.4.m4.3.3.4.1" xref="A3.SS2.p1.4.m4.3.3.4.1.cmml">~</mo></mover><mo id="A3.SS2.p1.4.m4.3.3.3" xref="A3.SS2.p1.4.m4.3.3.3.cmml">⁢</mo><mrow id="A3.SS2.p1.4.m4.3.3.2.2" xref="A3.SS2.p1.4.m4.3.3.2.3.cmml"><mo id="A3.SS2.p1.4.m4.3.3.2.2.3" stretchy="false" xref="A3.SS2.p1.4.m4.3.3.2.3.cmml">(</mo><msub id="A3.SS2.p1.4.m4.2.2.1.1.1" xref="A3.SS2.p1.4.m4.2.2.1.1.1.cmml"><mi id="A3.SS2.p1.4.m4.2.2.1.1.1.2" xref="A3.SS2.p1.4.m4.2.2.1.1.1.2.cmml">x</mi><mi id="A3.SS2.p1.4.m4.2.2.1.1.1.3" xref="A3.SS2.p1.4.m4.2.2.1.1.1.3.cmml">t</mi></msub><mo id="A3.SS2.p1.4.m4.3.3.2.2.4" xref="A3.SS2.p1.4.m4.3.3.2.3.cmml">,</mo><mrow id="A3.SS2.p1.4.m4.3.3.2.2.2" xref="A3.SS2.p1.4.m4.3.3.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.SS2.p1.4.m4.3.3.2.2.2.2" xref="A3.SS2.p1.4.m4.3.3.2.2.2.2.cmml">ℰ</mi><mo id="A3.SS2.p1.4.m4.3.3.2.2.2.1" xref="A3.SS2.p1.4.m4.3.3.2.2.2.1.cmml">⁢</mo><mrow id="A3.SS2.p1.4.m4.3.3.2.2.2.3.2" xref="A3.SS2.p1.4.m4.3.3.2.2.2.cmml"><mo id="A3.SS2.p1.4.m4.3.3.2.2.2.3.2.1" stretchy="false" xref="A3.SS2.p1.4.m4.3.3.2.2.2.cmml">(</mo><mi id="A3.SS2.p1.4.m4.1.1" xref="A3.SS2.p1.4.m4.1.1.cmml">x</mi><mo id="A3.SS2.p1.4.m4.3.3.2.2.2.3.2.2" stretchy="false" xref="A3.SS2.p1.4.m4.3.3.2.2.2.cmml">)</mo></mrow></mrow><mo id="A3.SS2.p1.4.m4.3.3.2.2.5" stretchy="false" xref="A3.SS2.p1.4.m4.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS2.p1.4.m4.3b"><apply id="A3.SS2.p1.4.m4.3.3.cmml" xref="A3.SS2.p1.4.m4.3.3"><times id="A3.SS2.p1.4.m4.3.3.3.cmml" xref="A3.SS2.p1.4.m4.3.3.3"></times><apply id="A3.SS2.p1.4.m4.3.3.4.cmml" xref="A3.SS2.p1.4.m4.3.3.4"><ci id="A3.SS2.p1.4.m4.3.3.4.1.cmml" xref="A3.SS2.p1.4.m4.3.3.4.1">~</ci><apply id="A3.SS2.p1.4.m4.3.3.4.2.cmml" xref="A3.SS2.p1.4.m4.3.3.4.2"><csymbol cd="ambiguous" id="A3.SS2.p1.4.m4.3.3.4.2.1.cmml" xref="A3.SS2.p1.4.m4.3.3.4.2">subscript</csymbol><ci id="A3.SS2.p1.4.m4.3.3.4.2.2.cmml" xref="A3.SS2.p1.4.m4.3.3.4.2.2">𝑒</ci><ci id="A3.SS2.p1.4.m4.3.3.4.2.3.cmml" xref="A3.SS2.p1.4.m4.3.3.4.2.3">𝜃</ci></apply></apply><interval closure="open" id="A3.SS2.p1.4.m4.3.3.2.3.cmml" xref="A3.SS2.p1.4.m4.3.3.2.2"><apply id="A3.SS2.p1.4.m4.2.2.1.1.1.cmml" xref="A3.SS2.p1.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="A3.SS2.p1.4.m4.2.2.1.1.1.1.cmml" xref="A3.SS2.p1.4.m4.2.2.1.1.1">subscript</csymbol><ci id="A3.SS2.p1.4.m4.2.2.1.1.1.2.cmml" xref="A3.SS2.p1.4.m4.2.2.1.1.1.2">𝑥</ci><ci id="A3.SS2.p1.4.m4.2.2.1.1.1.3.cmml" xref="A3.SS2.p1.4.m4.2.2.1.1.1.3">𝑡</ci></apply><apply id="A3.SS2.p1.4.m4.3.3.2.2.2.cmml" xref="A3.SS2.p1.4.m4.3.3.2.2.2"><times id="A3.SS2.p1.4.m4.3.3.2.2.2.1.cmml" xref="A3.SS2.p1.4.m4.3.3.2.2.2.1"></times><ci id="A3.SS2.p1.4.m4.3.3.2.2.2.2.cmml" xref="A3.SS2.p1.4.m4.3.3.2.2.2.2">ℰ</ci><ci id="A3.SS2.p1.4.m4.1.1.cmml" xref="A3.SS2.p1.4.m4.1.1">𝑥</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p1.4.m4.3c">\tilde{e_{\theta}}(x_{t},\mathcal{E}(x))</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p1.4.m4.3d">over~ start_ARG italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT end_ARG ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_E ( italic_x ) )</annotation></semantics></math> is extrapolated in the direction toward the conditional <math alttext="e_{\theta}(x_{t},\mathcal{E}(x))" class="ltx_Math" display="inline" id="A3.SS2.p1.5.m5.3"><semantics id="A3.SS2.p1.5.m5.3a"><mrow id="A3.SS2.p1.5.m5.3.3" xref="A3.SS2.p1.5.m5.3.3.cmml"><msub id="A3.SS2.p1.5.m5.3.3.4" xref="A3.SS2.p1.5.m5.3.3.4.cmml"><mi id="A3.SS2.p1.5.m5.3.3.4.2" xref="A3.SS2.p1.5.m5.3.3.4.2.cmml">e</mi><mi id="A3.SS2.p1.5.m5.3.3.4.3" xref="A3.SS2.p1.5.m5.3.3.4.3.cmml">θ</mi></msub><mo id="A3.SS2.p1.5.m5.3.3.3" xref="A3.SS2.p1.5.m5.3.3.3.cmml">⁢</mo><mrow id="A3.SS2.p1.5.m5.3.3.2.2" xref="A3.SS2.p1.5.m5.3.3.2.3.cmml"><mo id="A3.SS2.p1.5.m5.3.3.2.2.3" stretchy="false" xref="A3.SS2.p1.5.m5.3.3.2.3.cmml">(</mo><msub id="A3.SS2.p1.5.m5.2.2.1.1.1" xref="A3.SS2.p1.5.m5.2.2.1.1.1.cmml"><mi id="A3.SS2.p1.5.m5.2.2.1.1.1.2" xref="A3.SS2.p1.5.m5.2.2.1.1.1.2.cmml">x</mi><mi id="A3.SS2.p1.5.m5.2.2.1.1.1.3" xref="A3.SS2.p1.5.m5.2.2.1.1.1.3.cmml">t</mi></msub><mo id="A3.SS2.p1.5.m5.3.3.2.2.4" xref="A3.SS2.p1.5.m5.3.3.2.3.cmml">,</mo><mrow id="A3.SS2.p1.5.m5.3.3.2.2.2" xref="A3.SS2.p1.5.m5.3.3.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.SS2.p1.5.m5.3.3.2.2.2.2" xref="A3.SS2.p1.5.m5.3.3.2.2.2.2.cmml">ℰ</mi><mo id="A3.SS2.p1.5.m5.3.3.2.2.2.1" xref="A3.SS2.p1.5.m5.3.3.2.2.2.1.cmml">⁢</mo><mrow id="A3.SS2.p1.5.m5.3.3.2.2.2.3.2" xref="A3.SS2.p1.5.m5.3.3.2.2.2.cmml"><mo id="A3.SS2.p1.5.m5.3.3.2.2.2.3.2.1" stretchy="false" xref="A3.SS2.p1.5.m5.3.3.2.2.2.cmml">(</mo><mi id="A3.SS2.p1.5.m5.1.1" xref="A3.SS2.p1.5.m5.1.1.cmml">x</mi><mo id="A3.SS2.p1.5.m5.3.3.2.2.2.3.2.2" stretchy="false" xref="A3.SS2.p1.5.m5.3.3.2.2.2.cmml">)</mo></mrow></mrow><mo id="A3.SS2.p1.5.m5.3.3.2.2.5" stretchy="false" xref="A3.SS2.p1.5.m5.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS2.p1.5.m5.3b"><apply id="A3.SS2.p1.5.m5.3.3.cmml" xref="A3.SS2.p1.5.m5.3.3"><times id="A3.SS2.p1.5.m5.3.3.3.cmml" xref="A3.SS2.p1.5.m5.3.3.3"></times><apply id="A3.SS2.p1.5.m5.3.3.4.cmml" xref="A3.SS2.p1.5.m5.3.3.4"><csymbol cd="ambiguous" id="A3.SS2.p1.5.m5.3.3.4.1.cmml" xref="A3.SS2.p1.5.m5.3.3.4">subscript</csymbol><ci id="A3.SS2.p1.5.m5.3.3.4.2.cmml" xref="A3.SS2.p1.5.m5.3.3.4.2">𝑒</ci><ci id="A3.SS2.p1.5.m5.3.3.4.3.cmml" xref="A3.SS2.p1.5.m5.3.3.4.3">𝜃</ci></apply><interval closure="open" id="A3.SS2.p1.5.m5.3.3.2.3.cmml" xref="A3.SS2.p1.5.m5.3.3.2.2"><apply id="A3.SS2.p1.5.m5.2.2.1.1.1.cmml" xref="A3.SS2.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="A3.SS2.p1.5.m5.2.2.1.1.1.1.cmml" xref="A3.SS2.p1.5.m5.2.2.1.1.1">subscript</csymbol><ci id="A3.SS2.p1.5.m5.2.2.1.1.1.2.cmml" xref="A3.SS2.p1.5.m5.2.2.1.1.1.2">𝑥</ci><ci id="A3.SS2.p1.5.m5.2.2.1.1.1.3.cmml" xref="A3.SS2.p1.5.m5.2.2.1.1.1.3">𝑡</ci></apply><apply id="A3.SS2.p1.5.m5.3.3.2.2.2.cmml" xref="A3.SS2.p1.5.m5.3.3.2.2.2"><times id="A3.SS2.p1.5.m5.3.3.2.2.2.1.cmml" xref="A3.SS2.p1.5.m5.3.3.2.2.2.1"></times><ci id="A3.SS2.p1.5.m5.3.3.2.2.2.2.cmml" xref="A3.SS2.p1.5.m5.3.3.2.2.2.2">ℰ</ci><ci id="A3.SS2.p1.5.m5.1.1.cmml" xref="A3.SS2.p1.5.m5.1.1">𝑥</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p1.5.m5.3c">e_{\theta}(x_{t},\mathcal{E}(x))</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p1.5.m5.3d">italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_E ( italic_x ) )</annotation></semantics></math> and away from the unconditional <math alttext="e_{\theta}(x_{t},\varnothing)" class="ltx_Math" display="inline" id="A3.SS2.p1.6.m6.2"><semantics id="A3.SS2.p1.6.m6.2a"><mrow id="A3.SS2.p1.6.m6.2.2" xref="A3.SS2.p1.6.m6.2.2.cmml"><msub id="A3.SS2.p1.6.m6.2.2.3" xref="A3.SS2.p1.6.m6.2.2.3.cmml"><mi id="A3.SS2.p1.6.m6.2.2.3.2" xref="A3.SS2.p1.6.m6.2.2.3.2.cmml">e</mi><mi id="A3.SS2.p1.6.m6.2.2.3.3" xref="A3.SS2.p1.6.m6.2.2.3.3.cmml">θ</mi></msub><mo id="A3.SS2.p1.6.m6.2.2.2" xref="A3.SS2.p1.6.m6.2.2.2.cmml">⁢</mo><mrow id="A3.SS2.p1.6.m6.2.2.1.1" xref="A3.SS2.p1.6.m6.2.2.1.2.cmml"><mo id="A3.SS2.p1.6.m6.2.2.1.1.2" stretchy="false" xref="A3.SS2.p1.6.m6.2.2.1.2.cmml">(</mo><msub id="A3.SS2.p1.6.m6.2.2.1.1.1" xref="A3.SS2.p1.6.m6.2.2.1.1.1.cmml"><mi id="A3.SS2.p1.6.m6.2.2.1.1.1.2" xref="A3.SS2.p1.6.m6.2.2.1.1.1.2.cmml">x</mi><mi id="A3.SS2.p1.6.m6.2.2.1.1.1.3" xref="A3.SS2.p1.6.m6.2.2.1.1.1.3.cmml">t</mi></msub><mo id="A3.SS2.p1.6.m6.2.2.1.1.3" xref="A3.SS2.p1.6.m6.2.2.1.2.cmml">,</mo><mi id="A3.SS2.p1.6.m6.1.1" mathvariant="normal" xref="A3.SS2.p1.6.m6.1.1.cmml">∅</mi><mo id="A3.SS2.p1.6.m6.2.2.1.1.4" stretchy="false" xref="A3.SS2.p1.6.m6.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS2.p1.6.m6.2b"><apply id="A3.SS2.p1.6.m6.2.2.cmml" xref="A3.SS2.p1.6.m6.2.2"><times id="A3.SS2.p1.6.m6.2.2.2.cmml" xref="A3.SS2.p1.6.m6.2.2.2"></times><apply id="A3.SS2.p1.6.m6.2.2.3.cmml" xref="A3.SS2.p1.6.m6.2.2.3"><csymbol cd="ambiguous" id="A3.SS2.p1.6.m6.2.2.3.1.cmml" xref="A3.SS2.p1.6.m6.2.2.3">subscript</csymbol><ci id="A3.SS2.p1.6.m6.2.2.3.2.cmml" xref="A3.SS2.p1.6.m6.2.2.3.2">𝑒</ci><ci id="A3.SS2.p1.6.m6.2.2.3.3.cmml" xref="A3.SS2.p1.6.m6.2.2.3.3">𝜃</ci></apply><interval closure="open" id="A3.SS2.p1.6.m6.2.2.1.2.cmml" xref="A3.SS2.p1.6.m6.2.2.1.1"><apply id="A3.SS2.p1.6.m6.2.2.1.1.1.cmml" xref="A3.SS2.p1.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="A3.SS2.p1.6.m6.2.2.1.1.1.1.cmml" xref="A3.SS2.p1.6.m6.2.2.1.1.1">subscript</csymbol><ci id="A3.SS2.p1.6.m6.2.2.1.1.1.2.cmml" xref="A3.SS2.p1.6.m6.2.2.1.1.1.2">𝑥</ci><ci id="A3.SS2.p1.6.m6.2.2.1.1.1.3.cmml" xref="A3.SS2.p1.6.m6.2.2.1.1.1.3">𝑡</ci></apply><emptyset id="A3.SS2.p1.6.m6.1.1.cmml" xref="A3.SS2.p1.6.m6.1.1"></emptyset></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p1.6.m6.2c">e_{\theta}(x_{t},\varnothing)</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p1.6.m6.2d">italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , ∅ )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="A3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{e_{\theta}}(x_{t},\mathcal{E}(x))=e_{\theta}(x_{t},\varnothing)+s\cdot(%
e_{\theta}(x_{t},\mathcal{E}(x))-e_{\theta}(x_{t},\varnothing))." class="ltx_Math" display="block" id="A3.E7.m1.5"><semantics id="A3.E7.m1.5a"><mrow id="A3.E7.m1.5.5.1" xref="A3.E7.m1.5.5.1.1.cmml"><mrow id="A3.E7.m1.5.5.1.1" xref="A3.E7.m1.5.5.1.1.cmml"><mrow id="A3.E7.m1.5.5.1.1.2" xref="A3.E7.m1.5.5.1.1.2.cmml"><mover accent="true" id="A3.E7.m1.5.5.1.1.2.4" xref="A3.E7.m1.5.5.1.1.2.4.cmml"><msub id="A3.E7.m1.5.5.1.1.2.4.2" xref="A3.E7.m1.5.5.1.1.2.4.2.cmml"><mi id="A3.E7.m1.5.5.1.1.2.4.2.2" xref="A3.E7.m1.5.5.1.1.2.4.2.2.cmml">e</mi><mi id="A3.E7.m1.5.5.1.1.2.4.2.3" xref="A3.E7.m1.5.5.1.1.2.4.2.3.cmml">θ</mi></msub><mo id="A3.E7.m1.5.5.1.1.2.4.1" xref="A3.E7.m1.5.5.1.1.2.4.1.cmml">~</mo></mover><mo id="A3.E7.m1.5.5.1.1.2.3" xref="A3.E7.m1.5.5.1.1.2.3.cmml">⁢</mo><mrow id="A3.E7.m1.5.5.1.1.2.2.2" xref="A3.E7.m1.5.5.1.1.2.2.3.cmml"><mo id="A3.E7.m1.5.5.1.1.2.2.2.3" stretchy="false" xref="A3.E7.m1.5.5.1.1.2.2.3.cmml">(</mo><msub id="A3.E7.m1.5.5.1.1.1.1.1.1" xref="A3.E7.m1.5.5.1.1.1.1.1.1.cmml"><mi id="A3.E7.m1.5.5.1.1.1.1.1.1.2" xref="A3.E7.m1.5.5.1.1.1.1.1.1.2.cmml">x</mi><mi id="A3.E7.m1.5.5.1.1.1.1.1.1.3" xref="A3.E7.m1.5.5.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="A3.E7.m1.5.5.1.1.2.2.2.4" xref="A3.E7.m1.5.5.1.1.2.2.3.cmml">,</mo><mrow id="A3.E7.m1.5.5.1.1.2.2.2.2" xref="A3.E7.m1.5.5.1.1.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.E7.m1.5.5.1.1.2.2.2.2.2" xref="A3.E7.m1.5.5.1.1.2.2.2.2.2.cmml">ℰ</mi><mo id="A3.E7.m1.5.5.1.1.2.2.2.2.1" xref="A3.E7.m1.5.5.1.1.2.2.2.2.1.cmml">⁢</mo><mrow id="A3.E7.m1.5.5.1.1.2.2.2.2.3.2" xref="A3.E7.m1.5.5.1.1.2.2.2.2.cmml"><mo id="A3.E7.m1.5.5.1.1.2.2.2.2.3.2.1" stretchy="false" xref="A3.E7.m1.5.5.1.1.2.2.2.2.cmml">(</mo><mi id="A3.E7.m1.1.1" xref="A3.E7.m1.1.1.cmml">x</mi><mo id="A3.E7.m1.5.5.1.1.2.2.2.2.3.2.2" stretchy="false" xref="A3.E7.m1.5.5.1.1.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="A3.E7.m1.5.5.1.1.2.2.2.5" stretchy="false" xref="A3.E7.m1.5.5.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="A3.E7.m1.5.5.1.1.5" xref="A3.E7.m1.5.5.1.1.5.cmml">=</mo><mrow id="A3.E7.m1.5.5.1.1.4" xref="A3.E7.m1.5.5.1.1.4.cmml"><mrow id="A3.E7.m1.5.5.1.1.3.1" xref="A3.E7.m1.5.5.1.1.3.1.cmml"><msub id="A3.E7.m1.5.5.1.1.3.1.3" xref="A3.E7.m1.5.5.1.1.3.1.3.cmml"><mi id="A3.E7.m1.5.5.1.1.3.1.3.2" xref="A3.E7.m1.5.5.1.1.3.1.3.2.cmml">e</mi><mi id="A3.E7.m1.5.5.1.1.3.1.3.3" xref="A3.E7.m1.5.5.1.1.3.1.3.3.cmml">θ</mi></msub><mo id="A3.E7.m1.5.5.1.1.3.1.2" xref="A3.E7.m1.5.5.1.1.3.1.2.cmml">⁢</mo><mrow id="A3.E7.m1.5.5.1.1.3.1.1.1" xref="A3.E7.m1.5.5.1.1.3.1.1.2.cmml"><mo id="A3.E7.m1.5.5.1.1.3.1.1.1.2" stretchy="false" xref="A3.E7.m1.5.5.1.1.3.1.1.2.cmml">(</mo><msub id="A3.E7.m1.5.5.1.1.3.1.1.1.1" xref="A3.E7.m1.5.5.1.1.3.1.1.1.1.cmml"><mi id="A3.E7.m1.5.5.1.1.3.1.1.1.1.2" xref="A3.E7.m1.5.5.1.1.3.1.1.1.1.2.cmml">x</mi><mi id="A3.E7.m1.5.5.1.1.3.1.1.1.1.3" xref="A3.E7.m1.5.5.1.1.3.1.1.1.1.3.cmml">t</mi></msub><mo id="A3.E7.m1.5.5.1.1.3.1.1.1.3" xref="A3.E7.m1.5.5.1.1.3.1.1.2.cmml">,</mo><mi id="A3.E7.m1.2.2" mathvariant="normal" xref="A3.E7.m1.2.2.cmml">∅</mi><mo id="A3.E7.m1.5.5.1.1.3.1.1.1.4" stretchy="false" xref="A3.E7.m1.5.5.1.1.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="A3.E7.m1.5.5.1.1.4.3" xref="A3.E7.m1.5.5.1.1.4.3.cmml">+</mo><mrow id="A3.E7.m1.5.5.1.1.4.2" xref="A3.E7.m1.5.5.1.1.4.2.cmml"><mi id="A3.E7.m1.5.5.1.1.4.2.3" xref="A3.E7.m1.5.5.1.1.4.2.3.cmml">s</mi><mo id="A3.E7.m1.5.5.1.1.4.2.2" lspace="0.222em" rspace="0.222em" xref="A3.E7.m1.5.5.1.1.4.2.2.cmml">⋅</mo><mrow id="A3.E7.m1.5.5.1.1.4.2.1.1" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.cmml"><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.2" stretchy="false" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.cmml">(</mo><mrow id="A3.E7.m1.5.5.1.1.4.2.1.1.1" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.cmml"><mrow id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.cmml"><msub id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.cmml"><mi id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.2.cmml">e</mi><mi id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.3" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.3.cmml">θ</mi></msub><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.3" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.3.cmml">⁢</mo><mrow id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.3.cmml"><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.3" stretchy="false" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.3.cmml">(</mo><msub id="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.cmml"><mi id="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.3" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.4" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.3.cmml">,</mo><mrow id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.2.cmml">ℰ</mi><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.1" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.1.cmml">⁢</mo><mrow id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.3.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.cmml"><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.3.2.1" stretchy="false" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.cmml">(</mo><mi id="A3.E7.m1.3.3" xref="A3.E7.m1.3.3.cmml">x</mi><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.3.2.2" stretchy="false" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.5" stretchy="false" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.4" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.4.cmml">−</mo><mrow id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.cmml"><msub id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.cmml"><mi id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.2.cmml">e</mi><mi id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.3" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.3.cmml">θ</mi></msub><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.2.cmml">⁢</mo><mrow id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.2.cmml"><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.2" stretchy="false" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.2.cmml">(</mo><msub id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.cmml"><mi id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.2" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.2.cmml">x</mi><mi id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.3" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.3.cmml">t</mi></msub><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.3" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.2.cmml">,</mo><mi id="A3.E7.m1.4.4" mathvariant="normal" xref="A3.E7.m1.4.4.cmml">∅</mi><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.4" stretchy="false" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="A3.E7.m1.5.5.1.1.4.2.1.1.3" stretchy="false" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="A3.E7.m1.5.5.1.2" lspace="0em" xref="A3.E7.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.E7.m1.5b"><apply id="A3.E7.m1.5.5.1.1.cmml" xref="A3.E7.m1.5.5.1"><eq id="A3.E7.m1.5.5.1.1.5.cmml" xref="A3.E7.m1.5.5.1.1.5"></eq><apply id="A3.E7.m1.5.5.1.1.2.cmml" xref="A3.E7.m1.5.5.1.1.2"><times id="A3.E7.m1.5.5.1.1.2.3.cmml" xref="A3.E7.m1.5.5.1.1.2.3"></times><apply id="A3.E7.m1.5.5.1.1.2.4.cmml" xref="A3.E7.m1.5.5.1.1.2.4"><ci id="A3.E7.m1.5.5.1.1.2.4.1.cmml" xref="A3.E7.m1.5.5.1.1.2.4.1">~</ci><apply id="A3.E7.m1.5.5.1.1.2.4.2.cmml" xref="A3.E7.m1.5.5.1.1.2.4.2"><csymbol cd="ambiguous" id="A3.E7.m1.5.5.1.1.2.4.2.1.cmml" xref="A3.E7.m1.5.5.1.1.2.4.2">subscript</csymbol><ci id="A3.E7.m1.5.5.1.1.2.4.2.2.cmml" xref="A3.E7.m1.5.5.1.1.2.4.2.2">𝑒</ci><ci id="A3.E7.m1.5.5.1.1.2.4.2.3.cmml" xref="A3.E7.m1.5.5.1.1.2.4.2.3">𝜃</ci></apply></apply><interval closure="open" id="A3.E7.m1.5.5.1.1.2.2.3.cmml" xref="A3.E7.m1.5.5.1.1.2.2.2"><apply id="A3.E7.m1.5.5.1.1.1.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A3.E7.m1.5.5.1.1.1.1.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.1.1.1.1">subscript</csymbol><ci id="A3.E7.m1.5.5.1.1.1.1.1.1.2.cmml" xref="A3.E7.m1.5.5.1.1.1.1.1.1.2">𝑥</ci><ci id="A3.E7.m1.5.5.1.1.1.1.1.1.3.cmml" xref="A3.E7.m1.5.5.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="A3.E7.m1.5.5.1.1.2.2.2.2.cmml" xref="A3.E7.m1.5.5.1.1.2.2.2.2"><times id="A3.E7.m1.5.5.1.1.2.2.2.2.1.cmml" xref="A3.E7.m1.5.5.1.1.2.2.2.2.1"></times><ci id="A3.E7.m1.5.5.1.1.2.2.2.2.2.cmml" xref="A3.E7.m1.5.5.1.1.2.2.2.2.2">ℰ</ci><ci id="A3.E7.m1.1.1.cmml" xref="A3.E7.m1.1.1">𝑥</ci></apply></interval></apply><apply id="A3.E7.m1.5.5.1.1.4.cmml" xref="A3.E7.m1.5.5.1.1.4"><plus id="A3.E7.m1.5.5.1.1.4.3.cmml" xref="A3.E7.m1.5.5.1.1.4.3"></plus><apply id="A3.E7.m1.5.5.1.1.3.1.cmml" xref="A3.E7.m1.5.5.1.1.3.1"><times id="A3.E7.m1.5.5.1.1.3.1.2.cmml" xref="A3.E7.m1.5.5.1.1.3.1.2"></times><apply id="A3.E7.m1.5.5.1.1.3.1.3.cmml" xref="A3.E7.m1.5.5.1.1.3.1.3"><csymbol cd="ambiguous" id="A3.E7.m1.5.5.1.1.3.1.3.1.cmml" xref="A3.E7.m1.5.5.1.1.3.1.3">subscript</csymbol><ci id="A3.E7.m1.5.5.1.1.3.1.3.2.cmml" xref="A3.E7.m1.5.5.1.1.3.1.3.2">𝑒</ci><ci id="A3.E7.m1.5.5.1.1.3.1.3.3.cmml" xref="A3.E7.m1.5.5.1.1.3.1.3.3">𝜃</ci></apply><interval closure="open" id="A3.E7.m1.5.5.1.1.3.1.1.2.cmml" xref="A3.E7.m1.5.5.1.1.3.1.1.1"><apply id="A3.E7.m1.5.5.1.1.3.1.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.3.1.1.1.1"><csymbol cd="ambiguous" id="A3.E7.m1.5.5.1.1.3.1.1.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.3.1.1.1.1">subscript</csymbol><ci id="A3.E7.m1.5.5.1.1.3.1.1.1.1.2.cmml" xref="A3.E7.m1.5.5.1.1.3.1.1.1.1.2">𝑥</ci><ci id="A3.E7.m1.5.5.1.1.3.1.1.1.1.3.cmml" xref="A3.E7.m1.5.5.1.1.3.1.1.1.1.3">𝑡</ci></apply><emptyset id="A3.E7.m1.2.2.cmml" xref="A3.E7.m1.2.2"></emptyset></interval></apply><apply id="A3.E7.m1.5.5.1.1.4.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2"><ci id="A3.E7.m1.5.5.1.1.4.2.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.2">⋅</ci><ci id="A3.E7.m1.5.5.1.1.4.2.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.3">𝑠</ci><apply id="A3.E7.m1.5.5.1.1.4.2.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1"><minus id="A3.E7.m1.5.5.1.1.4.2.1.1.1.4.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.4"></minus><apply id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2"><times id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.3"></times><apply id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4"><csymbol cd="ambiguous" id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.1.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4">subscript</csymbol><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.2">𝑒</ci><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.4.3">𝜃</ci></apply><interval closure="open" id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2"><apply id="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.2">𝑥</ci><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2"><times id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.1.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.1"></times><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.2.2.2.2.2">ℰ</ci><ci id="A3.E7.m1.3.3.cmml" xref="A3.E7.m1.3.3">𝑥</ci></apply></interval></apply><apply id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3"><times id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.2"></times><apply id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3"><csymbol cd="ambiguous" id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.1.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3">subscript</csymbol><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.2">𝑒</ci><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.3.3">𝜃</ci></apply><interval closure="open" id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1"><apply id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1"><csymbol cd="ambiguous" id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.1.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1">subscript</csymbol><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.2.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.2">𝑥</ci><ci id="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.3.cmml" xref="A3.E7.m1.5.5.1.1.4.2.1.1.1.3.1.1.1.3">𝑡</ci></apply><emptyset id="A3.E7.m1.4.4.cmml" xref="A3.E7.m1.4.4"></emptyset></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.E7.m1.5c">\tilde{e_{\theta}}(x_{t},\mathcal{E}(x))=e_{\theta}(x_{t},\varnothing)+s\cdot(%
e_{\theta}(x_{t},\mathcal{E}(x))-e_{\theta}(x_{t},\varnothing)).</annotation><annotation encoding="application/x-llamapun" id="A3.E7.m1.5d">over~ start_ARG italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT end_ARG ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_E ( italic_x ) ) = italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , ∅ ) + italic_s ⋅ ( italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_E ( italic_x ) ) - italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , ∅ ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A3.SS2.p1.7">CFG enhances the visual quality of generated texture maps and ensures that the sampled images more accurately correspond to the input texture in terms of color, pattern, and scale.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3. </span>Strategy for determining tiling scales</h3>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">After extracting PBR material maps from an image exemplar, we tile them in the garment UV space for realistic rendering. The key question is how to determine the scale for tiling? We investigate two specific strategies: (1) Proportion-aware tiling. We use image segmentation to calculate the proportion of the captured region relative to the segmented clothing, maintaining the same ratio when tiling the generated texture onto the sewing pattern. (2) User-guided tiling. We emphasize that an end-to-end automatic tilling method may not be optimal, as user involvement is often necessary to resolve ambiguities and provide flexibility in fashion industries.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4. </span>Implementation details</h3>
<div class="ltx_para" id="A3.SS4.p1">
<p class="ltx_p" id="A3.SS4.p1.2">We use pre-trained Stable Diffusion v1.5 as the backbone of the normalized texture map generation and finetune it on our texture and print datasets, respectively. Both the input and output scales are set as 256<math alttext="\times" class="ltx_Math" display="inline" id="A3.SS4.p1.1.m1.1"><semantics id="A3.SS4.p1.1.m1.1a"><mo id="A3.SS4.p1.1.m1.1.1" xref="A3.SS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.SS4.p1.1.m1.1b"><times id="A3.SS4.p1.1.m1.1.1.cmml" xref="A3.SS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p1.1.m1.1d">×</annotation></semantics></math>256px. We use a batch size of 512 and a learning rate of <math alttext="5\times 10^{-5}" class="ltx_Math" display="inline" id="A3.SS4.p1.2.m2.1"><semantics id="A3.SS4.p1.2.m2.1a"><mrow id="A3.SS4.p1.2.m2.1.1" xref="A3.SS4.p1.2.m2.1.1.cmml"><mn id="A3.SS4.p1.2.m2.1.1.2" xref="A3.SS4.p1.2.m2.1.1.2.cmml">5</mn><mo id="A3.SS4.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A3.SS4.p1.2.m2.1.1.1.cmml">×</mo><msup id="A3.SS4.p1.2.m2.1.1.3" xref="A3.SS4.p1.2.m2.1.1.3.cmml"><mn id="A3.SS4.p1.2.m2.1.1.3.2" xref="A3.SS4.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="A3.SS4.p1.2.m2.1.1.3.3" xref="A3.SS4.p1.2.m2.1.1.3.3.cmml"><mo id="A3.SS4.p1.2.m2.1.1.3.3a" xref="A3.SS4.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="A3.SS4.p1.2.m2.1.1.3.3.2" xref="A3.SS4.p1.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A3.SS4.p1.2.m2.1b"><apply id="A3.SS4.p1.2.m2.1.1.cmml" xref="A3.SS4.p1.2.m2.1.1"><times id="A3.SS4.p1.2.m2.1.1.1.cmml" xref="A3.SS4.p1.2.m2.1.1.1"></times><cn id="A3.SS4.p1.2.m2.1.1.2.cmml" type="integer" xref="A3.SS4.p1.2.m2.1.1.2">5</cn><apply id="A3.SS4.p1.2.m2.1.1.3.cmml" xref="A3.SS4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="A3.SS4.p1.2.m2.1.1.3.1.cmml" xref="A3.SS4.p1.2.m2.1.1.3">superscript</csymbol><cn id="A3.SS4.p1.2.m2.1.1.3.2.cmml" type="integer" xref="A3.SS4.p1.2.m2.1.1.3.2">10</cn><apply id="A3.SS4.p1.2.m2.1.1.3.3.cmml" xref="A3.SS4.p1.2.m2.1.1.3.3"><minus id="A3.SS4.p1.2.m2.1.1.3.3.1.cmml" xref="A3.SS4.p1.2.m2.1.1.3.3"></minus><cn id="A3.SS4.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="A3.SS4.p1.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS4.p1.2.m2.1c">5\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A3.SS4.p1.2.m2.1d">5 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>. It takes roughly 2 days (20k iterations) to train on four NVIDIA A6000 GPUs. For PBR materials estimation, we fine-tuned the pre-trained MatFusion model for roughly 1 hour with our 3.8k BRDF materials training data.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Supplementary Material D D </span>Additional Results</h2>
<figure class="ltx_table" id="A4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table S1. </span>Quantitative comparison on texture images extraction from 3D garments. Results are evaluated on synthetic testing data. The ground-truths are normalized texture images that are flat and with a unified lighting condition. Our method outperforms Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> across different evaluation metrics.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T1.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T1.5.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A4.T1.5.5.6" style="padding-left:3.5pt;padding-right:3.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T1.1.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="A4.T1.1.1.1.1" style="font-size:90%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="A4.T1.1.1.1.m1.1"><semantics id="A4.T1.1.1.1.m1.1a"><mo id="A4.T1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="A4.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A4.T1.1.1.1.m1.1b"><ci id="A4.T1.1.1.1.m1.1.1.cmml" xref="A4.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A4.T1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T1.2.2.2" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="A4.T1.2.2.2.1" style="font-size:90%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T1.2.2.2.m1.1"><semantics id="A4.T1.2.2.2.m1.1a"><mo id="A4.T1.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="A4.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T1.2.2.2.m1.1b"><ci id="A4.T1.2.2.2.m1.1.1.cmml" xref="A4.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T1.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A4.T1.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T1.3.3.3" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="A4.T1.3.3.3.1" style="font-size:90%;">MS-SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T1.3.3.3.m1.1"><semantics id="A4.T1.3.3.3.m1.1a"><mo id="A4.T1.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="A4.T1.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T1.3.3.3.m1.1b"><ci id="A4.T1.3.3.3.m1.1.1.cmml" xref="A4.T1.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T1.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A4.T1.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T1.4.4.4" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="A4.T1.4.4.4.1" style="font-size:90%;">DIST</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="A4.T1.4.4.4.m1.1"><semantics id="A4.T1.4.4.4.m1.1a"><mo id="A4.T1.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="A4.T1.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A4.T1.4.4.4.m1.1b"><ci id="A4.T1.4.4.4.m1.1.1.cmml" xref="A4.T1.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T1.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A4.T1.4.4.4.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T1.5.5.5" style="padding-left:3.5pt;padding-right:3.5pt;">
<span class="ltx_text" id="A4.T1.5.5.5.1" style="font-size:90%;">CLIP-s</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T1.5.5.5.m1.1"><semantics id="A4.T1.5.5.5.m1.1a"><mo id="A4.T1.5.5.5.m1.1.1" mathsize="90%" stretchy="false" xref="A4.T1.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T1.5.5.5.m1.1b"><ci id="A4.T1.5.5.5.m1.1.1.cmml" xref="A4.T1.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T1.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A4.T1.5.5.5.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T1.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A4.T1.5.6.1.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A4.T1.5.6.1.1.1" style="font-size:90%;">Material Palette</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T1.5.6.1.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A4.T1.5.6.1.2.1" style="font-size:90%;">0.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T1.5.6.1.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A4.T1.5.6.1.3.1" style="font-size:90%;">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T1.5.6.1.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A4.T1.5.6.1.4.1" style="font-size:90%;">0.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T1.5.6.1.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A4.T1.5.6.1.5.1" style="font-size:90%;">0.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T1.5.6.1.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A4.T1.5.6.1.6.1" style="font-size:90%;">0.89</span></td>
</tr>
<tr class="ltx_tr" id="A4.T1.5.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A4.T1.5.7.2.1" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text" id="A4.T1.5.7.2.1.1" style="font-size:90%;">FabricDiffusion (ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T1.5.7.2.2" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T1.5.7.2.2.1" style="font-size:90%;">0.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T1.5.7.2.3" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T1.5.7.2.3.1" style="font-size:90%;">0.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T1.5.7.2.4" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T1.5.7.2.4.1" style="font-size:90%;">0.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T1.5.7.2.5" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T1.5.7.2.5.1" style="font-size:90%;">0.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T1.5.7.2.6" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T1.5.7.2.6.1" style="font-size:90%;">0.91</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="A4.F1">
<p class="ltx_p ltx_align_center" id="A4.F1.1"><span class="ltx_text" id="A4.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="608" id="A4.F1.1.1.g1" src="x12.png" width="829"/></span></p>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure S1. </span><span class="ltx_text" id="A4.F1.3.1" style="font-size:90%;">Texture transfer on synthetic data. Given the input image of the 3D garment and a captured patch, our method generates a normalized texture map that is flat and tileable, along with the corresponding PBR materials. The PBR materials maps can be applied to the target 3D garment with different geometry for reliable rendering. Our model is capable of removing shadows (1st row), disentangling distortions (1st &amp; 2nd row), and capturing physical properties (3rd row) from the input fabric texture. Note that, both the input 3D garment meshes and textures in this figure were not used for model training. See Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.T1" title="Table 1 ‣ 4.2.3. Image-to-garment texture transfer. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">1</span></a> of the main paper for qualitative results.
</span></figcaption>
</figure>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1. </span>Additional results on textures extraction</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.1">Generating a normalized texture image plays a crucial intermediate step to ensure reliable texture transfer. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.F7" title="Figure 7 ‣ 4.2.3. Image-to-garment texture transfer. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">7</span></a> (in the main paper) shows some cases of the generated normalized textures. In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A4.T1" title="Table S1 ‣ Supplementary Material D D Additional Results ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">S1</span></a>, we provide a quantitative analysis using synthetic data, for which we have ground-truth textures, and compare our method with state-of-the-are methods. As we observe, our method consistently outperforms Material Palette <cite class="ltx_cite ltx_citemacro_citep">(Lopes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#bib.bib28" title="">2024</a>)</cite> across various evaluation metrics. As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S2" title="2. Related Work ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">2</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.SS2" title="4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">4.2</span></a> of the main paper, personalization-based methods struggle at capturing fine-grained texture details, or disentangling the effects of distortion.</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2. </span>Texture transfer on synthetic data</h3>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1">We also validate our method using synthetic data and show the qualitative results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#A4.F1" title="Figure S1 ‣ Supplementary Material D D Additional Results ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">S1</span></a>. We test on textured garments with ground-truth BRDF materials, enabling controlled evaluation of geometric distortions and illumination variations. Our method reliably generates normalized textures and PBR materials. As our focus is on clothing fabrics with minimal metallic properties, we omit metallic map results for simplicity in the following experiments. Quantitative results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01801v1#S4.T1" title="Table 1 ‣ 4.2.3. Image-to-garment texture transfer. ‣ 4.2. Experimental Results ‣ 4. Experiments ‣ FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images"><span class="ltx_text ltx_ref_tag">1</span></a> of the main paper.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 18:58:18 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
