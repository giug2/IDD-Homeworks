<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.14377] Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction</title><meta property="og:description" content="Data-centric AI approach aims to enhance the model performance without modifying the model and has been shown to impact model performance positively. While recent attention has been given to data-centric AI based on sy…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.14377">

<!--Generated on Wed Feb 28 22:53:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chanjun Park
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Seonmin Koo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Seolhwa Lee
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jaehyung Seo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sugyeong Eo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hyeonseok Moon
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Heuiseok Lim
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Data-centric AI approach aims to enhance the model performance without modifying the model and has been shown to impact model performance positively. While recent attention has been given to data-centric AI based on synthetic data, due to its potential for performance improvement, data-centric AI has long been exclusively validated using real-world data and publicly available benchmark datasets. In respect of this, data-centric AI still highly depends on real-world data, and the verification of models using synthetic data has not yet been thoroughly carried out. Given the challenges above, we ask the question: <span id="id1.id1.1" class="ltx_text ltx_font_italic">“Does data quality control (noise injection and balanced data), a data-centric AI methodology acclaimed to have a positive impact, exhibit the same positive impact in models trained solely with synthetic data?”</span> To address this question, we conducted comparative analyses between models trained on synthetic and real-world data based on grammatical error correction (GEC) task. Our experimental results reveal that the data quality control method has a positive impact on models trained with real-world data, as previously reported in existing studies, while a <span id="id1.id1.2" class="ltx_text ltx_font_italic">negative</span> impact is observed in models trained solely on synthetic data.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Data-centric AI research has been actively conducted in natural language processing (NLP) to improve model performance without the need for significant cost and model modification. Several data-centric AI methods have been developed to achieve this goal, such as data management <cite class="ltx_cite ltx_citemacro_citep">(Choi &amp; Park, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, data filtering <cite class="ltx_cite ltx_citemacro_citep">(Koehn et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, noise injection (perturbation) <cite class="ltx_cite ltx_citemacro_citep">(Sarp et al., <a href="#bib.bib27" title="" class="ltx_ref">2021</a>; Partovyan et al., <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>, and data augmentation <cite class="ltx_cite ltx_citemacro_citep">(Shorten &amp; Khoshgoftaar, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>. Among these methods, the use of synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Nikolenko, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite> has gained increasing interest with the development of the large language models (LLMs), such as GPT3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>, ChatGPT<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://chat.openai.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://chat.openai.com/</a></span></span></span>, and LaMDA <cite class="ltx_cite ltx_citemacro_citep">(Thoppilan et al., <a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>,
These LLMs have demonstrated the potential for generating high-quality synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Wang et al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> and the possibility of replacing the need for human-annotated data with synthetic data.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:336.0pt;height:260.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.7pt,14.5pt) scale(0.9,0.9) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Correct sentence</th>
<th id="S1.T1.1.1.1.1.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S1.T1.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">바비큐 그릴도 이용할 수 있나요?</td>
</tr>
<tr id="S1.T1.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(Can I also use a barbecue grill?)</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.2.1" class="ltx_tr">
<th id="S1.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Example of separation error</th>
<td id="S1.T1.1.1.2.1.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">
<table id="S1.T1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.2.1.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">ㅂㅏㅂㅣ큐 그릴도 이용할 수 있나요?</td>
</tr>
<tr id="S1.T1.1.1.2.1.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(Can I also use a b ar b e cue grill?)</td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T1.1.1.3.2" class="ltx_tr">
<th id="S1.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Example of vowel alteration error</th>
<td id="S1.T1.1.1.3.2.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S1.T1.1.1.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.3.2.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">바비큐 그릴됴 이용할 수 있나요?</td>
</tr>
<tr id="S1.T1.1.1.3.2.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(Can I alsa use a barbecue grill?)</td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T1.1.1.4.3" class="ltx_tr">
<th id="S1.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Example of pronunciation error</th>
<td id="S1.T1.1.1.4.3.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S1.T1.1.1.4.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.4.3.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.4.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">바비큐 그릴도 이용할 수 잇나요?</td>
</tr>
<tr id="S1.T1.1.1.4.3.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.4.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(Canai also use a barbecue grill?)</td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T1.1.1.5.4" class="ltx_tr">
<th id="S1.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Example of punctuation attachment errors</th>
<td id="S1.T1.1.1.5.4.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S1.T1.1.1.5.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.5.4.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">바비큐 그릴도. 이용할 수 있나요?</td>
</tr>
<tr id="S1.T1.1.1.5.4.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(Can I also. use a barbecue grill?)</td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T1.1.1.6.5" class="ltx_tr">
<th id="S1.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Example of loanword error</th>
<td id="S1.T1.1.1.6.5.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S1.T1.1.1.6.5.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.6.5.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.6.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">비비큐 그릴도 이용할 수 있나요?</td>
</tr>
<tr id="S1.T1.1.1.6.5.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.6.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(Can I also use a bebecue grill?)</td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T1.1.1.7.6" class="ltx_tr">
<th id="S1.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Example of neologism error</th>
<td id="S1.T1.1.1.7.6.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t">
<table id="S1.T1.1.1.7.6.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.7.6.2.1.1" class="ltx_tr">
<td id="S1.T1.1.1.7.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">바비큐 그릴도 O.l용할 수 있나요?</td>
</tr>
<tr id="S1.T1.1.1.7.6.2.1.2" class="ltx_tr">
<td id="S1.T1.1.1.7.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(Can 1 also use a barbecue grill?)</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Example of noise injection according to noise type.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, we raise questions on the validity of prior research in data-centric AI, which has been shown to impact model performance positively. Existing data-centric AI studies have basically been conducted based on human-annotated data or publicly open data. Still, validation of models using only synthetic data has not been sufficiently conducted <cite class="ltx_cite ltx_citemacro_citep">(Polyzotis &amp; Zaharia, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>; Mazumder et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In the data-centric AI research, studies have focused on efficient methods of generating synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib19" title="" class="ltx_ref">2021a</a>)</cite> and human-like data <cite class="ltx_cite ltx_citemacro_citep">(Moon et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>. However, there has been limited validation of model performance improvement through data quality control using fully synthetic data. This paper analyzes whether a model trained only on synthetic data rather than human-annotated data can still demonstrate a positive impact in a data-centric approach.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To do this, we employ the grammatical error correction (GEC) task as it is one of the closely related tasks to the real-world. We conduct experiments on the GEC task using two models: (1) a BackTranscription (BTS) <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib20" title="" class="ltx_ref">2021b</a>)</cite>-based GEC model, which is a synthetic data generation method proposed in recent speech recognition post-processing, and (2) a GEC model for learning from real-world data <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>. To analyze the impact of data quality control on performance, we apply methods such as noise injection <cite class="ltx_cite ltx_citemacro_citep">(Ivanovs et al., <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> and balanced data <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib21" title="" class="ltx_ref">2022a</a>; Chen et al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> to both models and compare their results.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Moreover, there have been studies on the effectiveness of synthetic data based on self-supervised learning <cite class="ltx_cite ltx_citemacro_citep">(Ng et al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>; Ruiter et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Gan et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>. Also, through the scaling law that examines the performance of models based on the size of the dataset, it has been demonstrated to be highly effective to use data generated by models that increase the amount of data geometrically concerning the size of the dataset <cite class="ltx_cite ltx_citemacro_citep">(Jaiswal et al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Raghunathan, <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Kaplan et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>. In respect of this, we aim to revisit the synthetic data research.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Design for Revisiting the impact of Synthetic Data</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We raise the following question to revisit the impact of synthetic data—<span id="S2.p1.1.1" class="ltx_text ltx_font_italic">“Does the data quality control manifest the same positive impact in models trained only on synthetic data?”</span>. To validate this question, we design the experiments from two different perspectives. We investigate the following questions through comparative analyses between the performance of the models trained on synthetic and real-world data.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2306.14377/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="177" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Experimental results of noise injection. (a) is the result of inserting noise into real-world data. (b) is the result of inserting noise into synthetic data. Note that the x-axis indicates the strength of noise injection.</figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">How does the strength of noise injection impact the model performance?</span></p>
</div>
</li>
</ul>
<p id="S2.p2.1" class="ltx_p">To address this question, we propose employing the noise injection method, more specifically perturbation, which is representative of the data quality control method, to assess the performance between synthetic and real-world data.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">For the perturbation of synthetic data BTS, six types of noise—i.e. separation, vowel alteration, pronunciation, punctuation attachment, loanword, and neologism errors—are applied to the source sentence. The separation error refers to the case where the consonant and vowels of a character are separated. The vowel alteration error is where the vowel of a character is replaced with a different vowel. The pronunciation error indicates a case where a character is altered by pronunciation. The punctuation attachment error refers to a case where punctuation is attached in an unnecessary position within a sentence. The loanword conversion error deals with cases where part of a character is converted into English. The neologism error refers to a case where the character is altered using grammar not included in the existing grammatical system. See detailed examples in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Regarding perturbation for real-world data Lang-8, the same correct sentence pair is inserted into the source and target sentences. Due to the characteristics of the GEC task, the source sentence already contains errors, so it is considered noise to insert clean data into the source sentence, which should have noise that is opposed to the characteristics of the data.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Subsequently, the performance comparison between the baseline model trained on data without noise injection and the noised model trained on data with perturbation is conducted to examine the impact of noise injection on the model concerning synthetic and real-world data. Specifically, we conduct the experiment according to the strength of noise injection ranging from 0.1 to 1.0. Noise is inserted based on the ratio of noise set at the word level for each sentence. For example, if the noise ratio is 1.0, the noise will occur in all words within each sentence.</p>
</div>
<div id="S2.p6" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">How does the ratio of noised and cleaned text batches impact the model performance?</span></p>
</div>
</li>
</ul>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">The aim is to obtain answers to the question by comparing the performance of synthetic data and real-world data by applying the balanced data method. The balanced data method is a method of training by intentionally giving appropriate ratios to noise and clean data. That is, when forming batches during training, data with different features is composed based on the pre-set ratios, and the training method is performed based on these ratios <cite class="ltx_cite ltx_citemacro_citet">Park et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022a</a>)</cite>. The performance experiment of the balance between clean and noisy data is conducted with five different ratios of synthetic and real-world data—5:5, 4:6, 3:7, 2:8, and 1:9. The comparison of the performance between the baseline model without any operations and the model trained with balanced data method is then carried out to analyze the impact of the ratio of noise and clean on the performance of the only synthetic and real-world data-based models.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Settings</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Real-world &amp; Synthetic Data</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">We use the Lang-8 dataset<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://lang-8.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lang-8.com/</a></span></span></span> as our real-world data, a fully human-annotated corpus. The data settings are consistent with those used by <cite class="ltx_cite ltx_citemacro_citet">Park et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p">We generate synthesized datasets fitting for the GEC task from the above datasets (AI-HUB, TED) using BTS <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib20" title="" class="ltx_ref">2021b</a>)</cite>. BTS combines text-to-speech (TTS) technology and speech-to-text (STT) technology to generate GEC task synthesized data for speech recognition post-processor. Although BTS cannot represent synthetic data, BTS is a simple and efficient methodology for generating synthetic data. Thus, we use it for experiments. As raw data for generating BTS-based synthetic data, we use AI-HUB <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib22" title="" class="ltx_ref">2022b</a>)</cite>, which are representative Korean data platforms, and TED Korean dataset<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.ted.com/talks?language=ko" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ted.com/talks?language=ko</a></span></span></span>, the same as existing BTS work. In addition, since the existing BTS research was also conducted in Korean, this experiment also performs based on Korean for a fair evaluation.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p3.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ Real-world &amp; Synthetic Data ‣ 3 Experimental Settings ‣ Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the specific data statistics used in the experiment. We use 92,000 sentences from AI-HUB data and 119,883 sentences from TED’s Korean Transcript data to generate BTS-based synthetic data followed by <cite class="ltx_cite ltx_citemacro_citet">Park et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>’s method. These data are used as raw data for BTS, transformation into speech using TTS, and outputting the converted result as text using STT.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Train</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Test</span></th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Type of data</span></th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Lang-8</span></th>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.2.1" class="ltx_text" style="font-size:90%;">1,075,513</span></th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.3.1" class="ltx_text" style="font-size:90%;">631</span></th>
<th id="S3.T2.1.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.4.1" class="ltx_text" style="font-size:90%;">real-world</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.3.1" class="ltx_tr">
<td id="S3.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.1.1.1" class="ltx_text" style="font-size:90%;">AI-HUB</span></td>
<td id="S3.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.1.2.1" class="ltx_text" style="font-size:90%;">92,000</span></td>
<td id="S3.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.1.3.1" class="ltx_text" style="font-size:90%;">3,000</span></td>
<td id="S3.T2.1.3.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S3.T2.1.3.1.4.1" class="ltx_text" style="font-size:90%;">synthetic(BTS based)</span></td>
</tr>
<tr id="S3.T2.1.4.2" class="ltx_tr">
<td id="S3.T2.1.4.2.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.4.2.1.1" class="ltx_text" style="font-size:90%;">TED</span></td>
<td id="S3.T2.1.4.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.4.2.2.1" class="ltx_text" style="font-size:90%;">119,883</span></td>
<td id="S3.T2.1.4.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.4.2.3.1" class="ltx_text" style="font-size:90%;">3,000</span></td>
<td id="S3.T2.1.4.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S3.T2.1.4.2.4.1" class="ltx_text" style="font-size:90%;">synthetic(BTS based)</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Statistics on the number of sentences according to real-world and synthetic data.</figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Implementation Details</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">We train the models using the vanilla Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite> and set the same for hyperparameters. Fairseq <cite class="ltx_cite ltx_citemacro_citep">(Ott et al., <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite> is used for the implementation. For subword tokenization, we utilize SentencePiece  <cite class="ltx_cite ltx_citemacro_citep">(Kudo &amp; Richardson, <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> and set the vocabulary size to 50,000. We evaluate the Lang-8-based real-world model and the BTS-based synthetic data model as GLEU <cite class="ltx_cite ltx_citemacro_citep">(Napoles et al., <a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite> and BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a href="#bib.bib17" title="" class="ltx_ref">2002</a>)</cite>, respectively. These are the same metrics as previous GEC and BTS papers.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.14377/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="177" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Experimental results of balanced data. (a) is the balanced data result between noise and clean data in real-world data. (b) is the balanced data result between noise and clean data in synthetic data. Note that the x-axis indicates the ratio as (clean:noise).</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Results of Question 1: Noise Injection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Design for Revisiting the impact of Synthetic Data ‣ Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results of applying noise injection (perturbation) methods. Baseline for the real-world data (a) is 55.60. The performance tends to improve when injecting noise. Mainly, the 0.4 noise ratio result obtains a substantial gain of +2.44 to 58.04. Meanwhile, (b) indicates the experimental results of synthetic data on AI-HUB and TED datasets.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We denote the probability of noise being injected into a token in a sentence as the noise injection ratio. The baselines of AI-HUB and TED report 65.69 and 56.14, respectively, and models learned with the data subject to noise injection show performance degradation in most cases. A marginal performance gain is recorded when the noise ratio of the TED dataset is 0.2; rather, the overall performance decreased except for this case. <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">The results demonstrate that data quality control through noise injection, which is known to impact performance in many studies positively, has <span id="S4.SS1.p2.1.1.1" class="ltx_text ltx_font_bold">a negative impact</span> when training a model using only synthetic data.</span></p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">These results starkly contrast with the experimental results using models trained entirely of real-world data, highlighting our conclusion of the <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">negative impact</span> of the noise injection on the synthetic data. Namely, <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">we recommend applying synthetic data after extensively verifying whether the data-centric AI methods are effective in a synthetic data-only setting.</span></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results of Question 2: Balanced Data</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The experimental results of applying balanced data methods are described in Figure <a href="#S3.F2" title="Figure 2 ‣ Implementation Details ‣ 3 Experimental Settings ‣ Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. As mentioned, we combine clean and noise data with a five ratio. The noise injection ratio on the noisy data is 1.0, the highest noise for Section <a href="#S4.SS1" title="4.1 Results of Question 1: Noise Injection ‣ 4 Experimental Results ‣ Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">(a) is the experimental results for real-world data. Contrary to the synthetic data, all cases perform better than the baseline. The performance on the ratio of 1:9 is 58.84, showing a gain of 3.24 points from the baseline. As previously demonstrated, we confirm that leveraging data quality control techniques to real data positively impacts the model.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">(b) is the result of the model training with synthetic data by intentionally giving ratios to clean and noise. As a result, the model learned with synthetic data performs worse than the baseline in all ratios. In particular, as the ratio of noise data increases, performance tends to deteriorate. Experimental results show <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">a negative effect overall</span>, and <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_italic">we analyze that data quality control is less effective in model training in an environment consisting only of synthetic data.</span></p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">We conclude that data quality control positively affects real-world data-based models but does not always guarantee a positive effect in an environment consisting entirely of synthetic data. This implies that real-world and synthetic data have distinctly contrasting characteristics and should be treated differently. <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_italic">We argue that beyond effective synthetic data generation, which is the focus of the data-centric AI, data quality control methods should be inspected to ensure that models leveraging synthetic data produce sufficient performance.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we address the research question of <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">“Is the data quality control, a data-centric AI methodology known to have a positive effect, still observed to have a positive impact when the models are trained only using synthetic data?”</span> by performing experiments and evaluating the results. Our experiments reveal that while the conventional data-centric approach positively impacted real-world data, models trained solely on synthetic data showed a negative impact. This demonstrates that data-centric methodologies do not necessarily guarantee positive effects, dependent on the characteristics of the data. Based on the results, it was found that sufficient evaluation of data-centric methods in synthetic data environments is needed. The experimental results cannot be generalized since the experiment was limited to the GEC task and not all data-centric AI methodologies were tested. However, a clearly defined research question was addressed, and insightful results were obtained through a structured comparison. In future work, we aim to further analyze the characteristics of synthetic data environments by implementing data-centric approaches other than noise injection and balanced data.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was supported by the MSIT(Ministry of Science and ICT), Korea, under the ITRC(Information Technology Research Center) support program(IITP-2023-2018-0-01405) supervised by the IITP(Institute for Information &amp; Communications Technology Planning &amp; Evaluation), and This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT)(No. 2022R1A5A7026673).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Chen, L., Wan, S., and Dou, L.

</span>
<span class="ltx_bibblock">Improving diagnostic performance of high-voltage circuit breakers on
imbalanced data using an oversampling method.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Power Delivery</em>, 37(4):2704–2716, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Chen, M., Papangelis, A., Tao, C., Kim, S., Rosenbaum, A., Liu, Y., Yu, Z., and
Hakkani-Tur, D.

</span>
<span class="ltx_bibblock">Places: Prompting language models for social conversation synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference of the European Chapter of
the Association for Computational Linguistics</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi &amp; Park (2023)</span>
<span class="ltx_bibblock">
Choi, E. and Park, C.

</span>
<span class="ltx_bibblock">Dmops: Data management operation and recipes.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.01228</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. (2021)</span>
<span class="ltx_bibblock">
Gan, Z., Xu, H., and Zan, H.

</span>
<span class="ltx_bibblock">Self-supervised curriculum learning for spelling error correction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pp.  3487–3494, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivanovs et al. (2021)</span>
<span class="ltx_bibblock">
Ivanovs, M., Kadikis, R., and Ozols, K.

</span>
<span class="ltx_bibblock">Perturbation-based methods for explaining deep neural networks: A
survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition Letters</em>, 150:228–234, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaiswal et al. (2020)</span>
<span class="ltx_bibblock">
Jaiswal, A., Babu, A. R., Zadeh, M. Z., Banerjee, D., and Makedon, F.

</span>
<span class="ltx_bibblock">A survey on contrastive self-supervised learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Technologies</em>, 9(1):2, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. (2020)</span>
<span class="ltx_bibblock">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R.,
Gray, S., Radford, A., Wu, J., and Amodei, D.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et al. (2020)</span>
<span class="ltx_bibblock">
Koehn, P., Chaudhary, V., El-Kishky, A., Goyal, N., Chen, P.-J., and
Guzmán, F.

</span>
<span class="ltx_bibblock">Findings of the wmt 2020 shared task on parallel corpus filtering and
alignment.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth Conference on Machine Translation</em>,
pp.  726–742, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo &amp; Richardson (2018)</span>
<span class="ltx_bibblock">
Kudo, T. and Richardson, J.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.06226</em>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mazumder et al. (2022)</span>
<span class="ltx_bibblock">
Mazumder, M., Banbury, C., Yao, X., Karlaš, B., Rojas, W. G., Diamos, S.,
Diamos, G., He, L., Kiela, D., Jurado, D., et al.

</span>
<span class="ltx_bibblock">Dataperf: Benchmarks for data-centric ai development.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.10062</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. (2022)</span>
<span class="ltx_bibblock">
Moon, H., Park, C., Seo, J., Eo, S., and Lim, H.

</span>
<span class="ltx_bibblock">An automatic post editing with efficient and simple data generation
method.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 10:21032–21040, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Napoles et al. (2015)</span>
<span class="ltx_bibblock">
Napoles, C., Sakaguchi, K., Post, M., and Tetreault, J.

</span>
<span class="ltx_bibblock">Ground truth for grammatical error correction metrics.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 2: Short Papers)</em>, pp.  588–593, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ng et al. (2020)</span>
<span class="ltx_bibblock">
Ng, N., Cho, K., and Ghassemi, M.

</span>
<span class="ltx_bibblock">SSMBA: Self-supervised manifold based data augmentation for
improving out-of-domain robustness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pp.  1268–1283, Online, November
2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-main.97</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2020.emnlp-main.97" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2020.emnlp-main.97</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolenko (2019)</span>
<span class="ltx_bibblock">
Nikolenko, S. I.

</span>
<span class="ltx_bibblock">Synthetic data for deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.11512</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2019)</span>
<span class="ltx_bibblock">
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
Auli, M.

</span>
<span class="ltx_bibblock">fairseq: A fast, extensible toolkit for sequence modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.01038</em>, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association
for Computational Linguistics</em>, pp.  311–318, 2002.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2020)</span>
<span class="ltx_bibblock">
Park, C., Yang, Y., Lee, C., and Lim, H.

</span>
<span class="ltx_bibblock">Comparison of the evaluation metrics for neural grammatical error
correction with overcorrection.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 8:106264–106272, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2021a)</span>
<span class="ltx_bibblock">
Park, C., Lee, S., Moon, H., Eo, S., Seo, J., and Lim, H.

</span>
<span class="ltx_bibblock">How should human translation coexist with nmt? efficient tool for
building high quality parallel corpus.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.00191</em>, 2021a.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2021b)</span>
<span class="ltx_bibblock">
Park, C., Seo, J., Lee, S., Lee, C., Moon, H., Eo, S., and Lim, H.-S.

</span>
<span class="ltx_bibblock">Bts: Back transcription for speech-to-text post-processor using
text-to-speech-to-text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th Workshop on Asian Translation
(WAT2021)</em>, pp.  106–116, 2021b.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2022a)</span>
<span class="ltx_bibblock">
Park, C., Go, W.-Y., Eo, S., Moon, H., Lee, S., and Lim, H.

</span>
<span class="ltx_bibblock">Mimicking infants’ bilingual language acquisition for domain
specialized neural machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 10:38684–38693, 2022a.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2022b)</span>
<span class="ltx_bibblock">
Park, C., Shim, M., Eo, S., Lee, S., Seo, J., Moon, H., and Lim, H.

</span>
<span class="ltx_bibblock">Empirical analysis of parallel corpora and in-depth analysis using
liwc.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 12(11):5545,
2022b.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Partovyan et al. (2018)</span>
<span class="ltx_bibblock">
Partovyan, A., Nourani, V., and ALAMI, M. T.

</span>
<span class="ltx_bibblock">Noise injection–denoising techniques to improve artificial
intelligence-based rainfall–runoff modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Water Resources Engineering</em>, 11(36):81–94, 2018.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Polyzotis &amp; Zaharia (2021)</span>
<span class="ltx_bibblock">
Polyzotis, N. and Zaharia, M.

</span>
<span class="ltx_bibblock">What can data-centric ai learn from data and ml engineering?

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.06439</em>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raghunathan (2021)</span>
<span class="ltx_bibblock">
Raghunathan, T. E.

</span>
<span class="ltx_bibblock">Synthetic data.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Annual review of statistics and its application</em>, 8:129–140, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruiter et al. (2021)</span>
<span class="ltx_bibblock">
Ruiter, D., Klakow, D., van Genabith, J., and España-Bonet, C.

</span>
<span class="ltx_bibblock">Integrating unsupervised data generation into self-supervised neural
machine translation for low-resource languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Translation Summit XVIII: Research
Track</em>, pp.  76–91, Virtual, August 2021. Association for Machine
Translation in the Americas.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.mtsummit-research.7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.mtsummit-research.7</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarp et al. (2021)</span>
<span class="ltx_bibblock">
Sarp, S., Kuzlu, M., Cali, U., Elma, O., and Guler, O.

</span>
<span class="ltx_bibblock">Analysis of false data injection impact on ai based solar
photovoltaic power generation forecasting.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.09948</em>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shorten &amp; Khoshgoftaar (2019)</span>
<span class="ltx_bibblock">
Shorten, C. and Khoshgoftaar, T. M.

</span>
<span class="ltx_bibblock">A survey on image data augmentation for deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Journal of big data</em>, 6(1):1–48, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et al. (2022)</span>
<span class="ltx_bibblock">
Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08239</em>, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
Kaiser, Ł., and Polosukhin, I.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, Z., Yu, A. W., Firat, O., and Cao, Y.

</span>
<span class="ltx_bibblock">Towards zero-label language learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.09193</em>, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.14376" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.14377" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.14377">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.14377" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.14378" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 22:53:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
