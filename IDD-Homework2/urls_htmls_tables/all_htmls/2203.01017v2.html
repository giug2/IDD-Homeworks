<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2203.01017] TableFormer: Table Structure Understanding with Transformers.</title><meta property="og:description" content="Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph’s, etc, since they enhance their predictive capabilities.…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TableFormer: Table Structure Understanding with Transformers.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TableFormer: Table Structure Understanding with Transformers.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2203.01017">

<!--Generated on Wed Feb 28 09:52:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">TableFormer: Table Structure Understanding with Transformers.</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ahmed Nassar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Nikolaos Livathinos
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Maksym Lysak
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Peter Staar
<br class="ltx_break">IBM Research
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{ahn,nli,mly,taa}@zurich.ibm.com </span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph’s, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF’s directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The occurrence of tables in documents is ubiquitous.
They often summarise quantitative or factual data, which is cumbersome to describe in verbose text but nevertheless extremely valuable. Unfortunately, this compact representation is often not easy to parse by machines. There are many implicit conventions used to obtain a compact table representation. For example, tables often have complex column- and row-headers in order to reduce duplicated cell content. Lines of different shapes and sizes are leveraged to separate content or indicate a tree structure. Additionally, tables can also have empty/missing table-entries or multi-row textual table-entries. Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows a table which presents all these issues.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2203.01017/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="177" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Picture of a table with subtle, complex features such as (1) multi-column headers, (2) cell with multi-row text and (3) cells with no content. Image from PubTabNet evaluation set, filename: ‘PMC2944238_004_02’.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, significant progress has been made with vision based approaches to extract tables in documents. For the sake of completeness, the issue of table extraction from documents is typically decomposed into two separate challenges, i.e. (1) finding the location of the table(s) on a document-page and (2) finding the structure of a given table in the document.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The first problem is called table-location and has been previously addressed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> with state-of-the-art object-detection networks (e.g. YOLO and later on Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>). For all practical purposes, it can be considered as a solved problem, given enough ground-truth data to train on.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The second problem is called table-structure decomposition. The latter is a long standing problem in the community of document understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Contrary to the table-location problem, there are no commonly used approaches that can easily be re-purposed to solve this problem. Lately, a set of new model-architectures has been proposed by the community to address table-structure decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. All these models have some weaknesses (see Sec. <a href="#S2" title="2 Previous work and State of the Art ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The common denominator here is the reliance on textual features and/or the inability to provide the bounding box of each table-cell in the original image.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we want to address these weaknesses and present a robust table-structure decomposition algorithm. The design criteria for our model are the following. First, we want our algorithm to be language agnostic. In this way, we can obtain the structure of any table, irregardless of the language. Second, we want our algorithm to leverage as much data as possible from the original PDF document. For programmatic PDF documents, the text-cells can often be extracted much faster and with higher accuracy compared to OCR methods. Last but not least, we want to have a direct link between the table-cell and its bounding box in the image.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To meet the design criteria listed above, we developed a new model called <span id="S1.p6.1.1" class="ltx_text ltx_font_bold">TableFormer</span> and a synthetically generated table structure dataset called <span id="S1.p6.1.2" class="ltx_text ltx_font_bold">SynthTabNet<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_medium">1</span></span><span id="footnote1.5" class="ltx_text ltx_font_medium">https://github.com/IBM/SynthTabNet</span></span></span></span></span>. In particular, our contributions in this work can be summarised as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">TableFormer</span>, a transformer based model that predicts tables structure and bounding boxes for the table content simultaneously in an end-to-end approach.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Across all benchmark datasets <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">TableFormer</span> significantly outperforms existing state-of-the-art metrics, while being much more efficient in training and inference to existing works.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We present <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthTabNet</span> a synthetically generated dataset, with various appearance styles and complexity.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">An augmented dataset based on PubTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, FinTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and TableBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> with generated ground-truth for reproducibility.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The paper is structured as follows. In Sec. <a href="#S2" title="2 Previous work and State of the Art ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we give a brief overview of the current state-of-the-art. In Sec. <a href="#S3" title="3 Datasets ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we describe the datasets on which we train. In Sec. <a href="#S4" title="4 The TableFormer model ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we introduce the TableFormer model-architecture and describe its results &amp; performance in Sec. <a href="#S5" title="5 Experimental Results ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. As a conclusion, we describe how this new model-architecture can be re-purposed for other tasks in the computer-vision community.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Previous work and State of the Art</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Identifying the structure of a table has been an outstanding problem in the document-parsing community, that motivates many organised public challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The difficulty of the problem can be attributed to a number of factors. First, there is a large variety in the shapes and sizes of tables. Such large variety requires a flexible method. This is especially true for complex column- and row headers, which can be extremely intricate and demanding. A second factor of complexity is the lack of data with regard to table-structure. Until the publication of PubTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, there were no large datasets (i.e. <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="&gt;100" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml"></mi><mo id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><gt id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">&gt;100</annotation></semantics></math>K tables) that provided structure information. This happens primarily due to the fact that tables are notoriously time-consuming to annotate by hand. However, this has definitely changed in recent years with the deliverance of PubTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, FinTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, TableBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> etc.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Before the rising popularity of deep neural networks, the community relied heavily on heuristic and/or statistical methods to do table structure identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Although such methods work well on constrained tables <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, a more data-driven approach can be applied due to the advent of convolutional neural networks (CNNs) and the availability of large datasets.
To the best-of-our knowledge, there are currently two different types of network architecture that are being pursued for state-of-the-art table-structure identification.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.2" class="ltx_p"><span id="S2.p3.2.1" class="ltx_text ltx_font_bold">Image-to-Text networks</span>: In this type of network, one predicts a sequence of tokens starting from an encoded image. Such sequences of tokens can be HTML table tags <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> or LaTeX symbols<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The choice of symbols is ultimately not very important, since one can be transformed into the other. There are however subtle variations in the Image-to-Text networks. The easiest network architectures are “image-encoder <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S2.p3.1.m1.1a"><mo stretchy="false" id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\rightarrow</annotation></semantics></math> text-decoder” (IETD), similar to network architectures that try to provide captions to images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. In these IETD networks, one expects as output the LaTeX/HTML string of the entire table, i.e. the symbols necessary for creating the table with the content of the table. Another approach is the “image-encoder <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S2.p3.2.m2.1a"><mo stretchy="false" id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">\rightarrow</annotation></semantics></math> dual decoder” (IEDD) networks. In these type of networks, one has two consecutive decoders with different purposes. The first decoder is the <span id="S2.p3.2.2" class="ltx_text ltx_font_italic">tag-decoder</span>, i.e. it only produces the HTML/LaTeX tags which construct an empty table. The second <span id="S2.p3.2.3" class="ltx_text ltx_font_italic">content-decoder</span> uses the encoding of the image in combination with the output encoding of each cell-tag (from the <span id="S2.p3.2.4" class="ltx_text ltx_font_italic">tag-decoder</span>) to generate the textual content of each table cell. The network architecture of IEDD is certainly more elaborate, but it has the advantage that one can pre-train the tag-decoder which is constrained to the table-tags.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In practice, both network architectures (IETD and IEDD) require an implicit, custom trained object-character-recognition (OCR) to obtain the content of the table-cells. In the case of IETD, this OCR engine is implicit in the decoder similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. For the IEDD, the OCR is solely embedded in the content-decoder. This reliance on a custom, implicit OCR decoder is of course problematic. OCR is a well known and extremely tough problem, that often needs custom training for each individual language.
However, the limited availability for non-english content in the current datasets, makes it impractical to apply the IETD and IEDD methods on tables with other languages. Additionally, OCR can be completely omitted if the tables originate from programmatic PDF documents with known positions of each cell. The latter was the inspiration for the work of this paper.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Graph Neural networks</span>: Graph Neural networks (GNN’s) take a radically different approach to table-structure extraction. Note that one table cell can constitute out of multiple text-cells. To obtain the table-structure, one creates an initial graph, where each of the text-cells becomes a node in the graph similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Each node is then associated with en embedding vector coming from the encoded image, its coordinates and the encoded text. Furthermore, nodes that represent adjacent text-cells are linked. Graph Convolutional Networks (GCN’s) based methods take the image as an input, but also the position of the text-cells and their content <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The purpose of a GCN is to transform the input graph into a new graph, which replaces the old links with new ones. The new links then represent the table-structure. With this approach, one can avoid the need to build custom OCR decoders. However, the quality of the reconstructed structure is not comparable to the current state-of-the-art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Hybrid Deep Learning-Rule-Based approach</span>: A popular current model for table-structure identification is the use of a hybrid Deep Learning-Rule-Based approach similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. In this approach, one first detects the position of the table-cells with object detection (e.g. YoloVx or Mask-RCNN), then classifies the table into different types (from its images) and finally uses different rule-sets to obtain its table-structure. Currently, this approach achieves state-of-the-art results, but is not an end-to-end deep-learning method. As such, new rules need to be written if different types of tables are encountered.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2203.01017/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="191" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Distribution of the tables across different table dimensions in PubTabNet + FinTabNet datasets</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Datasets</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We rely on large-scale datasets such as PubTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, FinTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and TableBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> datasets to train and evaluate our models. These datasets span over various appearance styles and content. We also introduce our own synthetically generated SynthTabNet dataset to fix an imbalance in the previous datasets.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The PubTabNet dataset contains 509k tables delivered as annotated PNG images. The annotations consist of the table structure represented in HTML format, the tokenized text and its bounding boxes per table cell. Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the appearance style of PubTabNet. Depending on its complexity, a table is characterized as “simple” when it does not contain row spans or column spans, otherwise it is “complex”. The dataset is divided into Train and Val splits (roughly 98% and 2%). The Train split consists of 54% simple and 46% complex tables and the Val split of 51% and 49% respectively. The FinTabNet dataset contains 112k tables delivered as single-page PDF documents with mixed table structures and text content. Similarly to the PubTabNet, the annotations of FinTabNet include the table structure in HTML, the tokenized text and the bounding boxes on a table cell basis. The dataset is divided into Train, Test and Val splits (81%, 9.5%, 9.5%), and each one is almost equally divided into simple and complex tables (Train: 48% simple, 52% complex, Test: 48% simple, 52% complex, Test: 53% simple, 47% complex). Finally the TableBank dataset consists of 145k tables provided as JPEG images. The latter has annotations for the table structure, but only few with bounding boxes of the table cells. The entire dataset consists of simple tables and it is divided into 90% Train, 3% Test and 7% Val splits.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Due to the heterogeneity across the dataset formats, it was necessary to combine all available data into one homogenized dataset before we could train our models for practical purposes. Given the size of PubTabNet, we adopted its annotation format and we extracted and converted all tables as PNG images with a resolution of 72 dpi. Additionally, we have filtered out tables with extreme sizes due to small amount of such tables, and kept only those ones ranging between 1*1 and 20*10 (rows/columns).</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">The availability of the bounding boxes for all table cells is essential to train our models. In order to distinguish between empty and non-empty bounding boxes, we have introduced a binary class in the annotation.
Unfortunately, the original datasets either omit the bounding boxes for whole tables (e.g. TableBank) or they narrow their scope only to non-empty cells. Therefore, it was imperative to introduce a data pre-processing procedure that generates the missing bounding boxes out of the annotation information. This procedure first parses the provided table structure and calculates the dimensions of the most fine-grained grid that covers the table structure. Notice that each table cell may occupy multiple grid squares due to row or column spans. In case of PubTabNet we had to compute missing bounding boxes for 48% of the simple and 69% of the complex tables. Regarding FinTabNet, 68% of the simple and 98% of the complex tables require the generation of bounding boxes.
</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">As it is illustrated in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Previous work and State of the Art ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the table distributions from all datasets are skewed towards simpler structures with fewer number of rows/columns.
Additionally, there is very limited variance in the table styles, which in case of PubTabNet and FinTabNet means one styling format for the majority of the tables.
Similar limitations appear also in the type of table content, which in some cases (e.g. FinTabNet) is restricted to a certain domain. Ultimately, the lack of diversity in the training dataset damages the ability of the models to generalize well on unseen data.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Motivated by those observations we aimed at generating a synthetic table dataset named <span id="S3.p6.1.1" class="ltx_text ltx_font_italic">SynthTabNet</span>. This approach offers control over: 1) the size of the dataset, 2) the table structure, 3) the table style and 4) the type of content. The complexity of the table structure is described by the size of the table header and the table body, as well as the percentage of the table cells covered by row spans and column spans. A set of carefully designed styling templates provides the basis to build a wide range of table appearances. Lastly, the table content is generated out of a curated collection of text corpora. By controlling the size and scope of the synthetic datasets we are able to train and evaluate our models in a variety of different conditions.
For example, we can first generate a highly diverse dataset to train our models and then evaluate their performance on other synthetic datasets which are focused on a specific domain.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">In this regard, we have prepared four synthetic datasets, each one containing 150k examples. The corpora to generate the table text consists of the most frequent terms appearing in PubTabNet and FinTabNet together with randomly generated text. The first two synthetic datasets have been fine-tuned to mimic the appearance of the original datasets but encompass more complicated table structures. The third one adopts a colorful appearance with high contrast and the last one contains tables with sparse content. Lastly, we have combined all synthetic datasets into one big unified synthetic dataset of 600k examples.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">Tab. <a href="#S3.T1" title="Table 1 ‣ 3 Datasets ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the various attributes of the datasets.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Tags</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Bbox</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Size</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Format</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">PubTabNet</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.2.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">509k</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">PNG</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FinTabNet</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.2.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center">112k</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center">PDF</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TableBank</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.4.3.2.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.4.3.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center">145k</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center">JPEG</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Combined-Tabnet(*)</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.5.4.2.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.5.4.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center">400k</td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_center">PNG</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Combined(**)</th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.6.5.2.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.6.5.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center">500k</td>
<td id="S3.T1.1.6.5.5" class="ltx_td ltx_align_center">PNG</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SynthTabNet</th>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.7.6.2.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.7.6.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center">600k</td>
<td id="S3.T1.1.7.6.5" class="ltx_td ltx_align_center">PNG</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Both <span id="S3.T1.4.1" class="ltx_text ltx_font_italic">“Combined-Tabnet”</span> and <span id="S3.T1.5.2" class="ltx_text ltx_font_italic">”Combined-Tabnet”</span> are variations of the following: (*) The Combined-Tabnet dataset is the processed combination of PubTabNet and Fintabnet. (**) The combined dataset is the processed combination of PubTabNet, Fintabnet and TableBank.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2203.01017/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="353" height="93" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> <span id="S3.F3.2.1" class="ltx_text ltx_font_bold">TableFormer</span> takes in an image of the PDF and creates bounding box and HTML structure predictions that are synchronized. The bounding boxes grabs the content from the PDF and inserts it in the structure.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span> The TableFormer model</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Given the image of a table, TableFormer is able to predict:
1) a sequence of tokens that represent the structure of a table, and
2) a bounding box coupled to a subset of those tokens.
The conversion of an image into a sequence of tokens is a well-known task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. While attention is often used as an implicit method to associate each token of the sequence with a position in the original image,
an explicit association between the individual table-cells and the image bounding boxes is also required.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span> Model architecture.</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We now describe in detail the proposed method, which is composed of three main components, see Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Model architecture. ‣ 4 The TableFormer model ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Our <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">CNN Backbone Network</span> encodes the input as a feature vector of predefined length.
The input feature vector of the encoded image is passed to the <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">Structure Decoder</span> to produce a sequence of HTML tags that represent the structure of the table.
With each prediction of an HTML standard data cell (‘<span id="S4.SS1.p1.1.3" class="ltx_text ltx_lstlisting">&lt;<span id="S4.SS1.p1.1.3.1" class="ltx_text ltx_lst_identifier">td</span>&gt;</span>’) the hidden state of that cell is passed to the Cell BBox Decoder.
As for spanning cells, such as row or column span, the tag is broken down to ‘<span id="S4.SS1.p1.1.4" class="ltx_text ltx_lstlisting">&lt;</span>’, ‘<span id="S4.SS1.p1.1.5" class="ltx_text ltx_lstlisting"><span id="S4.SS1.p1.1.5.1" class="ltx_text ltx_lst_identifier">rowspan</span>=</span>’ or ‘<span id="S4.SS1.p1.1.6" class="ltx_text ltx_lstlisting"><span id="S4.SS1.p1.1.6.1" class="ltx_text ltx_lst_identifier">colspan</span>=</span>’, with the number of spanning cells (attribute), and ‘<span id="S4.SS1.p1.1.7" class="ltx_text ltx_lstlisting">&gt;</span>’. The hidden state attached to ‘<span id="S4.SS1.p1.1.8" class="ltx_text ltx_lstlisting">&lt;</span>’ is passed to the Cell BBox Decoder. A shared feed forward network (FFN) receives the hidden states from the Structure Decoder, to provide the final detection predictions of the bounding box coordinates and their classification.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2203.01017/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="186" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Given an input image of a table, the <span id="S4.F4.9.1" class="ltx_text ltx_font_bold">Encoder</span> produces fixed-length features that represent the input image.
The features are then passed to both the <span id="S4.F4.10.2" class="ltx_text ltx_font_bold">Structure Decoder</span> and <span id="S4.F4.11.3" class="ltx_text ltx_font_bold">Cell BBox Decoder</span>.
During training, the <span id="S4.F4.12.4" class="ltx_text ltx_font_bold">Structure Decoder</span> receives ‘tokenized tags’ of the HTML code that represent the table structure.
Afterwards, a transformer encoder and decoder architecture is employed to produce features that are received by a linear layer, and the <span id="S4.F4.13.5" class="ltx_text ltx_font_bold">Cell BBox Decoder. The linear layer is applied to the features to predict the tags. Simultaneously, the</span> <span id="S4.F4.14.6" class="ltx_text ltx_font_bold">Cell BBox Decoder</span> selects features referring to the data cells (‘<span id="S4.F4.15.7" class="ltx_text ltx_lstlisting">&lt;<span id="S4.F4.15.7.1" class="ltx_text ltx_lst_identifier">td</span>&gt;</span>’, ‘<span id="S4.F4.16.8" class="ltx_text ltx_lstlisting">&lt;</span>’) and passes them through an attention network, an MLP, and a linear layer to predict the bounding boxes.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">CNN Backbone Network.</span> A ResNet-18 CNN is the backbone that receives the table image and encodes it as a vector of predefined length.
The network has been modified by removing the linear and pooling layer, as we are not performing classification, and adding an adaptive pooling layer of size 28*28. ResNet by default downsamples the image resolution by 32 and then the encoded image is provided to both the <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">Structure Decoder</span>, and <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">Cell BBox Decoder</span>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Structure Decoder.</span> The transformer architecture of this component is based on the work proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. After extensive experimentation, the <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">Structure Decoder</span> is modeled as a transformer encoder with two encoder layers and a transformer decoder made from a stack of 4 decoder layers that comprise mainly of multi-head attention and feed forward layers. This configuration uses fewer layers and heads in comparison to networks applied to other problems (e.g. “Scene Understanding”, “Image Captioning”), something which we relate to the simplicity of table images.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">The transformer encoder receives an encoded image from the <span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_italic">CNN Backbone Network</span> and refines it through a multi-head dot-product attention layer, followed by a Feed Forward Network.
During training, the transformer decoder receives as input the output feature produced by the transformer encoder, and the tokenized input of the HTML ground-truth tags. Using a stack of multi-head attention layers, different aspects of the tag sequence could be inferred. This is achieved by each attention head on a layer operating in a different subspace, and then combining altogether their attention score.
</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Cell BBox Decoder.</span> Our architecture allows to simultaneously predict HTML tags and bounding boxes for each table cell without the need of a separate object detector end to end.
This approach is inspired by DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which employs a Transformer Encoder, and Decoder that looks for a specific number of object queries (potential object detections).
As our model utilizes a transformer architecture, the hidden state of the <span id="S4.SS1.p5.1.2" class="ltx_text ltx_lstlisting">&lt;<span id="S4.SS1.p5.1.2.1" class="ltx_text ltx_lst_identifier">td</span>&gt;</span>’ and ‘<span id="S4.SS1.p5.1.3" class="ltx_text ltx_lstlisting">&lt;</span>’ HTML structure tags become the object query.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">The encoding generated by the <span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_italic">CNN Backbone Network</span> along with the features acquired for every data cell from the Transformer Decoder are then passed to the attention network. The attention network takes both inputs and learns to provide an attention weighted encoding. This weighted attention encoding is then multiplied to the encoded image to produce a feature for each table cell.
Notice that this is different than the typical object detection problem where imbalances between the number of detections and the amount of objects may exist. In our case, we know up front that the produced detections always match with the table cells in number and correspondence.
</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">The output features for each table cell are then fed into the feed-forward network (FFN).
The FFN consists of a Multi-Layer Perceptron (3 layers with ReLU activation function) that predicts the normalized coordinates for the bounding box of each table cell.
Finally, the predicted bounding boxes are classified based on whether they are empty or not using a linear layer.
</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.5" class="ltx_p"><span id="S4.SS1.p8.5.1" class="ltx_text ltx_font_bold">Loss Functions.</span> We formulate a multi-task loss Eq. <a href="#S2a" title="2 Prediction post-processing for PDF documents ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to train our network.
The Cross-Entropy loss (denoted as <math id="S4.SS1.p8.1.m1.1" class="ltx_Math" alttext="l_{s}" display="inline"><semantics id="S4.SS1.p8.1.m1.1a"><msub id="S4.SS1.p8.1.m1.1.1" xref="S4.SS1.p8.1.m1.1.1.cmml"><mi id="S4.SS1.p8.1.m1.1.1.2" xref="S4.SS1.p8.1.m1.1.1.2.cmml">l</mi><mi id="S4.SS1.p8.1.m1.1.1.3" xref="S4.SS1.p8.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.1.m1.1b"><apply id="S4.SS1.p8.1.m1.1.1.cmml" xref="S4.SS1.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.1.m1.1.1.1.cmml" xref="S4.SS1.p8.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p8.1.m1.1.1.2.cmml" xref="S4.SS1.p8.1.m1.1.1.2">𝑙</ci><ci id="S4.SS1.p8.1.m1.1.1.3.cmml" xref="S4.SS1.p8.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.1.m1.1c">l_{s}</annotation></semantics></math>) is used to train the <span id="S4.SS1.p8.5.2" class="ltx_text ltx_font_italic">Structure Decoder</span> which predicts the structure tokens.
As for the <span id="S4.SS1.p8.5.3" class="ltx_text ltx_font_italic">Cell BBox Decoder</span> it is trained with a combination of losses denoted as <math id="S4.SS1.p8.2.m2.1" class="ltx_Math" alttext="l_{box}" display="inline"><semantics id="S4.SS1.p8.2.m2.1a"><msub id="S4.SS1.p8.2.m2.1.1" xref="S4.SS1.p8.2.m2.1.1.cmml"><mi id="S4.SS1.p8.2.m2.1.1.2" xref="S4.SS1.p8.2.m2.1.1.2.cmml">l</mi><mrow id="S4.SS1.p8.2.m2.1.1.3" xref="S4.SS1.p8.2.m2.1.1.3.cmml"><mi id="S4.SS1.p8.2.m2.1.1.3.2" xref="S4.SS1.p8.2.m2.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p8.2.m2.1.1.3.1" xref="S4.SS1.p8.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p8.2.m2.1.1.3.3" xref="S4.SS1.p8.2.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p8.2.m2.1.1.3.1a" xref="S4.SS1.p8.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p8.2.m2.1.1.3.4" xref="S4.SS1.p8.2.m2.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.2.m2.1b"><apply id="S4.SS1.p8.2.m2.1.1.cmml" xref="S4.SS1.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.2.m2.1.1.1.cmml" xref="S4.SS1.p8.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p8.2.m2.1.1.2.cmml" xref="S4.SS1.p8.2.m2.1.1.2">𝑙</ci><apply id="S4.SS1.p8.2.m2.1.1.3.cmml" xref="S4.SS1.p8.2.m2.1.1.3"><times id="S4.SS1.p8.2.m2.1.1.3.1.cmml" xref="S4.SS1.p8.2.m2.1.1.3.1"></times><ci id="S4.SS1.p8.2.m2.1.1.3.2.cmml" xref="S4.SS1.p8.2.m2.1.1.3.2">𝑏</ci><ci id="S4.SS1.p8.2.m2.1.1.3.3.cmml" xref="S4.SS1.p8.2.m2.1.1.3.3">𝑜</ci><ci id="S4.SS1.p8.2.m2.1.1.3.4.cmml" xref="S4.SS1.p8.2.m2.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.2.m2.1c">l_{box}</annotation></semantics></math>. <math id="S4.SS1.p8.3.m3.1" class="ltx_Math" alttext="l_{box}" display="inline"><semantics id="S4.SS1.p8.3.m3.1a"><msub id="S4.SS1.p8.3.m3.1.1" xref="S4.SS1.p8.3.m3.1.1.cmml"><mi id="S4.SS1.p8.3.m3.1.1.2" xref="S4.SS1.p8.3.m3.1.1.2.cmml">l</mi><mrow id="S4.SS1.p8.3.m3.1.1.3" xref="S4.SS1.p8.3.m3.1.1.3.cmml"><mi id="S4.SS1.p8.3.m3.1.1.3.2" xref="S4.SS1.p8.3.m3.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p8.3.m3.1.1.3.1" xref="S4.SS1.p8.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p8.3.m3.1.1.3.3" xref="S4.SS1.p8.3.m3.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p8.3.m3.1.1.3.1a" xref="S4.SS1.p8.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p8.3.m3.1.1.3.4" xref="S4.SS1.p8.3.m3.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.3.m3.1b"><apply id="S4.SS1.p8.3.m3.1.1.cmml" xref="S4.SS1.p8.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.3.m3.1.1.1.cmml" xref="S4.SS1.p8.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p8.3.m3.1.1.2.cmml" xref="S4.SS1.p8.3.m3.1.1.2">𝑙</ci><apply id="S4.SS1.p8.3.m3.1.1.3.cmml" xref="S4.SS1.p8.3.m3.1.1.3"><times id="S4.SS1.p8.3.m3.1.1.3.1.cmml" xref="S4.SS1.p8.3.m3.1.1.3.1"></times><ci id="S4.SS1.p8.3.m3.1.1.3.2.cmml" xref="S4.SS1.p8.3.m3.1.1.3.2">𝑏</ci><ci id="S4.SS1.p8.3.m3.1.1.3.3.cmml" xref="S4.SS1.p8.3.m3.1.1.3.3">𝑜</ci><ci id="S4.SS1.p8.3.m3.1.1.3.4.cmml" xref="S4.SS1.p8.3.m3.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.3.m3.1c">l_{box}</annotation></semantics></math> consists of the generally used <math id="S4.SS1.p8.4.m4.1" class="ltx_Math" alttext="l_{1}" display="inline"><semantics id="S4.SS1.p8.4.m4.1a"><msub id="S4.SS1.p8.4.m4.1.1" xref="S4.SS1.p8.4.m4.1.1.cmml"><mi id="S4.SS1.p8.4.m4.1.1.2" xref="S4.SS1.p8.4.m4.1.1.2.cmml">l</mi><mn id="S4.SS1.p8.4.m4.1.1.3" xref="S4.SS1.p8.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.4.m4.1b"><apply id="S4.SS1.p8.4.m4.1.1.cmml" xref="S4.SS1.p8.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.4.m4.1.1.1.cmml" xref="S4.SS1.p8.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p8.4.m4.1.1.2.cmml" xref="S4.SS1.p8.4.m4.1.1.2">𝑙</ci><cn type="integer" id="S4.SS1.p8.4.m4.1.1.3.cmml" xref="S4.SS1.p8.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.4.m4.1c">l_{1}</annotation></semantics></math> loss for object detection and the IoU loss (<math id="S4.SS1.p8.5.m5.1" class="ltx_Math" alttext="l_{iou}" display="inline"><semantics id="S4.SS1.p8.5.m5.1a"><msub id="S4.SS1.p8.5.m5.1.1" xref="S4.SS1.p8.5.m5.1.1.cmml"><mi id="S4.SS1.p8.5.m5.1.1.2" xref="S4.SS1.p8.5.m5.1.1.2.cmml">l</mi><mrow id="S4.SS1.p8.5.m5.1.1.3" xref="S4.SS1.p8.5.m5.1.1.3.cmml"><mi id="S4.SS1.p8.5.m5.1.1.3.2" xref="S4.SS1.p8.5.m5.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p8.5.m5.1.1.3.1" xref="S4.SS1.p8.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p8.5.m5.1.1.3.3" xref="S4.SS1.p8.5.m5.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p8.5.m5.1.1.3.1a" xref="S4.SS1.p8.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p8.5.m5.1.1.3.4" xref="S4.SS1.p8.5.m5.1.1.3.4.cmml">u</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.5.m5.1b"><apply id="S4.SS1.p8.5.m5.1.1.cmml" xref="S4.SS1.p8.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.5.m5.1.1.1.cmml" xref="S4.SS1.p8.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p8.5.m5.1.1.2.cmml" xref="S4.SS1.p8.5.m5.1.1.2">𝑙</ci><apply id="S4.SS1.p8.5.m5.1.1.3.cmml" xref="S4.SS1.p8.5.m5.1.1.3"><times id="S4.SS1.p8.5.m5.1.1.3.1.cmml" xref="S4.SS1.p8.5.m5.1.1.3.1"></times><ci id="S4.SS1.p8.5.m5.1.1.3.2.cmml" xref="S4.SS1.p8.5.m5.1.1.3.2">𝑖</ci><ci id="S4.SS1.p8.5.m5.1.1.3.3.cmml" xref="S4.SS1.p8.5.m5.1.1.3.3">𝑜</ci><ci id="S4.SS1.p8.5.m5.1.1.3.4.cmml" xref="S4.SS1.p8.5.m5.1.1.3.4">𝑢</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.5.m5.1c">l_{iou}</annotation></semantics></math>) to be scale invariant as explained in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. In comparison to DETR, we do not use the Hungarian algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to match the predicted bounding boxes with the ground-truth boxes, as we have already achieved a one-to-one match through two steps:
1) Our token input sequence is naturally ordered, therefore the hidden states of the table data cells are also in order when they are provided as input to the <span id="S4.SS1.p8.5.4" class="ltx_text ltx_font_italic">Cell BBox Decoder</span>, and
2) Our bounding boxes generation mechanism (see Sec. <a href="#S3" title="3 Datasets ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) ensures a one-to-one mapping between the cell content and its bounding box for all post-processed datasets.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.1" class="ltx_p">The loss used to train the TableFormer can be defined as following:</p>
</div>
<div id="S4.SS1.p10" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.25" class="ltx_Math" alttext="\begin{split}l_{box}=\lambda_{iou}l_{iou}+\lambda_{l1}\\
l=\lambda l_{s}+(1-\lambda)l_{box}\end{split}" display="block"><semantics id="S4.E1.m1.25a"><mtable displaystyle="true" rowspacing="0pt" id="S4.E1.m1.25.25.2" xref="S4.E1.m1.24.24.1.cmml"><mtr id="S4.E1.m1.25.25.2a" xref="S4.E1.m1.24.24.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S4.E1.m1.25.25.2b" xref="S4.E1.m1.24.24.1.cmml"><mrow id="S4.E1.m1.10.10.10.10.10" xref="S4.E1.m1.24.24.1.cmml"><msub id="S4.E1.m1.10.10.10.10.10.11" xref="S4.E1.m1.24.24.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml">l</mi><mrow id="S4.E1.m1.2.2.2.2.2.2.1" xref="S4.E1.m1.2.2.2.2.2.2.1.cmml"><mi id="S4.E1.m1.2.2.2.2.2.2.1.2" xref="S4.E1.m1.2.2.2.2.2.2.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S4.E1.m1.2.2.2.2.2.2.1.3" xref="S4.E1.m1.2.2.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2.2.2.2.1.1a" xref="S4.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S4.E1.m1.2.2.2.2.2.2.1.4" xref="S4.E1.m1.2.2.2.2.2.2.1.4.cmml">x</mi></mrow></msub><mo id="S4.E1.m1.3.3.3.3.3.3" xref="S4.E1.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S4.E1.m1.10.10.10.10.10.12" xref="S4.E1.m1.24.24.1.cmml"><mrow id="S4.E1.m1.10.10.10.10.10.12.1" xref="S4.E1.m1.24.24.1.cmml"><msub id="S4.E1.m1.10.10.10.10.10.12.1.2" xref="S4.E1.m1.24.24.1.cmml"><mi id="S4.E1.m1.4.4.4.4.4.4" xref="S4.E1.m1.4.4.4.4.4.4.cmml">λ</mi><mrow id="S4.E1.m1.5.5.5.5.5.5.1" xref="S4.E1.m1.5.5.5.5.5.5.1.cmml"><mi id="S4.E1.m1.5.5.5.5.5.5.1.2" xref="S4.E1.m1.5.5.5.5.5.5.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.5.5.5.5.5.5.1.1" xref="S4.E1.m1.5.5.5.5.5.5.1.1.cmml">​</mo><mi id="S4.E1.m1.5.5.5.5.5.5.1.3" xref="S4.E1.m1.5.5.5.5.5.5.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.5.5.5.5.5.5.1.1a" xref="S4.E1.m1.5.5.5.5.5.5.1.1.cmml">​</mo><mi id="S4.E1.m1.5.5.5.5.5.5.1.4" xref="S4.E1.m1.5.5.5.5.5.5.1.4.cmml">u</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.10.10.10.10.10.12.1.1" xref="S4.E1.m1.24.24.1a.cmml">​</mo><msub id="S4.E1.m1.10.10.10.10.10.12.1.3" xref="S4.E1.m1.24.24.1.cmml"><mi id="S4.E1.m1.6.6.6.6.6.6" xref="S4.E1.m1.6.6.6.6.6.6.cmml">l</mi><mrow id="S4.E1.m1.7.7.7.7.7.7.1" xref="S4.E1.m1.7.7.7.7.7.7.1.cmml"><mi id="S4.E1.m1.7.7.7.7.7.7.1.2" xref="S4.E1.m1.7.7.7.7.7.7.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.7.7.7.7.1.1" xref="S4.E1.m1.7.7.7.7.7.7.1.1.cmml">​</mo><mi id="S4.E1.m1.7.7.7.7.7.7.1.3" xref="S4.E1.m1.7.7.7.7.7.7.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.7.7.7.7.1.1a" xref="S4.E1.m1.7.7.7.7.7.7.1.1.cmml">​</mo><mi id="S4.E1.m1.7.7.7.7.7.7.1.4" xref="S4.E1.m1.7.7.7.7.7.7.1.4.cmml">u</mi></mrow></msub></mrow><mo id="S4.E1.m1.8.8.8.8.8.8" xref="S4.E1.m1.8.8.8.8.8.8.cmml">+</mo><msub id="S4.E1.m1.10.10.10.10.10.12.2" xref="S4.E1.m1.24.24.1.cmml"><mi id="S4.E1.m1.9.9.9.9.9.9" xref="S4.E1.m1.9.9.9.9.9.9.cmml">λ</mi><mrow id="S4.E1.m1.10.10.10.10.10.10.1" xref="S4.E1.m1.10.10.10.10.10.10.1.cmml"><mi id="S4.E1.m1.10.10.10.10.10.10.1.2" xref="S4.E1.m1.10.10.10.10.10.10.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.10.10.10.10.10.10.1.1" xref="S4.E1.m1.10.10.10.10.10.10.1.1.cmml">​</mo><mn id="S4.E1.m1.10.10.10.10.10.10.1.3" xref="S4.E1.m1.10.10.10.10.10.10.1.3.cmml">1</mn></mrow></msub></mrow></mrow></mtd></mtr><mtr id="S4.E1.m1.25.25.2c" xref="S4.E1.m1.24.24.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S4.E1.m1.25.25.2d" xref="S4.E1.m1.24.24.1.cmml"><mrow id="S4.E1.m1.25.25.2.24.14.14" xref="S4.E1.m1.24.24.1.cmml"><mi id="S4.E1.m1.11.11.11.1.1.1" xref="S4.E1.m1.11.11.11.1.1.1.cmml">l</mi><mo id="S4.E1.m1.12.12.12.2.2.2" xref="S4.E1.m1.12.12.12.2.2.2.cmml">=</mo><mrow id="S4.E1.m1.25.25.2.24.14.14.14" xref="S4.E1.m1.24.24.1.cmml"><mrow id="S4.E1.m1.25.25.2.24.14.14.14.2" xref="S4.E1.m1.24.24.1.cmml"><mi id="S4.E1.m1.13.13.13.3.3.3" xref="S4.E1.m1.13.13.13.3.3.3.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.25.25.2.24.14.14.14.2.1" xref="S4.E1.m1.24.24.1a.cmml">​</mo><msub id="S4.E1.m1.25.25.2.24.14.14.14.2.2" xref="S4.E1.m1.24.24.1.cmml"><mi id="S4.E1.m1.14.14.14.4.4.4" xref="S4.E1.m1.14.14.14.4.4.4.cmml">l</mi><mi id="S4.E1.m1.15.15.15.5.5.5.1" xref="S4.E1.m1.15.15.15.5.5.5.1.cmml">s</mi></msub></mrow><mo id="S4.E1.m1.16.16.16.6.6.6" xref="S4.E1.m1.16.16.16.6.6.6.cmml">+</mo><mrow id="S4.E1.m1.25.25.2.24.14.14.14.1" xref="S4.E1.m1.24.24.1.cmml"><mrow id="S4.E1.m1.25.25.2.24.14.14.14.1.1.1" xref="S4.E1.m1.24.24.1.cmml"><mo stretchy="false" id="S4.E1.m1.17.17.17.7.7.7" xref="S4.E1.m1.24.24.1a.cmml">(</mo><mrow id="S4.E1.m1.25.25.2.24.14.14.14.1.1.1.1" xref="S4.E1.m1.24.24.1.cmml"><mn id="S4.E1.m1.18.18.18.8.8.8" xref="S4.E1.m1.18.18.18.8.8.8.cmml">1</mn><mo id="S4.E1.m1.19.19.19.9.9.9" xref="S4.E1.m1.19.19.19.9.9.9.cmml">−</mo><mi id="S4.E1.m1.20.20.20.10.10.10" xref="S4.E1.m1.20.20.20.10.10.10.cmml">λ</mi></mrow><mo stretchy="false" id="S4.E1.m1.21.21.21.11.11.11" xref="S4.E1.m1.24.24.1a.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E1.m1.25.25.2.24.14.14.14.1.2" xref="S4.E1.m1.24.24.1a.cmml">​</mo><msub id="S4.E1.m1.25.25.2.24.14.14.14.1.3" xref="S4.E1.m1.24.24.1.cmml"><mi id="S4.E1.m1.22.22.22.12.12.12" xref="S4.E1.m1.22.22.22.12.12.12.cmml">l</mi><mrow id="S4.E1.m1.23.23.23.13.13.13.1" xref="S4.E1.m1.23.23.23.13.13.13.1.cmml"><mi id="S4.E1.m1.23.23.23.13.13.13.1.2" xref="S4.E1.m1.23.23.23.13.13.13.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.23.23.23.13.13.13.1.1" xref="S4.E1.m1.23.23.23.13.13.13.1.1.cmml">​</mo><mi id="S4.E1.m1.23.23.23.13.13.13.1.3" xref="S4.E1.m1.23.23.23.13.13.13.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.23.23.23.13.13.13.1.1a" xref="S4.E1.m1.23.23.23.13.13.13.1.1.cmml">​</mo><mi id="S4.E1.m1.23.23.23.13.13.13.1.4" xref="S4.E1.m1.23.23.23.13.13.13.1.4.cmml">x</mi></mrow></msub></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S4.E1.m1.25b"><apply id="S4.E1.m1.24.24.1.cmml" xref="S4.E1.m1.25.25.2"><and id="S4.E1.m1.24.24.1a.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1"></and><apply id="S4.E1.m1.24.24.1b.cmml" xref="S4.E1.m1.25.25.2"><eq id="S4.E1.m1.3.3.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3.3.3"></eq><apply id="S4.E1.m1.24.24.1.3.cmml" xref="S4.E1.m1.25.25.2"><csymbol cd="ambiguous" id="S4.E1.m1.24.24.1.3.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1">𝑙</ci><apply id="S4.E1.m1.2.2.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1"><times id="S4.E1.m1.2.2.2.2.2.2.1.1.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.1"></times><ci id="S4.E1.m1.2.2.2.2.2.2.1.2.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.2">𝑏</ci><ci id="S4.E1.m1.2.2.2.2.2.2.1.3.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.3">𝑜</ci><ci id="S4.E1.m1.2.2.2.2.2.2.1.4.cmml" xref="S4.E1.m1.2.2.2.2.2.2.1.4">𝑥</ci></apply></apply><apply id="S4.E1.m1.24.24.1.5.cmml" xref="S4.E1.m1.25.25.2"><plus id="S4.E1.m1.8.8.8.8.8.8.cmml" xref="S4.E1.m1.8.8.8.8.8.8"></plus><apply id="S4.E1.m1.24.24.1.5.2.cmml" xref="S4.E1.m1.25.25.2"><times id="S4.E1.m1.24.24.1.5.2.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1"></times><apply id="S4.E1.m1.24.24.1.5.2.2.cmml" xref="S4.E1.m1.25.25.2"><csymbol cd="ambiguous" id="S4.E1.m1.24.24.1.5.2.2.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1">subscript</csymbol><ci id="S4.E1.m1.4.4.4.4.4.4.cmml" xref="S4.E1.m1.4.4.4.4.4.4">𝜆</ci><apply id="S4.E1.m1.5.5.5.5.5.5.1.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1"><times id="S4.E1.m1.5.5.5.5.5.5.1.1.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1.1"></times><ci id="S4.E1.m1.5.5.5.5.5.5.1.2.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1.2">𝑖</ci><ci id="S4.E1.m1.5.5.5.5.5.5.1.3.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1.3">𝑜</ci><ci id="S4.E1.m1.5.5.5.5.5.5.1.4.cmml" xref="S4.E1.m1.5.5.5.5.5.5.1.4">𝑢</ci></apply></apply><apply id="S4.E1.m1.24.24.1.5.2.3.cmml" xref="S4.E1.m1.25.25.2"><csymbol cd="ambiguous" id="S4.E1.m1.24.24.1.5.2.3.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1">subscript</csymbol><ci id="S4.E1.m1.6.6.6.6.6.6.cmml" xref="S4.E1.m1.6.6.6.6.6.6">𝑙</ci><apply id="S4.E1.m1.7.7.7.7.7.7.1.cmml" xref="S4.E1.m1.7.7.7.7.7.7.1"><times id="S4.E1.m1.7.7.7.7.7.7.1.1.cmml" xref="S4.E1.m1.7.7.7.7.7.7.1.1"></times><ci id="S4.E1.m1.7.7.7.7.7.7.1.2.cmml" xref="S4.E1.m1.7.7.7.7.7.7.1.2">𝑖</ci><ci id="S4.E1.m1.7.7.7.7.7.7.1.3.cmml" xref="S4.E1.m1.7.7.7.7.7.7.1.3">𝑜</ci><ci id="S4.E1.m1.7.7.7.7.7.7.1.4.cmml" xref="S4.E1.m1.7.7.7.7.7.7.1.4">𝑢</ci></apply></apply></apply><apply id="S4.E1.m1.24.24.1.5.3.cmml" xref="S4.E1.m1.25.25.2"><times id="S4.E1.m1.24.24.1.5.3.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1"></times><apply id="S4.E1.m1.24.24.1.5.3.2.cmml" xref="S4.E1.m1.25.25.2"><csymbol cd="ambiguous" id="S4.E1.m1.24.24.1.5.3.2.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1">subscript</csymbol><ci id="S4.E1.m1.9.9.9.9.9.9.cmml" xref="S4.E1.m1.9.9.9.9.9.9">𝜆</ci><apply id="S4.E1.m1.10.10.10.10.10.10.1.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1"><times id="S4.E1.m1.10.10.10.10.10.10.1.1.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1.1"></times><ci id="S4.E1.m1.10.10.10.10.10.10.1.2.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1.2">𝑙</ci><cn type="integer" id="S4.E1.m1.10.10.10.10.10.10.1.3.cmml" xref="S4.E1.m1.10.10.10.10.10.10.1.3">1</cn></apply></apply><ci id="S4.E1.m1.11.11.11.1.1.1.cmml" xref="S4.E1.m1.11.11.11.1.1.1">𝑙</ci></apply></apply></apply><apply id="S4.E1.m1.24.24.1c.cmml" xref="S4.E1.m1.25.25.2"><eq id="S4.E1.m1.12.12.12.2.2.2.cmml" xref="S4.E1.m1.12.12.12.2.2.2"></eq><share href="#S4.E1.m1.24.24.1.5.cmml" id="S4.E1.m1.24.24.1d.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1"></share><apply id="S4.E1.m1.24.24.1.1.cmml" xref="S4.E1.m1.25.25.2"><plus id="S4.E1.m1.16.16.16.6.6.6.cmml" xref="S4.E1.m1.16.16.16.6.6.6"></plus><apply id="S4.E1.m1.24.24.1.1.3.cmml" xref="S4.E1.m1.25.25.2"><times id="S4.E1.m1.24.24.1.1.3.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1"></times><ci id="S4.E1.m1.13.13.13.3.3.3.cmml" xref="S4.E1.m1.13.13.13.3.3.3">𝜆</ci><apply id="S4.E1.m1.24.24.1.1.3.3.cmml" xref="S4.E1.m1.25.25.2"><csymbol cd="ambiguous" id="S4.E1.m1.24.24.1.1.3.3.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1">subscript</csymbol><ci id="S4.E1.m1.14.14.14.4.4.4.cmml" xref="S4.E1.m1.14.14.14.4.4.4">𝑙</ci><ci id="S4.E1.m1.15.15.15.5.5.5.1.cmml" xref="S4.E1.m1.15.15.15.5.5.5.1">𝑠</ci></apply></apply><apply id="S4.E1.m1.24.24.1.1.1.cmml" xref="S4.E1.m1.25.25.2"><times id="S4.E1.m1.24.24.1.1.1.2.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1"></times><apply id="S4.E1.m1.24.24.1.1.1.1.1.1.cmml" xref="S4.E1.m1.25.25.2"><minus id="S4.E1.m1.19.19.19.9.9.9.cmml" xref="S4.E1.m1.19.19.19.9.9.9"></minus><cn type="integer" id="S4.E1.m1.18.18.18.8.8.8.cmml" xref="S4.E1.m1.18.18.18.8.8.8">1</cn><ci id="S4.E1.m1.20.20.20.10.10.10.cmml" xref="S4.E1.m1.20.20.20.10.10.10">𝜆</ci></apply><apply id="S4.E1.m1.24.24.1.1.1.3.cmml" xref="S4.E1.m1.25.25.2"><csymbol cd="ambiguous" id="S4.E1.m1.24.24.1.1.1.3.1.cmml" xref="S4.E1.m1.10.10.10.10.10.12.1.1">subscript</csymbol><ci id="S4.E1.m1.22.22.22.12.12.12.cmml" xref="S4.E1.m1.22.22.22.12.12.12">𝑙</ci><apply id="S4.E1.m1.23.23.23.13.13.13.1.cmml" xref="S4.E1.m1.23.23.23.13.13.13.1"><times id="S4.E1.m1.23.23.23.13.13.13.1.1.cmml" xref="S4.E1.m1.23.23.23.13.13.13.1.1"></times><ci id="S4.E1.m1.23.23.23.13.13.13.1.2.cmml" xref="S4.E1.m1.23.23.23.13.13.13.1.2">𝑏</ci><ci id="S4.E1.m1.23.23.23.13.13.13.1.3.cmml" xref="S4.E1.m1.23.23.23.13.13.13.1.3">𝑜</ci><ci id="S4.E1.m1.23.23.23.13.13.13.1.4.cmml" xref="S4.E1.m1.23.23.23.13.13.13.1.4">𝑥</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.25c">\begin{split}l_{box}=\lambda_{iou}l_{iou}+\lambda_{l1}\\
l=\lambda l_{s}+(1-\lambda)l_{box}\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p10.2" class="ltx_p">where <math id="S4.SS1.p10.1.m1.1" class="ltx_Math" alttext="\lambda\in" display="inline"><semantics id="S4.SS1.p10.1.m1.1a"><mrow id="S4.SS1.p10.1.m1.1.1" xref="S4.SS1.p10.1.m1.1.1.cmml"><mi id="S4.SS1.p10.1.m1.1.1.2" xref="S4.SS1.p10.1.m1.1.1.2.cmml">λ</mi><mo id="S4.SS1.p10.1.m1.1.1.1" xref="S4.SS1.p10.1.m1.1.1.1.cmml">∈</mo><mi id="S4.SS1.p10.1.m1.1.1.3" xref="S4.SS1.p10.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p10.1.m1.1b"><apply id="S4.SS1.p10.1.m1.1.1.cmml" xref="S4.SS1.p10.1.m1.1.1"><in id="S4.SS1.p10.1.m1.1.1.1.cmml" xref="S4.SS1.p10.1.m1.1.1.1"></in><ci id="S4.SS1.p10.1.m1.1.1.2.cmml" xref="S4.SS1.p10.1.m1.1.1.2">𝜆</ci><csymbol cd="latexml" id="S4.SS1.p10.1.m1.1.1.3.cmml" xref="S4.SS1.p10.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p10.1.m1.1c">\lambda\in</annotation></semantics></math> [0, 1], and <math id="S4.SS1.p10.2.m2.2" class="ltx_Math" alttext="\lambda_{iou},\lambda_{l1}\in\mathbb{R}" display="inline"><semantics id="S4.SS1.p10.2.m2.2a"><mrow id="S4.SS1.p10.2.m2.2.2" xref="S4.SS1.p10.2.m2.2.2.cmml"><mrow id="S4.SS1.p10.2.m2.2.2.2.2" xref="S4.SS1.p10.2.m2.2.2.2.3.cmml"><msub id="S4.SS1.p10.2.m2.1.1.1.1.1" xref="S4.SS1.p10.2.m2.1.1.1.1.1.cmml"><mi id="S4.SS1.p10.2.m2.1.1.1.1.1.2" xref="S4.SS1.p10.2.m2.1.1.1.1.1.2.cmml">λ</mi><mrow id="S4.SS1.p10.2.m2.1.1.1.1.1.3" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.cmml"><mi id="S4.SS1.p10.2.m2.1.1.1.1.1.3.2" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p10.2.m2.1.1.1.1.1.3.1" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p10.2.m2.1.1.1.1.1.3.3" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p10.2.m2.1.1.1.1.1.3.1a" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p10.2.m2.1.1.1.1.1.3.4" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.4.cmml">u</mi></mrow></msub><mo id="S4.SS1.p10.2.m2.2.2.2.2.3" xref="S4.SS1.p10.2.m2.2.2.2.3.cmml">,</mo><msub id="S4.SS1.p10.2.m2.2.2.2.2.2" xref="S4.SS1.p10.2.m2.2.2.2.2.2.cmml"><mi id="S4.SS1.p10.2.m2.2.2.2.2.2.2" xref="S4.SS1.p10.2.m2.2.2.2.2.2.2.cmml">λ</mi><mrow id="S4.SS1.p10.2.m2.2.2.2.2.2.3" xref="S4.SS1.p10.2.m2.2.2.2.2.2.3.cmml"><mi id="S4.SS1.p10.2.m2.2.2.2.2.2.3.2" xref="S4.SS1.p10.2.m2.2.2.2.2.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p10.2.m2.2.2.2.2.2.3.1" xref="S4.SS1.p10.2.m2.2.2.2.2.2.3.1.cmml">​</mo><mn id="S4.SS1.p10.2.m2.2.2.2.2.2.3.3" xref="S4.SS1.p10.2.m2.2.2.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S4.SS1.p10.2.m2.2.2.3" xref="S4.SS1.p10.2.m2.2.2.3.cmml">∈</mo><mi id="S4.SS1.p10.2.m2.2.2.4" xref="S4.SS1.p10.2.m2.2.2.4.cmml">ℝ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p10.2.m2.2b"><apply id="S4.SS1.p10.2.m2.2.2.cmml" xref="S4.SS1.p10.2.m2.2.2"><in id="S4.SS1.p10.2.m2.2.2.3.cmml" xref="S4.SS1.p10.2.m2.2.2.3"></in><list id="S4.SS1.p10.2.m2.2.2.2.3.cmml" xref="S4.SS1.p10.2.m2.2.2.2.2"><apply id="S4.SS1.p10.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.p10.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p10.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS1.p10.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p10.2.m2.1.1.1.1.1.2.cmml" xref="S4.SS1.p10.2.m2.1.1.1.1.1.2">𝜆</ci><apply id="S4.SS1.p10.2.m2.1.1.1.1.1.3.cmml" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3"><times id="S4.SS1.p10.2.m2.1.1.1.1.1.3.1.cmml" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.1"></times><ci id="S4.SS1.p10.2.m2.1.1.1.1.1.3.2.cmml" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.2">𝑖</ci><ci id="S4.SS1.p10.2.m2.1.1.1.1.1.3.3.cmml" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.3">𝑜</ci><ci id="S4.SS1.p10.2.m2.1.1.1.1.1.3.4.cmml" xref="S4.SS1.p10.2.m2.1.1.1.1.1.3.4">𝑢</ci></apply></apply><apply id="S4.SS1.p10.2.m2.2.2.2.2.2.cmml" xref="S4.SS1.p10.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p10.2.m2.2.2.2.2.2.1.cmml" xref="S4.SS1.p10.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p10.2.m2.2.2.2.2.2.2.cmml" xref="S4.SS1.p10.2.m2.2.2.2.2.2.2">𝜆</ci><apply id="S4.SS1.p10.2.m2.2.2.2.2.2.3.cmml" xref="S4.SS1.p10.2.m2.2.2.2.2.2.3"><times id="S4.SS1.p10.2.m2.2.2.2.2.2.3.1.cmml" xref="S4.SS1.p10.2.m2.2.2.2.2.2.3.1"></times><ci id="S4.SS1.p10.2.m2.2.2.2.2.2.3.2.cmml" xref="S4.SS1.p10.2.m2.2.2.2.2.2.3.2">𝑙</ci><cn type="integer" id="S4.SS1.p10.2.m2.2.2.2.2.2.3.3.cmml" xref="S4.SS1.p10.2.m2.2.2.2.2.2.3.3">1</cn></apply></apply></list><ci id="S4.SS1.p10.2.m2.2.2.4.cmml" xref="S4.SS1.p10.2.m2.2.2.4">ℝ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p10.2.m2.2c">\lambda_{iou},\lambda_{l1}\in\mathbb{R}</annotation></semantics></math> are hyper-parameters.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Implementation Details</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">TableFormer uses ResNet-18 as the <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">CNN Backbone Network</span>. The input images are resized to 448*448 pixels and the feature map has a dimension of 28*28.
Additionally, we enforce the following input constraints:
</p>
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.2" class="ltx_Math" alttext="\begin{split}\textnormal{Image width and height $\leq$ 1024 pixels}\\
\textnormal{Structural tags length $\leq$ 512 tokens.}\end{split}" display="block"><semantics id="S5.E2.m1.2a"><mtable displaystyle="true" rowspacing="0pt" id="S5.E2.m1.2.2" xref="S5.E2.m1.2.3.1.cmml"><mtr id="S5.E2.m1.2.2a" xref="S5.E2.m1.2.3.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S5.E2.m1.2.2b" xref="S5.E2.m1.2.3.1.cmml"><mrow id="S5.E2.m1.1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.1.1c.cmml"><mtext id="S5.E2.m1.1.1.1.1.1.1a" xref="S5.E2.m1.1.1.1.1.1.1c.cmml">Image width and height </mtext><mo id="S5.E2.m1.1.1.1.1.1.1.1.m1.1.1" xref="S5.E2.m1.1.1.1.1.1.1.1.m1.1.1.cmml">≤</mo><mtext id="S5.E2.m1.1.1.1.1.1.1b" xref="S5.E2.m1.1.1.1.1.1.1c.cmml"> 1024 pixels</mtext></mrow></mtd></mtr><mtr id="S5.E2.m1.2.2c" xref="S5.E2.m1.2.3.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S5.E2.m1.2.2d" xref="S5.E2.m1.2.3.1.cmml"><mrow id="S5.E2.m1.2.2.2.1.1.1" xref="S5.E2.m1.2.2.2.1.1.1c.cmml"><mtext id="S5.E2.m1.2.2.2.1.1.1a" xref="S5.E2.m1.2.2.2.1.1.1c.cmml">Structural tags length </mtext><mo id="S5.E2.m1.2.2.2.1.1.1.1.m1.1.1" xref="S5.E2.m1.2.2.2.1.1.1.1.m1.1.1.cmml">≤</mo><mtext id="S5.E2.m1.2.2.2.1.1.1b" xref="S5.E2.m1.2.2.2.1.1.1c.cmml"> 512 tokens.</mtext></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S5.E2.m1.2b"><apply id="S5.E2.m1.2.3.1.cmml" xref="S5.E2.m1.2.2"><times id="S5.E2.m1.2.3.1.1.cmml" xref="S5.E2.m1.2.2"></times><ci id="S5.E2.m1.1.1.1.1.1.1c.cmml" xref="S5.E2.m1.1.1.1.1.1.1"><mrow id="S5.E2.m1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1.1"><mtext id="S5.E2.m1.1.1.1.1.1.1a.cmml" xref="S5.E2.m1.1.1.1.1.1.1">Image width and height </mtext><mo id="S5.E2.m1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.1.1.1.m1.1.1">≤</mo><mtext id="S5.E2.m1.1.1.1.1.1.1b.cmml" xref="S5.E2.m1.1.1.1.1.1.1"> 1024 pixels</mtext></mrow></ci><ci id="S5.E2.m1.2.2.2.1.1.1c.cmml" xref="S5.E2.m1.2.2.2.1.1.1"><mrow id="S5.E2.m1.2.2.2.1.1.1.cmml" xref="S5.E2.m1.2.2.2.1.1.1"><mtext id="S5.E2.m1.2.2.2.1.1.1a.cmml" xref="S5.E2.m1.2.2.2.1.1.1">Structural tags length </mtext><mo id="S5.E2.m1.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S5.E2.m1.2.2.2.1.1.1.1.m1.1.1">≤</mo><mtext id="S5.E2.m1.2.2.2.1.1.1b.cmml" xref="S5.E2.m1.2.2.2.1.1.1"> 512 tokens.</mtext></mrow></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.2c">\begin{split}\textnormal{Image width and height $\leq$ 1024 pixels}\\
\textnormal{Structural tags length $\leq$ 512 tokens.}\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S5.SS1.p1.2" class="ltx_p">Although input constraints are used also by other methods, such as EDD, ours are less restrictive due to the improved runtime performance and lower memory footprint of TableFormer. This allows to utilize input samples with longer sequences and images with larger dimensions.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The Transformer Encoder consists of two “Transformer Encoder Layers”, with an input feature size of 512, feed forward network of 1024, and 4 attention heads. As for the Transformer Decoder it is composed of four “Transformer Decoder Layers” with similar input and output dimensions as the “Transformer Encoder Layers”.
Even though our model uses fewer layers and heads than the default implementation parameters, our extensive experimentation has proved this setup to be more suitable for table images. We attribute this finding to the inherent design of table images, which contain mostly lines and text, unlike the more elaborate content present in other scopes (e.g. the COCO dataset).
Moreover, we have added ResNet blocks to the inputs of the Structure Decoder and Cell BBox Decoder.
This prevents a decoder having a stronger influence over the learned weights which would damage the other prediction task (structure vs bounding boxes), but learn task specific weights instead.
Lastly our dropout layers are set to 0.5.
</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">For training, TableFormer is trained with 3 Adam optimizers, each one for the <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">CNN Backbone Network</span>, <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">Structure Decoder</span>, and <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_italic">Cell BBox Decoder</span>. Taking the PubTabNet as an example for our parameter set up, the initializing learning rate is 0.001 for 12 epochs with a batch size of 24, and <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\lambda</annotation></semantics></math> set to 0.5.
Afterwards, we reduce the learning rate to 0.0001, the batch size to 18 and train for 12 more epochs or convergence.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">TableFormer is implemented with PyTorch and Torchvision libraries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
To speed up the inference, the image undergoes a single forward pass through the <span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_italic">CNN Backbone Network</span> and transformer encoder. This eliminates the overhead of generating the same features for each decoding step.
Similarly, we employ a ’caching’ technique to preform faster autoregressive decoding. This is achieved by storing the features of decoded tokens so we can reuse them for each time step. Therefore, we only compute the attention for each new tag.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Generalization</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">TableFormer is evaluated on three major publicly available datasets of different nature to prove the generalization and effectiveness of our model.
The datasets used for evaluation are the PubTabNet, FinTabNet and TableBank which stem from the scientific, financial and general domains respectively.
</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We also share our baseline results on the challenging SynthTabNet dataset. Throughout our experiments, the same parameters stated in Sec. <a href="#S5.SS1" title="5.1 Implementation Details ‣ 5 Experimental Results ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> are utilized.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Datasets and Metrics</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The Tree-Edit-Distance-Based Similarity (TEDS) metric was introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. It represents the prediction, and ground-truth as a tree structure of HTML tags. This similarity is calculated as:</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<table id="S5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E3.m1.9" class="ltx_Math" alttext="\operatorname{TEDS}\left(T_{a},T_{b}\right)=1-\frac{\operatorname{EditDist}\left(T_{a},T_{b}\right)}{\max\left(\left|T_{a}\right|,\left|T_{b}\right|\right)}" display="block"><semantics id="S5.E3.m1.9a"><mrow id="S5.E3.m1.9.9" xref="S5.E3.m1.9.9.cmml"><mrow id="S5.E3.m1.9.9.2.2" xref="S5.E3.m1.9.9.2.3.cmml"><mi id="S5.E3.m1.7.7" xref="S5.E3.m1.7.7.cmml">TEDS</mi><mo id="S5.E3.m1.9.9.2.2a" xref="S5.E3.m1.9.9.2.3.cmml">⁡</mo><mrow id="S5.E3.m1.9.9.2.2.2" xref="S5.E3.m1.9.9.2.3.cmml"><mo id="S5.E3.m1.9.9.2.2.2.3" xref="S5.E3.m1.9.9.2.3.cmml">(</mo><msub id="S5.E3.m1.8.8.1.1.1.1" xref="S5.E3.m1.8.8.1.1.1.1.cmml"><mi id="S5.E3.m1.8.8.1.1.1.1.2" xref="S5.E3.m1.8.8.1.1.1.1.2.cmml">T</mi><mi id="S5.E3.m1.8.8.1.1.1.1.3" xref="S5.E3.m1.8.8.1.1.1.1.3.cmml">a</mi></msub><mo id="S5.E3.m1.9.9.2.2.2.4" xref="S5.E3.m1.9.9.2.3.cmml">,</mo><msub id="S5.E3.m1.9.9.2.2.2.2" xref="S5.E3.m1.9.9.2.2.2.2.cmml"><mi id="S5.E3.m1.9.9.2.2.2.2.2" xref="S5.E3.m1.9.9.2.2.2.2.2.cmml">T</mi><mi id="S5.E3.m1.9.9.2.2.2.2.3" xref="S5.E3.m1.9.9.2.2.2.2.3.cmml">b</mi></msub><mo id="S5.E3.m1.9.9.2.2.2.5" xref="S5.E3.m1.9.9.2.3.cmml">)</mo></mrow></mrow><mo id="S5.E3.m1.9.9.3" xref="S5.E3.m1.9.9.3.cmml">=</mo><mrow id="S5.E3.m1.9.9.4" xref="S5.E3.m1.9.9.4.cmml"><mn id="S5.E3.m1.9.9.4.2" xref="S5.E3.m1.9.9.4.2.cmml">1</mn><mo id="S5.E3.m1.9.9.4.1" xref="S5.E3.m1.9.9.4.1.cmml">−</mo><mfrac id="S5.E3.m1.6.6" xref="S5.E3.m1.6.6.cmml"><mrow id="S5.E3.m1.3.3.3.3" xref="S5.E3.m1.3.3.3.4.cmml"><mi id="S5.E3.m1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.cmml">EditDist</mi><mo id="S5.E3.m1.3.3.3.3a" xref="S5.E3.m1.3.3.3.4.cmml">⁡</mo><mrow id="S5.E3.m1.3.3.3.3.2" xref="S5.E3.m1.3.3.3.4.cmml"><mo id="S5.E3.m1.3.3.3.3.2.3" xref="S5.E3.m1.3.3.3.4.cmml">(</mo><msub id="S5.E3.m1.2.2.2.2.1.1" xref="S5.E3.m1.2.2.2.2.1.1.cmml"><mi id="S5.E3.m1.2.2.2.2.1.1.2" xref="S5.E3.m1.2.2.2.2.1.1.2.cmml">T</mi><mi id="S5.E3.m1.2.2.2.2.1.1.3" xref="S5.E3.m1.2.2.2.2.1.1.3.cmml">a</mi></msub><mo id="S5.E3.m1.3.3.3.3.2.4" xref="S5.E3.m1.3.3.3.4.cmml">,</mo><msub id="S5.E3.m1.3.3.3.3.2.2" xref="S5.E3.m1.3.3.3.3.2.2.cmml"><mi id="S5.E3.m1.3.3.3.3.2.2.2" xref="S5.E3.m1.3.3.3.3.2.2.2.cmml">T</mi><mi id="S5.E3.m1.3.3.3.3.2.2.3" xref="S5.E3.m1.3.3.3.3.2.2.3.cmml">b</mi></msub><mo id="S5.E3.m1.3.3.3.3.2.5" xref="S5.E3.m1.3.3.3.4.cmml">)</mo></mrow></mrow><mrow id="S5.E3.m1.6.6.6.3" xref="S5.E3.m1.6.6.6.4.cmml"><mi id="S5.E3.m1.4.4.4.1" xref="S5.E3.m1.4.4.4.1.cmml">max</mi><mo id="S5.E3.m1.6.6.6.3a" xref="S5.E3.m1.6.6.6.4.cmml">⁡</mo><mrow id="S5.E3.m1.6.6.6.3.2" xref="S5.E3.m1.6.6.6.4.cmml"><mo id="S5.E3.m1.6.6.6.3.2.3" xref="S5.E3.m1.6.6.6.4.cmml">(</mo><mrow id="S5.E3.m1.5.5.5.2.1.1.1" xref="S5.E3.m1.5.5.5.2.1.1.2.cmml"><mo id="S5.E3.m1.5.5.5.2.1.1.1.2" xref="S5.E3.m1.5.5.5.2.1.1.2.1.cmml">|</mo><msub id="S5.E3.m1.5.5.5.2.1.1.1.1" xref="S5.E3.m1.5.5.5.2.1.1.1.1.cmml"><mi id="S5.E3.m1.5.5.5.2.1.1.1.1.2" xref="S5.E3.m1.5.5.5.2.1.1.1.1.2.cmml">T</mi><mi id="S5.E3.m1.5.5.5.2.1.1.1.1.3" xref="S5.E3.m1.5.5.5.2.1.1.1.1.3.cmml">a</mi></msub><mo id="S5.E3.m1.5.5.5.2.1.1.1.3" xref="S5.E3.m1.5.5.5.2.1.1.2.1.cmml">|</mo></mrow><mo id="S5.E3.m1.6.6.6.3.2.4" xref="S5.E3.m1.6.6.6.4.cmml">,</mo><mrow id="S5.E3.m1.6.6.6.3.2.2.1" xref="S5.E3.m1.6.6.6.3.2.2.2.cmml"><mo id="S5.E3.m1.6.6.6.3.2.2.1.2" xref="S5.E3.m1.6.6.6.3.2.2.2.1.cmml">|</mo><msub id="S5.E3.m1.6.6.6.3.2.2.1.1" xref="S5.E3.m1.6.6.6.3.2.2.1.1.cmml"><mi id="S5.E3.m1.6.6.6.3.2.2.1.1.2" xref="S5.E3.m1.6.6.6.3.2.2.1.1.2.cmml">T</mi><mi id="S5.E3.m1.6.6.6.3.2.2.1.1.3" xref="S5.E3.m1.6.6.6.3.2.2.1.1.3.cmml">b</mi></msub><mo id="S5.E3.m1.6.6.6.3.2.2.1.3" xref="S5.E3.m1.6.6.6.3.2.2.2.1.cmml">|</mo></mrow><mo id="S5.E3.m1.6.6.6.3.2.5" xref="S5.E3.m1.6.6.6.4.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.9b"><apply id="S5.E3.m1.9.9.cmml" xref="S5.E3.m1.9.9"><eq id="S5.E3.m1.9.9.3.cmml" xref="S5.E3.m1.9.9.3"></eq><apply id="S5.E3.m1.9.9.2.3.cmml" xref="S5.E3.m1.9.9.2.2"><ci id="S5.E3.m1.7.7.cmml" xref="S5.E3.m1.7.7">TEDS</ci><apply id="S5.E3.m1.8.8.1.1.1.1.cmml" xref="S5.E3.m1.8.8.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.8.8.1.1.1.1.1.cmml" xref="S5.E3.m1.8.8.1.1.1.1">subscript</csymbol><ci id="S5.E3.m1.8.8.1.1.1.1.2.cmml" xref="S5.E3.m1.8.8.1.1.1.1.2">𝑇</ci><ci id="S5.E3.m1.8.8.1.1.1.1.3.cmml" xref="S5.E3.m1.8.8.1.1.1.1.3">𝑎</ci></apply><apply id="S5.E3.m1.9.9.2.2.2.2.cmml" xref="S5.E3.m1.9.9.2.2.2.2"><csymbol cd="ambiguous" id="S5.E3.m1.9.9.2.2.2.2.1.cmml" xref="S5.E3.m1.9.9.2.2.2.2">subscript</csymbol><ci id="S5.E3.m1.9.9.2.2.2.2.2.cmml" xref="S5.E3.m1.9.9.2.2.2.2.2">𝑇</ci><ci id="S5.E3.m1.9.9.2.2.2.2.3.cmml" xref="S5.E3.m1.9.9.2.2.2.2.3">𝑏</ci></apply></apply><apply id="S5.E3.m1.9.9.4.cmml" xref="S5.E3.m1.9.9.4"><minus id="S5.E3.m1.9.9.4.1.cmml" xref="S5.E3.m1.9.9.4.1"></minus><cn type="integer" id="S5.E3.m1.9.9.4.2.cmml" xref="S5.E3.m1.9.9.4.2">1</cn><apply id="S5.E3.m1.6.6.cmml" xref="S5.E3.m1.6.6"><divide id="S5.E3.m1.6.6.7.cmml" xref="S5.E3.m1.6.6"></divide><apply id="S5.E3.m1.3.3.3.4.cmml" xref="S5.E3.m1.3.3.3.3"><ci id="S5.E3.m1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1">EditDist</ci><apply id="S5.E3.m1.2.2.2.2.1.1.cmml" xref="S5.E3.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.2.2.1.1.1.cmml" xref="S5.E3.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S5.E3.m1.2.2.2.2.1.1.2.cmml" xref="S5.E3.m1.2.2.2.2.1.1.2">𝑇</ci><ci id="S5.E3.m1.2.2.2.2.1.1.3.cmml" xref="S5.E3.m1.2.2.2.2.1.1.3">𝑎</ci></apply><apply id="S5.E3.m1.3.3.3.3.2.2.cmml" xref="S5.E3.m1.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S5.E3.m1.3.3.3.3.2.2.1.cmml" xref="S5.E3.m1.3.3.3.3.2.2">subscript</csymbol><ci id="S5.E3.m1.3.3.3.3.2.2.2.cmml" xref="S5.E3.m1.3.3.3.3.2.2.2">𝑇</ci><ci id="S5.E3.m1.3.3.3.3.2.2.3.cmml" xref="S5.E3.m1.3.3.3.3.2.2.3">𝑏</ci></apply></apply><apply id="S5.E3.m1.6.6.6.4.cmml" xref="S5.E3.m1.6.6.6.3"><max id="S5.E3.m1.4.4.4.1.cmml" xref="S5.E3.m1.4.4.4.1"></max><apply id="S5.E3.m1.5.5.5.2.1.1.2.cmml" xref="S5.E3.m1.5.5.5.2.1.1.1"><abs id="S5.E3.m1.5.5.5.2.1.1.2.1.cmml" xref="S5.E3.m1.5.5.5.2.1.1.1.2"></abs><apply id="S5.E3.m1.5.5.5.2.1.1.1.1.cmml" xref="S5.E3.m1.5.5.5.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.5.5.5.2.1.1.1.1.1.cmml" xref="S5.E3.m1.5.5.5.2.1.1.1.1">subscript</csymbol><ci id="S5.E3.m1.5.5.5.2.1.1.1.1.2.cmml" xref="S5.E3.m1.5.5.5.2.1.1.1.1.2">𝑇</ci><ci id="S5.E3.m1.5.5.5.2.1.1.1.1.3.cmml" xref="S5.E3.m1.5.5.5.2.1.1.1.1.3">𝑎</ci></apply></apply><apply id="S5.E3.m1.6.6.6.3.2.2.2.cmml" xref="S5.E3.m1.6.6.6.3.2.2.1"><abs id="S5.E3.m1.6.6.6.3.2.2.2.1.cmml" xref="S5.E3.m1.6.6.6.3.2.2.1.2"></abs><apply id="S5.E3.m1.6.6.6.3.2.2.1.1.cmml" xref="S5.E3.m1.6.6.6.3.2.2.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.6.6.6.3.2.2.1.1.1.cmml" xref="S5.E3.m1.6.6.6.3.2.2.1.1">subscript</csymbol><ci id="S5.E3.m1.6.6.6.3.2.2.1.1.2.cmml" xref="S5.E3.m1.6.6.6.3.2.2.1.1.2">𝑇</ci><ci id="S5.E3.m1.6.6.6.3.2.2.1.1.3.cmml" xref="S5.E3.m1.6.6.6.3.2.2.1.1.3">𝑏</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.9c">\operatorname{TEDS}\left(T_{a},T_{b}\right)=1-\frac{\operatorname{EditDist}\left(T_{a},T_{b}\right)}{\max\left(\left|T_{a}\right|,\left|T_{b}\right|\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.4" class="ltx_p">where <math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="T_{a}" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><msub id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml"><mi id="S5.SS3.p3.1.m1.1.1.2" xref="S5.SS3.p3.1.m1.1.1.2.cmml">T</mi><mi id="S5.SS3.p3.1.m1.1.1.3" xref="S5.SS3.p3.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><apply id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.1.m1.1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p3.1.m1.1.1.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2">𝑇</ci><ci id="S5.SS3.p3.1.m1.1.1.3.cmml" xref="S5.SS3.p3.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">T_{a}</annotation></semantics></math> and <math id="S5.SS3.p3.2.m2.1" class="ltx_Math" alttext="T_{b}" display="inline"><semantics id="S5.SS3.p3.2.m2.1a"><msub id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml"><mi id="S5.SS3.p3.2.m2.1.1.2" xref="S5.SS3.p3.2.m2.1.1.2.cmml">T</mi><mi id="S5.SS3.p3.2.m2.1.1.3" xref="S5.SS3.p3.2.m2.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><apply id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.2.m2.1.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p3.2.m2.1.1.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2">𝑇</ci><ci id="S5.SS3.p3.2.m2.1.1.3.cmml" xref="S5.SS3.p3.2.m2.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">T_{b}</annotation></semantics></math> represent tables in tree structure HTML format. EditDist denotes the tree-edit distance, and <math id="S5.SS3.p3.3.m3.1" class="ltx_Math" alttext="|T|" display="inline"><semantics id="S5.SS3.p3.3.m3.1a"><mrow id="S5.SS3.p3.3.m3.1.2.2" xref="S5.SS3.p3.3.m3.1.2.1.cmml"><mo stretchy="false" id="S5.SS3.p3.3.m3.1.2.2.1" xref="S5.SS3.p3.3.m3.1.2.1.1.cmml">|</mo><mi id="S5.SS3.p3.3.m3.1.1" xref="S5.SS3.p3.3.m3.1.1.cmml">T</mi><mo stretchy="false" id="S5.SS3.p3.3.m3.1.2.2.2" xref="S5.SS3.p3.3.m3.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.1b"><apply id="S5.SS3.p3.3.m3.1.2.1.cmml" xref="S5.SS3.p3.3.m3.1.2.2"><abs id="S5.SS3.p3.3.m3.1.2.1.1.cmml" xref="S5.SS3.p3.3.m3.1.2.2.1"></abs><ci id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.1c">|T|</annotation></semantics></math> represents the number of nodes in <math id="S5.SS3.p3.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S5.SS3.p3.4.m4.1a"><mi id="S5.SS3.p3.4.m4.1.1" xref="S5.SS3.p3.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.4.m4.1b"><ci id="S5.SS3.p3.4.m4.1.1.cmml" xref="S5.SS3.p3.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.4.m4.1c">T</annotation></semantics></math>.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Quantitative Analysis</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p"><span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_bold">Structure.</span> As shown in Tab. <a href="#S5.T2" title="Table 2 ‣ 5.4 Quantitative Analysis ‣ 5 Experimental Results ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, TableFormer outperforms all SOTA methods across different datasets by a large margin for predicting the table structure from an image.
All the more, our model outperforms pre-trained methods.
During the evaluation we do not apply any table filtering.
We also provide our baseline results on the SynthTabNet dataset. It has been observed that large tables (e.g. tables that occupy half of the page or more) yield poor predictions. We attribute this issue to the image resizing during the pre-processing step, that produces downsampled images with indistinguishable features.
This problem can be addressed by treating such big tables with a separate model which accepts a large input image size.
</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T2.1.1.1.2.1" class="ltx_text">Model</span></td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">TEDS</td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<th id="S5.T2.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_center">Dataset</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_center">Simple</td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_align_center">Complex</td>
<td id="S5.T2.1.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center">All</td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<th id="S5.T2.1.3.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">EDD</td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">PTN</td>
<td id="S5.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">91.1</td>
<td id="S5.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">88.7</td>
<td id="S5.T2.1.3.3.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">89.9</td>
</tr>
<tr id="S5.T2.1.4.4" class="ltx_tr">
<th id="S5.T2.1.4.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T2.1.4.4.2" class="ltx_td ltx_align_center">GTE</td>
<td id="S5.T2.1.4.4.3" class="ltx_td ltx_align_center">PTN</td>
<td id="S5.T2.1.4.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.4.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.4.4.6" class="ltx_td ltx_nopad_r ltx_align_center">93.01</td>
</tr>
<tr id="S5.T2.1.5.5" class="ltx_tr">
<th id="S5.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span></th>
<td id="S5.T2.1.5.5.2" class="ltx_td ltx_align_center">TableFormer</td>
<td id="S5.T2.1.5.5.3" class="ltx_td ltx_align_center">PTN</td>
<td id="S5.T2.1.5.5.4" class="ltx_td ltx_align_center">98.5</td>
<td id="S5.T2.1.5.5.5" class="ltx_td ltx_align_center">95.0</td>
<td id="S5.T2.1.5.5.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.5.5.6.1" class="ltx_text ltx_font_bold">96.75</span></td>
</tr>
<tr id="S5.T2.1.6.6" class="ltx_tr">
<th id="S5.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span></th>
<td id="S5.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">EDD</td>
<td id="S5.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">FTN</td>
<td id="S5.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">88.4</td>
<td id="S5.T2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">92.08</td>
<td id="S5.T2.1.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">90.6</td>
</tr>
<tr id="S5.T2.1.7.7" class="ltx_tr">
<th id="S5.T2.1.7.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T2.1.7.7.2" class="ltx_td ltx_align_center">GTE</td>
<td id="S5.T2.1.7.7.3" class="ltx_td ltx_align_center">FTN</td>
<td id="S5.T2.1.7.7.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.7.7.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.7.7.6" class="ltx_td ltx_nopad_r ltx_align_center">87.14</td>
</tr>
<tr id="S5.T2.1.8.8" class="ltx_tr">
<th id="S5.T2.1.8.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T2.1.8.8.2" class="ltx_td ltx_align_center">GTE (FT)</td>
<td id="S5.T2.1.8.8.3" class="ltx_td ltx_align_center">FTN</td>
<td id="S5.T2.1.8.8.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.8.8.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.8.8.6" class="ltx_td ltx_nopad_r ltx_align_center">91.02</td>
</tr>
<tr id="S5.T2.1.9.9" class="ltx_tr">
<th id="S5.T2.1.9.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T2.1.9.9.2" class="ltx_td ltx_align_center">TableFormer</td>
<td id="S5.T2.1.9.9.3" class="ltx_td ltx_align_center">FTN</td>
<td id="S5.T2.1.9.9.4" class="ltx_td ltx_align_center">97.5</td>
<td id="S5.T2.1.9.9.5" class="ltx_td ltx_align_center">96.0</td>
<td id="S5.T2.1.9.9.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.9.9.6.1" class="ltx_text ltx_font_bold">96.8</span></td>
</tr>
<tr id="S5.T2.1.10.10" class="ltx_tr">
<th id="S5.T2.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span></th>
<td id="S5.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_tt">EDD</td>
<td id="S5.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_tt">TB</td>
<td id="S5.T2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_tt">86.0</td>
<td id="S5.T2.1.10.10.5" class="ltx_td ltx_align_center ltx_border_tt">-</td>
<td id="S5.T2.1.10.10.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">86.0</td>
</tr>
<tr id="S5.T2.1.11.11" class="ltx_tr">
<th id="S5.T2.1.11.11.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T2.1.11.11.2" class="ltx_td ltx_align_center">TableFormer</td>
<td id="S5.T2.1.11.11.3" class="ltx_td ltx_align_center">TB</td>
<td id="S5.T2.1.11.11.4" class="ltx_td ltx_align_center">89.6</td>
<td id="S5.T2.1.11.11.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.11.11.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.11.11.6.1" class="ltx_text ltx_font_bold">89.6</span></td>
</tr>
<tr id="S5.T2.1.12.12" class="ltx_tr">
<th id="S5.T2.1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_tt"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span></th>
<td id="S5.T2.1.12.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">TableFormer</td>
<td id="S5.T2.1.12.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">STN</td>
<td id="S5.T2.1.12.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">96.9</td>
<td id="S5.T2.1.12.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">95.7</td>
<td id="S5.T2.1.12.12.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_tt">96.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span> Structure results on PubTabNet (PTN), FinTabNet (FTN), TableBank (TB) and SynthTabNet (STN). 
<br class="ltx_break">FT: Model was trained on PubTabNet then finetuned.</figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Cell Detection.</span> Like any object detector, our <span id="S5.SS4.p2.1.2" class="ltx_text ltx_font_italic">Cell BBox Detector</span> provides bounding boxes that can be improved with post-processing during inference.
We make use of the grid-like structure of tables to refine the predictions. A detailed explanation on the post-processing is available in the supplementary material. As shown in Tab. <a href="#S5.T3" title="Table 3 ‣ 5.4 Quantitative Analysis ‣ 5 Experimental Results ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we evaluate our <span id="S5.SS4.p2.1.3" class="ltx_text ltx_font_italic">Cell BBox Decoder</span> accuracy for cells with a class label of ‘content’ only using the PASCAL VOC mAP metric for pre-processing and post-processing. Note that we do not have post-processing results for SynthTabNet as images are only provided. To compare the performance of our proposed approach, we’ve integrated TableFormer’s <span id="S5.SS4.p2.1.4" class="ltx_text ltx_font_italic">Cell BBox Decoder</span> into EDD architecture. As mentioned previously, the Structure Decoder provides the <span id="S5.SS4.p2.1.5" class="ltx_text ltx_font_italic">Cell BBox Decoder</span> with the features needed to predict the bounding box predictions. Therefore, the accuracy of the <span id="S5.SS4.p2.1.6" class="ltx_text ltx_font_italic">Structure Decoder</span> directly influences the accuracy of the <span id="S5.SS4.p2.1.7" class="ltx_text ltx_font_italic">Cell BBox Decoder</span>. If the <span id="S5.SS4.p2.1.8" class="ltx_text ltx_font_italic">Structure Decoder</span> predicts an extra column, this will result in an extra column of predicted bounding boxes.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP (PP)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">EDD+BBox</td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">PubTabNet</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">79.2</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">82.7</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_center">TableFormer</td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center">PubTabNet</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.1.3.2.3.1" class="ltx_text ltx_font_bold">82.1</span></td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T3.1.3.2.4.1" class="ltx_text ltx_font_bold">86.8</span></td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<td id="S5.T3.1.4.3.1" class="ltx_td ltx_align_center">TableFormer</td>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center">SynthTabNet</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center">87.7</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span> Cell Bounding Box detection results on PubTabNet, and FinTabNet. PP: Post-processing. </figcaption>
</figure>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">Cell Content.</span> In this section, we evaluate the entire pipeline of recovering a table with content. Here we put our approach to test by capitalizing on extracting content from the PDF cells rather than decoding from images. Tab. <a href="#S5.T4" title="Table 4 ‣ 5.4 Quantitative Analysis ‣ 5 Experimental Results ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the TEDs score of HTML code representing the structure of the table along with the content inserted in the data cell and compared with the ground-truth. Our method achieved a <span id="S5.SS4.p3.1.2" class="ltx_text ltx_font_bold">5.3%</span> increase over the state-of-the-art, and commercial solutions. We believe our scores would be higher if the HTML ground-truth matched the extracted PDF cell content. Unfortunately, there are small discrepancies such as spacings around words or special characters with various unicode representations.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.2.1" class="ltx_tr">
<th id="S5.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S5.T4.1.2.1.1.1" class="ltx_text">Model</span></th>
<th id="S5.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">TEDS</th>
</tr>
<tr id="S5.T4.1.3.2" class="ltx_tr">
<th id="S5.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Simple</th>
<th id="S5.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Complex</th>
<th id="S5.T4.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.4.1" class="ltx_tr">
<td id="S5.T4.1.4.1.1" class="ltx_td ltx_align_center ltx_border_t">Tabula</td>
<td id="S5.T4.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t">78.0</td>
<td id="S5.T4.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t">57.8</td>
<td id="S5.T4.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t">67.9</td>
</tr>
<tr id="S5.T4.1.5.2" class="ltx_tr">
<td id="S5.T4.1.5.2.1" class="ltx_td ltx_align_center">Traprange</td>
<td id="S5.T4.1.5.2.2" class="ltx_td ltx_align_center">60.8</td>
<td id="S5.T4.1.5.2.3" class="ltx_td ltx_align_center">49.9</td>
<td id="S5.T4.1.5.2.4" class="ltx_td ltx_align_center">55.4</td>
</tr>
<tr id="S5.T4.1.6.3" class="ltx_tr">
<td id="S5.T4.1.6.3.1" class="ltx_td ltx_align_center">Camelot</td>
<td id="S5.T4.1.6.3.2" class="ltx_td ltx_align_center">80.0</td>
<td id="S5.T4.1.6.3.3" class="ltx_td ltx_align_center">66.0</td>
<td id="S5.T4.1.6.3.4" class="ltx_td ltx_align_center">73.0</td>
</tr>
<tr id="S5.T4.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_center">Acrobat<sup id="S5.T4.1.1.1.1" class="ltx_sup"><span id="S5.T4.1.1.1.1.1" class="ltx_text ltx_font_italic">®</span></sup> Pro</td>
<td id="S5.T4.1.1.2" class="ltx_td ltx_align_center">68.9</td>
<td id="S5.T4.1.1.3" class="ltx_td ltx_align_center">61.8</td>
<td id="S5.T4.1.1.4" class="ltx_td ltx_align_center">65.3</td>
</tr>
<tr id="S5.T4.1.7.4" class="ltx_tr">
<td id="S5.T4.1.7.4.1" class="ltx_td ltx_align_center">EDD</td>
<td id="S5.T4.1.7.4.2" class="ltx_td ltx_align_center">91.2</td>
<td id="S5.T4.1.7.4.3" class="ltx_td ltx_align_center">85.4</td>
<td id="S5.T4.1.7.4.4" class="ltx_td ltx_align_center">88.3</td>
</tr>
<tr id="S5.T4.1.8.5" class="ltx_tr">
<td id="S5.T4.1.8.5.1" class="ltx_td ltx_align_center">TableFormer</td>
<td id="S5.T4.1.8.5.2" class="ltx_td ltx_align_center">95.4</td>
<td id="S5.T4.1.8.5.3" class="ltx_td ltx_align_center">90.1</td>
<td id="S5.T4.1.8.5.4" class="ltx_td ltx_align_center"><span id="S5.T4.1.8.5.4.1" class="ltx_text ltx_font_bold">93.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span> Results of structure with content retrieved using cell detection on PubTabNet. In all cases the input is PDF documents with cropped tables.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2203.01017/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="390" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> One of the benefits of TableFormer is that it is language agnostic, as an example, the left part of the illustration demonstrates TableFormer predictions on previously unseen language (Japanese). Additionally, we see that TableFormer is robust to variability in style and content, right side of the illustration shows the example of the TableFormer prediction from the FinTabNet dataset.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2203.01017/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="380" height="52" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span> An example of TableFormer predictions (bounding boxes and structure) from generated SynthTabNet table.</figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Qualitative Analysis</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">We showcase several visualizations for the different components of our network on various <span id="S5.SS5.p1.1.1" class="ltx_text ltx_font_italic">“complex”</span> tables within datasets presented in this work in Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.4 Quantitative Analysis ‣ 5 Experimental Results ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Fig. <a href="#S5.F6" title="Figure 6 ‣ 5.4 Quantitative Analysis ‣ 5 Experimental Results ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>
As it is shown, our model is able to predict bounding boxes for all table cells, even for the empty ones. Additionally, our post-processing techniques can extract the cell content by matching the predicted bounding boxes to the PDF cells based on their overlap and spatial proximity.
The left part of Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.4 Quantitative Analysis ‣ 5 Experimental Results ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates also the adaptability of our method to any language, as it can successfully extract Japanese text, although the training set contains only English content.
We provide more visualizations including the intermediate steps in the supplementary material.
Overall these illustrations justify the versatility of our method across a diverse range of table appearances and content type.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Work &amp; Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we presented TableFormer an end-to-end transformer based approach to predict table structures and bounding boxes of cells from an image.
This approach enables us to recreate the table structure, and extract the cell content from PDF or OCR by using bounding boxes.
Additionally, it provides the versatility required in real-world scenarios when dealing with various types of PDF documents, and languages.
Furthermore, our method outperforms all state-of-the-arts with a wide margin.
Finally, we introduce “SynthTabNet” a challenging synthetically generated dataset that reinforces missing characteristics from other datasets.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
editors, </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2020</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 213–229, Cham, 2020.
Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanxuan Yin, and Xian-Ling Mao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Complicated table structure recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.04729</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Bertrand Couasnon and Aurelie Lemaitre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Recognition of Tables and Forms</span><span id="bib.bib3.3.2" class="ltx_text" style="font-size:90%;">, pages 647–677.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">Springer London, London, 2014.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang,
Florian Kleber, and Eva-Maria Lang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">ICDAR 2019 Competition on Table Detection and Recognition (cTDaR),
Apr. 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">http://sac.founderit.com/.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Basilios Gatos, Dimitrios Danatsas, Ioannis Pratikakis, and Stavros J
Perantonis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Automatic table detection in document images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Pattern Recognition and Image
Analysis</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 609–618. Springer, 2005.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Icdar 2013 table competition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2013 12th International Conference on Document Analysis and
Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 1449–1453, 2013.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
EA Green and M Krishnamoorthy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Recognition of tables using table grammars. procs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Symposium on Document Analysis and Recognition (SDAIR’95)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">,
pages 261–277.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Khurram Azeem Hashmi, Alain Pagani, Marcus Liwicki, Didier Stricker, and
Muhammad Zeshan Afzal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Castabdetectors: Cascade network for table detection in document
images with recursive feature pyramid and switchable atrous convolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Imaging</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 7(10), 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, Oct 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yelin He, X. Qi, Jiaquan Ye, Peng Gao, Yihao Chen, Bingcong Li, Xin Tang, and
Rong Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Pingan-vcgroup’s solution for icdar 2021 competition on scientific
table image recognition to latex.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, abs/2105.01846, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Jianying Hu, Ramanujan S Kashi, Daniel P Lopresti, and Gordon Wilfong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Medium-independent table detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Document Recognition and Retrieval VII</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, volume 3967, pages
291–302. International Society for Optics and Photonics, 1999.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Matthew Hurst.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">A constraint-based approach to table structure derivation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Seventh International Conference on
Document Analysis and Recognition - Volume 2</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, ICDAR ’03, page 911, USA,
2003. IEEE Computer Society.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Thotreingam Kasar, Philippine Barlas, Sebastien Adam, Clément Chatelain,
and Thierry Paquet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Learning to detect tables in scanned document images using line
information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2013 12th International Conference on Document Analysis and
Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 1185–1189. IEEE, 2013.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Pratik Kayal, Mrinal Anand, Harsh Desai, and Mayank Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Icdar 2021 competition on scientific table image recognition to
latex, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Harold W Kuhn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">The hungarian method for the assignment problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Naval research logistics quarterly</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2(1-2):83–97, 1955.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li,
Yejin Choi, Alexander C. Berg, and Tamara L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Babytalk: Understanding and generating simple image descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">,
35(12):2891–2903, 2013.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Tablebank: A benchmark dataset for table detection and recognition,
2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Yiren Li, Zheng Huang, Junchi Yan, Yi Zhou, Fan Ye, and Xianhui Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Gfte: Graph-based financial table extraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In Alberto Del Bimbo, Rita Cucchiara, Stan Sclaroff, Giovanni Maria
Farinella, Tao Mei, Marco Bertini, Hugo Jair Escalante, and Roberto Vezzani,
editors, </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition. ICPR International Workshops and
Challenges</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 644–658, Cham, 2021. Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed
Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and
Peter Staar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Robust pdf document conversion using recurrent neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">,
35(17):15137–15145, May 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Rujiao Long, Wen Wang, Nan Xue, Feiyu Gao, Zhibo Yang, Yongpan Wang, and
Gui-Song Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Parsing table structures in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 944–952, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Shubham Singh Paliwal, D Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh
Vig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Tablenet: Deep learning model for end-to-end table detection and
tabular data extraction from scanned document images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 128–133. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Pytorch: An imperative style, high-performance deep learning library.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural
Information Processing Systems 32</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 8024–8035. Curran Associates,
Inc., 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita
Sultanpure.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Cascadetabnet: An approach for end to end table detection and
structure recognition from image-based documents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 572–573, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Rethinking table recognition using graph neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 142–147. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and
Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Generalized intersection over union: A metric and a loss for bounding
box regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 658–666, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Deepdesrt: Deep learning for detection and structure recognition of
tables in document images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 14th IAPR International Conference on Document Analysis
and Recognition (ICDAR)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, volume 01, pages 1162–1167, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Deepdesrt: Deep learning for detection and structure recognition of
tables in document images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 14th IAPR international conference on document analysis
and recognition (ICDAR)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, volume 1, pages 1162–1167. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Faisal Shafait and Ray Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Table detection in heterogeneous documents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 9th IAPR International Workshop on
Document Analysis Systems</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 65–72, 2010.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Shoaib Ahmed Siddiqui, Imran Ali Fateh, Syed Tahseen Raza Rizvi, Andreas
Dengel, and Sheraz Ahmed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Deeptabstr: Deep learning based table structure recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 1403–1409. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Corpus conversion service: A machine learning platform to ingest
documents at scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 24th ACM SIGKDD</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, KDD ’18, pages 774–782,
New York, NY, USA, 2018. ACM.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S.
Vishwanathan, and R. Garnett, editors, </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information
Processing Systems 30</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 5998–6008. Curran Associates, Inc., 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Show and tell: A neural image caption generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, June 2015.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Wenyuan Xue, Qingyong Li, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Res2tim: reconstruct syntactic structures from table images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 749–755. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Wenyuan Xue, Baosheng Yu, Wen Wang, Dacheng Tao, and Qingyong Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Tgrnet: A table graph reconstruction network for table structure
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.10598</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Image captioning with semantic attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 4651–4659, 2016.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Xinyi Zheng, Doug Burdick, Lucian Popa, Peter Zhong, and Nancy Xin Ru Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Global table extractor (gte): A framework for joint table
identification and cell structure recognition using visual context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Winter Conference for Applications in Computer Vision (WACV)</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Image-based table recognition: Data, model, and evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
editors, </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2020</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 564–580, Cham, 2020.
Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Publaynet: Largest dataset ever for document layout analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 1015–1022, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">{strip}</span>
<p id="p1.2" class="ltx_p ltx_align_center"><span id="p1.2.1" class="ltx_text ltx_font_bold" style="font-size:120%;">TableFormer: Table Structure Understanding with Transformers 
<br class="ltx_break"></span><span id="p1.2.2" class="ltx_text" style="font-size:150%;">Supplementary Material</span></p>
</div>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Details on the datasets</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Data preparation</h3>

<figure id="S1.F7" class="ltx_figure"><img src="/html/2203.01017/assets/x7.png" id="S1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="386" height="63" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span> Distribution of the tables across different dimensions per dataset.
Simple vs complex tables per dataset and split, strict vs non strict html structures per dataset
and table complexity, missing bboxes per dataset and table complexity.</figcaption>
</figure>
<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">As a first step of our data preparation process, we have calculated statistics over the datasets across the following dimensions:
(1) table size measured in the number of rows and columns,
(2) complexity of the table,
(3) strictness of the provided HTML structure and
(4) completeness (i.e. no omitted bounding boxes).
A table is considered to be simple if it does not contain row spans or column spans.
Additionally, a table has a strict HTML structure if every row has the same number of columns after taking into account any row or column spans.
Therefore a strict HTML structure looks always rectangular.
However, HTML is a lenient encoding format, i.e. tables with rows of different sizes might still be regarded as correct due to implicit display rules.
These implicit rules leave room for ambiguity, which we want to avoid.
As such, we prefer to have ”strict” tables, i.e. tables where every row has exactly the same length.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">We have developed a technique that tries to derive a missing bounding box out of its neighbors.
As a first step, we use the annotation data to generate the most fine-grained grid that covers the table structure.
In case of strict HTML tables, all grid squares are associated with some table cell and in the presence of table spans a cell extends across multiple grid squares.
When enough bounding boxes are known for a rectangular table, it is possible to compute the geometrical border lines between the grid rows and columns.
Eventually this information is used to generate the missing bounding boxes.
Additionally, the existence of unused grid squares indicates that the table rows have unequal number of columns and the overall structure is non-strict.
The generation of missing bounding boxes for non-strict HTML tables is ambiguous and therefore quite challenging.
Thus, we have decided to simply discard those tables.
In case of PubTabNet we have computed missing bounding boxes for 48% of the simple and 69% of the complex tables.
Regarding FinTabNet, 68% of the simple and 98% of the complex tables require the generation of bounding boxes.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Figure <a href="#S1.F7" title="Figure 7 ‣ 1.1 Data preparation ‣ 1 Details on the datasets ‣ TableFormer: Table Structure Understanding with Transformers." class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the distribution of the tables across different dimensions per
dataset.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Synthetic datasets</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Aiming to train and evaluate our models in a broader spectrum of table data we have synthesized four types of datasets.
Each one contains tables with different appearances in regard to their size, structure, style and content.
Every synthetic dataset contains 150k examples, summing up to 600k synthetic examples.
All datasets are divided into Train, Test and Val splits (80%, 10%, 10%).</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">The process of generating a synthetic dataset can be decomposed into the following steps:</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">1. Prepare styling and content templates:
The styling templates have been manually designed and organized into groups of scope specific appearances (e.g. financial data, marketing data, etc.)
Additionally, we have prepared curated collections of content templates by extracting the most frequently used terms out of non-synthetic datasets (e.g. PubTabNet, FinTabNet, etc.).</p>
</div>
<div id="S1.SS2.p4" class="ltx_para">
<p id="S1.SS2.p4.1" class="ltx_p">2. Generate table structures:
The structure of each synthetic dataset assumes a horizontal table header which potentially spans over multiple rows and a table body that may contain a combination of row spans and column spans.
However, spans are not allowed to cross the header - body boundary.
The table structure is described by the parameters:
Total number of table rows and columns, number of header rows, type of spans (header only spans, row only spans, column only spans, both row and column spans), maximum span size and the ratio of the table area covered by spans.</p>
</div>
<div id="S1.SS2.p5" class="ltx_para">
<p id="S1.SS2.p5.1" class="ltx_p">3. Generate content:
Based on the dataset <span id="S1.SS2.p5.1.1" class="ltx_text ltx_font_italic">theme</span>, a set of suitable content templates is chosen first.
Then, this content can be combined with purely random text to produce the synthetic content.</p>
</div>
<div id="S1.SS2.p6" class="ltx_para">
<p id="S1.SS2.p6.1" class="ltx_p">4. Apply styling templates:
Depending on the domain of the synthetic dataset, a set of styling templates is first manually selected.
Then, a style is randomly selected to format the appearance of the synthesized table.</p>
</div>
<div id="S1.SS2.p7" class="ltx_para">
<p id="S1.SS2.p7.1" class="ltx_p">5. Render the complete tables:
The synthetic table is finally rendered by a web browser engine to generate the bounding boxes for each table cell.
A batching technique is utilized to optimize the runtime overhead of the rendering process.</p>
</div>
</section>
</section>
<section id="S2a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Prediction post-processing for PDF documents</h2>

<div id="S2a.p1" class="ltx_para">
<p id="S2a.p1.1" class="ltx_p">Although TableFormer can predict the table structure and the bounding boxes for tables recognized inside PDF documents,
this is not enough when a full reconstruction of the original table is required.
This happens mainly due the following reasons:</p>
</div>
<div id="S2a.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">TableFormer output does not include the table cell content.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">There are occasional inaccuracies in the predictions of the bounding boxes.</p>
</div>
</li>
</ul>
</div>
<div id="S2a.p3" class="ltx_para">
<p id="S2a.p3.1" class="ltx_p">However, it is possible to mitigate those limitations by combining the TableFormer predictions with the information already present inside a programmatic PDF document.
More specifically, PDF documents can be seen as a sequence of PDF cells where each cell is described by its content and bounding box.
If we are able to associate the PDF cells with the predicted table cells, we can directly link the PDF cell content to the table cell structure and
use the PDF bounding boxes to correct misalignments in the predicted table cell bounding boxes.</p>
</div>
<div id="S2a.p4" class="ltx_para">
<p id="S2a.p4.1" class="ltx_p">Here is a step-by-step description of the prediction post-processing:</p>
</div>
<div id="S2a.p5" class="ltx_para">
<p id="S2a.p5.1" class="ltx_p">1. Get the minimal grid dimensions - number of rows and columns for the predicted table structure.
This represents the most granular grid for the underlying table structure.</p>
</div>
<div id="S2a.p6" class="ltx_para">
<p id="S2a.p6.1" class="ltx_p">2. Generate pair-wise matches between the bounding boxes of the PDF cells and the predicted cells.
The Intersection Over Union (IOU) metric is used to evaluate the quality of the matches.</p>
</div>
<div id="S2a.p7" class="ltx_para">
<p id="S2a.p7.1" class="ltx_p">3. Use a carefully selected IOU threshold to designate the matches as “good” ones and “bad” ones.</p>
</div>
<div id="S2a.p8" class="ltx_para">
<p id="S2a.p8.1" class="ltx_p">3.a. If all IOU scores in a column are below the threshold, discard all predictions (structure and bounding boxes) for that column.</p>
</div>
<div id="S2a.p9" class="ltx_para">
<p id="S2a.p9.1" class="ltx_p">4. Find the best-fitting content alignment for the predicted cells with good IOU per each column.
The alignment of the column can be identified by the following formula:
</p>
</div>
<div id="S2a.p10" class="ltx_para">
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.42" class="ltx_Math" alttext="\begin{split}alignment=\operatorname*{arg\,min}_{c}\{D_{c}\}\\
D_{c}=max\{x_{c}\}-min\{x_{c}\}\end{split}" display="block"><semantics id="S2.E4.m1.42a"><mtable displaystyle="true" rowspacing="0pt" id="S2.E4.m1.42.42.8" xref="S2.E4.m1.38.38.4.cmml"><mtr id="S2.E4.m1.42.42.8a" xref="S2.E4.m1.38.38.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S2.E4.m1.42.42.8b" xref="S2.E4.m1.38.38.4.cmml"><mrow id="S2.E4.m1.40.40.6.36.18.18" xref="S2.E4.m1.38.38.4.cmml"><mrow id="S2.E4.m1.40.40.6.36.18.18.19" xref="S2.E4.m1.38.38.4.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.40.40.6.36.18.18.19.1" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.2.2.2.2.2.2" xref="S2.E4.m1.2.2.2.2.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.40.40.6.36.18.18.19.1a" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.3.3.3.3.3.3" xref="S2.E4.m1.3.3.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.40.40.6.36.18.18.19.1b" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.4.4.4.4.4.4" xref="S2.E4.m1.4.4.4.4.4.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.40.40.6.36.18.18.19.1c" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.5.5.5.5.5.5" xref="S2.E4.m1.5.5.5.5.5.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.40.40.6.36.18.18.19.1d" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.6.6.6.6.6.6" xref="S2.E4.m1.6.6.6.6.6.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.40.40.6.36.18.18.19.1e" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.7.7.7.7.7.7" xref="S2.E4.m1.7.7.7.7.7.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.40.40.6.36.18.18.19.1f" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.8.8.8.8.8.8" xref="S2.E4.m1.8.8.8.8.8.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.40.40.6.36.18.18.19.1g" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.9.9.9.9.9.9" xref="S2.E4.m1.9.9.9.9.9.9.cmml">t</mi></mrow><mo id="S2.E4.m1.10.10.10.10.10.10" xref="S2.E4.m1.10.10.10.10.10.10.cmml">=</mo><mrow id="S2.E4.m1.40.40.6.36.18.18.18.2" xref="S2.E4.m1.38.38.4.cmml"><munder id="S2.E4.m1.39.39.5.35.17.17.17.1.1" xref="S2.E4.m1.38.38.4.cmml"><mrow id="S2.E4.m1.11.11.11.11.11.11" xref="S2.E4.m1.11.11.11.11.11.11.cmml"><mi id="S2.E4.m1.11.11.11.11.11.11.2" xref="S2.E4.m1.11.11.11.11.11.11.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S2.E4.m1.11.11.11.11.11.11.1" xref="S2.E4.m1.11.11.11.11.11.11.1.cmml">​</mo><mi id="S2.E4.m1.11.11.11.11.11.11.3" xref="S2.E4.m1.11.11.11.11.11.11.3.cmml">min</mi></mrow><mi id="S2.E4.m1.12.12.12.12.12.12.1" xref="S2.E4.m1.12.12.12.12.12.12.1.cmml">c</mi></munder><mo id="S2.E4.m1.40.40.6.36.18.18.18.2a" xref="S2.E4.m1.38.38.4a.cmml">⁡</mo><mrow id="S2.E4.m1.40.40.6.36.18.18.18.2.2" xref="S2.E4.m1.38.38.4.cmml"><mo stretchy="false" id="S2.E4.m1.13.13.13.13.13.13" xref="S2.E4.m1.38.38.4a.cmml">{</mo><msub id="S2.E4.m1.40.40.6.36.18.18.18.2.2.1" xref="S2.E4.m1.38.38.4.cmml"><mi id="S2.E4.m1.14.14.14.14.14.14" xref="S2.E4.m1.14.14.14.14.14.14.cmml">D</mi><mi id="S2.E4.m1.15.15.15.15.15.15.1" xref="S2.E4.m1.15.15.15.15.15.15.1.cmml">c</mi></msub><mo stretchy="false" id="S2.E4.m1.16.16.16.16.16.16" xref="S2.E4.m1.38.38.4a.cmml">}</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S2.E4.m1.42.42.8c" xref="S2.E4.m1.38.38.4.cmml"><mtd class="ltx_align_right" columnalign="right" id="S2.E4.m1.42.42.8d" xref="S2.E4.m1.38.38.4.cmml"><mrow id="S2.E4.m1.42.42.8.38.20.20" xref="S2.E4.m1.38.38.4.cmml"><msub id="S2.E4.m1.42.42.8.38.20.20.21" xref="S2.E4.m1.38.38.4.cmml"><mi id="S2.E4.m1.17.17.17.1.1.1" xref="S2.E4.m1.17.17.17.1.1.1.cmml">D</mi><mi id="S2.E4.m1.18.18.18.2.2.2.1" xref="S2.E4.m1.18.18.18.2.2.2.1.cmml">c</mi></msub><mo id="S2.E4.m1.19.19.19.3.3.3" xref="S2.E4.m1.19.19.19.3.3.3.cmml">=</mo><mrow id="S2.E4.m1.42.42.8.38.20.20.20" xref="S2.E4.m1.38.38.4.cmml"><mrow id="S2.E4.m1.41.41.7.37.19.19.19.1" xref="S2.E4.m1.38.38.4.cmml"><mi id="S2.E4.m1.20.20.20.4.4.4" xref="S2.E4.m1.20.20.20.4.4.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.41.41.7.37.19.19.19.1.2" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.21.21.21.5.5.5" xref="S2.E4.m1.21.21.21.5.5.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.41.41.7.37.19.19.19.1.2a" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.22.22.22.6.6.6" xref="S2.E4.m1.22.22.22.6.6.6.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.41.41.7.37.19.19.19.1.2b" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mrow id="S2.E4.m1.41.41.7.37.19.19.19.1.1.1" xref="S2.E4.m1.38.38.4.cmml"><mo stretchy="false" id="S2.E4.m1.23.23.23.7.7.7" xref="S2.E4.m1.38.38.4a.cmml">{</mo><msub id="S2.E4.m1.41.41.7.37.19.19.19.1.1.1.1" xref="S2.E4.m1.38.38.4.cmml"><mi id="S2.E4.m1.24.24.24.8.8.8" xref="S2.E4.m1.24.24.24.8.8.8.cmml">x</mi><mi id="S2.E4.m1.25.25.25.9.9.9.1" xref="S2.E4.m1.25.25.25.9.9.9.1.cmml">c</mi></msub><mo stretchy="false" id="S2.E4.m1.26.26.26.10.10.10" xref="S2.E4.m1.38.38.4a.cmml">}</mo></mrow></mrow><mo id="S2.E4.m1.27.27.27.11.11.11" xref="S2.E4.m1.27.27.27.11.11.11.cmml">−</mo><mrow id="S2.E4.m1.42.42.8.38.20.20.20.2" xref="S2.E4.m1.38.38.4.cmml"><mi id="S2.E4.m1.28.28.28.12.12.12" xref="S2.E4.m1.28.28.28.12.12.12.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.42.42.8.38.20.20.20.2.2" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.29.29.29.13.13.13" xref="S2.E4.m1.29.29.29.13.13.13.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.42.42.8.38.20.20.20.2.2a" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mi id="S2.E4.m1.30.30.30.14.14.14" xref="S2.E4.m1.30.30.30.14.14.14.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.42.42.8.38.20.20.20.2.2b" xref="S2.E4.m1.38.38.4a.cmml">​</mo><mrow id="S2.E4.m1.42.42.8.38.20.20.20.2.1.1" xref="S2.E4.m1.38.38.4.cmml"><mo stretchy="false" id="S2.E4.m1.31.31.31.15.15.15" xref="S2.E4.m1.38.38.4a.cmml">{</mo><msub id="S2.E4.m1.42.42.8.38.20.20.20.2.1.1.1" xref="S2.E4.m1.38.38.4.cmml"><mi id="S2.E4.m1.32.32.32.16.16.16" xref="S2.E4.m1.32.32.32.16.16.16.cmml">x</mi><mi id="S2.E4.m1.33.33.33.17.17.17.1" xref="S2.E4.m1.33.33.33.17.17.17.1.cmml">c</mi></msub><mo stretchy="false" id="S2.E4.m1.34.34.34.18.18.18" xref="S2.E4.m1.38.38.4a.cmml">}</mo></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E4.m1.42b"><apply id="S2.E4.m1.38.38.4.cmml" xref="S2.E4.m1.42.42.8"><and id="S2.E4.m1.38.38.4a.cmml" xref="S2.E4.m1.40.40.6.36.18.18.19.1"></and><apply id="S2.E4.m1.38.38.4b.cmml" xref="S2.E4.m1.42.42.8"><eq id="S2.E4.m1.10.10.10.10.10.10.cmml" xref="S2.E4.m1.10.10.10.10.10.10"></eq><apply id="S2.E4.m1.38.38.4.6.cmml" xref="S2.E4.m1.42.42.8"><times id="S2.E4.m1.38.38.4.6.1.cmml" xref="S2.E4.m1.40.40.6.36.18.18.19.1"></times><ci id="S2.E4.m1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1">𝑎</ci><ci id="S2.E4.m1.2.2.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2.2.2">𝑙</ci><ci id="S2.E4.m1.3.3.3.3.3.3.cmml" xref="S2.E4.m1.3.3.3.3.3.3">𝑖</ci><ci id="S2.E4.m1.4.4.4.4.4.4.cmml" xref="S2.E4.m1.4.4.4.4.4.4">𝑔</ci><ci id="S2.E4.m1.5.5.5.5.5.5.cmml" xref="S2.E4.m1.5.5.5.5.5.5">𝑛</ci><ci id="S2.E4.m1.6.6.6.6.6.6.cmml" xref="S2.E4.m1.6.6.6.6.6.6">𝑚</ci><ci id="S2.E4.m1.7.7.7.7.7.7.cmml" xref="S2.E4.m1.7.7.7.7.7.7">𝑒</ci><ci id="S2.E4.m1.8.8.8.8.8.8.cmml" xref="S2.E4.m1.8.8.8.8.8.8">𝑛</ci><ci id="S2.E4.m1.9.9.9.9.9.9.cmml" xref="S2.E4.m1.9.9.9.9.9.9">𝑡</ci></apply><apply id="S2.E4.m1.36.36.2.2.cmml" xref="S2.E4.m1.42.42.8"><apply id="S2.E4.m1.36.36.2.2.2.3.cmml" xref="S2.E4.m1.42.42.8"><apply id="S2.E4.m1.35.35.1.1.1.1.1.cmml" xref="S2.E4.m1.42.42.8"><csymbol cd="ambiguous" id="S2.E4.m1.35.35.1.1.1.1.1.1.cmml" xref="S2.E4.m1.42.42.8">subscript</csymbol><apply id="S2.E4.m1.11.11.11.11.11.11.cmml" xref="S2.E4.m1.11.11.11.11.11.11"><times id="S2.E4.m1.11.11.11.11.11.11.1.cmml" xref="S2.E4.m1.11.11.11.11.11.11.1"></times><ci id="S2.E4.m1.11.11.11.11.11.11.2.cmml" xref="S2.E4.m1.11.11.11.11.11.11.2">arg</ci><ci id="S2.E4.m1.11.11.11.11.11.11.3.cmml" xref="S2.E4.m1.11.11.11.11.11.11.3">min</ci></apply><ci id="S2.E4.m1.12.12.12.12.12.12.1.cmml" xref="S2.E4.m1.12.12.12.12.12.12.1">𝑐</ci></apply><apply id="S2.E4.m1.36.36.2.2.2.2.2.1.cmml" xref="S2.E4.m1.42.42.8"><csymbol cd="ambiguous" id="S2.E4.m1.36.36.2.2.2.2.2.1.1.cmml" xref="S2.E4.m1.42.42.8">subscript</csymbol><ci id="S2.E4.m1.14.14.14.14.14.14.cmml" xref="S2.E4.m1.14.14.14.14.14.14">𝐷</ci><ci id="S2.E4.m1.15.15.15.15.15.15.1.cmml" xref="S2.E4.m1.15.15.15.15.15.15.1">𝑐</ci></apply></apply><apply id="S2.E4.m1.36.36.2.2.3.cmml" xref="S2.E4.m1.42.42.8"><csymbol cd="ambiguous" id="S2.E4.m1.36.36.2.2.3.1.cmml" xref="S2.E4.m1.40.40.6.36.18.18.19.1">subscript</csymbol><ci id="S2.E4.m1.17.17.17.1.1.1.cmml" xref="S2.E4.m1.17.17.17.1.1.1">𝐷</ci><ci id="S2.E4.m1.18.18.18.2.2.2.1.cmml" xref="S2.E4.m1.18.18.18.2.2.2.1">𝑐</ci></apply></apply></apply><apply id="S2.E4.m1.38.38.4c.cmml" xref="S2.E4.m1.42.42.8"><eq id="S2.E4.m1.19.19.19.3.3.3.cmml" xref="S2.E4.m1.19.19.19.3.3.3"></eq><share href="#S2.E4.m1.36.36.2.2.cmml" id="S2.E4.m1.38.38.4d.cmml" xref="S2.E4.m1.40.40.6.36.18.18.19.1"></share><apply id="S2.E4.m1.38.38.4.4.cmml" xref="S2.E4.m1.42.42.8"><minus id="S2.E4.m1.27.27.27.11.11.11.cmml" xref="S2.E4.m1.27.27.27.11.11.11"></minus><apply id="S2.E4.m1.37.37.3.3.1.cmml" xref="S2.E4.m1.42.42.8"><times id="S2.E4.m1.37.37.3.3.1.2.cmml" xref="S2.E4.m1.40.40.6.36.18.18.19.1"></times><ci id="S2.E4.m1.20.20.20.4.4.4.cmml" xref="S2.E4.m1.20.20.20.4.4.4">𝑚</ci><ci id="S2.E4.m1.21.21.21.5.5.5.cmml" xref="S2.E4.m1.21.21.21.5.5.5">𝑎</ci><ci id="S2.E4.m1.22.22.22.6.6.6.cmml" xref="S2.E4.m1.22.22.22.6.6.6">𝑥</ci><set id="S2.E4.m1.37.37.3.3.1.1.2.cmml" xref="S2.E4.m1.42.42.8"><apply id="S2.E4.m1.37.37.3.3.1.1.1.1.cmml" xref="S2.E4.m1.42.42.8"><csymbol cd="ambiguous" id="S2.E4.m1.37.37.3.3.1.1.1.1.1.cmml" xref="S2.E4.m1.42.42.8">subscript</csymbol><ci id="S2.E4.m1.24.24.24.8.8.8.cmml" xref="S2.E4.m1.24.24.24.8.8.8">𝑥</ci><ci id="S2.E4.m1.25.25.25.9.9.9.1.cmml" xref="S2.E4.m1.25.25.25.9.9.9.1">𝑐</ci></apply></set></apply><apply id="S2.E4.m1.38.38.4.4.2.cmml" xref="S2.E4.m1.42.42.8"><times id="S2.E4.m1.38.38.4.4.2.2.cmml" xref="S2.E4.m1.40.40.6.36.18.18.19.1"></times><ci id="S2.E4.m1.28.28.28.12.12.12.cmml" xref="S2.E4.m1.28.28.28.12.12.12">𝑚</ci><ci id="S2.E4.m1.29.29.29.13.13.13.cmml" xref="S2.E4.m1.29.29.29.13.13.13">𝑖</ci><ci id="S2.E4.m1.30.30.30.14.14.14.cmml" xref="S2.E4.m1.30.30.30.14.14.14">𝑛</ci><set id="S2.E4.m1.38.38.4.4.2.1.2.cmml" xref="S2.E4.m1.42.42.8"><apply id="S2.E4.m1.38.38.4.4.2.1.1.1.cmml" xref="S2.E4.m1.42.42.8"><csymbol cd="ambiguous" id="S2.E4.m1.38.38.4.4.2.1.1.1.1.cmml" xref="S2.E4.m1.42.42.8">subscript</csymbol><ci id="S2.E4.m1.32.32.32.16.16.16.cmml" xref="S2.E4.m1.32.32.32.16.16.16">𝑥</ci><ci id="S2.E4.m1.33.33.33.17.17.17.1.cmml" xref="S2.E4.m1.33.33.33.17.17.17.1">𝑐</ci></apply></set></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.42c">\begin{split}alignment=\operatorname*{arg\,min}_{c}\{D_{c}\}\\
D_{c}=max\{x_{c}\}-min\{x_{c}\}\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2a.p10.2" class="ltx_p">where <math id="S2a.p10.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2a.p10.1.m1.1a"><mi id="S2a.p10.1.m1.1.1" xref="S2a.p10.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2a.p10.1.m1.1b"><ci id="S2a.p10.1.m1.1.1.cmml" xref="S2a.p10.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2a.p10.1.m1.1c">c</annotation></semantics></math> is one of {left, centroid, right} and <math id="S2a.p10.2.m2.1" class="ltx_Math" alttext="x_{c}" display="inline"><semantics id="S2a.p10.2.m2.1a"><msub id="S2a.p10.2.m2.1.1" xref="S2a.p10.2.m2.1.1.cmml"><mi id="S2a.p10.2.m2.1.1.2" xref="S2a.p10.2.m2.1.1.2.cmml">x</mi><mi id="S2a.p10.2.m2.1.1.3" xref="S2a.p10.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S2a.p10.2.m2.1b"><apply id="S2a.p10.2.m2.1.1.cmml" xref="S2a.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S2a.p10.2.m2.1.1.1.cmml" xref="S2a.p10.2.m2.1.1">subscript</csymbol><ci id="S2a.p10.2.m2.1.1.2.cmml" xref="S2a.p10.2.m2.1.1.2">𝑥</ci><ci id="S2a.p10.2.m2.1.1.3.cmml" xref="S2a.p10.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2a.p10.2.m2.1c">x_{c}</annotation></semantics></math> is the x-coordinate for the corresponding point.</p>
</div>
<div id="S2a.p11" class="ltx_para">
<p id="S2a.p11.1" class="ltx_p">5. Use the alignment computed in step 4, to compute the median <math id="S2a.p11.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2a.p11.1.m1.1a"><mi id="S2a.p11.1.m1.1.1" xref="S2a.p11.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2a.p11.1.m1.1b"><ci id="S2a.p11.1.m1.1.1.cmml" xref="S2a.p11.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2a.p11.1.m1.1c">x</annotation></semantics></math>-coordinate for all table columns and the median cell size for all table cells.
The usage of median during the computations, helps to eliminate outliers caused by occasional column spans which are usually wider than the normal.</p>
</div>
<div id="S2a.p12" class="ltx_para">
<p id="S2a.p12.1" class="ltx_p">6. Snap all cells with bad IOU to their corresponding median <math id="S2a.p12.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2a.p12.1.m1.1a"><mi id="S2a.p12.1.m1.1.1" xref="S2a.p12.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2a.p12.1.m1.1b"><ci id="S2a.p12.1.m1.1.1.cmml" xref="S2a.p12.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2a.p12.1.m1.1c">x</annotation></semantics></math>-coordinates and cell sizes.</p>
</div>
<div id="S2a.p13" class="ltx_para">
<p id="S2a.p13.1" class="ltx_p">7. Generate a new set of pair-wise matches between the corrected bounding boxes and PDF cells.
This time use a modified version of the IOU metric, where the area of the intersection between the predicted and PDF cells is divided by the PDF cell area.
In case there are multiple matches for the same PDF cell, the prediction with the higher score is preferred.
This covers the cases where the PDF cells are smaller than the area of predicted or corrected prediction cells.
</p>
</div>
<div id="S2a.p14" class="ltx_para">
<p id="S2a.p14.1" class="ltx_p">8. In some rare occasions, we have noticed that TableFormer can confuse a single column as two.
When the post-processing steps are applied, this results with two predicted columns pointing to the same PDF column.
In such case we must de-duplicate the columns according to highest total column intersection score.
</p>
</div>
<div id="S2a.p15" class="ltx_para">
<p id="S2a.p15.1" class="ltx_p">9. Pick up the remaining orphan cells.
There could be cases, when after applying all the previous post-processing steps, some PDF cells could still remain without any match to predicted cells.
However, it is still possible to deduce the correct matching for an orphan PDF cell by mapping its bounding box on the geometry of the grid.
This mapping decides if the content of the orphan cell will be appended to an already matched table cell, or a new table cell should be created to match with the orphan.
</p>
</div>
<div id="S2a.p16" class="ltx_para">
<p id="S2a.p16.1" class="ltx_p">9a. Compute the top and bottom boundary of the horizontal band for each grid row (min/max <math id="S2a.p16.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2a.p16.1.m1.1a"><mi id="S2a.p16.1.m1.1.1" xref="S2a.p16.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2a.p16.1.m1.1b"><ci id="S2a.p16.1.m1.1.1.cmml" xref="S2a.p16.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2a.p16.1.m1.1c">y</annotation></semantics></math> coordinates per row).</p>
</div>
<div id="S2a.p17" class="ltx_para">
<p id="S2a.p17.1" class="ltx_p">9b. Intersect the orphan’s bounding box with the row bands, and map the cell to the closest grid row.</p>
</div>
<div id="S2a.p18" class="ltx_para">
<p id="S2a.p18.1" class="ltx_p">9c. Compute the left and right boundary of the vertical band for each grid column (min/max <math id="S2a.p18.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2a.p18.1.m1.1a"><mi id="S2a.p18.1.m1.1.1" xref="S2a.p18.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2a.p18.1.m1.1b"><ci id="S2a.p18.1.m1.1.1.cmml" xref="S2a.p18.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2a.p18.1.m1.1c">x</annotation></semantics></math> coordinates per column).</p>
</div>
<div id="S2a.p19" class="ltx_para">
<p id="S2a.p19.1" class="ltx_p">9d. Intersect the orphan’s bounding box with the column bands, and map the cell to the closest grid column.</p>
</div>
<div id="S2a.p20" class="ltx_para">
<p id="S2a.p20.1" class="ltx_p">9e. If the table cell under the identified row and column is not empty, extend its content with the content of the orphan cell.</p>
</div>
<div id="S2a.p21" class="ltx_para">
<p id="S2a.p21.1" class="ltx_p">9f. Otherwise create a new structural cell and match it wit the orphan cell.</p>
</div>
<div id="S2a.p22" class="ltx_para">
<p id="S2a.p22.1" class="ltx_p"><span id="S2a.p22.1.1" class="ltx_text ltx_font_italic">Aditional images with examples of TableFormer predictions and post-processing can be found below.</span></p>
</div>
<figure id="S2.F8" class="ltx_figure"><img src="/html/2203.01017/assets/pics/more_examples/example_10_00.png" id="S2.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="347" height="694" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span> Example of a table with multi-line header.</figcaption>
</figure>
<figure id="S2.F9" class="ltx_figure"><img src="/html/2203.01017/assets/pics/more_examples/example_21_00.png" id="S2.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="500" height="406" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span> Example of a table with big empty distance between cells.</figcaption>
</figure>
<figure id="S2.F10" class="ltx_figure"><img src="/html/2203.01017/assets/pics/more_examples/example_26_00.png" id="S2.F10.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="376" height="584" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span> Example of a complex table with empty cells.</figcaption>
</figure>
<figure id="S2.F11" class="ltx_figure"><img src="/html/2203.01017/assets/pics/more_examples/example_34_00.png" id="S2.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="470" height="489" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span> Simple table with different style and empty cells.</figcaption>
</figure>
<figure id="S2.F12" class="ltx_figure"><img src="/html/2203.01017/assets/pics/more_examples/example_01_00.png" id="S2.F12.g1" class="ltx_graphics ltx_centering ltx_img_square" width="534" height="432" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span> Simple table predictions and post processing.</figcaption>
</figure>
<figure id="S2.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.01017/assets/pics/more_examples/example_14_00.png" id="S2.F13.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="436" height="386" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.01017/assets/pics/more_examples/example_14_01.png" id="S2.F13.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="397" height="155" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span> Table predictions example on colorful table.</figcaption>
</figure>
<figure id="S2.F14" class="ltx_figure"><img src="/html/2203.01017/assets/pics/more_examples/example_32_00.png" id="S2.F14.g1" class="ltx_graphics ltx_centering ltx_img_square" width="434" height="496" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span> Example with multi-line text.</figcaption>
</figure>
<figure id="S2.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.01017/assets/pics/more_examples/example_06_00.png" id="S2.F15.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="458" height="498" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.01017/assets/pics/more_examples/example_06_01.png" id="S2.F15.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="457" height="245" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.01017/assets/pics/more_examples/example_06_02.png" id="S2.F15.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="542" height="263" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span> Example with triangular table.</figcaption>
</figure>
<figure id="S2.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.01017/assets/pics/more_examples/example_11_00.png" id="S2.F16.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="427" height="731" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2203.01017/assets/pics/more_examples/example_11_01.png" id="S2.F16.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="296" height="335" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span> Example of how post-processing helps to restore mis-aligned bounding boxes prediction artifact.</figcaption>
</figure>
<figure id="S2.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2203.01017/assets/pics/more_examples/example_02_00.png" id="S2.F17.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="231" height="487" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2203.01017/assets/pics/more_examples/example_02_01.png" id="S2.F17.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="231" height="486" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2203.01017/assets/pics/more_examples/example_02_02.png" id="S2.F17.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="227" height="489" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2203.01017/assets/pics/more_examples/example_02_03.png" id="S2.F17.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="227" height="482" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span> Example of long table. End-to-end example from initial PDF cells to prediction of bounding boxes, post processing and prediction of structure.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2203.01016" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2203.01017" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2203.01017">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2203.01017" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2203.01018" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 09:52:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
