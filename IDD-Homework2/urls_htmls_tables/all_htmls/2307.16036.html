<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.16036] Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data</title><meta property="og:description" content="Researchers in the field of ultra-intense laser science are beginning to embrace machine learning methods. In this study we consider three different machine learning methods â€“ a two-hidden layer neural network, Supportâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.16036">

<!--Generated on Wed Feb 28 15:52:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Applying Machine Learning Methods to Laser
Acceleration of Protons: Lessons Learned
from Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ronak Desai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:desai.458@buckeyemail.osu.edu">desai.458@buckeyemail.osu.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation">Department of Physics, The Ohio State University, Columbus, OH, 43210, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thomas Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Physics, The Ohio State University, Columbus, OH, 43210, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ricky Oropeza
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Physics, The Ohio State University, Columbus, OH, 43210, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">John J. Felice
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Physics, The Ohio State University, Columbus, OH, 43210, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joseph R. Smith
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Physics, Marietta College, Marietta, OH, 45750, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alona Kryshchenko
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Mathematics, California State University Channel Islands, Camarillo, CA 93012, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chris Orban
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Physics, The Ohio State University, Columbus, OH, 43210, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael L. Dexter
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Air Force Institute of Technology, WPAFB, OH 45433
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anil K. Patnaik
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Air Force Institute of Technology, WPAFB, OH 45433
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Researchers in the field of ultra-intense laser science are beginning to embrace machine learning methods. In this study we consider three different machine learning methods â€“ a two-hidden layer neural network, Support Vector Regression and Gaussian Process Regression â€“ and compare how well they can learn from a synthetic data set for proton acceleration in the Target Normal Sheath Acceleration regime. The synthetic data set was generated from a previously published theoretical model by Fuchs et al. 2005 that we modified. Once trained, these machine learning methods can assist with efforts to maximize the peak proton energy, or with the more general problem of configuring the laser system to produce a proton energy spectrum with desired characteristics. In our study we focus on both the accuracy of the machine learning methods and the performance on one GPU including the memory consumption. Although it is arguably the least sophisticated machine learning model we considered, Support Vector Regression performed very well in our tests.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The field of ultra-intense laser science is increasingly beginning to embrace machine learning methods <cite class="ltx_cite ltx_citemacro_cite">Anirudh etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); DÃ¶pp etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>. This is especially true as the repetition rates of ultra-intense laser systems increase and data acquisition systems improve (e.g. <cite class="ltx_cite ltx_citemacro_cite">Heuer etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>). However, to date, there has been limited use of machine learning (ML) to enhance and control proton acceleration from ultra-intense laser systems. Recently, <cite class="ltx_cite ltx_citemacro_citet">Loughran etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> provide results from training a ML model on proton acceleration data using a laser system that operates at 1Â Hz repetition rate using gaussian process regression and Bayesian optimization. <cite class="ltx_cite ltx_citemacro_citet">Ma etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite> also describe at a high level ongoing efforts towards similar goals on laser systems operating at repetition rate of a few Hz using a neural network approach.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.2" class="ltx_p">There are ultra-intense laser systems that operate at higher than 1Â Hz repetition rates, and these systems are already being used in efforts to accelerate protons. <cite class="ltx_cite ltx_citemacro_citet">Morrison etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>, for example, accelerated protons to <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p2.1.m1.1a"><mo id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><csymbol cd="latexml" id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">\sim</annotation></semantics></math>2Â MeV energies using few mJ ultra-intense laser pulses at a repetition rate of 1Â kHz. This experiment could potentially be replicated on the many mJ class, <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p2.2.m2.1a"><mo id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><csymbol cd="latexml" id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">\sim</annotation></semantics></math>kHz repetition rate laser systems that exist today (e.g. <cite class="ltx_cite ltx_citemacro_cite">Cao etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>). It is also true that future industrial or defense applications of ultra-intense laser systems will likely operate closer to this repetition rate regime <cite class="ltx_cite ltx_citemacro_cite">Palmer (<a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>. Ideally, we would like to train ML models on these systems in quasi-real time. A natural question is therefore, of the ML models that are being used today, can they be quickly trained on tens of thousands of shots or more? Can this be achieved with modest computational resources such as a single GPU, or would it require a supercomputer or GPU cluster?
The ability to train a ML model in quasi-real time could greatly assist efforts to optimize and/or control the properties of ion beams resulting from the laser interaction. Specifically, the ML model could help discover ways to increase the max proton energy, or it could help with the inverse problem of wanting a particular ion energy distribution and needing to know the laser parameters to produce that distribution.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In our study, which is essentially a numerical experiment, we will train three different ML models on synthetic data with added noise, described in Sec.Â <a href="#S2" title="II Synthetic Data â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. For brevity, we do not actually use these models to perform hypothetical optimization or control tasks. Instead, we are only concerned with the the accuracy, performance and GPU memory consumption of the ML models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In Sec.Â <a href="#S2" title="II Synthetic Data â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we discuss our modified <cite class="ltx_cite ltx_citemacro_citet">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> model and the noise added to it when generating the synthetic data set. In Sec.Â <a href="#S3" title="III Machine Learning Methods â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we describe the three different machine learning models used in this study. In Sec.<a href="#S4" title="IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we show our results. In Sec.<a href="#S5" title="V Discussion â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> and Sec.<a href="#S6" title="VI Conclusions â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, we summarize and conclude.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span>Synthetic Data</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II.1 </span>Modified Fuchs et al. Model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We generate synthetic data based on a physical model introduced by Fuchs et al. (2005) <cite class="ltx_cite ltx_citemacro_cite">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite>, which introduces a cutoff time to the plasma expansion model by <cite class="ltx_cite ltx_citemacro_citet">Mora (<a href="#bib.bib10" title="" class="ltx_ref">2003</a>)</cite>. This model has five input parameters: laser intensity, wavelength, pulse duration, target thickness, and spot size. It also includes empirical models to estimate quantities such as laser absorption and hot electron temperature.
We extend the model (in a trivial way) by adding focal distance as an input and using the ideal Gaussian beam formula to determine the effective spot size and intensity. As will be discussed later, in our study we kept the pulse duration and wavelength fixed. The spot size was also â€œfixedâ€ in the sense that the spot size at peak focus was always the same, but for non-zero focal distance the effective spot size on target depends on the distance from the target to peak focus. This approach of keeping the pulse duration and laser wavelength fixed while moving the target through the focus of the laser is similar to experimental studies like <cite class="ltx_cite ltx_citemacro_citet">Morrison etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Loughran etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, which both found interesting features in the proton acceleration results even when the target was not at the peak focus.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The <cite class="ltx_cite ltx_citemacro_citet">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> model provides an estimate of both the maximum proton energy as well as the total proton energy and average proton energy. In this way, the Fuchs model provides three outputs. To be clear, these are the protons accelerated by the laser interaction, not necessarily the protons in the bulk target that do not gain significant energy.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">There are many semi-analytic models that exist that could have been used for this purpose (e.g. <cite class="ltx_cite ltx_citemacro_cite">Schreiber etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2006</a>); Passoni and Lontano (<a href="#bib.bib12" title="" class="ltx_ref">2008</a>); Passoni etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2010</a>); Zimmer etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>) and that are potentially more accurate than <cite class="ltx_cite ltx_citemacro_citet">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite>, especially at high intensity. We used <cite class="ltx_cite ltx_citemacro_citet">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> because it is well known in the field, it is relatively simple, and because we are restricting our domain of interest to protons accelerated by the Target Normal Sheath Acceleration (TNSA) mechanism (e.g. <cite class="ltx_cite ltx_citemacro_cite">Clark etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2000</a>); Hatchett etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2000</a>); Snavely etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2000</a>); Passoni etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2010</a>)</cite>).</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.6" class="ltx_p">We did make one substantial modification of the Fuchs model to improve agreement with experiments. In order to improve the predicted maximum proton energy in the intensity regime between <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="10^{18}" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><msup id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml"><mn id="S2.SS1.p4.1.m1.1.1.2" xref="S2.SS1.p4.1.m1.1.1.2.cmml">10</mn><mn id="S2.SS1.p4.1.m1.1.1.3" xref="S2.SS1.p4.1.m1.1.1.3.cmml">18</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S2.SS1.p4.1.m1.1.1.2.cmml" xref="S2.SS1.p4.1.m1.1.1.2">10</cn><cn type="integer" id="S2.SS1.p4.1.m1.1.1.3.cmml" xref="S2.SS1.p4.1.m1.1.1.3">18</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">10^{18}</annotation></semantics></math>Â WÂ cm<sup id="S2.SS1.p4.6.1" class="ltx_sup"><span id="S2.SS1.p4.6.1.1" class="ltx_text ltx_font_italic">-2</span></sup> and <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="10^{19}" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><msup id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml"><mn id="S2.SS1.p4.3.m3.1.1.2" xref="S2.SS1.p4.3.m3.1.1.2.cmml">10</mn><mn id="S2.SS1.p4.3.m3.1.1.3" xref="S2.SS1.p4.3.m3.1.1.3.cmml">19</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><apply id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">superscript</csymbol><cn type="integer" id="S2.SS1.p4.3.m3.1.1.2.cmml" xref="S2.SS1.p4.3.m3.1.1.2">10</cn><cn type="integer" id="S2.SS1.p4.3.m3.1.1.3.cmml" xref="S2.SS1.p4.3.m3.1.1.3">19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">10^{19}</annotation></semantics></math>Â WÂ cm<sup id="S2.SS1.p4.6.2" class="ltx_sup"><span id="S2.SS1.p4.6.2.1" class="ltx_text ltx_font_italic">-2</span></sup>, we changed the relationship between the acceleration timescale of the protons (<math id="S2.SS1.p4.5.m5.1" class="ltx_Math" alttext="\tau_{\rm acc}" display="inline"><semantics id="S2.SS1.p4.5.m5.1a"><msub id="S2.SS1.p4.5.m5.1.1" xref="S2.SS1.p4.5.m5.1.1.cmml"><mi id="S2.SS1.p4.5.m5.1.1.2" xref="S2.SS1.p4.5.m5.1.1.2.cmml">Ï„</mi><mi id="S2.SS1.p4.5.m5.1.1.3" xref="S2.SS1.p4.5.m5.1.1.3.cmml">acc</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.5.m5.1b"><apply id="S2.SS1.p4.5.m5.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p4.5.m5.1.1.2.cmml" xref="S2.SS1.p4.5.m5.1.1.2">ğœ</ci><ci id="S2.SS1.p4.5.m5.1.1.3.cmml" xref="S2.SS1.p4.5.m5.1.1.3">acc</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.5.m5.1c">\tau_{\rm acc}</annotation></semantics></math>) and the laser pulse duration (<math id="S2.SS1.p4.6.m6.1" class="ltx_Math" alttext="\tau_{\rm laser}" display="inline"><semantics id="S2.SS1.p4.6.m6.1a"><msub id="S2.SS1.p4.6.m6.1.1" xref="S2.SS1.p4.6.m6.1.1.cmml"><mi id="S2.SS1.p4.6.m6.1.1.2" xref="S2.SS1.p4.6.m6.1.1.2.cmml">Ï„</mi><mi id="S2.SS1.p4.6.m6.1.1.3" xref="S2.SS1.p4.6.m6.1.1.3.cmml">laser</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.6.m6.1b"><apply id="S2.SS1.p4.6.m6.1.1.cmml" xref="S2.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.6.m6.1.1.1.cmml" xref="S2.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p4.6.m6.1.1.2.cmml" xref="S2.SS1.p4.6.m6.1.1.2">ğœ</ci><ci id="S2.SS1.p4.6.m6.1.1.3.cmml" xref="S2.SS1.p4.6.m6.1.1.3">laser</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.6.m6.1c">\tau_{\rm laser}</annotation></semantics></math>).</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\tau_{\rm acc}=4.0\cdot\tau_{\rm laser}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><msub id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">Ï„</mi><mi id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml">acc</mi></msub><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mn id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">4.0</mn><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">â‹…</mo><msub id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml">Ï„</mi><mi id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml">laser</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">ğœ</ci><ci id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3">acc</ci></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><ci id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1">â‹…</ci><cn type="float" id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2">4.0</cn><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2">ğœ</ci><ci id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3">laser</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\tau_{\rm acc}=4.0\cdot\tau_{\rm laser}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p4.7" class="ltx_p">This modification is inspired by <cite class="ltx_cite ltx_citemacro_cite">DjordjeviÄ‡ etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, who in their Sec.Â 5.3 treated the multiplier between the laser pulse duration and the proton acceleration timescale as a free parameter to be constrained by 1D Particle-in-Cell simulation results. Without this modification (i.e. the unmodified Fuchs et al. model), we found that the max proton energies in this intensity regime were too low compared to experiments in this intensity regime (e.g. <cite class="ltx_cite ltx_citemacro_cite">Morrison etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>).</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Note that in computing the total proton energy and average proton energy from the <cite class="ltx_cite ltx_citemacro_citet">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> model, we assumed the minimum kinetic energy to be zero for simplicity rather than using a non-zero cutoff.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II.2 </span>Range of Synthetic Data Generated</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.8" class="ltx_p">We generate a synthetic data based on a laser system that can generate 40 femtosecond pulses, a spot size of 1.8 microns Full-Width Half-Max, and target thicknesses between 0.5 microns and 10 microns. These parameters are similar to that of Morrison et al. 2018<cite class="ltx_cite ltx_citemacro_cite">Morrison etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. We generated synthetic data with total pulse energies between 2.27Â mJ and 22.72Â mJ. At peak focus, these pulses correspond to intensities of <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="10^{18}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msup id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">10</mn><mn id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">18</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">10</cn><cn type="integer" id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">18</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">10^{18}</annotation></semantics></math>Â WÂ cm<sup id="S2.SS2.p1.8.1" class="ltx_sup"><span id="S2.SS2.p1.8.1.1" class="ltx_text ltx_font_italic">-2</span></sup> and <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="10^{19}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><msup id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mn id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">10</mn><mn id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">19</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">superscript</csymbol><cn type="integer" id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">10</cn><cn type="integer" id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">10^{19}</annotation></semantics></math>Â WÂ cm<sup id="S2.SS2.p1.8.2" class="ltx_sup"><span id="S2.SS2.p1.8.2.1" class="ltx_text ltx_font_italic">-2</span></sup> respectively. The focal distance was varied between -10 microns and +10 microns. Consequently the intensity on target for the generated points ranges from <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="\sim 10^{17}" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><mrow id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2" xref="S2.SS2.p1.5.m5.1.1.2.cmml"></mi><mo id="S2.SS2.p1.5.m5.1.1.1" xref="S2.SS2.p1.5.m5.1.1.1.cmml">âˆ¼</mo><msup id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml"><mn id="S2.SS2.p1.5.m5.1.1.3.2" xref="S2.SS2.p1.5.m5.1.1.3.2.cmml">10</mn><mn id="S2.SS2.p1.5.m5.1.1.3.3" xref="S2.SS2.p1.5.m5.1.1.3.3.cmml">17</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="latexml" id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2">absent</csymbol><apply id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.3.1.cmml" xref="S2.SS2.p1.5.m5.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS2.p1.5.m5.1.1.3.2.cmml" xref="S2.SS2.p1.5.m5.1.1.3.2">10</cn><cn type="integer" id="S2.SS2.p1.5.m5.1.1.3.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3">17</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">\sim 10^{17}</annotation></semantics></math>Â WÂ cm<sup id="S2.SS2.p1.8.3" class="ltx_sup"><span id="S2.SS2.p1.8.3.1" class="ltx_text ltx_font_italic">-2</span></sup> to <math id="S2.SS2.p1.7.m7.1" class="ltx_Math" alttext="10^{19}" display="inline"><semantics id="S2.SS2.p1.7.m7.1a"><msup id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml"><mn id="S2.SS2.p1.7.m7.1.1.2" xref="S2.SS2.p1.7.m7.1.1.2.cmml">10</mn><mn id="S2.SS2.p1.7.m7.1.1.3" xref="S2.SS2.p1.7.m7.1.1.3.cmml">19</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><apply id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.7.m7.1.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">superscript</csymbol><cn type="integer" id="S2.SS2.p1.7.m7.1.1.2.cmml" xref="S2.SS2.p1.7.m7.1.1.2">10</cn><cn type="integer" id="S2.SS2.p1.7.m7.1.1.3.cmml" xref="S2.SS2.p1.7.m7.1.1.3">19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">10^{19}</annotation></semantics></math>Â WÂ cm<sup id="S2.SS2.p1.8.4" class="ltx_sup"><span id="S2.SS2.p1.8.4.1" class="ltx_text ltx_font_italic">-2</span></sup>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">We generated 25,000 data points by randomly sampling this parameter space.
The highest proton energy in the data set was near 2.3Â MeV while the lowest proton energy was 1.6Â keV. The random sampling of the parameter space is an important detail because a real, kHz repetition rate laser experiment would sample the parameter space in a very specific way, for example, by scanning through the focal distance or translating the target to monotonically investigate different thicknesses. We intentionally sampled the parameter space in a randomized way to avoid â€œgapsâ€ in the synthetic data set that might bias the results. The random sampling of the intensity was exponentiated uniform, so as not to focus too much on high intensity or low intensity, while the random sampling of other parameters was uniform across the range. In future work, we will investigate more realistic synthetic data sets that better mimic how data from real kHz laser experiments are gathered.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">We use up to 20,000 of the generated data points for training the ML models and reserve the remaining 5,000 points for testing. Our decision to use 20,000 data points as the maximum number of points in the training set, rather than set a higher or lower number, was informed by a few different priorities. On a 1Â Hz repetition rate laser system like the one described in Loughran et al.<cite class="ltx_cite ltx_citemacro_cite">Loughran etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, collecting 20,000 data points would take 5.5 hours, assuming continuous operation. Currently, there are a number of high-power laser systems operating near 1Â Hz repetition rate, so 20,000 data points represent roughly one full day of experimental time on these systems.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">As mentioned earlier, we are also interested in kHz repetition rate systems (e.g. <cite class="ltx_cite ltx_citemacro_cite">Morrison etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>). These systems produce 20,000 shots in 20 seconds and 3.6 Million shots in an hour, but a number of practical limitations ranging from data acquisition systems to diagnostics to changing the laser parameters on a millisecond timescale or the limited speed that the target can be moved can mean that many of the shots on a kHz system probe essentially the same conditions. These and other practical limitations can easily cause an hour long experiment on a kHz laser system to only produce tens of thousands of shots with distinctly different conditions.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Focusing on up to 20,000 points is therefore relevant to both repetition rate regimes and it is a stepping stone towards the million-plus shot data sets that kHz laser systems with upgraded data acquisition and diagnostics will eventually produce.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II.3 </span>Noise model</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To better represent experimental data, we added noise to all three output quantities of the model â€“ max proton energy, total proton energy, and average proton energy.
Our noise model involved making Fuchs model predictions for our parameter space and sampling from a normal distribution using <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">each</em> prediction as the mean. We generated data sets with different noise levels by assuming that the standard deviation is between 5% and 30% of the mean value. When, in later sections, we refer to a 10% Gaussian noise level, for example, this means that we generated data by sampling a normal distribution assuming that the standard deviation was 10% of the mean. The â€œGaussianâ€ in this context refers to the Gaussian function that underlies the normal distribution. Because the standard deviation is assumed to be some fraction of the mean, as the Fuchs model predictions become larger, the noise level also becomes proportionally larger.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">The higher noise levels can potentially produce negative values for the proton energy (which would be unphysical), so we resample those points if that occurs.
We include with this publication the the Python code that was used to generate the synthetic data and the code that was used to train the models <cite class="ltx_cite ltx_citemacro_cite">Ron (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Orb (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>); Desai etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span>Machine Learning Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We use the synthetic data to train three different machine learning models. These are Neural Network (NN), Support Vector Regression (SVR), and Gaussian Process Regression (GPR). In contrast to a neural network, SVR and GPR apply a â€œkernelâ€ function to map data to a higher dimensional space.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">There are a number of caveats and important details in comparing these models, and we do not attempt to make a definitive comparison. What we do aim to do is to make reasonable efforts to compare the methods and their performance on a single GPU. Specifically, we use a NVIDIA Volta V100 GPU on the Pitzer cluster at the Ohio Supercomputer Center with 32Â GB of GPU memory. Unless otherwise noted, we present results from training the ML models in single precision.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">As mentioned earlier, there are three independent variables in the data set â€“ target thickness, intensity and focal distance. First, we applied a logarithm to both the intensity and the three outputs of the model: maximum proton energy, total proton energy, average proton energy. Then, we applied standard z-score normalization<cite class="ltx_cite ltx_citemacro_cite">Han etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2011</a>)</cite> to all three inputs of the training set. We also apply z-score scaling to the outputs using the standard deviation and the mean of the training set.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III.1 </span>Support Vector Regression</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">SVR is based on Support Vector Machines which were developed for classification problems. In SVR, one specifies a tolerance value that is used to determine which points follow the relation given by a particular curve and which points are considered outliers. This makes the task of finding the best fit curve like a classification problem. For more information about SVR, see Smola 2004<cite class="ltx_cite ltx_citemacro_cite">Smola and SchÃ¶lkopf (<a href="#bib.bib23" title="" class="ltx_ref">2004</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p">To accelerate the calculations with GPU, we used the cuML library,<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref">cum </a></cite> which is part of the RAPIDS package. The version for our cuML library was 22.10.01. Our investigation used an epsilon of 0.001 and we used the so-called radial basis function (RBF) kernel. The <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\gamma</annotation></semantics></math> value is set by cuMLâ€™s â€œscaleâ€ option, which automatically sets the <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\gamma</annotation></semantics></math> value equal to 1 / (number of features * input variance). For more details of our specific implementation please see the attached Python file <cite class="ltx_cite ltx_citemacro_cite">Ron (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Orb (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>); Desai etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III.2 </span>Neural Network</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We used PyTorch version 1.12.1 to implement a neural network (NN) model using a two hidden layer architecture described in Fig.Â <a href="#S3.F1" title="Figure 1 â€£ III.2 Neural Network â€£ III Machine Learning Methods â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. By using a smaller number of nodes in the second layer, this approach is intended to model the main trends in the data. With 64 nodes in the first hidden layer and 16 nodes in the second hidden layer, there are 1347 weights (including bias weights) in the network. We used a â€œleakyâ€ ReLU activation function between fully connected nodes.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2307.16036/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of the neural network model. </figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We train the neural network using the â€œAdamâ€ scheme for backpropagation as described in <cite class="ltx_cite ltx_citemacro_citet">Kingma and Ba (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>. We use an initial learning rate of 0.001, which the Adam scheme can increase or decrease as the model improves. For comparing with the other two ML models, we show NN model predictions after 35 epochs of training.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III.3 </span>Gaussian Process Regression</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Gaussian Process Regression (GPR) is a Bayesian method which differs substantially from SVR and NN. The approach works by selecting a set of analytic functions that go through a series of data points. The GPR model can make a data-informed predictions by taking the weighted average of these functions, with the weights being the likelihood that any one of these functions is the â€œtrueâ€ function that correctly captures the model. A potential advantage over SVR and NN is that the GPR model uncertainty can be simply determined by the variance of these functions without any additional work. <cite class="ltx_cite ltx_citemacro_citet">Schulz etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite> provides an excellent introduction to GPR.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">To accelerate the computations with GPU, we used the GPyTorch library <cite class="ltx_cite ltx_citemacro_cite">Gardner etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite> version 1.9.1. We found this to be faster than the GPFlow library for our problem case. As discussed in <cite class="ltx_cite ltx_citemacro_cite">Gardner etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite>, numerical instabilities can arise in cases with low or zero noise which can affect the accuracy of the GPR model. To avoid this we configured GPyTorch to use a maximum of 50 iterations. In our tests (not shown for brevity) this improved the accuracy of GPR for Gaussian noise levels of <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mo id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">â‰¤</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><leq id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\leq</annotation></semantics></math>5%. We used the RBF kernel for our GPR model.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span>Results</h2>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2307.16036/assets/pct_err_noise=10.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="698" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Typical accuracy in terms of Mean Absolute Percentage Error (MAPE) for different ML models for predicting the max proton energy (left panel), total proton energy (center panel), and average proton energy (right panel) assuming 10% Gaussian noise. The horizontal black dashed line in each panel represents the ideal accuracy if the model worked perfectly except for the added Gaussian noise. [Associated dataset available at https://doi.org/10.5281/zenodo.8221343]<cite class="ltx_cite ltx_citemacro_cite">Desai etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2023c</a>)</cite></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV.1 </span>Accuracy of Trained Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> highlights results for the Mean Absolute Percentage Error (MAPE) for the different ML models trained on data with 10% Gaussian noise and different numbers of training points. To determine the percentage error, the ML models were compared to 5,000 testing points (regardless of the number of training points). Naturally, when a ML model prediction is compared to the 5,000 testing points, each of these points will have a percentage error associated with it. In Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we present the mean of that percentage error after an absolute value is taken (i.e.Â the â€œabsoluteâ€ in MAPE is referring to the absolute value). In understanding Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> it is important to note that if noiseless data were compared, for all of the same input conditions, to noise-added data assuming a 10% Gaussian noise level (Sec.<a href="#S2.SS3" title="II.3 Noise model â€£ II Synthetic Data â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II.3</span></a>), one would find that the noiseless data typically differs from the noise-added data by 8 percent.
Therefore, we do not expect the ML models to be able to predict better than about 8% in this comparison because the 5,000 testing points do include 10% Gaussian noise. We show this expected lower limit on the MAPE in Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> as dashed black horizontal lines.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Fig.Â <a href="#S4.F3" title="Figure 3 â€£ IV.1 Accuracy of Trained Models â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the accuracy in terms of root mean square error for predicting the max proton energy using the three different ML models with different levels of Gaussian noise.
Similar to Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, this plot was made by comparing trained ML models to 5,000 testing points. Dot-dashed black horizontal lines indicate how accurate the ML model would be if the only source of error was the noise.
In Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, at 10% Gaussian noise, there are differences in the ML models, but overall, the model predictions are comparable to the expected lower limit on the MAPE, even with as few as <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\sim</annotation></semantics></math>2,000 points. When the Gaussian noise level increases to 20% or 30%, the ML models require more training points to approach the expected lower limit on the MAPE.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We are interested in determining not only whether the ML models are reasonably accurate compared to the known level of noise in our synthetic data set, but also how well the ML models (which are trained on noisy data) compare to noiseless data. Fig.Â <a href="#S4.F4" title="Figure 4 â€£ IV.1 Accuracy of Trained Models â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> compares the accuracy of the ML models trained on 20,000 data points with between 0% to 30% Gaussian noise compared to 5,000 test data points that both do and do not include noise. The figure shows that when the ML models are compared to the noisy data, except for the low noise NN results, the RMS error tends to be dominated by the noise as expected. Compared to noiseless data, the ML models (which were trained on noisy data) tend to have an RMS error that is smaller than the data sets they were trained on. Again, the only exception is the NN results for <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">â‰¤</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><leq id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\leq</annotation></semantics></math>5% Gaussian noise.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2307.16036/assets/3_layer_rmse_err_noise=30.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="698" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>RMS error versus number of training points for different ML models predicting the max proton energy (left panel), total proton energy (center panel), and average proton energy (right panel). Each panel shows results from 10%, 20% and 30% Gaussian noise in the training data with the higher noise levels appearing higher in the plot. Dashed black lines indicate what the RMSE would be if the error were caused only the noise added to the data. The dashed black lines therefore indicate how accurate a perfectly ideal model would be. [Associated dataset available at https://doi.org/10.5281/zenodo.8221343]<cite class="ltx_cite ltx_citemacro_cite">Desai etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2023c</a>)</cite></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2307.16036/assets/fig4_rmse_20000pts_threefigs.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="698" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Solid lines show the typical RMS error in max proton energy (left panel), total proton energy (center panel), and average proton energy (right panel) when the ML models (which were trained on 20,000 synthetic data points with noise) are compared to data with different levels of noise. Dot-dashed lines show the typical error when those same ML models are compared to noiseless test data. A dashed black line shows the expected error if the ML model matched the noiseless data perfectly. [Associated dataset available at https://doi.org/10.5281/zenodo.8221343]<cite class="ltx_cite ltx_citemacro_cite">Desai etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2023c</a>)</cite></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV.2 </span>Performance and Memory Consumption</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Fig.Â <a href="#S4.F5" title="Figure 5 â€£ IV.2 Performance and Memory Consumption â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows our primary results for the execution time of the different ML models using between 1,000 and 20,000 synthetic data points for training with 10% Gaussian noise. As mentioned earlier, our primary results are from using one Nvidia V-100 GPU with 32Â GB of memory with single precision. An important note is that the â€œtraining timeâ€ on the y-axis in Fig.Â <a href="#S4.F5" title="Figure 5 â€£ IV.2 Performance and Memory Consumption â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> includes the time to train on all three of the outputs of interest â€“ the max proton energy, total proton energy and average proton energy. For GPR and SVR, we treated these three outputs separately which means that the training time for these models included three separate instances of training on the synthetic data. The training time for GPR and SVR are therefore the sum of these three training events. For the measured training time for NN, we used the same neural network to predict the three output quantities (Fig.Â <a href="#S3.F1" title="Figure 1 â€£ III.2 Neural Network â€£ III Machine Learning Methods â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), rather than create and train three separate neural networks. Tests with running the NN model separately for all three outputs (not shown) produced similar performance results.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2307.16036/assets/time_noise=10.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="349" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparing the execution time of the different ML models with 10% added Gaussian noise in computing the maximum, total, and average proton energies. The SVR and GPR accounts for the sum total time to train three separate models (one for each energy) whereas the NN trains all three with one model. [Associated dataset available at https://doi.org/10.5281/zenodo.8221343]<cite class="ltx_cite ltx_citemacro_cite">Desai etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2023c</a>)</cite></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">Of the three models, the clear winner with respect to execution time is SVR, followed by NN and, last, GPR. Although we expected the execution time for GPR to approach <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="O(N^{3})" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS2.p2.1.m1.1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S4.SS2.p2.1.m1.1.1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.1.1.1.2.cmml">N</mi><mn id="S4.SS2.p2.1.m1.1.1.1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.1.1.1.3.cmml">3</mn></msup><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2"></times><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">ğ‘‚</ci><apply id="S4.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1">superscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.1.2">ğ‘</ci><cn type="integer" id="S4.SS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.1.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">O(N^{3})</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Wang (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>, the increase in the execution time with increasing numbers of training data points was not as steep as this, at least for the range of <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">N</annotation></semantics></math> that we probed and the data set we used. Tests with double precision (not shown) did not fundamentally change the ordering of the results â€“ SVR remained the fastest method while GPR remained the slowest.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">TableÂ <a href="#S4.T1" title="Table 1 â€£ IV.2 Performance and Memory Consumption â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the average GPU memory consumption of the three different ML models on 20,000 synthetic data points. The NN had the lowest memory consumption while GPR consumed the most memory. All results are for single precision. In tests with double precision (not shown), GPR consumed about twice as much GPU memory while SVR and NN consumed about the same amount of GPU memory as single precision.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SVR</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GPR</td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ave GPU Memory Utilization (GiB)</td>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.9</td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.1</td>
<td id="S4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Memory Consumption Results (20,000 synthetic data points). [Associated dataset available at https://doi.org/10.5281/zenodo.8221343]<cite class="ltx_cite ltx_citemacro_cite">Desai etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2023c</a>)</cite>
</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The three ML models overall did an adequate job of training on the synthetic data set and predicting both noisy and noiseless data when compared to test data that the models had not seen before. The only exception to this was the low noise results for the neural network model, where the inaccuracy of the neural network model was the dominant source of error (Fig.Â <a href="#S4.F4" title="Figure 4 â€£ IV.1 Accuracy of Trained Models â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). Overall the neural network was not as accurate as SVR or GPR, which was true across the range of noise levels we examined and for both noisy and noiseless data (Fig.Â <a href="#S4.F4" title="Figure 4 â€£ IV.1 Accuracy of Trained Models â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). As a comment on this result, it is important to note that the approach of the neural network model is very different from either SVR and GPR in that it is essentially trying to fit 1347 free parameters using 20,000 data points or less. A reasonable question is whether our choice of using only two hidden layers (Fig.Â <a href="#S3.F1" title="Figure 1 â€£ III.2 Neural Network â€£ III Machine Learning Methods â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) instead some larger number of hidden layers could have affected this result. In AppendixÂ <a href="#A1" title="Appendix A On the Number of Hidden Layers â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> we consider this question and conclude that between one and two layers seem to give the best results if the total number of parameters are approximately fixed.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.2" class="ltx_p">An interesting result that we did not necessarily expect was that SVR and GPR often achieved high accuracy even with as few as <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.p2.1.m1.1a"><mo id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\sim</annotation></semantics></math>2000 training points (Fig.Â <a href="#S4.F2" title="Figure 2 â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> &amp; Fig.Â <a href="#S4.F3" title="Figure 3 â€£ IV.1 Accuracy of Trained Models â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The NN results were not as impressive but in some cases reasonably high accuracy was achieved for <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.p2.2.m2.1a"><mo id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><lt id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">&lt;</annotation></semantics></math>5,000 training points.
Achieving high accuracy without extreme numbers of training points certainly is a reflection of the quality of the ML models, but it is also true that the Fuchs et al.<cite class="ltx_cite ltx_citemacro_cite">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> based model that we used to generate the synthetic data set is a relatively well behaved function without any sharp features. The relative simplicity of the Fuchs et al.<cite class="ltx_cite ltx_citemacro_cite">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> generated data set (compared, for example, to an MNIST data set with hand written numbers between 0 and 9<cite class="ltx_cite ltx_citemacro_cite">YannÂ LeCun and Burges (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>) certainly helped the ML models to accurately represent our synthetic data sets even from a relatively small number of points. In future work, it will be interesting to see if more sophisticated synthetic data sets with less ideal noise characteristics or experimental data will require more points to reach a similar level of agreement.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Another expectation we had was that SVR would be less accurate than GPR or NN because it is arguably the least sophisticated ML model that we used. But we found, at least for our data sets, that the SVR was similarly as accurate as the other models.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">We examined the performance of the ML models running on one GPU. We found that the SVR training time was much less than GPR and NN, which is perhaps not surprising since SVR has a reputation for being a fast method (e.g. <cite class="ltx_cite ltx_citemacro_cite">Gandhi (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>); Xu and Suzuki (<a href="#bib.bib33" title="" class="ltx_ref">2011</a>)</cite>).
We found that the GPR method consumed the most GPU RAM of the three models. Ultra-intense laser experiments that produce significantly more then 20,000 data points on a short timescale may need to worry about GPU RAM constraints.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Overall, the performance of the ML models in terms of execution time were such that training on quasi-real time data from a kHz ultra-intense laser system using only one GPU should be feasible, especially for SVR. Of the three ML models, the longest execution time was the GPR which took about 30 seconds to train on 20,000 data points (Fig.Â <a href="#S4.F5" title="Figure 5 â€£ IV.2 Performance and Memory Consumption â€£ IV Results â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). It is difficult to use our results to extrapolate to an order of magnitude more data than 20,000 points, but overall we find that even the most time intensive ML model is still in the range where one could envision quasi-real time operation with a kHz repetition rate laser system with single-shot diagnostics.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">We stress that these results depend on the particular parameter scan that we chose and the details of the Fuchs et al. <cite class="ltx_cite ltx_citemacro_cite">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> model. More complicated data sets with more features will likely require more compute time to adequately train the ML models.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We tested three different machine learning models on a synthetic data set for laser-accelerated protons by training these models on up to 20,000 synthetic data points. The data set was generated using a modified Fuchs et al. <cite class="ltx_cite ltx_citemacro_cite">Fuchs etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2005</a>)</cite> model that included Gaussian noise to simulate the kind of noise that could be present in a real experiment. The machine learning models were Gaussian Process Regression (GPR), Support Vector Regression (SVR) and a two-hidden-layer neural network (NN).</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Overall, we find that the three ML models adequately modeled the synthetic data sets. When ML model predictions were compared to data points that were not included in the training set, the dominant source of error was typically the noise that was artificially added to the synthetic data sets. When compared to noiseless data, the models, which were trained on noisy data, typically deviated from the noiseless data at a level that was well below the noise that was added to the data set (i.e. the models successfully averaged out the noise). GPR and SVR were the most accurate models.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">In terms of performance on one GPU, SVR was by far the fastest model for execution time. GPR was the slowest, taking 30 seconds to train on 20,000 data points. The neural network used the least amount of GPU memory while GPR consumed 5-10 times more GPU memory than the other models. Generally the performance results indicate that quasi-real time training of these models on kHz repetition rate laser systems should be feasible, especially with SVR.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">We stress that these results should not be regarded as the final word on the usefulness of these ML models in the context of laser acceleration of protons. More complex data sets may yield different results. Moreover, there are certainly ML models that deserve attention that we did not study here.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">We provide Jupyter notebooks with our Python code. We also provide the 25,000 point synthetic data set that we generated using this code <cite class="ltx_cite ltx_citemacro_cite">Ron (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Orb (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>. By providing these files we hope to encourage others to compare other ML models against our results as a benchmark.</p>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>On the Number of Hidden Layers</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The number of hidden layers is an important concern when designing a neural network. Work done by Hornik et al 1989<cite class="ltx_cite ltx_citemacro_cite">Hornik etÂ al. (<a href="#bib.bib34" title="" class="ltx_ref">1989</a>)</cite> and Leshno et al 1993<cite class="ltx_cite ltx_citemacro_cite">Leshno etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">1993</a>)</cite> showed that neural networks with at least one hidden layer and using a non-polynomial activation function can serve as an universal approximator for any function given a sufficient number of nodes. Adding more hidden layers can, in some cases, reduce the training time and allow for faster convergence for a neural network. However, this depends on the data set with more complex data sets benefiting from additional hidden layers.</p>
</div>
<figure id="A1.T2" class="ltx_table">
<table id="A1.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T2.1.1.1" class="ltx_tr">
<td id="A1.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Hidden Layers</td>
<td id="A1.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Nodes per Hidden Layer</td>
<td id="A1.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Total Parameters</td>
</tr>
<tr id="A1.T2.1.2.2" class="ltx_tr">
<td id="A1.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1</td>
<td id="A1.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">192</td>
<td id="A1.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1347</td>
</tr>
<tr id="A1.T2.1.3.3" class="ltx_tr">
<td id="A1.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">2</td>
<td id="A1.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33</td>
<td id="A1.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1356</td>
</tr>
<tr id="A1.T2.1.4.4" class="ltx_tr">
<td id="A1.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">3</td>
<td id="A1.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24</td>
<td id="A1.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1371</td>
</tr>
<tr id="A1.T2.1.5.5" class="ltx_tr">
<td id="A1.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">4</td>
<td id="A1.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td id="A1.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1403</td>
</tr>
<tr id="A1.T2.1.6.6" class="ltx_tr">
<td id="A1.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">5</td>
<td id="A1.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17</td>
<td id="A1.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1346</td>
</tr>
<tr id="A1.T2.1.7.7" class="ltx_tr">
<td id="A1.T2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">6</td>
<td id="A1.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">15</td>
<td id="A1.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1308</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Details of six different neural networks</figcaption>
</figure>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2307.16036/assets/FCNN_Multi_Layer_Resized_Labels_Stylized_Lower_Layers_Thicker.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="698" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Accuracy of Neural Networks with different numbers of hidden layers trained on different numbers of training data points. The <math id="A1.F6.2.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="A1.F6.2.m1.1b"><mi id="A1.F6.2.m1.1.1" xref="A1.F6.2.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="A1.F6.2.m1.1c"><ci id="A1.F6.2.m1.1.1.cmml" xref="A1.F6.2.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F6.2.m1.1d">y</annotation></semantics></math>-axis shows the mean average percent error for the prediction of the maximum proton kinetic energy.</figcaption>
</figure>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">As discussed in Sec.Â <a href="#S3.SS2" title="III.2 Neural Network â€£ III Machine Learning Methods â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III.2</span></a>, our fiducial neural network model assumes two hidden layers (Fig.Â <a href="#S3.F1" title="Figure 1 â€£ III.2 Neural Network â€£ III Machine Learning Methods â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). In this Appendix, we provide results from tests we performed with larger numbers of hidden layers. To determine if additional hidden layers would be advantageous for our data set, we performed a numerical experiment where we trained neural networks with different numbers of hidden layers and compared their accuracy on noiseless data that was not in the training set. We note that, unlike the two-hidden layer neural network depicted in Fig. <a href="#S3.F1" title="Figure 1 â€£ III.2 Neural Network â€£ III Machine Learning Methods â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which has more nodes in the first layer than the second, the hidden layers for the neural networks in these tests used the same number of nodes for simplicity. In order to ensure that each neural network has roughly the same number of parameters, we set the number of nodes in each hidden layer to be fewer for neural networks with more hidden layers. Table <a href="#A1.T2" title="Table 2 â€£ Appendix A On the Number of Hidden Layers â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows exactly how many free parameters were in each neural network. This was an important step because keeping the number of nodes per layer the same would naturally advantage models with more hidden layers, as those models would have many times more free parameters than the models with fewer hidden layers.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">We found that neural networks with more hidden layers did not necessarily produce more accurate results for our specific problem case. Fig. <a href="#A1.F6" title="Figure 6 â€£ Appendix A On the Number of Hidden Layers â€£ Applying Machine Learning Methods to Laser Acceleration of Protons: Lessons Learned from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the mean average percent error for the max proton energy from the six different neural network architectures versus the number of training points. The figure shows that neural networks with more hidden layers did not yield more accurate results than neural networks with fewer hidden layers.</p>
</div>
</section>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Acknowledgements</h2>

<div id="Ax1.p1" class="ltx_para">
<p id="Ax1.p1.1" class="ltx_p">Supercomputer allocations for this project included time from the Ohio Supercomputer Center. We also thank Pedro Gaxiola for help with early work with SVR. We acknowledge support provided by the National Science Foundation (NSF) under Grant No. 2109222. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. CO and RO were supported in summer 2022 by the Air Force Office of Science Research summer faculty program. This work was supported by Air Force Office of Scientific Research (AFOSR) Award (PM: Dr. Andrew B. Stickrath). This work was also supported by Department of Energy (PM: Dr. Kramer Akli).</p>
</div>
</section>
<section id="Ax2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anirudh etÂ al. (2022)</span>
<span class="ltx_bibblock">
R.Â Anirudh,
R.Â Archibald,
M.Â S. Asif,
M.Â M. Becker,
S.Â Benkadda,
P.-T. Bremer,
R.Â H.Â S. BudÃ©,
C.Â S. Chang,
L.Â Chen,
R.Â M. Churchill,
etÂ al., <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2022 review of data-driven plasma
science</em> (2022), eprint 2205.15832.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DÃ¶pp etÂ al. (2023)</span>
<span class="ltx_bibblock">
A.Â DÃ¶pp,
C.Â Eberle,
S.Â Howard,
F.Â Irshad,
J.Â Lin, and
M.Â Streeter,
High Power Laser Science and Engineering p.
1â€“50 (2023).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heuer etÂ al. (2022)</span>
<span class="ltx_bibblock">
P.Â V. Heuer,
S.Â Feister,
D.Â B. Schaeffer,
and H.Â G.
Rinderknecht, Physics of Plasmas
<span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">29</span> (2022), ISSN
1070-664X, 110401,
eprint https://pubs.aip.org/aip/pop/article-pdf/doi/10.1063/5.0130801/16624799/110401_1_online.pdf,
URL <a target="_blank" href="https://doi.org/10.1063/5.0130801" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1063/5.0130801</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loughran etÂ al. (2023)</span>
<span class="ltx_bibblock">
B.Â Loughran,
M.Â J.Â V. Streeter,
H.Â Ahmed,
S.Â Astbury,
M.Â Balcazar,
M.Â Borghesi,
N.Â Bourgeois,
C.Â B. Curry,
S.Â J.Â D. Dann,
S.Â DiIorio,
etÂ al., High Power Laser Science and
Engineering <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">11</span>, e35
(2023).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2021)</span>
<span class="ltx_bibblock">
T.Â Ma,
D.Â Mariscal,
R.Â Anirudh,
T.Â Bremer,
B.Â Z. Djordjevic,
T.Â Galvin,
E.Â Grace,
S.Â Herriot,
S.Â Jacobs,
B.Â Kailkhura,
etÂ al., Plasma Physics and Controlled Fusion
<span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">63</span>, 104003 (2021).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morrison etÂ al. (2018)</span>
<span class="ltx_bibblock">
J.Â T. Morrison,
S.Â Feister,
K.Â D. Frische,
D.Â R. Austin,
G.Â K. Ngirmang,
N.Â R. Murphy,
C.Â Orban,
E.Â A. Chowdhury,
and W.Â M.
Roquemore, New Journal of Physics
<span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">20</span>, 022001 (2018).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Z.Â Cao,
Z.Â Peng,
Y.Â Shou,
J.Â Zhao,
S.Â Chen,
Y.Â Gao,
J.Â Liu,
P.Â Wang,
Z.Â Mei,
Z.Â Pan,
etÂ al., Frontiers in Physics
<span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">11</span>, 1172075 (2023).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Palmer (2018)</span>
<span class="ltx_bibblock">
C.Â Palmer, New
Journal of Physics <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">20</span>, 061001
(2018),
URL <a target="_blank" href="https://dx.doi.org/10.1088/1367-2630/aac5ce" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dx.doi.org/10.1088/1367-2630/aac5ce</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fuchs etÂ al. (2005)</span>
<span class="ltx_bibblock">
J.Â Fuchs,
P.Â Antici,
E.Â Dâ€™HumiÃ¨res,
E.Â Lefebvre,
M.Â Borghesi,
E.Â Brambrink,
C.Â Cecchetti,
M.Â Kaluza,
V.Â Malka,
M.Â Manclossi,
etÂ al., Nature Physics
<span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">2</span> (2005).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mora (2003)</span>
<span class="ltx_bibblock">
P.Â Mora, Phys.
Rev. Lett. <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">90</span>, 185002
(2003),
URL <a target="_blank" href="https://link.aps.org/doi/10.1103/PhysRevLett.90.185002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.aps.org/doi/10.1103/PhysRevLett.90.185002</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schreiber etÂ al. (2006)</span>
<span class="ltx_bibblock">
J.Â Schreiber,
F.Â Bell,
F.Â GrÃ¼ner,
U.Â Schramm,
M.Â Geissler,
M.Â SchnÃ¼rer,
S.Â Ter-Avetisyan,
B.Â M. Hegelich,
J.Â Cobble,
E.Â Brambrink,
etÂ al., Phys. Rev. Lett.
<span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">97</span>, 045005
(2006),
URL <a target="_blank" href="https://link.aps.org/doi/10.1103/PhysRevLett.97.045005" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.aps.org/doi/10.1103/PhysRevLett.97.045005</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Passoni and Lontano (2008)</span>
<span class="ltx_bibblock">
M.Â Passoni and
M.Â Lontano,
Phys. Rev. Lett. <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">101</span>,
115001 (2008),
URL <a target="_blank" href="https://link.aps.org/doi/10.1103/PhysRevLett.101.115001" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.aps.org/doi/10.1103/PhysRevLett.101.115001</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Passoni etÂ al. (2010)</span>
<span class="ltx_bibblock">
M.Â Passoni,
L.Â Bertagna, and
A.Â Zani, New
Journal of Physics <span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">12</span>, 045012
(2010),
URL <a target="_blank" href="https://dx.doi.org/10.1088/1367-2630/12/4/045012" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dx.doi.org/10.1088/1367-2630/12/4/045012</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zimmer etÂ al. (2021)</span>
<span class="ltx_bibblock">
M.Â Zimmer,
S.Â Scheuren,
T.Â Ebert,
G.Â Schaumann,
B.Â Schmitz,
J.Â Hornung,
V.Â Bagnoud,
C.Â RÃ¶del, and
M.Â Roth,
Phys. Rev. E <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">104</span>,
045210 (2021),
URL <a target="_blank" href="https://link.aps.org/doi/10.1103/PhysRevE.104.045210" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.aps.org/doi/10.1103/PhysRevE.104.045210</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark etÂ al. (2000)</span>
<span class="ltx_bibblock">
E.Â L. Clark,
K.Â Krushelnick,
J.Â R. Davies,
M.Â Zepf,
M.Â Tatarakis,
F.Â N. Beg,
A.Â Machacek,
P.Â A. Norreys,
M.Â I.Â K. Santala,
I.Â Watts,
etÂ al., Phys. Rev. Lett.
<span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">84</span>, 670 (2000),
URL <a target="_blank" href="https://link.aps.org/doi/10.1103/PhysRevLett.84.670" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.aps.org/doi/10.1103/PhysRevLett.84.670</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hatchett etÂ al. (2000)</span>
<span class="ltx_bibblock">
S.Â P. Hatchett,
C.Â G. Brown,
T.Â E. Cowan,
E.Â A. Henry,
J.Â S. Johnson,
M.Â H. Key,
J.Â A. Koch,
A.Â B. Langdon,
B.Â F. Lasinski,
R.Â W. Lee,
etÂ al., Physics of Plasmas
<span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">7</span>, 2076 (2000),
ISSN 1070-664X,
eprint https://pubs.aip.org/aip/pop/article-pdf/7/5/2076/12332262/2076_1_online.pdf,
URL <a target="_blank" href="https://doi.org/10.1063/1.874030" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1063/1.874030</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snavely etÂ al. (2000)</span>
<span class="ltx_bibblock">
R.Â A. Snavely,
M.Â H. Key,
S.Â P. Hatchett,
T.Â E. Cowan,
M.Â Roth,
T.Â W. Phillips,
M.Â A. Stoyer,
E.Â A. Henry,
T.Â C. Sangster,
M.Â S. Singh,
etÂ al., Phys. Rev. Lett.
<span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">85</span>, 2945 (2000),
URL <a target="_blank" href="https://link.aps.org/doi/10.1103/PhysRevLett.85.2945" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.aps.org/doi/10.1103/PhysRevLett.85.2945</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DjordjeviÄ‡ etÂ al. (2021)</span>
<span class="ltx_bibblock">
B.Â Z. DjordjeviÄ‡,
A.Â J. Kemp,
J.Â Kim,
J.Â Ludwig,
R.Â A. Simpson,
S.Â C. Wilks,
T.Â Ma, and
D.Â A. Mariscal,
Plasma Physics and Controlled Fusion
<span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">63</span>, 094005
(2021),
URL <a target="_blank" href="https://dx.doi.org/10.1088/1361-6587/ac172a" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dx.doi.org/10.1088/1361-6587/ac172a</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ron (2023)</span>
<span class="ltx_bibblock">
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Python codes and data sets for Desai et al. 2023</em>,
<a target="_blank" href="https://github.com/ronak-n-desai/fuchs-ml" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ronak-n-desai/fuchs-ml</a>
(2023).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Orb (2023)</span>
<span class="ltx_bibblock">
<em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Python codes and data sets for Desai et al. 2023</em>,
<a target="_blank" href="http://www.asc.ohio-state.edu/orban.14/fuchs-ml/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.asc.ohio-state.edu/orban.14/fuchs-ml/</a>
(2023).

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai etÂ al. (2023a)</span>
<span class="ltx_bibblock">
R.Â Desai,
T.Â Zhang,
R.Â Oropeza,
J.Â R. Smith,
J.Â Felice,
A.Â Kryshchenko,
and C.Â Orban,
<em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Modified fuchs et al. model synthetic data sets</em>
(2023a),
URL <a target="_blank" href="https://doi.org/10.5281/zenodo.8221310" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.8221310</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han etÂ al. (2011)</span>
<span class="ltx_bibblock">
J.Â Han,
M.Â Kamber, and
J.Â Pei,
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Data Mining: Concepts and Techniques</em>, The Morgan
Kaufmann Series in Data Management Systems (Elsevier
Science, 2011), ISBN 9780123814807,
URL <a target="_blank" href="https://books.google.com/books?id=pQws07tdpjoC" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://books.google.com/books?id=pQws07tdpjoC</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smola and SchÃ¶lkopf (2004)</span>
<span class="ltx_bibblock">
A.Â Smola and
B.Â SchÃ¶lkopf,
Statistics and Computing <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">14</span>,
199 (2004).

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">cuml - rapids machine learning library</em>,
URL <a target="_blank" href="https://github.com/rapidsai/cuml" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/rapidsai/cuml</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai etÂ al. (2023b)</span>
<span class="ltx_bibblock">
R.Â Desai,
T.Â Zhang,
R.Â Oropeza,
J.Â R. Smith,
J.Â Felice,
A.Â Kryshchenko,
and C.Â Orban,
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Training Notebooks for Synthetic Ion Acceleration
Data</em> (2023b),
URL <a target="_blank" href="https://doi.org/10.5281/zenodo.8221334" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.8221334</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2017)</span>
<span class="ltx_bibblock">
D.Â P. Kingma and
J.Â Ba,
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Adam: A method for stochastic optimization</em>
(2017), eprint 1412.6980.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulz etÂ al. (2017)</span>
<span class="ltx_bibblock">
E.Â Schulz,
M.Â Speekenbrink,
and A.Â Krause,
bioRxiv (2017).

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gardner etÂ al. (2018)</span>
<span class="ltx_bibblock">
J.Â R. Gardner,
G.Â Pleiss,
D.Â Bindel,
K.Â Q. Weinberger,
and A.Â G.
Wilson, in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em> (2018).

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai etÂ al. (2023c)</span>
<span class="ltx_bibblock">
R.Â Desai,
T.Â Zhang,
R.Â Oropeza,
J.Â R. Smith,
J.Â Felice,
A.Â Kryshchenko,
and C.Â Orban,
<em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Error Metrics from ML models trained in Desai et
al.</em> (2023c),
URL <a target="_blank" href="https://doi.org/10.5281/zenodo.8221343" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.8221343</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2022)</span>
<span class="ltx_bibblock">
J.Â Wang,
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">An intuitive tutorial to gaussian processes
regression</em> (2022), eprint 2009.10862.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">YannÂ LeCun and Burges (2023)</span>
<span class="ltx_bibblock">
C.Â C. YannÂ LeCun
and C.Â Burges,
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Mnist handwritten digit database</em>,
<a target="_blank" href="http://yann.lecun.com/exdb/mnist/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://yann.lecun.com/exdb/mnist/</a>
(2023).

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandhi (2023)</span>
<span class="ltx_bibblock">
R.Â Gandhi,
<em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Support vector machine â€” introduction to machine
learning algorithms</em>,
<a target="_blank" href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47</a>
(2023).

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Suzuki (2011)</span>
<span class="ltx_bibblock">
J.-W. Xu and
K.Â Suzuki,
Medical Physics <span id="bib.bib33.1.1" class="ltx_text ltx_font_bold">38</span>,
1888 (2011).

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hornik etÂ al. (1989)</span>
<span class="ltx_bibblock">
K.Â Hornik,
M.Â Stinchcombe,
and H.Â White,
Neural Networks <span id="bib.bib34.1.1" class="ltx_text ltx_font_bold">2</span>,
359 (1989), ISSN 0893-6080,
URL <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/0893608089900208" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/0893608089900208</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leshno etÂ al. (1993)</span>
<span class="ltx_bibblock">
M.Â Leshno,
V.Â Y. Lin,
A.Â Pinkus, and
S.Â Schocken,
Neural Networks <span id="bib.bib35.1.1" class="ltx_text ltx_font_bold">6</span>,
861 (1993), ISSN 0893-6080,
URL <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0893608005801315" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0893608005801315</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.16035" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.16036" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.16036">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.16036" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.16037" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 15:52:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
