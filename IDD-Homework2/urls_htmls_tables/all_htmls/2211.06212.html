<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.06212] From Competition to Collaboration: Making Toy Datasets on Kaggle Clinically Useful for Chest X-Ray Diagnosis Using Federated Learning</title><meta property="og:description" content="Chest X-ray (CXR) datasets hosted on Kaggle, though useful from a data science competition standpoint, have limited utility in clinical use because of their narrow focus on diagnosing one specific disease. In real-worlâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="From Competition to Collaboration: Making Toy Datasets on Kaggle Clinically Useful for Chest X-Ray Diagnosis Using Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="From Competition to Collaboration: Making Toy Datasets on Kaggle Clinically Useful for Chest X-Ray Diagnosis Using Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.06212">

<!--Generated on Thu Mar 14 05:42:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">From Competition to Collaboration: Making Toy Datasets on Kaggle Clinically Useful for Chest X-Ray Diagnosis Using Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pranav Kulkarni<sup id="id7.7.id1" class="ltx_sup"><span id="id7.7.id1.1" class="ltx_text ltx_font_italic">1</span></sup> â€ƒAdway Kanhere<sup id="id8.8.id2" class="ltx_sup"><span id="id8.8.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup> â€ƒPaul H. Yi<sup id="id9.9.id3" class="ltx_sup"><span id="id9.9.id3.1" class="ltx_text ltx_font_italic">1</span></sup> â€ƒVishwa S. Parekh<sup id="id10.10.id4" class="ltx_sup"><span id="id10.10.id4.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"><sup id="id11.11.id5" class="ltx_sup"><span id="id11.11.id5.1" class="ltx_text ltx_font_italic">1</span></sup>University of Maryland Medical Intelligent Imaging (UM2ii) Center 
<br class="ltx_break">Department of Diagnostic Radiology and Nuclear Medicine 
<br class="ltx_break">University of Maryland School of Medicine 
<br class="ltx_break">Baltimore, MD 21201 
<br class="ltx_break"><span id="id12.12.id6" class="ltx_text ltx_font_typewriter">{pkulkarni, pyi, vparekh}@som.umaryland.edu</span> 
<br class="ltx_break"><sup id="id13.13.id7" class="ltx_sup"><span id="id13.13.id7.1" class="ltx_text ltx_font_italic">2</span></sup>Department of Biomedical Engineering 
<br class="ltx_break">Johns Hopkins University 
<br class="ltx_break">Baltimore, MD 21218 
<br class="ltx_break"><span id="id14.14.id8" class="ltx_text ltx_font_typewriter">akanher1@jhu.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p"><span id="id15.id1.1" class="ltx_text">Chest X-ray (CXR) datasets hosted on Kaggle, though useful from a data science competition standpoint, have limited utility in clinical use because of their narrow focus on diagnosing one specific disease. In real-world clinical use, multiple diseases need to be considered since they can co-exist in the same patient. In this work, we demonstrate how federated learning (FL) can be used to make these toy CXR datasets from Kaggle clinically useful. Specifically, we train a single FL classification model (â€˜globalâ€˜) using two separate CXR datasets â€“ one annotated for presence of pneumonia and the other for presence of pneumothorax (two common and life-threatening conditions) â€“ capable of diagnosing both. We compare the performance of the global FL model with models trained separately on both datasets (â€˜baselineâ€˜) for two different model architectures. On a standard, naive 3-layer CNN architecture, the global FL model achieved AUROC of 0.84 and 0.81 for pneumonia and pneumothorax, respectively, compared to 0.85 and 0.82, respectively, for both baseline models (p&gt;0.05). Similarly, on a pretrained DenseNet121 architecture, the global FL model achieved AUROC of 0.88 and 0.91 for pneumonia and pneumothorax, respectively, compared to 0.89 and 0.91, respectively, for both baseline models (p&gt;0.05). Our results suggest that FL can be used to create global â€˜metaâ€˜ models to make toy datasets from Kaggle clinically useful, a step forward towards bridging the gap from bench to bedside.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Chest X-Ray (CXR) is the most commonly ordered medical imaging study globally and is critical for screening many life threatening conditions (e.g., pneumonia). Accordingly, many large-scale public CXR datasets have been released through curation of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. These, in turn, have resulted in numerous data science competitions hosted on platforms like Kaggle (e.g., RSNA pneumonia detection challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>), resulting in expert-level performance for disease diagnoses.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although useful from a data science competition standpoint, these Kaggle-hosted CXR datasets have limited clinical utility because of their narrow focus on one single diagnostic task. For example, the two Kaggle CXR competitions hosted by RSNA and SIIM have focused on diagnosis of a single disease, like pneumonia and pneumothorax <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Although impressive results have resulted from these competitions, their utility is limited given the dozens of diagnoses that could present in real-world clinical practice. Therefore, a method to harmonize these toy datasets to train a clinically-useful model could revolutionize how small, narrowly-focused datasets can be leveraged in aggregate for development of clinically-relevant deep learning models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We propose CheXViz, a federated learning (FL) framework for training a single model on spatially distributed datasets with different disease annotations into a â€˜globalâ€˜ meta-deep learning model. Briefly, FL is a machine learning technique that approaches the problem from a multi-domain and multi-task perspective. By using a decentralized and distributed approach, consisting of a central server and nodes, a global â€˜metaâ€˜ model can be trained to generalize distributed tasks with non-iid labels. During each training step (â€˜FL roundâ€˜), every node trains a local model. Then, the weights across all nodes are aggregated by the central server and redistributed back to the nodes. In medical imaging, FL has enabled training of large-scale â€˜globalâ€˜ deep learning models using datasets spread across multiple institutions without sharing sensitive patient data. In this preliminary work, we demonstrate the utility of CheXViz for training a single model to diagnose pneumonia and pneumothorax using two toy datasets from Kaggle for these two respective diseases. Put another way, we demonstrate how toy datasets from Kaggle can be made clinically useful using FL.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2211.06212/assets/fl.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="279" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the CheXViz framework</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>CheXViz</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We developed CheXViz using a multi-task FL setup. The CheXViz model is initialized as a deep neural network consisting of two distinct blocks - a representation block and a task block. The CheXViz model is distributed across all the participating nodes to train their tasks. During training, only the weights corresponding to the representation block are aggregated and redistributed by the central server back to the nodes, thereby preserving task-related information for each node in their task block. Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ From Competition to Collaboration: Making Toy Datasets on Kaggle Clinically Useful for Chest X-Ray Diagnosis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the CheXViz framework.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Experimental Setup</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We evaluated the CheXViz framework for the task of training a generalized global model that can classify cases of pneumonia and pneumothorax using distributed and non-iid CXR datasets from the RSNA Pneumonia Detection and the SIIM-ACR Pneumothorax Segmentation competitions on Kaggle. Our experiments involved two different deep network architectures: A naive convolutional neural network (CNN) consisting of 3 convolutional layers (â€˜standardâ€˜ model) and a pre-trained DenseNet121 architecture using transfer learning (TL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. We implemented Federated Averaging (FedAvg) for model weight aggregation as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The standard baseline models for pneumonia and pneumothorax classification were trained for 300 epochs using a learning rate scheduler with an initial learning rate of 1e-3. During FL, we trained the standard global model for 300 FL rounds where for each round, models were locally trained for 1 epoch before aggregation. The DenseNet121 model was initialized with pre-trained ImageNet weights (â€˜base modelâ€˜) and a new classification head was trained for 30 epochs with a learning rate of 1e-3 while the rest of the base model weights were frozen and then fine tuned with a slower learning rate of 1e-5 to prevent overfitting. For the baseline models, the models were fine tuned for 150 epochs. For FL, we utilized FedAvg during the fine tuning step of TL. The global model was fine tuned for 150 FL rounds with 1 epoch before aggregation with a learning rate of 1e-5. All the models were trained and evaluated on a Google Cloud VM with four NVIDIA T4 GPUs.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">We computed sensitivity, specificity, area under precision-recall (AUPR) curve, and the area under receiver operating characteristic (AUROC) curve for each model. The AUROC values between the FL global model and the taskâ€™s baseline model were compared using bootstrapping and a paired t-test. Statistical significance was defined as <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">p</mi><mo id="S2.SS2.p3.1.m1.1.1.1" xref="S2.SS2.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><lt id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1.1"></lt><ci id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">ğ‘</ci><cn type="float" id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">p&lt;0.05</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.06212/assets/roc/roc_model_pnm.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="200" height="207" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.06212/assets/roc/roc_model_ptx.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="200" height="207" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.06212/assets/roc/roc_densenet_pnm.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="200" height="207" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.06212/assets/roc/roc_densenet_ptx.png" id="S3.F2.sf4.g1" class="ltx_graphics ltx_img_square" width="200" height="205" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>ROC curves obtained from baseline and CheXViz models evaluated across both the datasets.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The CheXViz framework trained â€˜metaâ€˜ models demonstrated excellent performance compared to the baseline models for the diagnostic classification of pneumonia and pneumothorax abnormalities, as shown in Figure <a href="#S3.F2" title="Figure 2 â€£ 3 Results â€£ From Competition to Collaboration: Making Toy Datasets on Kaggle Clinically Useful for Chest X-Ray Diagnosis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For pneumonia classification with the standard model architecture, the baseline and FL models achieved a validation AUROC of 0.85 and 0.84 respectively. We observed a similar trend with the DenseNet121 architecture, where the baseline and FL models achieved a validation AUROC of 0.89 and 0.88 respectively. For pneumothorax classification with the standard model architecture, the baseline and FL models achieved a validation AUROC of 0.82 and 0.81 respectively. Again, a similar result was achieved with the DenseNet121 architecture, where the baseline and FL models both achieved a validation AUROC of 0.91. The results are detailed in Appendix Table <a href="#A1.T1" title="Table 1 â€£ Appendix A Appendix â€£ From Competition to Collaboration: Making Toy Datasets on Kaggle Clinically Useful for Chest X-Ray Diagnosis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We further visualized the Grad-CAM outputs for evaluating the explainability and generalizability of the models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Our preliminary analysis suggests that the heatmaps from CheXViz models demonstrate higher and focused activations within the lungs, compared to the baseline models as shown in Appendix Figure <a href="#A1.F3" title="Figure 3 â€£ Appendix A Appendix â€£ From Competition to Collaboration: Making Toy Datasets on Kaggle Clinically Useful for Chest X-Ray Diagnosis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For future work, we intend to outline a methodology to quantify the generalizability of models for CXR classification using Grad-CAM heatmaps.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Although Kaggle CXR datasets and data science competitions have made an indelible impact on data science and AI for healthcare, they are still a far cry from being clinically useful datasets. This is understandable, given the challenges in curating expert-level annotations for diseases, and ostensibly why these Kaggle-hosted competitions have focused largely on single diseases<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Nevertheless, there is a gap between these toy datasets and clinical utility. Put another way, it is unclear how to use these datasets to train clinically-useful models capable of detecting multiple diseases. Our findings demonstrate that our FL framework (CheXViz) can be used to create global â€˜metaâ€˜ models to make toy datasets from Kaggle clinically useful, a large step forward towards bridging the gap from bench to bedside. Although preliminary in nature and focusing on only two datasets, our framework and results are extensible to any number of datasets and disease labels, as well as tasks beyond classification (e.g., segmentation and object detection). It is our hope that our work can be a first step towards moving Kaggle CXR datasets from competition to collaboration and transform these toy datasets into clinically useful models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. [2017]</span>
<span class="ltx_bibblock">
G.Â Huang etÂ al.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 2017.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.48550/arXiv.1608.06993</span>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Irvin etÂ al. [2019]</span>
<span class="ltx_bibblock">
J.Â Irvin, P.Â Rajpurkar, M.Â Ko, Y.Â Yu, S.Â Ciurea-Ilcus, C.Â Chute, H.Â Marklund,
B.Â Haghgoo, R.Â Ball, K.Â Shpanskaya, etÂ al.

</span>
<span class="ltx_bibblock">Chexpert: A large chest radiograph dataset with uncertainty labels
and expert comparison.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</em>, volumeÂ 33, pages 590â€“597, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. [2019]</span>
<span class="ltx_bibblock">
A.Â E. Johnson, T.Â J. Pollard, N.Â R. Greenbaum, M.Â P. Lungren, C.-y. Deng,
Y.Â Peng, Z.Â Lu, R.Â G. Mark, S.Â J. Berkowitz, and S.Â Horng.

</span>
<span class="ltx_bibblock">Mimic-cxr-jpg, a large publicly available database of labeled chest
radiographs.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.07042</em>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan and more [2017]</span>
<span class="ltx_bibblock">
B.Â McMahan and more.

</span>
<span class="ltx_bibblock">Communication-Efficient Learning of Deep Networks from Decentralized
Data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em>, volumeÂ 54 of <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">Proceedings of
Machine Learning Research</em>, pages 1273â€“1282. PMLR, 20â€“22 Apr 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen etÂ al. [2022]</span>
<span class="ltx_bibblock">
H.Â Q. Nguyen, K.Â Lam, L.Â T. Le, H.Â H. Pham, D.Â Q. Tran, D.Â B. Nguyen, D.Â D. Le,
C.Â M. Pham, H.Â T. Tong, D.Â H. Dinh, etÂ al.

</span>
<span class="ltx_bibblock">Vindr-cxr: An open dataset of chest x-rays with radiologistâ€™s
annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Scientific Data</em>, 9(1):1â€“7, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju etÂ al. [2017]</span>
<span class="ltx_bibblock">
R.Â R. Selvaraju etÂ al.

</span>
<span class="ltx_bibblock">Grad-cam: Visual explanations from deep networks via gradient-based
localization.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, 2017.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1007/s11263-019-01228-7</span>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shih etÂ al. [2019]</span>
<span class="ltx_bibblock">
G.Â Shih, C.Â C. Wu, S.Â S. Halabi, M.Â D. Kohli, L.Â M. Prevedello, T.Â S. Cook,
A.Â Sharma, J.Â K. Amorosa, V.Â Arteaga, M.Â Galperin-Aizenberg, etÂ al.

</span>
<span class="ltx_bibblock">Augmenting the national institutes of health chest radiograph dataset
with expert annotations of possible pneumonia.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Radiology. Artificial intelligence</em>, 1(1), 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. [2017]</span>
<span class="ltx_bibblock">
X.Â Wang, Y.Â Peng, L.Â Lu, Z.Â Lu, M.Â Bagheri, and R.Â M. Summers.

</span>
<span class="ltx_bibblock">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common thorax diseases.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 2097â€“2106, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi etÂ al. [2021]</span>
<span class="ltx_bibblock">
P.Â H. Yi, T.Â K. Kim, E.Â Siegel, and Y.-F.-A. N.

</span>
<span class="ltx_bibblock">Demographic reporting in publicly available chest radiograph data
sets: Opportunities for mitigating sex and racial disparities in deep
learning models.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Journal of the American College of Radiology</em>, 19(1
Pt B):192â€“200, 2021.

</span>
</li>
</ul>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Societal Impact</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Our work has a potential positive societal impact by taking an important step towards translational research. Our work can be a first step towards moving Kaggle CXR datasets from competition to collaboration and transform these toy datasets into clinically useful models. However, the use of FL can potentially lead to privacy and security risks, making the system vulnerable to client and server attacks, thereby leading to potentially negative societal impacts. We are actively working to address these vulnerabilities in FL systems.</p>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<figure id="A1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Model Metrics</figcaption>
<table id="A1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T1.1.1.1" class="ltx_tr">
<th id="A1.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A1.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></th>
<th id="A1.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A1.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="A1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Loss</span></th>
<th id="A1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Sensitivity</span></th>
<th id="A1.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Specificity</span></th>
<th id="A1.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">AUPR</span></th>
<th id="A1.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">AUROC</span></th>
<th id="A1.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T1.1.1.1.8.1" class="ltx_text ltx_font_bold">p-value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T1.1.2.1" class="ltx_tr">
<th id="A1.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Pneumonia</th>
<th id="A1.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Standard</th>
<td id="A1.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.38</td>
<td id="A1.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">82.57</td>
<td id="A1.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">71.97</td>
<td id="A1.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.63</td>
<td id="A1.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">0.85</td>
<td id="A1.T1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="A1.T1.1.3.2" class="ltx_tr">
<th id="A1.T1.1.3.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A1.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Standard w/ FL</th>
<td id="A1.T1.1.3.2.3" class="ltx_td ltx_align_center">0.39</td>
<td id="A1.T1.1.3.2.4" class="ltx_td ltx_align_center">78.01</td>
<td id="A1.T1.1.3.2.5" class="ltx_td ltx_align_center">74.41</td>
<td id="A1.T1.1.3.2.6" class="ltx_td ltx_align_center">0.61</td>
<td id="A1.T1.1.3.2.7" class="ltx_td ltx_align_center">0.84</td>
<td id="A1.T1.1.3.2.8" class="ltx_td ltx_align_center">0.10</td>
</tr>
<tr id="A1.T1.1.4.3" class="ltx_tr">
<th id="A1.T1.1.4.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A1.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DenseNet121</th>
<td id="A1.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">0.34</td>
<td id="A1.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">84.90</td>
<td id="A1.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">76.76</td>
<td id="A1.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">0.71</td>
<td id="A1.T1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">0.89</td>
<td id="A1.T1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="A1.T1.1.5.4" class="ltx_tr">
<th id="A1.T1.1.5.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A1.T1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">DenseNet121 w/ FL</th>
<td id="A1.T1.1.5.4.3" class="ltx_td ltx_align_center">0.35</td>
<td id="A1.T1.1.5.4.4" class="ltx_td ltx_align_center">80.08</td>
<td id="A1.T1.1.5.4.5" class="ltx_td ltx_align_center">79.79</td>
<td id="A1.T1.1.5.4.6" class="ltx_td ltx_align_center">0.70</td>
<td id="A1.T1.1.5.4.7" class="ltx_td ltx_align_center">0.88</td>
<td id="A1.T1.1.5.4.8" class="ltx_td ltx_align_center">0.19</td>
</tr>
<tr id="A1.T1.1.6.5" class="ltx_tr">
<th id="A1.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Pneumothorax</th>
<th id="A1.T1.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Standard</th>
<td id="A1.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.41</td>
<td id="A1.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">74.13</td>
<td id="A1.T1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">75.89</td>
<td id="A1.T1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">0.54</td>
<td id="A1.T1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t">0.82</td>
<td id="A1.T1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="A1.T1.1.7.6" class="ltx_tr">
<th id="A1.T1.1.7.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A1.T1.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Standard w/ FL</th>
<td id="A1.T1.1.7.6.3" class="ltx_td ltx_align_center">0.42</td>
<td id="A1.T1.1.7.6.4" class="ltx_td ltx_align_center">80.22</td>
<td id="A1.T1.1.7.6.5" class="ltx_td ltx_align_center">69.87</td>
<td id="A1.T1.1.7.6.6" class="ltx_td ltx_align_center">0.52</td>
<td id="A1.T1.1.7.6.7" class="ltx_td ltx_align_center">0.81</td>
<td id="A1.T1.1.7.6.8" class="ltx_td ltx_align_center">0.71</td>
</tr>
<tr id="A1.T1.1.8.7" class="ltx_tr">
<th id="A1.T1.1.8.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A1.T1.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DenseNet121</th>
<td id="A1.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">0.31</td>
<td id="A1.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">91.30</td>
<td id="A1.T1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">76.31</td>
<td id="A1.T1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t">0.73</td>
<td id="A1.T1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_t">0.91</td>
<td id="A1.T1.1.8.7.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="A1.T1.1.9.8" class="ltx_tr">
<th id="A1.T1.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<th id="A1.T1.1.9.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">DenseNet121 w/ FL</th>
<td id="A1.T1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb">0.31</td>
<td id="A1.T1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb">84.57</td>
<td id="A1.T1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb">83.10</td>
<td id="A1.T1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_bb">0.73</td>
<td id="A1.T1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_bb">0.91</td>
<td id="A1.T1.1.9.8.8" class="ltx_td ltx_align_center ltx_border_bb">0.76</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A1.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.06212/assets/gradcam/gradcam_tp_pnm.png" id="A1.F3.sf1.g1" class="ltx_graphics ltx_img_portrait" width="269" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Pneumonia</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2211.06212/assets/gradcam/gradcam_tp_ptx.png" id="A1.F3.sf2.g1" class="ltx_graphics ltx_img_portrait" width="269" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Pneumothorax</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Grad-CAM Visualization of True Positives</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.06211" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.06212" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.06212">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.06212" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.06213" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 05:42:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
