<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.14852] SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data</title><meta property="og:description" content="State-of-the-art face recognition networks are often computationally expensive and cannot be used for mobile applications.
Training lightweight face recognition models also requires large identity-labeled datasets. Mea‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.14852">

<!--Generated on Wed Feb 28 10:14:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hatef Otroshi Shahreza<sup id="id7.7.id1" class="ltx_sup"><span id="id7.7.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>, Anjith George<sup id="id8.8.id2" class="ltx_sup"><span id="id8.8.id2.1" class="ltx_text ltx_font_italic">1</span></sup>,
S√©bastien Marcel<sup id="id9.9.id3" class="ltx_sup"><span id="id9.9.id3.1" class="ltx_text ltx_font_italic">1,3</span></sup>
<br class="ltx_break"><sup id="id10.10.id4" class="ltx_sup"><span id="id10.10.id4.1" class="ltx_text ltx_font_italic">1</span></sup>Idiap Research Institute, Martigny, Switzerland
<br class="ltx_break"><sup id="id11.11.id5" class="ltx_sup"><span id="id11.11.id5.1" class="ltx_text ltx_font_italic">2</span></sup>√âcole Polytechnique F√©d√©rale de Lausanne (EPFL), Lausanne, Switzerland 
<br class="ltx_break"><sup id="id12.12.id6" class="ltx_sup"><span id="id12.12.id6.1" class="ltx_text ltx_font_italic">3</span></sup>Universit√© de Lausanne (UNIL), Lausanne, Switzerland
<br class="ltx_break"><span id="id13.13.id7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{hatef.otroshi,anjith.george,sebastien.marcel}@idiap.ch</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">State-of-the-art face recognition networks are often computationally expensive and cannot be used for mobile applications.
Training lightweight face recognition models also requires large identity-labeled datasets. Meanwhile, there are privacy and ethical concerns with collecting and using large face recognition datasets.
While generating synthetic datasets for training face recognition models is an alternative option, it is challenging to generate synthetic data with sufficient intra-class variations. In addition, there is still a considerable gap between the performance of models trained on real and synthetic data.
In this paper, we propose a new framework (named SynthDistill) to train lightweight face recognition models by distilling the knowledge of a pretrained teacher face recognition model using synthetic data.
We use a pretrained face generator network to generate synthetic face images and use the synthesized images to learn a lightweight student network. We use synthetic face images without identity labels, mitigating the problems in the intra-class variation generation of synthetic datasets.
Instead, we propose a novel dynamic sampling strategy from the intermediate latent space of the face generator network to include new variations of the challenging images while further exploring new face images in the training batch.
The results on five different face recognition datasets demonstrate the superiority of our lightweight model compared to models trained on previous synthetic datasets, achieving a verification accuracy of 99.52% on the LFW dataset with a lightweight network. The results also show that our proposed framework significantly reduces the gap between training with real and synthetic data.
The source code for replicating the experiments is publicly released.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advancements in face recognition systems have been driven by deep neural networks trained on large-scale datasets, leading to remarkable progress in accuracy¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
However, the state-of-the-art face recognition networks are often computationally heavy and the deployment of these networks on edge devices poses practical challenges. Nevertheless, it is possible to develop efficient networks from these large models that achieve comparable accuracy with significantly reduced computational load, making them suitable for edge device deployment.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.14852/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Schematic showing the proposed approach (SynthDistill). Latent space of StyleGAN is first sampled from <math id="S1.F1.3.m1.1" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><semantics id="S1.F1.3.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml">ùíµ</mi><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><ci id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1">ùíµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">\mathcal{Z}</annotation></semantics></math> space, and then dynamically re-sampled from <math id="S1.F1.4.m2.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S1.F1.4.m2.1b"><mi class="ltx_font_mathcaligraphic" id="S1.F1.4.m2.1.1" xref="S1.F1.4.m2.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><ci id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">\mathcal{W}</annotation></semantics></math> space based on teacher-student agreement. This dynamic re-sampling leads to the generation of challenging samples that facilitate efficient learning.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One strategy is training lightweight and efficient networks on the large-scale face recognition datasets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. However, training an efficient face recognition model using large-scale face recognition datasets requires access to such a dataset.
Nonetheless, large-scale face recognition datasets, such as VGGFace2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, MS-Celeb¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, WebFace¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, etc., were collected by crawling images from the Internet, thus raising legal, ethical, and privacy concerns¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
To address such concerns, recently several works proposed generating synthetic face datasets and use the synthetic face images for training face recognition models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. However, generating synthetic face datasets with sufficient inter-class and intra-class variations is still a challenging problem. Our experimental results also show that there is still a large gap in the recognition performance when training a lightweight face recognition model on real data and existing synthetic face datasets.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Another strategy to train a lightweight face recognition model is to transfer the knowledge of a model trained on a large dataset to a lightweight network through knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
Notwithstanding, the knowledge distillation from a teacher model often requires access to the original or another large-scale real dataset.
Meanwhile, access to a real dataset for knowledge distillation may not always be feasible due to the size of the datasets.
Even if there is access to real large-scale dataset, there remain ethical and legal concerns of using large-scale face recognition datasets crawled from internet.
In this work, we propose a new framework to distill the knowledge of a pretrained teacher using synthetic face images without identity labels, and thus mitigating the need for real identity-labeled data during the distillation phase. We propose dynamic sampling from the intermediate latent space of a StyleGAN to generate new images and enhance training.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In contrast to previous approaches that rely on static generation of synthetic face datasets ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and then using the generated dataset for training the FR model,
we combine these two steps with an online-generation of synthetic images and training the lightweight network in the image generation loop within a knowledge distillation based framework. This avoids the requirements of hard identity labels for the generated images, and further assists the generation network to produce challenging samples though a feedback mechanism while exploring more image variations, thus enabling the training of more robust models.
In addition, compared to previous works for the training of face recognition models on synthetic datasets, our proposed knowledge distillation framework does not require identity labels in the training, simplifying the process of generating synthetic face images.
We should also note that previous synthetic datasets still used a face recognition model in the dataset generation pipeline.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In our case, we also employ a pre-trained face recognition model in our pipeline, but with the role as a teacher. However, instead of generating a static synthetic dataset with identity labels, we dynamically create synthetic face images during the knowledge distillation process. This novel approach allows us to frame our knowledge distillation as a label-free training paradigm, utilizing synthetic data to effectively train lightweight face recognition models.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">It is noteworthy that we do not need access to the complete whitebox knowledge of the teacher network in our proposed knowledge distillation approach, and thus our method can also be used in case of a blackbox access to the teacher model that can used to generate the embeddings, given the embeddings are available.
We adapt the TinyNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> architecture and train lightweight face recognition models (called <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">TinyFaR</span>) in our knowledge distillation approach.
We provide an extensive experimental evaluation on five different face recognition benchmarking datasets,
including LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, CA-LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, CP-LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and AgeDB-30 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
Our experimental results demonstrate
the effectiveness of our approach in achieving efficient face recognition systems with reduced computational requirements, while avoiding the use of real data for knowledge distillation. This opens new possibilities for developing privacy-aware and resource-efficient face recognition models suitable for edge devices. Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>
illustrates the general block diagram of our proposed knowledge distillation framework with dynamic sampling.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The main contributions of this work are listed below:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a novel framework to train a lightweight face recognition model using knowledge distillation.
The proposed knowledge distillation framework is based on synthetic face images and does not require real training data. In addition, we do not need identity-labeled training data in our knowledge distillation framework, mitigating problems in generating synthetic face recognition datasets.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our proposed knowledge distillation framework is based on a dynamic sampling of difficult samples during training to enhance the training. Dynamic sampling helps the student network to simultaneously learn on new images (i.e., increase generalization), while focusing on difficult samples. Therefore, the training images are synthesized online and during the distillation process.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We provide extensive experimental results on different face recognition datasets, showing superior recognition accuracy for lightweight face recognition models trained in our framework compared to training lightweight face recognition from scratch using other synthetic datasets.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The remainder of the paper is organized as follows.
In Section¬†<a href="#S2" title="2 Related works ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we review the related works in the literature.
We describe our proposed framework for knowledge distillation with synthetic data using dynamic latent sampling in Section¬†<a href="#S3" title="3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We report our experimental results in Section¬†<a href="#S4" title="4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and also discuss our results in Section¬†<a href="#S5" title="5 Discussions ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Finally, the paper is concluded in Section¬†<a href="#S6" title="6 Conclusions ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we discuss the relevant literature on synthetic datasets, light-weight face recognition networks, and knowledge distillation in the context of face recognition.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Synthetic Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Several works have explored the generation of synthetic datasets for training face recognition. It is worth noting that many large-scale datasets are typically collected through web-crawling without explicit informed consent. By leveraging synthetic datasets, it becomes possible to mitigate concerns regarding the privacy of individuals while also potentially addressing issues such as bias¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. These synthetic datasets are often generated using variations of StyleGAN, 3D models, and diffusion models.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Several prior works, including FaceID-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, identity-preserving face images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, have employed synthesis techniques to generate facial images. Notably, FF-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> (e.g., 3DMM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>) and DiscoFaceGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> leverages 3D priors. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, authors proposed an approach called SynFace which incorporates the use of identity mixup (IM) and domain mixup (DM) techniques to address the performance gap. They use a small portion of labeled real data in the training process to reduce the domain gap between real and synthetic data to improve the performance. Additionally, the controllable face synthesis model provides a convenient means to manipulate various aspects of synthetic face generation, such as pose, expression, illumination, the number of identities, and samples per identity. Boutros et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, presented a method to generate synthetic data using a class conditional generative adversarial network. The authors trained the StyleGAN2-ADA model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> on the CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> datasets, using identities as class labels. They have conducted experiments using the generated SFace dataset to show its utility in training face recognition models.
Bae et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, introduced a large-scale synthetic dataset for face recognition named DigiFace-1M. This dataset was created by utilizing a computer graphics pipeline to render digital faces. Each identity within the dataset is generated by incorporating randomized variations in facial geometry, texture, and hairstyle. The rendered faces exhibit diverse attributes such as different poses, expressions, hair color, hair thickness, and density, as well as accessories. Through the implementation of aggressive data augmentation techniques, they reduced the domain gap between the generated images and real face images leading to gains in face recognition performance. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, authors proposed a Dual Condition Face Generator (DCFace) utilizing a diffusion model. This approach incorporates a novel Patch-wise style extractor and Time-step dependent ID loss, enabling DCFace to consistently generate face images depicting the same individual in different styles, while maintaining precise control over the process.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Despite the advantages of synthetic data in terms of privacy and consent, the performance of face recognition models trained on these datasets falls short when compared to models trained on real data. This severely limits real-world usage of models trained on synthetic datasets. To address these challenges, we propose a novel strategy for training face recognition models using synthetic data within an kowledge distillation framework. Our method generates data online dynamically and eliminates the need for real data during the distillation phase.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.14852/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Schematic showing the proposed approach (SynthDistill). In step 1, <math id="S2.F2.4.m1.1" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><semantics id="S2.F2.4.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S2.F2.4.m1.1.1" xref="S2.F2.4.m1.1.1.cmml">ùíµ</mi><annotation-xml encoding="MathML-Content" id="S2.F2.4.m1.1c"><ci id="S2.F2.4.m1.1.1.cmml" xref="S2.F2.4.m1.1.1">ùíµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.m1.1d">\mathcal{Z}</annotation></semantics></math> space of the StyleGAN is sampled to generate face images. In step 2, the <math id="S2.F2.5.m2.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S2.F2.5.m2.1b"><mi class="ltx_font_mathcaligraphic" id="S2.F2.5.m2.1.1" xref="S2.F2.5.m2.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S2.F2.5.m2.1c"><ci id="S2.F2.5.m2.1.1.cmml" xref="S2.F2.5.m2.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m2.1d">\mathcal{W}</annotation></semantics></math> space is re-sampled based on the teacher-student agreement to generate more challenging samples. The student model is is updated based on the distillation loss <math id="S2.F2.6.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\text{KD}}" display="inline"><semantics id="S2.F2.6.m3.1b"><msub id="S2.F2.6.m3.1.1" xref="S2.F2.6.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.F2.6.m3.1.1.2" xref="S2.F2.6.m3.1.1.2.cmml">‚Ñí</mi><mtext id="S2.F2.6.m3.1.1.3" xref="S2.F2.6.m3.1.1.3a.cmml">KD</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.F2.6.m3.1c"><apply id="S2.F2.6.m3.1.1.cmml" xref="S2.F2.6.m3.1.1"><csymbol cd="ambiguous" id="S2.F2.6.m3.1.1.1.cmml" xref="S2.F2.6.m3.1.1">subscript</csymbol><ci id="S2.F2.6.m3.1.1.2.cmml" xref="S2.F2.6.m3.1.1.2">‚Ñí</ci><ci id="S2.F2.6.m3.1.1.3a.cmml" xref="S2.F2.6.m3.1.1.3"><mtext mathsize="70%" id="S2.F2.6.m3.1.1.3.cmml" xref="S2.F2.6.m3.1.1.3">KD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m3.1d">\mathcal{L}_{\text{KD}}</annotation></semantics></math>, all the other network blocks remains frozen.
</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Efficient Face Recognition</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As edge computing gained prevalence, there is an increased focus on developing lightweight face recognition models without compromising accuracy. In the initial phase of efficient model development, Wu et al. introduced LightCNN, a lightweight architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. MobileNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> employed depth-wise separable convolutions to improve the performance. Building upon the MobileNet architecture, MobileFaceNets were designed for real-time face verification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The concept of MixConv, which incorporates multiple kernel sizes in a single convolution, was used to develop MixFaceNet networks for lightweight face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Inspired by ShuffleNetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, ShuffleFaceNet models were proposed for face recognition, with parameter counts ranging from 0.5M to 4.5M and verification accuracies exceeding 99.20% on the LFW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Neural architecture search (NAS) was utilized in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to automatically design an efficient network called PocketNet for face recognition. The PocketNet architecture was learned using the differential architecture search (DARTS) algorithm on the CASIA-WebFace dataset, and knowledge distillation (KD) was employed during training. Yan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> employed knowledge distillation (KD) and variable group convolutions to address computational intensity imbalances in face recognition networks. Alansari et al. proposed GhostFaceNets, which exploit redundancy in convolutional layers to create compact networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. These modules generate a fixed percentage of convolutional feature maps using computationally inexpensive depth-wise convolutions. Recently, George et al. introduced EdgeFace, a combination of CNN-Transformer architecture that achieved strong verification performance with minimal FLOP and parameter complexity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Knowledge Distillation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The concept of Knowledge Distillation was first introduced by Hinton et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. The primary goal of knowledge distillation is to transfer the knowledge from a pre-trained, complex ‚Äúteacher‚Äù model to a simpler, more efficient ‚Äústudent‚Äù model. The methods for distillation in classification tasks can primarily be learned through the utilization of soft labels from a teacher and ground truth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Another approach involves feature-based learning, where the student aims to match the intermediate layers of the teacher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Additionally, contrastive-based methods have also been employed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> for distilling the knowledge of a teacher to a student.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Over the years, several methods have been proposed in the literature
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to enhance the efficiency of distillation. However, most of these methods rely on the availability of original or similar training datasets, which can be limited due to security and privacy concerns. Consequently, traditional data-dependent distillation methods become impractical. To address this challenge, researchers have introduced Data-free knowledge distillation (DFKD), without relying on the original or real training data. DFKD aims to develop a distillation strategy using a synthesis-based approach. These approaches utilize either whitebox teacher model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> or data augmentation techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to generate synthetic samples. These synthetic samples act as substitute training datasets for distillation. By training on such synthetic data, the student model can effectively learn from the teacher model without needing access to real training data making it privacy friendly.
Along the same lines, Boutros et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
proposed an unsupervised face recognition model
based on unlabeled synthetic data. They used contrastive learning to maximize the similarity between two augmented images (using geometric and color transformations) of the same synthetic image. However, since the data augmentation cannot provide enough inter-class variations, it affects the performance of trained face recognition model when evaluating on benchmark datasets.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Framework</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we describe our proposed framework for training a lightweight face recognition model using synthetic data using knowledge distillation. We describe the architecture of lightweight face recognition model in Section¬†<a href="#S3.SS1" title="3.1 Lightweight Network Architecture ‚Ä£ 3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and explain our knowledge distillation framework using synthetic data in Section¬†<a href="#S3.SS2" title="3.2 Knowledge Distillation with Synthetic Data ‚Ä£ 3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Lightweight Network Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As discussed in Section¬†<a href="#S2" title="2 Related works ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, lightweight face recognition models in the literature usually adapt lightweight neural network models for face recognition tasks. However, our knowledge distillation framework can be applied to any lightweight model with only the condition that the output of the lightweight network should have the same dimensions as the embedding of the teacher model. To eliminate this condition so that the proposed framework can be used for any lightweight network with different output sizes, we use a fully connected layer at the output of the lightweight network to have output with the same size as the teacher model.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In this paper, we use TinyNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> as the backbone for the lightweight FR model.
The TinyNet is an optimized version of
EfficientNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, which uses a structure that simultaneously enlarges the
resolution, depth, and width in a Rubik‚Äôs cube for neural networks and find networks with high efficiency by changing these three dimensions. However, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> show that the resolution and depth are more important than width for small networks, and propose smaller models derived from the EfficientNet-B0 as different variations of TinyNet, which are efficient and achieve high accuracy in recognition tasks.
The feature layer of TinyNet has 1280 dimensions and the embedding of our teacher network has 512 dimensions. Therefore, we add a fully connected layer to generate 512-length feature at the output of TinyNet and call our lightweight face recognition network based on TinyNet <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">TinyFaR</span>.
We should note that to our knowledge, TinyNet lightweight network structure has not been used before for face recognition in the literature.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2308.14852/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="356" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Schematic showing the re-sampling strategy in the proposed approach. When teacher-student agreement is high, the re-sampling method generates diverse images. Conversely, when the similarity is low, i.e, when the given sample is challenging, re-sampling generates similar (challenging) samples facilitating the learning.
</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Knowledge Distillation with Synthetic Data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.10" class="ltx_p">Let <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="F_{\text{T}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">F</mi><mtext id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ùêπ</ci><ci id="S3.SS2.p1.1.m1.1.1.3a.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">F_{\text{T}}</annotation></semantics></math> and <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="F_{\text{S}}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">F</mi><mtext id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3a.cmml">S</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ùêπ</ci><ci id="S3.SS2.p1.2.m2.1.1.3a.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">F_{\text{S}}</annotation></semantics></math> denote the teacher<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note that the teacher model can be blackbox and we do not use teacher‚Äôs gradients in our method.</span></span></span> and student (lightweight) face recognition models, respectively.
In this paper, we consider StyleGAN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> as a pretrained face generator model, which consists of a mapping network <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">M</annotation></semantics></math> and a generator network <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">G</annotation></semantics></math>. The mapping network takes a a noise <math id="S3.SS2.p1.5.m5.2" class="ltx_Math" alttext="\bm{z}\in\mathcal{Z}\sim N(0,\mathbb{I})" display="inline"><semantics id="S3.SS2.p1.5.m5.2a"><mrow id="S3.SS2.p1.5.m5.2.3" xref="S3.SS2.p1.5.m5.2.3.cmml"><mi id="S3.SS2.p1.5.m5.2.3.2" xref="S3.SS2.p1.5.m5.2.3.2.cmml">ùíõ</mi><mo id="S3.SS2.p1.5.m5.2.3.3" xref="S3.SS2.p1.5.m5.2.3.3.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.5.m5.2.3.4" xref="S3.SS2.p1.5.m5.2.3.4.cmml">ùíµ</mi><mo id="S3.SS2.p1.5.m5.2.3.5" xref="S3.SS2.p1.5.m5.2.3.5.cmml">‚àº</mo><mrow id="S3.SS2.p1.5.m5.2.3.6" xref="S3.SS2.p1.5.m5.2.3.6.cmml"><mi id="S3.SS2.p1.5.m5.2.3.6.2" xref="S3.SS2.p1.5.m5.2.3.6.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.2.3.6.1" xref="S3.SS2.p1.5.m5.2.3.6.1.cmml">‚Äã</mo><mrow id="S3.SS2.p1.5.m5.2.3.6.3.2" xref="S3.SS2.p1.5.m5.2.3.6.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.5.m5.2.3.6.3.2.1" xref="S3.SS2.p1.5.m5.2.3.6.3.1.cmml">(</mo><mn id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">0</mn><mo id="S3.SS2.p1.5.m5.2.3.6.3.2.2" xref="S3.SS2.p1.5.m5.2.3.6.3.1.cmml">,</mo><mi id="S3.SS2.p1.5.m5.2.2" xref="S3.SS2.p1.5.m5.2.2.cmml">ùïÄ</mi><mo stretchy="false" id="S3.SS2.p1.5.m5.2.3.6.3.2.3" xref="S3.SS2.p1.5.m5.2.3.6.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.2b"><apply id="S3.SS2.p1.5.m5.2.3.cmml" xref="S3.SS2.p1.5.m5.2.3"><and id="S3.SS2.p1.5.m5.2.3a.cmml" xref="S3.SS2.p1.5.m5.2.3"></and><apply id="S3.SS2.p1.5.m5.2.3b.cmml" xref="S3.SS2.p1.5.m5.2.3"><in id="S3.SS2.p1.5.m5.2.3.3.cmml" xref="S3.SS2.p1.5.m5.2.3.3"></in><ci id="S3.SS2.p1.5.m5.2.3.2.cmml" xref="S3.SS2.p1.5.m5.2.3.2">ùíõ</ci><ci id="S3.SS2.p1.5.m5.2.3.4.cmml" xref="S3.SS2.p1.5.m5.2.3.4">ùíµ</ci></apply><apply id="S3.SS2.p1.5.m5.2.3c.cmml" xref="S3.SS2.p1.5.m5.2.3"><csymbol cd="latexml" id="S3.SS2.p1.5.m5.2.3.5.cmml" xref="S3.SS2.p1.5.m5.2.3.5">similar-to</csymbol><share href="#S3.SS2.p1.5.m5.2.3.4.cmml" id="S3.SS2.p1.5.m5.2.3d.cmml" xref="S3.SS2.p1.5.m5.2.3"></share><apply id="S3.SS2.p1.5.m5.2.3.6.cmml" xref="S3.SS2.p1.5.m5.2.3.6"><times id="S3.SS2.p1.5.m5.2.3.6.1.cmml" xref="S3.SS2.p1.5.m5.2.3.6.1"></times><ci id="S3.SS2.p1.5.m5.2.3.6.2.cmml" xref="S3.SS2.p1.5.m5.2.3.6.2">ùëÅ</ci><interval closure="open" id="S3.SS2.p1.5.m5.2.3.6.3.1.cmml" xref="S3.SS2.p1.5.m5.2.3.6.3.2"><cn type="integer" id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">0</cn><ci id="S3.SS2.p1.5.m5.2.2.cmml" xref="S3.SS2.p1.5.m5.2.2">ùïÄ</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.2c">\bm{z}\in\mathcal{Z}\sim N(0,\mathbb{I})</annotation></semantics></math> from input latent space <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">ùíµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">ùíµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">\mathcal{Z}</annotation></semantics></math> with Gaussian distribution and generates an intermediate latent code <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="\bm{w}\in\mathcal{W}" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mrow id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml">ùíò</mi><mo id="S3.SS2.p1.7.m7.1.1.1" xref="S3.SS2.p1.7.m7.1.1.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml">ùí≤</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><in id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1.1"></in><ci id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2">ùíò</ci><ci id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3">ùí≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">\bm{w}\in\mathcal{W}</annotation></semantics></math>. Then, the intermediate latent code <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="\bm{w}" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">ùíò</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">ùíò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">\bm{w}</annotation></semantics></math> is used by the generator network to generate a face image <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="I=G(\bm{w})" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><mrow id="S3.SS2.p1.9.m9.1.2" xref="S3.SS2.p1.9.m9.1.2.cmml"><mi id="S3.SS2.p1.9.m9.1.2.2" xref="S3.SS2.p1.9.m9.1.2.2.cmml">I</mi><mo id="S3.SS2.p1.9.m9.1.2.1" xref="S3.SS2.p1.9.m9.1.2.1.cmml">=</mo><mrow id="S3.SS2.p1.9.m9.1.2.3" xref="S3.SS2.p1.9.m9.1.2.3.cmml"><mi id="S3.SS2.p1.9.m9.1.2.3.2" xref="S3.SS2.p1.9.m9.1.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.9.m9.1.2.3.1" xref="S3.SS2.p1.9.m9.1.2.3.1.cmml">‚Äã</mo><mrow id="S3.SS2.p1.9.m9.1.2.3.3.2" xref="S3.SS2.p1.9.m9.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.p1.9.m9.1.2.3.3.2.1" xref="S3.SS2.p1.9.m9.1.2.3.cmml">(</mo><mi id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">ùíò</mi><mo stretchy="false" id="S3.SS2.p1.9.m9.1.2.3.3.2.2" xref="S3.SS2.p1.9.m9.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><apply id="S3.SS2.p1.9.m9.1.2.cmml" xref="S3.SS2.p1.9.m9.1.2"><eq id="S3.SS2.p1.9.m9.1.2.1.cmml" xref="S3.SS2.p1.9.m9.1.2.1"></eq><ci id="S3.SS2.p1.9.m9.1.2.2.cmml" xref="S3.SS2.p1.9.m9.1.2.2">ùêº</ci><apply id="S3.SS2.p1.9.m9.1.2.3.cmml" xref="S3.SS2.p1.9.m9.1.2.3"><times id="S3.SS2.p1.9.m9.1.2.3.1.cmml" xref="S3.SS2.p1.9.m9.1.2.3.1"></times><ci id="S3.SS2.p1.9.m9.1.2.3.2.cmml" xref="S3.SS2.p1.9.m9.1.2.3.2">ùê∫</ci><ci id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">ùíò</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">I=G(\bm{w})</annotation></semantics></math>.
In our knowledge distillation framework, we first generate a batch of synthetic face images and extract the teacher‚Äôs embeddings <math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="e_{\text{T}}=F_{\text{T}}(I)" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><mrow id="S3.SS2.p1.10.m10.1.2" xref="S3.SS2.p1.10.m10.1.2.cmml"><msub id="S3.SS2.p1.10.m10.1.2.2" xref="S3.SS2.p1.10.m10.1.2.2.cmml"><mi id="S3.SS2.p1.10.m10.1.2.2.2" xref="S3.SS2.p1.10.m10.1.2.2.2.cmml">e</mi><mtext id="S3.SS2.p1.10.m10.1.2.2.3" xref="S3.SS2.p1.10.m10.1.2.2.3a.cmml">T</mtext></msub><mo id="S3.SS2.p1.10.m10.1.2.1" xref="S3.SS2.p1.10.m10.1.2.1.cmml">=</mo><mrow id="S3.SS2.p1.10.m10.1.2.3" xref="S3.SS2.p1.10.m10.1.2.3.cmml"><msub id="S3.SS2.p1.10.m10.1.2.3.2" xref="S3.SS2.p1.10.m10.1.2.3.2.cmml"><mi id="S3.SS2.p1.10.m10.1.2.3.2.2" xref="S3.SS2.p1.10.m10.1.2.3.2.2.cmml">F</mi><mtext id="S3.SS2.p1.10.m10.1.2.3.2.3" xref="S3.SS2.p1.10.m10.1.2.3.2.3a.cmml">T</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.10.m10.1.2.3.1" xref="S3.SS2.p1.10.m10.1.2.3.1.cmml">‚Äã</mo><mrow id="S3.SS2.p1.10.m10.1.2.3.3.2" xref="S3.SS2.p1.10.m10.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.p1.10.m10.1.2.3.3.2.1" xref="S3.SS2.p1.10.m10.1.2.3.cmml">(</mo><mi id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">I</mi><mo stretchy="false" id="S3.SS2.p1.10.m10.1.2.3.3.2.2" xref="S3.SS2.p1.10.m10.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><apply id="S3.SS2.p1.10.m10.1.2.cmml" xref="S3.SS2.p1.10.m10.1.2"><eq id="S3.SS2.p1.10.m10.1.2.1.cmml" xref="S3.SS2.p1.10.m10.1.2.1"></eq><apply id="S3.SS2.p1.10.m10.1.2.2.cmml" xref="S3.SS2.p1.10.m10.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.1.2.2.1.cmml" xref="S3.SS2.p1.10.m10.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.10.m10.1.2.2.2.cmml" xref="S3.SS2.p1.10.m10.1.2.2.2">ùëí</ci><ci id="S3.SS2.p1.10.m10.1.2.2.3a.cmml" xref="S3.SS2.p1.10.m10.1.2.2.3"><mtext mathsize="70%" id="S3.SS2.p1.10.m10.1.2.2.3.cmml" xref="S3.SS2.p1.10.m10.1.2.2.3">T</mtext></ci></apply><apply id="S3.SS2.p1.10.m10.1.2.3.cmml" xref="S3.SS2.p1.10.m10.1.2.3"><times id="S3.SS2.p1.10.m10.1.2.3.1.cmml" xref="S3.SS2.p1.10.m10.1.2.3.1"></times><apply id="S3.SS2.p1.10.m10.1.2.3.2.cmml" xref="S3.SS2.p1.10.m10.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.1.2.3.2.1.cmml" xref="S3.SS2.p1.10.m10.1.2.3.2">subscript</csymbol><ci id="S3.SS2.p1.10.m10.1.2.3.2.2.cmml" xref="S3.SS2.p1.10.m10.1.2.3.2.2">ùêπ</ci><ci id="S3.SS2.p1.10.m10.1.2.3.2.3a.cmml" xref="S3.SS2.p1.10.m10.1.2.3.2.3"><mtext mathsize="70%" id="S3.SS2.p1.10.m10.1.2.3.2.3.cmml" xref="S3.SS2.p1.10.m10.1.2.3.2.3">T</mtext></ci></apply><ci id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">ùêº</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">e_{\text{T}}=F_{\text{T}}(I)</annotation></semantics></math>.
Then, we train the student network by minimizing the mean squared error (MSE) of the teacher and student‚Äôs embeddings as follows:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathcal{L}_{\text{KD}}=\left\lVert\bm{e}_{\text{T}}-F_{\text{S}}(I)\right\rVert_{2}^{2}." display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml">‚Ñí</mi><mtext id="S3.E1.m1.2.2.1.1.3.3" xref="S3.E1.m1.2.2.1.1.3.3a.cmml">KD</mtext></msub><mo rspace="0.1389em" id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">=</mo><msubsup id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml"><mo fence="true" lspace="0.1389em" rspace="0em" stretchy="true" id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.1.cmml">‚à•</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">ùíÜ</mi><mtext id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3a.cmml">T</mtext></msub><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml">F</mi><mtext id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3a.cmml">S</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">I</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo fence="true" lspace="0em" rspace="0em" stretchy="true" id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.2.1.cmml">‚à•</mo></mrow><mn id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">2</mn><mn id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml">2</mn></msubsup></mrow><mo lspace="0em" id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"></eq><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2">‚Ñí</ci><ci id="S3.E1.m1.2.2.1.1.3.3a.cmml" xref="S3.E1.m1.2.2.1.1.3.3"><mtext mathsize="70%" id="S3.E1.m1.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3">KD</mtext></ci></apply><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">delimited-‚à•‚à•</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1"><minus id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.2">ùíÜ</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3a.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3">T</mtext></ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3"><times id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.1"></times><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2">ùêπ</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3a.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3">S</mtext></ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ùêº</ci></apply></apply></apply><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathcal{L}_{\text{KD}}=\left\lVert\bm{e}_{\text{T}}-F_{\text{S}}(I)\right\rVert_{2}^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Minimizing the MSE of embeddings helps the student network to extract embeddings similar to the teacher‚Äôs embeddings from a given face image.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Our proposed knowledge distillation approach</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="n_{\text{epoch}}" display="inline"><semantics id="alg1.l1.m1.1a"><msub id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml"><mi id="alg1.l1.m1.1.1.2" xref="alg1.l1.m1.1.1.2.cmml">n</mi><mtext id="alg1.l1.m1.1.1.3" xref="alg1.l1.m1.1.1.3a.cmml">epoch</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><apply id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1"><csymbol cd="ambiguous" id="alg1.l1.m1.1.1.1.cmml" xref="alg1.l1.m1.1.1">subscript</csymbol><ci id="alg1.l1.m1.1.1.2.cmml" xref="alg1.l1.m1.1.1.2">ùëõ</ci><ci id="alg1.l1.m1.1.1.3a.cmml" xref="alg1.l1.m1.1.1.3"><mtext mathsize="70%" id="alg1.l1.m1.1.1.3.cmml" xref="alg1.l1.m1.1.1.3">epoch</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">n_{\text{epoch}}</annotation></semantics></math>: number of epochs, <math id="alg1.l1.m2.1" class="ltx_Math" alttext="n_{\text{iteration}}" display="inline"><semantics id="alg1.l1.m2.1a"><msub id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">n</mi><mtext id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3a.cmml">iteration</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><csymbol cd="ambiguous" id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1">subscript</csymbol><ci id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2">ùëõ</ci><ci id="alg1.l1.m2.1.1.3a.cmml" xref="alg1.l1.m2.1.1.3"><mtext mathsize="70%" id="alg1.l1.m2.1.1.3.cmml" xref="alg1.l1.m2.1.1.3">iteration</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">n_{\text{iteration}}</annotation></semantics></math>: number of iterations in each epoch, <math id="alg1.l1.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">\alpha</annotation></semantics></math>: learning rate, <math id="alg1.l1.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="alg1.l1.m4.1a"><mi id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><ci id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">c</annotation></semantics></math>: re-sampling coefficient.

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text ltx_font_bold">procedure</span>¬†<span id="alg1.l2.3" class="ltx_text ltx_font_smallcaps">Training</span>

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span>¬†¬†¬†¬†¬†Initialize weights <math id="alg1.l3.m1.1" class="ltx_Math" alttext="\theta_{\text{S}}" display="inline"><semantics id="alg1.l3.m1.1a"><msub id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">Œ∏</mi><mtext id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3a.cmml">S</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">ùúÉ</ci><ci id="alg1.l3.m1.1.1.3a.cmml" xref="alg1.l3.m1.1.1.3"><mtext mathsize="70%" id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">\theta_{\text{S}}</annotation></semantics></math> of the student network

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>¬†¬†¬†¬†¬†<span id="alg1.l4.2" class="ltx_text ltx_font_bold">for</span>¬†<math id="alg1.l4.m1.3" class="ltx_Math" alttext="\text{epoch}=1,...,n_{\text{epoch}}" display="inline"><semantics id="alg1.l4.m1.3a"><mrow id="alg1.l4.m1.3.3" xref="alg1.l4.m1.3.3.cmml"><mtext id="alg1.l4.m1.3.3.3" xref="alg1.l4.m1.3.3.3a.cmml">epoch</mtext><mo id="alg1.l4.m1.3.3.2" xref="alg1.l4.m1.3.3.2.cmml">=</mo><mrow id="alg1.l4.m1.3.3.1.1" xref="alg1.l4.m1.3.3.1.2.cmml"><mn id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">1</mn><mo id="alg1.l4.m1.3.3.1.1.2" xref="alg1.l4.m1.3.3.1.2.cmml">,</mo><mi mathvariant="normal" id="alg1.l4.m1.2.2" xref="alg1.l4.m1.2.2.cmml">‚Ä¶</mi><mo id="alg1.l4.m1.3.3.1.1.3" xref="alg1.l4.m1.3.3.1.2.cmml">,</mo><msub id="alg1.l4.m1.3.3.1.1.1" xref="alg1.l4.m1.3.3.1.1.1.cmml"><mi id="alg1.l4.m1.3.3.1.1.1.2" xref="alg1.l4.m1.3.3.1.1.1.2.cmml">n</mi><mtext id="alg1.l4.m1.3.3.1.1.1.3" xref="alg1.l4.m1.3.3.1.1.1.3a.cmml">epoch</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.3b"><apply id="alg1.l4.m1.3.3.cmml" xref="alg1.l4.m1.3.3"><eq id="alg1.l4.m1.3.3.2.cmml" xref="alg1.l4.m1.3.3.2"></eq><ci id="alg1.l4.m1.3.3.3a.cmml" xref="alg1.l4.m1.3.3.3"><mtext id="alg1.l4.m1.3.3.3.cmml" xref="alg1.l4.m1.3.3.3">epoch</mtext></ci><list id="alg1.l4.m1.3.3.1.2.cmml" xref="alg1.l4.m1.3.3.1.1"><cn type="integer" id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">1</cn><ci id="alg1.l4.m1.2.2.cmml" xref="alg1.l4.m1.2.2">‚Ä¶</ci><apply id="alg1.l4.m1.3.3.1.1.1.cmml" xref="alg1.l4.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="alg1.l4.m1.3.3.1.1.1.1.cmml" xref="alg1.l4.m1.3.3.1.1.1">subscript</csymbol><ci id="alg1.l4.m1.3.3.1.1.1.2.cmml" xref="alg1.l4.m1.3.3.1.1.1.2">ùëõ</ci><ci id="alg1.l4.m1.3.3.1.1.1.3a.cmml" xref="alg1.l4.m1.3.3.1.1.1.3"><mtext mathsize="70%" id="alg1.l4.m1.3.3.1.1.1.3.cmml" xref="alg1.l4.m1.3.3.1.1.1.3">epoch</mtext></ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.3c">\text{epoch}=1,...,n_{\text{epoch}}</annotation></semantics></math>¬†<span id="alg1.l4.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†<span id="alg1.l5.2" class="ltx_text ltx_font_bold">for</span>¬†<math id="alg1.l5.m1.3" class="ltx_Math" alttext="\text{itr}=1,...,n_{\text{iteration}}" display="inline"><semantics id="alg1.l5.m1.3a"><mrow id="alg1.l5.m1.3.3" xref="alg1.l5.m1.3.3.cmml"><mtext id="alg1.l5.m1.3.3.3" xref="alg1.l5.m1.3.3.3a.cmml">itr</mtext><mo id="alg1.l5.m1.3.3.2" xref="alg1.l5.m1.3.3.2.cmml">=</mo><mrow id="alg1.l5.m1.3.3.1.1" xref="alg1.l5.m1.3.3.1.2.cmml"><mn id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">1</mn><mo id="alg1.l5.m1.3.3.1.1.2" xref="alg1.l5.m1.3.3.1.2.cmml">,</mo><mi mathvariant="normal" id="alg1.l5.m1.2.2" xref="alg1.l5.m1.2.2.cmml">‚Ä¶</mi><mo id="alg1.l5.m1.3.3.1.1.3" xref="alg1.l5.m1.3.3.1.2.cmml">,</mo><msub id="alg1.l5.m1.3.3.1.1.1" xref="alg1.l5.m1.3.3.1.1.1.cmml"><mi id="alg1.l5.m1.3.3.1.1.1.2" xref="alg1.l5.m1.3.3.1.1.1.2.cmml">n</mi><mtext id="alg1.l5.m1.3.3.1.1.1.3" xref="alg1.l5.m1.3.3.1.1.1.3a.cmml">iteration</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.3b"><apply id="alg1.l5.m1.3.3.cmml" xref="alg1.l5.m1.3.3"><eq id="alg1.l5.m1.3.3.2.cmml" xref="alg1.l5.m1.3.3.2"></eq><ci id="alg1.l5.m1.3.3.3a.cmml" xref="alg1.l5.m1.3.3.3"><mtext id="alg1.l5.m1.3.3.3.cmml" xref="alg1.l5.m1.3.3.3">itr</mtext></ci><list id="alg1.l5.m1.3.3.1.2.cmml" xref="alg1.l5.m1.3.3.1.1"><cn type="integer" id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">1</cn><ci id="alg1.l5.m1.2.2.cmml" xref="alg1.l5.m1.2.2">‚Ä¶</ci><apply id="alg1.l5.m1.3.3.1.1.1.cmml" xref="alg1.l5.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="alg1.l5.m1.3.3.1.1.1.1.cmml" xref="alg1.l5.m1.3.3.1.1.1">subscript</csymbol><ci id="alg1.l5.m1.3.3.1.1.1.2.cmml" xref="alg1.l5.m1.3.3.1.1.1.2">ùëõ</ci><ci id="alg1.l5.m1.3.3.1.1.1.3a.cmml" xref="alg1.l5.m1.3.3.1.1.1.3"><mtext mathsize="70%" id="alg1.l5.m1.3.3.1.1.1.3.cmml" xref="alg1.l5.m1.3.3.1.1.1.3">iteration</mtext></ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.3c">\text{itr}=1,...,n_{\text{iteration}}</annotation></semantics></math>¬†<span id="alg1.l5.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†<span id="alg1.l6.2" class="ltx_text ltx_font_bold">Step 1</span>: Train with random samples

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉSample a batch of random noise vectors:

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l8.m1.2" class="ltx_Math" alttext="\bm{z}\in\mathcal{Z}\sim\mathcal{N}(0,\mathbb{I})" display="inline"><semantics id="alg1.l8.m1.2a"><mrow id="alg1.l8.m1.2.3" xref="alg1.l8.m1.2.3.cmml"><mi id="alg1.l8.m1.2.3.2" xref="alg1.l8.m1.2.3.2.cmml">ùíõ</mi><mo id="alg1.l8.m1.2.3.3" xref="alg1.l8.m1.2.3.3.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" id="alg1.l8.m1.2.3.4" xref="alg1.l8.m1.2.3.4.cmml">ùíµ</mi><mo id="alg1.l8.m1.2.3.5" xref="alg1.l8.m1.2.3.5.cmml">‚àº</mo><mrow id="alg1.l8.m1.2.3.6" xref="alg1.l8.m1.2.3.6.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l8.m1.2.3.6.2" xref="alg1.l8.m1.2.3.6.2.cmml">ùí©</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.2.3.6.1" xref="alg1.l8.m1.2.3.6.1.cmml">‚Äã</mo><mrow id="alg1.l8.m1.2.3.6.3.2" xref="alg1.l8.m1.2.3.6.3.1.cmml"><mo stretchy="false" id="alg1.l8.m1.2.3.6.3.2.1" xref="alg1.l8.m1.2.3.6.3.1.cmml">(</mo><mn id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml">0</mn><mo id="alg1.l8.m1.2.3.6.3.2.2" xref="alg1.l8.m1.2.3.6.3.1.cmml">,</mo><mi id="alg1.l8.m1.2.2" xref="alg1.l8.m1.2.2.cmml">ùïÄ</mi><mo stretchy="false" id="alg1.l8.m1.2.3.6.3.2.3" xref="alg1.l8.m1.2.3.6.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.2b"><apply id="alg1.l8.m1.2.3.cmml" xref="alg1.l8.m1.2.3"><and id="alg1.l8.m1.2.3a.cmml" xref="alg1.l8.m1.2.3"></and><apply id="alg1.l8.m1.2.3b.cmml" xref="alg1.l8.m1.2.3"><in id="alg1.l8.m1.2.3.3.cmml" xref="alg1.l8.m1.2.3.3"></in><ci id="alg1.l8.m1.2.3.2.cmml" xref="alg1.l8.m1.2.3.2">ùíõ</ci><ci id="alg1.l8.m1.2.3.4.cmml" xref="alg1.l8.m1.2.3.4">ùíµ</ci></apply><apply id="alg1.l8.m1.2.3c.cmml" xref="alg1.l8.m1.2.3"><csymbol cd="latexml" id="alg1.l8.m1.2.3.5.cmml" xref="alg1.l8.m1.2.3.5">similar-to</csymbol><share href="#alg1.l8.m1.2.3.4.cmml" id="alg1.l8.m1.2.3d.cmml" xref="alg1.l8.m1.2.3"></share><apply id="alg1.l8.m1.2.3.6.cmml" xref="alg1.l8.m1.2.3.6"><times id="alg1.l8.m1.2.3.6.1.cmml" xref="alg1.l8.m1.2.3.6.1"></times><ci id="alg1.l8.m1.2.3.6.2.cmml" xref="alg1.l8.m1.2.3.6.2">ùí©</ci><interval closure="open" id="alg1.l8.m1.2.3.6.3.1.cmml" xref="alg1.l8.m1.2.3.6.3.2"><cn type="integer" id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">0</cn><ci id="alg1.l8.m1.2.2.cmml" xref="alg1.l8.m1.2.2">ùïÄ</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.2c">\bm{z}\in\mathcal{Z}\sim\mathcal{N}(0,\mathbb{I})</annotation></semantics></math>

</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉGenerate synthetic face images:

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l10.m1.1" class="ltx_Math" alttext="\bm{w}=M(\bm{z})" display="inline"><semantics id="alg1.l10.m1.1a"><mrow id="alg1.l10.m1.1.2" xref="alg1.l10.m1.1.2.cmml"><mi id="alg1.l10.m1.1.2.2" xref="alg1.l10.m1.1.2.2.cmml">ùíò</mi><mo id="alg1.l10.m1.1.2.1" xref="alg1.l10.m1.1.2.1.cmml">=</mo><mrow id="alg1.l10.m1.1.2.3" xref="alg1.l10.m1.1.2.3.cmml"><mi id="alg1.l10.m1.1.2.3.2" xref="alg1.l10.m1.1.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.2.3.1" xref="alg1.l10.m1.1.2.3.1.cmml">‚Äã</mo><mrow id="alg1.l10.m1.1.2.3.3.2" xref="alg1.l10.m1.1.2.3.cmml"><mo stretchy="false" id="alg1.l10.m1.1.2.3.3.2.1" xref="alg1.l10.m1.1.2.3.cmml">(</mo><mi id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml">ùíõ</mi><mo stretchy="false" id="alg1.l10.m1.1.2.3.3.2.2" xref="alg1.l10.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.2.cmml" xref="alg1.l10.m1.1.2"><eq id="alg1.l10.m1.1.2.1.cmml" xref="alg1.l10.m1.1.2.1"></eq><ci id="alg1.l10.m1.1.2.2.cmml" xref="alg1.l10.m1.1.2.2">ùíò</ci><apply id="alg1.l10.m1.1.2.3.cmml" xref="alg1.l10.m1.1.2.3"><times id="alg1.l10.m1.1.2.3.1.cmml" xref="alg1.l10.m1.1.2.3.1"></times><ci id="alg1.l10.m1.1.2.3.2.cmml" xref="alg1.l10.m1.1.2.3.2">ùëÄ</ci><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">ùíõ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">\bm{w}=M(\bm{z})</annotation></semantics></math>

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l11.m1.1" class="ltx_Math" alttext="\bm{I}=G(\bm{w})" display="inline"><semantics id="alg1.l11.m1.1a"><mrow id="alg1.l11.m1.1.2" xref="alg1.l11.m1.1.2.cmml"><mi id="alg1.l11.m1.1.2.2" xref="alg1.l11.m1.1.2.2.cmml">ùë∞</mi><mo id="alg1.l11.m1.1.2.1" xref="alg1.l11.m1.1.2.1.cmml">=</mo><mrow id="alg1.l11.m1.1.2.3" xref="alg1.l11.m1.1.2.3.cmml"><mi id="alg1.l11.m1.1.2.3.2" xref="alg1.l11.m1.1.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.2.3.1" xref="alg1.l11.m1.1.2.3.1.cmml">‚Äã</mo><mrow id="alg1.l11.m1.1.2.3.3.2" xref="alg1.l11.m1.1.2.3.cmml"><mo stretchy="false" id="alg1.l11.m1.1.2.3.3.2.1" xref="alg1.l11.m1.1.2.3.cmml">(</mo><mi id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">ùíò</mi><mo stretchy="false" id="alg1.l11.m1.1.2.3.3.2.2" xref="alg1.l11.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.2.cmml" xref="alg1.l11.m1.1.2"><eq id="alg1.l11.m1.1.2.1.cmml" xref="alg1.l11.m1.1.2.1"></eq><ci id="alg1.l11.m1.1.2.2.cmml" xref="alg1.l11.m1.1.2.2">ùë∞</ci><apply id="alg1.l11.m1.1.2.3.cmml" xref="alg1.l11.m1.1.2.3"><times id="alg1.l11.m1.1.2.3.1.cmml" xref="alg1.l11.m1.1.2.3.1"></times><ci id="alg1.l11.m1.1.2.3.2.cmml" xref="alg1.l11.m1.1.2.3.2">ùê∫</ci><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">ùíò</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">\bm{I}=G(\bm{w})</annotation></semantics></math>

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉExtract teacher‚Äôs embeddings <math id="alg1.l12.m1.1" class="ltx_Math" alttext="e_{\text{T}}" display="inline"><semantics id="alg1.l12.m1.1a"><msub id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml"><mi id="alg1.l12.m1.1.1.2" xref="alg1.l12.m1.1.1.2.cmml">e</mi><mtext id="alg1.l12.m1.1.1.3" xref="alg1.l12.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><apply id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1"><csymbol cd="ambiguous" id="alg1.l12.m1.1.1.1.cmml" xref="alg1.l12.m1.1.1">subscript</csymbol><ci id="alg1.l12.m1.1.1.2.cmml" xref="alg1.l12.m1.1.1.2">ùëí</ci><ci id="alg1.l12.m1.1.1.3a.cmml" xref="alg1.l12.m1.1.1.3"><mtext mathsize="70%" id="alg1.l12.m1.1.1.3.cmml" xref="alg1.l12.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">e_{\text{T}}</annotation></semantics></math>:

</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l13.m1.1" class="ltx_Math" alttext="e_{\text{T}}=F_{\text{T}}(I)" display="inline"><semantics id="alg1.l13.m1.1a"><mrow id="alg1.l13.m1.1.2" xref="alg1.l13.m1.1.2.cmml"><msub id="alg1.l13.m1.1.2.2" xref="alg1.l13.m1.1.2.2.cmml"><mi id="alg1.l13.m1.1.2.2.2" xref="alg1.l13.m1.1.2.2.2.cmml">e</mi><mtext id="alg1.l13.m1.1.2.2.3" xref="alg1.l13.m1.1.2.2.3a.cmml">T</mtext></msub><mo id="alg1.l13.m1.1.2.1" xref="alg1.l13.m1.1.2.1.cmml">=</mo><mrow id="alg1.l13.m1.1.2.3" xref="alg1.l13.m1.1.2.3.cmml"><msub id="alg1.l13.m1.1.2.3.2" xref="alg1.l13.m1.1.2.3.2.cmml"><mi id="alg1.l13.m1.1.2.3.2.2" xref="alg1.l13.m1.1.2.3.2.2.cmml">F</mi><mtext id="alg1.l13.m1.1.2.3.2.3" xref="alg1.l13.m1.1.2.3.2.3a.cmml">T</mtext></msub><mo lspace="0em" rspace="0em" id="alg1.l13.m1.1.2.3.1" xref="alg1.l13.m1.1.2.3.1.cmml">‚Äã</mo><mrow id="alg1.l13.m1.1.2.3.3.2" xref="alg1.l13.m1.1.2.3.cmml"><mo stretchy="false" id="alg1.l13.m1.1.2.3.3.2.1" xref="alg1.l13.m1.1.2.3.cmml">(</mo><mi id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml">I</mi><mo stretchy="false" id="alg1.l13.m1.1.2.3.3.2.2" xref="alg1.l13.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.1b"><apply id="alg1.l13.m1.1.2.cmml" xref="alg1.l13.m1.1.2"><eq id="alg1.l13.m1.1.2.1.cmml" xref="alg1.l13.m1.1.2.1"></eq><apply id="alg1.l13.m1.1.2.2.cmml" xref="alg1.l13.m1.1.2.2"><csymbol cd="ambiguous" id="alg1.l13.m1.1.2.2.1.cmml" xref="alg1.l13.m1.1.2.2">subscript</csymbol><ci id="alg1.l13.m1.1.2.2.2.cmml" xref="alg1.l13.m1.1.2.2.2">ùëí</ci><ci id="alg1.l13.m1.1.2.2.3a.cmml" xref="alg1.l13.m1.1.2.2.3"><mtext mathsize="70%" id="alg1.l13.m1.1.2.2.3.cmml" xref="alg1.l13.m1.1.2.2.3">T</mtext></ci></apply><apply id="alg1.l13.m1.1.2.3.cmml" xref="alg1.l13.m1.1.2.3"><times id="alg1.l13.m1.1.2.3.1.cmml" xref="alg1.l13.m1.1.2.3.1"></times><apply id="alg1.l13.m1.1.2.3.2.cmml" xref="alg1.l13.m1.1.2.3.2"><csymbol cd="ambiguous" id="alg1.l13.m1.1.2.3.2.1.cmml" xref="alg1.l13.m1.1.2.3.2">subscript</csymbol><ci id="alg1.l13.m1.1.2.3.2.2.cmml" xref="alg1.l13.m1.1.2.3.2.2">ùêπ</ci><ci id="alg1.l13.m1.1.2.3.2.3a.cmml" xref="alg1.l13.m1.1.2.3.2.3"><mtext mathsize="70%" id="alg1.l13.m1.1.2.3.2.3.cmml" xref="alg1.l13.m1.1.2.3.2.3">T</mtext></ci></apply><ci id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1">ùêº</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.1c">e_{\text{T}}=F_{\text{T}}(I)</annotation></semantics></math>

</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉCalculate loss <math id="alg1.l14.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{KD}}" display="inline"><semantics id="alg1.l14.m1.1a"><msub id="alg1.l14.m1.1.1" xref="alg1.l14.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l14.m1.1.1.2" xref="alg1.l14.m1.1.1.2.cmml">‚Ñí</mi><mtext id="alg1.l14.m1.1.1.3" xref="alg1.l14.m1.1.1.3a.cmml">KD</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l14.m1.1b"><apply id="alg1.l14.m1.1.1.cmml" xref="alg1.l14.m1.1.1"><csymbol cd="ambiguous" id="alg1.l14.m1.1.1.1.cmml" xref="alg1.l14.m1.1.1">subscript</csymbol><ci id="alg1.l14.m1.1.1.2.cmml" xref="alg1.l14.m1.1.1.2">‚Ñí</ci><ci id="alg1.l14.m1.1.1.3a.cmml" xref="alg1.l14.m1.1.1.3"><mtext mathsize="70%" id="alg1.l14.m1.1.1.3.cmml" xref="alg1.l14.m1.1.1.3">KD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m1.1c">\mathcal{L}_{\text{KD}}</annotation></semantics></math> and optimize <math id="alg1.l14.m2.1" class="ltx_Math" alttext="\theta_{\text{S}}" display="inline"><semantics id="alg1.l14.m2.1a"><msub id="alg1.l14.m2.1.1" xref="alg1.l14.m2.1.1.cmml"><mi id="alg1.l14.m2.1.1.2" xref="alg1.l14.m2.1.1.2.cmml">Œ∏</mi><mtext id="alg1.l14.m2.1.1.3" xref="alg1.l14.m2.1.1.3a.cmml">S</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l14.m2.1b"><apply id="alg1.l14.m2.1.1.cmml" xref="alg1.l14.m2.1.1"><csymbol cd="ambiguous" id="alg1.l14.m2.1.1.1.cmml" xref="alg1.l14.m2.1.1">subscript</csymbol><ci id="alg1.l14.m2.1.1.2.cmml" xref="alg1.l14.m2.1.1.2">ùúÉ</ci><ci id="alg1.l14.m2.1.1.3a.cmml" xref="alg1.l14.m2.1.1.3"><mtext mathsize="70%" id="alg1.l14.m2.1.1.3.cmml" xref="alg1.l14.m2.1.1.3">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m2.1c">\theta_{\text{S}}</annotation></semantics></math>:

</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l15.m1.1" class="ltx_Math" alttext="g_{\theta_{\text{S}}}\leftarrow\nabla_{\theta_{\text{S}}}\mathcal{L}_{\text{KD}}" display="inline"><semantics id="alg1.l15.m1.1a"><mrow id="alg1.l15.m1.1.1" xref="alg1.l15.m1.1.1.cmml"><msub id="alg1.l15.m1.1.1.2" xref="alg1.l15.m1.1.1.2.cmml"><mi id="alg1.l15.m1.1.1.2.2" xref="alg1.l15.m1.1.1.2.2.cmml">g</mi><msub id="alg1.l15.m1.1.1.2.3" xref="alg1.l15.m1.1.1.2.3.cmml"><mi id="alg1.l15.m1.1.1.2.3.2" xref="alg1.l15.m1.1.1.2.3.2.cmml">Œ∏</mi><mtext id="alg1.l15.m1.1.1.2.3.3" xref="alg1.l15.m1.1.1.2.3.3a.cmml">S</mtext></msub></msub><mo stretchy="false" id="alg1.l15.m1.1.1.1" xref="alg1.l15.m1.1.1.1.cmml">‚Üê</mo><mrow id="alg1.l15.m1.1.1.3" xref="alg1.l15.m1.1.1.3.cmml"><msub id="alg1.l15.m1.1.1.3.1" xref="alg1.l15.m1.1.1.3.1.cmml"><mo rspace="0.167em" id="alg1.l15.m1.1.1.3.1.2" xref="alg1.l15.m1.1.1.3.1.2.cmml">‚àá</mo><msub id="alg1.l15.m1.1.1.3.1.3" xref="alg1.l15.m1.1.1.3.1.3.cmml"><mi id="alg1.l15.m1.1.1.3.1.3.2" xref="alg1.l15.m1.1.1.3.1.3.2.cmml">Œ∏</mi><mtext id="alg1.l15.m1.1.1.3.1.3.3" xref="alg1.l15.m1.1.1.3.1.3.3a.cmml">S</mtext></msub></msub><msub id="alg1.l15.m1.1.1.3.2" xref="alg1.l15.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l15.m1.1.1.3.2.2" xref="alg1.l15.m1.1.1.3.2.2.cmml">‚Ñí</mi><mtext id="alg1.l15.m1.1.1.3.2.3" xref="alg1.l15.m1.1.1.3.2.3a.cmml">KD</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l15.m1.1b"><apply id="alg1.l15.m1.1.1.cmml" xref="alg1.l15.m1.1.1"><ci id="alg1.l15.m1.1.1.1.cmml" xref="alg1.l15.m1.1.1.1">‚Üê</ci><apply id="alg1.l15.m1.1.1.2.cmml" xref="alg1.l15.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l15.m1.1.1.2.1.cmml" xref="alg1.l15.m1.1.1.2">subscript</csymbol><ci id="alg1.l15.m1.1.1.2.2.cmml" xref="alg1.l15.m1.1.1.2.2">ùëî</ci><apply id="alg1.l15.m1.1.1.2.3.cmml" xref="alg1.l15.m1.1.1.2.3"><csymbol cd="ambiguous" id="alg1.l15.m1.1.1.2.3.1.cmml" xref="alg1.l15.m1.1.1.2.3">subscript</csymbol><ci id="alg1.l15.m1.1.1.2.3.2.cmml" xref="alg1.l15.m1.1.1.2.3.2">ùúÉ</ci><ci id="alg1.l15.m1.1.1.2.3.3a.cmml" xref="alg1.l15.m1.1.1.2.3.3"><mtext mathsize="50%" id="alg1.l15.m1.1.1.2.3.3.cmml" xref="alg1.l15.m1.1.1.2.3.3">S</mtext></ci></apply></apply><apply id="alg1.l15.m1.1.1.3.cmml" xref="alg1.l15.m1.1.1.3"><apply id="alg1.l15.m1.1.1.3.1.cmml" xref="alg1.l15.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg1.l15.m1.1.1.3.1.1.cmml" xref="alg1.l15.m1.1.1.3.1">subscript</csymbol><ci id="alg1.l15.m1.1.1.3.1.2.cmml" xref="alg1.l15.m1.1.1.3.1.2">‚àá</ci><apply id="alg1.l15.m1.1.1.3.1.3.cmml" xref="alg1.l15.m1.1.1.3.1.3"><csymbol cd="ambiguous" id="alg1.l15.m1.1.1.3.1.3.1.cmml" xref="alg1.l15.m1.1.1.3.1.3">subscript</csymbol><ci id="alg1.l15.m1.1.1.3.1.3.2.cmml" xref="alg1.l15.m1.1.1.3.1.3.2">ùúÉ</ci><ci id="alg1.l15.m1.1.1.3.1.3.3a.cmml" xref="alg1.l15.m1.1.1.3.1.3.3"><mtext mathsize="50%" id="alg1.l15.m1.1.1.3.1.3.3.cmml" xref="alg1.l15.m1.1.1.3.1.3.3">S</mtext></ci></apply></apply><apply id="alg1.l15.m1.1.1.3.2.cmml" xref="alg1.l15.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l15.m1.1.1.3.2.1.cmml" xref="alg1.l15.m1.1.1.3.2">subscript</csymbol><ci id="alg1.l15.m1.1.1.3.2.2.cmml" xref="alg1.l15.m1.1.1.3.2.2">‚Ñí</ci><ci id="alg1.l15.m1.1.1.3.2.3a.cmml" xref="alg1.l15.m1.1.1.3.2.3"><mtext mathsize="70%" id="alg1.l15.m1.1.1.3.2.3.cmml" xref="alg1.l15.m1.1.1.3.2.3">KD</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l15.m1.1c">g_{\theta_{\text{S}}}\leftarrow\nabla_{\theta_{\text{S}}}\mathcal{L}_{\text{KD}}</annotation></semantics></math>

</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l16.m1.2" class="ltx_Math" alttext="{\theta_{\text{S}}}\leftarrow{\theta_{\text{S}}}-\alpha\cdot\text{Adam}({\theta_{\text{S}}},g_{\theta_{\text{S}}})" display="inline"><semantics id="alg1.l16.m1.2a"><mrow id="alg1.l16.m1.2.2" xref="alg1.l16.m1.2.2.cmml"><msub id="alg1.l16.m1.2.2.4" xref="alg1.l16.m1.2.2.4.cmml"><mi id="alg1.l16.m1.2.2.4.2" xref="alg1.l16.m1.2.2.4.2.cmml">Œ∏</mi><mtext id="alg1.l16.m1.2.2.4.3" xref="alg1.l16.m1.2.2.4.3a.cmml">S</mtext></msub><mo stretchy="false" id="alg1.l16.m1.2.2.3" xref="alg1.l16.m1.2.2.3.cmml">‚Üê</mo><mrow id="alg1.l16.m1.2.2.2" xref="alg1.l16.m1.2.2.2.cmml"><msub id="alg1.l16.m1.2.2.2.4" xref="alg1.l16.m1.2.2.2.4.cmml"><mi id="alg1.l16.m1.2.2.2.4.2" xref="alg1.l16.m1.2.2.2.4.2.cmml">Œ∏</mi><mtext id="alg1.l16.m1.2.2.2.4.3" xref="alg1.l16.m1.2.2.2.4.3a.cmml">S</mtext></msub><mo id="alg1.l16.m1.2.2.2.3" xref="alg1.l16.m1.2.2.2.3.cmml">‚àí</mo><mrow id="alg1.l16.m1.2.2.2.2" xref="alg1.l16.m1.2.2.2.2.cmml"><mrow id="alg1.l16.m1.2.2.2.2.4" xref="alg1.l16.m1.2.2.2.2.4.cmml"><mi id="alg1.l16.m1.2.2.2.2.4.2" xref="alg1.l16.m1.2.2.2.2.4.2.cmml">Œ±</mi><mo lspace="0.222em" rspace="0.222em" id="alg1.l16.m1.2.2.2.2.4.1" xref="alg1.l16.m1.2.2.2.2.4.1.cmml">‚ãÖ</mo><mtext id="alg1.l16.m1.2.2.2.2.4.3" xref="alg1.l16.m1.2.2.2.2.4.3a.cmml">Adam</mtext></mrow><mo lspace="0em" rspace="0em" id="alg1.l16.m1.2.2.2.2.3" xref="alg1.l16.m1.2.2.2.2.3.cmml">‚Äã</mo><mrow id="alg1.l16.m1.2.2.2.2.2.2" xref="alg1.l16.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="alg1.l16.m1.2.2.2.2.2.2.3" xref="alg1.l16.m1.2.2.2.2.2.3.cmml">(</mo><msub id="alg1.l16.m1.1.1.1.1.1.1.1" xref="alg1.l16.m1.1.1.1.1.1.1.1.cmml"><mi id="alg1.l16.m1.1.1.1.1.1.1.1.2" xref="alg1.l16.m1.1.1.1.1.1.1.1.2.cmml">Œ∏</mi><mtext id="alg1.l16.m1.1.1.1.1.1.1.1.3" xref="alg1.l16.m1.1.1.1.1.1.1.1.3a.cmml">S</mtext></msub><mo id="alg1.l16.m1.2.2.2.2.2.2.4" xref="alg1.l16.m1.2.2.2.2.2.3.cmml">,</mo><msub id="alg1.l16.m1.2.2.2.2.2.2.2" xref="alg1.l16.m1.2.2.2.2.2.2.2.cmml"><mi id="alg1.l16.m1.2.2.2.2.2.2.2.2" xref="alg1.l16.m1.2.2.2.2.2.2.2.2.cmml">g</mi><msub id="alg1.l16.m1.2.2.2.2.2.2.2.3" xref="alg1.l16.m1.2.2.2.2.2.2.2.3.cmml"><mi id="alg1.l16.m1.2.2.2.2.2.2.2.3.2" xref="alg1.l16.m1.2.2.2.2.2.2.2.3.2.cmml">Œ∏</mi><mtext id="alg1.l16.m1.2.2.2.2.2.2.2.3.3" xref="alg1.l16.m1.2.2.2.2.2.2.2.3.3a.cmml">S</mtext></msub></msub><mo stretchy="false" id="alg1.l16.m1.2.2.2.2.2.2.5" xref="alg1.l16.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l16.m1.2b"><apply id="alg1.l16.m1.2.2.cmml" xref="alg1.l16.m1.2.2"><ci id="alg1.l16.m1.2.2.3.cmml" xref="alg1.l16.m1.2.2.3">‚Üê</ci><apply id="alg1.l16.m1.2.2.4.cmml" xref="alg1.l16.m1.2.2.4"><csymbol cd="ambiguous" id="alg1.l16.m1.2.2.4.1.cmml" xref="alg1.l16.m1.2.2.4">subscript</csymbol><ci id="alg1.l16.m1.2.2.4.2.cmml" xref="alg1.l16.m1.2.2.4.2">ùúÉ</ci><ci id="alg1.l16.m1.2.2.4.3a.cmml" xref="alg1.l16.m1.2.2.4.3"><mtext mathsize="70%" id="alg1.l16.m1.2.2.4.3.cmml" xref="alg1.l16.m1.2.2.4.3">S</mtext></ci></apply><apply id="alg1.l16.m1.2.2.2.cmml" xref="alg1.l16.m1.2.2.2"><minus id="alg1.l16.m1.2.2.2.3.cmml" xref="alg1.l16.m1.2.2.2.3"></minus><apply id="alg1.l16.m1.2.2.2.4.cmml" xref="alg1.l16.m1.2.2.2.4"><csymbol cd="ambiguous" id="alg1.l16.m1.2.2.2.4.1.cmml" xref="alg1.l16.m1.2.2.2.4">subscript</csymbol><ci id="alg1.l16.m1.2.2.2.4.2.cmml" xref="alg1.l16.m1.2.2.2.4.2">ùúÉ</ci><ci id="alg1.l16.m1.2.2.2.4.3a.cmml" xref="alg1.l16.m1.2.2.2.4.3"><mtext mathsize="70%" id="alg1.l16.m1.2.2.2.4.3.cmml" xref="alg1.l16.m1.2.2.2.4.3">S</mtext></ci></apply><apply id="alg1.l16.m1.2.2.2.2.cmml" xref="alg1.l16.m1.2.2.2.2"><times id="alg1.l16.m1.2.2.2.2.3.cmml" xref="alg1.l16.m1.2.2.2.2.3"></times><apply id="alg1.l16.m1.2.2.2.2.4.cmml" xref="alg1.l16.m1.2.2.2.2.4"><ci id="alg1.l16.m1.2.2.2.2.4.1.cmml" xref="alg1.l16.m1.2.2.2.2.4.1">‚ãÖ</ci><ci id="alg1.l16.m1.2.2.2.2.4.2.cmml" xref="alg1.l16.m1.2.2.2.2.4.2">ùõº</ci><ci id="alg1.l16.m1.2.2.2.2.4.3a.cmml" xref="alg1.l16.m1.2.2.2.2.4.3"><mtext id="alg1.l16.m1.2.2.2.2.4.3.cmml" xref="alg1.l16.m1.2.2.2.2.4.3">Adam</mtext></ci></apply><interval closure="open" id="alg1.l16.m1.2.2.2.2.2.3.cmml" xref="alg1.l16.m1.2.2.2.2.2.2"><apply id="alg1.l16.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l16.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l16.m1.1.1.1.1.1.1.1.1.cmml" xref="alg1.l16.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l16.m1.1.1.1.1.1.1.1.2.cmml" xref="alg1.l16.m1.1.1.1.1.1.1.1.2">ùúÉ</ci><ci id="alg1.l16.m1.1.1.1.1.1.1.1.3a.cmml" xref="alg1.l16.m1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="alg1.l16.m1.1.1.1.1.1.1.1.3.cmml" xref="alg1.l16.m1.1.1.1.1.1.1.1.3">S</mtext></ci></apply><apply id="alg1.l16.m1.2.2.2.2.2.2.2.cmml" xref="alg1.l16.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l16.m1.2.2.2.2.2.2.2.1.cmml" xref="alg1.l16.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="alg1.l16.m1.2.2.2.2.2.2.2.2.cmml" xref="alg1.l16.m1.2.2.2.2.2.2.2.2">ùëî</ci><apply id="alg1.l16.m1.2.2.2.2.2.2.2.3.cmml" xref="alg1.l16.m1.2.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="alg1.l16.m1.2.2.2.2.2.2.2.3.1.cmml" xref="alg1.l16.m1.2.2.2.2.2.2.2.3">subscript</csymbol><ci id="alg1.l16.m1.2.2.2.2.2.2.2.3.2.cmml" xref="alg1.l16.m1.2.2.2.2.2.2.2.3.2">ùúÉ</ci><ci id="alg1.l16.m1.2.2.2.2.2.2.2.3.3a.cmml" xref="alg1.l16.m1.2.2.2.2.2.2.2.3.3"><mtext mathsize="50%" id="alg1.l16.m1.2.2.2.2.2.2.2.3.3.cmml" xref="alg1.l16.m1.2.2.2.2.2.2.2.3.3">S</mtext></ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l16.m1.2c">{\theta_{\text{S}}}\leftarrow{\theta_{\text{S}}}-\alpha\cdot\text{Adam}({\theta_{\text{S}}},g_{\theta_{\text{S}}})</annotation></semantics></math>

</div>
<div id="alg1.l17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l17.1.1.1" class="ltx_text" style="font-size:80%;">17:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†<span id="alg1.l17.2" class="ltx_text ltx_font_bold">Step 2</span>: Train with dynamic re-sampling

</div>
<div id="alg1.l18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l18.1.1.1" class="ltx_text" style="font-size:80%;">18:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉCalculate similarity of <math id="alg1.l18.m1.1" class="ltx_Math" alttext="e_{\text{T}}" display="inline"><semantics id="alg1.l18.m1.1a"><msub id="alg1.l18.m1.1.1" xref="alg1.l18.m1.1.1.cmml"><mi id="alg1.l18.m1.1.1.2" xref="alg1.l18.m1.1.1.2.cmml">e</mi><mtext id="alg1.l18.m1.1.1.3" xref="alg1.l18.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l18.m1.1b"><apply id="alg1.l18.m1.1.1.cmml" xref="alg1.l18.m1.1.1"><csymbol cd="ambiguous" id="alg1.l18.m1.1.1.1.cmml" xref="alg1.l18.m1.1.1">subscript</csymbol><ci id="alg1.l18.m1.1.1.2.cmml" xref="alg1.l18.m1.1.1.2">ùëí</ci><ci id="alg1.l18.m1.1.1.3a.cmml" xref="alg1.l18.m1.1.1.3"><mtext mathsize="70%" id="alg1.l18.m1.1.1.3.cmml" xref="alg1.l18.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m1.1c">e_{\text{T}}</annotation></semantics></math> and <math id="alg1.l18.m2.1" class="ltx_Math" alttext="e_{\text{S}}" display="inline"><semantics id="alg1.l18.m2.1a"><msub id="alg1.l18.m2.1.1" xref="alg1.l18.m2.1.1.cmml"><mi id="alg1.l18.m2.1.1.2" xref="alg1.l18.m2.1.1.2.cmml">e</mi><mtext id="alg1.l18.m2.1.1.3" xref="alg1.l18.m2.1.1.3a.cmml">S</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l18.m2.1b"><apply id="alg1.l18.m2.1.1.cmml" xref="alg1.l18.m2.1.1"><csymbol cd="ambiguous" id="alg1.l18.m2.1.1.1.cmml" xref="alg1.l18.m2.1.1">subscript</csymbol><ci id="alg1.l18.m2.1.1.2.cmml" xref="alg1.l18.m2.1.1.2">ùëí</ci><ci id="alg1.l18.m2.1.1.3a.cmml" xref="alg1.l18.m2.1.1.3"><mtext mathsize="70%" id="alg1.l18.m2.1.1.3.cmml" xref="alg1.l18.m2.1.1.3">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m2.1c">e_{\text{S}}</annotation></semantics></math>:

</div>
<div id="alg1.l19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l19.1.1.1" class="ltx_text" style="font-size:80%;">19:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l19.m1.2" class="ltx_Math" alttext="\bm{s}_{\text{sim}}=\text{SIM}(e_{\text{T}},e_{\text{S}})" display="inline"><semantics id="alg1.l19.m1.2a"><mrow id="alg1.l19.m1.2.2" xref="alg1.l19.m1.2.2.cmml"><msub id="alg1.l19.m1.2.2.4" xref="alg1.l19.m1.2.2.4.cmml"><mi id="alg1.l19.m1.2.2.4.2" xref="alg1.l19.m1.2.2.4.2.cmml">ùíî</mi><mtext id="alg1.l19.m1.2.2.4.3" xref="alg1.l19.m1.2.2.4.3a.cmml">sim</mtext></msub><mo id="alg1.l19.m1.2.2.3" xref="alg1.l19.m1.2.2.3.cmml">=</mo><mrow id="alg1.l19.m1.2.2.2" xref="alg1.l19.m1.2.2.2.cmml"><mtext id="alg1.l19.m1.2.2.2.4" xref="alg1.l19.m1.2.2.2.4a.cmml">SIM</mtext><mo lspace="0em" rspace="0em" id="alg1.l19.m1.2.2.2.3" xref="alg1.l19.m1.2.2.2.3.cmml">‚Äã</mo><mrow id="alg1.l19.m1.2.2.2.2.2" xref="alg1.l19.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="alg1.l19.m1.2.2.2.2.2.3" xref="alg1.l19.m1.2.2.2.2.3.cmml">(</mo><msub id="alg1.l19.m1.1.1.1.1.1.1" xref="alg1.l19.m1.1.1.1.1.1.1.cmml"><mi id="alg1.l19.m1.1.1.1.1.1.1.2" xref="alg1.l19.m1.1.1.1.1.1.1.2.cmml">e</mi><mtext id="alg1.l19.m1.1.1.1.1.1.1.3" xref="alg1.l19.m1.1.1.1.1.1.1.3a.cmml">T</mtext></msub><mo id="alg1.l19.m1.2.2.2.2.2.4" xref="alg1.l19.m1.2.2.2.2.3.cmml">,</mo><msub id="alg1.l19.m1.2.2.2.2.2.2" xref="alg1.l19.m1.2.2.2.2.2.2.cmml"><mi id="alg1.l19.m1.2.2.2.2.2.2.2" xref="alg1.l19.m1.2.2.2.2.2.2.2.cmml">e</mi><mtext id="alg1.l19.m1.2.2.2.2.2.2.3" xref="alg1.l19.m1.2.2.2.2.2.2.3a.cmml">S</mtext></msub><mo stretchy="false" id="alg1.l19.m1.2.2.2.2.2.5" xref="alg1.l19.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l19.m1.2b"><apply id="alg1.l19.m1.2.2.cmml" xref="alg1.l19.m1.2.2"><eq id="alg1.l19.m1.2.2.3.cmml" xref="alg1.l19.m1.2.2.3"></eq><apply id="alg1.l19.m1.2.2.4.cmml" xref="alg1.l19.m1.2.2.4"><csymbol cd="ambiguous" id="alg1.l19.m1.2.2.4.1.cmml" xref="alg1.l19.m1.2.2.4">subscript</csymbol><ci id="alg1.l19.m1.2.2.4.2.cmml" xref="alg1.l19.m1.2.2.4.2">ùíî</ci><ci id="alg1.l19.m1.2.2.4.3a.cmml" xref="alg1.l19.m1.2.2.4.3"><mtext mathsize="70%" id="alg1.l19.m1.2.2.4.3.cmml" xref="alg1.l19.m1.2.2.4.3">sim</mtext></ci></apply><apply id="alg1.l19.m1.2.2.2.cmml" xref="alg1.l19.m1.2.2.2"><times id="alg1.l19.m1.2.2.2.3.cmml" xref="alg1.l19.m1.2.2.2.3"></times><ci id="alg1.l19.m1.2.2.2.4a.cmml" xref="alg1.l19.m1.2.2.2.4"><mtext id="alg1.l19.m1.2.2.2.4.cmml" xref="alg1.l19.m1.2.2.2.4">SIM</mtext></ci><interval closure="open" id="alg1.l19.m1.2.2.2.2.3.cmml" xref="alg1.l19.m1.2.2.2.2.2"><apply id="alg1.l19.m1.1.1.1.1.1.1.cmml" xref="alg1.l19.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l19.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l19.m1.1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l19.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l19.m1.1.1.1.1.1.1.2">ùëí</ci><ci id="alg1.l19.m1.1.1.1.1.1.1.3a.cmml" xref="alg1.l19.m1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="alg1.l19.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l19.m1.1.1.1.1.1.1.3">T</mtext></ci></apply><apply id="alg1.l19.m1.2.2.2.2.2.2.cmml" xref="alg1.l19.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l19.m1.2.2.2.2.2.2.1.cmml" xref="alg1.l19.m1.2.2.2.2.2.2">subscript</csymbol><ci id="alg1.l19.m1.2.2.2.2.2.2.2.cmml" xref="alg1.l19.m1.2.2.2.2.2.2.2">ùëí</ci><ci id="alg1.l19.m1.2.2.2.2.2.2.3a.cmml" xref="alg1.l19.m1.2.2.2.2.2.2.3"><mtext mathsize="70%" id="alg1.l19.m1.2.2.2.2.2.2.3.cmml" xref="alg1.l19.m1.2.2.2.2.2.2.3">S</mtext></ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l19.m1.2c">\bm{s}_{\text{sim}}=\text{SIM}(e_{\text{T}},e_{\text{S}})</annotation></semantics></math>

</div>
<div id="alg1.l20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l20.1.1.1" class="ltx_text" style="font-size:80%;">20:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉRe-sample based on similarity:

</div>
<div id="alg1.l21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l21.1.1.1" class="ltx_text" style="font-size:80%;">21:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l21.m1.1" class="ltx_Math" alttext="\bm{w}_{\text{resample}}=\bm{w}+c\times\bm{s}_{\text{sim}}\times\bm{n}," display="inline"><semantics id="alg1.l21.m1.1a"><mrow id="alg1.l21.m1.1.1.1" xref="alg1.l21.m1.1.1.1.1.cmml"><mrow id="alg1.l21.m1.1.1.1.1" xref="alg1.l21.m1.1.1.1.1.cmml"><msub id="alg1.l21.m1.1.1.1.1.2" xref="alg1.l21.m1.1.1.1.1.2.cmml"><mi id="alg1.l21.m1.1.1.1.1.2.2" xref="alg1.l21.m1.1.1.1.1.2.2.cmml">ùíò</mi><mtext id="alg1.l21.m1.1.1.1.1.2.3" xref="alg1.l21.m1.1.1.1.1.2.3a.cmml">resample</mtext></msub><mo id="alg1.l21.m1.1.1.1.1.1" xref="alg1.l21.m1.1.1.1.1.1.cmml">=</mo><mrow id="alg1.l21.m1.1.1.1.1.3" xref="alg1.l21.m1.1.1.1.1.3.cmml"><mi id="alg1.l21.m1.1.1.1.1.3.2" xref="alg1.l21.m1.1.1.1.1.3.2.cmml">ùíò</mi><mo id="alg1.l21.m1.1.1.1.1.3.1" xref="alg1.l21.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="alg1.l21.m1.1.1.1.1.3.3" xref="alg1.l21.m1.1.1.1.1.3.3.cmml"><mi id="alg1.l21.m1.1.1.1.1.3.3.2" xref="alg1.l21.m1.1.1.1.1.3.3.2.cmml">c</mi><mo lspace="0.222em" rspace="0.222em" id="alg1.l21.m1.1.1.1.1.3.3.1" xref="alg1.l21.m1.1.1.1.1.3.3.1.cmml">√ó</mo><msub id="alg1.l21.m1.1.1.1.1.3.3.3" xref="alg1.l21.m1.1.1.1.1.3.3.3.cmml"><mi id="alg1.l21.m1.1.1.1.1.3.3.3.2" xref="alg1.l21.m1.1.1.1.1.3.3.3.2.cmml">ùíî</mi><mtext id="alg1.l21.m1.1.1.1.1.3.3.3.3" xref="alg1.l21.m1.1.1.1.1.3.3.3.3a.cmml">sim</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="alg1.l21.m1.1.1.1.1.3.3.1a" xref="alg1.l21.m1.1.1.1.1.3.3.1.cmml">√ó</mo><mi id="alg1.l21.m1.1.1.1.1.3.3.4" xref="alg1.l21.m1.1.1.1.1.3.3.4.cmml">ùíè</mi></mrow></mrow></mrow><mo id="alg1.l21.m1.1.1.1.2" xref="alg1.l21.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l21.m1.1b"><apply id="alg1.l21.m1.1.1.1.1.cmml" xref="alg1.l21.m1.1.1.1"><eq id="alg1.l21.m1.1.1.1.1.1.cmml" xref="alg1.l21.m1.1.1.1.1.1"></eq><apply id="alg1.l21.m1.1.1.1.1.2.cmml" xref="alg1.l21.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="alg1.l21.m1.1.1.1.1.2.1.cmml" xref="alg1.l21.m1.1.1.1.1.2">subscript</csymbol><ci id="alg1.l21.m1.1.1.1.1.2.2.cmml" xref="alg1.l21.m1.1.1.1.1.2.2">ùíò</ci><ci id="alg1.l21.m1.1.1.1.1.2.3a.cmml" xref="alg1.l21.m1.1.1.1.1.2.3"><mtext mathsize="70%" id="alg1.l21.m1.1.1.1.1.2.3.cmml" xref="alg1.l21.m1.1.1.1.1.2.3">resample</mtext></ci></apply><apply id="alg1.l21.m1.1.1.1.1.3.cmml" xref="alg1.l21.m1.1.1.1.1.3"><plus id="alg1.l21.m1.1.1.1.1.3.1.cmml" xref="alg1.l21.m1.1.1.1.1.3.1"></plus><ci id="alg1.l21.m1.1.1.1.1.3.2.cmml" xref="alg1.l21.m1.1.1.1.1.3.2">ùíò</ci><apply id="alg1.l21.m1.1.1.1.1.3.3.cmml" xref="alg1.l21.m1.1.1.1.1.3.3"><times id="alg1.l21.m1.1.1.1.1.3.3.1.cmml" xref="alg1.l21.m1.1.1.1.1.3.3.1"></times><ci id="alg1.l21.m1.1.1.1.1.3.3.2.cmml" xref="alg1.l21.m1.1.1.1.1.3.3.2">ùëê</ci><apply id="alg1.l21.m1.1.1.1.1.3.3.3.cmml" xref="alg1.l21.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="alg1.l21.m1.1.1.1.1.3.3.3.1.cmml" xref="alg1.l21.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="alg1.l21.m1.1.1.1.1.3.3.3.2.cmml" xref="alg1.l21.m1.1.1.1.1.3.3.3.2">ùíî</ci><ci id="alg1.l21.m1.1.1.1.1.3.3.3.3a.cmml" xref="alg1.l21.m1.1.1.1.1.3.3.3.3"><mtext mathsize="70%" id="alg1.l21.m1.1.1.1.1.3.3.3.3.cmml" xref="alg1.l21.m1.1.1.1.1.3.3.3.3">sim</mtext></ci></apply><ci id="alg1.l21.m1.1.1.1.1.3.3.4.cmml" xref="alg1.l21.m1.1.1.1.1.3.3.4">ùíè</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m1.1c">\bm{w}_{\text{resample}}=\bm{w}+c\times\bm{s}_{\text{sim}}\times\bm{n},</annotation></semantics></math>

</div>
<div id="alg1.l22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l22.1.1.1" class="ltx_text" style="font-size:80%;">22:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ‚ÄÉwhere <math id="alg1.l22.m1.2" class="ltx_Math" alttext="\bm{n}\sim\mathcal{N}(0,\mathbb{I})" display="inline"><semantics id="alg1.l22.m1.2a"><mrow id="alg1.l22.m1.2.3" xref="alg1.l22.m1.2.3.cmml"><mi id="alg1.l22.m1.2.3.2" xref="alg1.l22.m1.2.3.2.cmml">ùíè</mi><mo id="alg1.l22.m1.2.3.1" xref="alg1.l22.m1.2.3.1.cmml">‚àº</mo><mrow id="alg1.l22.m1.2.3.3" xref="alg1.l22.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l22.m1.2.3.3.2" xref="alg1.l22.m1.2.3.3.2.cmml">ùí©</mi><mo lspace="0em" rspace="0em" id="alg1.l22.m1.2.3.3.1" xref="alg1.l22.m1.2.3.3.1.cmml">‚Äã</mo><mrow id="alg1.l22.m1.2.3.3.3.2" xref="alg1.l22.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="alg1.l22.m1.2.3.3.3.2.1" xref="alg1.l22.m1.2.3.3.3.1.cmml">(</mo><mn id="alg1.l22.m1.1.1" xref="alg1.l22.m1.1.1.cmml">0</mn><mo id="alg1.l22.m1.2.3.3.3.2.2" xref="alg1.l22.m1.2.3.3.3.1.cmml">,</mo><mi id="alg1.l22.m1.2.2" xref="alg1.l22.m1.2.2.cmml">ùïÄ</mi><mo stretchy="false" id="alg1.l22.m1.2.3.3.3.2.3" xref="alg1.l22.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l22.m1.2b"><apply id="alg1.l22.m1.2.3.cmml" xref="alg1.l22.m1.2.3"><csymbol cd="latexml" id="alg1.l22.m1.2.3.1.cmml" xref="alg1.l22.m1.2.3.1">similar-to</csymbol><ci id="alg1.l22.m1.2.3.2.cmml" xref="alg1.l22.m1.2.3.2">ùíè</ci><apply id="alg1.l22.m1.2.3.3.cmml" xref="alg1.l22.m1.2.3.3"><times id="alg1.l22.m1.2.3.3.1.cmml" xref="alg1.l22.m1.2.3.3.1"></times><ci id="alg1.l22.m1.2.3.3.2.cmml" xref="alg1.l22.m1.2.3.3.2">ùí©</ci><interval closure="open" id="alg1.l22.m1.2.3.3.3.1.cmml" xref="alg1.l22.m1.2.3.3.3.2"><cn type="integer" id="alg1.l22.m1.1.1.cmml" xref="alg1.l22.m1.1.1">0</cn><ci id="alg1.l22.m1.2.2.cmml" xref="alg1.l22.m1.2.2">ùïÄ</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l22.m1.2c">\bm{n}\sim\mathcal{N}(0,\mathbb{I})</annotation></semantics></math>

</div>
<div id="alg1.l23" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l23.1.1.1" class="ltx_text" style="font-size:80%;">23:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉGenerate synthetic face images:

</div>
<div id="alg1.l24" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l24.1.1.1" class="ltx_text" style="font-size:80%;">24:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l24.m1.1" class="ltx_Math" alttext="\bm{I}=G(\bm{w}_{\text{resample}})" display="inline"><semantics id="alg1.l24.m1.1a"><mrow id="alg1.l24.m1.1.1" xref="alg1.l24.m1.1.1.cmml"><mi id="alg1.l24.m1.1.1.3" xref="alg1.l24.m1.1.1.3.cmml">ùë∞</mi><mo id="alg1.l24.m1.1.1.2" xref="alg1.l24.m1.1.1.2.cmml">=</mo><mrow id="alg1.l24.m1.1.1.1" xref="alg1.l24.m1.1.1.1.cmml"><mi id="alg1.l24.m1.1.1.1.3" xref="alg1.l24.m1.1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="alg1.l24.m1.1.1.1.2" xref="alg1.l24.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="alg1.l24.m1.1.1.1.1.1" xref="alg1.l24.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l24.m1.1.1.1.1.1.2" xref="alg1.l24.m1.1.1.1.1.1.1.cmml">(</mo><msub id="alg1.l24.m1.1.1.1.1.1.1" xref="alg1.l24.m1.1.1.1.1.1.1.cmml"><mi id="alg1.l24.m1.1.1.1.1.1.1.2" xref="alg1.l24.m1.1.1.1.1.1.1.2.cmml">ùíò</mi><mtext id="alg1.l24.m1.1.1.1.1.1.1.3" xref="alg1.l24.m1.1.1.1.1.1.1.3a.cmml">resample</mtext></msub><mo stretchy="false" id="alg1.l24.m1.1.1.1.1.1.3" xref="alg1.l24.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l24.m1.1b"><apply id="alg1.l24.m1.1.1.cmml" xref="alg1.l24.m1.1.1"><eq id="alg1.l24.m1.1.1.2.cmml" xref="alg1.l24.m1.1.1.2"></eq><ci id="alg1.l24.m1.1.1.3.cmml" xref="alg1.l24.m1.1.1.3">ùë∞</ci><apply id="alg1.l24.m1.1.1.1.cmml" xref="alg1.l24.m1.1.1.1"><times id="alg1.l24.m1.1.1.1.2.cmml" xref="alg1.l24.m1.1.1.1.2"></times><ci id="alg1.l24.m1.1.1.1.3.cmml" xref="alg1.l24.m1.1.1.1.3">ùê∫</ci><apply id="alg1.l24.m1.1.1.1.1.1.1.cmml" xref="alg1.l24.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l24.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l24.m1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l24.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l24.m1.1.1.1.1.1.1.2">ùíò</ci><ci id="alg1.l24.m1.1.1.1.1.1.1.3a.cmml" xref="alg1.l24.m1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="alg1.l24.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l24.m1.1.1.1.1.1.1.3">resample</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l24.m1.1c">\bm{I}=G(\bm{w}_{\text{resample}})</annotation></semantics></math>

</div>
<div id="alg1.l25" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l25.1.1.1" class="ltx_text" style="font-size:80%;">25:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉExtract teacher‚Äôs embeddings <math id="alg1.l25.m1.1" class="ltx_Math" alttext="e_{\text{T}}" display="inline"><semantics id="alg1.l25.m1.1a"><msub id="alg1.l25.m1.1.1" xref="alg1.l25.m1.1.1.cmml"><mi id="alg1.l25.m1.1.1.2" xref="alg1.l25.m1.1.1.2.cmml">e</mi><mtext id="alg1.l25.m1.1.1.3" xref="alg1.l25.m1.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l25.m1.1b"><apply id="alg1.l25.m1.1.1.cmml" xref="alg1.l25.m1.1.1"><csymbol cd="ambiguous" id="alg1.l25.m1.1.1.1.cmml" xref="alg1.l25.m1.1.1">subscript</csymbol><ci id="alg1.l25.m1.1.1.2.cmml" xref="alg1.l25.m1.1.1.2">ùëí</ci><ci id="alg1.l25.m1.1.1.3a.cmml" xref="alg1.l25.m1.1.1.3"><mtext mathsize="70%" id="alg1.l25.m1.1.1.3.cmml" xref="alg1.l25.m1.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l25.m1.1c">e_{\text{T}}</annotation></semantics></math>:

</div>
<div id="alg1.l26" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l26.1.1.1" class="ltx_text" style="font-size:80%;">26:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l26.m1.1" class="ltx_Math" alttext="e_{\text{T}}=F_{\text{T}}(I)" display="inline"><semantics id="alg1.l26.m1.1a"><mrow id="alg1.l26.m1.1.2" xref="alg1.l26.m1.1.2.cmml"><msub id="alg1.l26.m1.1.2.2" xref="alg1.l26.m1.1.2.2.cmml"><mi id="alg1.l26.m1.1.2.2.2" xref="alg1.l26.m1.1.2.2.2.cmml">e</mi><mtext id="alg1.l26.m1.1.2.2.3" xref="alg1.l26.m1.1.2.2.3a.cmml">T</mtext></msub><mo id="alg1.l26.m1.1.2.1" xref="alg1.l26.m1.1.2.1.cmml">=</mo><mrow id="alg1.l26.m1.1.2.3" xref="alg1.l26.m1.1.2.3.cmml"><msub id="alg1.l26.m1.1.2.3.2" xref="alg1.l26.m1.1.2.3.2.cmml"><mi id="alg1.l26.m1.1.2.3.2.2" xref="alg1.l26.m1.1.2.3.2.2.cmml">F</mi><mtext id="alg1.l26.m1.1.2.3.2.3" xref="alg1.l26.m1.1.2.3.2.3a.cmml">T</mtext></msub><mo lspace="0em" rspace="0em" id="alg1.l26.m1.1.2.3.1" xref="alg1.l26.m1.1.2.3.1.cmml">‚Äã</mo><mrow id="alg1.l26.m1.1.2.3.3.2" xref="alg1.l26.m1.1.2.3.cmml"><mo stretchy="false" id="alg1.l26.m1.1.2.3.3.2.1" xref="alg1.l26.m1.1.2.3.cmml">(</mo><mi id="alg1.l26.m1.1.1" xref="alg1.l26.m1.1.1.cmml">I</mi><mo stretchy="false" id="alg1.l26.m1.1.2.3.3.2.2" xref="alg1.l26.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l26.m1.1b"><apply id="alg1.l26.m1.1.2.cmml" xref="alg1.l26.m1.1.2"><eq id="alg1.l26.m1.1.2.1.cmml" xref="alg1.l26.m1.1.2.1"></eq><apply id="alg1.l26.m1.1.2.2.cmml" xref="alg1.l26.m1.1.2.2"><csymbol cd="ambiguous" id="alg1.l26.m1.1.2.2.1.cmml" xref="alg1.l26.m1.1.2.2">subscript</csymbol><ci id="alg1.l26.m1.1.2.2.2.cmml" xref="alg1.l26.m1.1.2.2.2">ùëí</ci><ci id="alg1.l26.m1.1.2.2.3a.cmml" xref="alg1.l26.m1.1.2.2.3"><mtext mathsize="70%" id="alg1.l26.m1.1.2.2.3.cmml" xref="alg1.l26.m1.1.2.2.3">T</mtext></ci></apply><apply id="alg1.l26.m1.1.2.3.cmml" xref="alg1.l26.m1.1.2.3"><times id="alg1.l26.m1.1.2.3.1.cmml" xref="alg1.l26.m1.1.2.3.1"></times><apply id="alg1.l26.m1.1.2.3.2.cmml" xref="alg1.l26.m1.1.2.3.2"><csymbol cd="ambiguous" id="alg1.l26.m1.1.2.3.2.1.cmml" xref="alg1.l26.m1.1.2.3.2">subscript</csymbol><ci id="alg1.l26.m1.1.2.3.2.2.cmml" xref="alg1.l26.m1.1.2.3.2.2">ùêπ</ci><ci id="alg1.l26.m1.1.2.3.2.3a.cmml" xref="alg1.l26.m1.1.2.3.2.3"><mtext mathsize="70%" id="alg1.l26.m1.1.2.3.2.3.cmml" xref="alg1.l26.m1.1.2.3.2.3">T</mtext></ci></apply><ci id="alg1.l26.m1.1.1.cmml" xref="alg1.l26.m1.1.1">ùêº</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l26.m1.1c">e_{\text{T}}=F_{\text{T}}(I)</annotation></semantics></math>

</div>
<div id="alg1.l27" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l27.1.1.1" class="ltx_text" style="font-size:80%;">27:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉCalculate loss <math id="alg1.l27.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{KD}}" display="inline"><semantics id="alg1.l27.m1.1a"><msub id="alg1.l27.m1.1.1" xref="alg1.l27.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l27.m1.1.1.2" xref="alg1.l27.m1.1.1.2.cmml">‚Ñí</mi><mtext id="alg1.l27.m1.1.1.3" xref="alg1.l27.m1.1.1.3a.cmml">KD</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l27.m1.1b"><apply id="alg1.l27.m1.1.1.cmml" xref="alg1.l27.m1.1.1"><csymbol cd="ambiguous" id="alg1.l27.m1.1.1.1.cmml" xref="alg1.l27.m1.1.1">subscript</csymbol><ci id="alg1.l27.m1.1.1.2.cmml" xref="alg1.l27.m1.1.1.2">‚Ñí</ci><ci id="alg1.l27.m1.1.1.3a.cmml" xref="alg1.l27.m1.1.1.3"><mtext mathsize="70%" id="alg1.l27.m1.1.1.3.cmml" xref="alg1.l27.m1.1.1.3">KD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l27.m1.1c">\mathcal{L}_{\text{KD}}</annotation></semantics></math> and optimize <math id="alg1.l27.m2.1" class="ltx_Math" alttext="\theta_{\text{S}}" display="inline"><semantics id="alg1.l27.m2.1a"><msub id="alg1.l27.m2.1.1" xref="alg1.l27.m2.1.1.cmml"><mi id="alg1.l27.m2.1.1.2" xref="alg1.l27.m2.1.1.2.cmml">Œ∏</mi><mtext id="alg1.l27.m2.1.1.3" xref="alg1.l27.m2.1.1.3a.cmml">S</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l27.m2.1b"><apply id="alg1.l27.m2.1.1.cmml" xref="alg1.l27.m2.1.1"><csymbol cd="ambiguous" id="alg1.l27.m2.1.1.1.cmml" xref="alg1.l27.m2.1.1">subscript</csymbol><ci id="alg1.l27.m2.1.1.2.cmml" xref="alg1.l27.m2.1.1.2">ùúÉ</ci><ci id="alg1.l27.m2.1.1.3a.cmml" xref="alg1.l27.m2.1.1.3"><mtext mathsize="70%" id="alg1.l27.m2.1.1.3.cmml" xref="alg1.l27.m2.1.1.3">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l27.m2.1c">\theta_{\text{S}}</annotation></semantics></math>:

</div>
<div id="alg1.l28" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l28.1.1.1" class="ltx_text" style="font-size:80%;">28:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l28.m1.1" class="ltx_Math" alttext="g_{\theta_{\text{S}}}\leftarrow\nabla_{\theta_{\text{S}}}\mathcal{L}_{\text{KD}}" display="inline"><semantics id="alg1.l28.m1.1a"><mrow id="alg1.l28.m1.1.1" xref="alg1.l28.m1.1.1.cmml"><msub id="alg1.l28.m1.1.1.2" xref="alg1.l28.m1.1.1.2.cmml"><mi id="alg1.l28.m1.1.1.2.2" xref="alg1.l28.m1.1.1.2.2.cmml">g</mi><msub id="alg1.l28.m1.1.1.2.3" xref="alg1.l28.m1.1.1.2.3.cmml"><mi id="alg1.l28.m1.1.1.2.3.2" xref="alg1.l28.m1.1.1.2.3.2.cmml">Œ∏</mi><mtext id="alg1.l28.m1.1.1.2.3.3" xref="alg1.l28.m1.1.1.2.3.3a.cmml">S</mtext></msub></msub><mo stretchy="false" id="alg1.l28.m1.1.1.1" xref="alg1.l28.m1.1.1.1.cmml">‚Üê</mo><mrow id="alg1.l28.m1.1.1.3" xref="alg1.l28.m1.1.1.3.cmml"><msub id="alg1.l28.m1.1.1.3.1" xref="alg1.l28.m1.1.1.3.1.cmml"><mo rspace="0.167em" id="alg1.l28.m1.1.1.3.1.2" xref="alg1.l28.m1.1.1.3.1.2.cmml">‚àá</mo><msub id="alg1.l28.m1.1.1.3.1.3" xref="alg1.l28.m1.1.1.3.1.3.cmml"><mi id="alg1.l28.m1.1.1.3.1.3.2" xref="alg1.l28.m1.1.1.3.1.3.2.cmml">Œ∏</mi><mtext id="alg1.l28.m1.1.1.3.1.3.3" xref="alg1.l28.m1.1.1.3.1.3.3a.cmml">S</mtext></msub></msub><msub id="alg1.l28.m1.1.1.3.2" xref="alg1.l28.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l28.m1.1.1.3.2.2" xref="alg1.l28.m1.1.1.3.2.2.cmml">‚Ñí</mi><mtext id="alg1.l28.m1.1.1.3.2.3" xref="alg1.l28.m1.1.1.3.2.3a.cmml">KD</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l28.m1.1b"><apply id="alg1.l28.m1.1.1.cmml" xref="alg1.l28.m1.1.1"><ci id="alg1.l28.m1.1.1.1.cmml" xref="alg1.l28.m1.1.1.1">‚Üê</ci><apply id="alg1.l28.m1.1.1.2.cmml" xref="alg1.l28.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l28.m1.1.1.2.1.cmml" xref="alg1.l28.m1.1.1.2">subscript</csymbol><ci id="alg1.l28.m1.1.1.2.2.cmml" xref="alg1.l28.m1.1.1.2.2">ùëî</ci><apply id="alg1.l28.m1.1.1.2.3.cmml" xref="alg1.l28.m1.1.1.2.3"><csymbol cd="ambiguous" id="alg1.l28.m1.1.1.2.3.1.cmml" xref="alg1.l28.m1.1.1.2.3">subscript</csymbol><ci id="alg1.l28.m1.1.1.2.3.2.cmml" xref="alg1.l28.m1.1.1.2.3.2">ùúÉ</ci><ci id="alg1.l28.m1.1.1.2.3.3a.cmml" xref="alg1.l28.m1.1.1.2.3.3"><mtext mathsize="50%" id="alg1.l28.m1.1.1.2.3.3.cmml" xref="alg1.l28.m1.1.1.2.3.3">S</mtext></ci></apply></apply><apply id="alg1.l28.m1.1.1.3.cmml" xref="alg1.l28.m1.1.1.3"><apply id="alg1.l28.m1.1.1.3.1.cmml" xref="alg1.l28.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg1.l28.m1.1.1.3.1.1.cmml" xref="alg1.l28.m1.1.1.3.1">subscript</csymbol><ci id="alg1.l28.m1.1.1.3.1.2.cmml" xref="alg1.l28.m1.1.1.3.1.2">‚àá</ci><apply id="alg1.l28.m1.1.1.3.1.3.cmml" xref="alg1.l28.m1.1.1.3.1.3"><csymbol cd="ambiguous" id="alg1.l28.m1.1.1.3.1.3.1.cmml" xref="alg1.l28.m1.1.1.3.1.3">subscript</csymbol><ci id="alg1.l28.m1.1.1.3.1.3.2.cmml" xref="alg1.l28.m1.1.1.3.1.3.2">ùúÉ</ci><ci id="alg1.l28.m1.1.1.3.1.3.3a.cmml" xref="alg1.l28.m1.1.1.3.1.3.3"><mtext mathsize="50%" id="alg1.l28.m1.1.1.3.1.3.3.cmml" xref="alg1.l28.m1.1.1.3.1.3.3">S</mtext></ci></apply></apply><apply id="alg1.l28.m1.1.1.3.2.cmml" xref="alg1.l28.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg1.l28.m1.1.1.3.2.1.cmml" xref="alg1.l28.m1.1.1.3.2">subscript</csymbol><ci id="alg1.l28.m1.1.1.3.2.2.cmml" xref="alg1.l28.m1.1.1.3.2.2">‚Ñí</ci><ci id="alg1.l28.m1.1.1.3.2.3a.cmml" xref="alg1.l28.m1.1.1.3.2.3"><mtext mathsize="70%" id="alg1.l28.m1.1.1.3.2.3.cmml" xref="alg1.l28.m1.1.1.3.2.3">KD</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l28.m1.1c">g_{\theta_{\text{S}}}\leftarrow\nabla_{\theta_{\text{S}}}\mathcal{L}_{\text{KD}}</annotation></semantics></math>

</div>
<div id="alg1.l29" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l29.1.1.1" class="ltx_text" style="font-size:80%;">29:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄÉ‚ÄÉ<math id="alg1.l29.m1.2" class="ltx_Math" alttext="{\theta_{\text{S}}}\leftarrow{\theta_{\text{S}}}-\alpha\cdot\text{Adam}({\theta_{\text{S}}},g_{\theta_{\text{S}}})" display="inline"><semantics id="alg1.l29.m1.2a"><mrow id="alg1.l29.m1.2.2" xref="alg1.l29.m1.2.2.cmml"><msub id="alg1.l29.m1.2.2.4" xref="alg1.l29.m1.2.2.4.cmml"><mi id="alg1.l29.m1.2.2.4.2" xref="alg1.l29.m1.2.2.4.2.cmml">Œ∏</mi><mtext id="alg1.l29.m1.2.2.4.3" xref="alg1.l29.m1.2.2.4.3a.cmml">S</mtext></msub><mo stretchy="false" id="alg1.l29.m1.2.2.3" xref="alg1.l29.m1.2.2.3.cmml">‚Üê</mo><mrow id="alg1.l29.m1.2.2.2" xref="alg1.l29.m1.2.2.2.cmml"><msub id="alg1.l29.m1.2.2.2.4" xref="alg1.l29.m1.2.2.2.4.cmml"><mi id="alg1.l29.m1.2.2.2.4.2" xref="alg1.l29.m1.2.2.2.4.2.cmml">Œ∏</mi><mtext id="alg1.l29.m1.2.2.2.4.3" xref="alg1.l29.m1.2.2.2.4.3a.cmml">S</mtext></msub><mo id="alg1.l29.m1.2.2.2.3" xref="alg1.l29.m1.2.2.2.3.cmml">‚àí</mo><mrow id="alg1.l29.m1.2.2.2.2" xref="alg1.l29.m1.2.2.2.2.cmml"><mrow id="alg1.l29.m1.2.2.2.2.4" xref="alg1.l29.m1.2.2.2.2.4.cmml"><mi id="alg1.l29.m1.2.2.2.2.4.2" xref="alg1.l29.m1.2.2.2.2.4.2.cmml">Œ±</mi><mo lspace="0.222em" rspace="0.222em" id="alg1.l29.m1.2.2.2.2.4.1" xref="alg1.l29.m1.2.2.2.2.4.1.cmml">‚ãÖ</mo><mtext id="alg1.l29.m1.2.2.2.2.4.3" xref="alg1.l29.m1.2.2.2.2.4.3a.cmml">Adam</mtext></mrow><mo lspace="0em" rspace="0em" id="alg1.l29.m1.2.2.2.2.3" xref="alg1.l29.m1.2.2.2.2.3.cmml">‚Äã</mo><mrow id="alg1.l29.m1.2.2.2.2.2.2" xref="alg1.l29.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="alg1.l29.m1.2.2.2.2.2.2.3" xref="alg1.l29.m1.2.2.2.2.2.3.cmml">(</mo><msub id="alg1.l29.m1.1.1.1.1.1.1.1" xref="alg1.l29.m1.1.1.1.1.1.1.1.cmml"><mi id="alg1.l29.m1.1.1.1.1.1.1.1.2" xref="alg1.l29.m1.1.1.1.1.1.1.1.2.cmml">Œ∏</mi><mtext id="alg1.l29.m1.1.1.1.1.1.1.1.3" xref="alg1.l29.m1.1.1.1.1.1.1.1.3a.cmml">S</mtext></msub><mo id="alg1.l29.m1.2.2.2.2.2.2.4" xref="alg1.l29.m1.2.2.2.2.2.3.cmml">,</mo><msub id="alg1.l29.m1.2.2.2.2.2.2.2" xref="alg1.l29.m1.2.2.2.2.2.2.2.cmml"><mi id="alg1.l29.m1.2.2.2.2.2.2.2.2" xref="alg1.l29.m1.2.2.2.2.2.2.2.2.cmml">g</mi><msub id="alg1.l29.m1.2.2.2.2.2.2.2.3" xref="alg1.l29.m1.2.2.2.2.2.2.2.3.cmml"><mi id="alg1.l29.m1.2.2.2.2.2.2.2.3.2" xref="alg1.l29.m1.2.2.2.2.2.2.2.3.2.cmml">Œ∏</mi><mtext id="alg1.l29.m1.2.2.2.2.2.2.2.3.3" xref="alg1.l29.m1.2.2.2.2.2.2.2.3.3a.cmml">S</mtext></msub></msub><mo stretchy="false" id="alg1.l29.m1.2.2.2.2.2.2.5" xref="alg1.l29.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l29.m1.2b"><apply id="alg1.l29.m1.2.2.cmml" xref="alg1.l29.m1.2.2"><ci id="alg1.l29.m1.2.2.3.cmml" xref="alg1.l29.m1.2.2.3">‚Üê</ci><apply id="alg1.l29.m1.2.2.4.cmml" xref="alg1.l29.m1.2.2.4"><csymbol cd="ambiguous" id="alg1.l29.m1.2.2.4.1.cmml" xref="alg1.l29.m1.2.2.4">subscript</csymbol><ci id="alg1.l29.m1.2.2.4.2.cmml" xref="alg1.l29.m1.2.2.4.2">ùúÉ</ci><ci id="alg1.l29.m1.2.2.4.3a.cmml" xref="alg1.l29.m1.2.2.4.3"><mtext mathsize="70%" id="alg1.l29.m1.2.2.4.3.cmml" xref="alg1.l29.m1.2.2.4.3">S</mtext></ci></apply><apply id="alg1.l29.m1.2.2.2.cmml" xref="alg1.l29.m1.2.2.2"><minus id="alg1.l29.m1.2.2.2.3.cmml" xref="alg1.l29.m1.2.2.2.3"></minus><apply id="alg1.l29.m1.2.2.2.4.cmml" xref="alg1.l29.m1.2.2.2.4"><csymbol cd="ambiguous" id="alg1.l29.m1.2.2.2.4.1.cmml" xref="alg1.l29.m1.2.2.2.4">subscript</csymbol><ci id="alg1.l29.m1.2.2.2.4.2.cmml" xref="alg1.l29.m1.2.2.2.4.2">ùúÉ</ci><ci id="alg1.l29.m1.2.2.2.4.3a.cmml" xref="alg1.l29.m1.2.2.2.4.3"><mtext mathsize="70%" id="alg1.l29.m1.2.2.2.4.3.cmml" xref="alg1.l29.m1.2.2.2.4.3">S</mtext></ci></apply><apply id="alg1.l29.m1.2.2.2.2.cmml" xref="alg1.l29.m1.2.2.2.2"><times id="alg1.l29.m1.2.2.2.2.3.cmml" xref="alg1.l29.m1.2.2.2.2.3"></times><apply id="alg1.l29.m1.2.2.2.2.4.cmml" xref="alg1.l29.m1.2.2.2.2.4"><ci id="alg1.l29.m1.2.2.2.2.4.1.cmml" xref="alg1.l29.m1.2.2.2.2.4.1">‚ãÖ</ci><ci id="alg1.l29.m1.2.2.2.2.4.2.cmml" xref="alg1.l29.m1.2.2.2.2.4.2">ùõº</ci><ci id="alg1.l29.m1.2.2.2.2.4.3a.cmml" xref="alg1.l29.m1.2.2.2.2.4.3"><mtext id="alg1.l29.m1.2.2.2.2.4.3.cmml" xref="alg1.l29.m1.2.2.2.2.4.3">Adam</mtext></ci></apply><interval closure="open" id="alg1.l29.m1.2.2.2.2.2.3.cmml" xref="alg1.l29.m1.2.2.2.2.2.2"><apply id="alg1.l29.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l29.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l29.m1.1.1.1.1.1.1.1.1.cmml" xref="alg1.l29.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l29.m1.1.1.1.1.1.1.1.2.cmml" xref="alg1.l29.m1.1.1.1.1.1.1.1.2">ùúÉ</ci><ci id="alg1.l29.m1.1.1.1.1.1.1.1.3a.cmml" xref="alg1.l29.m1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="alg1.l29.m1.1.1.1.1.1.1.1.3.cmml" xref="alg1.l29.m1.1.1.1.1.1.1.1.3">S</mtext></ci></apply><apply id="alg1.l29.m1.2.2.2.2.2.2.2.cmml" xref="alg1.l29.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l29.m1.2.2.2.2.2.2.2.1.cmml" xref="alg1.l29.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="alg1.l29.m1.2.2.2.2.2.2.2.2.cmml" xref="alg1.l29.m1.2.2.2.2.2.2.2.2">ùëî</ci><apply id="alg1.l29.m1.2.2.2.2.2.2.2.3.cmml" xref="alg1.l29.m1.2.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="alg1.l29.m1.2.2.2.2.2.2.2.3.1.cmml" xref="alg1.l29.m1.2.2.2.2.2.2.2.3">subscript</csymbol><ci id="alg1.l29.m1.2.2.2.2.2.2.2.3.2.cmml" xref="alg1.l29.m1.2.2.2.2.2.2.2.3.2">ùúÉ</ci><ci id="alg1.l29.m1.2.2.2.2.2.2.2.3.3a.cmml" xref="alg1.l29.m1.2.2.2.2.2.2.2.3.3"><mtext mathsize="50%" id="alg1.l29.m1.2.2.2.2.2.2.2.3.3.cmml" xref="alg1.l29.m1.2.2.2.2.2.2.2.3.3">S</mtext></ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l29.m1.2c">{\theta_{\text{S}}}\leftarrow{\theta_{\text{S}}}-\alpha\cdot\text{Adam}({\theta_{\text{S}}},g_{\theta_{\text{S}}})</annotation></semantics></math>

</div>
<div id="alg1.l30" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l30.1.1.1" class="ltx_text" style="font-size:80%;">30:</span></span>¬†¬†¬†¬†¬†¬†¬†¬†¬†<span id="alg1.l30.2" class="ltx_text ltx_font_bold">end</span>¬†<span id="alg1.l30.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l31" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l31.1.1.1" class="ltx_text" style="font-size:80%;">31:</span></span>¬†¬†¬†¬†¬†<span id="alg1.l31.2" class="ltx_text ltx_font_bold">end</span>¬†<span id="alg1.l31.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l32" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l32.1.1.1" class="ltx_text" style="font-size:80%;">32:</span></span><span id="alg1.l32.2" class="ltx_text ltx_font_bold">end</span>¬†<span id="alg1.l32.3" class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
</figure>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.3" class="ltx_p">After updating the weights of student network with our knowledge distillation loss <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{KD}}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">‚Ñí</mi><mtext id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3a.cmml">KD</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">‚Ñí</ci><ci id="S3.SS2.p4.1.m1.1.1.3a.cmml" xref="S3.SS2.p4.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">KD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\mathcal{L}_{\text{KD}}</annotation></semantics></math> (as in Eq.¬†<a href="#S3.E1" title="In 3.2 Knowledge Distillation with Synthetic Data ‚Ä£ 3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), we sample around the intermediate latent codes based on the similarity of embeddings extracted by the student <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\bm{e}_{\text{S}}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><msub id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">ùíÜ</mi><mtext id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3a.cmml">S</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">ùíÜ</ci><ci id="S3.SS2.p4.2.m2.1.1.3a.cmml" xref="S3.SS2.p4.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\bm{e}_{\text{S}}</annotation></semantics></math> and teacher <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\bm{e}_{\text{T}}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">ùíÜ</mi><mtext id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3a.cmml">T</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">ùíÜ</ci><ci id="S3.SS2.p4.3.m3.1.1.3a.cmml" xref="S3.SS2.p4.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">T</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\bm{e}_{\text{T}}</annotation></semantics></math> networks in our batch. To this end, we use the cosine similarity and normalize it in (0,1) interval as follows:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\text{SIM}(\bm{e}_{\text{T}},\bm{e}_{\text{S}})=0.5\times(1+\frac{\bm{e}_{\text{S}}\cdot\bm{e}_{\text{T}}}{\left\lVert\bm{e}_{\text{S}}\right\rVert_{2}\cdot\left\lVert\bm{e}_{\text{T}}\right\rVert_{2}})." display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><mtext id="S3.E2.m1.3.3.1.1.2.4" xref="S3.E2.m1.3.3.1.1.2.4a.cmml">SIM</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml">‚Äã</mo><mrow id="S3.E2.m1.3.3.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.2.2.2.3" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml">ùíÜ</mi><mtext id="S3.E2.m1.3.3.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3a.cmml">T</mtext></msub><mo id="S3.E2.m1.3.3.1.1.2.2.2.4" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.2.2.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.2.2.cmml">ùíÜ</mi><mtext id="S3.E2.m1.3.3.1.1.2.2.2.2.3" xref="S3.E2.m1.3.3.1.1.2.2.2.2.3a.cmml">S</mtext></msub><mo stretchy="false" id="S3.E2.m1.3.3.1.1.2.2.2.5" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.4" xref="S3.E2.m1.3.3.1.1.4.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><mn id="S3.E2.m1.3.3.1.1.3.3" xref="S3.E2.m1.3.3.1.1.3.3.cmml">0.5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml">√ó</mo><mrow id="S3.E2.m1.3.3.1.1.3.1.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.3.1.1.2" xref="S3.E2.m1.3.3.1.1.3.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.3.1.1.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.cmml"><mn id="S3.E2.m1.3.3.1.1.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.3.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.3.3.1.1.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.cmml">+</mo><mfrac id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.2.2.4" xref="S3.E2.m1.2.2.4.cmml"><msub id="S3.E2.m1.2.2.4.2" xref="S3.E2.m1.2.2.4.2.cmml"><mi id="S3.E2.m1.2.2.4.2.2" xref="S3.E2.m1.2.2.4.2.2.cmml">ùíÜ</mi><mtext id="S3.E2.m1.2.2.4.2.3" xref="S3.E2.m1.2.2.4.2.3a.cmml">S</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.2.2.4.1" xref="S3.E2.m1.2.2.4.1.cmml">‚ãÖ</mo><msub id="S3.E2.m1.2.2.4.3" xref="S3.E2.m1.2.2.4.3.cmml"><mi id="S3.E2.m1.2.2.4.3.2" xref="S3.E2.m1.2.2.4.3.2.cmml">ùíÜ</mi><mtext id="S3.E2.m1.2.2.4.3.3" xref="S3.E2.m1.2.2.4.3.3a.cmml">T</mtext></msub></mrow><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><msub id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" stretchy="true" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.1.cmml">‚à•</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">ùíÜ</mi><mtext id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3a.cmml">S</mtext></msub><mo fence="true" lspace="0em" stretchy="true" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.2.1.cmml">‚à•</mo></mrow><mn id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">2</mn></msub><mo lspace="0.222em" id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">‚ãÖ</mo><msub id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml"><mrow id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.2.cmml"><mo fence="true" lspace="0.055em" rspace="0em" stretchy="true" id="S3.E2.m1.2.2.2.2.1.1.2" xref="S3.E2.m1.2.2.2.2.1.2.1.cmml">‚à•</mo><msub id="S3.E2.m1.2.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.1.2.cmml">ùíÜ</mi><mtext id="S3.E2.m1.2.2.2.2.1.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.3a.cmml">T</mtext></msub><mo fence="true" lspace="0em" stretchy="true" id="S3.E2.m1.2.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.2.1.2.1.cmml">‚à•</mo></mrow><mn id="S3.E2.m1.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.3.cmml">2</mn></msub></mrow></mfrac></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.4"></eq><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><times id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3"></times><ci id="S3.E2.m1.3.3.1.1.2.4a.cmml" xref="S3.E2.m1.3.3.1.1.2.4"><mtext id="S3.E2.m1.3.3.1.1.2.4.cmml" xref="S3.E2.m1.3.3.1.1.2.4">SIM</mtext></ci><interval closure="open" id="S3.E2.m1.3.3.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2">ùíÜ</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3">T</mtext></ci></apply><apply id="S3.E2.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2.2">ùíÜ</ci><ci id="S3.E2.m1.3.3.1.1.2.2.2.2.3a.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2.3">S</mtext></ci></apply></interval></apply><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><times id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2"></times><cn type="float" id="S3.E2.m1.3.3.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3">0.5</cn><apply id="S3.E2.m1.3.3.1.1.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1"><plus id="S3.E2.m1.3.3.1.1.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1"></plus><cn type="integer" id="S3.E2.m1.3.3.1.1.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.2">1</cn><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><divide id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2"></divide><apply id="S3.E2.m1.2.2.4.cmml" xref="S3.E2.m1.2.2.4"><ci id="S3.E2.m1.2.2.4.1.cmml" xref="S3.E2.m1.2.2.4.1">‚ãÖ</ci><apply id="S3.E2.m1.2.2.4.2.cmml" xref="S3.E2.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.4.2.1.cmml" xref="S3.E2.m1.2.2.4.2">subscript</csymbol><ci id="S3.E2.m1.2.2.4.2.2.cmml" xref="S3.E2.m1.2.2.4.2.2">ùíÜ</ci><ci id="S3.E2.m1.2.2.4.2.3a.cmml" xref="S3.E2.m1.2.2.4.2.3"><mtext mathsize="70%" id="S3.E2.m1.2.2.4.2.3.cmml" xref="S3.E2.m1.2.2.4.2.3">S</mtext></ci></apply><apply id="S3.E2.m1.2.2.4.3.cmml" xref="S3.E2.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.4.3.1.cmml" xref="S3.E2.m1.2.2.4.3">subscript</csymbol><ci id="S3.E2.m1.2.2.4.3.2.cmml" xref="S3.E2.m1.2.2.4.3.2">ùíÜ</ci><ci id="S3.E2.m1.2.2.4.3.3a.cmml" xref="S3.E2.m1.2.2.4.3.3"><mtext mathsize="70%" id="S3.E2.m1.2.2.4.3.3.cmml" xref="S3.E2.m1.2.2.4.3.3">T</mtext></ci></apply></apply><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><ci id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3">‚ãÖ</ci><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">delimited-‚à•‚à•</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">ùíÜ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">S</mtext></ci></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3">2</cn></apply><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">subscript</csymbol><apply id="S3.E2.m1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.2.2.1.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.2">delimited-‚à•‚à•</csymbol><apply id="S3.E2.m1.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.2">ùíÜ</ci><ci id="S3.E2.m1.2.2.2.2.1.1.1.3a.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3">T</mtext></ci></apply></apply><cn type="integer" id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\text{SIM}(\bm{e}_{\text{T}},\bm{e}_{\text{S}})=0.5\times(1+\frac{\bm{e}_{\text{S}}\cdot\bm{e}_{\text{T}}}{\left\lVert\bm{e}_{\text{S}}\right\rVert_{2}\cdot\left\lVert\bm{e}_{\text{T}}\right\rVert_{2}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.4" class="ltx_p">Having normalized similarity score <math id="S3.SS2.p4.4.m1.2" class="ltx_Math" alttext="\bm{s}_{\text{sim}}=\text{SIM}(e_{\text{T}},e_{\text{S}})" display="inline"><semantics id="S3.SS2.p4.4.m1.2a"><mrow id="S3.SS2.p4.4.m1.2.2" xref="S3.SS2.p4.4.m1.2.2.cmml"><msub id="S3.SS2.p4.4.m1.2.2.4" xref="S3.SS2.p4.4.m1.2.2.4.cmml"><mi id="S3.SS2.p4.4.m1.2.2.4.2" xref="S3.SS2.p4.4.m1.2.2.4.2.cmml">ùíî</mi><mtext id="S3.SS2.p4.4.m1.2.2.4.3" xref="S3.SS2.p4.4.m1.2.2.4.3a.cmml">sim</mtext></msub><mo id="S3.SS2.p4.4.m1.2.2.3" xref="S3.SS2.p4.4.m1.2.2.3.cmml">=</mo><mrow id="S3.SS2.p4.4.m1.2.2.2" xref="S3.SS2.p4.4.m1.2.2.2.cmml"><mtext id="S3.SS2.p4.4.m1.2.2.2.4" xref="S3.SS2.p4.4.m1.2.2.2.4a.cmml">SIM</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m1.2.2.2.3" xref="S3.SS2.p4.4.m1.2.2.2.3.cmml">‚Äã</mo><mrow id="S3.SS2.p4.4.m1.2.2.2.2.2" xref="S3.SS2.p4.4.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p4.4.m1.2.2.2.2.2.3" xref="S3.SS2.p4.4.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.SS2.p4.4.m1.1.1.1.1.1.1" xref="S3.SS2.p4.4.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.4.m1.1.1.1.1.1.1.2" xref="S3.SS2.p4.4.m1.1.1.1.1.1.1.2.cmml">e</mi><mtext id="S3.SS2.p4.4.m1.1.1.1.1.1.1.3" xref="S3.SS2.p4.4.m1.1.1.1.1.1.1.3a.cmml">T</mtext></msub><mo id="S3.SS2.p4.4.m1.2.2.2.2.2.4" xref="S3.SS2.p4.4.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p4.4.m1.2.2.2.2.2.2" xref="S3.SS2.p4.4.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS2.p4.4.m1.2.2.2.2.2.2.2" xref="S3.SS2.p4.4.m1.2.2.2.2.2.2.2.cmml">e</mi><mtext id="S3.SS2.p4.4.m1.2.2.2.2.2.2.3" xref="S3.SS2.p4.4.m1.2.2.2.2.2.2.3a.cmml">S</mtext></msub><mo stretchy="false" id="S3.SS2.p4.4.m1.2.2.2.2.2.5" xref="S3.SS2.p4.4.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m1.2b"><apply id="S3.SS2.p4.4.m1.2.2.cmml" xref="S3.SS2.p4.4.m1.2.2"><eq id="S3.SS2.p4.4.m1.2.2.3.cmml" xref="S3.SS2.p4.4.m1.2.2.3"></eq><apply id="S3.SS2.p4.4.m1.2.2.4.cmml" xref="S3.SS2.p4.4.m1.2.2.4"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m1.2.2.4.1.cmml" xref="S3.SS2.p4.4.m1.2.2.4">subscript</csymbol><ci id="S3.SS2.p4.4.m1.2.2.4.2.cmml" xref="S3.SS2.p4.4.m1.2.2.4.2">ùíî</ci><ci id="S3.SS2.p4.4.m1.2.2.4.3a.cmml" xref="S3.SS2.p4.4.m1.2.2.4.3"><mtext mathsize="70%" id="S3.SS2.p4.4.m1.2.2.4.3.cmml" xref="S3.SS2.p4.4.m1.2.2.4.3">sim</mtext></ci></apply><apply id="S3.SS2.p4.4.m1.2.2.2.cmml" xref="S3.SS2.p4.4.m1.2.2.2"><times id="S3.SS2.p4.4.m1.2.2.2.3.cmml" xref="S3.SS2.p4.4.m1.2.2.2.3"></times><ci id="S3.SS2.p4.4.m1.2.2.2.4a.cmml" xref="S3.SS2.p4.4.m1.2.2.2.4"><mtext id="S3.SS2.p4.4.m1.2.2.2.4.cmml" xref="S3.SS2.p4.4.m1.2.2.2.4">SIM</mtext></ci><interval closure="open" id="S3.SS2.p4.4.m1.2.2.2.2.3.cmml" xref="S3.SS2.p4.4.m1.2.2.2.2.2"><apply id="S3.SS2.p4.4.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.4.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.4.m1.1.1.1.1.1.1.2">ùëí</ci><ci id="S3.SS2.p4.4.m1.1.1.1.1.1.1.3a.cmml" xref="S3.SS2.p4.4.m1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.4.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.4.m1.1.1.1.1.1.1.3">T</mtext></ci></apply><apply id="S3.SS2.p4.4.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.p4.4.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p4.4.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p4.4.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p4.4.m1.2.2.2.2.2.2.2">ùëí</ci><ci id="S3.SS2.p4.4.m1.2.2.2.2.2.2.3a.cmml" xref="S3.SS2.p4.4.m1.2.2.2.2.2.2.3"><mtext mathsize="70%" id="S3.SS2.p4.4.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p4.4.m1.2.2.2.2.2.2.3">S</mtext></ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m1.2c">\bm{s}_{\text{sim}}=\text{SIM}(e_{\text{T}},e_{\text{S}})</annotation></semantics></math>, we re-sample around each latent code:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\bm{w}_{\text{resample}}=\bm{w}+c\times\bm{s}_{\text{sim}}\times\bm{n}," display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.2.cmml">ùíò</mi><mtext id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3a.cmml">resample</mtext></msub><mo id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml">ùíò</mi><mo id="S3.E3.m1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.2.cmml">c</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.3.3.1.cmml">√ó</mo><msub id="S3.E3.m1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.3.2.cmml">ùíî</mi><mtext id="S3.E3.m1.1.1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.3a.cmml">sim</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.3.3.1a" xref="S3.E3.m1.1.1.1.1.3.3.1.cmml">√ó</mo><mi id="S3.E3.m1.1.1.1.1.3.3.4" xref="S3.E3.m1.1.1.1.1.3.3.4.cmml">ùíè</mi></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"></eq><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2">ùíò</ci><ci id="S3.E3.m1.1.1.1.1.2.3a.cmml" xref="S3.E3.m1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3">resample</mtext></ci></apply><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><plus id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1"></plus><ci id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2">ùíò</ci><apply id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3"><times id="S3.E3.m1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2">ùëê</ci><apply id="S3.E3.m1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.2">ùíî</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.3a.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.E3.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.3">sim</mtext></ci></apply><ci id="S3.E3.m1.1.1.1.1.3.3.4.cmml" xref="S3.E3.m1.1.1.1.1.3.3.4">ùíè</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\bm{w}_{\text{resample}}=\bm{w}+c\times\bm{s}_{\text{sim}}\times\bm{n},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.8" class="ltx_p">where <math id="S3.SS2.p4.5.m1.2" class="ltx_Math" alttext="\bm{n}\sim\mathcal{N}(0,\mathbb{I})" display="inline"><semantics id="S3.SS2.p4.5.m1.2a"><mrow id="S3.SS2.p4.5.m1.2.3" xref="S3.SS2.p4.5.m1.2.3.cmml"><mi id="S3.SS2.p4.5.m1.2.3.2" xref="S3.SS2.p4.5.m1.2.3.2.cmml">ùíè</mi><mo id="S3.SS2.p4.5.m1.2.3.1" xref="S3.SS2.p4.5.m1.2.3.1.cmml">‚àº</mo><mrow id="S3.SS2.p4.5.m1.2.3.3" xref="S3.SS2.p4.5.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.5.m1.2.3.3.2" xref="S3.SS2.p4.5.m1.2.3.3.2.cmml">ùí©</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.5.m1.2.3.3.1" xref="S3.SS2.p4.5.m1.2.3.3.1.cmml">‚Äã</mo><mrow id="S3.SS2.p4.5.m1.2.3.3.3.2" xref="S3.SS2.p4.5.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p4.5.m1.2.3.3.3.2.1" xref="S3.SS2.p4.5.m1.2.3.3.3.1.cmml">(</mo><mn id="S3.SS2.p4.5.m1.1.1" xref="S3.SS2.p4.5.m1.1.1.cmml">0</mn><mo id="S3.SS2.p4.5.m1.2.3.3.3.2.2" xref="S3.SS2.p4.5.m1.2.3.3.3.1.cmml">,</mo><mi id="S3.SS2.p4.5.m1.2.2" xref="S3.SS2.p4.5.m1.2.2.cmml">ùïÄ</mi><mo stretchy="false" id="S3.SS2.p4.5.m1.2.3.3.3.2.3" xref="S3.SS2.p4.5.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m1.2b"><apply id="S3.SS2.p4.5.m1.2.3.cmml" xref="S3.SS2.p4.5.m1.2.3"><csymbol cd="latexml" id="S3.SS2.p4.5.m1.2.3.1.cmml" xref="S3.SS2.p4.5.m1.2.3.1">similar-to</csymbol><ci id="S3.SS2.p4.5.m1.2.3.2.cmml" xref="S3.SS2.p4.5.m1.2.3.2">ùíè</ci><apply id="S3.SS2.p4.5.m1.2.3.3.cmml" xref="S3.SS2.p4.5.m1.2.3.3"><times id="S3.SS2.p4.5.m1.2.3.3.1.cmml" xref="S3.SS2.p4.5.m1.2.3.3.1"></times><ci id="S3.SS2.p4.5.m1.2.3.3.2.cmml" xref="S3.SS2.p4.5.m1.2.3.3.2">ùí©</ci><interval closure="open" id="S3.SS2.p4.5.m1.2.3.3.3.1.cmml" xref="S3.SS2.p4.5.m1.2.3.3.3.2"><cn type="integer" id="S3.SS2.p4.5.m1.1.1.cmml" xref="S3.SS2.p4.5.m1.1.1">0</cn><ci id="S3.SS2.p4.5.m1.2.2.cmml" xref="S3.SS2.p4.5.m1.2.2">ùïÄ</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m1.2c">\bm{n}\sim\mathcal{N}(0,\mathbb{I})</annotation></semantics></math> is a random noise with Gaussian distribution and <math id="S3.SS2.p4.6.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p4.6.m2.1a"><mi id="S3.SS2.p4.6.m2.1.1" xref="S3.SS2.p4.6.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m2.1b"><ci id="S3.SS2.p4.6.m2.1.1.cmml" xref="S3.SS2.p4.6.m2.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m2.1c">c</annotation></semantics></math> is a constant coefficient. As a matter of fact, in our re-sampling based on similarity score <math id="S3.SS2.p4.7.m3.1" class="ltx_Math" alttext="\bm{s}_{\text{sim}}" display="inline"><semantics id="S3.SS2.p4.7.m3.1a"><msub id="S3.SS2.p4.7.m3.1.1" xref="S3.SS2.p4.7.m3.1.1.cmml"><mi id="S3.SS2.p4.7.m3.1.1.2" xref="S3.SS2.p4.7.m3.1.1.2.cmml">ùíî</mi><mtext id="S3.SS2.p4.7.m3.1.1.3" xref="S3.SS2.p4.7.m3.1.1.3a.cmml">sim</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m3.1b"><apply id="S3.SS2.p4.7.m3.1.1.cmml" xref="S3.SS2.p4.7.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m3.1.1.1.cmml" xref="S3.SS2.p4.7.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.7.m3.1.1.2.cmml" xref="S3.SS2.p4.7.m3.1.1.2">ùíî</ci><ci id="S3.SS2.p4.7.m3.1.1.3a.cmml" xref="S3.SS2.p4.7.m3.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.7.m3.1.1.3.cmml" xref="S3.SS2.p4.7.m3.1.1.3">sim</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m3.1c">\bm{s}_{\text{sim}}</annotation></semantics></math> as in Eq.¬†<a href="#S3.E3" title="In 3.2 Knowledge Distillation with Synthetic Data ‚Ä£ 3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we sample with higher standard deviation values around the latent codes which achieved higher similarity in our initial sampling, and thus letting more variation in re-sampling. While, for the lower similarity between embeddings extracted by the student and teacher networks, the standard deviation values for re-sampling are smaller so that during re-sampling we can sample around the same latent codes. Therefore, our dynamic re-sampling approach helps us further sample difficult images while exploring the latent space. Fig.¬†<a href="#S3.F3" title="Figure 3 ‚Ä£ 3.1 Lightweight Network Architecture ‚Ä£ 3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates our re-sampling strategy.
After re-sampling new latent codes, we generate synthetic face images and optimize our student network with our knowledge distillation loss <math id="S3.SS2.p4.8.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{KD}}" display="inline"><semantics id="S3.SS2.p4.8.m4.1a"><msub id="S3.SS2.p4.8.m4.1.1" xref="S3.SS2.p4.8.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.8.m4.1.1.2" xref="S3.SS2.p4.8.m4.1.1.2.cmml">‚Ñí</mi><mtext id="S3.SS2.p4.8.m4.1.1.3" xref="S3.SS2.p4.8.m4.1.1.3a.cmml">KD</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m4.1b"><apply id="S3.SS2.p4.8.m4.1.1.cmml" xref="S3.SS2.p4.8.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m4.1.1.1.cmml" xref="S3.SS2.p4.8.m4.1.1">subscript</csymbol><ci id="S3.SS2.p4.8.m4.1.1.2.cmml" xref="S3.SS2.p4.8.m4.1.1.2">‚Ñí</ci><ci id="S3.SS2.p4.8.m4.1.1.3a.cmml" xref="S3.SS2.p4.8.m4.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.8.m4.1.1.3.cmml" xref="S3.SS2.p4.8.m4.1.1.3">KD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m4.1c">\mathcal{L}_{\text{KD}}</annotation></semantics></math> (as in Eq.¬†<a href="#S3.E1" title="In 3.2 Knowledge Distillation with Synthetic Data ‚Ä£ 3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Our knowledge distillation framework using synthetic data (named <span id="S3.SS2.p4.8.1" class="ltx_text ltx_font_italic">SynthDistill</span>) is depicted in Fig.¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ 2.1 Synthetic Datasets ‚Ä£ 2 Related works ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and summarized in Algorithm¬†<a href="#alg1" title="Algorithm 1 ‚Ä£ 3.2 Knowledge Distillation with Synthetic Data ‚Ä£ 3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we report our experiments and discuss our results. First, in Section¬†<a href="#S4.SS1" title="4.1 Datasets ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> we describe our evaluation datasets, and in Section¬†<a href="#S4.SS2" title="4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> we explain our training details. In Section¬†<a href="#S4.SS3" title="4.3 Comparison ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, we compare our method with previous methods based on synthetic data for face recognition in the literature. Then, we report different ablation studies and discuss effect of each part in our proposed framework in Section¬†<a href="#S4.SS4" title="4.4 Ablation studies ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate our trained student models using five different benchmarking datasets. The datasets chosen for evaluation comprised Labeled Faces in the Wild (LFW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Cross-age LFW (CA-LFW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, CrossPose LFW (CP-LFW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, Celebrities in Frontal-Profile in the Wild (CFP-FP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, AgeDB-30 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. To maintain consistency with previous work, we present recognition accuracy values on these datasets.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Complexity of different network structures</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:184.3pt;height:63.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.8pt,13.1pt) scale(0.708881680503152,0.708881680503152) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Role in our KD</th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Network</th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">M FLOPS</th>
<th id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">M Params</th>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.1.1.2.2.1.1" class="ltx_ERROR undefined">\rowcolor</span>Gray
Teacher</th>
<th id="S4.T1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">IResNet100</th>
<th id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">24,179.2</th>
<th id="S4.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">65.2</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.3.1" class="ltx_tr">
<th id="S4.T1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T1.1.1.3.1.1.1" class="ltx_text">Student</span></th>
<th id="S4.T1.1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">TinyFaR-A</th>
<td id="S4.T1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">254.3</td>
<td id="S4.T1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">5.6</td>
</tr>
<tr id="S4.T1.1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TinyFaR-B</th>
<td id="S4.T1.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">151.3</td>
<td id="S4.T1.1.1.4.2.3" class="ltx_td ltx_align_center">3.1</td>
</tr>
<tr id="S4.T1.1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">TinyFaR-C</th>
<td id="S4.T1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">76.8</td>
<td id="S4.T1.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">1.8</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">For the teacher network, we use the pretrained ArcFace model<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The performance of our teacher network on our benchmarking datasets in terms of recognition accuracy is as follows:
LFW (99.77 ¬± 0.28),
CA-LFW (96.10 ¬± 1.10),
CP-LFW (92.88 ¬± 1.52),
CFP-FP (96.27 ¬± 1.10), and
AgeDB-30 (98.25 ¬± 0.71).
</span></span></span> with IResnet100 backbone from Insightface¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> trained on the MS-Celeb dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
The embedding of our teacher network has 512 dimensions, but the feature layer of TinyNet has 1280 dimensions. Therefore, as discussed in Section¬†<a href="#S3" title="3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we use a fully connected layer at the output of our TinyNet model so that it can generate embeddings with the same dimension as the teacher‚Äôs embeddings and call it <span id="S4.SS2.p1.3.1" class="ltx_text ltx_font_italic">TinyFaR</span>.
In our experiments, we use different variations of TinyNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and build corresponding version of TinyFaR with 512-length feature as our student (lightweight) network.
Table¬†<a href="#S4.T1" title="Table 1 ‚Ä£ 4.1 Datasets ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares IResnet100 with different variations of TinyFaR in terms of computation complexity and number of parameters.
We use StyleGAN2-ADA model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to generate synthetic face images with <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">256\times 256</annotation></semantics></math> resolution and crop and resize images to have <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="112\times 112" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">112</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">112</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">112</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">112</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">112\times 112</annotation></semantics></math> face images for our knowledge distillation. We train our student networks with 17 epochs, where in each epoch we sampled one million images in step 1 of our algorithm¬†<a href="#alg1" title="Algorithm 1 ‚Ä£ 3.2 Knowledge Distillation with Synthetic Data ‚Ä£ 3 Proposed Framework ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and re-sampled the same number of images with the re-sampling coefficient of <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">c</mi><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><eq id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">ùëê</ci><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">c=1</annotation></semantics></math>. We trained our student networks using Adam optimizer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> on a system equipped with a single NVIDIA GeForce RTX<sup id="S4.SS2.p1.3.2" class="ltx_sup">TM</sup> 3090.
For training face recognition from scratch in our experiments, we used CosFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> loss function.
The source codes of our experiments are publicly available<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Source code: <a target="_blank" href="https://gitlab.idiap.ch/bob/bob.paper.ijcb2023_synthdistill" title="" class="ltx_ref ltx_href">https://gitlab.idiap.ch/bob/bob.paper.ijcb2023_synthdistill</a></span></span></span>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Synthetic and real face datasets</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:41.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-127.1pt,24.3pt) scale(0.46039908365131,0.46039908365131) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">Dataset</th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">#Images</th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">#Subjects</th>
<th id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Data</th>
<th id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Method</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<td id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.2.1.1.1" class="ltx_ERROR undefined">\rowcolor</span>Gray
WebFace-4M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,235,242</td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">205,990</td>
<td id="S4.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Real</td>
<td id="S4.T2.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">Web-crawled</td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> (IJCB 2022)</td>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">1,885,877</td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">10,572</td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">Synthetic</td>
<td id="S4.T2.1.1.3.2.5" class="ltx_td ltx_align_left">StyleGAN model</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<td id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">DigiFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> (WACV 2023)</td>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">1,219,995</td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">109,999</td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">Synthetic</td>
<td id="S4.T2.1.1.4.3.5" class="ltx_td ltx_align_left">Rendering</td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<td id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (CVPR 2023)</td>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">1,300,000</td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">60,000</td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Synthetic</td>
<td id="S4.T2.1.1.5.4.5" class="ltx_td ltx_align_left ltx_border_bb">Diffusion model</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of our knowledge distillation approach with training from scratch using other synthetic datasets</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:472.0pt;height:188.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-124.4pt,49.7pt) scale(0.654850834580002,0.654850834580002) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Network</th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Training</th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Dataset</th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">LFW</th>
<th id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">CA-LFW</th>
<th id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">CP-LFW</th>
<th id="S4.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">CFP-FP</th>
<th id="S4.T3.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">AgeDB-30</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;" rowspan="5">
<span id="S4.T3.1.1.2.1.1.1" class="ltx_ERROR undefined">\rowcolor</span>Gray
<span id="S4.T3.1.1.2.1.1.2" class="ltx_ERROR undefined">\cellcolor</span>white
<span id="S4.T3.1.1.2.1.1.3" class="ltx_text">TinyFaR-A</span>
</th>
<th id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;" rowspan="4">
<span id="S4.T3.1.1.2.1.2.1" class="ltx_ERROR undefined">\cellcolor</span>white<span id="S4.T3.1.1.2.1.2.2" class="ltx_text">Classification</span>
</th>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">WebFace-4M (real)</td>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">99.58 ¬± 0.37</td>
<td id="S4.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">95.02 ¬± 1.00</td>
<td id="S4.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">91.82 ¬± 1.29</td>
<td id="S4.T3.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">95.09 ¬± 1.15</td>
<td id="S4.T3.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">94.62 ¬± 1.21</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<td id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">DCFace (synthetic)</td>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">97.35 ¬± 0.66</td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">90.08 ¬± 1.27</td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">79.63 ¬± 2.08</td>
<td id="S4.T3.1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.01 ¬± 1.62</td>
<td id="S4.T3.1.1.3.2.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">85.12 ¬± 2.05</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<td id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">SFace (synthetic)</td>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">90.48 ¬± 1.54</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">75.48 ¬± 2.27</td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">71.40 ¬± 1.89</td>
<td id="S4.T3.1.1.4.3.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">72.07 ¬± 2.38</td>
<td id="S4.T3.1.1.4.3.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.65 ¬± 2.53</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<td id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">DigiFace (synthetic)</td>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">89.12 ¬± 1.30</td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">71.65 ¬± 2.14</td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">69.63 ¬± 1.70</td>
<td id="S4.T3.1.1.5.4.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">76.24 ¬± 1.34</td>
<td id="S4.T3.1.1.5.4.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.60 ¬± 1.23</td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<th id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Knowledge Distillation</th>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.6.5.2.1" class="ltx_text ltx_font_bold">SynthDistill (synthetic) [ours]</span></td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.6.5.3.1" class="ltx_text ltx_font_bold">99.52 ¬± 0.31</span></td>
<td id="S4.T3.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.6.5.4.1" class="ltx_text ltx_font_bold">94.57 ¬± 1.01</span></td>
<td id="S4.T3.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.6.5.5.1" class="ltx_text ltx_font_bold">87.00 ¬± 1.64</span></td>
<td id="S4.T3.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.6.5.6.1" class="ltx_text ltx_font_bold">90.89 ¬± 1.54</span></td>
<td id="S4.T3.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.6.5.7.1" class="ltx_text ltx_font_bold">94.93 ¬± 1.35</span></td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<th id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;" rowspan="5">
<span id="S4.T3.1.1.7.6.1.1" class="ltx_ERROR undefined">\rowcolor</span>Gray
<span id="S4.T3.1.1.7.6.1.2" class="ltx_ERROR undefined">\cellcolor</span>white
<span id="S4.T3.1.1.7.6.1.3" class="ltx_text">TinyFaR-B</span>
</th>
<th id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;" rowspan="4">
<span id="S4.T3.1.1.7.6.2.1" class="ltx_ERROR undefined">\cellcolor</span>white<span id="S4.T3.1.1.7.6.2.2" class="ltx_text">Classification</span>
</th>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">WebFace-4M (real)</td>
<td id="S4.T3.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">99.55 ¬± 0.40</td>
<td id="S4.T3.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">94.73 ¬± 0.88</td>
<td id="S4.T3.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">90.95 ¬± 1.43</td>
<td id="S4.T3.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">94.00 ¬± 1.23</td>
<td id="S4.T3.1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">93.72 ¬± 1.37</td>
</tr>
<tr id="S4.T3.1.1.8.7" class="ltx_tr">
<td id="S4.T3.1.1.8.7.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">DCFace (synthetic)</td>
<td id="S4.T3.1.1.8.7.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">97.40 ¬± 0.75</td>
<td id="S4.T3.1.1.8.7.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">89.62 ¬± 1.37</td>
<td id="S4.T3.1.1.8.7.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">78.93 ¬± 1.74</td>
<td id="S4.T3.1.1.8.7.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.47 ¬± 1.74</td>
<td id="S4.T3.1.1.8.7.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">85.03 ¬± 1.97</td>
</tr>
<tr id="S4.T3.1.1.9.8" class="ltx_tr">
<td id="S4.T3.1.1.9.8.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">SFace (synthetic)</td>
<td id="S4.T3.1.1.9.8.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">91.10 ¬± 1.22</td>
<td id="S4.T3.1.1.9.8.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">76.15 ¬± 1.46</td>
<td id="S4.T3.1.1.9.8.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">72.02 ¬± 1.34</td>
<td id="S4.T3.1.1.9.8.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">71.13 ¬± 2.43</td>
<td id="S4.T3.1.1.9.8.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.73 ¬± 1.68</td>
</tr>
<tr id="S4.T3.1.1.10.9" class="ltx_tr">
<td id="S4.T3.1.1.10.9.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">DigiFace (synthetic)</td>
<td id="S4.T3.1.1.10.9.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">88.03 ¬± 1.05</td>
<td id="S4.T3.1.1.10.9.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">70.27 ¬± 2.17</td>
<td id="S4.T3.1.1.10.9.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.22 ¬± 1.74</td>
<td id="S4.T3.1.1.10.9.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">75.29 ¬± 2.14</td>
<td id="S4.T3.1.1.10.9.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.38 ¬± 1.82</td>
</tr>
<tr id="S4.T3.1.1.11.10" class="ltx_tr">
<th id="S4.T3.1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Knowledge Distillation</th>
<td id="S4.T3.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.11.10.2.1" class="ltx_text ltx_font_bold">SynthDistill (synthetic) [ours]</span></td>
<td id="S4.T3.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.11.10.3.1" class="ltx_text ltx_font_bold">99.20 ¬± 0.41</span></td>
<td id="S4.T3.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.11.10.4.1" class="ltx_text ltx_font_bold">93.78 ¬± 0.78</span></td>
<td id="S4.T3.1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.11.10.5.1" class="ltx_text ltx_font_bold">84.93 ¬± 2.10</span></td>
<td id="S4.T3.1.1.11.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.11.10.6.1" class="ltx_text ltx_font_bold">88.19 ¬± 1.34</span></td>
<td id="S4.T3.1.1.11.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.11.10.7.1" class="ltx_text ltx_font_bold">93.02 ¬± 1.30</span></td>
</tr>
<tr id="S4.T3.1.1.12.11" class="ltx_tr">
<th id="S4.T3.1.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;" rowspan="5">
<span id="S4.T3.1.1.12.11.1.1" class="ltx_ERROR undefined">\rowcolor</span>Gray
<span id="S4.T3.1.1.12.11.1.2" class="ltx_ERROR undefined">\cellcolor</span>white
<span id="S4.T3.1.1.12.11.1.3" class="ltx_text">TinyFaR-C</span>
</th>
<th id="S4.T3.1.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;" rowspan="4">
<span id="S4.T3.1.1.12.11.2.1" class="ltx_ERROR undefined">\cellcolor</span>white<span id="S4.T3.1.1.12.11.2.2" class="ltx_text">Classification</span>
</th>
<td id="S4.T3.1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">WebFace-4M (real)</td>
<td id="S4.T3.1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">99.37 ¬± 0.26</td>
<td id="S4.T3.1.1.12.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">93.08 ¬± 1.11</td>
<td id="S4.T3.1.1.12.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">88.98 ¬± 1.12</td>
<td id="S4.T3.1.1.12.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">92.30 ¬± 1.74</td>
<td id="S4.T3.1.1.12.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">91.18 ¬± 1.80</td>
</tr>
<tr id="S4.T3.1.1.13.12" class="ltx_tr">
<td id="S4.T3.1.1.13.12.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">DCFace (synthetic)</td>
<td id="S4.T3.1.1.13.12.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">96.78 ¬± 0.73</td>
<td id="S4.T3.1.1.13.12.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">88.48 ¬± 1.02</td>
<td id="S4.T3.1.1.13.12.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">77.22 ¬± 1.80</td>
<td id="S4.T3.1.1.13.12.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.59 ¬± 1.80</td>
<td id="S4.T3.1.1.13.12.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.65 ¬± 2.14</td>
</tr>
<tr id="S4.T3.1.1.14.13" class="ltx_tr">
<td id="S4.T3.1.1.14.13.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">SFace (synthetic)</td>
<td id="S4.T3.1.1.14.13.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">91.12 ¬± 1.01</td>
<td id="S4.T3.1.1.14.13.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">76.70 ¬± 1.25</td>
<td id="S4.T3.1.1.14.13.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">71.27 ¬± 1.98</td>
<td id="S4.T3.1.1.14.13.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">72.24 ¬± 1.53</td>
<td id="S4.T3.1.1.14.13.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">71.13 ¬± 1.35</td>
</tr>
<tr id="S4.T3.1.1.15.14" class="ltx_tr">
<td id="S4.T3.1.1.15.14.1" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">DigiFace (synthetic)</td>
<td id="S4.T3.1.1.15.14.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">87.47 ¬± 0.87</td>
<td id="S4.T3.1.1.15.14.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">69.18 ¬± 1.94</td>
<td id="S4.T3.1.1.15.14.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.05 ¬± 1.94</td>
<td id="S4.T3.1.1.15.14.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">74.16 ¬± 2.68</td>
<td id="S4.T3.1.1.15.14.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">67.23 ¬± 1.85</td>
</tr>
<tr id="S4.T3.1.1.16.15" class="ltx_tr">
<th id="S4.T3.1.1.16.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Knowledge Distillation</th>
<td id="S4.T3.1.1.16.15.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.16.15.2.1" class="ltx_text ltx_font_bold">SynthDistill (synthetic) [ours]</span></td>
<td id="S4.T3.1.1.16.15.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.16.15.3.1" class="ltx_text ltx_font_bold">98.58 ¬± 0.44</span></td>
<td id="S4.T3.1.1.16.15.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.16.15.4.1" class="ltx_text ltx_font_bold">91.80 ¬± 1.04</span></td>
<td id="S4.T3.1.1.16.15.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.16.15.5.1" class="ltx_text ltx_font_bold">82.00 ¬± 2.14</span></td>
<td id="S4.T3.1.1.16.15.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.16.15.6.1" class="ltx_text ltx_font_bold">84.54 ¬± 1.57</span></td>
<td id="S4.T3.1.1.16.15.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T3.1.1.16.15.7.1" class="ltx_text ltx_font_bold">88.98 ¬± 1.49</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation study on the effect of dynamic sampling</figcaption>
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:397.5pt;height:64.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-80.2pt,12.9pt) scale(0.712517340302979,0.712517340302979) ;">
<table id="S4.T4.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2.3.1" class="ltx_tr">
<th id="S4.T4.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Sampling</th>
<th id="S4.T4.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"># Samples/epoch</th>
<th id="S4.T4.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LFW</th>
<th id="S4.T4.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CA-LFW</th>
<th id="S4.T4.2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CP-LFW</th>
<th id="S4.T4.2.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CFP-FP</th>
<th id="S4.T4.2.2.3.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">AgeDB-30</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.4.1" class="ltx_tr">
<th id="S4.T4.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">static</th>
<td id="S4.T4.2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">1 M</td>
<td id="S4.T4.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">98.87 ¬± 0.39</td>
<td id="S4.T4.2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">92.93 ¬± 0.94</td>
<td id="S4.T4.2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">83.52 ¬± 1.63</td>
<td id="S4.T4.2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t">87.60 ¬± 1.44</td>
<td id="S4.T4.2.2.4.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">91.25 ¬± 2.18</td>
</tr>
<tr id="S4.T4.2.2.5.2" class="ltx_tr">
<th id="S4.T4.2.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">static</th>
<td id="S4.T4.2.2.5.2.2" class="ltx_td ltx_align_center">2 M</td>
<td id="S4.T4.2.2.5.2.3" class="ltx_td ltx_align_center">98.95 ¬± 0.47</td>
<td id="S4.T4.2.2.5.2.4" class="ltx_td ltx_align_center">93.67 ¬± 0.78</td>
<td id="S4.T4.2.2.5.2.5" class="ltx_td ltx_align_center">84.75 ¬± 1.94</td>
<td id="S4.T4.2.2.5.2.6" class="ltx_td ltx_align_center">88.51 ¬± 1.63</td>
<td id="S4.T4.2.2.5.2.7" class="ltx_td ltx_nopad_r ltx_align_center">92.83 ¬± 1.76</td>
</tr>
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">dynamic (re-sampling in <math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">ùíµ</mi><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">ùíµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\mathcal{Z}</annotation></semantics></math>)</th>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center">1M + 1M</td>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center">99.28 ¬± 0.30</td>
<td id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center">93.88 ¬± 1.09</td>
<td id="S4.T4.1.1.1.5" class="ltx_td ltx_align_center">84.45 ¬± 1.95</td>
<td id="S4.T4.1.1.1.6" class="ltx_td ltx_align_center">87.59 ¬± 1.20</td>
<td id="S4.T4.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center">92.45 ¬± 1.69</td>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<th id="S4.T4.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">dynamic (re-sampling in <math id="S4.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S4.T4.2.2.2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.T4.2.2.2.1.m1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><ci id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">\mathcal{W}</annotation></semantics></math>)</th>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb">1M + 1M</td>
<td id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_bb">99.52 ¬± 0.31</td>
<td id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_bb">94.57 ¬± 1.01</td>
<td id="S4.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_border_bb">87.00 ¬± 1.64</td>
<td id="S4.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_bb">90.89 ¬± 1.54</td>
<td id="S4.T4.2.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">94.93 ¬± 1.35</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We compare the performance of our proposed knowledge distillation framework with training the same network using synthetic datasets in the literature, including DigiFace¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, SFace¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and DCFace¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. In addition, we also consider training with real data using WebFace-4M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> as our baseline.
Table¬†<a href="#S4.T2" title="Table 2 ‚Ä£ 4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares these datasets in terms of the number of images and samples and their generation method. All these datasets are generated to have inter-class and intra-class variation, and thus have identity labels. Therefore, these datasets can be used for training lightweight face recognition from scratch using the classification training. In contrast, our proposed framework based on dynamic sampling approach does not provide idenity labels and can be used within a knowledge distillation training.
Table¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> reports the recognition performance of different variations of TinyFaR when training with datasets.
As the results in this table show, our knowledge distillation approach with synthetic data (and no identity labels) far outperforms training from scratch using synthetic data and has comparable performance with training using real data.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation studies</h3>

<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of dynamic sampling:</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.4" class="ltx_p">To evaluate the effect of dynamic sampling in our proposed framework, we compare the performance network trained with knowledge distillation using our dynamic sampling (sampling + re-sampling) using static sampling (with no re-sampling). Table¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> compares the performance of TinyFaR-A trained with knowledge distillation using our dynamic sampling (sampling + re-sampling in <math id="S4.SS4.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S4.SS4.SSS0.Px1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px1.p1.1.m1.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.1.m1.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px1.p1.1.m1.1c">\mathcal{W}</annotation></semantics></math> space) with one million samples plus one million re-sampling (1M+1M) in each epoch as well as static sampling with one million and two million samples in each epoch. As the results in this table show knowledge distillation using our dynamic sampling with one million iterations in each epoch outperforms the same number of iterations or sample total samples with static sampling. This table also compares our dynamic re-sampling in <math id="S4.SS4.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S4.SS4.SSS0.Px1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px1.p1.2.m2.1b"><ci id="S4.SS4.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px1.p1.2.m2.1c">\mathcal{W}</annotation></semantics></math> space to dynamic re-sampling in <math id="S4.SS4.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{Z}" display="inline"><semantics id="S4.SS4.SSS0.Px1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.cmml">ùíµ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px1.p1.3.m3.1b"><ci id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1">ùíµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px1.p1.3.m3.1c">\mathcal{Z}</annotation></semantics></math> space.
As the results show dynamic re-sampling in both spaces achieves better performance than static sampling. In addition, comparing dynamic re-sampling space, the results show that dynamic re-sampling in <math id="S4.SS4.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S4.SS4.SSS0.Px1.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px1.p1.4.m4.1b"><ci id="S4.SS4.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px1.p1.4.m4.1c">\mathcal{W}</annotation></semantics></math> leads to superior performance.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation study on the effect of number of sampling</figcaption>
<div id="S4.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:44.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.4pt,13.8pt) scale(0.61661744180526,0.61661744180526) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"># Itr</th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">LFW</th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">CA-LFW</th>
<th id="S4.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">CP-LFW</th>
<th id="S4.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">CFP-FP</th>
<th id="S4.T5.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">AgeDB-30</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.2.1" class="ltx_tr">
<th id="S4.T5.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">0.5 M</th>
<td id="S4.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">99.43 ¬± 0.37</td>
<td id="S4.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">93.90 ¬± 1.11</td>
<td id="S4.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">86.13 ¬± 1.81</td>
<td id="S4.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">89.46 ¬± 1.48</td>
<td id="S4.T5.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">93.53 ¬± 1.36</td>
</tr>
<tr id="S4.T5.1.1.3.2" class="ltx_tr">
<th id="S4.T5.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">1 M</th>
<td id="S4.T5.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">99.52 ¬± 0.31</td>
<td id="S4.T5.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.57 ¬± 1.01</td>
<td id="S4.T5.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.00 ¬± 1.64</td>
<td id="S4.T5.1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">90.89 ¬± 1.54</td>
<td id="S4.T5.1.1.3.2.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.93 ¬± 1.35</td>
</tr>
<tr id="S4.T5.1.1.4.3" class="ltx_tr">
<th id="S4.T5.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">2 M</th>
<td id="S4.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">99.48 ¬± 0.39</td>
<td id="S4.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">95.07 ¬± 0.97</td>
<td id="S4.T5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">87.78 ¬± 1.64</td>
<td id="S4.T5.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">91.31 ¬± 1.99</td>
<td id="S4.T5.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">95.25 ¬± 1.19</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of number of sampled images:</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">To evaluate the effect of the number of sample images in our dynamic sampling, we train TinyFaR-A with different numbers of iterations (sampling and re-sampling) per epoch in our knowledge distillation approach. Table¬†<a href="#S4.T6" title="Table 6 ‚Ä£ Effect of number of sampled images: ‚Ä£ 4.4 Ablation studies ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reports the performance of the trained model with different numbers of iterations. As the results in this table show, higher iterations help our knowledge distillation with the cost of more training computation. However, to reduce computations in our experiments we use one million iterations (1M sampling + 1M re-sampling) in our experiments.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Ablation study on the effect of re-sampling coefficient</figcaption>
<div id="S4.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:97.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-72.5pt,32.5pt) scale(0.599298861084749,0.599298861084749) ;">
<table id="S4.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">coef. (<math id="S4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.T6.1.1.1.1.m1.1a"><mi id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">c</annotation></semantics></math>)</th>
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">LFW</th>
<th id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">CA-LFW</th>
<th id="S4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">CP-LFW</th>
<th id="S4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">CFP-FP</th>
<th id="S4.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">AgeDB-30</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.2.1" class="ltx_tr">
<th id="S4.T6.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">0.8</th>
<td id="S4.T6.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">99.45 ¬± 0.32</td>
<td id="S4.T6.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">94.58 ¬± 0.95</td>
<td id="S4.T6.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">86.10 ¬± 2.23</td>
<td id="S4.T6.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">90.23 ¬± 1.68</td>
<td id="S4.T6.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">94.82 ¬± 1.15</td>
</tr>
<tr id="S4.T6.1.1.3.2" class="ltx_tr">
<th id="S4.T6.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">0.9</th>
<td id="S4.T6.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">99.40 ¬± 0.41</td>
<td id="S4.T6.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.90 ¬± 1.09</td>
<td id="S4.T6.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.23 ¬± 2.02</td>
<td id="S4.T6.1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">90.36 ¬± 1.45</td>
<td id="S4.T6.1.1.3.2.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.72 ¬± 1.07</td>
</tr>
<tr id="S4.T6.1.1.4.3" class="ltx_tr">
<th id="S4.T6.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<td id="S4.T6.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">99.52 ¬± 0.31</td>
<td id="S4.T6.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.57 ¬± 1.01</td>
<td id="S4.T6.1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.00 ¬± 1.64</td>
<td id="S4.T6.1.1.4.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">90.89 ¬± 1.54</td>
<td id="S4.T6.1.1.4.3.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.93 ¬± 1.35</td>
</tr>
<tr id="S4.T6.1.1.5.4" class="ltx_tr">
<th id="S4.T6.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">1.1</th>
<td id="S4.T6.1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">99.47 ¬± 0.44</td>
<td id="S4.T6.1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.95 ¬± 0.84</td>
<td id="S4.T6.1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.53 ¬± 1.78</td>
<td id="S4.T6.1.1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">90.81 ¬± 1.61</td>
<td id="S4.T6.1.1.5.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">95.13 ¬± 1.08</td>
</tr>
<tr id="S4.T6.1.1.6.5" class="ltx_tr">
<th id="S4.T6.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">1.2</th>
<td id="S4.T6.1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">99.53 ¬± 0.32</td>
<td id="S4.T6.1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.95 ¬± 0.90</td>
<td id="S4.T6.1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.47 ¬± 1.27</td>
<td id="S4.T6.1.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">90.94 ¬± 1.63</td>
<td id="S4.T6.1.1.6.5.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.52 ¬± 1.47</td>
</tr>
<tr id="S4.T6.1.1.7.6" class="ltx_tr">
<th id="S4.T6.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">1.3</th>
<td id="S4.T6.1.1.7.6.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">99.52 ¬± 0.31</td>
<td id="S4.T6.1.1.7.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.50 ¬± 0.97</td>
<td id="S4.T6.1.1.7.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.58 ¬± 1.84</td>
<td id="S4.T6.1.1.7.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">91.17 ¬± 1.50</td>
<td id="S4.T6.1.1.7.6.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">95.05 ¬± 1.28</td>
</tr>
<tr id="S4.T6.1.1.8.7" class="ltx_tr">
<th id="S4.T6.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">1.4</th>
<td id="S4.T6.1.1.8.7.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">99.48 ¬± 0.32</td>
<td id="S4.T6.1.1.8.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.77 ¬± 0.97</td>
<td id="S4.T6.1.1.8.7.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">87.40 ¬± 1.74</td>
<td id="S4.T6.1.1.8.7.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">90.56 ¬± 1.49</td>
<td id="S4.T6.1.1.8.7.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">94.78 ¬± 1.29</td>
</tr>
<tr id="S4.T6.1.1.9.8" class="ltx_tr">
<th id="S4.T6.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">1.5</th>
<td id="S4.T6.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">99.47 ¬± 0.32</td>
<td id="S4.T6.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">94.58 ¬± 1.00</td>
<td id="S4.T6.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">88.17 ¬± 1.64</td>
<td id="S4.T6.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">90.84 ¬± 1.24</td>
<td id="S4.T6.1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">94.80 ¬± 1.07</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of re-sampling coefficient:</h4>

<div id="S4.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p1.3" class="ltx_p">As another ablation study, we evaluate the effect of re-sampling coefficient <math id="S4.SS4.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.SS4.SSS0.Px3.p1.1.m1.1a"><mi id="S4.SS4.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px3.p1.1.m1.1b"><ci id="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.1.m1.1c">c</annotation></semantics></math> in our dynamic sampling. Table¬†<a href="#S4.T6" title="Table 6 ‚Ä£ Effect of number of sampled images: ‚Ä£ 4.4 Ablation studies ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reports the performance of TinyFaR-A trained with our knowledge distillation using different re-sampling coefficient values. As the results in this table show, with a higher re-sampling coefficient our dynamic re-sampling can generate more diverse images and achieve higher recognition performance. However, a very high re-sampling coefficient can also cause <math id="S4.SS4.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="\bm{w}_{\text{resample}}" display="inline"><semantics id="S4.SS4.SSS0.Px3.p1.2.m2.1a"><msub id="S4.SS4.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1.2.cmml">ùíò</mi><mtext id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.3" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1.3a.cmml">resample</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1.2">ùíò</ci><ci id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.3a.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1.3">resample</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.2.m2.1c">\bm{w}_{\text{resample}}</annotation></semantics></math> to be out of the distribution of <math id="S4.SS4.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S4.SS4.SSS0.Px3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS0.Px3.p1.3.m3.1.1" xref="S4.SS4.SSS0.Px3.p1.3.m3.1.1.cmml">ùí≤</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px3.p1.3.m3.1b"><ci id="S4.SS4.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.3.m3.1.1">ùí≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.3.m3.1c">\mathcal{W}</annotation></semantics></math>, and thus drop the performance.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The results in Table¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show that our proposed knowledge distillation framework outperforms training using synthetic datasets in the literature and achieves comparable performance with training using real face images. Comparing the performance of networks trained with previous synthetic datasets to networks trained with real data, we observe a considerable gap in the performance of trained face recognition models with synthetic and real data. Meanwhile, our proposed knowledge distillation method still achieves lower but is very close to the performance of training with real data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Unlike previous synthetic face datasets, our method does not require identity labels, and thus does not have many issues in generating synthetic datasets with inter-class and intra-class variations. Instead, our knowledge distillation approach with dynamic sampling leverages the most capacity of StyleGAN to generate training samples, which helps to achieve comparable performance to training with real data.
Our proposed framework avoids the requirements of hard identity labels for the generated images, which further assists the generation network to produce challenging samples though a feedback mechanism during our knowledge distillation, thus enabling the training of much robust models.
We should also note that, for generation of synthetic face datsets in the literature, a pretrained face recognition model (which has been trained on a large-scale real face recognition dataset) is used in the process of generation of synthetic dataset. Therefore, training with synthetic face datasets in the literature indirectly benefits from the information and knowledge of the pretrained face recognition model (trained on real images) used for generating the synthetic dataset.
In our proposed framework, we also use the pretrained face recognition model, but instead of following common two-step approach (generation of dataset and training with new dataset), we use the pretrained face recognition model as a teacher in our knowledge distillation approach and generate synthetic face images used in our training with no identity label.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Our ablation studies show the effect of each part in our knowledge distillation framework. In particular, the results demonstrate that our dynamic sampling improves our knowledge distillation compared to static sampling. In addition, using our dynamic sampling and with more number of iterations or higher re-sampling coefficient can improve the knowledge distillation, as it helps our student to learn embeddings of more face images from the teacher.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we proposed a data-free framework (named <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">SynthDistill</span>) to train lightweight face recognition models based on knowledge distillation using synthetic data.
We combined the two steps of data generation and training the lightweight network and have an online-generation and training in the loop using a distillation framework.
We dynamically generated synthetic face images during training and distilled the knowledge of a pretrained and blackbox face recognition model. Our dynamic sampling helps our student network to further see difficult samples while exploring new samples, leading to more robust training. Our knowledge distillation framework does not require identity-labeled training data, and thus mitigates challenges in generating intra-class variations in synthesized datasets.
We adapted the TinyNet architecture to use in our knowledge distillation framework and trained lightweight face recognition models (called <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">TinyFaR</span>).
We reported extensive experimental evaluation on five different face recognition benchmarking datasets,
including LFW, CA-LFW, CP-LFW, CFP-FP, and AgeDB-30. The experimental results demonstrate the superiority of our proposed knowledge distillation approach compared to training previous synthetic datasets.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Our experimental results also showed that while there is a considerable gap between training with synthetic datasets and real data, our knowledge distillation framework based on synthetic data achieves comparable performance with training with real data and significantly reduces the gap between models trained on synthetic data and models trained on real data. Achieving such an improvement in training using synthetic data within our proposed framework shows more potential in training with synthetic data and motivates further research on training with synthetic data.
Furthermore, our results for lightweight student networks pave the way for developing privacy-aware and resource-efficient face recognition models.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research is based upon work supported by the H2020 TReSPAsS-ETN Marie Sk≈Çodowska-Curie early training network (grant agreement 860813).</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">This research is also based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via [2022-21102100007]. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding
any copyright annotation therein.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Alansari, O.¬†A. Hay, S.¬†Javed, A.¬†Shoufan, Y.¬†Zweiri, and N.¬†Werghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Ghostfacenets: Lightweight face recognition model from cheap
operations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†M. Asano and A.¬†Saeed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">The augmented image prior: Distilling 1000 classes by extrapolating
from a single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.00725</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
G.¬†Bae, M.¬†de¬†La¬†Gorce, T.¬†Baltru≈°aitis, C.¬†Hewitt, D.¬†Chen, J.¬†Valentin,
R.¬†Cipolla, and J.¬†Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Digiface-1m: 1 million digital face images for face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 3526‚Äì3535, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Bao, D.¬†Chen, F.¬†Wen, H.¬†Li, and G.¬†Hua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Towards open-set identity preserving face synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 6713‚Äì6722, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
V.¬†Blanz and T.¬†Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">A morphable model for the synthesis of 3d faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 26th annual conference on Computer
graphics and interactive techniques</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 187‚Äì194, 1999.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
F.¬†Boutros, N.¬†Damer, M.¬†Fang, F.¬†Kirchbuchner, and A.¬†Kuijper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Mixfacenets: Extremely efficient face recognition networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE International Joint Conference on Biometrics
(IJCB)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 1‚Äì8. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
F.¬†Boutros, M.¬†Huber, P.¬†Siebke, T.¬†Rieber, and N.¬†Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Sface: Privacy-friendly and accurate face recognition using synthetic
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2022 IEEE International Joint Conference on Biometrics
(IJCB)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 1‚Äì11. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
F.¬†Boutros, M.¬†Klemt, M.¬†Fang, A.¬†Kuijper, and N.¬†Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Unsupervised face recognition using unlabeled synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2023 IEEE 17th International Conference on Automatic Face and
Gesture Recognition (FG)</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 1‚Äì8. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
F.¬†Boutros, P.¬†Siebke, M.¬†Klemt, N.¬†Damer, F.¬†Kirchbuchner, and A.¬†Kuijper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Pocketnet: Extreme lightweight face recognition network using neural
architecture search and multistep knowledge distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 10:46823‚Äì46833, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
F.¬†Boutros, V.¬†Struc, J.¬†Fierrez, and N.¬†Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Synthetic data for face recognition: Current state and future
prospects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Image and Vision Computing</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, page 104688, 2023.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Q.¬†Cao, L.¬†Shen, W.¬†Xie, O.¬†M. Parkhi, and A.¬†Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Vggface2: A dataset for recognising faces across pose and age.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 13th IEEE international conference on automatic face &amp;
gesture recognition (FG 2018)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 67‚Äì74. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
D.¬†Chen, J.-P. Mei, C.¬†Wang, Y.¬†Feng, and C.¬†Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Online knowledge distillation with diverse peers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial
intelligence</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, volume¬†34, pages 3430‚Äì3437, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
H.¬†Chen, Y.¬†Wang, C.¬†Xu, Z.¬†Yang, C.¬†Liu, B.¬†Shi, C.¬†Xu, C.¬†Xu, and Q.¬†Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Data-free learning of student networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 3514‚Äì3522, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
P.¬†Chen, S.¬†Liu, H.¬†Zhao, and J.¬†Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Distilling knowledge via knowledge review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 5008‚Äì5017, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Chen, Y.¬†Liu, X.¬†Gao, and Z.¬†Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Mobilefacenets: Efficient cnns for accurate real-time face
verification on mobile devices.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Biometric Recognition: 13th Chinese Conference, CCBR 2018,
Urumqi, China, August 11-12, 2018, Proceedings 13</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 428‚Äì438. Springer,
2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Deng, J.¬†Guo, N.¬†Xue, and S.¬†Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Arcface: Additive angular margin loss for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 4690‚Äì4699, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Deng, J.¬†Yang, D.¬†Chen, F.¬†Wen, and X.¬†Tong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Disentangled and controllable face image generation via 3d
imitative-contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 5154‚Äì5163, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
A.¬†George, C.¬†Ecabert, H.¬†O. Shahreza, K.¬†Kotwal, and S.¬†Marcel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Edgeface: Efficient face recognition model for edge devices.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2307.01838</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Guo, L.¬†Zhang, Y.¬†Hu, X.¬†He, and J.¬†Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Ms-celeb-1m: A dataset and benchmark for large-scale face
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 87‚Äì102.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
K.¬†Han, Y.¬†Wang, Q.¬†Zhang, W.¬†Zhang, C.¬†Xu, and T.¬†Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Model rubik‚Äôs cube: Twisting resolution, depth and width for
tinynets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">,
33:19353‚Äì19364, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
G.¬†Hinton, O.¬†Vinyals, and J.¬†Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Distilling the knowledge in a neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1503.02531</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
A.¬†G. Howard, M.¬†Zhu, B.¬†Chen, D.¬†Kalenichenko, W.¬†Wang, T.¬†Weyand,
M.¬†Andreetto, and H.¬†Adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Mobilenets: Efficient convolutional neural networks for mobile vision
applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1704.04861</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
G.¬†B. Huang, M.¬†Mattar, T.¬†Berg, and E.¬†Learned-Miller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Labeled faces in the wild: A database forstudying face recognition in
unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop on faces in‚ÄôReal-Life‚ÄôImages: detection, alignment,
and recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
A.¬†Jain, N.¬†Memon, and J.¬†Togelius.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Zero-shot racially balanced dataset generation using an existing
biased stylegan2.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.07710</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
T.¬†Karras, M.¬†Aittala, J.¬†Hellsten, S.¬†Laine, J.¬†Lehtinen, and T.¬†Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Training generative adversarial networks with limited data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">,
33:12104‚Äì12114, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Kim, S.¬†Park, and N.¬†Kwak.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Paraphrasing complex network: Network compression via factor
transfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 31, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Kim, A.¬†K. Jain, and X.¬†Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Adaface: Quality adaptive margin for face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 18750‚Äì18759, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Kim, F.¬†Liu, A.¬†Jain, and X.¬†Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Dcface: Synthetic face generation with dual condition diffusion
model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 12715‚Äì12725, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
D.¬†P. Kingma and J.¬†Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Learning
Representations (ICLR)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, San Diego, California., USA, May 2015.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
J.¬†N. Kolf, F.¬†Boutros, J.¬†Elliesen, M.¬†Theuerkauf, N.¬†Damer, M.¬†Alansari,
O.¬†A. Hay, S.¬†Alansari, S.¬†Javed, N.¬†Werghi, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Efar 2023: Efficient face recognition competition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.04168</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
N.¬†Komodakis and S.¬†Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Paying more attention to attention: improving the performance of
convolutional neural networks via attention transfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Z.¬†Li, X.¬†Li, L.¬†Yang, B.¬†Zhao, R.¬†Song, L.¬†Luo, J.¬†Li, and J.¬†Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Curriculum temperature for knowledge distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, volume¬†37, pages 1504‚Äì1512, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
R.¬†G. Lopes, S.¬†Fenu, and T.¬†Starner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Data-free knowledge distillation for deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.07535</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
N.¬†Ma, X.¬†Zhang, H.-T. Zheng, and J.¬†Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Shufflenet v2: Practical guidelines for efficient cnn architecture
design.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 116‚Äì131, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Martindez-Diaz, L.¬†S. Luevano, H.¬†Mendez-Vazquez, M.¬†Nicolas-Diaz, L.¬†Chang,
and M.¬†Gonzalez-Mendoza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Shufflefacenet: A lightweight face architecture for efficient and
highly-accurate face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision Workshops</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 0‚Äì0, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Moschoglou, A.¬†Papaioannou, C.¬†Sagonas, J.¬†Deng, I.¬†Kotsia, and
S.¬†Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Agedb: the first manually collected, in-the-wild age database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">proceedings of the IEEE conference on computer vision and
pattern recognition workshops</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 51‚Äì59, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
H.¬†Qiu, B.¬†Yu, D.¬†Gong, Z.¬†Li, W.¬†Liu, and D.¬†Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Synface: Face recognition with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 10880‚Äì10890, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
A.¬†Romero, N.¬†Ballas, S.¬†E. Kahou, A.¬†Chassang, C.¬†Gatta, and Y.¬†Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Fitnets: Hints for thin deep nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6550</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Sandler, A.¬†Howard, M.¬†Zhu, A.¬†Zhmoginov, and L.-C. Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Mobilenetv2: Inverted residuals and linear bottlenecks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 4510‚Äì4520, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Sengupta, J.-C. Chen, C.¬†Castillo, V.¬†M. Patel, R.¬†Chellappa, and D.¬†W.
Jacobs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Frontal to profile face verification in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE winter conference on applications of computer
vision (WACV)</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 1‚Äì9. IEEE, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
A.¬†Sevastopolsky, Y.¬†Malkov, N.¬†Durasov, L.¬†Verdoliva, and M.¬†Nie√üner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">How to boost face recognition with stylegan?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.10090</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Shen, P.¬†Luo, J.¬†Yan, X.¬†Wang, and X.¬†Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Faceid-gan: Learning a symmetry three-player gan for
identity-preserving face synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 821‚Äì830, 2018.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Tan and Q.¬†Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Efficientnet: Rethinking model scaling for convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages
6105‚Äì6114. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Tan and Q.¬†V. Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Mixconv: Mixed depthwise convolutional kernels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.09595</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Tian, D.¬†Krishnan, and P.¬†Isola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Contrastive representation distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.10699</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
H.¬†Wang, Y.¬†Wang, Z.¬†Zhou, X.¬†Ji, D.¬†Gong, J.¬†Zhou, Z.¬†Li, and W.¬†Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Cosface: Large margin cosine loss for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 5265‚Äì5274, 2018.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
X.¬†Wu, R.¬†He, Z.¬†Sun, and T.¬†Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">A light cnn for deep face representation with noisy labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">,
13(11):2884‚Äì2896, 2018.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Yan, M.¬†Zhao, Z.¬†Xu, Q.¬†Zhang, G.¬†Wang, and Z.¬†Su.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Vargfacenet: An efficient variable group convolutional neural network
for lightweight face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision Workshops</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 0‚Äì0, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
D.¬†Yi, Z.¬†Lei, S.¬†Liao, and S.¬†Z. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Learning face representation from scratch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1411.7923</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
H.¬†Yin, P.¬†Molchanov, J.¬†M. Alvarez, Z.¬†Li, A.¬†Mallya, D.¬†Hoiem, N.¬†K. Jha, and
J.¬†Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Dreaming to distill: Data-free knowledge transfer via deepinversion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 8715‚Äì8724, 2020.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
X.¬†Yin, X.¬†Yu, K.¬†Sohn, X.¬†Liu, and M.¬†Chandraker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Towards large-pose face frontalization in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, pages 3990‚Äì3999, 2017.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
L.¬†Zhang, J.¬†Song, A.¬†Gao, J.¬†Chen, C.¬†Bao, and K.¬†Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Be your own teacher: Improve the performance of convolutional neural
networks via self distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 3713‚Äì3722, 2019.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
T.¬†Zheng and W.¬†Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Cross-pose lfw: A database for studying cross-pose face recognition
in unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Beijing University of Posts and Telecommunications, Tech. Rep</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">,
5(7), 2018.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
T.¬†Zheng, W.¬†Deng, and J.¬†Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Cross-age lfw: A database for studying cross-age face recognition in
unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1708.08197</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
X.¬†Zhu, S.¬†Gong, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Knowledge distillation by on-the-fly native ensemble.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 31, 2018.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Z.¬†Zhu, G.¬†Huang, J.¬†Deng, Y.¬†Ye, J.¬†Huang, X.¬†Chen, J.¬†Zhu, T.¬†Yang, J.¬†Lu,
D.¬†Du, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Webface260m: A benchmark unveiling the power of million-scale deep
face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages 10492‚Äì10502, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.14851" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.14852" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.14852">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.14852" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.14853" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 10:14:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
