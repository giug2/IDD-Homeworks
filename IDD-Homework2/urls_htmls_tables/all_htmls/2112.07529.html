<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.07529] Improving COVID-19 CXR Detection with Synthetic Data Augmentation</title><meta property="og:description" content="Since the beginning of the COVID-19 pandemic, researchers have developed deep learning models to classify COVID-19 induced pneumonia. As with many medical imaging tasks, the quality and quantity of the available data i…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving COVID-19 CXR Detection with Synthetic Data Augmentation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving COVID-19 CXR Detection with Synthetic Data Augmentation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.07529">

<!--Generated on Mon Feb 26 20:03:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Deep Learning,  Medical Imaging,  GANs,  Data Augmentation">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Technische Hochschule Ulm - Ulm University of Applied Sciences
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>daniel.schaudt@thu.de, spaete@mail.hs-ulm.de, reinhold.vonschwerin@thu.de</span></span></span>
</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Universitätsklinikum Ulm - Ulm University Medical Center
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>christopher.kloth@uniklinik-ulm.de, andreas.hinteregger@uni-ulm.de, meinrad.beer@uniklinik-ulm.de</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Improving COVID-19 CXR Detection with Synthetic Data Augmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Schaudt
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christopher Kloth
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christian Späte
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andreas Hinteregger
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Meinrad Beer
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Reinhold von Schwerin
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Since the beginning of the COVID-19 pandemic, researchers have developed deep learning models to classify COVID-19 induced pneumonia. As with many medical imaging tasks, the quality and quantity of the available data is often limited. In this work we train a deep learning model on publicly available COVID-19 image data and evaluate the model on local hospital chest X-ray data. The data has been reviewed and labeled by two radiologists to ensure a high-quality estimation of the generalization capabilities of the model. Furthermore, we are using a Generative Adversarial Network to generate synthetic X-ray images based on this data. Our results show that using those synthetic images for data augmentation can improve the model’s performance significantly. This can be a promising approach for many sparse data domains.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Deep Learning, Medical Imaging, GANs, Data Augmentation
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The ongoing COVID-19 pandemic brings many challenges for societies all around the globe. For the healthcare sector, it is important to screen infected patients in an effective and reliable manner. This is especially true in an emergency setting, where patients already experience advanced symptoms. The prevalent test used for COVID-19 detection is the reverse transcription polymerase chain reaction (RT-PCR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. This method has a high false negative rate and the processing requires dedicated personnel and can take hours to days <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Since chest X-ray (CXR) images of COVID-19 patients show typical findings including peripheral opacities and ground class patterns in the absence of pleural effusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, they can be used as a first-line triage tool <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This could speed up the identification process, as CXR images are easy to obtain and rather inexpensive with a lower radiation dose than computed tomography (CT) images. Using deep learning models for detection of COVID-19 prevalence in CXR images is promising, because it eliminates the need for specialized medical staff in an emergency setting. This can further help to alleviate the challenges to the healthcare systems around the world and has the potential to save lives.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this retrospective study, we are training a deep convolutional neural network (CNN) on the openly available COVIDx V8b dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and evaluate the model on local hospital CXR data. We specifically choose this learning framework to assess the generalization abilities of a CNN in the medical imaging context. Since high quality CXR image data is sparse, we see this as the most common use case for models in production.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Furthermore, we are using a modified version of the StyleGAN architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> to generate synthetic COVID-19 positive and COVID-19 negative CXR images for data augmentation. This is done to offset some negative side effects encountered by a distributional shift between the training and testing data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There has been a lot of previous work on applying deep learning to CXR images to detect a COVID-19 pulmonary disease <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. However, most of the existing work is using publicly available CXR and COVID-19 image data. Most of those images are collected from heterogeneous sources with varying image and label quality, which raises concerns about the quality and valid evaluation of deep learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> have been used for many applications in the medical imaging domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Some studies show promising results specifically for the CXR and COVID-19 domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In contrast to existing work, we integrate differentiable augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> into our GAN architecture. This enables us to train on a very small dataset and still get meaningful results.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Materials and Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our goal for this work is to correctly detect a COVID-19 pulmonary disease in chest X-ray images on local university hospital study data. Therefore, we train a deep learning model on publicly available COVID-19 image data and evaluate the model based on our study data. We further enhance the amount of available training data by generating synthetic X-ray images. In this section we explain the origin and distribution of the data, as well as the deep learning model and training process.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this work we analyze chest X-ray images in posteroanterior (PA) and anteroposterior (AP) front view. Typically the AP view is encountered for cases where the patient is bedridden. Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Data ‣ 3 Materials and Methods ‣ Improving COVID-19 CXR Detection with Synthetic Data Augmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows two male patient example CXR images from our study data.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2112.07529/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Chest X-ray images with lungs marked in red. (Left) COVID-19 positive image, typical Ground-glass opacification marked in blue. (Right) COVID-19 negative image.</figcaption>
</figure>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Training data</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">We use two different training datasets, see Table <a href="#S3.T1" title="Table 1 ‣ 3.1.3 Testing data ‣ 3.1 Data ‣ 3 Materials and Methods ‣ Improving COVID-19 CXR Detection with Synthetic Data Augmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As a first step, we use the COVIDx V8b dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to train our model. This dataset is one of the biggest curated and publicly available COVID-19 CXR datasets. We use the training split of the dataset, which contains 13.794 COVID-19 <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">negative</span> and 2.158 COVID-19 <span id="S3.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">positive</span> frontal view X-ray images of 14.978 unique patients.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">In a second step we enhance this training data by using 20.000 additional synthetic CXR images that we generated based on our study data. With that, we can add 10.000 COVID-19 <span id="S3.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_italic">positive</span> and 10.000 COVID-19 <span id="S3.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_italic">negative</span> images to our existing COVIDx V8b training data. This synthetic data is used to further augment the training of the classification model and increase image diversity. A sample of the generated images has been reviewed by a radiologist to ensure that the model produces meaningful data.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Validation data</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">We validate the model by calculating loss metrics on the so called <em id="S3.SS1.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">test</em> split of the COVIDx V8B dataset. This dataset contains 200 COVID-19 positive and 200 COVID-19 negative images. We used this dataset to tune model parameters. This is to avoid overfitting our model to the testing data.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Testing data</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">The central data in this work comes from a single center retrospective study of the Universitätsklinikum Ulm. For this study 566 patients (average age 51.12y +/- 18.73y; range 23-82y, 315 women) of a single institution (11/2019-05/2020) were included. The data has been carefully reviewed and labeled by two radiologists after dedicated training into <em id="S3.SS1.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">COVID-19 positive</em> and <em id="S3.SS1.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">COVID-19 negative</em>. The senior radiologist (CK) has 8 years of experience in thoracic imaging. This resulted in 110 positive images and 223 negative images, as seen in Table <a href="#S3.T1" title="Table 1 ‣ 3.1.3 Testing data ‣ 3.1 Data ‣ 3 Materials and Methods ‣ Improving COVID-19 CXR Detection with Synthetic Data Augmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">This testing data is used as a holdout set for final model evaluation. With this method we make sure to avoid any patient overlap between the training and testing data. Furthermore, we get a high-quality estimation of the generalization capabilities of the model when evaluating on the testing data. This is because the testing images come from a different data source, which leads to a <em id="S3.SS1.SSS3.p2.1.1" class="ltx_emph ltx_font_italic">distributional shift</em>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Distribution of images for all datasets</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Split</span></th>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">COVID-19 positive</span></td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">COVID-19 negative</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">COVIDx V8B</th>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Training</th>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.158</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.794</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">COVIDx V8B + Synthetic</th>
<th id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">Training</th>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_right" style="padding-top:1.5pt;padding-bottom:1.5pt;">12.158</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_right" style="padding-top:1.5pt;padding-bottom:1.5pt;">23.794</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<th id="S3.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">COVIDx V8B</th>
<th id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">Validation</th>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_right" style="padding-top:1.5pt;padding-bottom:1.5pt;">200</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_right" style="padding-top:1.5pt;padding-bottom:1.5pt;">200</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<th id="S3.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">Uniklinik Ulm Study</th>
<th id="S3.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">Test</th>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">110</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">223</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Network Architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For classification we use the ResNet50 architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The network has been pretrained on the ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> database. We replace the final fully connected layer with a linear layer of two outputs, one for each class. To get the predictions we apply a softmax activation function. Since training was very stable, we did not use any additional dropout layers or regularization methods.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The generative model is based on a modified version of StyleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We specifically integrate differentiable augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> into our StyleGAN architecture. This is to prevent memorization of the training data and helps to stabilize the training process. This combined architecture enables us to generate meaningful synthetic images based on our small study dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Network Architecture ‣ 3 Materials and Methods ‣ Improving COVID-19 CXR Detection with Synthetic Data Augmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows one example image along a classification by the COVIDx+Synth model.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2112.07529/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="388" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example images for classification by the COVIDx+Synth model. (Left) Real COVID-19 positive image, correctly predicted as <span id="S3.F2.3.1" class="ltx_text ltx_font_italic">positive</span>. (Right) Synthetic COVID-19 positive image, correctly predicted as <span id="S3.F2.4.2" class="ltx_text ltx_font_italic">positive</span>.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training details</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.4" class="ltx_p">To train the ResNet classifier we use the Adam solver <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> with default parameters (<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><msub id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2.2" xref="S3.SS3.p1.1.m1.1.1.2.2.cmml">β</mi><mn id="S3.SS3.p1.1.m1.1.1.2.3" xref="S3.SS3.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><eq id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></eq><apply id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.2.3">1</cn></apply><cn type="float" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\beta_{1}=0.9</annotation></semantics></math> and <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\beta_{2}=0.999" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><msub id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.2.2.cmml">β</mi><mn id="S3.SS3.p1.2.m2.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><eq id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></eq><apply id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2.2">𝛽</ci><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.2.3">2</cn></apply><cn type="float" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\beta_{2}=0.999</annotation></semantics></math>) and a cross-entropy loss. We train the model using minibatches of size 16. We use an initial learning rate of <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mn id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><cn type="float" id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">0.001</annotation></semantics></math> and apply the One-cycle learning rate scheduler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> with a maximum learning rate of <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="0.006" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mn id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">0.006</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><cn type="float" id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">0.006</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">0.006</annotation></semantics></math>. We initially freeze all but the new last network layer for 5 epochs of training. After those 5 epochs all network parameters are trained for 30 additional epochs. The One-cycle learning rate scheduler is only applied after the initial freeze period. Increasing the amount of training showed no further improvement empirically.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">All images are being scaled down to <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">224</cn><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">224\times 224</annotation></semantics></math> and normalized with the mean and standard deviation of images in the COVIDx V8B dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> before feeding them into the network. During training, we augment the images with random horizontal flipping and random rotation (<math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\pm 5^{\circ}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mo id="S3.SS3.p2.2.m2.1.1a" xref="S3.SS3.p2.2.m2.1.1.cmml">±</mo><msup id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml"><mn id="S3.SS3.p2.2.m2.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.2.2.cmml">5</mn><mo id="S3.SS3.p2.2.m2.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.2.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">plus-or-minus</csymbol><apply id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.2.m2.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2.2">5</cn><compose id="S3.SS3.p2.2.m2.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\pm 5^{\circ}</annotation></semantics></math>).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Since we use two different training datasets (see Section <a href="#S3.SS1.SSS1" title="3.1.1 Training data ‣ 3.1 Data ‣ 3 Materials and Methods ‣ Improving COVID-19 CXR Detection with Synthetic Data Augmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>) we get two different models: <span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_italic">COVIDx</span> and <span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_italic">COVIDx+Synth</span>. Both classification models use the exact same hyperparameters and training procedures as described. This is to evaluate the effect of using the synthetic data and make the results comparable.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">For the StyleGAN generator, we train two different models: one for each class of COVID-19 positive and negative images. This is a simple method to ensure that we can generate a specific image class. For further details regarding the training process of the StyleGAN generator see Späte 2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To investigate our models in a quantitative manner, we computed the accuracy, as well as F1-score, precision and recall for each class on the validation and testing data. The metrics for the validation data are shown in Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Improving COVID-19 CXR Detection with Synthetic Data Augmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Both models perform quite well on the validation data with an accuracy of 96 % and 95.5 % respectively. The results are in line with Wang et al. 2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and their COVIDNet-CXR-2 model. Interestingly, the Covidx+Synth model falls behind the other models, despite having a lot more training data. This could be another indication of a distributional shift between the COVIDx dataset and the study data of the Universitätsklinikum Ulm.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The results for the testing data are also shown in Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Improving COVID-19 CXR Detection with Synthetic Data Augmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The table shows that a model trained on the COVIDx dataset can adapt quite well to the testing data, with an accuracy of 89.49 %. The model achieves a decent precision for COVID-19 cases (90.32 %), which is good since too many false positives would increase the burden for the healthcare system due to the need for additional PCR testing. With a rather low recall of 76.36 % the model does miss quite a lot of COVID-19 cases. This can be especially problematic in this sensitive medical setting, since false negatives lead to undetected cases of COVID-19.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">This drawback can be controlled by using additional synthetic data to train the model. Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Improving COVID-19 CXR Detection with Synthetic Data Augmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an increase in accuracy (92.49 %) and most evaluation metrics. Especially the improved recall of 95.45 % is very
desirable. This comes with the cost of a slight reduction in precision (-6.32 %). Based on those results, it can be seen that our models perform quite well, especially when incorporating the synthetic data, but there are still several areas for improvement.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation metrics for models COVIDx and COVIDx+Synth on validation data (with reported metrics from Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> for comparison) and on testing data.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">F1-Score</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">Recall</span></td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.2.2.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S4.T2.1.2.2.2" class="ltx_td" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">C19 pos.</td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">C19 neg.</td>
<td id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">C19 pos.</td>
<td id="S4.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">C19 neg.</td>
<td id="S4.T2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">C19 pos.</td>
<td id="S4.T2.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">C19 neg.</td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><em id="S4.T2.1.3.3.1.1" class="ltx_emph ltx_font_italic">Validation Data</em></th>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.3.3.5" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.3.3.6" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.3.3.7" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.3.3.8" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<th id="S4.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">COVIDx</th>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.4.4.2.1" class="ltx_text ltx_font_bold">0.9600</span></td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.4.4.3.1" class="ltx_text ltx_font_bold">0.9583</span></td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.4.4.4.1" class="ltx_text ltx_font_bold">0.9615</span></td>
<td id="S4.T2.1.4.4.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.4.4.5.1" class="ltx_text ltx_font_bold">1.0000</span></td>
<td id="S4.T2.1.4.4.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9259</td>
<td id="S4.T2.1.4.4.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9200</td>
<td id="S4.T2.1.4.4.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.4.4.8.1" class="ltx_text ltx_font_bold">1.0000</span></td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<th id="S4.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">COVIDx+Synth</th>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9550</td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9548</td>
<td id="S4.T2.1.5.5.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9552</td>
<td id="S4.T2.1.5.5.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9596</td>
<td id="S4.T2.1.5.5.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9505</td>
<td id="S4.T2.1.5.5.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9500</td>
<td id="S4.T2.1.5.5.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9600</td>
</tr>
<tr id="S4.T2.1.6.6" class="ltx_tr">
<th id="S4.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">COVIDNet-CXR-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T2.1.6.6.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="S4.T2.1.6.6.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="S4.T2.1.6.6.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="S4.T2.1.6.6.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9700</td>
<td id="S4.T2.1.6.6.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.6.6.6.1" class="ltx_text ltx_font_bold">0.9560</span></td>
<td id="S4.T2.1.6.6.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.6.6.7.1" class="ltx_text ltx_font_bold">0.9550</span></td>
<td id="S4.T2.1.6.6.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9700</td>
</tr>
<tr id="S4.T2.1.7.7" class="ltx_tr">
<th id="S4.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><em id="S4.T2.1.7.7.1.1" class="ltx_emph ltx_font_italic">Testing Data</em></th>
<td id="S4.T2.1.7.7.2" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.7.7.3" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.7.7.4" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.7.7.5" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.7.7.6" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.7.7.7" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T2.1.7.7.8" class="ltx_td ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S4.T2.1.8.8" class="ltx_tr">
<th id="S4.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">COVIDx</th>
<td id="S4.T2.1.8.8.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.8949</td>
<td id="S4.T2.1.8.8.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.8276</td>
<td id="S4.T2.1.8.8.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9244</td>
<td id="S4.T2.1.8.8.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.8.8.5.1" class="ltx_text ltx_font_bold">0.9032</span></td>
<td id="S4.T2.1.8.8.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.8917</td>
<td id="S4.T2.1.8.8.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.7636</td>
<td id="S4.T2.1.8.8.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.8.8.8.1" class="ltx_text ltx_font_bold">0.9596</span></td>
</tr>
<tr id="S4.T2.1.9.9" class="ltx_tr">
<th id="S4.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">COVIDx+Synth</th>
<td id="S4.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.9.9.2.1" class="ltx_text ltx_font_bold">0.9249</span></td>
<td id="S4.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.9.9.3.1" class="ltx_text ltx_font_bold">0.8936</span></td>
<td id="S4.T2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.9.9.4.1" class="ltx_text ltx_font_bold">0.9420</span></td>
<td id="S4.T2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.8400</td>
<td id="S4.T2.1.9.9.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.9.9.6.1" class="ltx_text ltx_font_bold">0.9760</span></td>
<td id="S4.T2.1.9.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.9.9.7.1" class="ltx_text ltx_font_bold">0.9545</span></td>
<td id="S4.T2.1.9.9.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.9103</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work we showed that a deep learning model trained with a comparatively large volume of publicly available data for COVID-19 detection is able to generalize well to single source, local hospital data with patient demographics and technical parameters independent of the training data. This is not without limitations, since the distributional shift between the training and testing data can lead to some undesirable results, especially for important metrics like low recall values.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We show that this can be improved by using synthetically generated data to augment the training data. Although this works quite well, one of the reasons could be a rebalancing effect, that could have been achieved with various resampling methods as well. Another reason could be a light form of <em id="S5.p2.1.1" class="ltx_emph ltx_font_italic">data leakage</em>, since the synthetic data was generated based on the testing data. This is not fully clear, since the StyleGAN generator has no direct access to the ground truth data and just learns based on the feedback of a discriminator. Despite these concerns, the model shows promising first results and using such a model in an emergency setting could give a fast estimation for the prevalence of pulmonary infiltrates and therefore improve clinical decision-making and resource allocation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Vogels, C.B., Brito, A.F., Wyllie, A.L., Fauver, J.R., Ott, I.M., Kalinich,
C.C., Petrone, M.E., Casanovas-Massana, A., Muenker, M.C., Moore, A.J.,
Klein, J., Lu, P., Lu-Culligan, A., Jiang, X., Kim, D.J., Kudo, E., Mao, T.,
Moriyama, M., Oh, J.E., Park, A., Silva, J., Song, E., Takahashi, T., Taura,
M., Tokuyama, M., Venkataraman, A., Weizman, O.E., Wong, P., Yang, Y.,
Cheemarla, N.R., White, E.B., Lapidus, S., Earnest, R., Geng, B.,
Vijayakumar, P., Odio, C., Fournier, J., Bermejo, S., Farhadian, S., Cruz,
C.S.D., Iwasaki, A., Ko, A.I., Landry, M.L., Foxman, E.F., Grubaugh, N.D.:

</span>
<span class="ltx_bibblock">Analytical sensitivity and efficiency comparisons of SARS-COV-2
qRT-PCR primer-probe sets.

</span>
<span class="ltx_bibblock">(apr 2020)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Udugama, B., Kadhiresan, P., Kozlowski, H.N., Malekjahani, A., Osborne, M., Li,
V.Y.C., Chen, H., Mubareka, S., Gubbay, J.B., Chan, W.C.W.:

</span>
<span class="ltx_bibblock">Diagnosing COVID-19: The disease and tools for detection.

</span>
<span class="ltx_bibblock">ACS Nano <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">14</span>(4) (mar 2020) 3822–3835

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yang, Y., Yang, M., Shen, C., Wang, F., Yuan, J., Li, J., Zhang, M., Wang, Z.,
Xing, L., Wei, J., Peng, L., Wong, G., Zheng, H., Wu, W., Liao, M., Feng, K.,
Li, J., Yang, Q., Zhao, J., Zhang, Z., Liu, L., Liu, Y.:

</span>
<span class="ltx_bibblock">Evaluating the accuracy of different respiratory specimens in the
laboratory diagnosis and monitoring the viral shedding of 2019-nCoV
infections.

</span>
<span class="ltx_bibblock">(feb 2020)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Arevalo-Rodriguez, I., Buitrago-Garcia, D., Simancas-Racines, D.,
Zambrano-Achig, P., Campo, R.D., Ciapponi, A., Sued, O.,
Martínez-García, L., Rutjes, A., Low, N., Bossuyt, P.M.,
Perez-Molina, J.A., Zamora, J.:

</span>
<span class="ltx_bibblock">FALSE-NEGATIVE RESULTS OF INITIAL RT-PCR ASSAYS FOR
COVID-19: A SYSTEMATIC REVIEW.

</span>
<span class="ltx_bibblock">(apr 2020)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Kong, W., Agarwal, P.P.:

</span>
<span class="ltx_bibblock">Chest imaging appearance of COVID-19 infection.

</span>
<span class="ltx_bibblock">Radiology: Cardiothoracic Imaging <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">2</span>(1) (feb 2020) e200028

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Rubin, G.D., Ryerson, C.J., Haramati, L.B., Sverzellati, N., Kanne, J.P.,
Raoof, S., Schluger, N.W., Volpi, A., Yim, J.J., Martin, I.B.K., Anderson,
D.J., Kong, C., Altes, T., Bush, A., Desai, S.R., onathan Goldin, Goo, J.M.,
Humbert, M., Inoue, Y., Kauczor, H.U., Luo, F., Mazzone, P.J., Prokop, M.,
Remy-Jardin, M., Richeldi, L., Schaefer-Prokop, C.M., Tomiyama, N., Wells,
A.U., Leung, A.N.:

</span>
<span class="ltx_bibblock">The role of chest imaging in patient management during the COVID-19
pandemic: A multinational consensus statement from the fleischner society.

</span>
<span class="ltx_bibblock">Radiology <span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">296</span>(1) (July 2020) 172–180

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Wang, L., Lin, Z.Q., Wong, A.:

</span>
<span class="ltx_bibblock">COVID-net: a tailored deep convolutional neural network design for
detection of COVID-19 cases from chest x-ray images.

</span>
<span class="ltx_bibblock">Scientific Reports <span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">10</span>(1) (November 2020)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Karras, T., Laine, S., Aila, T.:

</span>
<span class="ltx_bibblock">A style-based generator architecture for generative adversarial
networks.

</span>
<span class="ltx_bibblock">In: 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), IEEE (June 2019)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Khan, A.I., Shah, J.L., Bhat, M.M.:

</span>
<span class="ltx_bibblock">CoroNet: A deep neural network for detection and diagnosis of
COVID-19 from chest x-ray images.

</span>
<span class="ltx_bibblock">Computer Methods and Programs in Biomedicine <span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">196</span> (November
2020) 105581

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ucar, F., Korkmaz, D.:

</span>
<span class="ltx_bibblock">COVIDiagnosis-net: Deep bayes-SqueezeNet based diagnosis of the
coronavirus disease 2019 (COVID-19) from x-ray images.

</span>
<span class="ltx_bibblock">Medical Hypotheses <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">140</span> (July 2020) 109761

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Keidar, D., Yaron, D., Goldstein, E., Shachar, Y., Blass, A., Charbinsky, L.,
Aharony, I., Lifshitz, L., Lumelsky, D., Neeman, Z., Mizrachi, M., Hajouj,
M., Eizenbach, N., Sela, E., Weiss, C.S., Levin, P., Benjaminov, O., Bachar,
G.N., Tamir, S., Rapson, Y., Suhami, D., Atar, E., Dror, A.A., Bogot, N.R.,
Grubstein, A., Shabshin, N., Elyada, Y.M., Eldar, Y.C.:

</span>
<span class="ltx_bibblock">COVID-19 classification of x-ray images using deep neural networks.

</span>
<span class="ltx_bibblock">European Radiology (may 2021)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Shamout, F.E., Shen, Y., Wu, N., Kaku, A., Park, J., Makino, T.,
Jastrzębski, S., Witowski, J., Wang, D., Zhang, B., Dogra, S., Cao, M.,
Razavian, N., Kudlowitz, D., Azour, L., Moore, W., Lui, Y.W.,
Aphinyanaphongs, Y., Fernandez-Granda, C., Geras, K.J.:

</span>
<span class="ltx_bibblock">An artificial intelligence system for predicting the deterioration of
COVID-19 patients in the emergency department.

</span>
<span class="ltx_bibblock">npj Digital Medicine <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">4</span>(1) (may 2021)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Tartaglione, E., Barbano, C.A., Berzovini, C., Calandri, M., Grangetto, M.:

</span>
<span class="ltx_bibblock">Unveiling COVID-19 from CHEST x-ray with deep learning: A hurdles
race with small data.

</span>
<span class="ltx_bibblock">International Journal of Environmental Research and Public Health
<span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">17</span>(18) (September 2020) 6933

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Oakden-Rayner, L.:

</span>
<span class="ltx_bibblock">Exploring the chestxray14 dataset: problems (Dec 2017)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.:

</span>
<span class="ltx_bibblock">Generative adversarial nets.

</span>
<span class="ltx_bibblock">In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger,
K.Q., eds.: Advances in Neural Information Processing Systems. Volume 27.,
Curran Associates, Inc. (2014)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Frid-Adar, M., Diamant, I., Klang, E., Amitai, M., Goldberger, J., Greenspan,
H.:

</span>
<span class="ltx_bibblock">GAN-based synthetic medical image augmentation for increased CNN
performance in liver lesion classification.

</span>
<span class="ltx_bibblock">Neurocomputing <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">321</span> (dec 2018) 321–331

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yi, X., Walia, E., Babyn, P.:

</span>
<span class="ltx_bibblock">Generative adversarial network in medical imaging: A review.

</span>
<span class="ltx_bibblock">Medical Image Analysis <span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">58</span> (dec 2019) 101552

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kazeminia, S., Baur, C., Kuijper, A., van Ginneken, B., Navab, N., Albarqouni,
S., Mukhopadhyay, A.:

</span>
<span class="ltx_bibblock">GANs for medical image analysis.

</span>
<span class="ltx_bibblock">Artificial Intelligence in Medicine <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">109</span> (sep 2020) 101938

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Karbhari, Y., Basu, A., Geem, Z.W., Han, G.T., Sarkar, R.:

</span>
<span class="ltx_bibblock">Generation of synthetic chest x-ray images and detection of
COVID-19: A deep learning based approach.

</span>
<span class="ltx_bibblock">Diagnostics <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">11</span>(5) (may 2021) 895

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Motamed, S., Rogalla, P., Khalvati, F.:

</span>
<span class="ltx_bibblock">RANDGAN: Randomized generative adversarial network for detection of
COVID-19 in chest x-ray.

</span>
<span class="ltx_bibblock">Scientific Reports <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">11</span>(1) (apr 2021)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Zhao, S., Liu, Z., Lin, J., Zhu, J.Y., Han, S.:

</span>
<span class="ltx_bibblock">Differentiable augmentation for data-efficient gan training.

</span>
<span class="ltx_bibblock">In: Conference on Neural Information Processing Systems (NeurIPS).
(2020)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.:

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In: 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), IEEE (June 2016)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.:

</span>
<span class="ltx_bibblock">ImageNet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In: 2009 IEEE Conference on Computer Vision and Pattern
Recognition, IEEE (jun 2009)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Spaete, C.:

</span>
<span class="ltx_bibblock">Synthetic generation of medical images (unpublished master’s thesis).

</span>
<span class="ltx_bibblock">Master’s thesis, Technische Hochschule Ulm, Ulm (2021)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.:

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">(2014)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Smith, L.N., Topin, N.:

</span>
<span class="ltx_bibblock">Super-convergence: Very fast training of neural networks using large
learning rates.

</span>
<span class="ltx_bibblock">(2017)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.07528" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.07529" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.07529">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.07529" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.07530" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 20:03:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
