<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.03509] Fine-grained Intent Classification in the Legal Domain</title><meta property="og:description" content="A law practitioner has to go through a lot of long legal case proceedings. To understand the motivation behind the actions of different parties/individuals in a legal case, it is essential that the parts of the documen…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fine-grained Intent Classification in the Legal Domain">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fine-grained Intent Classification in the Legal Domain">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.03509">

<!--Generated on Mon Mar 11 14:30:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fine-grained Intent Classification in the Legal Domain</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ankan Mullick<span id="id3.1.id1" class="ltx_ERROR undefined">\equalcontrib</span><sup id="id4.2.id2" class="ltx_sup">1</sup>, Abhilash Nandy<span id="id5.3.id3" class="ltx_ERROR undefined">\equalcontrib</span><sup id="id6.4.id4" class="ltx_sup">1</sup>,
Manav Nitin Kapadnis<span id="id7.5.id5" class="ltx_ERROR undefined">\equalcontrib</span><sup id="id8.6.id6" class="ltx_sup">2</sup>,
Sohan Patnaik <sup id="id9.7.id7" class="ltx_sup">3</sup>,
R Raghav <sup id="id10.8.id8" class="ltx_sup">4</sup>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">A law practitioner has to go through a lot of long legal case proceedings. To understand the motivation behind the actions of different parties/individuals in a legal case, it is essential that the parts of the document that express an intent corresponding to the case be clearly understood. In this paper, we introduce a dataset of <math id="id1.1.m1.1" class="ltx_Math" alttext="93" display="inline"><semantics id="id1.1.m1.1a"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">93</mn><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn type="integer" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">93</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">93</annotation></semantics></math> legal documents, belonging to the case categories of either Murder, Land Dispute, Robbery, or Corruption, where phrases expressing intent same as the category of the document are annotated. Also, we annotate fine-grained intents for each such phrase to enable a deeper understanding of the case for a reader. Finally, we analyze the performance of several transformer-based models in automating the process of extracting intent phrases (both at a coarse and a fine-grained level), and classifying a document into one of the possible <math id="id2.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="id2.2.m2.1a"><mn id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><cn type="integer" id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">4</annotation></semantics></math> categories, and observe that, our dataset is challenging, especially in the case of fine-grained intent classification.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Documents which record legal case proceedings are often perused by many law practitioners. In any Court Judgement, these documents can contain as much as 4500 words (for example - Indian Supreme Court Judgements). Knowing the amount of intent in the text before hand will help a person understand the case better (intent here refers to the intention latent in a piece of text. e.g. ‘Mr. XYZ robbed a bank yesterday’ - in this sentence, the phrase ‘robbed a bank’ depicts the intent of Robbery).</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">There can be different levels of intent. For example, stating that a legal case deals with murder is a document level intent. It conveys a generalized information about the document. Sentence level and phrase level intents will give much more information about the document. To understand the documents much efficiently various summarization techniques exist. However, an analysis of intents conditioned on the legal cases, along with summarization, would improve the reader’s understanding and clarity of the content of the document significantly.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">We curate a dataset that consists of 93 legal documents, spread across four intents - Murder, Robbery, Land Dispute and Corruption. We manually annotate certain phrases which bring out the intent of the document. Additionally, we painstakingly assign fine-grained intents (referred to as ‘sub-intent’ interchangeably from here on) to each phrase. These intent phrases are annotated in a coarse (4 categories) as well as in a fine-grained manner (with several sub-intents in each category of intent). For example, under the intent of Robbery, ’Mr. ABC saw Mr. XYZ picking the lock of the neighbour’s house’ is an example of a witness. Another example is, ’Gold and silver ornaments missing’, indicating the stolen items.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">Another contribution is the analysis of different off-the-shelf models on intent based task. We finally present a proof-of-concept, which shows that coarse-grained document intent and document classification, as well as fine-grained annotation of phrases in legal documents, can be automated with reasonable accuracy.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Dataset Description</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.2" class="ltx_p"><math id="Sx2.p1.1.m1.1" class="ltx_Math" alttext="5000" display="inline"><semantics id="Sx2.p1.1.m1.1a"><mn id="Sx2.p1.1.m1.1.1" xref="Sx2.p1.1.m1.1.1.cmml">5000</mn><annotation-xml encoding="MathML-Content" id="Sx2.p1.1.m1.1b"><cn type="integer" id="Sx2.p1.1.m1.1.1.cmml" xref="Sx2.p1.1.m1.1.1">5000</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p1.1.m1.1c">5000</annotation></semantics></math> legal documents are scraped from CommonLII <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">http://www.commonlii.org/resources/221.html</span></span></span></span> using ‘selenium’ python package. <math id="Sx2.p1.2.m2.1" class="ltx_Math" alttext="93" display="inline"><semantics id="Sx2.p1.2.m2.1a"><mn id="Sx2.p1.2.m2.1.1" xref="Sx2.p1.2.m2.1.1.cmml">93</mn><annotation-xml encoding="MathML-Content" id="Sx2.p1.2.m2.1b"><cn type="integer" id="Sx2.p1.2.m2.1.1.cmml" xref="Sx2.p1.2.m2.1.1">93</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p1.2.m2.1c">93</annotation></semantics></math> documents belonging to the categories of Corruption, Murder, Land Dispute, and Robbery are randomly sampled from this larger set.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">Intent phrases are annotated for each document in the following manner -</p>
<ol id="Sx2.I1" class="ltx_enumerate">
<li id="Sx2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Sx2.I1.i1.p1" class="ltx_para">
<p id="Sx2.I1.i1.p1.1" class="ltx_p"><span id="Sx2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Initial filtering:</span> <math id="Sx2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="Sx2.I1.i1.p1.1.m1.1a"><mn id="Sx2.I1.i1.p1.1.m1.1.1" xref="Sx2.I1.i1.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="Sx2.I1.i1.p1.1.m1.1b"><cn type="integer" id="Sx2.I1.i1.p1.1.m1.1.1.cmml" xref="Sx2.I1.i1.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I1.i1.p1.1.m1.1c">2</annotation></semantics></math> annotators filter out sentences that convey an intent matching the category of the document at hand.</p>
</div>
</li>
<li id="Sx2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Sx2.I1.i2.p1" class="ltx_para">
<p id="Sx2.I1.i2.p1.2" class="ltx_p"><span id="Sx2.I1.i2.p1.2.1" class="ltx_text ltx_font_bold">Intent Phrase annotation</span> <math id="Sx2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="Sx2.I1.i2.p1.1.m1.1a"><mn id="Sx2.I1.i2.p1.1.m1.1.1" xref="Sx2.I1.i2.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="Sx2.I1.i2.p1.1.m1.1b"><cn type="integer" id="Sx2.I1.i2.p1.1.m1.1.1.cmml" xref="Sx2.I1.i2.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I1.i2.p1.1.m1.1c">2</annotation></semantics></math> other annotators then extract a span from each sentence, so as to exclude any details do not contribute to the intent (such as name of the person, date of incident etc.), and only include the words expressing corresponding intent. The resulting spans are the intent phrases. Inter-annotator agreement (Cohen <math id="Sx2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\kappa" display="inline"><semantics id="Sx2.I1.i2.p1.2.m2.1a"><mi id="Sx2.I1.i2.p1.2.m2.1.1" xref="Sx2.I1.i2.p1.2.m2.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="Sx2.I1.i2.p1.2.m2.1b"><ci id="Sx2.I1.i2.p1.2.m2.1.1.cmml" xref="Sx2.I1.i2.p1.2.m2.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I1.i2.p1.2.m2.1c">\kappa</annotation></semantics></math>) is 0.79.</p>
</div>
</li>
<li id="Sx2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Sx2.I1.i3.p1" class="ltx_para">
<p id="Sx2.I1.i3.p1.3" class="ltx_p"><span id="Sx2.I1.i3.p1.3.1" class="ltx_text ltx_font_bold">Sub-intent annotation</span>: <math id="Sx2.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="Sx2.I1.i3.p1.1.m1.1a"><mn id="Sx2.I1.i3.p1.1.m1.1.1" xref="Sx2.I1.i3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="Sx2.I1.i3.p1.1.m1.1b"><cn type="integer" id="Sx2.I1.i3.p1.1.m1.1.1.cmml" xref="Sx2.I1.i3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I1.i3.p1.1.m1.1c">1</annotation></semantics></math> annotator who is aware of legal terminology, is asked to go through the intent phrases of several documents from all the <math id="Sx2.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="Sx2.I1.i3.p1.2.m2.1a"><mn id="Sx2.I1.i3.p1.2.m2.1.1" xref="Sx2.I1.i3.p1.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="Sx2.I1.i3.p1.2.m2.1b"><cn type="integer" id="Sx2.I1.i3.p1.2.m2.1.1.cmml" xref="Sx2.I1.i3.p1.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I1.i3.p1.2.m2.1c">4</annotation></semantics></math> intent categories in order to come up with possible set of sub-intents for each intent category, that covers almost all aspects of that category. After coming up with the sets of sub-intents, <math id="Sx2.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="4" display="inline"><semantics id="Sx2.I1.i3.p1.3.m3.1a"><mn id="Sx2.I1.i3.p1.3.m3.1.1" xref="Sx2.I1.i3.p1.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="Sx2.I1.i3.p1.3.m3.1b"><cn type="integer" id="Sx2.I1.i3.p1.3.m3.1.1.cmml" xref="Sx2.I1.i3.p1.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I1.i3.p1.3.m3.1c">4</annotation></semantics></math> annotators are then shown some samples on how to annotate sub-intent for a given phrase. Then, the intent phrases are divided amongst these annotators, and the sub-intent of each intent phrase is annotated thereafter.</p>
</div>
</li>
</ol>
</div>
<div id="Sx2.p3" class="ltx_para">
<p id="Sx2.p3.4" class="ltx_p">Table <a href="#Sx2.T1" title="Table 1 ‣ Dataset Description ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the statistics of our dataset, describing the number of documents, average length of documents and intent phrases, and average sentiment score for each of the <math id="Sx2.p3.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="Sx2.p3.1.m1.1a"><mn id="Sx2.p3.1.m1.1.1" xref="Sx2.p3.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="Sx2.p3.1.m1.1b"><cn type="integer" id="Sx2.p3.1.m1.1.1.cmml" xref="Sx2.p3.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p3.1.m1.1c">4</annotation></semantics></math> intent categories. The documents on Corruption and Land Dispute are roughly longer than those on Murder and Robbery. Table <a href="#Sx2.T1" title="Table 1 ‣ Dataset Description ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> also shows average sentiment scores across annotated intent phrases (calculated using <em id="Sx2.p3.4.1" class="ltx_emph ltx_font_italic">sentifish</em> <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://pypi.org/project/sentifish/</span></span></span></span> Python Package) for each of the four categories. The sentiment scores of the categories follow the following order - Land Dispute <math id="Sx2.p3.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="Sx2.p3.2.m2.1a"><mo id="Sx2.p3.2.m2.1.1" xref="Sx2.p3.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Sx2.p3.2.m2.1b"><gt id="Sx2.p3.2.m2.1.1.cmml" xref="Sx2.p3.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p3.2.m2.1c">&gt;</annotation></semantics></math> Corruption <math id="Sx2.p3.3.m3.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="Sx2.p3.3.m3.1a"><mo id="Sx2.p3.3.m3.1.1" xref="Sx2.p3.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Sx2.p3.3.m3.1b"><gt id="Sx2.p3.3.m3.1.1.cmml" xref="Sx2.p3.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p3.3.m3.1c">&gt;</annotation></semantics></math> Robbery <math id="Sx2.p3.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="Sx2.p3.4.m4.1a"><mo id="Sx2.p3.4.m4.1.1" xref="Sx2.p3.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Sx2.p3.4.m4.1b"><gt id="Sx2.p3.4.m4.1.1.cmml" xref="Sx2.p3.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p3.4.m4.1c">&gt;</annotation></semantics></math> Murder, which follows common intuition.</p>
</div>
<figure id="Sx2.T1" class="ltx_table">
<div id="Sx2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:404.7pt;height:123.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.3pt,6.5pt) scale(0.904891948488998,0.904891948488998) ;">
<table id="Sx2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx2.T1.1.1.1.1" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="Sx2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="Sx2.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="Sx2.T1.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx2.T1.1.1.1.1.2.1.1" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">No. of</span></td>
</tr>
<tr id="Sx2.T1.1.1.1.1.2.1.2" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">documents</span></td>
</tr>
</table>
</td>
<td id="Sx2.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="Sx2.T1.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx2.T1.1.1.1.1.3.1.1" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Avg. no. of</span></td>
</tr>
<tr id="Sx2.T1.1.1.1.1.3.1.2" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">words/doc</span></td>
</tr>
</table>
</td>
<td id="Sx2.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="Sx2.T1.1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx2.T1.1.1.1.1.4.1.1" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Avg. no. of</span></td>
</tr>
<tr id="Sx2.T1.1.1.1.1.4.1.2" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">sentences/doc</span></td>
</tr>
</table>
</td>
<td id="Sx2.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="Sx2.T1.1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx2.T1.1.1.1.1.5.1.1" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Avg. length</span></td>
</tr>
<tr id="Sx2.T1.1.1.1.1.5.1.2" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">of intent</span></td>
</tr>
<tr id="Sx2.T1.1.1.1.1.5.1.3" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.5.1.3.1.1" class="ltx_text ltx_font_bold">phrase</span></td>
</tr>
</table>
</td>
<td id="Sx2.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="Sx2.T1.1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx2.T1.1.1.1.1.6.1.1" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Avg.</span></td>
</tr>
<tr id="Sx2.T1.1.1.1.1.6.1.2" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">Sentiment Score</span></td>
</tr>
<tr id="Sx2.T1.1.1.1.1.6.1.3" class="ltx_tr">
<td id="Sx2.T1.1.1.1.1.6.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx2.T1.1.1.1.1.6.1.3.1.1" class="ltx_text ltx_font_bold">of intent phrases</span></td>
</tr>
</table>
</td>
</tr>
<tr id="Sx2.T1.1.1.2.2" class="ltx_tr">
<td id="Sx2.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Corruption</td>
<td id="Sx2.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17</td>
<td id="Sx2.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4466</td>
<td id="Sx2.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">174</td>
<td id="Sx2.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17</td>
<td id="Sx2.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
</tr>
<tr id="Sx2.T1.1.1.3.3" class="ltx_tr">
<td id="Sx2.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">
<table id="Sx2.T1.1.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx2.T1.1.1.3.3.1.1.1" class="ltx_tr">
<td id="Sx2.T1.1.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Land Dispute</td>
</tr>
</table>
</td>
<td id="Sx2.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">25</td>
<td id="Sx2.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">4681</td>
<td id="Sx2.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">186</td>
<td id="Sx2.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r">19</td>
<td id="Sx2.T1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r">0.02</td>
</tr>
<tr id="Sx2.T1.1.1.4.4" class="ltx_tr">
<td id="Sx2.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Murder</td>
<td id="Sx2.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">30</td>
<td id="Sx2.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">2876</td>
<td id="Sx2.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">135</td>
<td id="Sx2.T1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">17</td>
<td id="Sx2.T1.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r">-0.012</td>
</tr>
<tr id="Sx2.T1.1.1.5.5" class="ltx_tr">
<td id="Sx2.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Robbery</td>
<td id="Sx2.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">21</td>
<td id="Sx2.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">2756</td>
<td id="Sx2.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">118</td>
<td id="Sx2.T1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">9</td>
<td id="Sx2.T1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-0.002</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Statistics for each category in the dataset. The numbers (other than the average sentiment score) are rounded to the nearest integer.</figcaption>
</figure>
<div id="Sx2.p4" class="ltx_para">
<p id="Sx2.p4.1" class="ltx_p">Fig. <a href="#Sx2.F1" title="Figure 1 ‣ Dataset Description ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the top <math id="Sx2.p4.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="Sx2.p4.1.m1.1a"><mn id="Sx2.p4.1.m1.1.1" xref="Sx2.p4.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="Sx2.p4.1.m1.1b"><cn type="integer" id="Sx2.p4.1.m1.1.1.cmml" xref="Sx2.p4.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p4.1.m1.1c">200</annotation></semantics></math> most frequent words (excluding stopwords) occurring in the intent phrases for each of the four categories, with the font size of the word being proportional to its frequency. In each wordcloud, we can observe that each category has words that match the corresponding intent (E.g. ’bribe’ in Corruption, ’property’ in Land Dispute etc.)</p>
</div>
<figure id="Sx2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="Sx2.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.03509/assets/images/corruption_wordcloud_inverted.png" id="Sx2.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="Sx2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.03509/assets/images/land_wordcloud_inverted.png" id="Sx2.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="Sx2.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.03509/assets/images/murder_wordcloud_inverted.png" id="Sx2.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="Sx2.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.03509/assets/images/robbery_wordcloud_inverted.png" id="Sx2.F1.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Wordclouds for each intent category, showing the <math id="Sx2.F1.2.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="Sx2.F1.2.m1.1b"><mn id="Sx2.F1.2.m1.1.1" xref="Sx2.F1.2.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="Sx2.F1.2.m1.1c"><cn type="integer" id="Sx2.F1.2.m1.1.1.cmml" xref="Sx2.F1.2.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.F1.2.m1.1d">200</annotation></semantics></math> most frequently occurring words in the intent phrases for the corresponding category</figcaption>
</figure>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiment and Results</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">This section is organized to describe the use of transformers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al. <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> for document classification, which will be followed by the explanation for the use of JointBERT <cite class="ltx_cite ltx_citemacro_citep">(Chen, Zhuo, and Wang <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> for intent as well as slot classification. We use two Tesla P100 GPUs with 16 GB RAM to perform all the experiments.</p>
</div>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Document Classification</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.1" class="ltx_p">Recent advancements show that, Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al. <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> based pre-trained language models like BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al. <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>, ALBERT <cite class="ltx_cite ltx_citemacro_citep">(Lan et al. <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>, and DeBERTa <cite class="ltx_cite ltx_citemacro_citep">(He et al. <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>, have proven to be very successful in learning robust context-based representations of lexicons and applying these to achieve state of the art performance on a variety of downstream tasks such as document classification in our case.</p>
</div>
<figure id="Sx3.T2" class="ltx_table">
<table id="Sx3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T2.1.1.1" class="ltx_tr">
<th id="Sx3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model Name</span></th>
<th id="Sx3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
<th id="Sx3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="Sx3.T2.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T2.1.1.1.3.1.1" class="ltx_tr">
<td id="Sx3.T2.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T2.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Macro</span></td>
</tr>
<tr id="Sx3.T2.1.1.1.3.1.2" class="ltx_tr">
<td id="Sx3.T2.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T2.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">F1-score</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T2.1.2.1" class="ltx_tr">
<td id="Sx3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">BERT</td>
<td id="Sx3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.63</td>
<td id="Sx3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.53</td>
</tr>
<tr id="Sx3.T2.1.3.2" class="ltx_tr">
<td id="Sx3.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">RoBERTa</td>
<td id="Sx3.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.74</td>
<td id="Sx3.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.64</td>
</tr>
<tr id="Sx3.T2.1.4.3" class="ltx_tr">
<td id="Sx3.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">ALBERT</td>
<td id="Sx3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.53</td>
<td id="Sx3.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.61</td>
</tr>
<tr id="Sx3.T2.1.5.4" class="ltx_tr">
<td id="Sx3.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">DeBERTa</td>
<td id="Sx3.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx3.T2.1.5.4.2.1" class="ltx_text ltx_font_bold">0.74</span></td>
<td id="Sx3.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx3.T2.1.5.4.3.1" class="ltx_text ltx_font_bold">0.71</span></td>
</tr>
<tr id="Sx3.T2.1.6.5" class="ltx_tr">
<td id="Sx3.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">LEGAL-BERT</td>
<td id="Sx3.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx3.T2.1.6.5.2.1" class="ltx_text ltx_font_bold">0.74</span></td>
<td id="Sx3.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">0.68</td>
</tr>
<tr id="Sx3.T2.1.7.6" class="ltx_tr">
<td id="Sx3.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">LEGAL-RoBERTa</td>
<td id="Sx3.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.68</td>
<td id="Sx3.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.69</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of Transformer Models</figcaption>
</figure>
<div id="Sx3.SSx1.p2" class="ltx_para">
<p id="Sx3.SSx1.p2.1" class="ltx_p">We then implemented different models mentioned in Table <a href="#Sx3.T2" title="Table 2 ‣ Document Classification ‣ Experiment and Results ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, for learning contextual representations of the documents whose outputs were then fed to a softmax layer to get the final predicted class of the document. Along with this, we also implemented a variant of LEGAL-BERT <cite class="ltx_cite ltx_citemacro_citep">(Chalkidis et al. <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> and LEGAL-RoBERTa <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://huggingface.co/saibo/legal-roberta-base</span></span></span> which were pre-trained on large scale datasets of legal domain-specific corpora which in turn led to much better scores than their counterparts pre-trained on general corpora.</p>
</div>
<div id="Sx3.SSx1.p3" class="ltx_para">
<p id="Sx3.SSx1.p3.1" class="ltx_p">Recent improvements to the state-of-the-art in contextual language models such as in the case of DeBERTa perform significantly better than BERT. The same is observed from Table <a href="#Sx3.T2" title="Table 2 ‣ Document Classification ‣ Experiment and Results ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> which shows that the Accuracy and Macro F1-score for DeBERTa came to be the highest among the other models, whereas LEGAL-BERT was at par with DeBERTa in terms of Accuracy score. Further, since DeBERTa is trained previously using the disentangled attention mechanism along with an enhanced mask decoder. The training method is same as that of BERT. Owing to the novel attention mechanism used in DeBERTa, it outperforms the other models in terms of both Accuracy and Macro F1-score.</p>
</div>
<div id="Sx3.SSx1.p4" class="ltx_para">
<p id="Sx3.SSx1.p4.1" class="ltx_p">LEGAL-BERT on the other hand is pre-trained and further fine-tuned on legal-domain specific corpora, which in turn lead to its state-of-the-art performance on various legal domain specific tasks. In our case, leveraging LEGAL-BERT outperforms other models since the contextual representation is more inclined towards legal matters.</p>
</div>
<div id="Sx3.SSx1.p5" class="ltx_para">
<p id="Sx3.SSx1.p5.1" class="ltx_p">All of the transformer models were implemented using sliding window attention <cite class="ltx_cite ltx_citemacro_citep">(Masood, Abbasi, and Wee Keong <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, since the document length for all the documents is greater than the transformer maximum token size. They were trained with a sliding window ratio of 20% over three epochs with learning rate and batch size set at 2e-5 and 32 respectively. The docuemnts in the dataset are randomly split into train, validation and test sets in the ratio of 6:2:2. Note that, when classifying fine-grained intents, we only consider those sub-intents that have atleast <math id="Sx3.SSx1.p5.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="Sx3.SSx1.p5.1.m1.1a"><mn id="Sx3.SSx1.p5.1.m1.1.1" xref="Sx3.SSx1.p5.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p5.1.m1.1b"><cn type="integer" id="Sx3.SSx1.p5.1.m1.1.1.cmml" xref="Sx3.SSx1.p5.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p5.1.m1.1c">50</annotation></semantics></math> corresponding phrases.</p>
</div>
<div id="Sx3.SSx1.p6" class="ltx_para">
<p id="Sx3.SSx1.p6.1" class="ltx_p">We report the Accuracy score and Macro average score for each of the model so as to get an intuition on how the state of art transformer-based architectures perform on document classification in the legal domain.</p>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">JointBERT</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.1" class="ltx_p">We implemented BERT for joint intent classification and slot filling <cite class="ltx_cite ltx_citemacro_citep">(Chen, Zhuo, and Wang <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> on our dataset. We also replaced the BERT backbone with other transformer-based models such as DistilBERT and ALBERT. Slot Filling is a sequence labelling task, where BIO Tags are for the classes of ‘Corruption’, Land Dispute’, ‘Robbery’ and ‘Murder’, and then the intent classification task for those classes. The dataset is prepared in the following manner -
Since there is a majority of ‘O’ Tags for the slot filling task, only sentences containing an intent phrase, the one before that, and the one after that are used for training to mitigate class imbalance.
Each token has an intent BIO tag and each sentence with an intent phrase has a target intent. We randomly selected 20% sample for testing, 20% for validation. Rest 60% samples were used for training.</p>
</div>
<div id="Sx3.SSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.p2.1" class="ltx_p">The models were trained over 10 epochs with a batch size of 16, at a learning rate of 2e-5. At each epoch checkpoint, the model was saved and the model with the highest validation accuracy was picked to evaluate on the test set. As can be seen from Table <a href="#Sx3.T3" title="Table 3 ‣ JointBERT ‣ Experiment and Results ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, BERT proved to be the best model with an Intent Accuracy as well as Intent Macro F1-score of 0.9.</p>
</div>
<figure id="Sx3.T3" class="ltx_table">
<table id="Sx3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T3.1.1.1" class="ltx_tr">
<th id="Sx3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Model Name</span></th>
<th id="Sx3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="Sx3.T3.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T3.1.1.1.2.1.1" class="ltx_tr">
<td id="Sx3.T3.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T3.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Intent</span></td>
</tr>
<tr id="Sx3.T3.1.1.1.2.1.2" class="ltx_tr">
<td id="Sx3.T3.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T3.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
</table>
</th>
<th id="Sx3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="Sx3.T3.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T3.1.1.1.3.1.1" class="ltx_tr">
<td id="Sx3.T3.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T3.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Intent</span></td>
</tr>
<tr id="Sx3.T3.1.1.1.3.1.2" class="ltx_tr">
<td id="Sx3.T3.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T3.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">Macro</span></td>
</tr>
<tr id="Sx3.T3.1.1.1.3.1.3" class="ltx_tr">
<td id="Sx3.T3.1.1.1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T3.1.1.1.3.1.3.1.1" class="ltx_text ltx_font_bold">F1-score</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T3.1.2.1" class="ltx_tr">
<th id="Sx3.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">BERT</th>
<td id="Sx3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T3.1.2.1.2.1" class="ltx_text ltx_font_bold">0.90</span></td>
<td id="Sx3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T3.1.2.1.3.1" class="ltx_text ltx_font_bold">0.90</span></td>
</tr>
<tr id="Sx3.T3.1.3.2" class="ltx_tr">
<th id="Sx3.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">DistilBERT</th>
<td id="Sx3.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.90</td>
<td id="Sx3.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.89</td>
</tr>
<tr id="Sx3.T3.1.4.3" class="ltx_tr">
<th id="Sx3.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">ALBERT</th>
<td id="Sx3.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.88</td>
<td id="Sx3.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.87</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results on Intent classification</figcaption>
</figure>
<div id="Sx3.SSx2.p3" class="ltx_para">
<p id="Sx3.SSx2.p3.1" class="ltx_p">Table <a href="#Sx3.T4" title="Table 4 ‣ JointBERT ‣ Experiment and Results ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives the evaluation metric scores for each intent separately and the analysis provides evidence that the transformer-based models perform poorly on Corruption intent due to the number of ocuments in that category being the lowest, whereas they perform significantly better on other intents.</p>
</div>
<figure id="Sx3.T4" class="ltx_table">
<div id="Sx3.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:177pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.5pt,-34.5pt) scale(1.63904852117066,1.63904852117066) ;">
<table id="Sx3.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T4.1.1.1.1" class="ltx_tr">
<th id="Sx3.T4.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="Sx3.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="Sx3.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="Sx3.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T4.1.1.1.1.4.1" class="ltx_text ltx_font_bold">F1-score</span></th>
<th id="Sx3.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T4.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Support</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T4.1.1.2.1" class="ltx_tr">
<td id="Sx3.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Corruption</span></td>
<td id="Sx3.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.75</td>
<td id="Sx3.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.89</td>
<td id="Sx3.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.81</td>
<td id="Sx3.T4.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27</td>
</tr>
<tr id="Sx3.T4.1.1.3.2" class="ltx_tr">
<td id="Sx3.T4.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T4.1.1.3.2.1.1" class="ltx_text ltx_font_bold">Land Dispute</span></td>
<td id="Sx3.T4.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.95</td>
<td id="Sx3.T4.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.88</td>
<td id="Sx3.T4.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.91</td>
<td id="Sx3.T4.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">42</td>
</tr>
<tr id="Sx3.T4.1.1.4.3" class="ltx_tr">
<td id="Sx3.T4.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T4.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Murder</span></td>
<td id="Sx3.T4.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.94</td>
<td id="Sx3.T4.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.94</td>
<td id="Sx3.T4.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.94</td>
<td id="Sx3.T4.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">50</td>
</tr>
<tr id="Sx3.T4.1.1.5.4" class="ltx_tr">
<td id="Sx3.T4.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T4.1.1.5.4.1.1" class="ltx_text ltx_font_bold">Robbery</span></td>
<td id="Sx3.T4.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.96</td>
<td id="Sx3.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.89</td>
<td id="Sx3.T4.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.92</td>
<td id="Sx3.T4.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">27</td>
</tr>
<tr id="Sx3.T4.1.1.6.5" class="ltx_tr">
<td id="Sx3.T4.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span id="Sx3.T4.1.1.6.5.1.1" class="ltx_text ltx_font_bold">Macro Average</span></td>
<td id="Sx3.T4.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.90</td>
<td id="Sx3.T4.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.90</td>
<td id="Sx3.T4.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.90</td>
<td id="Sx3.T4.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">146</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results of Joint BERT on Intent Classification </figcaption>
</figure>
<div id="Sx3.SSx2.p4" class="ltx_para">
<p id="Sx3.SSx2.p4.1" class="ltx_p">Table <a href="#Sx3.T5" title="Table 5 ‣ JointBERT ‣ Experiment and Results ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> enumerates the results of Joint BERT on the task of Slot Classification. The model performs best on Murder intent when compared with others, which is again due to the number of samples in the Murder category being the largest.</p>
</div>
<figure id="Sx3.T5" class="ltx_table">
<div id="Sx3.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:177pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.5pt,-34.5pt) scale(1.63904852117066,1.63904852117066) ;">
<table id="Sx3.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T5.1.1.1.1" class="ltx_tr">
<th id="Sx3.T5.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="Sx3.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T5.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="Sx3.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T5.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="Sx3.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T5.1.1.1.1.4.1" class="ltx_text ltx_font_bold">F1-score</span></th>
<th id="Sx3.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T5.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Support</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T5.1.1.2.1" class="ltx_tr">
<td id="Sx3.T5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Corruption</span></td>
<td id="Sx3.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.74</td>
<td id="Sx3.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.38</td>
<td id="Sx3.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.51</td>
<td id="Sx3.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">326</td>
</tr>
<tr id="Sx3.T5.1.1.3.2" class="ltx_tr">
<td id="Sx3.T5.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T5.1.1.3.2.1.1" class="ltx_text ltx_font_bold">Land Dispute</span></td>
<td id="Sx3.T5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.71</td>
<td id="Sx3.T5.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.55</td>
<td id="Sx3.T5.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.62</td>
<td id="Sx3.T5.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">317</td>
</tr>
<tr id="Sx3.T5.1.1.4.3" class="ltx_tr">
<td id="Sx3.T5.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T5.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Murder</span></td>
<td id="Sx3.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.80</td>
<td id="Sx3.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.63</td>
<td id="Sx3.T5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.70</td>
<td id="Sx3.T5.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">361</td>
</tr>
<tr id="Sx3.T5.1.1.5.4" class="ltx_tr">
<td id="Sx3.T5.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T5.1.1.5.4.1.1" class="ltx_text ltx_font_bold">Robbery</span></td>
<td id="Sx3.T5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.66</td>
<td id="Sx3.T5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.53</td>
<td id="Sx3.T5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.59</td>
<td id="Sx3.T5.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">137</td>
</tr>
<tr id="Sx3.T5.1.1.6.5" class="ltx_tr">
<td id="Sx3.T5.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span id="Sx3.T5.1.1.6.5.1.1" class="ltx_text ltx_font_bold">Macro Average</span></td>
<td id="Sx3.T5.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.73</td>
<td id="Sx3.T5.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.52</td>
<td id="Sx3.T5.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.60</td>
<td id="Sx3.T5.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">1041</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Results of Joint BERT on Slot Classification </figcaption>
</figure>
<div id="Sx3.SSx2.p5" class="ltx_para">
<p id="Sx3.SSx2.p5.1" class="ltx_p">Table <a href="#Sx3.T6" title="Table 6 ‣ JointBERT ‣ Experiment and Results ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> provides the classification accuracy and Intent Macro F1-score on fine grained Intent Classification task. As the intent becomes more specific, the scores drop significantly, showing that the models are unable to capture the in-depth context of the intent phrases. However, modle with the BERT backbone still performs the best. This can be attributed to the fact, that BERT has the highest number of parameters ( 110 million) as compared to ALBERT ( 31 million), and DistilBERT ( 50 million).</p>
</div>
<figure id="Sx3.T6" class="ltx_table">
<table id="Sx3.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T6.1.1.1" class="ltx_tr">
<th id="Sx3.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Model Name</span></th>
<th id="Sx3.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="Sx3.T6.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T6.1.1.1.2.1.1" class="ltx_tr">
<td id="Sx3.T6.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T6.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Intent</span></td>
</tr>
<tr id="Sx3.T6.1.1.1.2.1.2" class="ltx_tr">
<td id="Sx3.T6.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T6.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
</table>
</th>
<th id="Sx3.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="Sx3.T6.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T6.1.1.1.3.1.1" class="ltx_tr">
<td id="Sx3.T6.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T6.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Intent</span></td>
</tr>
<tr id="Sx3.T6.1.1.1.3.1.2" class="ltx_tr">
<td id="Sx3.T6.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T6.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">Macro</span></td>
</tr>
<tr id="Sx3.T6.1.1.1.3.1.3" class="ltx_tr">
<td id="Sx3.T6.1.1.1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T6.1.1.1.3.1.3.1.1" class="ltx_text ltx_font_bold">F1-score</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T6.1.2.1" class="ltx_tr">
<th id="Sx3.T6.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">BERT</th>
<td id="Sx3.T6.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T6.1.2.1.2.1" class="ltx_text ltx_font_bold">0.53</span></td>
<td id="Sx3.T6.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx3.T6.1.2.1.3.1" class="ltx_text ltx_font_bold">0.50</span></td>
</tr>
<tr id="Sx3.T6.1.3.2" class="ltx_tr">
<th id="Sx3.T6.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">DistilBERT</th>
<td id="Sx3.T6.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.46</td>
<td id="Sx3.T6.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.40</td>
</tr>
<tr id="Sx3.T6.1.4.3" class="ltx_tr">
<th id="Sx3.T6.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">ALBERT</th>
<td id="Sx3.T6.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.48</td>
<td id="Sx3.T6.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.47</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results on fine-grained Intent Classification </figcaption>
</figure>
<div id="Sx3.SSx2.p6" class="ltx_para">
<p id="Sx3.SSx2.p6.5" class="ltx_p">Table <a href="#Sx3.T7" title="Table 7 ‣ JointBERT ‣ Experiment and Results ‣ Fine-grained Intent Classification in the Legal Domain" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> provides the precision, recall and macro F1 Score for fine-grained intent classification for the best performing model among the three models, i.e., JointBERT with a BERT Backbone. The labels are presented in the form of <math id="Sx3.SSx2.p6.1.m1.1" class="ltx_Math" alttext="X\_Y" display="inline"><semantics id="Sx3.SSx2.p6.1.m1.1a"><mrow id="Sx3.SSx2.p6.1.m1.1.1" xref="Sx3.SSx2.p6.1.m1.1.1.cmml"><mi id="Sx3.SSx2.p6.1.m1.1.1.2" xref="Sx3.SSx2.p6.1.m1.1.1.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p6.1.m1.1.1.1" xref="Sx3.SSx2.p6.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="Sx3.SSx2.p6.1.m1.1.1.3" xref="Sx3.SSx2.p6.1.m1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p6.1.m1.1.1.1a" xref="Sx3.SSx2.p6.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.SSx2.p6.1.m1.1.1.4" xref="Sx3.SSx2.p6.1.m1.1.1.4.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p6.1.m1.1b"><apply id="Sx3.SSx2.p6.1.m1.1.1.cmml" xref="Sx3.SSx2.p6.1.m1.1.1"><times id="Sx3.SSx2.p6.1.m1.1.1.1.cmml" xref="Sx3.SSx2.p6.1.m1.1.1.1"></times><ci id="Sx3.SSx2.p6.1.m1.1.1.2.cmml" xref="Sx3.SSx2.p6.1.m1.1.1.2">𝑋</ci><ci id="Sx3.SSx2.p6.1.m1.1.1.3.cmml" xref="Sx3.SSx2.p6.1.m1.1.1.3">_</ci><ci id="Sx3.SSx2.p6.1.m1.1.1.4.cmml" xref="Sx3.SSx2.p6.1.m1.1.1.4">𝑌</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p6.1.m1.1c">X\_Y</annotation></semantics></math>, where <math id="Sx3.SSx2.p6.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="Sx3.SSx2.p6.2.m2.1a"><mi id="Sx3.SSx2.p6.2.m2.1.1" xref="Sx3.SSx2.p6.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p6.2.m2.1b"><ci id="Sx3.SSx2.p6.2.m2.1.1.cmml" xref="Sx3.SSx2.p6.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p6.2.m2.1c">X</annotation></semantics></math> is an intent (e.g. Robbery), and <math id="Sx3.SSx2.p6.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="Sx3.SSx2.p6.3.m3.1a"><mi id="Sx3.SSx2.p6.3.m3.1.1" xref="Sx3.SSx2.p6.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p6.3.m3.1b"><ci id="Sx3.SSx2.p6.3.m3.1.1.cmml" xref="Sx3.SSx2.p6.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p6.3.m3.1c">Y</annotation></semantics></math> is a fine-grained intent/sub-intent (e.g. action). We observe that, even though the number of training samples per fine-grained class is quite low, performance on the test set is quite good - The F1-Score for all classes is above <math id="Sx3.SSx2.p6.4.m4.1" class="ltx_Math" alttext="0.4" display="inline"><semantics id="Sx3.SSx2.p6.4.m4.1a"><mn id="Sx3.SSx2.p6.4.m4.1.1" xref="Sx3.SSx2.p6.4.m4.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p6.4.m4.1b"><cn type="float" id="Sx3.SSx2.p6.4.m4.1.1.cmml" xref="Sx3.SSx2.p6.4.m4.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p6.4.m4.1c">0.4</annotation></semantics></math>, and except for two classes, it is above the halfway mark of <math id="Sx3.SSx2.p6.5.m5.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="Sx3.SSx2.p6.5.m5.1a"><mn id="Sx3.SSx2.p6.5.m5.1.1" xref="Sx3.SSx2.p6.5.m5.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p6.5.m5.1b"><cn type="float" id="Sx3.SSx2.p6.5.m5.1.1.cmml" xref="Sx3.SSx2.p6.5.m5.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p6.5.m5.1c">0.5</annotation></semantics></math>.</p>
</div>
<figure id="Sx3.T7" class="ltx_table">
<div id="Sx3.T7.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:245.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(58.1pt,-32.9pt) scale(1.36608876443654,1.36608876443654) ;">
<table id="Sx3.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T7.1.1.1.1" class="ltx_tr">
<th id="Sx3.T7.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="Sx3.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T7.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="Sx3.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T7.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="Sx3.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T7.1.1.1.1.4.1" class="ltx_text ltx_font_bold">F1-score</span></th>
<th id="Sx3.T7.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T7.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Support</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T7.1.1.2.1" class="ltx_tr">
<td id="Sx3.T7.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T7.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Corruption_action</span></td>
<td id="Sx3.T7.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.46</td>
<td id="Sx3.T7.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.60</td>
<td id="Sx3.T7.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.52</td>
<td id="Sx3.T7.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
</tr>
<tr id="Sx3.T7.1.1.3.2" class="ltx_tr">
<td id="Sx3.T7.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T7.1.1.3.2.1.1" class="ltx_text ltx_font_bold">Land_Dispute_action</span></td>
<td id="Sx3.T7.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.54</td>
<td id="Sx3.T7.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.70</td>
<td id="Sx3.T7.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.61</td>
<td id="Sx3.T7.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">20</td>
</tr>
<tr id="Sx3.T7.1.1.4.3" class="ltx_tr">
<td id="Sx3.T7.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T7.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Land_Dispute_description</span></td>
<td id="Sx3.T7.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.60</td>
<td id="Sx3.T7.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.35</td>
<td id="Sx3.T7.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.44</td>
<td id="Sx3.T7.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">17</td>
</tr>
<tr id="Sx3.T7.1.1.5.4" class="ltx_tr">
<td id="Sx3.T7.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T7.1.1.5.4.1.1" class="ltx_text ltx_font_bold">Murder_action</span></td>
<td id="Sx3.T7.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.57</td>
<td id="Sx3.T7.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.48</td>
<td id="Sx3.T7.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.52</td>
<td id="Sx3.T7.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">25</td>
</tr>
<tr id="Sx3.T7.1.1.6.5" class="ltx_tr">
<td id="Sx3.T7.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T7.1.1.6.5.1.1" class="ltx_text ltx_font_bold">Murder_description</span></td>
<td id="Sx3.T7.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">0.44</td>
<td id="Sx3.T7.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">0.71</td>
<td id="Sx3.T7.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">0.54</td>
<td id="Sx3.T7.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">24</td>
</tr>
<tr id="Sx3.T7.1.1.7.6" class="ltx_tr">
<td id="Sx3.T7.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T7.1.1.7.6.1.1" class="ltx_text ltx_font_bold">Murder_evidence</span></td>
<td id="Sx3.T7.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">0.38</td>
<td id="Sx3.T7.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">0.23</td>
<td id="Sx3.T7.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">0.29</td>
<td id="Sx3.T7.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r">13</td>
</tr>
<tr id="Sx3.T7.1.1.8.7" class="ltx_tr">
<td id="Sx3.T7.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T7.1.1.8.7.1.1" class="ltx_text ltx_font_bold">Robbery_action</span></td>
<td id="Sx3.T7.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">0.71</td>
<td id="Sx3.T7.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">0.63</td>
<td id="Sx3.T7.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">0.67</td>
<td id="Sx3.T7.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r">19</td>
</tr>
<tr id="Sx3.T7.1.1.9.8" class="ltx_tr">
<td id="Sx3.T7.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx3.T7.1.1.9.8.1.1" class="ltx_text ltx_font_bold">Robbery_description</span></td>
<td id="Sx3.T7.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r">0.67</td>
<td id="Sx3.T7.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">0.33</td>
<td id="Sx3.T7.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r">0.44</td>
<td id="Sx3.T7.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r">12</td>
</tr>
<tr id="Sx3.T7.1.1.10.9" class="ltx_tr">
<td id="Sx3.T7.1.1.10.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span id="Sx3.T7.1.1.10.9.1.1" class="ltx_text ltx_font_bold">Macro Average</span></td>
<td id="Sx3.T7.1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.54</td>
<td id="Sx3.T7.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.50</td>
<td id="Sx3.T7.1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.50</td>
<td id="Sx3.T7.1.1.10.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">140</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results of Joint BERT on fine-grained Intent Classification </figcaption>
</figure>
<div id="Sx3.SSx2.p7" class="ltx_para">
<p id="Sx3.SSx2.p7.1" class="ltx_p">Note that we have not reported the slot classification results for the fine-grained intents. This is because the number of labels becomes almost twice in this case as compared to intent classification (due to the presence of both B and I tags corresponding to each fine-grained intent, and an O class additionally, as we consider BIO tags for annotation). Hence, the number of samples per class is insufficient to learn a good slot classifier.</p>
</div>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Discussion</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">We observe that, although transformer-based models are performing well in the case of document classification and coarse-grained intent classification, there is a need for better performance in the fine-grained intent classification case. Hence, we argue that our dataset could be a crucial starting point for research on fine-grained intent classification in the legal domain.</p>
</div>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">This paper presents a new dataset for coarse and fine-grained annotation, as well as, shows a proof-of-concept as to how document as well as intent classification can be automated with reasonably good results. We use different transformer-based models for document classification, and observe that DeBERTa performs the best. We try transformer-based models such as BERT, ALBERT and DistilBERT as the backbones of a joint intent and slot classification neural network, and observe that, BERT performs the best among all the three, both in coarse as well as fine-grained intent classification. However, our dataset is challenging, as there is a lot of scope of improvement in the results, especially in fine-grained intent classification. Hence, our dataset could serve as a crucial benchmark for fine-grained intent classification in the legal domain.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chalkidis et al. (2020)</span>
<span class="ltx_bibblock">
Chalkidis, I.; Fergadiotis, M.; Malakasiotis, P.; Aletras, N.; and
Androutsopoulos, I. 2020.

</span>
<span class="ltx_bibblock">LEGAL-BERT: The Muppets straight out of Law School.

</span>
<span class="ltx_bibblock">arXiv:2010.02559.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen, Zhuo, and Wang (2019)</span>
<span class="ltx_bibblock">
Chen, Q.; Zhuo, Z.; and Wang, W. 2019.

</span>
<span class="ltx_bibblock">BERT for Joint Intent Classification and Slot Filling.

</span>
<span class="ltx_bibblock">arXiv:1902.10909.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding.

</span>
<span class="ltx_bibblock">arXiv:1810.04805.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2021)</span>
<span class="ltx_bibblock">
He, P.; Liu, X.; Gao, J.; and Chen, W. 2021.

</span>
<span class="ltx_bibblock">DeBERTa: Decoding-enhanced BERT with Disentangled Attention.

</span>
<span class="ltx_bibblock">arXiv:2006.03654.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. (2020)</span>
<span class="ltx_bibblock">
Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Soricut, R. 2020.

</span>
<span class="ltx_bibblock">ALBERT: A Lite BERT for Self-supervised Learning of Language
Representations.

</span>
<span class="ltx_bibblock">arXiv:1909.11942.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.;
Zettlemoyer, L.; and Stoyanov, V. 2019.

</span>
<span class="ltx_bibblock">RoBERTa: A Robustly Optimized BERT Pretraining Approach.

</span>
<span class="ltx_bibblock">arXiv:1907.11692.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masood, Abbasi, and Wee Keong (2020)</span>
<span class="ltx_bibblock">
Masood, M. A.; Abbasi, R. A.; and Wee Keong, N. 2020.

</span>
<span class="ltx_bibblock">Context-Aware Sliding Window for Sentiment Classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 8: 4870–4884.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.;
Kaiser, L.; and Polosukhin, I. 2017.

</span>
<span class="ltx_bibblock">Attention Is All You Need.

</span>
<span class="ltx_bibblock">arXiv:1706.03762.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.03507" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.03509" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.03509">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.03509" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.03510" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 14:30:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
