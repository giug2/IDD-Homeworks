<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.20756] SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models</title><meta property="og:description" content="Recently, with the rise of web images, managing and understanding large-scale image datasets has become increasingly important. Vision Large Language Models (VLLMs) have recently emerged due to their robust vision-unde…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.20756">

<!--Generated on Mon Aug  5 19:18:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zheng Liu<sup id="id17.17.id1" class="ltx_sup"><span id="id17.17.id1.1" class="ltx_text ltx_font_italic">†♠</span></sup>, Hao Liang<sup id="id18.18.id2" class="ltx_sup"><span id="id18.18.id2.1" class="ltx_text ltx_font_italic">†♠</span></sup>, Xijie Huang<sup id="id19.19.id3" class="ltx_sup"><span id="id19.19.id3.1" class="ltx_text ltx_font_italic">♠</span></sup>, Wentao Xiong<sup id="id20.20.id4" class="ltx_sup"><span id="id20.20.id4.1" class="ltx_text ltx_font_italic">♠</span></sup>, Qinhan Yu<sup id="id21.21.id5" class="ltx_sup"><span id="id21.21.id5.1" class="ltx_text ltx_font_italic">♠</span></sup>, Linzhuang Sun<sup id="id22.22.id6" class="ltx_sup">♢</sup>, Chong Chen<sup id="id23.23.id7" class="ltx_sup">♡</sup>, Conghui He<sup id="id24.24.id8" class="ltx_sup">♣</sup>, Bin Cui<sup id="id25.25.id9" class="ltx_sup">♠</sup>, Wentao Zhang<sup id="id26.26.id10" class="ltx_sup">♠</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id27.27.id1" class="ltx_sup">♠</sup>Peking University     <sup id="id28.28.id2" class="ltx_sup">♡</sup>Huawei Cloud BU     <sup id="id29.29.id3" class="ltx_sup">♣</sup>Shanghai AI Laboratory     <sup id="id30.30.id4" class="ltx_sup">♢</sup>University of Chinese Academy of Sciences

</span>
<span class="ltx_contact ltx_role_affiliation"><sup id="id31.31.id1" class="ltx_sup">†</sup>lz030515123@gmail.com, <sup id="id32.32.id2" class="ltx_sup">†</sup>hao.liang@stu.pku.edu.cn, {bin.cui, wentao.zhang}@pku.edu.cn

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id33.id1" class="ltx_p">Recently, with the rise of web images, managing and understanding large-scale image datasets has become increasingly important. Vision Large Language Models (VLLMs) have recently emerged due to their robust vision-understanding capabilities. However, training these models requires vast amounts of data, posing challenges to efficiency, effectiveness, data quality, and privacy. In this paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs. Unlike existing methods that generate captions from images, SynthVLM employs advanced diffusion models and high-quality captions to automatically generate and select high-resolution images from captions, creating precisely aligned image-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA) performance on various vision question answering tasks, maintaining high alignment quality and preserving advanced language abilities. Moreover, SynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in performance while significantly reducing computational overhead. Crucially, our method’s reliance on purely generated data ensures the preservation of privacy, achieving SoTA performance with just 100K data points (only 18% of the official dataset size). The code is made available at <a target="_blank" href="https://github.com/starriver030515/SynthVLM" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/starriver030515/SynthVLM</a>.</p>
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><math id="footnote1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="footnote1.m1.1b"><mo id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">\dagger</annotation></semantics></math> The first two authors have equal contributions. 
<br class="ltx_break"><math id="footnote1.m2.1" class="ltx_Math" alttext="*" display="inline"><semantics id="footnote1.m2.1b"><mo id="footnote1.m2.1.1" xref="footnote1.m2.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="footnote1.m2.1c"><times id="footnote1.m2.1.1.cmml" xref="footnote1.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m2.1d">*</annotation></semantics></math> Corresponding Author
</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, with the rapid advancements in large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib40" title="" class="ltx_ref">2023a</a>; Touvron et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite> and multimodal large language models (MLLMs) <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2023</a>; Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>, data management has become a crucial aspect of these technologies <cite class="ltx_cite ltx_citemacro_citep">(Fernandez et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>; Trummer, <a href="#bib.bib49" title="" class="ltx_ref">2023</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2023a</a>; Miao et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>; Nie et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite>. At the same time, <cite class="ltx_cite ltx_citemacro_citet">Bai et al<span class="ltx_text">.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2024</a>)</cite> also demonstrates that data processing, selection, and management can significantly influence the performance of MLLMs.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Among MLLMs, VLLMs achieve competitive performance in traditional multimodal tasks such as image classification <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2024</a>)</cite>, image understanding <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib21" title="" class="ltx_ref">c</a>)</cite>, and image captioning <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>. Moreover, their excellent language understanding capabilities enable strong performance in text-rich tasks, such as vision question-answering <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib27" title="" class="ltx_ref">a</a>)</cite> and image-text retrieval <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While most existing VLLMs focus on modifying model architecture to utilize information from multiple modalities <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib21" title="" class="ltx_ref">c</a>; Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2023</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib27" title="" class="ltx_ref">a</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2024</a>)</cite>, data also significantly impacts the success of VLLMs. For instance, <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2023d</a>); Wang et al<span class="ltx_text">.</span> (<a href="#bib.bib52" title="" class="ltx_ref">2024a</a>)</cite> demonstrates that higher-quality training data can enhance the performance of VLLMs. The key to ensuring high-quality data lies in the precise alignment between multimodal data, such as the alignment between captions and images. With the introduction of the DataComp <cite class="ltx_cite ltx_citemacro_citep">(Gadre et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, increasing research efforts have focused on exploring how to achieve effective caption-image alignment. Key approaches include the use of advanced alignment techniques, the development of refined annotation guidelines, and the implementation of novel training methodologies.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">With the advancement of generative models, data generation strategies have increasingly been utilized to achieve data creation and alignment. For example, <cite class="ltx_cite ltx_citemacro_citet">Nguyen et al<span class="ltx_text">.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> employed BLIP2 to generate numerous image captions, achieving SoTA results on DataComp. In the domain of VLLMs, <cite class="ltx_cite ltx_citemacro_citet">Chen et al<span class="ltx_text">.</span> (<a href="#bib.bib4" title="" class="ltx_ref">2023b</a>)</cite> utilized GPT-4 Vision to produce highly descriptive image captions, leading to significant improvements in LLaVA. The integration of these generative models has opened new avenues for enhancing data quality and alignment, further boosting VLLM performance.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Despite the notable contributions and advances in VLLMs, current data generation and alignment strategies for VLLMs ignore generating images and face the following three key challenges:</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.20756/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">We compared the synthetic image-text dataset with existing datasets. As shown in (a), the generated image can avoid content such as watermarks and advertisements. In (b), the caption mentions a joicy named plate, however in the bottom right image, joicy is actually a book. The generated images better reflect the content of the captions. Additionally, the resolution of the generated images is 1024x1024, which is higher than the existing images, is more beneficial for model training and expansion.</span></figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">C1. Low Data Quality.</span>
Our evaluations using metrics such as CLIPScore  <cite class="ltx_cite ltx_citemacro_citep">(Hessel et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite> and training results on VLLMs reveal that existing datasets still align the modalities sub-optimally. Web images often suffer from issues such as blurriness and watermarks. Additionally, methods employing BLIP2  <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> for generating captions frequently result in logical inconsistencies and unclear grammar, which mislead VLLM training and degrade their language capabilities.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">C2.Poor Effectiveness.</span>
It has been demonstrated in <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2023</a>; He et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023c</a>)</cite> that low-quality data can lead to poor model performance. Consequently, VLLMs are usually trained on low-quality data, the VLLMs exhibit reduced effectiveness.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_bold">C3. Low Efficiency.</span>
Methods that rely on manual captioning are labor-intensive and resource-demanding. Automated solutions like ShareGPT4v  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2023b</a>)</cite>, which employ GPT-4 Vision for labeling, are expensive and difficult to scale. Moreover, current strategies often necessitate the creation of extensive datasets to enhance performance, leading to significant data redundancy.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p"><span id="S1.p9.1.1" class="ltx_text ltx_font_bold">C4. Security Risks.</span>
Utilizing internet-sourced data introduces numerous security and privacy concerns  <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2023a</a>; Das et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024</a>)</cite>. They may contain personal information or copyrighted materials, posing potential legal and ethical challenges. Moreover, the inclusion of sensitive or inappropriate content within training datasets can instigate ethical issues, thereby compromising the models’ integrity and fairness.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">To address these issues, we introduced a new data generation pipeline: caption-to-image synthesis. We first implemented a quality selection process for high-quality caption data. Subsequently, we employed advanced diffusion models to generate images from these captions. For quality control, we used CLIPScore as the quality metric to select high-quality image-text pairs. Our data generation method achieved higher alignment between images and captions compared to existing approaches. Utilizing 100K curated synthetic data, we achieved SoTA results on multiple benchmarks, using only 18% of the official dataset size.</p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">Overall, our contributions are as follows:</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2407.20756/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="216" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">With only 100k synthetic pre-training data, our SynthVLM outperforms the LLAVA 1.5 model, which is trained on 558k data.</span></figcaption>
</figure>
<div id="S1.p12" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">New Perspective.</span>
To the best of our knowledge, we are the first to utilize Caption-to-Image strategy to construct a high-aligned training dataset for VLLMs and achieved superior performance with just 100K data points(only 18% of the official dataset size). This sets a precedent for utilizing generated images to train large-scale VLLMs, offering a solution to the limitation of data availability, quality, and privacy.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">New Method.</span>
We proposed a new image-text pair generation pipeline to ensure high-quality data. Additionally, we proposed a new paradigm for utilizing generated images for VLLMs training.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">SoTA Performance.</span>
<span id="S1.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">(1) <span id="S1.I1.i3.p1.1.2.1" class="ltx_text ltx_font_italic">High Data Quality.</span></span> As shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our generated data achieved higher CLIPScores, indicating better image-text alignment. Additionally, we achieved higher resolution, which is beneficial for tasks requiring high-quality images. Furthermore, our generated images avoid issues such as blurriness and watermarks.</p>
</div>
<div id="S1.I1.i3.p2" class="ltx_para">
<p id="S1.I1.i3.p2.1" class="ltx_p"><span id="S1.I1.i3.p2.1.1" class="ltx_text ltx_font_bold">(2) <span id="S1.I1.i3.p2.1.1.1" class="ltx_text ltx_font_italic">High Efficiency and Effectiveness.</span></span> As shown in Figure <a href="#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, with only 18% of the high-quality pre-training data, our method outperforms the baseline that uses 100% of the data. We not only achieve SoTA performance on vision understanding tasks but also demonstrate excellent pure text task capabilities, highlighting superior modality alignment.</p>
</div>
<div id="S1.I1.i3.p3" class="ltx_para">
<p id="S1.I1.i3.p3.1" class="ltx_p"><span id="S1.I1.i3.p3.1.1" class="ltx_text ltx_font_bold">(3) <span id="S1.I1.i3.p3.1.1.1" class="ltx_text ltx_font_italic">Data Privacy.</span></span> Using generated data avoids the need for real personal images, documents, and other sensitive information, ensuring data privacy.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Diffusion Model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Denoising diffusion probabilistic models (DDPMs)  <cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Rombach et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022</a>; Podell et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite> are a class of generative models renowned for their ability to generate extremely high-quality images. The core idea of DDPMs involves modeling the data distribution by gradually adding Gaussian noise to the input image during the forward process and then predicting and removing this noise to reconstruct the image during the backward process.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.3" class="ltx_p">Given a source image data distribution <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="x_{0}\sim q(x_{0})" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><msub id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">x</mi><mn id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">0</mn></msub><mo id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">∼</mo><mrow id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.1.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.1.2.cmml">​</mo><mrow id="S2.SS1.p2.1.m1.1.1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p2.1.m1.1.1.1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p2.1.m1.1.1.1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.1.1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.SS1.p2.1.m1.1.1.1.1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo stretchy="false" id="S2.SS1.p2.1.m1.1.1.1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">similar-to</csymbol><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">𝑥</ci><cn type="integer" id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3">0</cn></apply><apply id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"><times id="S2.SS1.p2.1.m1.1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.1.2"></times><ci id="S2.SS1.p2.1.m1.1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.1.3">𝑞</ci><apply id="S2.SS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S2.SS1.p2.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1.1.1.3">0</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">x_{0}\sim q(x_{0})</annotation></semantics></math>, Gaussian noise is added over <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">T</annotation></semantics></math> steps to obtain <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="x_{T}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><msub id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mi id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">x</mi><mi id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">𝑥</ci><ci id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">x_{T}</annotation></semantics></math>. The forward process is defined as:</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.2" class="ltx_Math" alttext="q(x_{1},\ldots,x_{T}\mid x_{0}):=\prod_{t=1}^{T}q(x_{t}\mid x_{t-1})," display="block"><semantics id="S2.Ex1.m1.2a"><mrow id="S2.Ex1.m1.2.2.1" xref="S2.Ex1.m1.2.2.1.1.cmml"><mrow id="S2.Ex1.m1.2.2.1.1" xref="S2.Ex1.m1.2.2.1.1.cmml"><mrow id="S2.Ex1.m1.2.2.1.1.2" xref="S2.Ex1.m1.2.2.1.1.2.cmml"><mi id="S2.Ex1.m1.2.2.1.1.2.4" xref="S2.Ex1.m1.2.2.1.1.2.4.cmml">q</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.2.2.1.1.2.3" xref="S2.Ex1.m1.2.2.1.1.2.3.cmml">​</mo><mrow id="S2.Ex1.m1.2.2.1.1.2.2.2" xref="S2.Ex1.m1.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S2.Ex1.m1.2.2.1.1.2.2.2.3" xref="S2.Ex1.m1.2.2.1.1.2.2.3.cmml">(</mo><msub id="S2.Ex1.m1.2.2.1.1.1.1.1.1" xref="S2.Ex1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.2.2.1.1.1.1.1.1.2" xref="S2.Ex1.m1.2.2.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.Ex1.m1.2.2.1.1.1.1.1.1.3" xref="S2.Ex1.m1.2.2.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.Ex1.m1.2.2.1.1.2.2.2.4" xref="S2.Ex1.m1.2.2.1.1.2.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">…</mi><mo id="S2.Ex1.m1.2.2.1.1.2.2.2.5" xref="S2.Ex1.m1.2.2.1.1.2.2.3.cmml">,</mo><mrow id="S2.Ex1.m1.2.2.1.1.2.2.2.2" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.cmml"><msub id="S2.Ex1.m1.2.2.1.1.2.2.2.2.2" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.cmml"><mi id="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.2" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.2.cmml">x</mi><mi id="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.3" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.3.cmml">T</mi></msub><mo id="S2.Ex1.m1.2.2.1.1.2.2.2.2.1" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.1.cmml">∣</mo><msub id="S2.Ex1.m1.2.2.1.1.2.2.2.2.3" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.cmml"><mi id="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.2" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.2.cmml">x</mi><mn id="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.3" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.3.cmml">0</mn></msub></mrow><mo rspace="0.278em" stretchy="false" id="S2.Ex1.m1.2.2.1.1.2.2.2.6" xref="S2.Ex1.m1.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S2.Ex1.m1.2.2.1.1.4" xref="S2.Ex1.m1.2.2.1.1.4.cmml">:=</mo><mrow id="S2.Ex1.m1.2.2.1.1.3" xref="S2.Ex1.m1.2.2.1.1.3.cmml"><munderover id="S2.Ex1.m1.2.2.1.1.3.2" xref="S2.Ex1.m1.2.2.1.1.3.2.cmml"><mo movablelimits="false" id="S2.Ex1.m1.2.2.1.1.3.2.2.2" xref="S2.Ex1.m1.2.2.1.1.3.2.2.2.cmml">∏</mo><mrow id="S2.Ex1.m1.2.2.1.1.3.2.2.3" xref="S2.Ex1.m1.2.2.1.1.3.2.2.3.cmml"><mi id="S2.Ex1.m1.2.2.1.1.3.2.2.3.2" xref="S2.Ex1.m1.2.2.1.1.3.2.2.3.2.cmml">t</mi><mo id="S2.Ex1.m1.2.2.1.1.3.2.2.3.1" xref="S2.Ex1.m1.2.2.1.1.3.2.2.3.1.cmml">=</mo><mn id="S2.Ex1.m1.2.2.1.1.3.2.2.3.3" xref="S2.Ex1.m1.2.2.1.1.3.2.2.3.3.cmml">1</mn></mrow><mi id="S2.Ex1.m1.2.2.1.1.3.2.3" xref="S2.Ex1.m1.2.2.1.1.3.2.3.cmml">T</mi></munderover><mrow id="S2.Ex1.m1.2.2.1.1.3.1" xref="S2.Ex1.m1.2.2.1.1.3.1.cmml"><mi id="S2.Ex1.m1.2.2.1.1.3.1.3" xref="S2.Ex1.m1.2.2.1.1.3.1.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.2.2.1.1.3.1.2" xref="S2.Ex1.m1.2.2.1.1.3.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.2.2.1.1.3.1.1.1" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.2.2.1.1.3.1.1.1.2" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.cmml">(</mo><mrow id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.cmml"><msub id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.cmml"><mi id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.2" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.2.cmml">x</mi><mi id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.3" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.1" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.1.cmml">∣</mo><msub id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.cmml"><mi id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.2" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.2.cmml">x</mi><mrow id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.cmml"><mi id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.2" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.2.cmml">t</mi><mo id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.1" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.1.cmml">−</mo><mn id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.3" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="S2.Ex1.m1.2.2.1.1.3.1.1.1.3" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.Ex1.m1.2.2.1.2" xref="S2.Ex1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.2b"><apply id="S2.Ex1.m1.2.2.1.1.cmml" xref="S2.Ex1.m1.2.2.1"><csymbol cd="latexml" id="S2.Ex1.m1.2.2.1.1.4.cmml" xref="S2.Ex1.m1.2.2.1.1.4">assign</csymbol><apply id="S2.Ex1.m1.2.2.1.1.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2"><times id="S2.Ex1.m1.2.2.1.1.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.2.3"></times><ci id="S2.Ex1.m1.2.2.1.1.2.4.cmml" xref="S2.Ex1.m1.2.2.1.1.2.4">𝑞</ci><vector id="S2.Ex1.m1.2.2.1.1.2.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2"><apply id="S2.Ex1.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S2.Ex1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.1.1.3">1</cn></apply><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">…</ci><apply id="S2.Ex1.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2"><csymbol cd="latexml" id="S2.Ex1.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.1">conditional</csymbol><apply id="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.1.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.2">subscript</csymbol><ci id="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.2">𝑥</ci><ci id="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.2.3">𝑇</ci></apply><apply id="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.1.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.3">subscript</csymbol><ci id="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.2.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.2">𝑥</ci><cn type="integer" id="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.3.cmml" xref="S2.Ex1.m1.2.2.1.1.2.2.2.2.3.3">0</cn></apply></apply></vector></apply><apply id="S2.Ex1.m1.2.2.1.1.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3"><apply id="S2.Ex1.m1.2.2.1.1.3.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.1.1.3.2.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2">superscript</csymbol><apply id="S2.Ex1.m1.2.2.1.1.3.2.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.1.1.3.2.2.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2">subscript</csymbol><csymbol cd="latexml" id="S2.Ex1.m1.2.2.1.1.3.2.2.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2.2.2">product</csymbol><apply id="S2.Ex1.m1.2.2.1.1.3.2.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2.2.3"><eq id="S2.Ex1.m1.2.2.1.1.3.2.2.3.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2.2.3.1"></eq><ci id="S2.Ex1.m1.2.2.1.1.3.2.2.3.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2.2.3.2">𝑡</ci><cn type="integer" id="S2.Ex1.m1.2.2.1.1.3.2.2.3.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2.2.3.3">1</cn></apply></apply><ci id="S2.Ex1.m1.2.2.1.1.3.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3.2.3">𝑇</ci></apply><apply id="S2.Ex1.m1.2.2.1.1.3.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1"><times id="S2.Ex1.m1.2.2.1.1.3.1.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.2"></times><ci id="S2.Ex1.m1.2.2.1.1.3.1.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.3">𝑞</ci><apply id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.1">conditional</csymbol><apply id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.2">𝑥</ci><ci id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.2.3">𝑡</ci></apply><apply id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.2">𝑥</ci><apply id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3"><minus id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.1.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.1"></minus><ci id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.2.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.3.cmml" xref="S2.Ex1.m1.2.2.1.1.3.1.1.1.1.3.3.3">1</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.2c">q(x_{1},\ldots,x_{T}\mid x_{0}):=\prod_{t=1}^{T}q(x_{t}\mid x_{t-1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table id="S2.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex2.m1.1" class="ltx_Math" alttext="q(x_{t}\mid x_{t-1})=\mathcal{N}(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}I)," display="block"><semantics id="S2.Ex2.m1.1a"><mrow id="S2.Ex2.m1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.cmml"><mrow id="S2.Ex2.m1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.cmml"><mrow id="S2.Ex2.m1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.1.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.Ex2.m1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex2.m1.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex2.m1.1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.Ex2.m1.1.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S2.Ex2.m1.1.1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml">∣</mo><msub id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mrow id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="S2.Ex2.m1.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.1.1.1.1.5" xref="S2.Ex2.m1.1.1.1.1.5.cmml">=</mo><mrow id="S2.Ex2.m1.1.1.1.1.4" xref="S2.Ex2.m1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex2.m1.1.1.1.1.4.5" xref="S2.Ex2.m1.1.1.1.1.4.5.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.1.1.4.4" xref="S2.Ex2.m1.1.1.1.1.4.4.cmml">​</mo><mrow id="S2.Ex2.m1.1.1.1.1.4.3.3" xref="S2.Ex2.m1.1.1.1.1.4.3.4.cmml"><mo stretchy="false" id="S2.Ex2.m1.1.1.1.1.4.3.3.4" xref="S2.Ex2.m1.1.1.1.1.4.3.4.cmml">(</mo><msub id="S2.Ex2.m1.1.1.1.1.2.1.1.1" xref="S2.Ex2.m1.1.1.1.1.2.1.1.1.cmml"><mi id="S2.Ex2.m1.1.1.1.1.2.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.2.1.1.1.2.cmml">x</mi><mi id="S2.Ex2.m1.1.1.1.1.2.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.2.1.1.1.3.cmml">t</mi></msub><mo id="S2.Ex2.m1.1.1.1.1.4.3.3.5" xref="S2.Ex2.m1.1.1.1.1.4.3.4.cmml">;</mo><mrow id="S2.Ex2.m1.1.1.1.1.3.2.2.2" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.cmml"><msqrt id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.cmml"><mrow id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.cmml"><mn id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.2" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.2.cmml">1</mn><mo id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.1" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.1.cmml">−</mo><msub id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.cmml"><mi id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.2" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.2.cmml">β</mi><mi id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.3" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.3.cmml">t</mi></msub></mrow></msqrt><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.1.1.3.2.2.2.1" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.1.cmml">​</mo><msub id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.cmml"><mi id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.2" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.2.cmml">x</mi><mrow id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.cmml"><mi id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.2" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.2.cmml">t</mi><mo id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.1" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.1.cmml">−</mo><mn id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.3" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.Ex2.m1.1.1.1.1.4.3.3.6" xref="S2.Ex2.m1.1.1.1.1.4.3.4.cmml">,</mo><mrow id="S2.Ex2.m1.1.1.1.1.4.3.3.3" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.cmml"><msub id="S2.Ex2.m1.1.1.1.1.4.3.3.3.2" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.cmml"><mi id="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.2" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.2.cmml">β</mi><mi id="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.3" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.1.1.4.3.3.3.1" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.1.cmml">​</mo><mi id="S2.Ex2.m1.1.1.1.1.4.3.3.3.3" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.3.cmml">I</mi></mrow><mo stretchy="false" id="S2.Ex2.m1.1.1.1.1.4.3.3.7" xref="S2.Ex2.m1.1.1.1.1.4.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S2.Ex2.m1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.1b"><apply id="S2.Ex2.m1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1"><eq id="S2.Ex2.m1.1.1.1.1.5.cmml" xref="S2.Ex2.m1.1.1.1.1.5"></eq><apply id="S2.Ex2.m1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1"><times id="S2.Ex2.m1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.2"></times><ci id="S2.Ex2.m1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.3">𝑞</ci><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.2">𝑥</ci><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3"><minus id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></apply><apply id="S2.Ex2.m1.1.1.1.1.4.cmml" xref="S2.Ex2.m1.1.1.1.1.4"><times id="S2.Ex2.m1.1.1.1.1.4.4.cmml" xref="S2.Ex2.m1.1.1.1.1.4.4"></times><ci id="S2.Ex2.m1.1.1.1.1.4.5.cmml" xref="S2.Ex2.m1.1.1.1.1.4.5">𝒩</ci><list id="S2.Ex2.m1.1.1.1.1.4.3.4.cmml" xref="S2.Ex2.m1.1.1.1.1.4.3.3"><apply id="S2.Ex2.m1.1.1.1.1.2.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.2.1.1.1">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.2.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.2.1.1.1.2">𝑥</ci><ci id="S2.Ex2.m1.1.1.1.1.2.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.1.2.1.1.1.3">𝑡</ci></apply><apply id="S2.Ex2.m1.1.1.1.1.3.2.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2"><times id="S2.Ex2.m1.1.1.1.1.3.2.2.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.1"></times><apply id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2"><root id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2a.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2"></root><apply id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2"><minus id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.1"></minus><cn type="integer" id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.2">1</cn><apply id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.1.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.2">𝛽</ci><ci id="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.2.2.3.3">𝑡</ci></apply></apply></apply><apply id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.1.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.2">𝑥</ci><apply id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3"><minus id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.1.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.1"></minus><ci id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.2">𝑡</ci><cn type="integer" id="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2.2.3.3.3">1</cn></apply></apply></apply><apply id="S2.Ex2.m1.1.1.1.1.4.3.3.3.cmml" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3"><times id="S2.Ex2.m1.1.1.1.1.4.3.3.3.1.cmml" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.1"></times><apply id="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.2">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.2">𝛽</ci><ci id="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.3.cmml" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.2.3">𝑡</ci></apply><ci id="S2.Ex2.m1.1.1.1.1.4.3.3.3.3.cmml" xref="S2.Ex2.m1.1.1.1.1.4.3.3.3.3">𝐼</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.1c">q(x_{t}\mid x_{t-1})=\mathcal{N}(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}I),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.4" class="ltx_p">where <math id="S2.SS1.p2.4.m1.1" class="ltx_Math" alttext="\beta_{t}" display="inline"><semantics id="S2.SS1.p2.4.m1.1a"><msub id="S2.SS1.p2.4.m1.1.1" xref="S2.SS1.p2.4.m1.1.1.cmml"><mi id="S2.SS1.p2.4.m1.1.1.2" xref="S2.SS1.p2.4.m1.1.1.2.cmml">β</mi><mi id="S2.SS1.p2.4.m1.1.1.3" xref="S2.SS1.p2.4.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m1.1b"><apply id="S2.SS1.p2.4.m1.1.1.cmml" xref="S2.SS1.p2.4.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m1.1.1.1.cmml" xref="S2.SS1.p2.4.m1.1.1">subscript</csymbol><ci id="S2.SS1.p2.4.m1.1.1.2.cmml" xref="S2.SS1.p2.4.m1.1.1.2">𝛽</ci><ci id="S2.SS1.p2.4.m1.1.1.3.cmml" xref="S2.SS1.p2.4.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m1.1c">\beta_{t}</annotation></semantics></math> controls the variance of the noise added at each step.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The distribution after <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">t</annotation></semantics></math> steps can be written as:</p>
<table id="S2.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex3.m1.1" class="ltx_Math" alttext="q(x_{t}\mid x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})I)," display="block"><semantics id="S2.Ex3.m1.1a"><mrow id="S2.Ex3.m1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.cmml"><mrow id="S2.Ex3.m1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.cmml"><mrow id="S2.Ex3.m1.1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.1.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S2.Ex3.m1.1.1.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.Ex3.m1.1.1.1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex3.m1.1.1.1.1.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex3.m1.1.1.1.1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.Ex3.m1.1.1.1.1.1.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.cmml">∣</mo><msub id="S2.Ex3.m1.1.1.1.1.1.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mn id="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo stretchy="false" id="S2.Ex3.m1.1.1.1.1.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex3.m1.1.1.1.1.5" xref="S2.Ex3.m1.1.1.1.1.5.cmml">=</mo><mrow id="S2.Ex3.m1.1.1.1.1.4" xref="S2.Ex3.m1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex3.m1.1.1.1.1.4.5" xref="S2.Ex3.m1.1.1.1.1.4.5.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S2.Ex3.m1.1.1.1.1.4.4" xref="S2.Ex3.m1.1.1.1.1.4.4.cmml">​</mo><mrow id="S2.Ex3.m1.1.1.1.1.4.3.3" xref="S2.Ex3.m1.1.1.1.1.4.3.4.cmml"><mo stretchy="false" id="S2.Ex3.m1.1.1.1.1.4.3.3.4" xref="S2.Ex3.m1.1.1.1.1.4.3.4.cmml">(</mo><msub id="S2.Ex3.m1.1.1.1.1.2.1.1.1" xref="S2.Ex3.m1.1.1.1.1.2.1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.1.1.2.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.2.1.1.1.2.cmml">x</mi><mi id="S2.Ex3.m1.1.1.1.1.2.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.2.1.1.1.3.cmml">t</mi></msub><mo id="S2.Ex3.m1.1.1.1.1.4.3.3.5" xref="S2.Ex3.m1.1.1.1.1.4.3.4.cmml">;</mo><mrow id="S2.Ex3.m1.1.1.1.1.3.2.2.2" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.cmml"><msqrt id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.cmml"><msub id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.cmml"><mover accent="true" id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.cmml"><mi id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.2" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.2.cmml">α</mi><mo id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.1" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.1.cmml">¯</mo></mover><mi id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.3" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.3.cmml">t</mi></msub></msqrt><mo lspace="0em" rspace="0em" id="S2.Ex3.m1.1.1.1.1.3.2.2.2.1" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.1.cmml">​</mo><msub id="S2.Ex3.m1.1.1.1.1.3.2.2.2.3" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.cmml"><mi id="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.2" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.2.cmml">x</mi><mn id="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.3" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.3.cmml">0</mn></msub></mrow><mo id="S2.Ex3.m1.1.1.1.1.4.3.3.6" xref="S2.Ex3.m1.1.1.1.1.4.3.4.cmml">,</mo><mrow id="S2.Ex3.m1.1.1.1.1.4.3.3.3" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.cmml"><mrow id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.2" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.cmml">(</mo><mrow id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.cmml"><mn id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.2.cmml">1</mn><mo id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.1.cmml">−</mo><msub id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.cmml"><mover accent="true" id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.cmml"><mi id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.2" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.2.cmml">α</mi><mo id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.1" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.1.cmml">¯</mo></mover><mi id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.3" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.3" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.Ex3.m1.1.1.1.1.4.3.3.3.2" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.2.cmml">​</mo><mi id="S2.Ex3.m1.1.1.1.1.4.3.3.3.3" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.3.cmml">I</mi></mrow><mo stretchy="false" id="S2.Ex3.m1.1.1.1.1.4.3.3.7" xref="S2.Ex3.m1.1.1.1.1.4.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S2.Ex3.m1.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.1b"><apply id="S2.Ex3.m1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1"><eq id="S2.Ex3.m1.1.1.1.1.5.cmml" xref="S2.Ex3.m1.1.1.1.1.5"></eq><apply id="S2.Ex3.m1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1"><times id="S2.Ex3.m1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.2"></times><ci id="S2.Ex3.m1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.1.3">𝑞</ci><apply id="S2.Ex3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.2">𝑥</ci><cn type="integer" id="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply><apply id="S2.Ex3.m1.1.1.1.1.4.cmml" xref="S2.Ex3.m1.1.1.1.1.4"><times id="S2.Ex3.m1.1.1.1.1.4.4.cmml" xref="S2.Ex3.m1.1.1.1.1.4.4"></times><ci id="S2.Ex3.m1.1.1.1.1.4.5.cmml" xref="S2.Ex3.m1.1.1.1.1.4.5">𝒩</ci><list id="S2.Ex3.m1.1.1.1.1.4.3.4.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3"><apply id="S2.Ex3.m1.1.1.1.1.2.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.2.1.1.1">subscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.2.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.2.1.1.1.2">𝑥</ci><ci id="S2.Ex3.m1.1.1.1.1.2.1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.2.1.1.1.3">𝑡</ci></apply><apply id="S2.Ex3.m1.1.1.1.1.3.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2"><times id="S2.Ex3.m1.1.1.1.1.3.2.2.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.1"></times><apply id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2"><root id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2a.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2"></root><apply id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2">subscript</csymbol><apply id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2"><ci id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.1">¯</ci><ci id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.2.2">𝛼</ci></apply><ci id="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.2.2.3">𝑡</ci></apply></apply><apply id="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.1.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.3">subscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.2.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.2">𝑥</ci><cn type="integer" id="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.3.2.2.2.3.3">0</cn></apply></apply><apply id="S2.Ex3.m1.1.1.1.1.4.3.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3"><times id="S2.Ex3.m1.1.1.1.1.4.3.3.3.2.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.2"></times><apply id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1"><minus id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.1"></minus><cn type="integer" id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.2">1</cn><apply id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.1.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3">subscript</csymbol><apply id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2"><ci id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.1">¯</ci><ci id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.2.2">𝛼</ci></apply><ci id="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.1.1.1.3.3">𝑡</ci></apply></apply><ci id="S2.Ex3.m1.1.1.1.1.4.3.3.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.4.3.3.3.3">𝐼</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.1c">q(x_{t}\mid x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})I),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.2" class="ltx_p">where <math id="S2.SS1.p3.2.m1.1" class="ltx_Math" alttext="\bar{\alpha}_{t}=\prod_{i=1}^{t}(1-\beta_{i})" display="inline"><semantics id="S2.SS1.p3.2.m1.1a"><mrow id="S2.SS1.p3.2.m1.1.1" xref="S2.SS1.p3.2.m1.1.1.cmml"><msub id="S2.SS1.p3.2.m1.1.1.3" xref="S2.SS1.p3.2.m1.1.1.3.cmml"><mover accent="true" id="S2.SS1.p3.2.m1.1.1.3.2" xref="S2.SS1.p3.2.m1.1.1.3.2.cmml"><mi id="S2.SS1.p3.2.m1.1.1.3.2.2" xref="S2.SS1.p3.2.m1.1.1.3.2.2.cmml">α</mi><mo id="S2.SS1.p3.2.m1.1.1.3.2.1" xref="S2.SS1.p3.2.m1.1.1.3.2.1.cmml">¯</mo></mover><mi id="S2.SS1.p3.2.m1.1.1.3.3" xref="S2.SS1.p3.2.m1.1.1.3.3.cmml">t</mi></msub><mo rspace="0.111em" id="S2.SS1.p3.2.m1.1.1.2" xref="S2.SS1.p3.2.m1.1.1.2.cmml">=</mo><mrow id="S2.SS1.p3.2.m1.1.1.1" xref="S2.SS1.p3.2.m1.1.1.1.cmml"><msubsup id="S2.SS1.p3.2.m1.1.1.1.2" xref="S2.SS1.p3.2.m1.1.1.1.2.cmml"><mo rspace="0em" id="S2.SS1.p3.2.m1.1.1.1.2.2.2" xref="S2.SS1.p3.2.m1.1.1.1.2.2.2.cmml">∏</mo><mrow id="S2.SS1.p3.2.m1.1.1.1.2.2.3" xref="S2.SS1.p3.2.m1.1.1.1.2.2.3.cmml"><mi id="S2.SS1.p3.2.m1.1.1.1.2.2.3.2" xref="S2.SS1.p3.2.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.SS1.p3.2.m1.1.1.1.2.2.3.1" xref="S2.SS1.p3.2.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.SS1.p3.2.m1.1.1.1.2.2.3.3" xref="S2.SS1.p3.2.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.SS1.p3.2.m1.1.1.1.2.3" xref="S2.SS1.p3.2.m1.1.1.1.2.3.cmml">t</mi></msubsup><mrow id="S2.SS1.p3.2.m1.1.1.1.1.1" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.2.m1.1.1.1.1.1.2" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.2.m1.1.1.1.1.1.1" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.cmml"><mn id="S2.SS1.p3.2.m1.1.1.1.1.1.1.2" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S2.SS1.p3.2.m1.1.1.1.1.1.1.1" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.SS1.p3.2.m1.1.1.1.1.1.1.3" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.2" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.2.cmml">β</mi><mi id="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.3" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S2.SS1.p3.2.m1.1.1.1.1.1.3" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m1.1b"><apply id="S2.SS1.p3.2.m1.1.1.cmml" xref="S2.SS1.p3.2.m1.1.1"><eq id="S2.SS1.p3.2.m1.1.1.2.cmml" xref="S2.SS1.p3.2.m1.1.1.2"></eq><apply id="S2.SS1.p3.2.m1.1.1.3.cmml" xref="S2.SS1.p3.2.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m1.1.1.3.1.cmml" xref="S2.SS1.p3.2.m1.1.1.3">subscript</csymbol><apply id="S2.SS1.p3.2.m1.1.1.3.2.cmml" xref="S2.SS1.p3.2.m1.1.1.3.2"><ci id="S2.SS1.p3.2.m1.1.1.3.2.1.cmml" xref="S2.SS1.p3.2.m1.1.1.3.2.1">¯</ci><ci id="S2.SS1.p3.2.m1.1.1.3.2.2.cmml" xref="S2.SS1.p3.2.m1.1.1.3.2.2">𝛼</ci></apply><ci id="S2.SS1.p3.2.m1.1.1.3.3.cmml" xref="S2.SS1.p3.2.m1.1.1.3.3">𝑡</ci></apply><apply id="S2.SS1.p3.2.m1.1.1.1.cmml" xref="S2.SS1.p3.2.m1.1.1.1"><apply id="S2.SS1.p3.2.m1.1.1.1.2.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m1.1.1.1.2.1.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2">superscript</csymbol><apply id="S2.SS1.p3.2.m1.1.1.1.2.2.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m1.1.1.1.2.2.1.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S2.SS1.p3.2.m1.1.1.1.2.2.2.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2.2.2">product</csymbol><apply id="S2.SS1.p3.2.m1.1.1.1.2.2.3.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2.2.3"><eq id="S2.SS1.p3.2.m1.1.1.1.2.2.3.1.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2.2.3.1"></eq><ci id="S2.SS1.p3.2.m1.1.1.1.2.2.3.2.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S2.SS1.p3.2.m1.1.1.1.2.2.3.3.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.SS1.p3.2.m1.1.1.1.2.3.cmml" xref="S2.SS1.p3.2.m1.1.1.1.2.3">𝑡</ci></apply><apply id="S2.SS1.p3.2.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.2.m1.1.1.1.1.1"><minus id="S2.SS1.p3.2.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S2.SS1.p3.2.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.2">1</cn><apply id="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.2">𝛽</ci><ci id="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.SS1.p3.2.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m1.1c">\bar{\alpha}_{t}=\prod_{i=1}^{t}(1-\beta_{i})</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.4" class="ltx_p">The backward process aims to reconstruct the data by learning a series of Gaussian distributions that approximate the forward process:</p>
<table id="S2.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex4.m1.3" class="ltx_Math" alttext="p_{\theta}(x_{t-1}\mid x_{t})=\mathcal{N}(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t))," display="block"><semantics id="S2.Ex4.m1.3a"><mrow id="S2.Ex4.m1.3.3.1" xref="S2.Ex4.m1.3.3.1.1.cmml"><mrow id="S2.Ex4.m1.3.3.1.1" xref="S2.Ex4.m1.3.3.1.1.cmml"><mrow id="S2.Ex4.m1.3.3.1.1.1" xref="S2.Ex4.m1.3.3.1.1.1.cmml"><msub id="S2.Ex4.m1.3.3.1.1.1.3" xref="S2.Ex4.m1.3.3.1.1.1.3.cmml"><mi id="S2.Ex4.m1.3.3.1.1.1.3.2" xref="S2.Ex4.m1.3.3.1.1.1.3.2.cmml">p</mi><mi id="S2.Ex4.m1.3.3.1.1.1.3.3" xref="S2.Ex4.m1.3.3.1.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.3.3.1.1.1.2" xref="S2.Ex4.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S2.Ex4.m1.3.3.1.1.1.1.1" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex4.m1.3.3.1.1.1.1.1.2" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex4.m1.3.3.1.1.1.1.1.1" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.2" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.2.cmml">x</mi><mrow id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.cmml"><mi id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.2" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.2.cmml">t</mi><mo id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.1" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.3" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.Ex4.m1.3.3.1.1.1.1.1.1.1" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.1.cmml">∣</mo><msub id="S2.Ex4.m1.3.3.1.1.1.1.1.1.3" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S2.Ex4.m1.3.3.1.1.1.1.1.3" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex4.m1.3.3.1.1.5" xref="S2.Ex4.m1.3.3.1.1.5.cmml">=</mo><mrow id="S2.Ex4.m1.3.3.1.1.4" xref="S2.Ex4.m1.3.3.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex4.m1.3.3.1.1.4.5" xref="S2.Ex4.m1.3.3.1.1.4.5.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.3.3.1.1.4.4" xref="S2.Ex4.m1.3.3.1.1.4.4.cmml">​</mo><mrow id="S2.Ex4.m1.3.3.1.1.4.3.3" xref="S2.Ex4.m1.3.3.1.1.4.3.4.cmml"><mo stretchy="false" id="S2.Ex4.m1.3.3.1.1.4.3.3.4" xref="S2.Ex4.m1.3.3.1.1.4.3.4.cmml">(</mo><msub id="S2.Ex4.m1.3.3.1.1.2.1.1.1" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.cmml"><mi id="S2.Ex4.m1.3.3.1.1.2.1.1.1.2" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.2.cmml">x</mi><mrow id="S2.Ex4.m1.3.3.1.1.2.1.1.1.3" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.cmml"><mi id="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.2" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.2.cmml">t</mi><mo id="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.1" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.1.cmml">−</mo><mn id="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.3" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.Ex4.m1.3.3.1.1.4.3.3.5" xref="S2.Ex4.m1.3.3.1.1.4.3.4.cmml">;</mo><mrow id="S2.Ex4.m1.3.3.1.1.3.2.2.2" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.cmml"><msub id="S2.Ex4.m1.3.3.1.1.3.2.2.2.3" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.cmml"><mi id="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.2" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.2.cmml">μ</mi><mi id="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.3" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.3.3.1.1.3.2.2.2.2" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.2.cmml">​</mo><mrow id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.2.cmml"><mo stretchy="false" id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.2" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.2.cmml">(</mo><msub id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.cmml"><mi id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.2" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.2.cmml">x</mi><mi id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.3" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.3" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.2.cmml">,</mo><mi id="S2.Ex4.m1.1.1" xref="S2.Ex4.m1.1.1.cmml">t</mi><mo stretchy="false" id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.4" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex4.m1.3.3.1.1.4.3.3.6" xref="S2.Ex4.m1.3.3.1.1.4.3.4.cmml">,</mo><mrow id="S2.Ex4.m1.3.3.1.1.4.3.3.3" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.cmml"><msub id="S2.Ex4.m1.3.3.1.1.4.3.3.3.3" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.cmml"><mi mathvariant="normal" id="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.2" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.2.cmml">Σ</mi><mi id="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.3" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex4.m1.3.3.1.1.4.3.3.3.2" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.2.cmml">​</mo><mrow id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.2.cmml"><mo stretchy="false" id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.2" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.2.cmml">(</mo><msub id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.cmml"><mi id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.2" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.2.cmml">x</mi><mi id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.3" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.3.cmml">t</mi></msub><mo id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.3" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.2.cmml">,</mo><mi id="S2.Ex4.m1.2.2" xref="S2.Ex4.m1.2.2.cmml">t</mi><mo stretchy="false" id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.4" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.Ex4.m1.3.3.1.1.4.3.3.7" xref="S2.Ex4.m1.3.3.1.1.4.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S2.Ex4.m1.3.3.1.2" xref="S2.Ex4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex4.m1.3b"><apply id="S2.Ex4.m1.3.3.1.1.cmml" xref="S2.Ex4.m1.3.3.1"><eq id="S2.Ex4.m1.3.3.1.1.5.cmml" xref="S2.Ex4.m1.3.3.1.1.5"></eq><apply id="S2.Ex4.m1.3.3.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.1"><times id="S2.Ex4.m1.3.3.1.1.1.2.cmml" xref="S2.Ex4.m1.3.3.1.1.1.2"></times><apply id="S2.Ex4.m1.3.3.1.1.1.3.cmml" xref="S2.Ex4.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.1.1.1.3.1.cmml" xref="S2.Ex4.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S2.Ex4.m1.3.3.1.1.1.3.2.cmml" xref="S2.Ex4.m1.3.3.1.1.1.3.2">𝑝</ci><ci id="S2.Ex4.m1.3.3.1.1.1.3.3.cmml" xref="S2.Ex4.m1.3.3.1.1.1.3.3">𝜃</ci></apply><apply id="S2.Ex4.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.2">𝑥</ci><apply id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3"><minus id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.1.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.1"></minus><ci id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.2.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.2">𝑡</ci><cn type="integer" id="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.3.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex4.m1.3.3.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><apply id="S2.Ex4.m1.3.3.1.1.4.cmml" xref="S2.Ex4.m1.3.3.1.1.4"><times id="S2.Ex4.m1.3.3.1.1.4.4.cmml" xref="S2.Ex4.m1.3.3.1.1.4.4"></times><ci id="S2.Ex4.m1.3.3.1.1.4.5.cmml" xref="S2.Ex4.m1.3.3.1.1.4.5">𝒩</ci><list id="S2.Ex4.m1.3.3.1.1.4.3.4.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3"><apply id="S2.Ex4.m1.3.3.1.1.2.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.1.1.2.1.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1">subscript</csymbol><ci id="S2.Ex4.m1.3.3.1.1.2.1.1.1.2.cmml" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.2">𝑥</ci><apply id="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.cmml" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.3"><minus id="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.1.cmml" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.1"></minus><ci id="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.2.cmml" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.2">𝑡</ci><cn type="integer" id="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.3.cmml" xref="S2.Ex4.m1.3.3.1.1.2.1.1.1.3.3">1</cn></apply></apply><apply id="S2.Ex4.m1.3.3.1.1.3.2.2.2.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2"><times id="S2.Ex4.m1.3.3.1.1.3.2.2.2.2.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.2"></times><apply id="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.1.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.3">subscript</csymbol><ci id="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.2.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.2">𝜇</ci><ci id="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.3.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.3.3">𝜃</ci></apply><interval closure="open" id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.2.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1"><apply id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1">subscript</csymbol><ci id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.2.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.2">𝑥</ci><ci id="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.3.cmml" xref="S2.Ex4.m1.3.3.1.1.3.2.2.2.1.1.1.3">𝑡</ci></apply><ci id="S2.Ex4.m1.1.1.cmml" xref="S2.Ex4.m1.1.1">𝑡</ci></interval></apply><apply id="S2.Ex4.m1.3.3.1.1.4.3.3.3.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3"><times id="S2.Ex4.m1.3.3.1.1.4.3.3.3.2.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.2"></times><apply id="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.3"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.1.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.3">subscript</csymbol><ci id="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.2.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.2">Σ</ci><ci id="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.3.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.3.3">𝜃</ci></apply><interval closure="open" id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.2.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1"><apply id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.1.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1">subscript</csymbol><ci id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.2.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.2">𝑥</ci><ci id="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.3.cmml" xref="S2.Ex4.m1.3.3.1.1.4.3.3.3.1.1.1.3">𝑡</ci></apply><ci id="S2.Ex4.m1.2.2.cmml" xref="S2.Ex4.m1.2.2">𝑡</ci></interval></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex4.m1.3c">p_{\theta}(x_{t-1}\mid x_{t})=\mathcal{N}(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t)),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p4.3" class="ltx_p">where <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mu_{\theta}" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><msub id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml"><mi id="S2.SS1.p4.1.m1.1.1.2" xref="S2.SS1.p4.1.m1.1.1.2.cmml">μ</mi><mi id="S2.SS1.p4.1.m1.1.1.3" xref="S2.SS1.p4.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p4.1.m1.1.1.2.cmml" xref="S2.SS1.p4.1.m1.1.1.2">𝜇</ci><ci id="S2.SS1.p4.1.m1.1.1.3.cmml" xref="S2.SS1.p4.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">\mu_{\theta}</annotation></semantics></math> and <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="\Sigma_{\theta}" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><msub id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml"><mi mathvariant="normal" id="S2.SS1.p4.2.m2.1.1.2" xref="S2.SS1.p4.2.m2.1.1.2.cmml">Σ</mi><mi id="S2.SS1.p4.2.m2.1.1.3" xref="S2.SS1.p4.2.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><apply id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.2.m2.1.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p4.2.m2.1.1.2.cmml" xref="S2.SS1.p4.2.m2.1.1.2">Σ</ci><ci id="S2.SS1.p4.2.m2.1.1.3.cmml" xref="S2.SS1.p4.2.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">\Sigma_{\theta}</annotation></semantics></math> are neural networks parameterized by <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><mi id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><ci id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">\theta</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">While DDPMs have shown promising results, several improvements have been proposed to enhance their efficiency  <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2021</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2024</a>)</cite> and sample quality  <cite class="ltx_cite ltx_citemacro_citep">(Nichol and Dhariwal, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>; Dhariwal and Nichol, <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>. The superior performance of diffusion models has been leveraged in various sub-tasks, including image generation, image translation, inpainting  <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2022</a>; Su et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>. Our approach leverages diffusion models to create high-quality caption-image pairs specifically for VLLM training. These generated pairs not only enrich the training data but also provide a more diverse and comprehensive dataset, enabling VLLMs to learn better representations and achieve higher performance in downstream tasks such as image captioning and visual question answering.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Vision Language Models</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The integration of visual knowledge into large language models (LLMs) has become a pivotal area of research due to the rapid advancements in LLMs. VLLMs combine vision information from vision encoders with LLMs, thus enabling these models to process and interpret visual inputs for various visual tasks  <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023d</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2022</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022b</a>)</cite> with enhanced accuracy and efficiency. Pioneering frameworks like CLIP  <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2021</a>)</cite> leverage contrastive learning on expansive image-caption datasets to align modalities, forming the groundwork for cross-modal comprehension. Various adapters  <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib27" title="" class="ltx_ref">a</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib22" title="" class="ltx_ref">2022a</a>; Jian et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2023</a>; Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2023a</a>)</cite> are introduced to further integrate different modalities. For example, LLaVA  <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib27" title="" class="ltx_ref">a</a>)</cite> employs a straightforward MLP to inject the vision information into LLMs. Whereas more complex implementations like the Q-Former in BLIP  <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022a</a>, <a href="#bib.bib20" title="" class="ltx_ref">2023b</a>)</cite> utilize cross-attention to enhance modality integration.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Recent studies  <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2024b</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2023b</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib27" title="" class="ltx_ref">a</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023f</a>)</cite> aims to boost VLLM performance by focusing on the quality of both pre-training and fine-tuning datasets. Models like LLaVA  <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib27" title="" class="ltx_ref">a</a>)</cite> and ShareGPT4V  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2023b</a>)</cite> have shown remarkable advancements in understanding and following complex instructions through instruction tuning. Although these improvements help align the vision modality and establish a solid basis for cross-modal comprehension, they require extensive datasets for training and could potentially diminish the model’s language capabilities. In this work, we propose a novel data generation strategy that leverages high-quality caption-image pairs, enabling the model to achieve state-of-the-art (SoTA) results with minimal data. By aligning during the pre-training (PT) phase, we have further enhanced the model’s language capabilities.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Data Quality and Selection</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The advent of large language models has brought about a substantial increase in the volume of training data. <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2023</a>; OpenAI, <a href="#bib.bib41" title="" class="ltx_ref">2023b</a>)</cite> In this scenario, the quality and quantity of data become paramount. LLMs, trained on vast amounts of data, can capture subtle nuances and complex patterns in language, excelling in various natural language processing tasks. However, the increase in data volume also brings new challenges, particularly in data management, cleaning, and annotation. <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2024</a>)</cite> In this section, we mainly discuss the effectiveness of data quality and data selection.</p>
</div>
<section id="S2.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Quality</h4>

<div id="S2.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px1.p1.1" class="ltx_p">: High-quality data can significantly enhance the performance of models <cite class="ltx_cite ltx_citemacro_citep">(meta llama, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>)</cite>. As the volume of data increases, ensuring high data quality becomes more challenging because it requires more resources for data cleaning, selection and annotation. <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2024</a>)</cite> Poor quality data can lead to models learning incorrect patterns and making inaccurate predictions.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Selection</h4>

<div id="S2.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px2.p1.1" class="ltx_p">:
LLMs-based methods were commonly used in data selection. <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2024</a>)</cite> For instance, <cite class="ltx_cite ltx_citemacro_citet">Du et al<span class="ltx_text">.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> leverages DeBERTa <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> for scoring, retaining high-quality data, and combining it with the k-center greedy algorithm to select diverse data. <cite class="ltx_cite ltx_citemacro_citet">Chen et al<span class="ltx_text">.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2023c</a>)</cite> score the accuracy of data using ChatGPT to pick out high-quality data. <cite class="ltx_cite ltx_citemacro_citet">Xu et al<span class="ltx_text">.</span> (<a href="#bib.bib58" title="" class="ltx_ref">2023b</a>)</cite> use GPT-4 to rewrite data to increase their complexity and then streamline it by reducing its variety and improving its quality. <cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2023c</a>)</cite> train two models using ChatGPT’s labeled data to score the quality and complexity of the data. <cite class="ltx_cite ltx_citemacro_citet">Lu et al<span class="ltx_text">.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2023b</a>)</cite> rely on ChatGPT to tag each instance, defining its complexity and diversity based on these tags. <cite class="ltx_cite ltx_citemacro_citet">Parkar et al<span class="ltx_text">.</span> (<a href="#bib.bib42" title="" class="ltx_ref">2024</a>)</cite> first cluster the data, and then use GPT-4 to select high-quality data for each cluster.</p>
</div>
<div id="S2.SS3.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS3.SSS0.Px2.p2.1" class="ltx_p">Given the critical role of data quality and selection in enhancing model performance, our paper focuses on leveraging advanced data selection techniques to optimize caption and image-text pair quality. By employing methods that integrate LLM data selection and image-text alignment scores, we aim to efficiently identify and utilize high-quality data for VLLMs.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Data Generation</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Data has always been the key driver behind the success of LLMs. Recent advancements of LLMs largely due to the availability of large-scale, diverse, and high-quality datasets for training these models  <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>. However, the scarcity of data and the high costs present substantial challenges in obtaining such datasets  <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2024</a>; Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023a</a>)</cite>. Recent advancements in generating synthetic data and improving the performance of LLMs have shown promising results across various domains. Synthetic data holds great potential in building large-scale, high-quality datasets. Researchers have explored multiple approaches, from leveraging differential privacy to creating instruction-tuning frameworks, to enhance the quality, diversity, and utility of synthetic data  <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2024c</a>; Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>; Lou et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2023</a>)</cite>. A key component in generating high-quality synthetic datasets is precise alignment. Fan et al.  <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2024</a>)</cite> introduce REALIGN, a method that enhances the quality of instruction data by reformatting responses to better align with pre-established criteria and evidence, thereby improving LLMs’ alignment with human values while minimizing human annotation and model hallucinations. Li et al.  <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2023e</a>)</cite> build a high-quality instruction-following language model by automatically labeling human-written text with corresponding instructions and demonstrating highly effective self-alignment.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">In the field of VLLMs, the task of constructing generative datasets has been relatively underexplored. VLLMs primarily focus on the alignment between images and captions. Existing approaches, such as ShareGPT4V  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2023b</a>)</cite>, leverage GPT4-Vision to generate high-quality captions for images, thereby achieving alignment and producing SoTA results on models like LLaVA. However, this method incurs high costs and often results in suboptimal alignment due to the complexity of the captions. Our approach introduces a new method for aligning images and captions by utilizing text-to-image models to generate large-scale, high-quality data. Specifically, our method surpasses existing techniques in alignment accuracy and efficiency.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2407.20756/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="245" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Curated image-text pair generation pipeline. We first use the diffusion model to generate images and then select the high clip score image-text pairs.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce our data generation pipeline and then compare the generated dataset with other commonly used datasets. In subsection <a href="#S3.SS1" title="3.1. Synthetic Dataset Construction ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we explore the construction of our caption set and the subsequent generation of 1 million caption-image pairs using diffusion models. In subsection <a href="#S3.SS2" title="3.2. Synthetic Data Selection ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we outline the filtering process applied to the generated 1 million image-text pairs. Then we meticulously select 100k high-quality, well-aligned caption-image pairs, ensuring the robustness and relevance of our dataset. In subsection <a href="#S3.SS3" title="3.3. High Quality Synthetic Dataset ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we demonstrate the effectiveness of our method by comparing the SynthVLM dataset with other existing datasets. In subsection <a href="#S3.SS4" title="3.4. SynthVLM ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, we summarize the VLLMs training pipeline by utilizing the synthetic dataset.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2407.20756/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="105" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">This is our process and prompt design for match assessment using GPT4-Vision. We consider various aspects, including the quality of the image and the match between the image and the caption, to help GPT4-Vision make a better selection. Based on this process, we compare SynthVLM with existing datasets from the model’s perspective.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Synthetic Dataset Construction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this section, we introduce the image generation pipeline. First, we construct a large pool of captions for selection. Next, we generate high-quality images corresponding to these captions. We then select the best captions from the pool for image-text generation. Utilizing these high-quality captions, we employ diffusion models to generate the images.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">LCS abbreviates the LAION, CC, and SBU datasets. SynthVLM uses captions to generate images, while others use images to generate captions or manual labeling.</span></figcaption>
<div id="S3.T1.4" class="ltx_inline-block ltx_transformed_outer" style="width:203.8pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-114.2pt,28.5pt) scale(0.471474985430068,0.471474985430068) ;">
<table id="S3.T1.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T1.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Name</span></th>
<th id="S3.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Image Source</span></th>
<th id="S3.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Caption Source</span></th>
<th id="S3.T1.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.4.1.1.1.4.1" class="ltx_text ltx_font_bold">Sample</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.1.2.1" class="ltx_tr">
<th id="S3.T1.4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">COCO-Caption</th>
<td id="S3.T1.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">COCO</td>
<td id="S3.T1.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Human</td>
<td id="S3.T1.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">118K</td>
</tr>
<tr id="S3.T1.4.1.3.2" class="ltx_tr">
<th id="S3.T1.4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BLIP-LCS</th>
<td id="S3.T1.4.1.3.2.2" class="ltx_td ltx_align_center">LCS</td>
<td id="S3.T1.4.1.3.2.3" class="ltx_td ltx_align_center">BLIP</td>
<td id="S3.T1.4.1.3.2.4" class="ltx_td ltx_align_center">558K</td>
</tr>
<tr id="S3.T1.4.1.4.3" class="ltx_tr">
<th id="S3.T1.4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ShareGPT4V</th>
<td id="S3.T1.4.1.4.3.2" class="ltx_td ltx_align_center">LCS, COCO, etc</td>
<td id="S3.T1.4.1.4.3.3" class="ltx_td ltx_align_center">GPT4-Vision</td>
<td id="S3.T1.4.1.4.3.4" class="ltx_td ltx_align_center">100K</td>
</tr>
<tr id="S3.T1.4.1.5.4" class="ltx_tr">
<th id="S3.T1.4.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ShareGPT4V-PT</th>
<td id="S3.T1.4.1.5.4.2" class="ltx_td ltx_align_center">LCS, COCO, etc</td>
<td id="S3.T1.4.1.5.4.3" class="ltx_td ltx_align_center">Share-Captioner</td>
<td id="S3.T1.4.1.5.4.4" class="ltx_td ltx_align_center">1246K</td>
</tr>
<tr id="S3.T1.4.1.6.5" class="ltx_tr">
<th id="S3.T1.4.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">SynthVLM</th>
<td id="S3.T1.4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Diffusion</td>
<td id="S3.T1.4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">LCS, COCO, BLIP2-DataComp, etc</td>
<td id="S3.T1.4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1000K</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Data Source.</span>
To ensure the diversity of the captions, we combined human-generated and machine-generated captions. As shown in Table  <a href="#S3.T1" title="Table 1 ‣ 3.1. Synthetic Dataset Construction ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The human-generated captions were primarily sourced from LAION, CC, and SBU, while the machine-generated captions were predominantly created using the method described in  <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. They utilize BLIP2 to regenerate captions for images in the datacomp dataset  <cite class="ltx_cite ltx_citemacro_citep">(Gadre et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Caption Curation.</span>
To maintain dataset quality, we first removed low-quality captions, such as advertisements, overly repetitive descriptions, and captions with significant grammatical errors. This filtering process was performed using GPT-3, ensuring that only high-quality, informative captions were used for training. For the remaining captions, we calculated the CLIPScore for these captions and their corresponding raw images. CLIPScore is a metric that measures the cosine similarity between images and their corresponding captions.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.7" class="ltx_p">The formula for calculating CLIPScore is as follows:</p>
<table id="S3.Ex5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex5.m1.8" class="ltx_Math" alttext="\text{CLIPScore}(I,C)=\frac{\text{CLIP}(I)\cdot\text{CLIP}(C)}{||\text{CLIP}(I)||\cdot||\text{CLIP}(C)||}" display="block"><semantics id="S3.Ex5.m1.8a"><mrow id="S3.Ex5.m1.8.9" xref="S3.Ex5.m1.8.9.cmml"><mrow id="S3.Ex5.m1.8.9.2" xref="S3.Ex5.m1.8.9.2.cmml"><mtext id="S3.Ex5.m1.8.9.2.2" xref="S3.Ex5.m1.8.9.2.2a.cmml">CLIPScore</mtext><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.8.9.2.1" xref="S3.Ex5.m1.8.9.2.1.cmml">​</mo><mrow id="S3.Ex5.m1.8.9.2.3.2" xref="S3.Ex5.m1.8.9.2.3.1.cmml"><mo stretchy="false" id="S3.Ex5.m1.8.9.2.3.2.1" xref="S3.Ex5.m1.8.9.2.3.1.cmml">(</mo><mi id="S3.Ex5.m1.7.7" xref="S3.Ex5.m1.7.7.cmml">I</mi><mo id="S3.Ex5.m1.8.9.2.3.2.2" xref="S3.Ex5.m1.8.9.2.3.1.cmml">,</mo><mi id="S3.Ex5.m1.8.8" xref="S3.Ex5.m1.8.8.cmml">C</mi><mo stretchy="false" id="S3.Ex5.m1.8.9.2.3.2.3" xref="S3.Ex5.m1.8.9.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex5.m1.8.9.1" xref="S3.Ex5.m1.8.9.1.cmml">=</mo><mfrac id="S3.Ex5.m1.6.6" xref="S3.Ex5.m1.6.6.cmml"><mrow id="S3.Ex5.m1.2.2.2" xref="S3.Ex5.m1.2.2.2.cmml"><mrow id="S3.Ex5.m1.2.2.2.4" xref="S3.Ex5.m1.2.2.2.4.cmml"><mrow id="S3.Ex5.m1.2.2.2.4.2" xref="S3.Ex5.m1.2.2.2.4.2.cmml"><mtext id="S3.Ex5.m1.2.2.2.4.2.2" xref="S3.Ex5.m1.2.2.2.4.2.2a.cmml">CLIP</mtext><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.2.2.2.4.2.1" xref="S3.Ex5.m1.2.2.2.4.2.1.cmml">​</mo><mrow id="S3.Ex5.m1.2.2.2.4.2.3.2" xref="S3.Ex5.m1.2.2.2.4.2.cmml"><mo stretchy="false" id="S3.Ex5.m1.2.2.2.4.2.3.2.1" xref="S3.Ex5.m1.2.2.2.4.2.cmml">(</mo><mi id="S3.Ex5.m1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.cmml">I</mi><mo rspace="0.055em" stretchy="false" id="S3.Ex5.m1.2.2.2.4.2.3.2.2" xref="S3.Ex5.m1.2.2.2.4.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.Ex5.m1.2.2.2.4.1" xref="S3.Ex5.m1.2.2.2.4.1.cmml">⋅</mo><mtext id="S3.Ex5.m1.2.2.2.4.3" xref="S3.Ex5.m1.2.2.2.4.3a.cmml">CLIP</mtext></mrow><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.2.2.2.3" xref="S3.Ex5.m1.2.2.2.3.cmml">​</mo><mrow id="S3.Ex5.m1.2.2.2.5.2" xref="S3.Ex5.m1.2.2.2.cmml"><mo stretchy="false" id="S3.Ex5.m1.2.2.2.5.2.1" xref="S3.Ex5.m1.2.2.2.cmml">(</mo><mi id="S3.Ex5.m1.2.2.2.2" xref="S3.Ex5.m1.2.2.2.2.cmml">C</mi><mo stretchy="false" id="S3.Ex5.m1.2.2.2.5.2.2" xref="S3.Ex5.m1.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.Ex5.m1.6.6.6" xref="S3.Ex5.m1.6.6.6.cmml"><mrow id="S3.Ex5.m1.5.5.5.3.1" xref="S3.Ex5.m1.5.5.5.3.2.cmml"><mo stretchy="false" id="S3.Ex5.m1.5.5.5.3.1.2" xref="S3.Ex5.m1.5.5.5.3.2.1.cmml">‖</mo><mrow id="S3.Ex5.m1.5.5.5.3.1.1" xref="S3.Ex5.m1.5.5.5.3.1.1.cmml"><mtext id="S3.Ex5.m1.5.5.5.3.1.1.2" xref="S3.Ex5.m1.5.5.5.3.1.1.2a.cmml">CLIP</mtext><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.5.5.5.3.1.1.1" xref="S3.Ex5.m1.5.5.5.3.1.1.1.cmml">​</mo><mrow id="S3.Ex5.m1.5.5.5.3.1.1.3.2" xref="S3.Ex5.m1.5.5.5.3.1.1.cmml"><mo stretchy="false" id="S3.Ex5.m1.5.5.5.3.1.1.3.2.1" xref="S3.Ex5.m1.5.5.5.3.1.1.cmml">(</mo><mi id="S3.Ex5.m1.3.3.3.1" xref="S3.Ex5.m1.3.3.3.1.cmml">I</mi><mo stretchy="false" id="S3.Ex5.m1.5.5.5.3.1.1.3.2.2" xref="S3.Ex5.m1.5.5.5.3.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.Ex5.m1.5.5.5.3.1.3" xref="S3.Ex5.m1.5.5.5.3.2.1.cmml">‖</mo></mrow><mo rspace="0.222em" id="S3.Ex5.m1.6.6.6.5" xref="S3.Ex5.m1.6.6.6.5.cmml">⋅</mo><mrow id="S3.Ex5.m1.6.6.6.4.1" xref="S3.Ex5.m1.6.6.6.4.2.cmml"><mo stretchy="false" id="S3.Ex5.m1.6.6.6.4.1.2" xref="S3.Ex5.m1.6.6.6.4.2.1.cmml">‖</mo><mrow id="S3.Ex5.m1.6.6.6.4.1.1" xref="S3.Ex5.m1.6.6.6.4.1.1.cmml"><mtext id="S3.Ex5.m1.6.6.6.4.1.1.2" xref="S3.Ex5.m1.6.6.6.4.1.1.2a.cmml">CLIP</mtext><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.6.6.6.4.1.1.1" xref="S3.Ex5.m1.6.6.6.4.1.1.1.cmml">​</mo><mrow id="S3.Ex5.m1.6.6.6.4.1.1.3.2" xref="S3.Ex5.m1.6.6.6.4.1.1.cmml"><mo stretchy="false" id="S3.Ex5.m1.6.6.6.4.1.1.3.2.1" xref="S3.Ex5.m1.6.6.6.4.1.1.cmml">(</mo><mi id="S3.Ex5.m1.4.4.4.2" xref="S3.Ex5.m1.4.4.4.2.cmml">C</mi><mo stretchy="false" id="S3.Ex5.m1.6.6.6.4.1.1.3.2.2" xref="S3.Ex5.m1.6.6.6.4.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex5.m1.6.6.6.4.1.3" xref="S3.Ex5.m1.6.6.6.4.2.1.cmml">‖</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.8b"><apply id="S3.Ex5.m1.8.9.cmml" xref="S3.Ex5.m1.8.9"><eq id="S3.Ex5.m1.8.9.1.cmml" xref="S3.Ex5.m1.8.9.1"></eq><apply id="S3.Ex5.m1.8.9.2.cmml" xref="S3.Ex5.m1.8.9.2"><times id="S3.Ex5.m1.8.9.2.1.cmml" xref="S3.Ex5.m1.8.9.2.1"></times><ci id="S3.Ex5.m1.8.9.2.2a.cmml" xref="S3.Ex5.m1.8.9.2.2"><mtext id="S3.Ex5.m1.8.9.2.2.cmml" xref="S3.Ex5.m1.8.9.2.2">CLIPScore</mtext></ci><interval closure="open" id="S3.Ex5.m1.8.9.2.3.1.cmml" xref="S3.Ex5.m1.8.9.2.3.2"><ci id="S3.Ex5.m1.7.7.cmml" xref="S3.Ex5.m1.7.7">𝐼</ci><ci id="S3.Ex5.m1.8.8.cmml" xref="S3.Ex5.m1.8.8">𝐶</ci></interval></apply><apply id="S3.Ex5.m1.6.6.cmml" xref="S3.Ex5.m1.6.6"><divide id="S3.Ex5.m1.6.6.7.cmml" xref="S3.Ex5.m1.6.6"></divide><apply id="S3.Ex5.m1.2.2.2.cmml" xref="S3.Ex5.m1.2.2.2"><times id="S3.Ex5.m1.2.2.2.3.cmml" xref="S3.Ex5.m1.2.2.2.3"></times><apply id="S3.Ex5.m1.2.2.2.4.cmml" xref="S3.Ex5.m1.2.2.2.4"><ci id="S3.Ex5.m1.2.2.2.4.1.cmml" xref="S3.Ex5.m1.2.2.2.4.1">⋅</ci><apply id="S3.Ex5.m1.2.2.2.4.2.cmml" xref="S3.Ex5.m1.2.2.2.4.2"><times id="S3.Ex5.m1.2.2.2.4.2.1.cmml" xref="S3.Ex5.m1.2.2.2.4.2.1"></times><ci id="S3.Ex5.m1.2.2.2.4.2.2a.cmml" xref="S3.Ex5.m1.2.2.2.4.2.2"><mtext id="S3.Ex5.m1.2.2.2.4.2.2.cmml" xref="S3.Ex5.m1.2.2.2.4.2.2">CLIP</mtext></ci><ci id="S3.Ex5.m1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1">𝐼</ci></apply><ci id="S3.Ex5.m1.2.2.2.4.3a.cmml" xref="S3.Ex5.m1.2.2.2.4.3"><mtext id="S3.Ex5.m1.2.2.2.4.3.cmml" xref="S3.Ex5.m1.2.2.2.4.3">CLIP</mtext></ci></apply><ci id="S3.Ex5.m1.2.2.2.2.cmml" xref="S3.Ex5.m1.2.2.2.2">𝐶</ci></apply><apply id="S3.Ex5.m1.6.6.6.cmml" xref="S3.Ex5.m1.6.6.6"><ci id="S3.Ex5.m1.6.6.6.5.cmml" xref="S3.Ex5.m1.6.6.6.5">⋅</ci><apply id="S3.Ex5.m1.5.5.5.3.2.cmml" xref="S3.Ex5.m1.5.5.5.3.1"><csymbol cd="latexml" id="S3.Ex5.m1.5.5.5.3.2.1.cmml" xref="S3.Ex5.m1.5.5.5.3.1.2">norm</csymbol><apply id="S3.Ex5.m1.5.5.5.3.1.1.cmml" xref="S3.Ex5.m1.5.5.5.3.1.1"><times id="S3.Ex5.m1.5.5.5.3.1.1.1.cmml" xref="S3.Ex5.m1.5.5.5.3.1.1.1"></times><ci id="S3.Ex5.m1.5.5.5.3.1.1.2a.cmml" xref="S3.Ex5.m1.5.5.5.3.1.1.2"><mtext id="S3.Ex5.m1.5.5.5.3.1.1.2.cmml" xref="S3.Ex5.m1.5.5.5.3.1.1.2">CLIP</mtext></ci><ci id="S3.Ex5.m1.3.3.3.1.cmml" xref="S3.Ex5.m1.3.3.3.1">𝐼</ci></apply></apply><apply id="S3.Ex5.m1.6.6.6.4.2.cmml" xref="S3.Ex5.m1.6.6.6.4.1"><csymbol cd="latexml" id="S3.Ex5.m1.6.6.6.4.2.1.cmml" xref="S3.Ex5.m1.6.6.6.4.1.2">norm</csymbol><apply id="S3.Ex5.m1.6.6.6.4.1.1.cmml" xref="S3.Ex5.m1.6.6.6.4.1.1"><times id="S3.Ex5.m1.6.6.6.4.1.1.1.cmml" xref="S3.Ex5.m1.6.6.6.4.1.1.1"></times><ci id="S3.Ex5.m1.6.6.6.4.1.1.2a.cmml" xref="S3.Ex5.m1.6.6.6.4.1.1.2"><mtext id="S3.Ex5.m1.6.6.6.4.1.1.2.cmml" xref="S3.Ex5.m1.6.6.6.4.1.1.2">CLIP</mtext></ci><ci id="S3.Ex5.m1.4.4.4.2.cmml" xref="S3.Ex5.m1.4.4.4.2">𝐶</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.8c">\text{CLIPScore}(I,C)=\frac{\text{CLIP}(I)\cdot\text{CLIP}(C)}{||\text{CLIP}(I)||\cdot||\text{CLIP}(C)||}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p4.6" class="ltx_p">where <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">I</annotation></semantics></math> represents the image, <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">C</annotation></semantics></math> represents the caption, and <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\text{CLIP}(I)" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.2" xref="S3.SS1.p4.3.m3.1.2.cmml"><mtext id="S3.SS1.p4.3.m3.1.2.2" xref="S3.SS1.p4.3.m3.1.2.2a.cmml">CLIP</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.2.1" xref="S3.SS1.p4.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS1.p4.3.m3.1.2.3.2" xref="S3.SS1.p4.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.3.m3.1.2.3.2.1" xref="S3.SS1.p4.3.m3.1.2.cmml">(</mo><mi id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">I</mi><mo stretchy="false" id="S3.SS1.p4.3.m3.1.2.3.2.2" xref="S3.SS1.p4.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.2.cmml" xref="S3.SS1.p4.3.m3.1.2"><times id="S3.SS1.p4.3.m3.1.2.1.cmml" xref="S3.SS1.p4.3.m3.1.2.1"></times><ci id="S3.SS1.p4.3.m3.1.2.2a.cmml" xref="S3.SS1.p4.3.m3.1.2.2"><mtext id="S3.SS1.p4.3.m3.1.2.2.cmml" xref="S3.SS1.p4.3.m3.1.2.2">CLIP</mtext></ci><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\text{CLIP}(I)</annotation></semantics></math> and <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\text{CLIP}(C)" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mrow id="S3.SS1.p4.4.m4.1.2" xref="S3.SS1.p4.4.m4.1.2.cmml"><mtext id="S3.SS1.p4.4.m4.1.2.2" xref="S3.SS1.p4.4.m4.1.2.2a.cmml">CLIP</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p4.4.m4.1.2.1" xref="S3.SS1.p4.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS1.p4.4.m4.1.2.3.2" xref="S3.SS1.p4.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.4.m4.1.2.3.2.1" xref="S3.SS1.p4.4.m4.1.2.cmml">(</mo><mi id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">C</mi><mo stretchy="false" id="S3.SS1.p4.4.m4.1.2.3.2.2" xref="S3.SS1.p4.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.2.cmml" xref="S3.SS1.p4.4.m4.1.2"><times id="S3.SS1.p4.4.m4.1.2.1.cmml" xref="S3.SS1.p4.4.m4.1.2.1"></times><ci id="S3.SS1.p4.4.m4.1.2.2a.cmml" xref="S3.SS1.p4.4.m4.1.2.2"><mtext id="S3.SS1.p4.4.m4.1.2.2.cmml" xref="S3.SS1.p4.4.m4.1.2.2">CLIP</mtext></ci><ci id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\text{CLIP}(C)</annotation></semantics></math> denote the image and text feature vectors extracted by the CLIP model. The dot product of the vectors is denoted by <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mo id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><ci id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">\cdot</annotation></semantics></math>, and <math id="S3.SS1.p4.6.m6.1" class="ltx_math_unparsed" alttext="||\cdot||" display="inline"><semantics id="S3.SS1.p4.6.m6.1a"><mrow id="S3.SS1.p4.6.m6.1b"><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS1.p4.6.m6.1.1">|</mo><mo fence="false" stretchy="false" id="S3.SS1.p4.6.m6.1.2">|</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p4.6.m6.1.3">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS1.p4.6.m6.1.4">|</mo><mo fence="false" stretchy="false" id="S3.SS1.p4.6.m6.1.5">|</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">||\cdot||</annotation></semantics></math> denotes the norm of the vectors.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">We selected the top 40% of image-caption pairs with the highest scores. These selected captions were included in the candidate caption set. At this step, we believe that captions with better image-caption alignment are more likely to generate high-quality, well-aligned images. Ultimately, we sampled a dataset of 1,000k captions for data generation. By using only captions, our method significantly reduces storage overhead and processing time. The caption curation pipeline is summarized in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.4. Data Generation ‣ 2. Related Work ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a).</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">Image Generation.</span>
After filtering through 1000k high-quality captions, we utilized Stable Diffusion XL (SDXL)  <cite class="ltx_cite ltx_citemacro_citep">(Podell et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>, a SoTA model which can efficiently generate high-quality, high-resolution images. By configuring SDXL’s steps to 60 and utilizing 8 A100 GPUs, we were able to generate all images within a week. Furthermore, SynthVLM generates images at a resolution of 1024x1024, effectively addressing the issue of low resolution found in existing datasets. This enhancement significantly improves the quality and utility of the training data for a variety of image generation and recognition tasks.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Synthetic Data Selection</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this section, we introduce the quality control process for synthetic datasets. To further ensure the alignment between images and their corresponding text descriptions, we employ CLIPScore a second time, evaluating the quality of image-text pairs with enhanced precision.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.4. Data Generation ‣ 2. Related Work ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b), we initially computed CLIPScores for the 1,000K synthetic image-caption pairs. We then selected the top 100K pairs that demonstrated the highest scores, indicating the most accurate and meaningful matches between images and captions. By curating this subset, we constructed a high-quality, highly aligned synthetic dataset.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>. </span><span id="S3.T2.3.2" class="ltx_text" style="font-size:90%;">We compared the average CLIPScores of our synthetic dataset, ShareGPT4V, COCO-Caption, and BLIP-LCS. The results indicate that SynthVLM exhibits the highest alignment in terms of CLIPScore values.</span></figcaption>
<div id="S3.T2.4" class="ltx_inline-block ltx_transformed_outer" style="width:160.4pt;height:75.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.8pt,16.3pt) scale(0.697299520577511,0.697299520577511) ;">
<table id="S3.T2.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.4.1.1.1" class="ltx_tr">
<th id="S3.T2.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T2.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Name</span></th>
<td id="S3.T2.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Sample</span></td>
<td id="S3.T2.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Avg CLIPScore</span></td>
</tr>
<tr id="S3.T2.4.1.2.2" class="ltx_tr">
<th id="S3.T2.4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">COCO-Caption</th>
<td id="S3.T2.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">118K</td>
<td id="S3.T2.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">0.31</td>
</tr>
<tr id="S3.T2.4.1.3.3" class="ltx_tr">
<th id="S3.T2.4.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">BLIP-LCS</th>
<td id="S3.T2.4.1.3.3.2" class="ltx_td ltx_align_center">558K</td>
<td id="S3.T2.4.1.3.3.3" class="ltx_td ltx_align_center">0.32</td>
</tr>
<tr id="S3.T2.4.1.4.4" class="ltx_tr">
<th id="S3.T2.4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ShareGPT4V</th>
<td id="S3.T2.4.1.4.4.2" class="ltx_td ltx_align_center">100K</td>
<td id="S3.T2.4.1.4.4.3" class="ltx_td ltx_align_center">0.32</td>
</tr>
<tr id="S3.T2.4.1.5.5" class="ltx_tr">
<th id="S3.T2.4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">SynthVLM</th>
<td id="S3.T2.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">1000K</td>
<td id="S3.T2.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.34</td>
</tr>
<tr id="S3.T2.4.1.6.6" class="ltx_tr">
<th id="S3.T2.4.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Curated-SynthVLM</th>
<td id="S3.T2.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb">100K</td>
<td id="S3.T2.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">0.38</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>High Quality Synthetic Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this section, we compare commonly used image-caption datasets with the SynthVLM synthetic dataset. The synthetic data offers high image quality, excellent image-text alignment, superior machine ratings, and robust data privacy protection.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">High Image Quality.</span>
As illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, SynthVLM significantly advances image quality by providing images at a resolution of 1024x1024 pixels. This high resolution addresses the common issue of inadequate image quality in existing datasets, thereby supplying high-quality image-caption pairs invaluable for VLLMs. Furthermore, Curated-SynthVLM effectively mitigates issues such as watermarks and advertisements. For image-caption alignment, Curated-SynthVLM leverages the SoTA SDXL to generate images, ensuring that generated images closely correspond to the provided captions.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Excellent Image-Text Alignment.</span> As shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.2. Synthetic Data Selection ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the generated SynthVLM dataset exhibits a higher CLIPScore. By selecting curated image-text pairs of higher quality, Curated-SynthVLM achieves an even higher CLIPScore, surpassing COCO-Caption, BLIP-LCS, and ShareGPT4V. This demonstrates the excellent alignment of our data.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>. </span><span id="S3.T3.3.2" class="ltx_text" style="font-size:90%;">We employed GPT4-Vision and InternVL to vote on the match between each caption and its corresponding generated image and raw image. The results demonstrate that the generated images align more closely with the captions.</span></figcaption>
<div id="S3.T3.4" class="ltx_inline-block ltx_transformed_outer" style="width:203.8pt;height:40.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.2pt,6.8pt) scale(0.748773689359305,0.748773689359305) ;">
<table id="S3.T3.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.4.1.1.1" class="ltx_tr">
<th id="S3.T3.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T3.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Sample</span></th>
<th id="S3.T3.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T3.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S3.T3.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Image-gen win</span></th>
<th id="S3.T3.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.4.1.1.1.4.1" class="ltx_text ltx_font_bold">Image-raw win</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.4.1.2.1" class="ltx_tr">
<th id="S3.T3.4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1K</th>
<th id="S3.T3.4.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">GPT4-Vision</th>
<td id="S3.T3.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">633</td>
<td id="S3.T3.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">367</td>
</tr>
<tr id="S3.T3.4.1.3.2" class="ltx_tr">
<th id="S3.T3.4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">1K</th>
<th id="S3.T3.4.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">InternVL</th>
<td id="S3.T3.4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">692</td>
<td id="S3.T3.4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">308</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Excellent Machine Rating.</span> Since our data will be used for VLLMs training, we use VLLMs to evaluate the data quality. We selected 1K image-caption pairs and submitted the caption along with the synthetic image and the original image. We used GPT-4 Vision and Intern-VL as the judge model and requested it to select the pair that exhibited higher alignment. The specific prompt used for this evaluation is illustrated in Figure <a href="#S3.F4" title="Figure 4 ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The results, presented in Table <a href="#S3.T3" title="Table 3 ‣ 3.3. High Quality Synthetic Dataset ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, demonstrate that images generated have better alignment with the caption.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Protect Data Privacy.</span> A notable advantage of our dataset is its exclusive reliance on captions, which mitigates data privacy concerns. By not using existing images, we ensure that no sensitive or private information associated with those images is compromised. This approach adheres to data privacy best practices, ensuring that our dataset maintains both high quality and ethical integrity.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>SynthVLM</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In this section, we utilize the synthetic dataset in section <a href="#S3.SS3" title="3.3. High Quality Synthetic Dataset ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> to pre-train the Vision Language model. We adopt the same widely-used model architecture summarized in <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2024</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>)</cite>, as depicted in Figure <a href="#S3.F5" title="Figure 5 ‣ SFT Stage ‣ 3.4. SynthVLM ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Pre-training Stage</h4>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">As illustrated in Figure <a href="#S3.F5" title="Figure 5 ‣ SFT Stage ‣ 3.4. SynthVLM ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a), we train the projector during the pre-training stage to achieve alignment between the image and text modalities. The SynthVLM dataset described in Section <a href="#S3.SS3" title="3.3. High Quality Synthetic Dataset ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> is utilized for this purpose.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">SFT Stage</h4>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F5" title="Figure 5 ‣ SFT Stage ‣ 3.4. SynthVLM ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b), we further train the projector along with the LLM during the SFT stage to enhance visual understanding capabilities. For this stage, we utilize the commonly used LLaVA 665k dataset from <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S3.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p2.1" class="ltx_p">Through these two training stages, we successfully developed the SynthVLM model. SynthVLM is efficient, utilizing only 100k pre-training data while also protecting privacy by leveraging synthetic data. Additionally, SynthVLM provides a new paradigm for effective alignment between modalities in Vision Language Models using synthetic data.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2407.20756/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">We introduce the pre-training model structure in Figure (a). Using synthetic data, we pre-train the projector to align the image and text modalities. As shown in Figure (b), we subsequently use LLaVA 665k data to fine-tune the projector and the LLM.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we first introduce the experimental setups. We then aim to answer the following questions to verify the effectiveness, efficiency, and privacy protection of our proposed SynthVLM: <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Q1</span>: Can our SynthVLM achieve SoTA performance compared to previous SoTA methods? <span id="S4.p1.1.2" class="ltx_text ltx_font_bold">Q2</span>: Can our SynthVLM have better image-text alignment compared to previous methods? <span id="S4.p1.1.3" class="ltx_text ltx_font_bold">Q3</span>: How efficient is our SynthVLM compared to previous methods? <span id="S4.p1.1.4" class="ltx_text ltx_font_bold">Q4</span>: Can SynthVLM protect privacy while achieving SoTA performance? <span id="S4.p1.1.5" class="ltx_text ltx_font_bold">Q5</span>: Do we need the generate module and the data quality selection module to enhance model performance?</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experimental Settings</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Datasets.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We utilized the 558k pre-training dataset and the 665k SFT dataset from LLaVA, in addition to the synthetic 100k dataset in section <a href="#S3.SS3" title="3.3. High Quality Synthetic Dataset ‣ 3. Method ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> for training our SynthVLM.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Models.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">For the image generation model, we select SDXL, and SDXL’s steps are set to 60. We employed the well-known LLaVA 1.5 model configured with 13 billion parameters for its robust visual understanding capabilities. For the encoder, we chose the CLIP 336 with a 14-patch configuration, and for the language model, the Vicuna v1.5 configured with 13 billion parameters was selected.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Baselines.</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">The LLaVA 1.5<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/haotian-liu/LLaVA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/haotian-liu/LLaVA</a></span></span></span> model was reproduced using configurations sourced from the official repository to establish a baseline. We opted for the LLaVA model trained with Vicuna v1.5 7B and 13B models as the baseline <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Benchmarks.</h4>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">We select benchmarks for both visual understanding and language understanding. For visual understanding, we choose SQA_Img, MMVet, VizWiz, VQAv2, GQA, MME, and PoPE for a comprehensive evaluation. For pure text benchmarks, we select MMLU and SQA to assess language understanding abilities.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>. </span><span id="S4.T4.5.2" class="ltx_text" style="font-size:90%;">Comparison of SynthVLM and LLaVA using the same model structure. We can see SynthVLM outperforms LLaVA on all the evaluation benchmarks.</span></figcaption>
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:62.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-100.2pt,14.2pt) scale(0.683928988131316,0.683928988131316) ;">
<table id="S4.T4.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2.2" class="ltx_tr">
<th id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.3.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.4.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.5.1" class="ltx_text ltx_font_bold">SQA</span></th>
<th id="S4.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.6.1" class="ltx_text ltx_font_bold">SQA_Img</span></th>
<th id="S4.T4.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.7.1" class="ltx_text ltx_font_bold">MMVet</span></th>
<th id="S4.T4.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.8.1" class="ltx_text ltx_font_bold">VizWiz</span></th>
<th id="S4.T4.2.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.9.1" class="ltx_text ltx_font_bold">VQAv2</span></th>
<th id="S4.T4.2.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.10.1" class="ltx_text ltx_font_bold">GQA</span></th>
<th id="S4.T4.2.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.11.1" class="ltx_text ltx_font_bold">MMB</span></th>
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">MME<sup id="S4.T4.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T4.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">P</span></sup></span></th>
<th id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.2.1" class="ltx_text ltx_font_bold">MME<sup id="S4.T4.2.2.2.2.1.1" class="ltx_sup"><span id="S4.T4.2.2.2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">C</span></sup></span></th>
<th id="S4.T4.2.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.12.1" class="ltx_text ltx_font_bold">PoPE</span></th>
<th id="S4.T4.2.2.2.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.2.13.1" class="ltx_text ltx_font_bold">MMLU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.3.1" class="ltx_tr">
<td id="S4.T4.2.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Baseline</span></td>
<td id="S4.T4.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">Vicuna-1.5-7B</td>
<td id="S4.T4.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">69.3</td>
<td id="S4.T4.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">67.3</td>
<td id="S4.T4.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">30.5</td>
<td id="S4.T4.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.3.1.6.1" class="ltx_text ltx_font_bold">49.9</span></td>
<td id="S4.T4.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">78.7</td>
<td id="S4.T4.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t">62.5</td>
<td id="S4.T4.2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t">65.3</td>
<td id="S4.T4.2.2.3.1.10" class="ltx_td ltx_align_center ltx_border_t">1484.8</td>
<td id="S4.T4.2.2.3.1.11" class="ltx_td ltx_align_center ltx_border_t">315.6</td>
<td id="S4.T4.2.2.3.1.12" class="ltx_td ltx_align_center ltx_border_t">86.0</td>
<td id="S4.T4.2.2.3.1.13" class="ltx_td ltx_align_center ltx_border_t">36.3</td>
</tr>
<tr id="S4.T4.2.2.4.2" class="ltx_tr">
<td id="S4.T4.2.2.4.2.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.1.1" class="ltx_text ltx_font_bold">Synth Select 100k</span></td>
<td id="S4.T4.2.2.4.2.2" class="ltx_td ltx_align_center">Vicuna-1.5-7B</td>
<td id="S4.T4.2.2.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.3.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T4.2.2.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.4.1" class="ltx_text ltx_font_bold">68.9</span></td>
<td id="S4.T4.2.2.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.5.1" class="ltx_text ltx_font_bold">32.2</span></td>
<td id="S4.T4.2.2.4.2.6" class="ltx_td ltx_align_center">49.3</td>
<td id="S4.T4.2.2.4.2.7" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.7.1" class="ltx_text ltx_font_bold">79.4</span></td>
<td id="S4.T4.2.2.4.2.8" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.8.1" class="ltx_text ltx_font_bold">63.1</span></td>
<td id="S4.T4.2.2.4.2.9" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.9.1" class="ltx_text ltx_font_bold">66.8</span></td>
<td id="S4.T4.2.2.4.2.10" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.10.1" class="ltx_text ltx_font_bold">1518.5</span></td>
<td id="S4.T4.2.2.4.2.11" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.11.1" class="ltx_text ltx_font_bold">345.7</span></td>
<td id="S4.T4.2.2.4.2.12" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.12.1" class="ltx_text ltx_font_bold">87.0</span></td>
<td id="S4.T4.2.2.4.2.13" class="ltx_td ltx_align_center"><span id="S4.T4.2.2.4.2.13.1" class="ltx_text ltx_font_bold">41.2</span></td>
</tr>
<tr id="S4.T4.2.2.5.3" class="ltx_tr">
<td id="S4.T4.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.5.3.1.1" class="ltx_text ltx_font_bold">Baseline</span></td>
<td id="S4.T4.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t">Vicuna-1.5-13B</td>
<td id="S4.T4.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">74.2</td>
<td id="S4.T4.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_t">71.0</td>
<td id="S4.T4.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">35.0</td>
<td id="S4.T4.2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_t">53.6</td>
<td id="S4.T4.2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_t">80.0</td>
<td id="S4.T4.2.2.5.3.8" class="ltx_td ltx_align_center ltx_border_t">63.0</td>
<td id="S4.T4.2.2.5.3.9" class="ltx_td ltx_align_center ltx_border_t">67.7</td>
<td id="S4.T4.2.2.5.3.10" class="ltx_td ltx_align_center ltx_border_t">1531.3</td>
<td id="S4.T4.2.2.5.3.11" class="ltx_td ltx_align_center ltx_border_t">294.5</td>
<td id="S4.T4.2.2.5.3.12" class="ltx_td ltx_align_center ltx_border_t">86.9</td>
<td id="S4.T4.2.2.5.3.13" class="ltx_td ltx_align_center ltx_border_t">52.4</td>
</tr>
<tr id="S4.T4.2.2.6.4" class="ltx_tr">
<td id="S4.T4.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.1.1" class="ltx_text ltx_font_bold">Synth Select 100k</span></td>
<td id="S4.T4.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">Vicuna-1.5-13B</td>
<td id="S4.T4.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.3.1" class="ltx_text ltx_font_bold">74.9</span></td>
<td id="S4.T4.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.4.1" class="ltx_text ltx_font_bold">72.5</span></td>
<td id="S4.T4.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.5.1" class="ltx_text ltx_font_bold">35.0</span></td>
<td id="S4.T4.2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.6.1" class="ltx_text ltx_font_bold">55.9</span></td>
<td id="S4.T4.2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.7.1" class="ltx_text ltx_font_bold">80.0</span></td>
<td id="S4.T4.2.2.6.4.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.8.1" class="ltx_text ltx_font_bold">63.5</span></td>
<td id="S4.T4.2.2.6.4.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.9.1" class="ltx_text ltx_font_bold">68.3</span></td>
<td id="S4.T4.2.2.6.4.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.10.1" class="ltx_text ltx_font_bold">1573.0</span></td>
<td id="S4.T4.2.2.6.4.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.11.1" class="ltx_text ltx_font_bold">316.1</span></td>
<td id="S4.T4.2.2.6.4.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.12.1" class="ltx_text ltx_font_bold">88.4</span></td>
<td id="S4.T4.2.2.6.4.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.2.6.4.13.1" class="ltx_text ltx_font_bold">54.6</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>. </span><span id="S4.T5.3.2" class="ltx_text" style="font-size:90%;">Result comparison of MMLU shows that with the synthetic 100k data, our SynthVLM outperforms LLaVA in pure language tasks. This demonstrates the effectiveness of the synthetic data in modality alignment.</span></figcaption>
<table id="S4.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.4.1.1" class="ltx_tr">
<th id="S4.T5.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T5.4.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T5.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T5.4.1.1.2.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T5.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T5.4.1.1.3.1" class="ltx_text ltx_font_bold">SQA</span></th>
<th id="S4.T5.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5"><span id="S4.T5.4.1.1.4.1" class="ltx_text ltx_font_bold">MMLU</span></th>
</tr>
<tr id="S4.T5.4.2.2" class="ltx_tr">
<th id="S4.T5.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T5.4.2.2.1.1" class="ltx_text ltx_font_bold">Avg</span></th>
<th id="S4.T5.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.2.2.2.1" class="ltx_text ltx_font_bold">STEM</span></th>
<th id="S4.T5.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.2.2.3.1" class="ltx_text ltx_font_bold">Humanities</span></th>
<th id="S4.T5.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.2.2.4.1" class="ltx_text ltx_font_bold">Social Sciences</span></th>
<th id="S4.T5.4.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T5.4.2.2.5.1" class="ltx_text ltx_font_bold">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.4.3.1" class="ltx_tr">
<td id="S4.T5.4.3.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.4.3.1.1.1" class="ltx_text ltx_font_bold">Baseline</span></td>
<td id="S4.T5.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t">Vicuna-1.5-7B</td>
<td id="S4.T5.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t">69.3</td>
<td id="S4.T5.4.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.3</td>
<td id="S4.T5.4.3.1.5" class="ltx_td ltx_align_center ltx_border_t">28.6</td>
<td id="S4.T5.4.3.1.6" class="ltx_td ltx_align_center ltx_border_t">33.4</td>
<td id="S4.T5.4.3.1.7" class="ltx_td ltx_align_center ltx_border_t">39.5</td>
<td id="S4.T5.4.3.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">44.5</td>
</tr>
<tr id="S4.T5.4.4.2" class="ltx_tr">
<td id="S4.T5.4.4.2.1" class="ltx_td ltx_align_left"><span id="S4.T5.4.4.2.1.1" class="ltx_text ltx_font_bold">Synth Select 100k</span></td>
<td id="S4.T5.4.4.2.2" class="ltx_td ltx_align_center">Vicuna-1.5-7B</td>
<td id="S4.T5.4.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T5.4.4.2.3.1" class="ltx_text ltx_font_bold">70.4</span></td>
<td id="S4.T5.4.4.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.4.4.2.4.1" class="ltx_text ltx_font_bold">41.2</span></td>
<td id="S4.T5.4.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T5.4.4.2.5.1" class="ltx_text ltx_font_bold">31.7</span></td>
<td id="S4.T5.4.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T5.4.4.2.6.1" class="ltx_text ltx_font_bold">37.4</span></td>
<td id="S4.T5.4.4.2.7" class="ltx_td ltx_align_center"><span id="S4.T5.4.4.2.7.1" class="ltx_text ltx_font_bold">47.0</span></td>
<td id="S4.T5.4.4.2.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.4.4.2.8.1" class="ltx_text ltx_font_bold">50.2</span></td>
</tr>
<tr id="S4.T5.4.5.3" class="ltx_tr">
<td id="S4.T5.4.5.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.4.5.3.1.1" class="ltx_text ltx_font_bold">Baseline</span></td>
<td id="S4.T5.4.5.3.2" class="ltx_td ltx_align_center ltx_border_t">Vicuna-1.5-13B</td>
<td id="S4.T5.4.5.3.3" class="ltx_td ltx_align_center ltx_border_t">74.2</td>
<td id="S4.T5.4.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.4</td>
<td id="S4.T5.4.5.3.5" class="ltx_td ltx_align_center ltx_border_t">41.9</td>
<td id="S4.T5.4.5.3.6" class="ltx_td ltx_align_center ltx_border_t">45.8</td>
<td id="S4.T5.4.5.3.7" class="ltx_td ltx_align_center ltx_border_t">62.9</td>
<td id="S4.T5.4.5.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">61.8</td>
</tr>
<tr id="S4.T5.4.6.4" class="ltx_tr">
<td id="S4.T5.4.6.4.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.4.6.4.1.1" class="ltx_text ltx_font_bold">Synth Select 100k</span></td>
<td id="S4.T5.4.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">Vicuna-1.5-13B</td>
<td id="S4.T5.4.6.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.4.6.4.3.1" class="ltx_text ltx_font_bold">74.9</span></td>
<td id="S4.T5.4.6.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.4.6.4.4.1" class="ltx_text ltx_font_bold">54.6</span></td>
<td id="S4.T5.4.6.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.4.6.4.5.1" class="ltx_text ltx_font_bold">45.0</span></td>
<td id="S4.T5.4.6.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.4.6.4.6.1" class="ltx_text ltx_font_bold">49.3</span></td>
<td id="S4.T5.4.6.4.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.4.6.4.7.1" class="ltx_text ltx_font_bold">64.0</span></td>
<td id="S4.T5.4.6.4.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T5.4.6.4.8.1" class="ltx_text ltx_font_bold">62.2</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Synthetic Data Achieves SoTA Performance</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To address <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Q1</span>, we selected LLaVA 1.5 with the CLIP 336 encoder and Vicuna v1.5 7B and 13B as the base model. We used the synthetic 100k dataset to pretrain the LLaVA 1.5 model and subsequently fine-tuned it with LLaVA 665k SFT data. The trained model is denoted as ”Synth Select 100k.” We then compared it with the LLaVA 1.5 model pre-trained on 558k data and fine-tuned on the same 665k SFT data from the official repository, referred to as ”Baseline”</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">From Table <a href="#S4.T4" title="Table 4 ‣ Benchmarks. ‣ 4.1. Experimental Settings ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, it is evident that Synth Select 100k outperforms the Baseline across all evaluation benchmarks on both the 7B and 13B model. Specifically, Synth Select 100k achieves SoTA results on visual benchmarks such as SQA_Img, MMVet, VizWiz, VQAv2, GQA, MME, MMB, and PoPE. Furthermore, SynthVLM also excels in pure language benchmarks, demonstrating superior performance in SQA and MMLU, thus showcasing its comprehensive capabilities in both vision and language tasks.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Effective Vision Language Alignment</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To address <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Q2</span>, we utilize pure language ability to demonstrate the model’s alignment. During the SFT stage, the LLM can be adjusted to align with images; hence, better alignment can preserve the LLM’s pure language ability <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>. We select the MMLU and SQA benchmarks for their comprehensive language understanding capabilities.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">From Table <a href="#S4.T5" title="Table 5 ‣ Benchmarks. ‣ 4.1. Experimental Settings ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, it is evident that our Synth Select 100k model outperforms the Baseline on all MMLU benchmarks and SQA benchmarks.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">These results demonstrate the strong alignment capability of our synthetic data. Additionally, this provides a new paradigm for effective visual understanding model modality alignment using generated data. During pre-training, it is common to train on all available data due to uncertainty about data selection. Here, we offer 100k high-quality synthetic data as a benchmark for selecting aligned generated data efficiently.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.2.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>. </span><span id="S4.T6.3.2" class="ltx_text" style="font-size:90%;">Comparison of data utilization for generating image-caption pairs. This indicates that our SynthVLM have superior efficiency compared to other methods.</span></figcaption>
<table id="S4.T6.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.4.1.1" class="ltx_tr">
<th id="S4.T6.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T6.4.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T6.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.4.1.1.2.1" class="ltx_text ltx_font_bold">SynthVLM</span></th>
<th id="S4.T6.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.4.1.1.3.1" class="ltx_text ltx_font_bold">LLaVA</span></th>
<th id="S4.T6.4.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.4.1.1.4.1" class="ltx_text ltx_font_bold">w/o selection</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.4.2.1" class="ltx_tr">
<th id="S4.T6.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T6.4.2.1.1.1" class="ltx_text ltx_font_bold">Dataset Number (k)</span></th>
<td id="S4.T6.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">100</td>
<td id="S4.T6.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">558</td>
<td id="S4.T6.4.2.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">1000</td>
</tr>
<tr id="S4.T6.4.3.2" class="ltx_tr">
<th id="S4.T6.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T6.4.3.2.1.1" class="ltx_text ltx_font_bold">Data Usage</span></th>
<td id="S4.T6.4.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">33MB</td>
<td id="S4.T6.4.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">27GB</td>
<td id="S4.T6.4.3.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">330MB</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Efficient Vision Language Alignment</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To address <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Q3</span>, we examine the computational resource usage during training and evaluate data utilization efficiency for generating image-caption pairs.</p>
</div>
<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Computational Resource Usage</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">As shown in Table <a href="#S4.T6" title="Table 6 ‣ 4.3. Effective Vision Language Alignment ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, by integrating a data selection module, our approach utilizes only 19% of the LLAVA data and 10% of the original synthetic data while achieving SoTA performance. This demonstrates that our data selection method can reduce computational usage by more than 80%.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Data Utilization Efficiency</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">Table <a href="#S4.T6" title="Table 6 ‣ 4.3. Effective Vision Language Alignment ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> compares the data utilization of our proposed method with the conventional BLIP-LCS method, which employs the BLIP model for caption generation. Our approach is significantly more efficient, requiring only 330 MB for captions to generate 1,000,000 image-caption pairs, compared to the traditional methods, which may exceed 50 GB for images. This substantial difference highlights the efficiency of relying solely on captions rather than images in generating image-caption pairs.</p>
</div>
<div id="S4.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p2.1" class="ltx_p">Overall, our method efficiently aligns image and text modalities, demonstrating strong potential for effective modality alignment. Additionally, acquiring captions is considerably less resource-intensive than obtaining images. By integrating existing large language models (LLMs), it is feasible to fully automate the generation of both captions and images, further enhancing the efficiency of dataset construction in VLLMs.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Privacy Protection Pre-training</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">To address <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">Q4</span>, we compare the synthetic image and the original image in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.5. Privacy Protection Pre-training ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Figure <a href="#S4.F7" title="Figure 7 ‣ 4.5. Privacy Protection Pre-training ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We can see synthetic data offers significant advantages in protecting data privacy.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">As illustrated in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.5. Privacy Protection Pre-training ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, synthetic image (a) effectively avoids privacy issues by not representing real human faces, while original image (b) contains human faces, potentially leading to privacy concerns. Similarly, in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.5. Privacy Protection Pre-training ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, synthetic images in (a) show vehicles and tickets without revealing real license plates and ticket information, ensuring privacy protection. In contrast, original images in (b) display actual license plates and ticket information, which can potentially lead to privacy issues.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">By utilizing generative models like DDPM, synthetic data can be created with similar statistical properties to real data without involving actual personal information. This provides a secure pathway for data sharing, model training, and analysis, helping to comply with privacy regulations, protecting user privacy, and simultaneously advancing the fields of artificial intelligence and machine learning.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.44.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>. </span><span id="S4.T7.45.2" class="ltx_text" style="font-size:90%;">Ablation study of visual understanding ability and pure language ability. The results demonstrate that removing either the data generation or data selection module results in a performance drop.</span></figcaption>
<div id="S4.T7.42" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:84.6pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-108.6pt,21.0pt) scale(0.666166868591746,0.666166868591746) ;">
<table id="S4.T7.42.42" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.2.2.2" class="ltx_tr">
<th id="S4.T7.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.3.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T7.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.4.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T7.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.5.1" class="ltx_text ltx_font_bold">SQA</span></th>
<th id="S4.T7.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.6.1" class="ltx_text ltx_font_bold">SQA_Img</span></th>
<th id="S4.T7.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.7.1" class="ltx_text ltx_font_bold">MMVet</span></th>
<th id="S4.T7.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.8.1" class="ltx_text ltx_font_bold">VizWiz</span></th>
<th id="S4.T7.2.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.9.1" class="ltx_text ltx_font_bold">VQAv2</span></th>
<th id="S4.T7.2.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.10.1" class="ltx_text ltx_font_bold">GQA</span></th>
<th id="S4.T7.2.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.11.1" class="ltx_text ltx_font_bold">MMB</span></th>
<th id="S4.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.1.1.1.1.1" class="ltx_text ltx_font_bold">MME<sup id="S4.T7.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T7.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">P</span></sup></span></th>
<th id="S4.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.2.1" class="ltx_text ltx_font_bold">MME<sup id="S4.T7.2.2.2.2.1.1" class="ltx_sup"><span id="S4.T7.2.2.2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">C</span></sup></span></th>
<th id="S4.T7.2.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.12.1" class="ltx_text ltx_font_bold">PoPE</span></th>
<th id="S4.T7.2.2.2.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.2.2.2.13.1" class="ltx_text ltx_font_bold">MMLU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.42.42.43.1" class="ltx_tr">
<td id="S4.T7.42.42.43.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.42.42.43.1.1.1" class="ltx_text ltx_font_bold">Synth Select 100k</span></td>
<td id="S4.T7.42.42.43.1.2" class="ltx_td ltx_align_center ltx_border_t">Vicuna-1.5-7B</td>
<td id="S4.T7.42.42.43.1.3" class="ltx_td ltx_align_center ltx_border_t">70.4</td>
<td id="S4.T7.42.42.43.1.4" class="ltx_td ltx_align_center ltx_border_t">68.9</td>
<td id="S4.T7.42.42.43.1.5" class="ltx_td ltx_align_center ltx_border_t">32.2</td>
<td id="S4.T7.42.42.43.1.6" class="ltx_td ltx_align_center ltx_border_t">49.3</td>
<td id="S4.T7.42.42.43.1.7" class="ltx_td ltx_align_center ltx_border_t">79.4</td>
<td id="S4.T7.42.42.43.1.8" class="ltx_td ltx_align_center ltx_border_t">63.1</td>
<td id="S4.T7.42.42.43.1.9" class="ltx_td ltx_align_center ltx_border_t">66.8</td>
<td id="S4.T7.42.42.43.1.10" class="ltx_td ltx_align_center ltx_border_t">1518.5</td>
<td id="S4.T7.42.42.43.1.11" class="ltx_td ltx_align_center ltx_border_t">345.7</td>
<td id="S4.T7.42.42.43.1.12" class="ltx_td ltx_align_center ltx_border_t">87.0</td>
<td id="S4.T7.42.42.43.1.13" class="ltx_td ltx_align_center ltx_border_t">41.2</td>
</tr>
<tr id="S4.T7.13.13.13" class="ltx_tr">
<td id="S4.T7.13.13.13.12" class="ltx_td ltx_align_center"><span id="S4.T7.13.13.13.12.1" class="ltx_text ltx_font_bold">w/o generation 100k</span></td>
<td id="S4.T7.13.13.13.13" class="ltx_td ltx_align_center">Vicuna-1.5-7B</td>
<td id="S4.T7.3.3.3.1" class="ltx_td ltx_align_center">69.3<math id="S4.T7.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.3.3.3.1.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.3.3.3.1.m1.1.1" xref="S4.T7.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.1.m1.1b"><ci id="S4.T7.3.3.3.1.m1.1.1.cmml" xref="S4.T7.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.4.4.4.2" class="ltx_td ltx_align_center">67.0<math id="S4.T7.4.4.4.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.4.4.4.2.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.4.4.4.2.m1.1.1" xref="S4.T7.4.4.4.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.2.m1.1b"><ci id="S4.T7.4.4.4.2.m1.1.1.cmml" xref="S4.T7.4.4.4.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.5.5.5.3" class="ltx_td ltx_align_center">31.2<math id="S4.T7.5.5.5.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.5.5.5.3.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.5.5.5.3.m1.1.1" xref="S4.T7.5.5.5.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.5.5.5.3.m1.1b"><ci id="S4.T7.5.5.5.3.m1.1.1.cmml" xref="S4.T7.5.5.5.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.5.5.5.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.6.6.6.4" class="ltx_td ltx_align_center">46.8<math id="S4.T7.6.6.6.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.6.6.6.4.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.6.6.6.4.m1.1.1" xref="S4.T7.6.6.6.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.6.6.6.4.m1.1b"><ci id="S4.T7.6.6.6.4.m1.1.1.cmml" xref="S4.T7.6.6.6.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.6.6.6.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.7.7.7.5" class="ltx_td ltx_align_center">79.3<math id="S4.T7.7.7.7.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.7.7.7.5.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.7.7.7.5.m1.1.1" xref="S4.T7.7.7.7.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.7.7.7.5.m1.1b"><ci id="S4.T7.7.7.7.5.m1.1.1.cmml" xref="S4.T7.7.7.7.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.7.7.7.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.8.8.8.6" class="ltx_td ltx_align_center">62.9<math id="S4.T7.8.8.8.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.8.8.8.6.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.8.8.8.6.m1.1.1" xref="S4.T7.8.8.8.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.8.8.8.6.m1.1b"><ci id="S4.T7.8.8.8.6.m1.1.1.cmml" xref="S4.T7.8.8.8.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.8.8.8.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.9.9.9.7" class="ltx_td ltx_align_center">66.2<math id="S4.T7.9.9.9.7.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.9.9.9.7.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.9.9.9.7.m1.1.1" xref="S4.T7.9.9.9.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.9.9.9.7.m1.1b"><ci id="S4.T7.9.9.9.7.m1.1.1.cmml" xref="S4.T7.9.9.9.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.9.9.9.7.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.10.10.10.8" class="ltx_td ltx_align_center">1488.8<math id="S4.T7.10.10.10.8.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.10.10.10.8.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.10.10.10.8.m1.1.1" xref="S4.T7.10.10.10.8.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.10.10.10.8.m1.1b"><ci id="S4.T7.10.10.10.8.m1.1.1.cmml" xref="S4.T7.10.10.10.8.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.10.10.10.8.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.11.11.11.9" class="ltx_td ltx_align_center">327.5<math id="S4.T7.11.11.11.9.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.11.11.11.9.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.11.11.11.9.m1.1.1" xref="S4.T7.11.11.11.9.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.11.11.11.9.m1.1b"><ci id="S4.T7.11.11.11.9.m1.1.1.cmml" xref="S4.T7.11.11.11.9.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.11.11.11.9.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.12.12.12.10" class="ltx_td ltx_align_center">86.2<math id="S4.T7.12.12.12.10.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.12.12.12.10.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.12.12.12.10.m1.1.1" xref="S4.T7.12.12.12.10.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.12.12.12.10.m1.1b"><ci id="S4.T7.12.12.12.10.m1.1.1.cmml" xref="S4.T7.12.12.12.10.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.12.12.12.10.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.13.13.13.11" class="ltx_td ltx_align_center">39.1<math id="S4.T7.13.13.13.11.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.13.13.13.11.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.13.13.13.11.m1.1.1" xref="S4.T7.13.13.13.11.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.13.13.13.11.m1.1b"><ci id="S4.T7.13.13.13.11.m1.1.1.cmml" xref="S4.T7.13.13.13.11.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.13.13.13.11.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T7.22.22.22" class="ltx_tr">
<td id="S4.T7.22.22.22.10" class="ltx_td ltx_align_center"><span id="S4.T7.22.22.22.10.1" class="ltx_text ltx_font_bold">w/o selection 100k</span></td>
<td id="S4.T7.22.22.22.11" class="ltx_td ltx_align_center">Vicuna-1.5-7B</td>
<td id="S4.T7.14.14.14.1" class="ltx_td ltx_align_center">69.9<math id="S4.T7.14.14.14.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.14.14.14.1.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.14.14.14.1.m1.1.1" xref="S4.T7.14.14.14.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.14.14.14.1.m1.1b"><ci id="S4.T7.14.14.14.1.m1.1.1.cmml" xref="S4.T7.14.14.14.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.14.14.14.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.15.15.15.2" class="ltx_td ltx_align_center">67.7<math id="S4.T7.15.15.15.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.15.15.15.2.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.15.15.15.2.m1.1.1" xref="S4.T7.15.15.15.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.15.15.15.2.m1.1b"><ci id="S4.T7.15.15.15.2.m1.1.1.cmml" xref="S4.T7.15.15.15.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.15.15.15.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.16.16.16.3" class="ltx_td ltx_align_center">30.2<math id="S4.T7.16.16.16.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.16.16.16.3.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.16.16.16.3.m1.1.1" xref="S4.T7.16.16.16.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.16.16.16.3.m1.1b"><ci id="S4.T7.16.16.16.3.m1.1.1.cmml" xref="S4.T7.16.16.16.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.16.16.16.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.22.22.22.12" class="ltx_td ltx_align_center">50.2</td>
<td id="S4.T7.17.17.17.4" class="ltx_td ltx_align_center">79.1<math id="S4.T7.17.17.17.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.17.17.17.4.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.17.17.17.4.m1.1.1" xref="S4.T7.17.17.17.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.17.17.17.4.m1.1b"><ci id="S4.T7.17.17.17.4.m1.1.1.cmml" xref="S4.T7.17.17.17.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.17.17.17.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.18.18.18.5" class="ltx_td ltx_align_center">62.2<math id="S4.T7.18.18.18.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.18.18.18.5.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.18.18.18.5.m1.1.1" xref="S4.T7.18.18.18.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.18.18.18.5.m1.1b"><ci id="S4.T7.18.18.18.5.m1.1.1.cmml" xref="S4.T7.18.18.18.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.18.18.18.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.19.19.19.6" class="ltx_td ltx_align_center">63.5<math id="S4.T7.19.19.19.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.19.19.19.6.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.19.19.19.6.m1.1.1" xref="S4.T7.19.19.19.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.19.19.19.6.m1.1b"><ci id="S4.T7.19.19.19.6.m1.1.1.cmml" xref="S4.T7.19.19.19.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.19.19.19.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.20.20.20.7" class="ltx_td ltx_align_center">1421.7<math id="S4.T7.20.20.20.7.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.20.20.20.7.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.20.20.20.7.m1.1.1" xref="S4.T7.20.20.20.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.20.20.20.7.m1.1b"><ci id="S4.T7.20.20.20.7.m1.1.1.cmml" xref="S4.T7.20.20.20.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.20.20.20.7.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.21.21.21.8" class="ltx_td ltx_align_center">301.8<math id="S4.T7.21.21.21.8.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.21.21.21.8.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.21.21.21.8.m1.1.1" xref="S4.T7.21.21.21.8.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.21.21.21.8.m1.1b"><ci id="S4.T7.21.21.21.8.m1.1.1.cmml" xref="S4.T7.21.21.21.8.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.21.21.21.8.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.22.22.22.13" class="ltx_td ltx_align_center">87.3</td>
<td id="S4.T7.22.22.22.9" class="ltx_td ltx_align_center">40.6<math id="S4.T7.22.22.22.9.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.22.22.22.9.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.22.22.22.9.m1.1.1" xref="S4.T7.22.22.22.9.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.22.22.22.9.m1.1b"><ci id="S4.T7.22.22.22.9.m1.1.1.cmml" xref="S4.T7.22.22.22.9.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.22.22.22.9.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T7.42.42.44.2" class="ltx_tr">
<td id="S4.T7.42.42.44.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.42.42.44.2.1.1" class="ltx_text ltx_font_bold">Synth Select 100k</span></td>
<td id="S4.T7.42.42.44.2.2" class="ltx_td ltx_align_center ltx_border_t">Vicuna-1.5-13B</td>
<td id="S4.T7.42.42.44.2.3" class="ltx_td ltx_align_center ltx_border_t">74.9</td>
<td id="S4.T7.42.42.44.2.4" class="ltx_td ltx_align_center ltx_border_t">72.5</td>
<td id="S4.T7.42.42.44.2.5" class="ltx_td ltx_align_center ltx_border_t">35.0</td>
<td id="S4.T7.42.42.44.2.6" class="ltx_td ltx_align_center ltx_border_t">55.9</td>
<td id="S4.T7.42.42.44.2.7" class="ltx_td ltx_align_center ltx_border_t">80.0</td>
<td id="S4.T7.42.42.44.2.8" class="ltx_td ltx_align_center ltx_border_t">63.5</td>
<td id="S4.T7.42.42.44.2.9" class="ltx_td ltx_align_center ltx_border_t">68.3</td>
<td id="S4.T7.42.42.44.2.10" class="ltx_td ltx_align_center ltx_border_t">1573.0</td>
<td id="S4.T7.42.42.44.2.11" class="ltx_td ltx_align_center ltx_border_t">316.1</td>
<td id="S4.T7.42.42.44.2.12" class="ltx_td ltx_align_center ltx_border_t">88.4</td>
<td id="S4.T7.42.42.44.2.13" class="ltx_td ltx_align_center ltx_border_t">54.6</td>
</tr>
<tr id="S4.T7.32.32.32" class="ltx_tr">
<td id="S4.T7.32.32.32.11" class="ltx_td ltx_align_center"><span id="S4.T7.32.32.32.11.1" class="ltx_text ltx_font_bold">w/o generation 100k</span></td>
<td id="S4.T7.32.32.32.12" class="ltx_td ltx_align_center">Vicuna-1.5-13B</td>
<td id="S4.T7.23.23.23.1" class="ltx_td ltx_align_center">73.6<math id="S4.T7.23.23.23.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.23.23.23.1.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.23.23.23.1.m1.1.1" xref="S4.T7.23.23.23.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.23.23.23.1.m1.1b"><ci id="S4.T7.23.23.23.1.m1.1.1.cmml" xref="S4.T7.23.23.23.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.23.23.23.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.24.24.24.2" class="ltx_td ltx_align_center">71.4<math id="S4.T7.24.24.24.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.24.24.24.2.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.24.24.24.2.m1.1.1" xref="S4.T7.24.24.24.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.24.24.24.2.m1.1b"><ci id="S4.T7.24.24.24.2.m1.1.1.cmml" xref="S4.T7.24.24.24.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.24.24.24.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.25.25.25.3" class="ltx_td ltx_align_center">33.0<math id="S4.T7.25.25.25.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.25.25.25.3.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.25.25.25.3.m1.1.1" xref="S4.T7.25.25.25.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.25.25.25.3.m1.1b"><ci id="S4.T7.25.25.25.3.m1.1.1.cmml" xref="S4.T7.25.25.25.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.25.25.25.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.26.26.26.4" class="ltx_td ltx_align_center">53.6<math id="S4.T7.26.26.26.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.26.26.26.4.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.26.26.26.4.m1.1.1" xref="S4.T7.26.26.26.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.26.26.26.4.m1.1b"><ci id="S4.T7.26.26.26.4.m1.1.1.cmml" xref="S4.T7.26.26.26.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.26.26.26.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.32.32.32.13" class="ltx_td ltx_align_center">80.0</td>
<td id="S4.T7.27.27.27.5" class="ltx_td ltx_align_center">63.4<math id="S4.T7.27.27.27.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.27.27.27.5.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.27.27.27.5.m1.1.1" xref="S4.T7.27.27.27.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.27.27.27.5.m1.1b"><ci id="S4.T7.27.27.27.5.m1.1.1.cmml" xref="S4.T7.27.27.27.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.27.27.27.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.28.28.28.6" class="ltx_td ltx_align_center">67.5<math id="S4.T7.28.28.28.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.28.28.28.6.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.28.28.28.6.m1.1.1" xref="S4.T7.28.28.28.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.28.28.28.6.m1.1b"><ci id="S4.T7.28.28.28.6.m1.1.1.cmml" xref="S4.T7.28.28.28.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.28.28.28.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.29.29.29.7" class="ltx_td ltx_align_center">1514.3<math id="S4.T7.29.29.29.7.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.29.29.29.7.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.29.29.29.7.m1.1.1" xref="S4.T7.29.29.29.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.29.29.29.7.m1.1b"><ci id="S4.T7.29.29.29.7.m1.1.1.cmml" xref="S4.T7.29.29.29.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.29.29.29.7.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.30.30.30.8" class="ltx_td ltx_align_center">295.7<math id="S4.T7.30.30.30.8.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.30.30.30.8.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.30.30.30.8.m1.1.1" xref="S4.T7.30.30.30.8.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.30.30.30.8.m1.1b"><ci id="S4.T7.30.30.30.8.m1.1.1.cmml" xref="S4.T7.30.30.30.8.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.30.30.30.8.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.31.31.31.9" class="ltx_td ltx_align_center">88.2<math id="S4.T7.31.31.31.9.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.31.31.31.9.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.31.31.31.9.m1.1.1" xref="S4.T7.31.31.31.9.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.31.31.31.9.m1.1b"><ci id="S4.T7.31.31.31.9.m1.1.1.cmml" xref="S4.T7.31.31.31.9.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.31.31.31.9.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.32.32.32.10" class="ltx_td ltx_align_center">53.6<math id="S4.T7.32.32.32.10.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.32.32.32.10.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.32.32.32.10.m1.1.1" xref="S4.T7.32.32.32.10.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.32.32.32.10.m1.1b"><ci id="S4.T7.32.32.32.10.m1.1.1.cmml" xref="S4.T7.32.32.32.10.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.32.32.32.10.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T7.42.42.42" class="ltx_tr">
<td id="S4.T7.42.42.42.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.42.42.42.11.1" class="ltx_text ltx_font_bold">w/o selection 100k</span></td>
<td id="S4.T7.42.42.42.12" class="ltx_td ltx_align_center ltx_border_bb">Vicuna-1.5-13B</td>
<td id="S4.T7.33.33.33.1" class="ltx_td ltx_align_center ltx_border_bb">74.1<math id="S4.T7.33.33.33.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.33.33.33.1.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.33.33.33.1.m1.1.1" xref="S4.T7.33.33.33.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.33.33.33.1.m1.1b"><ci id="S4.T7.33.33.33.1.m1.1.1.cmml" xref="S4.T7.33.33.33.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.33.33.33.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.34.34.34.2" class="ltx_td ltx_align_center ltx_border_bb">70.5<math id="S4.T7.34.34.34.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.34.34.34.2.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.34.34.34.2.m1.1.1" xref="S4.T7.34.34.34.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.34.34.34.2.m1.1b"><ci id="S4.T7.34.34.34.2.m1.1.1.cmml" xref="S4.T7.34.34.34.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.34.34.34.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.42.42.42.13" class="ltx_td ltx_align_center ltx_border_bb">35.6</td>
<td id="S4.T7.35.35.35.3" class="ltx_td ltx_align_center ltx_border_bb">53.2<math id="S4.T7.35.35.35.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.35.35.35.3.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.35.35.35.3.m1.1.1" xref="S4.T7.35.35.35.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.35.35.35.3.m1.1b"><ci id="S4.T7.35.35.35.3.m1.1.1.cmml" xref="S4.T7.35.35.35.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.35.35.35.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.36.36.36.4" class="ltx_td ltx_align_center ltx_border_bb">79.7<math id="S4.T7.36.36.36.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.36.36.36.4.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.36.36.36.4.m1.1.1" xref="S4.T7.36.36.36.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.36.36.36.4.m1.1b"><ci id="S4.T7.36.36.36.4.m1.1.1.cmml" xref="S4.T7.36.36.36.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.36.36.36.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.37.37.37.5" class="ltx_td ltx_align_center ltx_border_bb">63.1<math id="S4.T7.37.37.37.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.37.37.37.5.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.37.37.37.5.m1.1.1" xref="S4.T7.37.37.37.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.37.37.37.5.m1.1b"><ci id="S4.T7.37.37.37.5.m1.1.1.cmml" xref="S4.T7.37.37.37.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.37.37.37.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.38.38.38.6" class="ltx_td ltx_align_center ltx_border_bb">67.5<math id="S4.T7.38.38.38.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.38.38.38.6.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.38.38.38.6.m1.1.1" xref="S4.T7.38.38.38.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.38.38.38.6.m1.1b"><ci id="S4.T7.38.38.38.6.m1.1.1.cmml" xref="S4.T7.38.38.38.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.38.38.38.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.39.39.39.7" class="ltx_td ltx_align_center ltx_border_bb">1512.7<math id="S4.T7.39.39.39.7.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.39.39.39.7.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.39.39.39.7.m1.1.1" xref="S4.T7.39.39.39.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.39.39.39.7.m1.1b"><ci id="S4.T7.39.39.39.7.m1.1.1.cmml" xref="S4.T7.39.39.39.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.39.39.39.7.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.40.40.40.8" class="ltx_td ltx_align_center ltx_border_bb">303.2<math id="S4.T7.40.40.40.8.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.40.40.40.8.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.40.40.40.8.m1.1.1" xref="S4.T7.40.40.40.8.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.40.40.40.8.m1.1b"><ci id="S4.T7.40.40.40.8.m1.1.1.cmml" xref="S4.T7.40.40.40.8.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.40.40.40.8.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.41.41.41.9" class="ltx_td ltx_align_center ltx_border_bb">86.9<math id="S4.T7.41.41.41.9.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.41.41.41.9.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.41.41.41.9.m1.1.1" xref="S4.T7.41.41.41.9.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.41.41.41.9.m1.1b"><ci id="S4.T7.41.41.41.9.m1.1.1.cmml" xref="S4.T7.41.41.41.9.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.41.41.41.9.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T7.42.42.42.10" class="ltx_td ltx_align_center ltx_border_bb">53.0<math id="S4.T7.42.42.42.10.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.42.42.42.10.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T7.42.42.42.10.m1.1.1" xref="S4.T7.42.42.42.10.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.42.42.42.10.m1.1b"><ci id="S4.T7.42.42.42.10.m1.1.1.cmml" xref="S4.T7.42.42.42.10.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.42.42.42.10.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T8.26.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>. </span><span id="S4.T8.27.2" class="ltx_text" style="font-size:90%;">Ablation study of modality alignment. The results demonstrate that removing either the data generation or data selection module results in a performance drop.</span></figcaption>
<table id="S4.T8.24" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.24.25.1" class="ltx_tr">
<th id="S4.T8.24.25.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T8.24.25.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T8.24.25.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T8.24.25.1.2.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T8.24.25.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T8.24.25.1.3.1" class="ltx_text ltx_font_bold">SQA</span></th>
<th id="S4.T8.24.25.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5"><span id="S4.T8.24.25.1.4.1" class="ltx_text ltx_font_bold">MMLU</span></th>
</tr>
<tr id="S4.T8.24.26.2" class="ltx_tr">
<th id="S4.T8.24.26.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.24.26.2.1.1" class="ltx_text ltx_font_bold">Avg</span></th>
<th id="S4.T8.24.26.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.24.26.2.2.1" class="ltx_text ltx_font_bold">STEM</span></th>
<th id="S4.T8.24.26.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.24.26.2.3.1" class="ltx_text ltx_font_bold">Humanities</span></th>
<th id="S4.T8.24.26.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T8.24.26.2.4.1" class="ltx_text ltx_font_bold">Social Sciences</span></th>
<th id="S4.T8.24.26.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T8.24.26.2.5.1" class="ltx_text ltx_font_bold">Other</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.24.27.1" class="ltx_tr">
<th id="S4.T8.24.27.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T8.24.27.1.1.1" class="ltx_text ltx_font_bold">Synth Select 100k</span></th>
<td id="S4.T8.24.27.1.2" class="ltx_td ltx_align_center ltx_border_t">Vicuna-1.5-7B</td>
<td id="S4.T8.24.27.1.3" class="ltx_td ltx_align_center ltx_border_t">70.4</td>
<td id="S4.T8.24.27.1.4" class="ltx_td ltx_align_center ltx_border_t">41.2</td>
<td id="S4.T8.24.27.1.5" class="ltx_td ltx_align_center ltx_border_t">31.7</td>
<td id="S4.T8.24.27.1.6" class="ltx_td ltx_align_center ltx_border_t">37.4</td>
<td id="S4.T8.24.27.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.0</td>
<td id="S4.T8.24.27.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">50.2</td>
</tr>
<tr id="S4.T8.6.6" class="ltx_tr">
<th id="S4.T8.6.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T8.6.6.7.1" class="ltx_text ltx_font_bold">w/o generation</span></th>
<td id="S4.T8.6.6.8" class="ltx_td ltx_align_center">Vicuna-1.5-7B</td>
<td id="S4.T8.1.1.1" class="ltx_td ltx_align_center">69.3<math id="S4.T8.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.1.1.1.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.1.1.1.m1.1.1" xref="S4.T8.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.m1.1b"><ci id="S4.T8.1.1.1.m1.1.1.cmml" xref="S4.T8.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.2.2.2" class="ltx_td ltx_align_center">39.1<math id="S4.T8.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.2.2.2.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.2.2.2.m1.1.1" xref="S4.T8.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.2.2.2.m1.1b"><ci id="S4.T8.2.2.2.m1.1.1.cmml" xref="S4.T8.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.3.3.3" class="ltx_td ltx_align_center">30.0<math id="S4.T8.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.3.3.3.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.3.3.3.m1.1.1" xref="S4.T8.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.3.3.3.m1.1b"><ci id="S4.T8.3.3.3.m1.1.1.cmml" xref="S4.T8.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.4.4.4" class="ltx_td ltx_align_center">36.6<math id="S4.T8.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.4.4.4.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.4.4.4.m1.1.1" xref="S4.T8.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.4.4.4.m1.1b"><ci id="S4.T8.4.4.4.m1.1.1.cmml" xref="S4.T8.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.5.5.5" class="ltx_td ltx_align_center ltx_border_r">43.1<math id="S4.T8.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.5.5.5.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.5.5.5.m1.1.1" xref="S4.T8.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.5.5.5.m1.1b"><ci id="S4.T8.5.5.5.m1.1.1.cmml" xref="S4.T8.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center">47.3<math id="S4.T8.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.6.6.6.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.6.6.6.m1.1.1" xref="S4.T8.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.6.6.6.m1.1b"><ci id="S4.T8.6.6.6.m1.1.1.cmml" xref="S4.T8.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T8.12.12" class="ltx_tr">
<th id="S4.T8.12.12.7" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T8.12.12.7.1" class="ltx_text ltx_font_bold">w/o selection</span></th>
<td id="S4.T8.12.12.8" class="ltx_td ltx_align_center">Vicuna-1.5-7B</td>
<td id="S4.T8.7.7.1" class="ltx_td ltx_align_center">69.9<math id="S4.T8.7.7.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.7.7.1.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.7.7.1.m1.1.1" xref="S4.T8.7.7.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.7.7.1.m1.1b"><ci id="S4.T8.7.7.1.m1.1.1.cmml" xref="S4.T8.7.7.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.7.7.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.8.8.2" class="ltx_td ltx_align_center">40.6<math id="S4.T8.8.8.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.8.8.2.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.8.8.2.m1.1.1" xref="S4.T8.8.8.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.8.8.2.m1.1b"><ci id="S4.T8.8.8.2.m1.1.1.cmml" xref="S4.T8.8.8.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.8.8.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.9.9.3" class="ltx_td ltx_align_center">30.8<math id="S4.T8.9.9.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.9.9.3.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.9.9.3.m1.1.1" xref="S4.T8.9.9.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.9.9.3.m1.1b"><ci id="S4.T8.9.9.3.m1.1.1.cmml" xref="S4.T8.9.9.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.9.9.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.10.10.4" class="ltx_td ltx_align_center">37.2<math id="S4.T8.10.10.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.10.10.4.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.10.10.4.m1.1.1" xref="S4.T8.10.10.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.10.10.4.m1.1b"><ci id="S4.T8.10.10.4.m1.1.1.cmml" xref="S4.T8.10.10.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.10.10.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.11.11.5" class="ltx_td ltx_align_center ltx_border_r">45.3<math id="S4.T8.11.11.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.11.11.5.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.11.11.5.m1.1.1" xref="S4.T8.11.11.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.11.11.5.m1.1b"><ci id="S4.T8.11.11.5.m1.1.1.cmml" xref="S4.T8.11.11.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.11.11.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.12.12.6" class="ltx_td ltx_nopad_r ltx_align_center">48.9<math id="S4.T8.12.12.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.12.12.6.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.12.12.6.m1.1.1" xref="S4.T8.12.12.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.12.12.6.m1.1b"><ci id="S4.T8.12.12.6.m1.1.1.cmml" xref="S4.T8.12.12.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.12.12.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T8.24.28.2" class="ltx_tr">
<th id="S4.T8.24.28.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T8.24.28.2.1.1" class="ltx_text ltx_font_bold">Synth Select 100k</span></th>
<td id="S4.T8.24.28.2.2" class="ltx_td ltx_align_center ltx_border_t">Vicuna-1.5-13B</td>
<td id="S4.T8.24.28.2.3" class="ltx_td ltx_align_center ltx_border_t">74.9</td>
<td id="S4.T8.24.28.2.4" class="ltx_td ltx_align_center ltx_border_t">54.6</td>
<td id="S4.T8.24.28.2.5" class="ltx_td ltx_align_center ltx_border_t">45.0</td>
<td id="S4.T8.24.28.2.6" class="ltx_td ltx_align_center ltx_border_t">49.3</td>
<td id="S4.T8.24.28.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64.0</td>
<td id="S4.T8.24.28.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">62.2</td>
</tr>
<tr id="S4.T8.18.18" class="ltx_tr">
<th id="S4.T8.18.18.7" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T8.18.18.7.1" class="ltx_text ltx_font_bold">w/o generation</span></th>
<td id="S4.T8.18.18.8" class="ltx_td ltx_align_center">Vicuna-1.5-13B</td>
<td id="S4.T8.13.13.1" class="ltx_td ltx_align_center">74.1<math id="S4.T8.13.13.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.13.13.1.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.13.13.1.m1.1.1" xref="S4.T8.13.13.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.13.13.1.m1.1b"><ci id="S4.T8.13.13.1.m1.1.1.cmml" xref="S4.T8.13.13.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.13.13.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.14.14.2" class="ltx_td ltx_align_center">53.6<math id="S4.T8.14.14.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.14.14.2.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.14.14.2.m1.1.1" xref="S4.T8.14.14.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.14.14.2.m1.1b"><ci id="S4.T8.14.14.2.m1.1.1.cmml" xref="S4.T8.14.14.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.14.14.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.15.15.3" class="ltx_td ltx_align_center">43.5<math id="S4.T8.15.15.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.15.15.3.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.15.15.3.m1.1.1" xref="S4.T8.15.15.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.15.15.3.m1.1b"><ci id="S4.T8.15.15.3.m1.1.1.cmml" xref="S4.T8.15.15.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.15.15.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.16.16.4" class="ltx_td ltx_align_center">48.2<math id="S4.T8.16.16.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.16.16.4.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.16.16.4.m1.1.1" xref="S4.T8.16.16.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.16.16.4.m1.1b"><ci id="S4.T8.16.16.4.m1.1.1.cmml" xref="S4.T8.16.16.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.16.16.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.17.17.5" class="ltx_td ltx_align_center ltx_border_r">63.1<math id="S4.T8.17.17.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.17.17.5.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.17.17.5.m1.1.1" xref="S4.T8.17.17.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.17.17.5.m1.1b"><ci id="S4.T8.17.17.5.m1.1.1.cmml" xref="S4.T8.17.17.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.17.17.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.18.18.6" class="ltx_td ltx_nopad_r ltx_align_center">61.8<math id="S4.T8.18.18.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.18.18.6.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.18.18.6.m1.1.1" xref="S4.T8.18.18.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.18.18.6.m1.1b"><ci id="S4.T8.18.18.6.m1.1.1.cmml" xref="S4.T8.18.18.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.18.18.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T8.24.24" class="ltx_tr">
<th id="S4.T8.24.24.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T8.24.24.7.1" class="ltx_text ltx_font_bold">w/o selection</span></th>
<td id="S4.T8.24.24.8" class="ltx_td ltx_align_center ltx_border_bb">Vicuna-1.5-13B</td>
<td id="S4.T8.19.19.1" class="ltx_td ltx_align_center ltx_border_bb">73.6<math id="S4.T8.19.19.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.19.19.1.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.19.19.1.m1.1.1" xref="S4.T8.19.19.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.19.19.1.m1.1b"><ci id="S4.T8.19.19.1.m1.1.1.cmml" xref="S4.T8.19.19.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.19.19.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.20.20.2" class="ltx_td ltx_align_center ltx_border_bb">53.0<math id="S4.T8.20.20.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.20.20.2.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.20.20.2.m1.1.1" xref="S4.T8.20.20.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.20.20.2.m1.1b"><ci id="S4.T8.20.20.2.m1.1.1.cmml" xref="S4.T8.20.20.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.20.20.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.21.21.3" class="ltx_td ltx_align_center ltx_border_bb">42.9<math id="S4.T8.21.21.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.21.21.3.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.21.21.3.m1.1.1" xref="S4.T8.21.21.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.21.21.3.m1.1b"><ci id="S4.T8.21.21.3.m1.1.1.cmml" xref="S4.T8.21.21.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.21.21.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.22.22.4" class="ltx_td ltx_align_center ltx_border_bb">46.8<math id="S4.T8.22.22.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.22.22.4.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.22.22.4.m1.1.1" xref="S4.T8.22.22.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.22.22.4.m1.1b"><ci id="S4.T8.22.22.4.m1.1.1.cmml" xref="S4.T8.22.22.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.22.22.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.23.23.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">63.8<math id="S4.T8.23.23.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.23.23.5.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.23.23.5.m1.1.1" xref="S4.T8.23.23.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.23.23.5.m1.1b"><ci id="S4.T8.23.23.5.m1.1.1.cmml" xref="S4.T8.23.23.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.23.23.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T8.24.24.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">61.3<math id="S4.T8.24.24.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T8.24.24.6.m1.1a"><mo mathcolor="#0000FF" stretchy="false" id="S4.T8.24.24.6.m1.1.1" xref="S4.T8.24.24.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T8.24.24.6.m1.1b"><ci id="S4.T8.24.24.6.m1.1.1.cmml" xref="S4.T8.24.24.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.24.24.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2407.20756/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">From (a), it is evident that synthetic images can avoid representing real human faces, while (b) contains human faces, potentially leading to privacy issues.</span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2407.20756/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">From (a), it is evident that synthetic images can avoid displaying real license plates and ticket information. In contrast, (b) contains actual license plates and ticket information, which can potentially lead to privacy issues.</span></figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Ablation Study</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">To address <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_bold">Q5</span>, we conducted the following ablation study. Specifically, we conducted an ablation study where we removed the data generation module and the data selection module separately to evaluate their individual contributions to the effectiveness of our data generation pipeline.</p>
</div>
<section id="S4.SS6.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Excluding Data Generation Module</h4>

<div id="S4.SS6.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS6.SSS0.Px1.p1.1" class="ltx_p">The exclusion of the data generation module significantly impacts the model’s performance, as illustrated in Tables <a href="#S4.T7" title="Table 7 ‣ 4.5. Privacy Protection Pre-training ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S4.T8" title="Table 8 ‣ 4.5. Privacy Protection Pre-training ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, labeled as ”w/o generation 100k”. The variant without this module demonstrates markedly lower accuracy across all benchmarks. These results emphasize the crucial role of the data generation process in sustaining the high performance of the SynthVLM model. This also underscores SynthVLM’s potential in constructing highly aligned datasets.</p>
</div>
</section>
<section id="S4.SS6.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Excluding Data Selection Module</h4>

<div id="S4.SS6.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS6.SSS0.Px2.p1.1" class="ltx_p">The absence of the data selection module similarly leads to a noticeable decline in performance, indicated as ”w/o selection 100k” in Tables <a href="#S4.T7" title="Table 7 ‣ 4.5. Privacy Protection Pre-training ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S4.T8" title="Table 8 ‣ 4.5. Privacy Protection Pre-training ‣ 4. Experiment ‣ SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Given the inherent randomness of diffusion models, which inevitably generate some low-quality images, the data selection module is crucial for removing these subpar elements.</p>
</div>
<div id="S4.SS6.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS6.SSS0.Px2.p2.1" class="ltx_p">Overall, the ablation study highlights the critical role of data generation and data selection modules in SynthVLM. These experiments provide valuable insights into the contributions of each module, guiding future improvements and optimizations of the SynthVLM model.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In recent years, with the development of VLLMs, data generation has become increasingly important. Synthetic images are crucial for LLMs due to the lack of high-quality data for extensive training. In this paper, we propose a new image generation pipeline for generating VLLMs pre-training data, providing a new paradigm for image generation for VLLMs. Remarkably, the synthetic data trained SynthVLM model outperforms the baseline using only 18% synthetic data. Additionally, it achieves SoTA alignment ability efficiently. Furthermore, SynthVLM protects data privacy by using synthetic datasets.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Tianyi Bai, Hao Liang, Binwang Wan, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, Conghui He, Binhang Yuan, and Wentao Zhang. 2024.

</span>
<span class="ltx_bibblock">A Survey of Multimodal Large Language Model from A Data-centric Perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.16640</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023b.

</span>
<span class="ltx_bibblock">ShareGPT4V: Improving Large Multi-Modal Models with Better Captions.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2311.12793 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib5.3.3.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al<span id="bib.bib5.4.1" class="ltx_text">.</span> 2023c.

</span>
<span class="ltx_bibblock">Alpagasus: Training a better alpaca with fewer data.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.08701</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zui Chen, Lei Cao, and Sam Madden. 2023a.

</span>
<span class="ltx_bibblock">Lingua manga: A generic large language model centric system for data curation.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.11702</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib7.3.3.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al<span id="bib.bib7.4.1" class="ltx_text">.</span> 2024.

</span>
<span class="ltx_bibblock">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In <em id="bib.bib7.5.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 24185–24198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu. 2024.

</span>
<span class="ltx_bibblock">Security and Privacy Challenges of Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2402.00888 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal and Nichol (2021)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander Quinn Nichol. 2021.

</span>
<span class="ltx_bibblock">Diffusion Models Beat GANs on Image Synthesis. In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual</em>. 8780–8794.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Qianlong Du, Chengqing Zong, and Jiajun Zhang. 2023.

</span>
<span class="ltx_bibblock">Mods: Model-oriented data selection for instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.15653</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu. 2024.

</span>
<span class="ltx_bibblock">Reformatted Alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2402.12219 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandez et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Raul Castro Fernandez, Aaron J Elmore, Michael J Franklin, Sanjay Krishnan, and Chenhao Tan. 2023.

</span>
<span class="ltx_bibblock">How large language models will disrupt data management.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Proceedings of the VLDB Endowment</em> 16, 11 (2023), 3302–3309.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadre et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah M. Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis,
Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. 2023.

</span>
<span class="ltx_bibblock">DataComp: In search of the next generation of multimodal datasets. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020.

</span>
<span class="ltx_bibblock">DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hessel et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021.

</span>
<span class="ltx_bibblock">CLIPScore: A Reference-free Evaluation Metric for Image Captioning. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>. Association for Computational Linguistics, 7514–7528.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.

</span>
<span class="ltx_bibblock">Denoising Diffusion Probabilistic Models. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jian et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yiren Jian, Chongyang Gao, and Soroush Vosoughi. 2023.

</span>
<span class="ltx_bibblock">Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2023f)</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023f.

</span>
<span class="ltx_bibblock">Otter: A Multi-Modal Model with In-Context Instruction Tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2305.03726 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, and Yangqiu Song. 2023a.

</span>
<span class="ltx_bibblock">Privacy in Large Language Models: Attacks, Defenses and Future Directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2310.10383 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 19730–19742.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 19730–19742.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022a.

</span>
<span class="ltx_bibblock">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, Vol. 162. 12888–12900.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib23.3.3.1" class="ltx_text">.</span> (2023d)</span>
<span class="ltx_bibblock">
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al<span id="bib.bib23.4.1" class="ltx_text">.</span> 2023d.

</span>
<span class="ltx_bibblock">Mvbench: A comprehensive multi-modal video understanding benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17005</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. 2022b.

</span>
<span class="ltx_bibblock">Grounded Language-Image Pre-training. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</em>. IEEE, 10955–10965.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2023e)</span>
<span class="ltx_bibblock">
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023e.

</span>
<span class="ltx_bibblock">Self-Alignment with Instruction Backtranslation.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2308.06259 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Harsha Nori, and Sergey Yekhanin. 2023.

</span>
<span class="ltx_bibblock">Differentially Private Synthetic Data via Foundation Model APIs 1: Images.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2305.15560 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03744</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b.

</span>
<span class="ltx_bibblock">Visual Instruction Tuning. In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2023d)</span>
<span class="ltx_bibblock">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. 2023d.

</span>
<span class="ltx_bibblock">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2303.05499 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023c.

</span>
<span class="ltx_bibblock">What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lou et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, and Wenpeng Yin. 2023.

</span>
<span class="ltx_bibblock">MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2312.02436 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Junyu Lu, Ruyi Gan, Dixiang Zhang, Xiaojun Wu, Ziwei Wu, Renliang Sun, Jiaxing Zhang, Pingjian Zhang, and Yan Song. 2023a.

</span>
<span class="ltx_bibblock">Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2312.05278 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023b.

</span>
<span class="ltx_bibblock"># InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, and Yu Tsao. 2022.

</span>
<span class="ltx_bibblock">Conditional Diffusion Probabilistic Model for Speech Enhancement. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022</em>. 7402–7406.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">meta llama (2024)</span>
<span class="ltx_bibblock">
meta llama. 2024.

</span>
<span class="ltx_bibblock">Introducing Meta Llama 3: The most capable openly available LLM to date.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://ai.meta.com/blog/meta-llama-3/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.meta.com/blog/meta-llama-3/</a>

</span>
<span class="ltx_bibblock">Accessed: 2024-05-02.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Xupeng Miao, Zhihao Jia, and Bin Cui. 2024.

</span>
<span class="ltx_bibblock">Demystifying Data Management for Large Language Models. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Companion of the 2024 International Conference on Management of Data</em>. 547–555.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. 2023.

</span>
<span class="ltx_bibblock">Improving multimodal datasets with image captioning. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nichol and Dhariwal (2021)</span>
<span class="ltx_bibblock">
Alexander Quinn Nichol and Prafulla Dhariwal. 2021.

</span>
<span class="ltx_bibblock">Improved Denoising Diffusion Probabilistic Models. In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, Vol. 139. 8162–8171.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Gang Cao, and Bin Cui. 2023.

</span>
<span class="ltx_bibblock">Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Management of Data</em> 1, 1 (2023), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI. 2023a.

</span>
<span class="ltx_bibblock">ChatGPT.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
R OpenAI. 2023b.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv</em> (2023), 2303–08774.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parkar et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, and Dongyeop Kang. 2024.

</span>
<span class="ltx_bibblock">SelectLLM: Can LLMs Select Important Instructions to Annotate?

</span>
<span class="ltx_bibblock"><em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.16553</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Podell et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023.

</span>
<span class="ltx_bibblock">SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2307.01952 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib44.3.3.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al<span id="bib.bib44.4.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. In <em id="bib.bib44.5.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.

</span>
<span class="ltx_bibblock">High-Resolution Image Synthesis with Latent Diffusion Models. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</em>. 10674–10685.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021.

</span>
<span class="ltx_bibblock">Denoising Diffusion Implicit Models. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. 2023.

</span>
<span class="ltx_bibblock">Dual Diffusion Implicit Bridges for Image-to-Image Translation. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span id="bib.bib48.3.3.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al<span id="bib.bib48.4.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trummer (2023)</span>
<span class="ltx_bibblock">
Immanuel Trummer. 2023.

</span>
<span class="ltx_bibblock">From BERT to GPT-3 codex: harnessing the potential of very large language models for data management.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.09339</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib50.3.3.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al<span id="bib.bib50.4.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Cogvlm: Visual expert for pretrained language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.03079</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, and Heng Wang. 2024b.

</span>
<span class="ltx_bibblock">Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2403.02677 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib52.3.3.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al<span id="bib.bib52.4.1" class="ltx_text">.</span> 2024a.

</span>
<span class="ltx_bibblock">Internvideo2: Scaling video foundation models for multimodal video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.15377</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2024c)</span>
<span class="ltx_bibblock">
Yifei Wang, Jizhe Zhang, and Yisen Wang. 2024c.

</span>
<span class="ltx_bibblock">Do Generated Data Always Help Contrastive Learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2403.12448 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023.

</span>
<span class="ltx_bibblock">Magicoder: Source Code Is All You Need.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2312.02120 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Philip S Yu. 2023.

</span>
<span class="ltx_bibblock">Multimodal large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.13165</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin A. Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, and Sergey Yekhanin. 2024.

</span>
<span class="ltx_bibblock">Differentially Private Synthetic Data via Foundation Model APIs 2: Text.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2403.01749 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Canwen Xu, Daya Guo, Nan Duan, and Julian J. McAuley. 2023a.

</span>
<span class="ltx_bibblock">Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>. 6268–6278.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yang Xu, Yongqiang Yao, Yufan Huang, Mengnan Qi, Maoquan Wang, Bin Gu, and Neel Sundaresan. 2023b.

</span>
<span class="ltx_bibblock">Rethinking the Instruction Quality: LIFT is What You Need.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.11508 [cs.CL]

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang, Zili Wang, Shusen Wang, and Hai Zhao. 2023.

</span>
<span class="ltx_bibblock">RefGPT: Dialogue Generation of GPT, by GPT, and for GPT. In <em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>. Association for Computational Linguistics, 2511–2535.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. 2024.

</span>
<span class="ltx_bibblock">Mm-llms: Recent advances in multimodal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.13601</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. 2022.

</span>
<span class="ltx_bibblock">GLIPv2: Unifying Localization and Vision-Language Understanding. In <em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib62.3.3.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al<span id="bib.bib62.4.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.18223</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Jinchao Zhu, Yuxuan Wang, Siyuan Pan, Pengfei Wan, Di Zhang, and Gao Huang. 2024.

</span>
<span class="ltx_bibblock">A-SDM: Accelerating Stable Diffusion through Model Assembly and Feature Inheritance Strategies.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2406.00210 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.20755" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.20756" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.20756">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.20756" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.20757" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 19:18:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
