<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cognitive Biases in Large Language Models for News Recommendation</title>
<!--Generated on Thu Oct  3 18:40:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02897v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#S1" title="In Cognitive Biases in Large Language Models for News Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#S2" title="In Cognitive Biases in Large Language Models for News Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Risks of Cognitive Biases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#S3" title="In Cognitive Biases in Large Language Models for News Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Mitigating Strategies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#S4" title="In Cognitive Biases in Large Language Models for News Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#S5" title="In Cognitive Biases in Large Language Models for News Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\copyrightclause</span>
<p class="ltx_p" id="p1.2">Copyright for this paper by its authors.
Use permitted under Creative Commons License Attribution 4.0
International (CC BY 4.0).</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\conference</span>
<p class="ltx_p" id="p2.2">The 1st Workshop on Risks, Opportunities, and Evaluation of Generative Models in Recommender Systems (ROEGEN@RecSys 2024), October 2024, Bari, Italy.</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">[orcid=0009-0000-1082-9267,
email=youanglyu@gmail.com,
]</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">[orcid=0000-0002-5667-1036,
email=tinyoctopus1999@gmail.com,
]</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">[orcid=0000-0002-9076-6565,
email=z.ren@liacs.leidenuniv.nl,]</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">[orcid=0000-0002-1086-0202,
email=m.derijke@uva.nl,
]</p>
</div>
<h1 class="ltx_title ltx_title_document">Cognitive Biases in Large Language Models for
<br class="ltx_break"/>News Recommendation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yougang Lyu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">University of Amsterdam, Amsterdam, The Netherlands
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoyu Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Shandong University, Qingdao, China
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhaochun Ren
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Leiden University, Amsterdam, The Netherlands
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maarten de Rijke
</span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Despite large language models (LLMs) increasingly becoming important components of news recommender systems, employing LLMs in such systems introduces new risks, such as the influence of cognitive biases in LLMs. Cognitive biases refer to systematic patterns of deviation from norms or rationality in the judgment process, which can result in inaccurate outputs from LLMs, thus threatening the reliability of news recommender systems. Specifically, LLM-based news recommender systems affected by cognitive biases could lead to the propagation of misinformation, reinforcement of stereotypes, and the formation of echo chambers. In this paper, we explore the potential impact of multiple cognitive biases on LLM-based news recommender systems, including anchoring bias, framing bias, status quo bias and group attribution bias. Furthermore, to facilitate future research at improving the reliability of LLM-based news recommender systems, we discuss strategies to mitigate these biases through data augmentation, prompt engineering and learning algorithms aspects.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
News recommender system <span class="ltx_ERROR undefined" id="id2.id1">\sep</span>Large language models <span class="ltx_ERROR undefined" id="id3.id2">\sep</span>Cognitive bias

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Background</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) are becoming crucial components of recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib4" title="">4</a>]</cite>. In particular, news recommender systems rely heavily on LLMs to analyze vast amounts of textual data, ensuring that users receive news articles that align with their interests and preferences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite their growing importance and effectiveness, the deployment of LLMs in news recommender systems is not without risks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib12" title="">12</a>]</cite>. One significant issue that has emerged is the influence of cognitive biases in LLMs on the ability to make decisions correctly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib14" title="">14</a>]</cite>. Cognitive biases are systematic patterns of deviation from norm or rationality in judgment, can lead to the production of inaccurate or skewed outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib18" title="">18</a>]</cite>. Specifically, <cite class="ltx_cite ltx_citemacro_citet">Itzhak et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib19" title="">19</a>]</cite> find that LLMs fine-tuned on human-generated data are significantly affected by cognitive biases during the inference phase, which seriously affects the reliability of LLM-based news recommender systems.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Identifying and addressing the potential impact of cognitive biases in LLMs for recommender systems is critical, especially in the high-stake news recommendation task. News recommender systems play a crucial role in shaping public opinion, informing decision-making, and influencing societal discourse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib20" title="">20</a>]</cite>. Therefore, any distortion in the news recommendation process caused by cognitive biases can have far-reaching consequences, potentially spreading misinformation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib22" title="">22</a>]</cite>, reinforcing stereotypes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib23" title="">23</a>]</cite>, or contributing to echo chambers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Consequently, there is a pressing need to discuss the potential risks of cognitive bias in LLMs for news recommender systems and to explore possible solutions. This leads to our two central research questions: (1) <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">How do cognitive biases influence the decision-making processes of LLMs for news recommendations?</span> (2) <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">What strategies can be employed to mitigate these cognitive biases?</span> To this end, we analyze the impact of various cognitive biases on LLM-based news recommender systems. Additionally, we discuss strategies for mitigating these biases through data augmentation, prompt engineering, and learning algorithms.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Risks of Cognitive Biases</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">News recommender systems aim to personalize the news articles presented to users, tailoring the recommendation based on individual preferences, behavior, and historical data. LLM-based news recommender systems often leverage LLMs to filter and rank news articles, determining which articles to prioritize. However, LLMs trained on large amounts of human-generated data may inherit human cognitive biases, reflecting similar patterns in their outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib19" title="">19</a>]</cite>. Cognitive biases in LLMs can profoundly influence LLM-based news recommender systems, potentially leading to several adverse effects:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Anchoring bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib17" title="">17</a>]</cite>:</span> The reliance on the first piece of information (the “anchor”) when making decisions. Assuming a conversational presentation mode of LLM-based news recommender systems, LLMs are likely to be influenced by users’ initial interaction with a certain type of news (e.g., a particular political viewpoint), which could disproportionately influence future recommendations.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Framing bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib25" title="">25</a>]</cite>:</span> The way information is presented (framed) can influence decision-making and judgments. For example, news headlines and summaries framed in a particular way can lead LLM-based news recommender systems to severely prefer these news articles.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Status quo bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib26" title="">26</a>]</cite>:</span> The tendency to favor familiar content that has appeared in previous experiences. In LLM-based news recommender systems, LLM may prefer news articles that they have seen in the pre-training or fine-tuning stages, resulting in users finding it hard to view the latest news articles and reducing the diversity of information consumed.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Group attribution bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib27" title="">27</a>]</cite>:</span> This type of bias refers to the tendency to associate specific topics or opinions with particular demographic groups. E.g., LLM-based news recommender systems may disproportionately recommend certain news topics to specific ethnic or social groups, reinforcing harmful biases and deepening societal divisions.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In conclusion, cognitive biases in LLM-based news recommender systems can have significant effects on society. These biases may trap users in information bubbles, exposing them mostly to content that reinforces their existing beliefs and preferences. Over time, this can lead to increased social polarization, reduced critical thinking, and a less informed public. The risks posed by cognitive biases in LLM-based news recommender systems highlight the need for strategies that mitigate these biases and promote more balanced and diverse news recommendations.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Mitigating Strategies</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">LLM-based news recommender systems are trained on extensive human-generated datasets, which inherently reflect cognitive biases present in human society. LLM-based news recommender systems might inadvertently absorb and replicate these biases, leading to outputs that may not only reflect but also reinforce existing cognitive biases.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To address the cognitive biases of LLM-based news recommender systems, we propose several strategies to mitigate cognitive biases through data augmentation, prompt engineering and learning algorithms aspects. Below is a detailed list of mitigating strategies:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Synthetic data augmentation:</span> LLMs can be employed to generate synthetic datasets that are carefully crafted to break correlations between cognitive biases and irrational outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib29" title="">29</a>]</cite>. First, we can construct balanced synthetic datasets that counteract the skewed cognitive bias pattern existing in human-generated datasets. Then, we train LLM-based news recommender systems on balanced datasets to reduce the influence of cognitive biases and generate more rational outputs. This strategy helps to reduce specific biases in human-generated datasets, such as group attribution bias, by ensuring that the training data represents a more diverse and equitable distribution of content.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Self-debiasing via iterative refinement:</span> LLMs possess a self-refinement ability that can be harnessed through specially designed debiasing prompts. By iteratively refining the outputs, the model can be guided to recognize and correct biases in its recommendations. For example, in the context of LLM-based news recommender systems, LLMs can be prompted to reconsider their choices and adjust them to minimize the influence of cognitive biases. This process involves LLMs generating outputs, evaluating outputs against debiasing criteria, and revising outputs if necessary <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Cognitive debiasing through human feedback:</span> After an LLM generates a news recommendation, human evaluators can assess whether the output exhibits cognitive biases. If biases are detected, techniques such as direct preference optimization (DPO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib31" title="">31</a>]</cite> or reinforcement learning from human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib32" title="">32</a>]</cite> can be employed to adjust the model’s behavior. This learning from the human feedback process helps to align the model’s outputs more closely with human ethical standards and reduces the likelihood of biased recommendations in the future. By integrating human feedback into the training loop, the model learns to prioritize objectivity and fairness, thus mitigating the effects of cognitive biases.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Overall, LLM-based news recommender systems fine-tuned on extensive human-generated data are susceptible to cognitive biases. However, developing strategies such as synthetic data augmentation, self-debiasing via iterative refinement, and cognitive debiasing through human feedback can help mitigate these biases, fostering more rational and equitable outputs. These strategies are essential for ensuring that LLMs serve as reliable components in decision-making processes for news recommendations.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Large language models (LLMs) have become essential components of news recommender systems due to their advanced generation abilities. <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib33" title="">33</a>]</cite> directly prompt ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib34" title="">34</a>]</cite> to generate news recommendations. Prompt4NR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib35" title="">35</a>]</cite> introduces the prompt learning paradigm to news recommendation by reframing the task of predicting user clicks on news articles as a cloze-style mask-prediction problem. PGNR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib36" title="">36</a>]</cite> transfers the personalized news recommendation task into a text-to-text generation task for LLMs, following a generative training and inference paradigm that directly generates recommendations. ONCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib37" title="">37</a>]</cite> investigates the integration of both open- and closed-source LLMs to improve news recommendation. While the deployment of LLMs into news recommender systems has led to significant improvements, previous works ignore the potential risks associated with cognitive bias in LLMs for news recommender systems.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Recent studies have shown that LLMs are affected by cognitive biases when making decisions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib44" title="">44</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Jones and Steinhardt [<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib13" title="">13</a>]</cite> identify that error patterns of GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib45" title="">45</a>]</cite> resemble human cognitive biases. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Agrawal et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib38" title="">38</a>]</cite> discover the framing effect bias of GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib45" title="">45</a>]</cite> in the clinical information extraction task. <cite class="ltx_cite ltx_citemacro_citet">Itzhak et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib19" title="">19</a>]</cite> suggest that LLMs develop emergent cognitive biases after fine-tuning on extensive human-generated data. Furthermore, <cite class="ltx_cite ltx_citemacro_citet">Echterhoff et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.02897v1#bib.bib16" title="">16</a>]</cite> introduce a framework for the quantitative evaluation of cognitive biases in LLMs within a student admissions task. To the best of our knowledge, we are the first to discuss the influence of cognitive biases and mitigation strategies in LLM-based news recommender systems.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we focus on exploring possible risks of cognitive biases in LLM-based recommender systems and discussing potential mitigating techniques. Given the critical role news recommender systems play in creating public opinion and influencing social discourse, any distortion brought about by cognitive biases could have far-reaching effects. To explore the risks of cognitive bias, we have introduced the general definitions of anchoring bias, framing bias, status quo bias and group attribution bias, and how they specifically affect the decision-making process of LLM-based news recommender systems. Our discussion of the cognitive biases inherent in LLMs reveals that, without careful deployment, these biases can lead to skewed and incorrect outputs in news recommendations. Our analysis of the cognitive biases in LLM-based news recommender systems shows that these biases can cause biased and inaccurate news recommendations without careful deployment of LLMs. To address these challenges, we have proposed mitigating strategies from three aspects, including data augmentation, prompt engineering and learning algorithms.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">While LLMs demonstrate the potential to enhance the personalization and relevance of news recommendations, their implementation must be carefully managed to avoid exacerbating cognitive biases. By implementing the proposed strategies, we can create more reliable and equitable news recommender systems, better meeting the diverse needs of users and contributing to a healthier news recommendation ecosystem.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This research was (partially) supported by the Dutch Research Council (NWO), under project numbers 024.004.022, NWA.1389.20.183, and KICH3.LTP.20.006, and the European Union’s Horizon Europe program under grant agreement No 101070212.
All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024]</span>
<span class="ltx_bibblock">
L. Li, Y. Zhang, D. Liu, L. Chen,

</span>
<span class="ltx_bibblock">Large language models for generative recommendation: A survey and visionary discussions,

</span>
<span class="ltx_bibblock">in: Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, ELRA and ICCL, 2024, pp. 10146–10159. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.lrec-main.886" title="">https://aclanthology.org/2024.lrec-main.886</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023]</span>
<span class="ltx_bibblock">
L. Wu, Z. Zheng, Z. Qiu, H. Wang, H. Gu, T. Shen, C. Qin, C. Zhu, H. Zhu, Q. Liu, H. Xiong, E. Chen,

</span>
<span class="ltx_bibblock">A survey on large language models for recommendation,

</span>
<span class="ltx_bibblock">CoRR abs/2305.19860 (2023). URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2305.19860" title="">https://doi.org/10.48550/arXiv.2305.19860</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vats et al. [2024]</span>
<span class="ltx_bibblock">
A. Vats, V. Jain, R. Raja, A. Chadha,

</span>
<span class="ltx_bibblock">Exploring the impact of large language models on recommender systems: An extensive review,

</span>
<span class="ltx_bibblock">CoRR abs/2402.18590 (2024). URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2402.18590" title="">https://doi.org/10.48550/arXiv.2402.18590</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024]</span>
<span class="ltx_bibblock">
X. Zhang, R. Xie, Y. Lyu, X. Xin, P. Ren, M. Liang, B. Zhang, Z. Kang, M. de Rijke, Z. Ren,

</span>
<span class="ltx_bibblock">Towards empathetic conversational recommender systems,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2409.10527 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Wang [2023]</span>
<span class="ltx_bibblock">
Z. Zhang, B. Wang,

</span>
<span class="ltx_bibblock">Prompt learning for news recommendation,

</span>
<span class="ltx_bibblock">in: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, ACM, 2023, pp. 227–237. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3539618.3591752" title="">https://doi.org/10.1145/3539618.3591752</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2024]</span>
<span class="ltx_bibblock">
Q. Liu, N. Chen, T. Sakai, X. Wu,

</span>
<span class="ltx_bibblock">ONCE: Boosting content-based recommendation with both open- and closed-source large language models,

</span>
<span class="ltx_bibblock">in: Proceedings of the 17th ACM International Conference on Web Search and Data Mining, WSDM 2024, Merida, Mexico, March 4-8, 2024, ACM, 2024, pp. 452–461. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3616855.3635845" title="">https://doi.org/10.1145/3616855.3635845</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024]</span>
<span class="ltx_bibblock">
X. Li, Y. Zhang, E. C. Malthouse,

</span>
<span class="ltx_bibblock">Prompt-based generative news recommendation (PGNR): accuracy and controllability,

</span>
<span class="ltx_bibblock">in: Advances in Information Retrieval - 46th European Conference on Information Retrieval, ECIR 2024, Glasgow, UK, March 24-28, 2024, Proceedings, Part II, volume 14609 of <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Lecture Notes in Computer Science</span>, Springer, 2024, pp. 66–79. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-031-56060-6_5" title="">https://doi.org/10.1007/978-3-031-56060-6_5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023]</span>
<span class="ltx_bibblock">
J. Zhang, K. Bao, Y. Zhang, W. Wang, F. Feng, X. He,

</span>
<span class="ltx_bibblock">Is ChatGPT fair for recommendation? Evaluating fairness in large language model recommendation,

</span>
<span class="ltx_bibblock">in: Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023, Singapore, Singapore, September 18-22, 2023, ACM, 2023, pp. 993–999. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3608860" title="">https://doi.org/10.1145/3604915.3608860</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua et al. [2023]</span>
<span class="ltx_bibblock">
W. Hua, L. Li, S. Xu, L. Chen, Y. Zhang,

</span>
<span class="ltx_bibblock">Tutorial on large language models for recommendation,

</span>
<span class="ltx_bibblock">in: Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023, Singapore, Singapore, September 18-22, 2023, ACM, 2023, pp. 1281–1283. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3609494" title="">https://doi.org/10.1145/3604915.3609494</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024]</span>
<span class="ltx_bibblock">
J. Zhang, K. Bao, Y. Zhang, W. Wang, F. Feng, X. He,

</span>
<span class="ltx_bibblock">Large language models for recommendation: Progresses and future directions,

</span>
<span class="ltx_bibblock">in: Companion Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, Singapore, May 13-17, 2024, ACM, 2024, pp. 1268–1271. URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3589335.3641247" title="">https://doi.org/10.1145/3589335.3641247</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2024]</span>
<span class="ltx_bibblock">
M. Jiang, K. Bao, J. Zhang, W. Wang, Z. Yang, F. Feng, X. He,

</span>
<span class="ltx_bibblock">Item-side fairness of large language model-based recommendation system,

</span>
<span class="ltx_bibblock">in: Proceedings of the ACM on Web Conference 2024, 2024, pp. 4717–4726.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deldjoo [2024]</span>
<span class="ltx_bibblock">
Y. Deldjoo,

</span>
<span class="ltx_bibblock">Understanding biases in ChatGPT-based recommender systems: Provider fairness, temporal stability, and recency,

</span>
<span class="ltx_bibblock">ACM Transactions on Recommender Systems (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones and Steinhardt [2022]</span>
<span class="ltx_bibblock">
E. Jones, J. Steinhardt,

</span>
<span class="ltx_bibblock">Capturing failures of large language models via human cognitive biases,

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35 (2022) 11785–11799.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schramowski et al. [2022]</span>
<span class="ltx_bibblock">
P. Schramowski, C. Turan, N. Andersen, C. A. Rothkopf, K. Kersting,

</span>
<span class="ltx_bibblock">Large pre-trained language models contain human-like biases of what is right and wrong to do,

</span>
<span class="ltx_bibblock">Nature Machine Intelligence 4 (2022) 258–268.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tjuatja et al. [2023]</span>
<span class="ltx_bibblock">
L. Tjuatja, V. Chen, S. T. Wu, A. Talwalkar, G. Neubig,

</span>
<span class="ltx_bibblock">Do LLMs exhibit human-like response biases? A case study in survey design,

</span>
<span class="ltx_bibblock">CoRR abs/2311.04076 (2023). URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2311.04076" title="">https://doi.org/10.48550/arXiv.2311.04076</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Echterhoff et al. [2024]</span>
<span class="ltx_bibblock">
J. M. Echterhoff, Y. Liu, A. Alessa, J. J. McAuley, Z. He,

</span>
<span class="ltx_bibblock">Cognitive bias in high-stakes decision-making with LLMs,

</span>
<span class="ltx_bibblock">CoRR abs/2403.00811 (2024). URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2403.00811" title="">https://doi.org/10.48550/arXiv.2403.00811</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tversky and Kahneman [1974]</span>
<span class="ltx_bibblock">
A. Tversky, D. Kahneman,

</span>
<span class="ltx_bibblock">Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty.,

</span>
<span class="ltx_bibblock">Science 185 (1974) 1124–1131.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilke and Mata [2012]</span>
<span class="ltx_bibblock">
A. Wilke, R. Mata,

</span>
<span class="ltx_bibblock">Cognitive bias,

</span>
<span class="ltx_bibblock">in: Encyclopedia of human behavior, Academic Press, 2012, pp. 531–535.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Itzhak et al. [2023]</span>
<span class="ltx_bibblock">
I. Itzhak, G. Stanovsky, N. Rosenfeld, Y. Belinkov,

</span>
<span class="ltx_bibblock">Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias,

</span>
<span class="ltx_bibblock">CoRR abs/2308.00225 (2023). URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2308.00225" title="">https://doi.org/10.48550/arXiv.2308.00225</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Helberger [2021]</span>
<span class="ltx_bibblock">
N. Helberger,

</span>
<span class="ltx_bibblock">On the democratic role of news recommenders,

</span>
<span class="ltx_bibblock">in: Algorithms, Automation, and News, Routledge, 2021, pp. 14–33.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spina et al. [2023]</span>
<span class="ltx_bibblock">
D. Spina, M. Sanderson, D. Angus, G. Demartini, D. Mckay, L. L. Saling, R. W. White,

</span>
<span class="ltx_bibblock">Human-AI cooperation to tackle misinformation and polarization,

</span>
<span class="ltx_bibblock">Communications of the ACM 66 (2023) 40–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewandowsky et al. [2012]</span>
<span class="ltx_bibblock">
S. Lewandowsky, U. K. Ecker, C. M. Seifert, N. Schwarz, J. Cook,

</span>
<span class="ltx_bibblock">Misinformation and its correction: Continued influence and successful debiasing,

</span>
<span class="ltx_bibblock">Psychological science in the public interest 13 (2012) 106–131.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yunkaporta [2023]</span>
<span class="ltx_bibblock">
T. Yunkaporta, Right Story, Wrong Story: Adventures in Indigenous Thinking, Text Publishing, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020]</span>
<span class="ltx_bibblock">
X. Wang, A. D. Sirianni, S. Tang, Z. Zheng, F. Fu,

</span>
<span class="ltx_bibblock">Public discourse and social network echo chambers driven by socio-cognitive biases,

</span>
<span class="ltx_bibblock">Physical Review X 10 (2020) 041042.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tversky and Kahneman [1981]</span>
<span class="ltx_bibblock">
A. Tversky, D. Kahneman,

</span>
<span class="ltx_bibblock">The framing of decisions and the psychology of choice,

</span>
<span class="ltx_bibblock">Science 211 (1981) 453–458.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samuelson and Zeckhauser [1988]</span>
<span class="ltx_bibblock">
W. Samuelson, R. Zeckhauser,

</span>
<span class="ltx_bibblock">Status quo bias in decision making,

</span>
<span class="ltx_bibblock">Journal of Risk and Uncertainty 1 (1988) 7–59.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton and Gifford [1976]</span>
<span class="ltx_bibblock">
D. L. Hamilton, R. K. Gifford,

</span>
<span class="ltx_bibblock">Illusory correlation in interpersonal perception: A cognitive basis of stereotypic judgments,

</span>
<span class="ltx_bibblock">Journal of Experimental Social Psychology 12 (1976) 392–407.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. [2024]</span>
<span class="ltx_bibblock">
B. Ding, C. Qin, R. Zhao, T. Luo, X. Li, G. Chen, W. Xia, J. Hu, A. T. Luu, S. Joty,

</span>
<span class="ltx_bibblock">Data augmentation using LLMs: Data perspectives, learning paradigms and challenges,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2403.02990 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. [2024]</span>
<span class="ltx_bibblock">
Y. Lyu, L. Yan, S. Wang, H. Shi, D. Yin, P. Ren, Z. Chen, M. de Rijke, Z. Ren,

</span>
<span class="ltx_bibblock">Knowtuning: Knowledge-aware fine-tuning for large language models,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2402.11176 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. [2024]</span>
<span class="ltx_bibblock">
A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al.,

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback,

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 36 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al. [2024]</span>
<span class="ltx_bibblock">
R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, C. Finn,

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model,

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 36 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stiennon et al. [2020]</span>
<span class="ltx_bibblock">
N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, P. F. Christiano,

</span>
<span class="ltx_bibblock">Learning to summarize with human feedback,

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 33 (2020) 3008–3021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
X. Li, Y. Zhang, E. C. Malthouse,

</span>
<span class="ltx_bibblock">A preliminary study of ChatGPT on news recommendation: Personalization, provider fairness, fake news,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2306.10702 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. [2022]</span>
<span class="ltx_bibblock">
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, R. Lowe,

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback,

</span>
<span class="ltx_bibblock">in: Proceedings of NeurIPS, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Wang [2023]</span>
<span class="ltx_bibblock">
Z. Zhang, B. Wang,

</span>
<span class="ltx_bibblock">Prompt learning for news recommendation,

</span>
<span class="ltx_bibblock">in: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023, pp. 227–237.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024]</span>
<span class="ltx_bibblock">
X. Li, Y. Zhang, E. C. Malthouse,

</span>
<span class="ltx_bibblock">Prompt-based generative news recommendation (PGNR): Accuracy and controllability,

</span>
<span class="ltx_bibblock">in: European Conference on Information Retrieval, Springer, 2024, pp. 66–79.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2024]</span>
<span class="ltx_bibblock">
Q. Liu, N. Chen, T. Sakai, X.-M. Wu,

</span>
<span class="ltx_bibblock">Once: Boosting content-based recommendation with both open-and closed-source large language models,

</span>
<span class="ltx_bibblock">in: Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024, pp. 452–461.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. [2023]</span>
<span class="ltx_bibblock">
M. Agrawal, S. Hegselmann, H. Lang, Y. Kim, D. Sontag, Large language models are zero-shot clinical information extractors, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Ng [2023]</span>
<span class="ltx_bibblock">
R. Lin, H. T. Ng,

</span>
<span class="ltx_bibblock">Mind the biases: Quantifying cognitive biases in language model prompting,

</span>
<span class="ltx_bibblock">in: Findings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 5269–5281.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2021]</span>
<span class="ltx_bibblock">
Y. Lu, M. Bartolo, A. Moore, S. Riedel, P. Stenetorp,

</span>
<span class="ltx_bibblock">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2104.08786 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmidgall et al. [2024]</span>
<span class="ltx_bibblock">
S. Schmidgall, C. Harris, I. Essien, D. Olshvang, T. Rahman, J. W. Kim, R. Ziaei, J. Eshraghian, P. Abadir, R. Chellappa,

</span>
<span class="ltx_bibblock">Addressing cognitive bias in medical language models,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2402.08113 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tjuatja et al. [2023]</span>
<span class="ltx_bibblock">
L. Tjuatja, V. Chen, S. T. Wu, A. Talwalkar, G. Neubig,

</span>
<span class="ltx_bibblock">Do LLMs exhibit human-like response biases? a case study in survey design,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2311.04076 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. [2023]</span>
<span class="ltx_bibblock">
Y. Lyu, P. Li, Y. Yang, M. de Rijke, P. Ren, Y. Zhao, D. Yin, Z. Ren,

</span>
<span class="ltx_bibblock">Feature-level debiased natural language understanding,

</span>
<span class="ltx_bibblock">in: Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 2023, pp. 13353–13361.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al. [2020]</span>
<span class="ltx_bibblock">
M. T. Ribeiro, T. Wu, C. Guestrin, S. Singh,

</span>
<span class="ltx_bibblock">Beyond accuracy: Behavioral testing of NLP models with checklist,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2005.04118 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown [2020]</span>
<span class="ltx_bibblock">
T. B. Brown,

</span>
<span class="ltx_bibblock">Language models are few-shot learners,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2005.14165 (2020).

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 18:40:44 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
