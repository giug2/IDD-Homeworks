<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.07700] Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme.</title><meta property="og:description" content="Federated learning (FL) was proposed to facilitate the training of models in a distributed environment. It supports the protection of (local) data privacy and uses local resources for model training. Until now, the maj…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.07700">

<!--Generated on Wed Mar 13 14:28:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
applied federated learning,  Internet of Things,  federated learning topology
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations
<br class="ltx_break"><span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karolina Bogacka
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">Systems Research Institute
<br class="ltx_break">Polish Academy of Sciences</span>
<br class="ltx_break">Warsaw, Poland 
<br class="ltx_break">0000-0002-7109-891X
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Katarzyna Wasielewska-Michniewska
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Systems Research Institute
<br class="ltx_break">Polish Academy of Sciences</span>
<br class="ltx_break">Warsaw, Poland 
<br class="ltx_break">0000-0002-3763-2373
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marcin Paprzycki
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_font_italic">Systems Research Institute
<br class="ltx_break">Polish Academy of Sciences</span>
<br class="ltx_break">Warsaw, Poland 
<br class="ltx_break">0000-0002-8069-2152
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria Ganzha, Anastasiya Danilenka
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_font_italic">Faculty of Mathematics and Information Science</span>
<br class="ltx_break"><span id="id6.2.id2" class="ltx_text ltx_font_italic">Warsaw University of Technology
<br class="ltx_break"></span>Warsaw, Poland 
<br class="ltx_break">0000-0001-7714-4844, 0000-0002-3080-0303
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lambis Tassakos
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_font_italic">TwoTronic Gmbh</span>
<br class="ltx_break">Meitingen, Germany 
<br class="ltx_break">0000-0003-2511-9035
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eduardo Garro
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id8.1.id1" class="ltx_text ltx_font_italic">Prodevelop</span>
<br class="ltx_break">Valencia, Spain 
<br class="ltx_break">0000-0002-8160-0125
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Federated learning (FL) was proposed to facilitate the training of models in a distributed environment. It supports the protection of (local) data privacy and uses local resources for model training. Until now, the majority of research has been devoted to “core issues”, such as adaptation of machine learning algorithms to FL, data privacy protection, or dealing with the effects of uneven data distribution between clients. This contribution is anchored in a practical use case, where FL is to be actually deployed within an Internet of Things ecosystem. Hence, somewhat different issues that need to be considered, beyond popular considerations found in the literature, are identified. Moreover, an architecture that enables the building of flexible, and adaptable, FL solutions is introduced.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
applied federated learning, Internet of Things, federated learning topology

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">One of the critical (and practical) bottlenecks of the application of Machine Learning (ML) lies in the limited ability to collect, consistently label, and use large datasets. This is particularly the case for businesses that do not possess almost unlimited resources, as Google or Amazon do <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Moreover, while existing data may be large and labeled, it may be “split between stakeholders”, who do not want to and/or cannot share their datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. For instance, this is the case for the medical data, which belongs to different hospitals/clinics. Moreover, there are ongoing controversies concerning the collection and storage of information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Many ML developments, e.g. in mobile applications, rely on the models being periodically (re/up)trained on sensitive private data (e.g., browsing history, or geo-positioning). Hosting such data in a centralized location, even in adherence to strict legislation, still poses serious security risks, as can be seen through repeated data leaks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">It is also worth noting that the latest advancements in ML involve training very large models that require enormous computational resources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. This not only increases the cost but also the carbon footprint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To overcome these, and other related, problems, Federated Learning (FL) has been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The name of the approach came from the use of a flexible <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">federation</span> of collaborating (often heterogeneous, edge) devices, known as clients, “synchronized” and “orchestrated” by a “central server”. In FL, (i) clients train copies of the global model, using local data, and (ii) send updates to the server, which (iii) aggregates them, and (iv) updates the shared model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which (v) is sent back to the clients to continue the process, until a stopping criterion is met. Therefore, private data never leaves the clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. While a lot of research is devoted to the FL process itself, it is mostly implemented and tested in a cloud. This means that important practical issues that, as we will argue, have to be resolved, are omitted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">One should immediately realize that one of the future areas of application of FL is the Internet of Things. Among others, this is the result of a general trend to replace cloud-centric solutions with edge-cloud continuum-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. This is happening because storing data, and providing resources, in the data center is not sustainable for large-scale complex deployments, where latency can negatively impact performance. Hence, computing has to take place near (at) the edge of the network, physically close to sensors and/or users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The resulting ecosystem represents the edge-cloud continuum and is the necessary direction for the evolution of Next-Generation Internet of Things deployments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Here, among others, FL will deliver intelligence at the edge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. However, combining FL with IoT brings about its own issues: (i) heterogeneity of clients and networks can cause delays (latency variability), or the presence of “stragglers” (weaker/more busy clients); (ii) computing and/or storage resources on the (far) edge devices, as well as their battery life, tend to be very limited, which impedes the use of large models and poses restrictions on training time; and (iii) data used for the training can be highly redundant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">As noted, core research on Federated Learning is focused on machine learning (ML) and its intricacies. This can be seen also when one considers state-of-the-art of FL frameworks. For example, though TensorFlow Federated Framework (TFF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> offers a wide variety of stable ML models, it supports experimentation only in a simulated environment. In other words, TFF currently does <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">not</span> enable use of actual “edge devices”. Another widely known FL platform is FATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Here, 6GB of RAM, and 100 GB of disk space, on the server as well as on the clients are expected. While this would work in a laboratory, it exceeds the capabilities of the majority of edge devices (at least of today). Among platforms, PaddleFL enables the implementation of decentralized architectures by default. However, due to the low number of current contributors, and the employment of PaddlePaddle, a lesser-known Deep Learning platform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, PaddleFL may not be an optimal choice for future work. Flower (A Friendly Federated Learning Framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>) can be run on a diverse range of environments and devices, including Android, iOS, Raspberry Pi, and Nvidia Jetson. It is also compatible with popular ML frameworks like PyTorch and Keras. Finally, PySyft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> allows the use of clients on the edge, using <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">pygrid</span>, which is a novel development. However, even the latest two platforms can be seen, primarily, as tools for studying the “nature of FL”, rather than to be used to run FL in IoT ecosystems.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this context, this work aims to (a) reflect on the nature of challenges that actual FL deployments in IoT have to address, (b) show how a reference architecture, proposed for Next-Generation IoT supports the deployment of Federated Learning, and (c) illustrate the flexibility of the proposed approach through its capability of setting systems with different FL topologies.
Hence, the remaining parts of this work are organized as follows. In Section <a href="#S2" title="II Federated Learning use case in IoT deployment ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> a practical IoT-based scenario from ASSIST-IoT <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://assist-iot.eu/</span></span></span> project is described. Since FL will be actually deployed and experimented with in this use case, it will be used to summarize key requirements for “practical FL in IoT”. In Section <a href="#S3" title="III Work related to Federated Learning topology ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> we summarize pertinent state-of-the-art. Following, in Section <a href="#S4" title="IV Federated Learning in IoT – proposed architecture ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, an architecture that fulfills the requirements of the use case and addresses issues materializing in IoT-based deployments is described. Next, in Section <a href="#S5" title="V Other FL topologies, applicability and usability ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, the ways in which the proposed architecture can be adapted and used are outlined. Finally, in Section <a href="#S6" title="VI Concluding remarks ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, a summary of contributions, and directions of future work are provided.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Federated Learning use case in IoT deployment</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The foundation of this contribution is provided by ASSIST-IoT project. There, a sample use case, in which FL is to be applied, is a part of the car damage recognition pilot.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The main goal of this scenario is to provide a fast and accurate inspection of car exterior damage, with minimal data transfer from edge devices to the cloud. Here, the task of car damage detection can be separated into three steps: (i) efficiently separating the vehicle from the background, (ii) vehicle part segmentation, and (iii) automatic defect detection. The results are to be used to support expert-delivered-evaluation, and to facilitate decisions involving insurance claims, as well as car return or leasing services.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">As it can be seen in Fig. <a href="#S2.F1" title="Figure 1 ‣ II Federated Learning use case in IoT deployment ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the functional pipeline involves multiple professional scanners, equipped with high-quality cameras, based on the TwoTronic solution <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.fahrzeugscanner.de/</span></span></span>. A high volume (more than 200) of scanned vehicles per day is expected. TwoTronic scanners, and “attached” medium-class computers, will serve as FL clients. The FL server will be located in an external data center in Nürnberg, Germany.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<figure id="S2.F1.sf1" class="ltx_figure ltx_align_center"><img src="/html/2207.07700/assets/images/lambis/gate1.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="305" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Car damage recognition - scanner gate</figcaption>
</figure>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Deploying a FL system is a complex task, depending not only on the availability of FL libraries and algorithms but also characteristics and limitations of a distributed system. Importantly, to be able to practically apply FL solution in this real-life use case, additional issues that are rarely addressed in literature, such as: (a) sudden user dropout, (b) weak network connection with potential interruptions, (c) geographical constraints (leading to unequal groups of clients), (d) data distribution (local distribution on the client differing from global distribution, with no additional public information that would enable problem mitigation through client grouping), and (e) system limitations, notably available RAM and number of cores, need to be considered. Lastly, (f) in environments with heterogeneous devices, interoperability may also become an issue.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">It is worth noting that, due to (geographical) distances between scanners and the FL server, located in Nürnberg, as well as the high speed and accuracy of prediction, necessary for this scenario, examining different Fl topologies may be in order. First, divergence from a centralized (client-server) schema to a decentralized one could protect the system from having a single point of failure. This could increase its reliability and resilience. Second, the introduction of additional aggregating clients into a centralized system, would mean that more information about an interrupted training is being preserved. Moreover, training could continue within lower levels of aggregating hierarchy. Additionally, a decrease in direct communication between scanners and the central server could mean faster training. The employment of non-standard topologies may also reduce sensitivity of the training process to interruptions and sudden client dropouts; by introducing additional communication channels.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Work related to Federated Learning topology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Taking into account potential importance of FL topology, let us summarize related state-of-the-art. Currently, the effect of topology between clients on FL systems is not fully understood, but hard to deny <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. For sure, there is no “best topology”, but rather it needs to be selected to match the characteristics of a specific use case. It has been observed that the centralized approach may not be appropriate, due to significant communication overhead and a single point of failure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. On the other hand, fully decentralized topologies can involve a significant cost of communication not related to client-to-server one <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. It is worth mentioning that some works combine these approaches to improve convergence and scalability, for example by combining decentralized groups with a centralized update schema <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Some approaches experimented with star and ring architectures and their combination. The reason was to avoid the communication bottleneck of the former while gaining improved scalability and accuracy of the latter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. There, a star architecture with ring-based groups, supported by a self-balancing framework designed to mitigate the problem of a skewed global distribution, was evaluated.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Work presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> uses a ring architecture with star-based groups, in a realistic use case with non-IID data with periodic variance. Overall, while a linear speedup with respect to the number of clients is reported, the need for periodic variance is a limiting factor.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Ring-based groups, without global communication, while further elaborating on the periodically variational distribution of the data samples, treated by semi-cyclic Stochastic Gradient Descent (SGD) is discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Here, it is observed that the use of ring-based groups may lead to slower training due to the higher number of rounds the process has to undergo for the model to gather information from all the nodes belonging to the group when compared with star-based groups.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Work reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> investigates combinations of star and ring architectures and proposes two forms of the TornadoAggregate algorithm: one with a ring architecture with star-based groups, the other with a star architecture with ring-based groups.
Interestingly, a substantial difference in results between the two TornadoAggregate versions is reported. The version with star architecture and ring-based groups, outperformed the ring architecture with star-based groups.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> D-Cliques, a topology that aims at reducing gradient bias, by grouping clients in sparsely interconnected cliques, such that the label distribution in the clique would be representative of the global distribution, is presented. This approach led to the convergence speed similar to that of a fully-connected topology with a 98% reduction in the total number of edges,
and 96% reduction in the total number of messages.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">A contrasting approach to data skewness mitigation, in the form of a hierarchical FL system with Federated Gradient Descent being conducted on the user-edge layer and Federated Averaging between edges and the cloud, is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. The resulting architecture is designed with an IoT environment in mind, with the potentially less efficient connections between edge and the server supporting less frequent communication.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">Work described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> uses segmentation to allow for large model training on far edge. The proposed approach relies on a combination of model segmentation level synchronization mechanisms, which divides the model into a set of not overlapping subsets, and a decentralized design reminiscent of the gossip protocol, with each worker randomly transferring the model segment to a few other workers. Model redundancy had to be included in order to ensure convergence. Discussed prototype acknowledges the problem of workers suddenly exiting and returning.
This work has been further extended in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, forming a bandwidth-aware solution by greedily choosing a client with sufficient bandwidth to avoid delays. The convergence guarantees were provided, with the training time being reduced up to 18 times, compared to that of baselines with no accuracy degradation.</p>
</div>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.1" class="ltx_p">Another approach to decentralized FL (DAFCL) can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. In DAFCL, all clients are connected through an undirected graph. Each of them is supposed to train the model based on its local data, and exchange the results with its neighbors, through a symmetric doubly stochastic matrix. To avoid a single point of failure, the average model estimation is tracked using First Order Dynamic Average Consensus (FODAC). This architecture shows promising results. Nevertheless, to use it in Next Generation IoT environments further work on communication efficiency, and increasing resilience to sudden catastrophic events, such as user dropout, would be necessary.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p">In summary, research related to FL topology introduces a multitude of approaches to the problem. from the perspective of this contribution, it “does not matter” which topology should be used or is the best in a given scenario. The question is: how to make sure that any needed topology can be instantiated in Next Generation IoT Ecosystems. Proposing a pathway to answering this question is the goal of the remaining parts of this contribution.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Federated Learning in IoT – proposed architecture</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Let us now introduce the proposed architectural approach to Federated Learning in IoT ecosystems. Since support for different topologies has been shown to be important in large-scale real-life deployments, the possibility of easily implementing them is crucial. Moreover, the proposed architecture should be resistant to sudden user dropout, network connection with interruptions or uneven grouping of clients.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The proposed FL architecture is developed according to the Reference Architecture (RA) introduced in the ASSIST-IoT project, and motivated by real-life scenarios, coming from three industrial pilots. This RA is based on the concept of encapsulation, in which is instantiated in the form of enablers. Interested readers should consult <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for necessary details. Note that the fact that the proposed FL architecture is compatible with ASSIST-IoT RA principles allows the use of additional enablers that can extend its capabilities, e.g., with a semantic toolset to enable interoperability, or self-* functionalities such as automated configuration (e.g., to control the state of topology and adjust its configuration) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. These aspects are, however, outside of the scope of this contribution.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">As it can be seen in Fig. <a href="#S4.F2" title="Figure 2 ‣ IV Federated Learning in IoT – proposed architecture ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the proposed FL architecture is formed by four enablers: <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">FL Orchestrator</span>, <span id="S4.p3.1.2" class="ltx_text ltx_font_italic">FL Repository</span>, <span id="S4.p3.1.3" class="ltx_text ltx_font_italic">FL Training Collector</span>, and <span id="S4.p3.1.4" class="ltx_text ltx_font_italic">FL Local Operations</span>.
The <span id="S4.p3.1.5" class="ltx_text ltx_font_italic">FL Orchestrator</span> is the enabler responsible for the configuration propagation to other enablers, workflow management, and control over the FL life cycle. It also acts as the entrance gate for human interactions. Moreover, <span id="S4.p3.1.6" class="ltx_text ltx_font_italic">FL Orchestrator</span> may control the FL training process, and constraints related to e.g., the minimum number of clients, or minimum system requirements. On the other hand, the <span id="S4.p3.1.7" class="ltx_text ltx_font_italic">FL Repository</span> is a supplementary enabler for storing models, algorithms, and any data needed in the FL process. Last, the <span id="S4.p3.1.8" class="ltx_text ltx_font_italic">FL Training Collector</span> and <span id="S4.p3.1.9" class="ltx_text ltx_font_italic">FL Local Operations</span> act as FL servers and clients, respectively. They are used in the constructed system as communicating components, remaining in constant contact according to the gRPC protocol, by utilizing functionalities implemented as a part of the Flower library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. In other words, the <span id="S4.p3.1.10" class="ltx_text ltx_font_italic">FL Training Collector</span> possesses the capabilities of a FL centralized server, while the <span id="S4.p3.1.11" class="ltx_text ltx_font_italic">FL Local Operations</span> (located on edge clients) has the abilities of an FL client, with the main focus placed on local model training and dataset loading. Let us now describe the <span id="S4.p3.1.12" class="ltx_text ltx_font_italic">FL Training Collector</span> and the <span id="S4.p3.1.13" class="ltx_text ltx_font_italic">FL Local Operations</span> in more detail.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2207.07700/assets/images/FL_architecture.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Proposed FL in IoT architecture</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">FL Training Collector</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">FL Training Collector</span> mainly serves the role of a server node. Uploading configuration (e.g. from the <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">FL Repository</span>) initiates the training process. The configuration data can include, among others, the type of aggregation algorithm used for FL, the minimal number of clients necessary in order to start training, the minimum number of clients necessary for training each round, the fraction of clients to be sampled for training or evaluation, a set timeout for the responses coming from clients, the number of clients to choose for training with blacklisting and some additional values used for later testing. The behaviour exhibited by the <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">FL Training Collector</span> before and after each training, as well as evaluation round, is defined in the form of a Strategy class, in accordance with the requirements of the Flower library. This class is used by the Flower server to group clients, selected from available client interfaces, with the appropriate weights to be later sent by the server and to define the mechanisms used to aggregate results from the clients and evaluate current model performance. Due to its periodic nature (methods are called in the defined order, before and after every round), this class is also used for gathering metrics and saving current model weights, for later analysis. The metrics, which are gathered after each training round, consist of aggregated evaluation loss, global evaluation loss, and global accuracy. They are collected in order to facilitate monitoring of the training process. Later, they are locally stored in the enabler in the form of a serialized object inside a pickle file <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">FL Local Operations</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">An instance of <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">FL Local Operations</span>, the analogue for the FL client, is created similarly to the <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">FL Training Collector</span>. In order to start the training, it needs to be provided with a training configuration, and the address of the <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">FL Training Collector</span> instance, which it should be connected to.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">FL Local Operations</span> enabler is responsible for loading and preprocessing the right subset of local data, and setting up the local model. It not only executes but also enhances the behaviour of an FL client in the form of classes extending the flower.client.Client class, by implementing methods of initiating, fitting the model, and evaluating the model performance.
The evaluation accuracy and loss of the current model are computed on the local test set. The values of these metrics, in their original form, as well as an average (in the case of clustered architecture – weighted average, in an attempt to increase the precision of the visualization, for unstable client groupings) over the metric values from all <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">FL Local Operations</span> is used to assess the efficiency of the training process. Similarly to <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_italic">FL Training Collector</span>, these statistics are regularly stored as pickle files <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. <span id="S4.SS2.p2.1.4" class="ltx_text ltx_font_italic">FL Local Operations</span> may also include mechanisms related to privacy, such as data encryption or differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">FL training process</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Let us now describe the FL training process that is to take place in the case of basic, centralized, topology.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">An instance of <span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">FL Training Collector</span> receives a training configuration from the <span id="S4.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">FL Orchestrator</span>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">FL Training Collector</span> waits for a minimal number of clients, as specified by the configuration.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Required number of <span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">FL Local Operations</span> instances receive their training configuration, from the <span id="S4.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">FL Orchestrator</span>, similar in content to that supplied to the <span id="S4.I1.i3.p1.1.3" class="ltx_text ltx_font_italic">FL Training Collector</span>, but also including identifying information about the <span id="S4.I1.i3.p1.1.4" class="ltx_text ltx_font_italic">FL Training Collector</span> participating in the process.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Activated instances of <span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">FL Local Operations</span> establish a connection with the <span id="S4.I1.i4.p1.1.2" class="ltx_text ltx_font_italic">FL Training Collector</span>.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">FL Training Collector</span> samples <span id="S4.I1.i5.p1.1.2" class="ltx_text ltx_font_italic">FL Local Operations</span> and provides them with model weights and, possibly, additional configuration, which triggers the training process
on <span id="S4.I1.i5.p1.1.3" class="ltx_text ltx_font_italic">FL Local Operations</span>.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p"><span id="S4.I1.i6.p1.1.1" class="ltx_text ltx_font_italic">FL Local Operations</span> instances train the model (in parallel) and return the weights along with any metrics they were requested to gather.</p>
</div>
</li>
<li id="S4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S4.I1.i7.p1" class="ltx_para">
<p id="S4.I1.i7.p1.1" class="ltx_p">Next, the weights are aggregated according to a strategy supplied by the <span id="S4.I1.i7.p1.1.1" class="ltx_text ltx_font_italic">FL Training Collector</span>. The data, along with any computed metrics, is communicated (as required) before and after model evaluation (after each round).</p>
</div>
</li>
</ol>
<p id="S4.SS3.p2.1" class="ltx_p">This approach, formulated for the basic centralized architecture, can be then modified in order to support other topologies.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Other FL topologies, applicability and usability</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">By performing slight modifications to the basic architecture, it is possible to instantiate other topologies proposed in the literature. In particular, four topologies have been implemented and initially tried: centralized architecture, clustered architecture, hierarchical architecture, and star architecture with ring-based groups. They are illustrated in Fig. <a href="#S5.F3" title="Figure 3 ‣ V Other FL topologies, applicability and usability ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. It should be noted that the aim of this work was to establish that the proposed architectural approach, based on enablers originating from the ASSIST-IoT RA can be used to easily set up “any” FL topology. Thus, this is what was implemented and tested. The usage of these topologies for the car maintenance use case, described above, will be explored in the near future.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2207.07700/assets/images/architecture/cent....png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Centralized architecture</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2207.07700/assets/images/architecture/cnem....png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="162" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Clustered architecture</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2207.07700/assets/images/architecture/h...2.png" id="S5.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="174" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Hierarchical architecture</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2207.07700/assets/images/architecture/snew.....png" id="S5.F3.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="206" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Star architecture with ring-based groups</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>ASSIST-IoT FL alternative architectures for IoT environments</figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The basic centralized architecture was implemented following the description presented above. The possibility of using different “parameters” of the FL process, as represented in the setup, including client numbers, model architecture, approach to model averaging, data collected by the <span id="S5.p2.1.1" class="ltx_text ltx_font_italic">FL Local Operations</span> and <span id="S5.p2.1.2" class="ltx_text ltx_font_italic">FL Training Collector</span> has been tested.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">As for the clustered architecture, the implemented version, first, accepts a set number of clusters and then uses the Iterative Federated Clustering Algorithm (IFCA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> to dynamically determine the adherence of a given client to a cluster at the beginning of each round. Next, in the aggregation stage, the cluster models are updated, based only on the data from the clients that belong to them at the moment. When faced with IID data, the clients are determined to belong to a single cluster, which leads the architecture to behave similarly to the centralized one. For the non-IID data, the clustered architecture leads to the development of a number of models, each tailored exactly to a given cluster of clients, instead of a single global solution. The implemented architecture was tested using CIFAR-10 (for IID data) and German Traffic Sign Recognition Benchmark dataset (for non-IID data) and the results matched these found in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The hierarchical topology necessitates the creation of an additional component <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. In this work it is implemented as a special case of <span id="S5.p4.1.1" class="ltx_text ltx_font_italic">FL Local Operations</span> called <span id="S5.p4.1.2" class="ltx_text ltx_font_italic">1st Layer Local Operations</span>. This necessitates that the version of <span id="S5.p4.1.3" class="ltx_text ltx_font_italic">FL Local Operations</span> acts as a basic FL client called <span id="S5.p4.1.4" class="ltx_text ltx_font_italic">2nd Layer Local Operations</span>. This additional enabler serves as FL server to <span id="S5.p4.1.5" class="ltx_text ltx_font_italic">2nd Layer Local Operations</span> and as FL client to the <span id="S5.p4.1.6" class="ltx_text ltx_font_italic">FL Training Collector</span>, aggregating the updates from the <span id="S5.p4.1.7" class="ltx_text ltx_font_italic">2nd Layer Local Operations</span> for a set number of local rounds, and afterwards propagating them to the global <span id="S5.p4.1.8" class="ltx_text ltx_font_italic">FL Training Collector</span> for aggregation. Again, the instantiated, hierarchical, topology was tested using the CIFAR-10 and German Traffic Sign Recognition Benchmark dataset and obtained results matched these reported in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">In yet another experiment, the star topology with ring-based groups introduced decentralized elements, based on the Tornadoes architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Here, the training process starts with the <span id="S5.p5.1.1" class="ltx_text ltx_font_italic">FL Training Collector</span> sending the initial model to all available <span id="S5.p5.1.2" class="ltx_text ltx_font_italic">FL Local Operations</span>. Then, the <span id="S5.p5.1.3" class="ltx_text ltx_font_italic">FL Local Operations</span> uses every local round to train the model on its local data to pass it to the next instance belonging to its ring-based group, and accept an incoming model from the previous instance, for further training. After a given number of local rounds a global aggregation (performed by the <span id="S5.p5.1.4" class="ltx_text ltx_font_italic">FL Training Collector</span> occurs. As in previous cases, the constructed topology was tested (on the CIFAR-10 and German Traffic Sign Recognition Benchmark datasets) and obtained results match these found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Finally, Fig. <a href="#S5.F4" title="Figure 4 ‣ V Other FL topologies, applicability and usability ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents a solution envisioned for the use case described in Section <a href="#S2" title="II Federated Learning use case in IoT deployment ‣ Introducing Federated Learning into Internet of Things ecosystems – preliminary considerations This work is part of ASSIST-IoT project that has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement 957258. Work of Maria Ganzha and Anastasiya Danilenka is funded in part by the Centre for Priority Research Area Artificial Intelligence and Robotics of Warsaw University of Technology within the Excellence Initiative: Research University (IDUB) programme." class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> using enablers from the proposed architecture. Here, we use a centralized topology where <span id="S5.p6.1.1" class="ltx_text ltx_font_italic">FL Local Operations</span> are run on clients (cameras).
<span id="S5.p6.1.2" class="ltx_text ltx_font_italic">FL Orchestrator</span>, <span id="S5.p6.1.3" class="ltx_text ltx_font_italic">FL Training Collector</span> and <span id="S5.p6.1.4" class="ltx_text ltx_font_italic">FL Repository</span> are located in the cloud. This environment is going to be somewhat more “stable”, because there is a predefined number of clients in the business environment. The main goal is to distribute the processing, instead of sending all the images to the cloud and processing it centrally. Here, although the centralized topology seems to be a good choice for initial implementation, it is clear that a more complex topology will ultimately be needed.
One of the reasons is that in extended deployment groups of scanners (one or more) may belong to different stakeholders. Therefore, a hierarchical topology would be a natural choice. Nonetheless, it has been already established (above) that such topology is easy to deliver using the existing set of enablers.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2207.07700/assets/images/lambis/architecture.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>FL architecture for the car damage use case</figcaption>
</figure>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">On the diagram, besides FL enablers, additional enablers designed and implemented within ASSIST-IoT (following ASSIST-IoT RA) are included addressing: cybersecurity (specifically authentication and authorization), <span id="S5.p7.1.1" class="ltx_text ltx_font_italic">Long Term Storage</span> enabler (that can provide local storage of images for FL clients), and <span id="S5.p7.1.2" class="ltx_text ltx_font_italic">Tactile Dashboard</span> (for visualizations needed in the system). These elements can provide all additional functions needed in the ecosystem.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Concluding remarks</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Even though there is a lot of research in the field of FL, most of it is devoted to FL processes, algorithms, or specific aspects such as data security. Here, we try to address issues related to the deployment of FL system in a real-life use case in an IoT ecosystem. This requires the choice of an appropriate architecture. In this context, the ASSIST-IoT RA was extended to deliver a set of enablers that allow easy configuration of FL system with machine learning parameters, as well as any required topology. Moreover, additional enablers, created for the RA allow turning the FL process into a complete, robust solution.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Q. Xia, W. Ye, Z. Tao, J. Wu, and Q. Li, “A survey of federated learning for
edge computing: Research problems and solutions,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">High-Confidence
Computing</em>, vol. 1, no. 1, p. 100008, 2021. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S266729522100009X

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y. Roh, G. Heo, and S. E. Whang, “A survey on data collection for machine
learning: A big data - ai integration perspective,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions
on Knowledge and Data Engineering</em>, vol. 33, no. 4, pp. 1328–1347, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Murdoch, “Privacy and artificial intelligence: Challenges for protecting
health information in a new era,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">BMC Medical Ethics</em>, vol. 22, no. 1,
2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Zheng, H. Hu, and Z. Han, “Preserving user privacy for machine learning:
Local differential privacy or federated machine learning?” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE
Intelligent Systems</em>, vol. 35, no. 4, pp. 5–14, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, vol. 115, pp. 619–640, 2021.
[Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0167739X20329848

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain and federated
learning for privacy-preserved data sharing in industrial iot,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Industrial Informatics</em>, vol. 16, no. 6, pp. 4177–4186,
2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Q. Hu, P. Sun, S. Yan, Y. Wen, and T. Zhang, “Characterization and prediction
of deep learning workloads in large-scale GPU datacenters,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2109.01313, 2021. [Online]. Available:
https://arxiv.org/abs/2109.01313

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
X. Qiu, T. Parcollet, J. Fernández-Marqués, P. P. B.
de Gusmão, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane, “A first
look into the carbon footprint of federated learning,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2102.07627, 2021. [Online]. Available:
https://arxiv.org/abs/2102.07627

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Federated learning of
deep networks using model averaging,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1602.05629,
2016. [Online]. Available: http://arxiv.org/abs/1602.05629

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, and
H. Vincent Poor, “Federated learning for internet of things: A comprehensive
survey,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys Tutorials</em>, vol. 23, no. 3, pp.
1622–1658, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Kumar, R. Schlegel, E. Rosnes, and A. G. i. Amat,
“Coding for straggler mitigation in federated
learning,” 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Tak and S. Cherkaoui, “Federated edge learning: Design issues and
challenges,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 35, no. 2, pp. 252–258, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L. U. Khan, W. Saad, Z. Han, E. Hossain, and C. S. Hong, “Federated learning
for internet of things: Recent advances, taxonomy, and open challenges,”
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2009.13012, 2020. [Online]. Available:
https://arxiv.org/abs/2009.13012

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Cao, Q. Zhang, and W. Shi, <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Challenges and Opportunities in Edge
Computing</em>.   Cham: Springer
International Publishing, 2018, pp. 59–70.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
C.-H. Hong and B. Varghese, “Resource management in
fog/edge computing: A survey on architectures, infrastructure, and
algorithms,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ACM computing surveys</em>,
vol. 52, no. 5, pp. 1–37, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
<em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">TensorFlow Federated: Machine Learning on Decentralized Data</em>, accessed
in 2022. [Online]. Available:
https://www.tensorflow.org/federated?hl=en

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">An Industrial Grade Federated Learning Framework</em>, accessed in 2022.
[Online]. Available: https://fate.readthedocs.io/en/latest/

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
I. Kholod, E. Yanaki, D. Fomichev, E. Shalugin, E. Novikova, E. Filippov, and
M. Nordlund, “Open-source federated learning frameworks for iot: A
comparative review and analysis,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 21, p. 167, 12 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D. J. Beutel, T. Topal, N. D. Lane, A. Mathur, T. Parcollet, and X. Qiu,
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Flower: A Friendly Federated Learning Framework reference manual</em>.
[Online]. Available: https://flower.dev/docs/

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
OpenMined, <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">PySyft</em>, accessed in 2022. [Online]. Available:
https://blog.openmined.org/tag/pysyft/

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Bellet, A. Kermarrec, and E. Lavoie, “D-cliques: Compensating noniidness in
decentralized federated learning with topology,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2104.07365, 2021. [Online]. Available:
https://arxiv.org/abs/2104.07365

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
L. Chou, Z. Liu, Z. Wang, and A. Shrivastava, “Efficient and less centralized
federated learning,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2106.06627, 2021. [Online].
Available: https://arxiv.org/abs/2106.06627

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Jiang, L. Hu, C. Hu, J. Liu, and Z. Wang, “Bacombo—bandwidth-aware
decentralized federated learning,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Electronics</em>, vol. 9, no. 3, 2020.
[Online]. Available: https://www.mdpi.com/2079-9292/9/3/440

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Lee, J. Oh, S. Lim, S. Yun, and J. Lee, “Tornadoaggregate: Accurate and
scalable federated learning via the ring-based architecture,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2012.03214, 2020. [Online]. Available:
https://arxiv.org/abs/2012.03214

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Duan, D. Liu, X. Chen, R. Liu, Y. Tan, and L. Liang, “Self-balancing
federated learning with global imbalanced data in mobile systems,”
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em>, vol. 32, no. 1,
pp. 59–71, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y. Ding, C. Niu, Y. Yan, Z. Zheng, F. Wu, G. Chen, S. Tang, and R. Jia,
“Distributed optimization over block-cyclic data,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2002.07454, 2020. [Online]. Available:
https://arxiv.org/abs/2002.07454

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
H. Eichner, T. Koren, H. B. McMahan, N. Srebro, and K. Talwar, “Semi-cyclic
stochastic gradient descent,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1904.10120, 2019.
[Online]. Available: http://arxiv.org/abs/1904.10120

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
N. Mhaisen, A. A. Abdellatif, A. Mohamed, A. Erbad, and M. Guizani, “Optimal
user-edge assignment in hierarchical federated learning based on statistical
properties and network topology constraints,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Network Science and Engineering</em>, vol. 9, no. 1, pp. 55–66, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
C. Hu, J. Jiang, and Z. Wang, “Decentralized federated learning: A segmented
gossip approach,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1908.07782, 2019. [Online].
Available: http://arxiv.org/abs/1908.07782

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Z. Chen, D. Li, J. Zhu, and S. Zhang, “Dacfl: Dynamic average consensus based
federated learning in decentralized topology,” 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Fornés-Leal, I. Lacalle, C. E. Palau, P. Szmeja, M. Ganzha, M. Paprzycki,
E. Garro, and F. Blanquer, “Assist-iot: A reference architecture for next
generation internet of things,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 21st
International Conference on Intelligent Software Methodologies, Tools, and
Techniques, IN PRESS</em>, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
K. Nalinaksh, P. Lewandowski, M. Ganzha, M. Paprzycki, W. Pawłowski, and
K. Wasielewska-Michniewska, “Implementing autonomic internet of things
ecosystems – practical considerations,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Parallel Computing
Technologies</em>, V. Malyshkin, Ed.   Cham:
Springer International Publishing, 2021, pp. 420–433.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
D. J. Beutel, T. Topal, A. Mathur, X. Qiu, T. Parcollet, and N. D. Lane,
“Flower: A friendly federated learning research framework,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/2007.14390, 2020. [Online]. Available:
https://arxiv.org/abs/2007.14390

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
G. Van Rossum, <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">The Python Library Reference, release 3.8.2</em>.   Python Software Foundation, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A. Ghosh, J. Chung, D. Yin, and K. Ramchandran, “An efficient framework for
clustered federated learning,” 2020. [Online]. Available:
https://arxiv.org/abs/2006.04088

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving for
simplicity: The all convolutional net,” 2014. [Online]. Available:
https://arxiv.org/abs/1412.6806

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M. Thoma, “Analysis and optimization of convolutional neural network
architectures,” 2017. [Online]. Available:
https://arxiv.org/abs/1707.09725

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.07699" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.07700" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.07700">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.07700" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.07701" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 14:28:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
