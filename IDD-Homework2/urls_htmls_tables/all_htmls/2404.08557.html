<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.08557] SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA</title><meta property="og:description" content="Computer vision models trained on Google Street View images can create material cadastres. However, current approaches need manually annotated datasets that are difficult to obtain and often have class imbalance. To adâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.08557">

<!--Generated on Sun May  5 23:44:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Josie Harrison<sup id="5.5.1" class="ltx_sup">1</sup>, Alexander Hollberg<sup id="6.6.2" class="ltx_sup">1</sup>, and Yinan Yu<sup id="7.7.3" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="8.8.4" class="ltx_sup">1</sup>Chalmers University of Technology, Gothenburg, Sweden
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="9.1" class="ltx_p">Computer vision models trained on Google Street View images can create material cadastres. However, current approaches need manually annotated datasets that are difficult to obtain and often have class imbalance. To address these challenges, this paper fine-tuned a Swin Transformer model on a synthetic dataset generated with OpenAIâ€™s DALL-E and compared the performance to a similar manually annotated dataset. Although manual annotation remains the gold standard, the synthetic dataset performance demonstrates a reasonable alternative. The findings will ease annotation needed to develop material cadastres, offering architects insights into opportunities for material reuse, thus contributing to the reduction of demolition waste.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
synthetic data , computer vision , material recognition , material cadastres , circularity

</div>
<span id="10" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journal: </span>2024 European Conference of Computing in Construction</span></span></span>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Demolition waste from construction and renovation activity is a growing problem internationally, making up 25%-30% of all waste generated in the EU <cite class="ltx_cite ltx_citemacro_citep">(Anastasiou, GeorgiadisÂ Filikas and
Stefanidou, <a href="#bib.bib2" title="" class="ltx_ref">2014</a>)</cite>.
There is a general consensus that current practices have room for improvement for diverting and recovering materials from demolition waste <cite class="ltx_cite ltx_citemacro_citep">(Kabirifar, Mojtahedi, Wang and
Tam, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Recovering demolition waste is not a common activity in many countries because there remain limitations that practitioners currently face. Not having insight into the
availability of which specific materials will be available and at what time is often identified as a top barrier <cite class="ltx_cite ltx_citemacro_citep">(Akanbi, Oyedele, Oyedele and
Salami, <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>. The planning of
projects that could use recovered materials typically begin several years before the actual construction begins. A material cadastre that maps detailed building material
information for every building at the scale of a country with an estimate of when the materials could become available would significantly alleviate the current time window bottleneck.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">Because of the significant potential of this solution, several researchers have turned their attention to developing city-scale material cadastres with computer vision
models trained on images of building exteriors <cite class="ltx_cite ltx_citemacro_citep">(Raghu, Bucher and
DeÂ Wolf, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Arbabi, Lanau, Li, Meyers, Dai, Mayfield and
DensleyÂ Tingley, <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>. A top pain-point in developing these models is acquiring the training data
necessary for machine learning (ML). For deep learning models, some practitioners say that the size of training data should be 10x the number of weights in a network
<cite class="ltx_cite ltx_citemacro_citep">(Baum and Haussler, <a href="#bib.bib4" title="" class="ltx_ref">1988</a>)</cite>â€”often leading to six figure digits and more.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">Additionally, because the type of material is not known before the GPS coordinates are requested to collect training data, this process can lead to some materials (i.e. classes) having higher counts
than other classesâ€”also known as class imbalance. ML practitioners try to avoid class imbalance because a ML model will place importance on a class
that has a higher chance of occurring in the training data. For example, if brick is the most common material in the training data, then the model is likely to predict brick
when deployed in a real-world situation. This effect can be negligible if brick actually is the most common material in a city, but it is preferable to assume equal class
counts since the true distribution of materials in a city is unknown.</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">The scale achieved by previous studies has been at the scale of a city; however, the blue sky vision for a tool like this would be at the scale of a country.
This would mimic current practices of sourcing materials while staying within the bounds of maximum distance for a recovered material to travel before its sustainability
benefits become negligible <cite class="ltx_cite ltx_citemacro_citep">(Ginga, Ongpeng and Daly, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. There is some truth to the logic that if a model works at the scale of a city then it should have similar
performance at the scale of a country; however, previous studies have identified that several problems can arise when scaling an image classification task
<cite class="ltx_cite ltx_citemacro_citep">(Maggiori, Tarabalka, Charpiat and
Alliez, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>; Perronnin, SÃ¡nchez and
Mensink, <a href="#bib.bib17" title="" class="ltx_ref">2010</a>; Hendrycks, Basart, Mazeika, Zou, Kwon,
Mostajabi, Steinhardt and
Song, <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>. For example, wood siding
can take on different colours and textures in different cities since it is often a regional material, but an image classification model may struggle to perform well if it hasnâ€™t
been trained on wood with a particular colour. Because itâ€™s impossible to foresee all potential
quirks that may arise with a given use case, itâ€™s important to move beyond proof-of-concepts at the scale of a city and demonstrate that the model maintains reasonable
performance at the scale of a country.</p>
</div>
<div id="Sx1.p6" class="ltx_para">
<p id="Sx1.p6.1" class="ltx_p">Furthermore, it would be desirable to test an image classification modelâ€™s performance on residential and office interiors since this layer of a building typically has a high rate of change, resulting in a potentially high yield of reusable renovation waste. However, obtaining large and diverse datasets of building interiors can be challenging because of privacy concerns, data access limitations, and the labor-intensive process of data collection and annotation. As a result, public datasets for this task are limited in size, diversity, and representativeness, which may not fully capture the complexity and variability of real-world interior environments. Addressing the scarcity of this type of dataset could unlock an important layer in the quest for creating country-wide material cadastres.</p>
</div>
<div id="Sx1.p7" class="ltx_para">
<p id="Sx1.p7.1" class="ltx_p">Therefore, the aim of this paper is to investigate the potential of using synthetically generated images to augment and extend previous research done to classify faÃ§ade
materials in Google Street View (GSV) images. More specifically, the research questions that this paper addresses are:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">What are the impacts on an image classifierâ€™s performance when correcting the class imbalance of a manually annotated dataset with synthetic data?</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">Is the error distribution of an image classifier trained exclusively on synthetic data comparable to a classifier trained on manually annotated data when both are evaluated on a manually annotated test set?</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">Does a model trained and tested only with synthetic data have a similar error distribution to the manually annotated dataset?</p>
</div>
</li>
</ul>
</div>
<div id="Sx1.p8" class="ltx_para">
<p id="Sx1.p8.1" class="ltx_p">The results from these research questions make several contributions to the field of urban mining for material cadastres. To the authorsâ€™ best knowledge, no previous studies have
utilized synthetic images that mimic GSV images for augmenting and extending an image classification model to detect faÃ§ade material. As a result, we present a novel approach
to reduce the amount of time needed to develop a training dataset for the classification of faÃ§ade materials, as well as provide an indication of the potential to use synthetic images to create datasets that would otherwise be difficult to obtain. Additionally, we present two methods to decrease errors in the current
approaches: utilizing a higher resolution model by reducing the problem space and augmenting a manually annotated dataset to correct class imbalance. The workflow developed
in this research can be extended to detect faÃ§ade materials that have no manually annotated training data and the workflow can be used to improve performance of the current
state-of-the-art for this specific use case.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Background</h2>

<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Material stock datasets</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">In 2023, researchers from ETH Zurich published the first urban-scale ground truth datasetâ€”hereon referred to as the Urban Resource Cadastre (URC) datasetâ€”to detect faÃ§ade
materials in Tokyo, New York City, and Zurich <cite class="ltx_cite ltx_citemacro_citep">(Raghu, Bucher and
DeÂ Wolf, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. The researchers collected this data by first identifying GPS coordinates of
interest, requesting the corresponding GSV image for these GPS coordinates, and manually annotating all resulting images. This dataset contains 971 annotated 400x600 pixel
images with the labels â€˜brickâ€™, â€˜stuccoâ€™, â€˜rusticationâ€™, â€˜metalâ€™, â€˜sidingâ€™, â€˜woodâ€™, â€˜nullâ€™ (for images with no faÃ§ade), and â€˜otherâ€™ (for faÃ§ade material that did
not belong to any of the other labels). However, the authors noted that collecting this type of dataset can easily become a time-consuming process.</p>
</div>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Generating images for training data</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">In recent years, there were significant developments with synthetic images generated for the purpose of training downstream models. Research in this area is often fueled by
use cases where acquiring more data is time/cost prohibitive, it is necessary to mask personally identifiable information for privacy, or it is not possible to collect data
at all <cite class="ltx_cite ltx_citemacro_citep">(Man and Chahl, <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>. There exists a wide variety of generative text-to-image models; however, it has recently been accepted that diffusion models outperform
previous text-to-image generative models. Inspired by non-equilibrium thermodynamics, diffusion models replicate the diffusion process observed in physics by teaching a deep
learning model to add and reverse noise in images <cite class="ltx_cite ltx_citemacro_citep">(Sohl-Dickstein, Weiss, Maheswaranathan and
Ganguli, <a href="#bib.bib20" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
<div id="Sx2.SSx2.p2" class="ltx_para">
<p id="Sx2.SSx2.p2.1" class="ltx_p">The lack of diversity within a synthetic dataset is a known limitation associated with generating images for training models. To overcome this, <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>
used â€œlanguage enhancementâ€ to add variety to the prompts given to the generative model while also using a filter to discard images that didnâ€™t resemble the target image.
The â€œlanguage enhancementâ€ involved using a simple keyword-to-sentence Natural Language Processing (NLP) model based on the T5 model <cite class="ltx_cite ltx_citemacro_citep">(He, Sun, Yu, Xue, Zhang, Torr, Bai and
Qi, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>
to make sentences from keywords. This method to diversify the generated output is prone to straying far from the desired image, however. To counteract this effect,
<cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> proposed a â€œCLIP Filterâ€ that leveraged CLIP zero-shot classification confidence. However, the authors aimed to create a fully
automated process, which could be excessive for use cases where there is enough value in just generating synthetic images aloneâ€”which is this paperâ€™s use case.</p>
</div>
</section>
<section id="Sx2.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">FaÃ§ade materials with high reusability potential</h3>

<div id="Sx2.SSx3.p1" class="ltx_para">
<p id="Sx2.SSx3.p1.1" class="ltx_p">At this point, it becomes apparent that we should justify the selection of specific faÃ§ade materials for the synthetic generation of images. For example, there exist buildings
built with ETFE plastic bubbles, but since these projects are rare, their inclusion in a training dataset would confuse a ML model. While research in the area
of the reusability potential of specific building materials is rather sparse, <cite class="ltx_cite ltx_citemacro_cite">Icibaci (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Iacovidou and Purnell (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> identified stone with lime-based mortar, curtain walls, and concrete panels as materials with high reusability potentialâ€”hereon referred to as the High Reusability Potential (HRP) labels.</p>
</div>
</section>
<section id="Sx2.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Image classification</h3>

<div id="Sx2.SSx4.p1" class="ltx_para">
<p id="Sx2.SSx4.p1.1" class="ltx_p">Originally developed for NLP tasks <cite class="ltx_cite ltx_citemacro_citep">(Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
Gomez, Kaiser and Polosukhin, <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, vision transformers (ViTs) are deep learning models that take
an image separated into patches (with its positional information saved), encode these patches into a memory, and compare that memory with a target value
<cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit and
Houlsby, <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>. The memory is created with â€˜attentionâ€™, which maps all the patches to each other to establish relative importance between them
all <cite class="ltx_cite ltx_citemacro_citep">(Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
Gomez, Kaiser and Polosukhin, <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>. The Swin Transformer is a variant of the ViT methodology, which has shown to outperform many other models on image classification tasks
while also being conscious of computational efficiency <cite class="ltx_cite ltx_citemacro_citep">(Liu, Lin, Cao, Hu, Wei, Zhang, Lin and
Guo, <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<figure id="Sx2.F1" class="ltx_figure"><img src="/html/2404.08557/assets/figure_1.png" id="Sx2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Workflow showing all four experiments: baseline, augmented, mixed, synthetic with sub-experiments for stucco and siding.</figcaption>
</figure>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiment</h2>

<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Workflow</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.1" class="ltx_p">The workflow shown in FigureÂ <a href="#Sx2.F1" title="Figure 1 â€£ Image classification â€£ Background â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> assembled all materials and methods together into four experiments. There are three main experiments compared to a baseline
experiment: augmented, mixed, and synthetic. For each experiment, a pre-trained image classification model was fine-tuned with the respective dataset, with synthetic images
first passing through a brute-force image quality evaluation. After model training, the predictions were evaluated on the experimentâ€™s test dataset. The synthetic experimentâ€™s
test set is entirely synthetic while the other three experiments use a manually annotated test set.</p>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Materials</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.1" class="ltx_p">Drawing on previous research, four datasets (baseline, augmented, mixed, and synthetic) were collected for the purposes of training an image classification model to replicate
previous work and to also train a model on unseen labels. The baseline, augmented, and mixed datasets used the URC labels, while the synthetic dataset used the HRP labels. For
the augmented, mixed, and synthetic experiments (shown in FigureÂ <a href="#Sx3.F2" title="Figure 2 â€£ Materials â€£ Experiment â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), varying amounts of 512x512 pixel synthetic images were generated. This
resolution was suitable for image classification models that can be trained on consumer-grade hardware.</p>
</div>
<div id="Sx3.SSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.p2.1" class="ltx_p">For the baseline, augmented, and mixed experiments, the number of labels was reduced to three to create sub-experiments: null, other, and one label of interest in order to
reduce the parameter space of the image classification model. â€˜Stuccoâ€™ and â€˜sidingâ€™ were isolated as two labels of interest because â€˜stuccoâ€™ had a similar class distribution
as the â€˜nullâ€™ and â€˜otherâ€™ labels, and because â€˜sidingâ€™ had a high class imbalance in the original dataset. Therefore, there were two sub-experiments for the baseline,
augmented, and mixed experiments using two sets of labels: â€œnull, other, stuccoâ€ and â€œnull, other, sidingâ€.</p>
</div>
<div id="Sx3.SSx2.p3" class="ltx_para">
<p id="Sx3.SSx2.p3.1" class="ltx_p">As mentioned earlier, the URC dataset came with class imbalance, so the augmented experiment corrected this class imbalance with synthetic images. The mixed experiment used
only synthetic images for training while leaving the entire URC dataset as the test dataset. Finally, the synthetic experiment trained and tested entirely on synthetic images
using the HRP labels: â€˜stoneâ€™, â€˜curtain wallâ€™, and â€˜concrete panelsâ€™. The synthetic experiment dataset size was made to match the mixed stucco experimentâ€™s dataset set to
maintain comparability.</p>
</div>
<figure id="Sx3.F2" class="ltx_figure"><img src="/html/2404.08557/assets/x1.png" id="Sx3.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="664" height="1118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Class distributions for the different datasets.</figcaption>
</figure>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Methods</h3>

<div id="Sx3.SSx3.p1" class="ltx_para">
<p id="Sx3.SSx3.p1.1" class="ltx_p">There are several open source versions of diffusion models that are capable of creating synthetic images; however, the effort required to get them into an operational state can often be time-prohibitive while also
requiring intensive hardware. For that reason, OpenAIâ€™s DALL-E 2 model was chosen to generate images because it is the most effective to leverage with just a simple API request. A portmanteau of the famous surrealist artist Salvador DalÃ­ and the Pixar character WALL-E, DALL-E takes in a user defined text prompt to create highly realistic images. The DALL-E 2 text-to-image generation model was trained on 650 million images <cite class="ltx_cite ltx_citemacro_citep">(Ramesh etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>; however, OpenAI does not disclose the source of training data for any of their models <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>.
To reduce this dependency on an external organization, it would be ideal to develop an open source model in-house, however.</p>
</div>
<div id="Sx3.SSx3.p2" class="ltx_para">
<p id="Sx3.SSx3.p2.1" class="ltx_p">A limitation of generating synthetic GSV images was the inability for DALL-E to create believable images with more than one faÃ§ade material. Other researchers found
similar difficulties when requesting more than three components with descriptions about shape or colours <cite class="ltx_cite ltx_citemacro_citep">(Marcus, Davis and
Aaronson, <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>. Additionally, it was
possible to utilize a higher resolution image classification model by reducing the label set to one material of interest combined with the â€˜nullâ€™ and â€˜otherâ€™ labels. For
these reasons, it was decided to generate images with one label. Since this paperâ€™s use case would not require predicting several labels in real-time, it is
proposed that the implementation would require several models with each model focusing on one label (as well as the non-buildings and other materials).</p>
</div>
<div id="Sx3.SSx3.p3" class="ltx_para">
<p id="Sx3.SSx3.p3.1" class="ltx_p">As mentioned earlier, there was enough value in creating the synthetic images alone to not utilize a fully automated filtering process. Therefore, a brute force version of
this process was suitable. This brute force method shown in FigureÂ <a href="#Sx3.F3" title="Figure 3 â€£ Methods â€£ Experiment â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> involved generating a collection of prompts from keywords, generating a few
hundred images by randomly sampling a prompt from the collection, manually removing irrelevant images from the final dataset, and taking note of the few prompts that were
likely to produce the best images. The remaining dataset was generated using this final collection of â€˜high batting averageâ€™ prompts.</p>
</div>
<div id="Sx3.SSx3.p4" class="ltx_para">
<p id="Sx3.SSx3.p4.1" class="ltx_p">To achieve a classification model that can perform at the scale of a country, it is desirable to encourage diversity in the synthetic images by injecting additional keywords into the prompt. For this experiment, the facade material label was given possible synonyms, a time period when the faÃ§ade material was likely to be common, and cities where the facade material can be found. However, these keywords could be extended to any aspect of a building, such as type, size, or neighbourhood density. For example, â€˜sidingâ€™ was assigned the
synonyms â€™shiplapâ€™, â€™feather edgeâ€™, â€™fiber cementâ€™ and the time period â€™20th centuryâ€™. For all labels in the augmented and mixed experiments, the original URC
dataset cities (New York City, Zurich, and Tokyo) were used as keywords. For the synthetic experiments, the cities â€™Vancouverâ€™, â€™San Franciscoâ€™, and â€™Amsterdamâ€™ were chosen arbitrarily but would benefit from additional cities in future experiments.</p>
</div>
<div id="Sx3.SSx3.p5" class="ltx_para">
<p id="Sx3.SSx3.p5.1" class="ltx_p">For the image classification task, it was decided to use a Swin Transformer model pre-trained on the ImageNet-21k dataset to maximize comparability to previous studies and
to leverage a well-known image classification model. A pre-trained image classification model is trained on a large generic dataset with the intention to later â€™fine-tuneâ€™ the â€™headâ€™ of the model on tailored datasets. This method generally improves accuracy and reduces the need for a large tailored dataset. The ImageNet-21k dataset is commonly used for pre-training because it is 14 million images with 21,000 labels and contains a variety of natural images. <cite class="ltx_cite ltx_citemacro_cite">Raghu etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> achieved a macro-averaged F1 score of 0.93 by applying the pre-trained Swin Transformer v2 model
to the URC dataset. However, the authors used the model version with a resolution of 192 pixels, which was not adequate for the synthetic image dataset. It was seen that the
synthetic images struggled with creating the same texture detail as real images, and this data loss was compounded when used with a low-resolution model. This is akin to
taking a photocopy of a photocopy. Therefore, the experiments in this study use the Swin Transformer v2 model with 384 pixel resolution.</p>
</div>
<figure id="Sx3.F3" class="ltx_figure"><img src="/html/2404.08557/assets/brute_force_method.png" id="Sx3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="304" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Brute force method to filter irrelevant synthetic images from accepted images.</figcaption>
</figure>
<div id="Sx3.SSx3.p6" class="ltx_para">
<p id="Sx3.SSx3.p6.1" class="ltx_p">To provide a quantitative and objective measurement of the image classification performance, it was decided to calculate the weighted F1 score, precision, recall, as well as
create a receiver operating characteristic (ROC) graph and confusion matrices for each experiment. The confusion matrix is a grid of all target labels with the y-axis
representing the true test values and the x-axis representing the predicted test values. In this way, itâ€™s possible to index how many samples were predicted for a specific
label: the first cell in the grid shows the number of samples predicted as â€˜nullâ€™ were correctly classified. It is not possible to decide which dataset performed the best by
looking at these confusion matrices, however, so quantitative metrics were calculated: precision and recall.</p>
</div>
<div id="Sx3.SSx3.p7" class="ltx_para">
<table id="Sx3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="Sx3.E1.m1.1" class="ltx_Math" alttext="precision=\frac{tp}{tp+fp}" display="block"><semantics id="Sx3.E1.m1.1a"><mrow id="Sx3.E1.m1.1.1" xref="Sx3.E1.m1.1.1.cmml"><mrow id="Sx3.E1.m1.1.1.2" xref="Sx3.E1.m1.1.1.2.cmml"><mi id="Sx3.E1.m1.1.1.2.2" xref="Sx3.E1.m1.1.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.2.1" xref="Sx3.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.2.3" xref="Sx3.E1.m1.1.1.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.2.1a" xref="Sx3.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.2.4" xref="Sx3.E1.m1.1.1.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.2.1b" xref="Sx3.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.2.5" xref="Sx3.E1.m1.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.2.1c" xref="Sx3.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.2.6" xref="Sx3.E1.m1.1.1.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.2.1d" xref="Sx3.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.2.7" xref="Sx3.E1.m1.1.1.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.2.1e" xref="Sx3.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.2.8" xref="Sx3.E1.m1.1.1.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.2.1f" xref="Sx3.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.2.9" xref="Sx3.E1.m1.1.1.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.2.1g" xref="Sx3.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.2.10" xref="Sx3.E1.m1.1.1.2.10.cmml">n</mi></mrow><mo id="Sx3.E1.m1.1.1.1" xref="Sx3.E1.m1.1.1.1.cmml">=</mo><mfrac id="Sx3.E1.m1.1.1.3" xref="Sx3.E1.m1.1.1.3.cmml"><mrow id="Sx3.E1.m1.1.1.3.2" xref="Sx3.E1.m1.1.1.3.2.cmml"><mi id="Sx3.E1.m1.1.1.3.2.2" xref="Sx3.E1.m1.1.1.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.3.2.1" xref="Sx3.E1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.3.2.3" xref="Sx3.E1.m1.1.1.3.2.3.cmml">p</mi></mrow><mrow id="Sx3.E1.m1.1.1.3.3" xref="Sx3.E1.m1.1.1.3.3.cmml"><mrow id="Sx3.E1.m1.1.1.3.3.2" xref="Sx3.E1.m1.1.1.3.3.2.cmml"><mi id="Sx3.E1.m1.1.1.3.3.2.2" xref="Sx3.E1.m1.1.1.3.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.3.3.2.1" xref="Sx3.E1.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.3.3.2.3" xref="Sx3.E1.m1.1.1.3.3.2.3.cmml">p</mi></mrow><mo id="Sx3.E1.m1.1.1.3.3.1" xref="Sx3.E1.m1.1.1.3.3.1.cmml">+</mo><mrow id="Sx3.E1.m1.1.1.3.3.3" xref="Sx3.E1.m1.1.1.3.3.3.cmml"><mi id="Sx3.E1.m1.1.1.3.3.3.2" xref="Sx3.E1.m1.1.1.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.1.1.3.3.3.1" xref="Sx3.E1.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.1.1.3.3.3.3" xref="Sx3.E1.m1.1.1.3.3.3.3.cmml">p</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E1.m1.1b"><apply id="Sx3.E1.m1.1.1.cmml" xref="Sx3.E1.m1.1.1"><eq id="Sx3.E1.m1.1.1.1.cmml" xref="Sx3.E1.m1.1.1.1"></eq><apply id="Sx3.E1.m1.1.1.2.cmml" xref="Sx3.E1.m1.1.1.2"><times id="Sx3.E1.m1.1.1.2.1.cmml" xref="Sx3.E1.m1.1.1.2.1"></times><ci id="Sx3.E1.m1.1.1.2.2.cmml" xref="Sx3.E1.m1.1.1.2.2">ğ‘</ci><ci id="Sx3.E1.m1.1.1.2.3.cmml" xref="Sx3.E1.m1.1.1.2.3">ğ‘Ÿ</ci><ci id="Sx3.E1.m1.1.1.2.4.cmml" xref="Sx3.E1.m1.1.1.2.4">ğ‘’</ci><ci id="Sx3.E1.m1.1.1.2.5.cmml" xref="Sx3.E1.m1.1.1.2.5">ğ‘</ci><ci id="Sx3.E1.m1.1.1.2.6.cmml" xref="Sx3.E1.m1.1.1.2.6">ğ‘–</ci><ci id="Sx3.E1.m1.1.1.2.7.cmml" xref="Sx3.E1.m1.1.1.2.7">ğ‘ </ci><ci id="Sx3.E1.m1.1.1.2.8.cmml" xref="Sx3.E1.m1.1.1.2.8">ğ‘–</ci><ci id="Sx3.E1.m1.1.1.2.9.cmml" xref="Sx3.E1.m1.1.1.2.9">ğ‘œ</ci><ci id="Sx3.E1.m1.1.1.2.10.cmml" xref="Sx3.E1.m1.1.1.2.10">ğ‘›</ci></apply><apply id="Sx3.E1.m1.1.1.3.cmml" xref="Sx3.E1.m1.1.1.3"><divide id="Sx3.E1.m1.1.1.3.1.cmml" xref="Sx3.E1.m1.1.1.3"></divide><apply id="Sx3.E1.m1.1.1.3.2.cmml" xref="Sx3.E1.m1.1.1.3.2"><times id="Sx3.E1.m1.1.1.3.2.1.cmml" xref="Sx3.E1.m1.1.1.3.2.1"></times><ci id="Sx3.E1.m1.1.1.3.2.2.cmml" xref="Sx3.E1.m1.1.1.3.2.2">ğ‘¡</ci><ci id="Sx3.E1.m1.1.1.3.2.3.cmml" xref="Sx3.E1.m1.1.1.3.2.3">ğ‘</ci></apply><apply id="Sx3.E1.m1.1.1.3.3.cmml" xref="Sx3.E1.m1.1.1.3.3"><plus id="Sx3.E1.m1.1.1.3.3.1.cmml" xref="Sx3.E1.m1.1.1.3.3.1"></plus><apply id="Sx3.E1.m1.1.1.3.3.2.cmml" xref="Sx3.E1.m1.1.1.3.3.2"><times id="Sx3.E1.m1.1.1.3.3.2.1.cmml" xref="Sx3.E1.m1.1.1.3.3.2.1"></times><ci id="Sx3.E1.m1.1.1.3.3.2.2.cmml" xref="Sx3.E1.m1.1.1.3.3.2.2">ğ‘¡</ci><ci id="Sx3.E1.m1.1.1.3.3.2.3.cmml" xref="Sx3.E1.m1.1.1.3.3.2.3">ğ‘</ci></apply><apply id="Sx3.E1.m1.1.1.3.3.3.cmml" xref="Sx3.E1.m1.1.1.3.3.3"><times id="Sx3.E1.m1.1.1.3.3.3.1.cmml" xref="Sx3.E1.m1.1.1.3.3.3.1"></times><ci id="Sx3.E1.m1.1.1.3.3.3.2.cmml" xref="Sx3.E1.m1.1.1.3.3.3.2">ğ‘“</ci><ci id="Sx3.E1.m1.1.1.3.3.3.3.cmml" xref="Sx3.E1.m1.1.1.3.3.3.3">ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E1.m1.1c">precision=\frac{tp}{tp+fp}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="Sx3.SSx3.p8" class="ltx_para">
<table id="Sx3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="Sx3.E2.m1.1" class="ltx_Math" alttext="recall=\frac{tp}{tp+fn}" display="block"><semantics id="Sx3.E2.m1.1a"><mrow id="Sx3.E2.m1.1.1" xref="Sx3.E2.m1.1.1.cmml"><mrow id="Sx3.E2.m1.1.1.2" xref="Sx3.E2.m1.1.1.2.cmml"><mi id="Sx3.E2.m1.1.1.2.2" xref="Sx3.E2.m1.1.1.2.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx3.E2.m1.1.1.2.1" xref="Sx3.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E2.m1.1.1.2.3" xref="Sx3.E2.m1.1.1.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="Sx3.E2.m1.1.1.2.1a" xref="Sx3.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E2.m1.1.1.2.4" xref="Sx3.E2.m1.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.E2.m1.1.1.2.1b" xref="Sx3.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E2.m1.1.1.2.5" xref="Sx3.E2.m1.1.1.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx3.E2.m1.1.1.2.1c" xref="Sx3.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E2.m1.1.1.2.6" xref="Sx3.E2.m1.1.1.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx3.E2.m1.1.1.2.1d" xref="Sx3.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="Sx3.E2.m1.1.1.2.7" xref="Sx3.E2.m1.1.1.2.7.cmml">l</mi></mrow><mo id="Sx3.E2.m1.1.1.1" xref="Sx3.E2.m1.1.1.1.cmml">=</mo><mfrac id="Sx3.E2.m1.1.1.3" xref="Sx3.E2.m1.1.1.3.cmml"><mrow id="Sx3.E2.m1.1.1.3.2" xref="Sx3.E2.m1.1.1.3.2.cmml"><mi id="Sx3.E2.m1.1.1.3.2.2" xref="Sx3.E2.m1.1.1.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="Sx3.E2.m1.1.1.3.2.1" xref="Sx3.E2.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="Sx3.E2.m1.1.1.3.2.3" xref="Sx3.E2.m1.1.1.3.2.3.cmml">p</mi></mrow><mrow id="Sx3.E2.m1.1.1.3.3" xref="Sx3.E2.m1.1.1.3.3.cmml"><mrow id="Sx3.E2.m1.1.1.3.3.2" xref="Sx3.E2.m1.1.1.3.3.2.cmml"><mi id="Sx3.E2.m1.1.1.3.3.2.2" xref="Sx3.E2.m1.1.1.3.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="Sx3.E2.m1.1.1.3.3.2.1" xref="Sx3.E2.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E2.m1.1.1.3.3.2.3" xref="Sx3.E2.m1.1.1.3.3.2.3.cmml">p</mi></mrow><mo id="Sx3.E2.m1.1.1.3.3.1" xref="Sx3.E2.m1.1.1.3.3.1.cmml">+</mo><mrow id="Sx3.E2.m1.1.1.3.3.3" xref="Sx3.E2.m1.1.1.3.3.3.cmml"><mi id="Sx3.E2.m1.1.1.3.3.3.2" xref="Sx3.E2.m1.1.1.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="Sx3.E2.m1.1.1.3.3.3.1" xref="Sx3.E2.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx3.E2.m1.1.1.3.3.3.3" xref="Sx3.E2.m1.1.1.3.3.3.3.cmml">n</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E2.m1.1b"><apply id="Sx3.E2.m1.1.1.cmml" xref="Sx3.E2.m1.1.1"><eq id="Sx3.E2.m1.1.1.1.cmml" xref="Sx3.E2.m1.1.1.1"></eq><apply id="Sx3.E2.m1.1.1.2.cmml" xref="Sx3.E2.m1.1.1.2"><times id="Sx3.E2.m1.1.1.2.1.cmml" xref="Sx3.E2.m1.1.1.2.1"></times><ci id="Sx3.E2.m1.1.1.2.2.cmml" xref="Sx3.E2.m1.1.1.2.2">ğ‘Ÿ</ci><ci id="Sx3.E2.m1.1.1.2.3.cmml" xref="Sx3.E2.m1.1.1.2.3">ğ‘’</ci><ci id="Sx3.E2.m1.1.1.2.4.cmml" xref="Sx3.E2.m1.1.1.2.4">ğ‘</ci><ci id="Sx3.E2.m1.1.1.2.5.cmml" xref="Sx3.E2.m1.1.1.2.5">ğ‘</ci><ci id="Sx3.E2.m1.1.1.2.6.cmml" xref="Sx3.E2.m1.1.1.2.6">ğ‘™</ci><ci id="Sx3.E2.m1.1.1.2.7.cmml" xref="Sx3.E2.m1.1.1.2.7">ğ‘™</ci></apply><apply id="Sx3.E2.m1.1.1.3.cmml" xref="Sx3.E2.m1.1.1.3"><divide id="Sx3.E2.m1.1.1.3.1.cmml" xref="Sx3.E2.m1.1.1.3"></divide><apply id="Sx3.E2.m1.1.1.3.2.cmml" xref="Sx3.E2.m1.1.1.3.2"><times id="Sx3.E2.m1.1.1.3.2.1.cmml" xref="Sx3.E2.m1.1.1.3.2.1"></times><ci id="Sx3.E2.m1.1.1.3.2.2.cmml" xref="Sx3.E2.m1.1.1.3.2.2">ğ‘¡</ci><ci id="Sx3.E2.m1.1.1.3.2.3.cmml" xref="Sx3.E2.m1.1.1.3.2.3">ğ‘</ci></apply><apply id="Sx3.E2.m1.1.1.3.3.cmml" xref="Sx3.E2.m1.1.1.3.3"><plus id="Sx3.E2.m1.1.1.3.3.1.cmml" xref="Sx3.E2.m1.1.1.3.3.1"></plus><apply id="Sx3.E2.m1.1.1.3.3.2.cmml" xref="Sx3.E2.m1.1.1.3.3.2"><times id="Sx3.E2.m1.1.1.3.3.2.1.cmml" xref="Sx3.E2.m1.1.1.3.3.2.1"></times><ci id="Sx3.E2.m1.1.1.3.3.2.2.cmml" xref="Sx3.E2.m1.1.1.3.3.2.2">ğ‘¡</ci><ci id="Sx3.E2.m1.1.1.3.3.2.3.cmml" xref="Sx3.E2.m1.1.1.3.3.2.3">ğ‘</ci></apply><apply id="Sx3.E2.m1.1.1.3.3.3.cmml" xref="Sx3.E2.m1.1.1.3.3.3"><times id="Sx3.E2.m1.1.1.3.3.3.1.cmml" xref="Sx3.E2.m1.1.1.3.3.3.1"></times><ci id="Sx3.E2.m1.1.1.3.3.3.2.cmml" xref="Sx3.E2.m1.1.1.3.3.3.2">ğ‘“</ci><ci id="Sx3.E2.m1.1.1.3.3.3.3.cmml" xref="Sx3.E2.m1.1.1.3.3.3.3">ğ‘›</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E2.m1.1c">recall=\frac{tp}{tp+fn}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="Sx3.SSx3.p9" class="ltx_para">
<p id="Sx3.SSx3.p9.1" class="ltx_p">Precision (<a href="#Sx3.E1" title="In Methods â€£ Experiment â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) refers to the likelihood that a prediction will be correct, while recall (<a href="#Sx3.E2" title="In Methods â€£ Experiment â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) refers to the likelihood that all samples will be
correctly classified. In its basic binary form, the precision is the true positives (<span id="Sx3.SSx3.p9.1.1" class="ltx_text ltx_font_italic">tp</span>) divided by all samples marked as positive (<span id="Sx3.SSx3.p9.1.2" class="ltx_text ltx_font_italic">tp + fp</span>) (<a href="#Sx3.E2" title="In Methods â€£ Experiment â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Similarly, the recall formula (<a href="#Sx3.E2" title="In Methods â€£ Experiment â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) divides the true positives (<span id="Sx3.SSx3.p9.1.3" class="ltx_text ltx_font_italic">tp</span>) by the sum of true positives and false negatives (<span id="Sx3.SSx3.p9.1.4" class="ltx_text ltx_font_italic">tp + fn</span>). The F1 score
(<a href="#Sx3.E3" title="In Methods â€£ Experiment â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) provides a harmonic mean of both precision and recall.</p>
</div>
<div id="Sx3.SSx3.p10" class="ltx_para">
<table id="Sx3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="Sx3.E3.m1.1" class="ltx_Math" alttext="F=\frac{2*precision*recall}{precision+recall}" display="block"><semantics id="Sx3.E3.m1.1a"><mrow id="Sx3.E3.m1.1.1" xref="Sx3.E3.m1.1.1.cmml"><mi id="Sx3.E3.m1.1.1.2" xref="Sx3.E3.m1.1.1.2.cmml">F</mi><mo id="Sx3.E3.m1.1.1.1" xref="Sx3.E3.m1.1.1.1.cmml">=</mo><mfrac id="Sx3.E3.m1.1.1.3" xref="Sx3.E3.m1.1.1.3.cmml"><mrow id="Sx3.E3.m1.1.1.3.2" xref="Sx3.E3.m1.1.1.3.2.cmml"><mrow id="Sx3.E3.m1.1.1.3.2.2" xref="Sx3.E3.m1.1.1.3.2.2.cmml"><mrow id="Sx3.E3.m1.1.1.3.2.2.2" xref="Sx3.E3.m1.1.1.3.2.2.2.cmml"><mrow id="Sx3.E3.m1.1.1.3.2.2.2.2" xref="Sx3.E3.m1.1.1.3.2.2.2.2.cmml"><mn id="Sx3.E3.m1.1.1.3.2.2.2.2.2" xref="Sx3.E3.m1.1.1.3.2.2.2.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="Sx3.E3.m1.1.1.3.2.2.2.2.1" xref="Sx3.E3.m1.1.1.3.2.2.2.2.1.cmml">âˆ—</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.2.3" xref="Sx3.E3.m1.1.1.3.2.2.2.2.3.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.2.2.1" xref="Sx3.E3.m1.1.1.3.2.2.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.3" xref="Sx3.E3.m1.1.1.3.2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.2.2.1a" xref="Sx3.E3.m1.1.1.3.2.2.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.4" xref="Sx3.E3.m1.1.1.3.2.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.2.2.1b" xref="Sx3.E3.m1.1.1.3.2.2.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.5" xref="Sx3.E3.m1.1.1.3.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.2.2.1c" xref="Sx3.E3.m1.1.1.3.2.2.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.6" xref="Sx3.E3.m1.1.1.3.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.2.2.1d" xref="Sx3.E3.m1.1.1.3.2.2.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.7" xref="Sx3.E3.m1.1.1.3.2.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.2.2.1e" xref="Sx3.E3.m1.1.1.3.2.2.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.8" xref="Sx3.E3.m1.1.1.3.2.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.2.2.1f" xref="Sx3.E3.m1.1.1.3.2.2.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.9" xref="Sx3.E3.m1.1.1.3.2.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.2.2.1g" xref="Sx3.E3.m1.1.1.3.2.2.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.2.2.10" xref="Sx3.E3.m1.1.1.3.2.2.2.10.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="Sx3.E3.m1.1.1.3.2.2.1" xref="Sx3.E3.m1.1.1.3.2.2.1.cmml">âˆ—</mo><mi id="Sx3.E3.m1.1.1.3.2.2.3" xref="Sx3.E3.m1.1.1.3.2.2.3.cmml">r</mi></mrow><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.1" xref="Sx3.E3.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.3" xref="Sx3.E3.m1.1.1.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.1a" xref="Sx3.E3.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.4" xref="Sx3.E3.m1.1.1.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.1b" xref="Sx3.E3.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.5" xref="Sx3.E3.m1.1.1.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.1c" xref="Sx3.E3.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.6" xref="Sx3.E3.m1.1.1.3.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.2.1d" xref="Sx3.E3.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.2.7" xref="Sx3.E3.m1.1.1.3.2.7.cmml">l</mi></mrow><mrow id="Sx3.E3.m1.1.1.3.3" xref="Sx3.E3.m1.1.1.3.3.cmml"><mrow id="Sx3.E3.m1.1.1.3.3.2" xref="Sx3.E3.m1.1.1.3.3.2.cmml"><mi id="Sx3.E3.m1.1.1.3.3.2.2" xref="Sx3.E3.m1.1.1.3.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.2.1" xref="Sx3.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.2.3" xref="Sx3.E3.m1.1.1.3.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.2.1a" xref="Sx3.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.2.4" xref="Sx3.E3.m1.1.1.3.3.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.2.1b" xref="Sx3.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.2.5" xref="Sx3.E3.m1.1.1.3.3.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.2.1c" xref="Sx3.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.2.6" xref="Sx3.E3.m1.1.1.3.3.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.2.1d" xref="Sx3.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.2.7" xref="Sx3.E3.m1.1.1.3.3.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.2.1e" xref="Sx3.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.2.8" xref="Sx3.E3.m1.1.1.3.3.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.2.1f" xref="Sx3.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.2.9" xref="Sx3.E3.m1.1.1.3.3.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.2.1g" xref="Sx3.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.2.10" xref="Sx3.E3.m1.1.1.3.3.2.10.cmml">n</mi></mrow><mo id="Sx3.E3.m1.1.1.3.3.1" xref="Sx3.E3.m1.1.1.3.3.1.cmml">+</mo><mrow id="Sx3.E3.m1.1.1.3.3.3" xref="Sx3.E3.m1.1.1.3.3.3.cmml"><mi id="Sx3.E3.m1.1.1.3.3.3.2" xref="Sx3.E3.m1.1.1.3.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.3.1" xref="Sx3.E3.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.3.3" xref="Sx3.E3.m1.1.1.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.3.1a" xref="Sx3.E3.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.3.4" xref="Sx3.E3.m1.1.1.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.3.1b" xref="Sx3.E3.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.3.5" xref="Sx3.E3.m1.1.1.3.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.3.1c" xref="Sx3.E3.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.3.6" xref="Sx3.E3.m1.1.1.3.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="Sx3.E3.m1.1.1.3.3.3.1d" xref="Sx3.E3.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="Sx3.E3.m1.1.1.3.3.3.7" xref="Sx3.E3.m1.1.1.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E3.m1.1b"><apply id="Sx3.E3.m1.1.1.cmml" xref="Sx3.E3.m1.1.1"><eq id="Sx3.E3.m1.1.1.1.cmml" xref="Sx3.E3.m1.1.1.1"></eq><ci id="Sx3.E3.m1.1.1.2.cmml" xref="Sx3.E3.m1.1.1.2">ğ¹</ci><apply id="Sx3.E3.m1.1.1.3.cmml" xref="Sx3.E3.m1.1.1.3"><divide id="Sx3.E3.m1.1.1.3.1.cmml" xref="Sx3.E3.m1.1.1.3"></divide><apply id="Sx3.E3.m1.1.1.3.2.cmml" xref="Sx3.E3.m1.1.1.3.2"><times id="Sx3.E3.m1.1.1.3.2.1.cmml" xref="Sx3.E3.m1.1.1.3.2.1"></times><apply id="Sx3.E3.m1.1.1.3.2.2.cmml" xref="Sx3.E3.m1.1.1.3.2.2"><times id="Sx3.E3.m1.1.1.3.2.2.1.cmml" xref="Sx3.E3.m1.1.1.3.2.2.1"></times><apply id="Sx3.E3.m1.1.1.3.2.2.2.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2"><times id="Sx3.E3.m1.1.1.3.2.2.2.1.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.1"></times><apply id="Sx3.E3.m1.1.1.3.2.2.2.2.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.2"><times id="Sx3.E3.m1.1.1.3.2.2.2.2.1.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.2.1"></times><cn type="integer" id="Sx3.E3.m1.1.1.3.2.2.2.2.2.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.2.2">2</cn><ci id="Sx3.E3.m1.1.1.3.2.2.2.2.3.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.2.3">ğ‘</ci></apply><ci id="Sx3.E3.m1.1.1.3.2.2.2.3.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.3">ğ‘Ÿ</ci><ci id="Sx3.E3.m1.1.1.3.2.2.2.4.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.4">ğ‘’</ci><ci id="Sx3.E3.m1.1.1.3.2.2.2.5.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.5">ğ‘</ci><ci id="Sx3.E3.m1.1.1.3.2.2.2.6.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.6">ğ‘–</ci><ci id="Sx3.E3.m1.1.1.3.2.2.2.7.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.7">ğ‘ </ci><ci id="Sx3.E3.m1.1.1.3.2.2.2.8.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.8">ğ‘–</ci><ci id="Sx3.E3.m1.1.1.3.2.2.2.9.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.9">ğ‘œ</ci><ci id="Sx3.E3.m1.1.1.3.2.2.2.10.cmml" xref="Sx3.E3.m1.1.1.3.2.2.2.10">ğ‘›</ci></apply><ci id="Sx3.E3.m1.1.1.3.2.2.3.cmml" xref="Sx3.E3.m1.1.1.3.2.2.3">ğ‘Ÿ</ci></apply><ci id="Sx3.E3.m1.1.1.3.2.3.cmml" xref="Sx3.E3.m1.1.1.3.2.3">ğ‘’</ci><ci id="Sx3.E3.m1.1.1.3.2.4.cmml" xref="Sx3.E3.m1.1.1.3.2.4">ğ‘</ci><ci id="Sx3.E3.m1.1.1.3.2.5.cmml" xref="Sx3.E3.m1.1.1.3.2.5">ğ‘</ci><ci id="Sx3.E3.m1.1.1.3.2.6.cmml" xref="Sx3.E3.m1.1.1.3.2.6">ğ‘™</ci><ci id="Sx3.E3.m1.1.1.3.2.7.cmml" xref="Sx3.E3.m1.1.1.3.2.7">ğ‘™</ci></apply><apply id="Sx3.E3.m1.1.1.3.3.cmml" xref="Sx3.E3.m1.1.1.3.3"><plus id="Sx3.E3.m1.1.1.3.3.1.cmml" xref="Sx3.E3.m1.1.1.3.3.1"></plus><apply id="Sx3.E3.m1.1.1.3.3.2.cmml" xref="Sx3.E3.m1.1.1.3.3.2"><times id="Sx3.E3.m1.1.1.3.3.2.1.cmml" xref="Sx3.E3.m1.1.1.3.3.2.1"></times><ci id="Sx3.E3.m1.1.1.3.3.2.2.cmml" xref="Sx3.E3.m1.1.1.3.3.2.2">ğ‘</ci><ci id="Sx3.E3.m1.1.1.3.3.2.3.cmml" xref="Sx3.E3.m1.1.1.3.3.2.3">ğ‘Ÿ</ci><ci id="Sx3.E3.m1.1.1.3.3.2.4.cmml" xref="Sx3.E3.m1.1.1.3.3.2.4">ğ‘’</ci><ci id="Sx3.E3.m1.1.1.3.3.2.5.cmml" xref="Sx3.E3.m1.1.1.3.3.2.5">ğ‘</ci><ci id="Sx3.E3.m1.1.1.3.3.2.6.cmml" xref="Sx3.E3.m1.1.1.3.3.2.6">ğ‘–</ci><ci id="Sx3.E3.m1.1.1.3.3.2.7.cmml" xref="Sx3.E3.m1.1.1.3.3.2.7">ğ‘ </ci><ci id="Sx3.E3.m1.1.1.3.3.2.8.cmml" xref="Sx3.E3.m1.1.1.3.3.2.8">ğ‘–</ci><ci id="Sx3.E3.m1.1.1.3.3.2.9.cmml" xref="Sx3.E3.m1.1.1.3.3.2.9">ğ‘œ</ci><ci id="Sx3.E3.m1.1.1.3.3.2.10.cmml" xref="Sx3.E3.m1.1.1.3.3.2.10">ğ‘›</ci></apply><apply id="Sx3.E3.m1.1.1.3.3.3.cmml" xref="Sx3.E3.m1.1.1.3.3.3"><times id="Sx3.E3.m1.1.1.3.3.3.1.cmml" xref="Sx3.E3.m1.1.1.3.3.3.1"></times><ci id="Sx3.E3.m1.1.1.3.3.3.2.cmml" xref="Sx3.E3.m1.1.1.3.3.3.2">ğ‘Ÿ</ci><ci id="Sx3.E3.m1.1.1.3.3.3.3.cmml" xref="Sx3.E3.m1.1.1.3.3.3.3">ğ‘’</ci><ci id="Sx3.E3.m1.1.1.3.3.3.4.cmml" xref="Sx3.E3.m1.1.1.3.3.3.4">ğ‘</ci><ci id="Sx3.E3.m1.1.1.3.3.3.5.cmml" xref="Sx3.E3.m1.1.1.3.3.3.5">ğ‘</ci><ci id="Sx3.E3.m1.1.1.3.3.3.6.cmml" xref="Sx3.E3.m1.1.1.3.3.3.6">ğ‘™</ci><ci id="Sx3.E3.m1.1.1.3.3.3.7.cmml" xref="Sx3.E3.m1.1.1.3.3.3.7">ğ‘™</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E3.m1.1c">F=\frac{2*precision*recall}{precision+recall}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="Sx3.SSx3.p11" class="ltx_para">
<p id="Sx3.SSx3.p11.1" class="ltx_p">When working with multi-class problems, it is necessary to choose an averaging mechanism for the F1 score, precision, and recall metrics. The â€˜weightedâ€™ average is recommended
for imbalanced classes because it assigns different weights to each class based on their prevalence in the dataset. This prevents the dominant class from strongly influencing
these metrics.</p>
</div>
<div id="Sx3.SSx3.p12" class="ltx_para">
<p id="Sx3.SSx3.p12.1" class="ltx_p">Lastly, the ROC curve graph was created to visually compare the performance of all four experiments. In this studyâ€™s context, the ROC curve can be used to visualize the
characteristics of the performance; at what threshold between the true positive rate and false positive rate the model becomes uncertain about its predictions.</p>
</div>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Discussion and result analysis</h2>

<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Image generation</h3>

<div id="Sx4.SSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.p1.1" class="ltx_p">As with many NLP models, the generated prompts contained a fair amount of hallucination; however, these nonsensical prompts surprisingly created some of the best images. For
example, a successful prompt for the â€˜stoneâ€™ label was, â€The construction of building made of stone is in the medium of the Artistic Gymnastics.â€ It was valuable to refrain
from editorial oversight of the initial prompts to allow for spontaneity in the generated images. Because of this, the resulting datasets had more diversity than if
one prompt created all the synthetic images.</p>
</div>
<div id="Sx4.SSx1.p2" class="ltx_para">
<p id="Sx4.SSx1.p2.1" class="ltx_p">Some labels had a higher number of generated images flagged as irrelevant than other labels, with an average of 24% of all images being flagged as irrelevant. In most cases,
the irrelevant images featured materials different from the target label. For example, the â€˜stuccoâ€™ label often created images showing metal, siding, or rustication, which
resulted in 45% of all â€˜stuccoâ€™ images being flagged as irrelevant. In some cases, the generative image model created images with no building shown at all.</p>
</div>
<figure id="Sx4.F4" class="ltx_figure"><img src="/html/2404.08557/assets/confusion_matrices.png" id="Sx4.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="1180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Confusion matrices for all experiments.</figcaption>
</figure>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Image classification</h3>

<div id="Sx4.SSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.p1.1" class="ltx_p">By reviewing the confusion matrices, ROC curve graph, and bar graph comparing F1 scores, precision, and recall, we are able to achieve a nuanced and holistic evaluation of the
image classification results in the context of the research questions.</p>
</div>
<div id="Sx4.SSx2.p2" class="ltx_para">
<p id="Sx4.SSx2.p2.1" class="ltx_p">The confusion matrix compares the number of true labels versus predicted labels. To aid in comparability and interpretability, the numbers of each row in the matrix were
normalized to a percentage scale of 0% to 100%. The set of confusion matrices (FigureÂ <a href="#Sx4.F4" title="Figure 4 â€£ Image generation â€£ Discussion and result analysis â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) show which labels perform the worst and best for each experiment and whether any degradation of performance
is acceptable. Comparing the augmented stucco experiment to the baseline, it can be seen that the augmented dataset offered a slight advantage for the â€˜nullâ€™ and â€˜otherâ€™
labels, while maintaining perfect performance for the â€˜stuccoâ€™ label. When using the mixed dataset, there was decrease in performance (100% of true â€˜stuccoâ€™ samples predicted
as â€˜stuccoâ€™ versus 91% of true â€˜stuccoâ€™ samples predicted as â€˜stuccoâ€™). Moving to the siding experiment set, the augmented siding experiment had a decrease in performance
(as opposed to the augmented stucco experiment). This is likely because the â€˜sidingâ€™ augmented class had a higher proportion of synthetic to manually annotated images, which
triggers the question that there might be a threshold where the ratio of synthetic to manually annotated images hinders performance. The mixed siding experiment showed further degradation with only 48% of predicted â€˜sidingâ€™
samples matching the true â€˜sidingâ€™ label. This may have been caused by the size of the mixed siding dataset (248 training samples and 62 testing samples for each class), which
was half the size of the mixed stucco dataset (596 training samples and 149 testing samples for each class). Lastly, it is no surprise that the synthetic experiment achieved
near perfect results since the model had no way to test its performance on â€˜in the wildâ€™ images. Comparing the synthetic experiment results to the mixed experiment offers
insight into the possible ways a model trained and tested on purely synthetic data might degrade when brought into a real data scenario.</p>
</div>
<div id="Sx4.SSx2.p3" class="ltx_para">
<p id="Sx4.SSx2.p3.1" class="ltx_p">The ROC curve graph is helpful in comparing each experimentâ€™s discrimination capability between different classes. A model with a curve that hugs closely toward the top left
corner suggests high performance in sensitivity and specificity, while the straight, black, dotted line represents the performance of a random classifier. A model with a ROC
curve too close to this random classifier line indicates that the model is essentially predicting a class at random.</p>
</div>
<figure id="Sx4.F5" class="ltx_figure"><img src="/html/2404.08557/assets/ROCcurve.png" id="Sx4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="483" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>ROC curve graphs comparing the stucco and siding sub-experiments.</figcaption>
</figure>
<div id="Sx4.SSx2.p4" class="ltx_para">
<p id="Sx4.SSx2.p4.1" class="ltx_p">From the ROC curves (FigureÂ <a href="#Sx4.F5" title="Figure 5 â€£ Image classification â€£ Discussion and result analysis â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), it can be seen that there are a few experiments that achieved both high sensitivity and specificity across all thresholds,
indicated with an Area Under the Curve (AUC) value of 1.00: both baseline experiments, the augmented stucco experiment, and both synthetic experiments. The two mixed
experiments achieved AUC values of 0.95 and 0.91 for stucco and siding, respectively, which is generally considered quite a good score. Since those ROC curves are positioned
well above the random classifier line, it can be interpreted that these mixed models achieved a meaningful separation between true positives and false positives.</p>
</div>
<div id="Sx4.SSx2.p5" class="ltx_para">
<p id="Sx4.SSx2.p5.1" class="ltx_p">Evaluating the weighted precision, recall, and F1 score of the four experiments provides a global view of the overall performance of each experiment. A higher precision
indicates a lower rate of false positives, a higher recall indicates a lower rate of false negatives, while a higher F1 score indicates a balance between precision and recall.</p>
</div>
<div id="Sx4.SSx2.p6" class="ltx_para">
<p id="Sx4.SSx2.p6.1" class="ltx_p">The bar graphs showing the weighted F1 score, precision, and recall (FigureÂ <a href="#Sx4.F6" title="Figure 6 â€£ Image classification â€£ Discussion and result analysis â€£ SCALABILITY IN BUILDING COMPONENT DATA ANNOTATION: ENHANCING FAÃ‡ADE MATERIAL CLASSIFICATION WITH SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) further supports the observations that models trained with some
augmentation can increase performance and models trained with synthetic images maintain reasonable results on a manually annotated test dataset. The highest performing
experiments were the synthetic experiment and the baseline siding experiment, achieving a value of 0.98 for all three metrics. Although the confusion matrices differed
slightly, the baseline stucco and augmented stucco experiments achieved the same score of 0.96 for all three metrics. The augmented siding experiment performed slightly
worse than the baseline siding experiment with a value of 0.91 for F1 score and recall (and a value of 0.93 for precision). This further supports the finding that too much
augmentation can degrade performance. Although the mixed experiments had a decrease in metrics, the mixed stucco experiment maintained a reasonable performance with the
values 0.86, 0.87, 0.86 for F1 score, precision, and recall. The mixed siding experiment showed further degradation with values 0.72, 0.75, 0.74 for F1 score, precision,
and recall. However, the training and testing sets were half the size of the mixed stucco experiment, which could have influenced performance.</p>
</div>
<figure id="Sx4.F6" class="ltx_figure"><img src="/html/2404.08557/assets/global_metrics.png" id="Sx4.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="909" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Weighted F1, precision, and recall for all experiments.</figcaption>
</figure>
<div id="Sx4.SSx2.p7" class="ltx_para">
<p id="Sx4.SSx2.p7.1" class="ltx_p">There were several key findings from this study; some that offer an improvement over the state-of-the-art, and some that provide a nuanced analysis into the trade-offs made
when training a model on synthetic images. In terms of overall improvement, this study achieved a higher F1 score, precision, and recall than the state-of-the-art by utilizing
a higher resolution model (Swin Transformer v2 at 384 pixel resolution) and reducing the problem space to three labels (null, other, and a label of interest). This study
achieved a 0.98 weighted F1 score, 0.98 recall, and 0.98 precision on the baseline siding experiment while the state-of-the-art achieved 0.93 macro F1 score, 0.91 recall,
and 0.96 precision <cite class="ltx_cite ltx_citemacro_citep">(Raghu, Bucher and
DeÂ Wolf, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. For marginal improvements, it was found that augmenting a dataset with 27% synthetic data (for the â€˜stuccoâ€™ class) offered a slight
improvement over the baseline. This had the opposite effect when using a dataset with 80% synthetic data (for the siding class). As for the possibility to train a model on
entirely synthetic data, the performance was generally worse than the baseline, but still a reasonable performance with the mixed stucco experiment capturing 91% of true
positives in the confusion matrix, achieving an AUC value of 0.95, and scoring 0.87 for precision and 0.86 for the F1 score and recall. The results from the mixed siding
experiment may have been adversely affected by the size of the dataset (62 samples for test and 248 samples for training); potentially signaling a need to have a larger
dataset when training on synthetic images. The near perfect results of the synthetic experiment show that it is possible to train a model on synthetic images for a new label
set, but demonstrates the value of testing a model on manually annotated images.</p>
</div>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusions</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">Recent advancements in text-to-image generation have made it feasible to train image classification models on synthetic images, addressing previous challenges of data scarcity and enabling scalability to a national or global level for faÃ§ade material identification. In this study, we explored the potential benefits and trade-offs that could be made when using varying amounts of synthetic images for training a classification model, as well as proposed an improvement in
overall performance with a higher resolution model.
The research contributes a novel approach in urban mining for material cadastres by utilizing synthetic images to augment and extend an image classification model for detecting faÃ§ade materials, offering a potential solution to reduce training dataset development time and address data scarcity challenges while improving current approachesâ€™ performance.
This, of course, comes with caveats: the text-to-image prompts should trigger diversity in the generated images, the model should be
at least 384 pixel resolution, the problem space should be reduced to three labels (null, other, and label of interest), the synthetic training data should be large enough,
and it is worthwhile to still manually annotate a test set to develop an intuition for the types of errors that will happen. The results from this study are limited to replicating GSV images for the studyâ€™s specific set of labels and would not necessarily guarantee reasonable performance for synthetic images of building interiors or niche facade materials (such as ceramic tile cladding). Access to a high-performance GPU (an nVidia GeForce RTX 3070 8GB GPU) enabled utilization of the pre-trained Swin Transformer v2 384 pixel resolution model, which may not be possible to train on a CPU alone. Additionally, there could be privacy concerns associated with remotely compiling building material inventories, particularly regarding historical (materials that should not be moved from their location) or high-security faÃ§ade materials (materials that may reveal structural vulnerabilities).</p>
</div>
<div id="Sx5.p2" class="ltx_para">
<p id="Sx5.p2.1" class="ltx_p">These limitations indicate future research that could progress the field of urban mining for material cadastres further. While this experiment achieved good results mimicking GSV images, it would not be safe yet to conclude that any type of building image could be replicated with synthetic images. Therefore, it would be valuable for future work to explore applying the workflow to interior office and residential synthetic images. For the resolution of synthetic images, there was a considerable difference in performance when moving from the 192 pixel model to the 384 pixel
model resolution, which begs the question whether generating higher resolution images and training with an even higher resolution model may increase performance further. When generating the synthetic images, a brute-force method was used to separate
irrelevant images from the training set because it was suitable for this use case; however, an automatic filtering method would greatly reduce the time needed to prune the
output of the generative model. Further studies using synthetic images could benefit material cadastre research by unlocking building interior datasets, by increasing performance of image classification models even more, and by reducing the time needed to filter synthetic images. Developing a robust image classification model that has proven reliable performance in detecting building materials in a wide variety of contexts, scales, and building layers would be an important milestone in the goal to develop country-wide material cadastres for reusable demolition and renovation waste.</p>
</div>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">This project was financially supported by the Swedish Foundation for Strategic Research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akanbi etÂ al. (2020)</span>
<span class="ltx_bibblock">
Akanbi, L.A., Oyedele, A.O.,
Oyedele, L.O., Salami, R.O.,
2020.

</span>
<span class="ltx_bibblock">Deep learning model for demolition waste prediction
in a circular economy.

</span>
<span class="ltx_bibblock">Journal of Cleaner Production
274, 122843.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1016/j.jclepro.2020.122843" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.jclepro.2020.122843</span></a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anastasiou etÂ al. (2014)</span>
<span class="ltx_bibblock">
Anastasiou, E., GeorgiadisÂ Filikas, K.,
Stefanidou, M., 2014.

</span>
<span class="ltx_bibblock">Utilization of fine recycled aggregates in concrete
with fly ash and steel slag.

</span>
<span class="ltx_bibblock">Construction and Building Materials
50, 154â€“161.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1016/j.conbuildmat.2013.09.037" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.conbuildmat.2013.09.037</span></a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arbabi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Arbabi, H., Lanau, M., Li,
X., Meyers, G., Dai, M.,
Mayfield, M., DensleyÂ Tingley, D.,
2022.

</span>
<span class="ltx_bibblock">A scalable data collection, characterization, and
accounting framework for urban material stocks.

</span>
<span class="ltx_bibblock">Journal of Industrial Ecology
26, 58â€“71.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1111/jiec.13198" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1111/jiec.13198</span></a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baum and Haussler (1988)</span>
<span class="ltx_bibblock">
Baum, E., Haussler, D.,
1988.

</span>
<span class="ltx_bibblock">What size net gives valid generalization?, in:
Advances in Neural Information Processing Systems,
Morgan-Kaufmann.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy etÂ al. (2020)</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L.,
Kolesnikov, A., Weissenborn, D.,
Zhai, X., Unterthiner, T.,
Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S.,
Uszkoreit, J., Houlsby, N.,
2020.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image
recognition at scale.

</span>
<span class="ltx_bibblock">ICLR URL: <a target="_blank" href="http://arxiv.org/abs/2010.11929" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2010.11929</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.48550/arXiv.2010.11929" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.2010.11929</span></a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ginga etÂ al. (2020)</span>
<span class="ltx_bibblock">
Ginga, C.P., Ongpeng, J.M.C.,
Daly, M.K.M., 2020.

</span>
<span class="ltx_bibblock">Circular economy on construction and demolition
waste: A literature review on material recovery and production.

</span>
<span class="ltx_bibblock">Materials 13,
2970.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.3390/ma13132970" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/ma13132970</span></a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2023)</span>
<span class="ltx_bibblock">
He, R., Sun, S., Yu, X.,
Xue, C., Zhang, W.,
Torr, P., Bai, S., Qi,
X., 2023.

</span>
<span class="ltx_bibblock">Is synthetic data from generative models ready for
image recognition?

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2210.07574 URL: <a target="_blank" href="http://arxiv.org/abs/2210.07574" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2210.07574</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.48550/arXiv.2210.07574" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.2210.07574</span></a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks etÂ al. (2022)</span>
<span class="ltx_bibblock">
Hendrycks, D., Basart, S.,
Mazeika, M., Zou, A.,
Kwon, J., Mostajabi, M.,
Steinhardt, J., Song, D.,
2022.

</span>
<span class="ltx_bibblock">Scaling out-of-distribution detection for real-world
settings.

</span>
<span class="ltx_bibblock">ICML abs/1911.11132.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/1911.11132" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1911.11132</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.48550/arXiv.1911.11132" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.1911.11132</span></a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iacovidou and Purnell (2016)</span>
<span class="ltx_bibblock">
Iacovidou, E., Purnell, P.,
2016.

</span>
<span class="ltx_bibblock">Mining the physical infrastructure: Opportunities,
barriers and interventions in promoting structural components reuse.

</span>
<span class="ltx_bibblock">Science of The Total Environment
557â€“558, 791â€“807.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1016/j.scitotenv.2016.03.098" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.scitotenv.2016.03.098</span></a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Icibaci (2019)</span>
<span class="ltx_bibblock">
Icibaci, L., 2019.

</span>
<span class="ltx_bibblock">Re-use of building products in the netherlands: The
development of a metabolism based assessment approach.

</span>
<span class="ltx_bibblock">A+BE â€” Architecture and the Built Environment ,
1â€“422doi:<a target="_blank" href="http://dx.doi.org/10.7480/abe.2019.2.3248" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.7480/abe.2019.2.3248</span></a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jiang, H.H., Brown, L.,
Cheng, J., Khan, M.,
Gupta, A., Workman, D.,
Hanna, A., Flowers, J.,
Gebru, T., 2023.

</span>
<span class="ltx_bibblock">Ai art and its impact on artists, in:
Proceedings of the 2023 AAAI/ACM Conference on AI,
Ethics, and Society, Association for Computing
Machinery, New York, NY, USA. p.
363â€“374.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://dl.acm.org/doi/10.1145/3600211.3604681" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dl.acm.org/doi/10.1145/3600211.3604681</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.1145/3600211.3604681" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3600211.3604681</span></a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kabirifar etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kabirifar, K., Mojtahedi, M.,
Wang, C., Tam, V.W.Y.,
2020.

</span>
<span class="ltx_bibblock">Construction and demolition waste management
contributing factors coupled with reduce, reuse, and recycle strategies for
effective waste management: A review.

</span>
<span class="ltx_bibblock">Journal of Cleaner Production
263, 121265.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1016/j.jclepro.2020.121265" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.jclepro.2020.121265</span></a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao,
Y., Hu, H., Wei, Y.,
Zhang, Z., Lin, S., Guo,
B., 2021.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer
using shifted windows.

</span>
<span class="ltx_bibblock">Proceedings of the IEEE/CVF international
conference on computer vision abs/2103.14030.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2103.14030" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2103.14030</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.48550/arXiv.2103.14030" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.2103.14030</span></a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maggiori etÂ al. (2017)</span>
<span class="ltx_bibblock">
Maggiori, E., Tarabalka, Y.,
Charpiat, G., Alliez, P.,
2017.

</span>
<span class="ltx_bibblock">Convolutional neural networks for large-scale
remote-sensing image classification.

</span>
<span class="ltx_bibblock">IEEE Transactions on Geoscience and Remote Sensing
55, 645â€“657.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1109/TGRS.2016.2612821" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TGRS.2016.2612821</span></a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Man and Chahl (2022)</span>
<span class="ltx_bibblock">
Man, K., Chahl, J., 2022.

</span>
<span class="ltx_bibblock">A review of synthetic image data and its use in
computer vision.

</span>
<span class="ltx_bibblock">Journal of Imaging 8,
310.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.3390/jimaging8110310" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/jimaging8110310</span></a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus etÂ al. (2022)</span>
<span class="ltx_bibblock">
Marcus, G., Davis, E.,
Aaronson, S., 2022.

</span>
<span class="ltx_bibblock">A very preliminary analysis of dall-e 2.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2204.13807 URL: <a target="_blank" href="http://arxiv.org/abs/2204.13807" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2204.13807</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.48550/arXiv.2204.13807" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.2204.13807</span></a>,
<a target="_blank" href="http://arxiv.org/abs/2204.13807" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:2204.13807</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perronnin etÂ al. (2010)</span>
<span class="ltx_bibblock">
Perronnin, F., SÃ¡nchez, J.,
Mensink, T., 2010.

</span>
<span class="ltx_bibblock">Improving the fisher kernel for large-scale image
classification, in: Daniilidis, K.,
Maragos, P., Paragios, N. (Eds.),
ECCV 2010, Springer. p.
143â€“156.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raghu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Raghu, D., Bucher, M.J.J.,
DeÂ Wolf, C., 2023.

</span>
<span class="ltx_bibblock">Towards a â€˜resource cadastreâ€™ for a circular
economy â€“ urban-scale building material detection using street view imagery
and computer vision.

</span>
<span class="ltx_bibblock">Resources, Conservation and Recycling
198, 107140.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="http://dx.doi.org/10.1016/j.resconrec.2023.107140" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.resconrec.2023.107140</span></a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ramesh, A., Dhariwal, P.,
Nichol, A., Chu, C.,
Chen, M., 2022.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with
clip latents.

</span>
<span class="ltx_bibblock">arXiv e-prints URL: <a target="_blank" href="http://arxiv.org/abs/2204.06125" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2204.06125</a>,
<a target="_blank" href="http://arxiv.org/abs/2204.06125v1" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:2204.06125v1</a>.
<a target="_blank" href="http://arxiv.org/abs/2204.06125" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2204.06125</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohl-Dickstein etÂ al. (2015)</span>
<span class="ltx_bibblock">
Sohl-Dickstein, J., Weiss, E.A.,
Maheswaranathan, N., Ganguli, S.,
2015.

</span>
<span class="ltx_bibblock">Deep unsupervised learning using nonequilibrium
thermodynamics.

</span>
<span class="ltx_bibblock">International conference on machine learning ,
2256â€“2265URL: <a target="_blank" href="http://arxiv.org/abs/1503.03585" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1503.03585</a>,
doi:<a target="_blank" href="http://dx.doi.org/10.48550/arXiv.1503.03585" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.48550/arXiv.1503.03585</span></a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N.,
Parmar, N., Uszkoreit, J.,
Jones, L., Gomez, A.N.,
Kaiser, L., Polosukhin, I.,
2017.

</span>
<span class="ltx_bibblock">Attention is all you need, in:
Advances in Neural Information Processing Systems 30
(NIPS 2017).

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.08556" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.08557" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.08557">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.08557" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.08558" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 23:44:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
