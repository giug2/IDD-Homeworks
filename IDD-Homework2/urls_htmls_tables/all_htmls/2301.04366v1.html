<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.04366] Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI.</title><meta property="og:description" content="We present a new pre-training method, Multimodal Inverse Cloze Task, for Knowledge-based Visual Question Answering about
nam-ed
Entities (KVQAE). KVQAE is a recently introduced task that consists in answering questionsâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.04366">

<!--Generated on Fri Mar  1 03:59:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Visual Question Answering Pre-training Multimodal Fusion.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>UniversitÃ© Paris-Saclay, CNRS, LISN, 91400, Orsay, France 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{paul.lerner,camille.guinaudeau}@lisn.upsaclay.fr</span></span></span>
</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>UniversitÃ© Paris-Saclay, CEA, List, F-91120, Palaiseau, France
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>olivier.ferret@cea.fr</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Multimodal Inverse Cloze Task for Knowledge-based Visual Question Answering<span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>This work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul Lerner
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Olivier Ferret
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Camille Guinaudeau
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">We present a new pre-training method, Multimodal Inverse Cloze Task, for Knowledge-based Visual Question Answering about
nam-ed
Entities (KVQAE). KVQAE is a recently introduced task that consists in answering questions about named entities grounded in a visual context using a Knowledge
Base. Therefore, the interaction between the modalities is paramount to retrieve information and must be captured with complex fusion models. As these models require a lot of training data, we design this pre-training task from existing work in textual Question Answering. It consists in considering a sentence as a pseudo-question and its context as a pseudo-relevant passage and is extended by considering images near texts in multimodal documents. Our method is applicable to different neural network architectures and leads to a 9% relative-MRR and 15% relative-F1 gain for retrieval and reading comprehension, respectively, over a no-pre-training baseline.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Visual Question Answering Pre-training Multimodal Fusion.
</div>
<figure id="S0.F1" class="ltx_figure">
<table id="S0.F1.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S0.F1.4.5.1" class="ltx_tr">
<th id="S0.F1.4.5.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="2"><span id="S0.F1.4.5.1.1.1" class="ltx_text ltx_font_bold">Visual Question (input)</span></th>
<th id="S0.F1.4.5.1.2" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" colspan="2"><span id="S0.F1.4.5.1.2.1" class="ltx_text ltx_font_bold">Relevant visual passage in the Knowledge Base</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S0.F1.2.2" class="ltx_tr">
<th id="S0.F1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_t">
<span id="S0.F1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.F1.1.1.1.1.1" class="ltx_p" style="width:52.0pt;"><span id="S0.F1.1.1.1.1.1.1" class="ltx_text" style="position:relative; bottom:-0.9pt;"><img src="/html/2301.04366/assets/Figures/Harold_Macmillan_in_1961.jpg" id="S0.F1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="78" height="102" alt="Refer to caption"></span></span>
</span>
</th>
<td id="S0.F1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S0.F1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.F1.2.2.3.1.1" class="ltx_p" style="width:95.4pt;"><span id="S0.F1.2.2.3.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">â€œWhich constituency did this man represent when he was Prime Minister?â€</span></span>
</span>
</td>
<th id="S0.F1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t">
<span id="S0.F1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.F1.2.2.2.1.1" class="ltx_p" style="width:52.0pt;"><span id="S0.F1.2.2.2.1.1.1" class="ltx_text" style="position:relative; bottom:-0.9pt;"><img src="/html/2301.04366/assets/Figures/Harold_Macmillan.jpg" id="S0.F1.2.2.2.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="78" height="102" alt="Refer to caption"></span></span>
</span>
</th>
<td id="S0.F1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S0.F1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.F1.2.2.4.1.1" class="ltx_p" style="width:216.8pt;"><span id="S0.F1.2.2.4.1.1.1" class="ltx_text" style="font-size:70%;">â€œMacmillan indeed lost Stockton in the landslide Labour victory of 1945, but returned to Parliament in the November 1945 by-election in <span id="S0.F1.2.2.4.1.1.1.1" class="ltx_text ltx_font_bold">Bromley</span>.â€</span></span>
</span>
</td>
</tr>
<tr id="S0.F1.4.4" class="ltx_tr">
<th id="S0.F1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_t">
<span id="S0.F1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S0.F1.3.3.1.1.1" class="ltx_p" style="width:52.0pt;"><span id="S0.F1.3.3.1.1.1.1" class="ltx_text" style="position:relative; bottom:-0.9pt;"><img src="/html/2301.04366/assets/Figures/Queen_Elizabeth_2.jpg" id="S0.F1.3.3.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="78" height="59" alt="Refer to caption"></span></span>
</span>
</th>
<td id="S0.F1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S0.F1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S0.F1.4.4.3.1.1" class="ltx_p" style="width:95.4pt;"><span id="S0.F1.4.4.3.1.1.1" class="ltx_text" style="font-size:70%;">â€œ<span id="S0.F1.4.4.3.1.1.1.1" class="ltx_text ltx_font_italic">In which year did this ocean liner make her maiden voyage?</span>â€</span></span>
</span>
</td>
<th id="S0.F1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_t">
<span id="S0.F1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S0.F1.4.4.2.1.1" class="ltx_p" style="width:52.0pt;"><span id="S0.F1.4.4.2.1.1.1" class="ltx_text" style="position:relative; bottom:-0.9pt;"><img src="/html/2301.04366/assets/Figures/RMS_Queen_Elizabeth_2_in_Trondheim_2008.jpg" id="S0.F1.4.4.2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="78" height="59" alt="Refer to caption"></span></span>
</span>
</th>
<td id="S0.F1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S0.F1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S0.F1.4.4.4.1.1" class="ltx_p" style="width:216.8pt;"><span id="S0.F1.4.4.4.1.1.1" class="ltx_text" style="font-size:70%;">â€œQueen Elizabeth 2, often referred to simply as QE2, is a floating hotel and retired ocean liner built for the Cunard Line which was operated by Cunard as both a transatlantic liner and a cruise ship from <span id="S0.F1.4.4.4.1.1.1.1" class="ltx_text ltx_font_bold">1969</span> to 2008.â€</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.6.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.7.2" class="ltx_text" style="font-size:90%;">Example of visual questions about named entities from the ViQuAE dataset along with relevant visual passages from its Knowledge BaseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Knowledge-based Visual Question Answering about named Entities (KVQAE) is a challenging task recently introduced inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. It consists in answering questions about named entities grounded in a visual context using a Knowledge Base (KB). FigureÂ <a href="#S0.F1" title="Figure 1 â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides two examples of visual questions along with relevant visual passages from a KB. To address the task, one must thus <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">retrieve</span> relevant information from a KB. This contrasts with standard Visual Question Answering (VQA, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>), where questions target the content of the image (e.g. the color of an object or the number of objects) or Knowledge-based VQA (about coarse-grained object categories)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, where one can rely on off-the-shelf object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
In KVQAE, both text and image modalities bring useful information that must be combined. Therefore, the task is more broadly related to <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">Multimodal Information Retrieval</span> (IR) and <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">Multimodal Fusion</span>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">There are two paradigms for multimodal IR and for multimodal learning more generally: early fusion (data- and feature-level) and late fusion (score- and decision-level)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. On the one hand, late fusion is more straightforward as both Natural Language Processing and Computer Vision techniques can be applied independently, but it neglects interaction between the modalities. On the other hand, the richness of early fusion often comes at the cost of increasing complexity and model parameters. This adds an extra challenge for KVQAE where the two existing datasets are either small (ViQuAEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>) or generated automatically (KVQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>). To overcome this challenge, we propose to pre-train our fusion model using a multimodal Inverse Cloze Task (ICT). ICT has been introduced inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to pre-train a neural retriever for textual Question Answering (QA). It consists in considering a sentence as a pseudo-question and its context as a pseudo-relevant passage. It can be seen as a generalization
of the skip-gram objectiveÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. We extend it by considering images near texts in multimodal documents.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our main contributions are: (i) Multimodal ICT, a new pre-training method that allows tackling small KVQAE datasets such as ViQuAE; (ii) a multimodal IR framework for KVQAE; (iii) experiments with different neural network architectures, including recently proposed multimodal BERTs.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.1 </span>Dense Retrieval.</h4>

<div id="S2.SS0.SSS1.p1" class="ltx_para">
<p id="S2.SS0.SSS1.p1.1" class="ltx_p">Dense Retrieval is a rapidly evolving field, surveyed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, with new pre-training tasks, optimizing methods, and variants of the Transformer architecture emerging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> were the first to outperform sparse bag-of-words representations such as BM25 with dense representations for QA. Their approach relies on three components: 
<span id="S2.I1" class="ltx_inline-enumerate">
<span id="S2.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span id="S2.I1.i1.1" class="ltx_text">pre-trained language models such as BERTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which allow to encode the semantic of a sentence in a dense vector;
</span></span>
<span id="S2.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span id="S2.I1.i2.1" class="ltx_text">a contrastive learning objective that optimizes the similarities between questionsâ€™ and text passagesâ€™ embeddings (see SectionÂ <a href="#S3" title="3 Methods â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>); </span></span>
<span id="S2.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(iii)</span> <span id="S2.I1.i3.1" class="ltx_text">an unsupervised training task, ICT (see SectionÂ <a href="#S1" title="1 Introduction â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
</span></span>
</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> criticize the latter for being computationally intensive<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> use a batch size of over 4K questions.</span></span></span> and argue that regular sentences are not good surrogates of questions. Instead, they propose DPR, which takes advantage of 
<span id="S2.I2" class="ltx_inline-enumerate">
<span id="S2.I2.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span id="S2.I2.i1.1" class="ltx_text">the heuristic of whether the passage contains the answer to the question to deem it relevant;
</span></span>
<span id="S2.I2.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span id="S2.I2.i2.1" class="ltx_text">unsupervised IR methods such as BM25 to mine hard negatives examples, which proved to be the key of their methodâ€™s success.
</span></span>
</span>
We aim at taking advantage of both approaches by 
<span id="S2.I3" class="ltx_inline-enumerate">
<span id="S2.I3.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span id="S2.I3.i1.1" class="ltx_text">pre-training our model on text QA datasets like DPR;
</span></span>
<span id="S2.I3.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span id="S2.I3.i2.1" class="ltx_text">incorporating multimodality into this hopefully-well-initialized model by adapting the ICT ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to multimodal documents.
</span></span>
</span></p>
</div>
</section>
<section id="S2.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.2 </span>Multimodal Fusion and Pre-Training.</h4>

<div id="S2.SS0.SSS2.p1" class="ltx_para">
<p id="S2.SS0.SSS2.p1.1" class="ltx_p">The success of BERT in NLPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which relies on the easily-parallelizable Transformer architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, an unsupervised training objective, and a task-agnostic architecture, has concurrently inspired many works in the VQA and cross-modal retrieval fieldsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. These models are unified under a single framework inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and partly reviewed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
All of these models rely on the Transformer architecture, often initialized with a pre-trained BERT, in order to fuse image and text. The training is weakly supervised, based upon image caption datasets such as COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> or Conceptual CaptionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, and pre-trained object detectors like Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> show that these models learn nontrivial interactions between the modalities for VQA.
Multimodal BERTs can be broadly categorized into <span id="S2.SS0.SSS2.p1.1.1" class="ltx_text ltx_font_italic">single-stream</span> and <span id="S2.SS0.SSS2.p1.1.2" class="ltx_text ltx_font_italic">multi-stream</span>. Single-stream models feed both text tokensâ€™ embeddings and image regionsâ€™ embeddings to the same Transformer model, relying on the <span id="S2.SS0.SSS2.p1.1.3" class="ltx_text ltx_font_italic">self-attention</span> mechanism to fuse them. Instead, in the multi-stream architecture, text and image are first processed by two independent Transformers before using <span id="S2.SS0.SSS2.p1.1.4" class="ltx_text ltx_font_italic">cross-attention</span> to fuse the modalities. Both architectures have been shown to perform equally well inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
In this work, we use a single-stream model to take advantage of pre-training on text-only (on QA datasets). Also note that, while inspired by these work, we do not use the same training objectives or data, which are arguably unsuited for named entitiesâ€™ representations, as explained in the next section.</p>
</div>
</section>
<section id="S2.SS0.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.3 </span>Multimodal Information Retrieval and KVQAE.</h4>

<div id="S2.SS0.SSS3.p1" class="ltx_para">
<p id="S2.SS0.SSS3.p1.1" class="ltx_p">Multimodal IR has largely been addressed using late fusion techniques (seeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> for a survey) but we are mostly interested in early fusion techniques in this work.</p>
</div>
<div id="S2.SS0.SSS3.p2" class="ltx_para">
<p id="S2.SS0.SSS3.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> review first attempts at early fusion. It was then systematically done by concatenating the features of both modalities in a single vector, with a focus on the feature weighting scheme. Concatenation is confronted with the curse of dimensionality as the resulting feature space equals the sum of the
dimensions of each modalityâ€™s features.</p>
</div>
<div id="S2.SS0.SSS3.p3" class="ltx_para">
<p id="S2.SS0.SSS3.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> andÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> concurrently proposed an approach quite similar to ours for Knowledge-based VQA. They adapt DPRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> by replacing the question encoder with LXMERTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, which allows to fuse the question and image. However, unlike us, they keep the passage encoder based on text-only and use the same pre-training objectives asÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, namely Masked Language Modeling, Masked Region Modeling, and Image-Text Matching. We expect that these objectives are suited to learn representations of coarse-grained object categories but not named entities. In other words, they are suited for standard VQA but not KVQAE. For example, Masked Region Modeling relies on an object detector, which is not applicable to KVQAE. While bothÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> andÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> experiment on OK-VQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, their results are inconsistent: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> show that their model is competitive with a BM25 baseline that takes as input the question and the <span id="S2.SS0.SSS3.p3.1.1" class="ltx_text ltx_font_italic">human-written</span> caption of the image while the model ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> is outperformed by BM25 with an <span id="S2.SS0.SSS3.p3.1.2" class="ltx_text ltx_font_italic">automatically-generated</span> caption. The discrepancies between these works can be explained because they use neither the same KB nor the same evaluation metrics. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> also experiment with different multimodal BERTs but dispense passage-level annotation for an end-to-end training of the retriever and answer classifier<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Standard (Knowledge-based) VQA is often treated as a classification task.</span></span></span>.</p>
</div>
<div id="S2.SS0.SSS3.p4" class="ltx_para">
<p id="S2.SS0.SSS3.p4.1" class="ltx_p">Although they experiment with KVQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, we do not consider the work ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> as their systems take a <span id="S2.SS0.SSS3.p4.1.1" class="ltx_text ltx_font_italic">human-written</span> caption as input, which makes the role of the image content unclear.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> follow a late fusion approach at the decision-level. First, they detect and disambiguate the named entity mentions in the question. Then, they rely on a face recognition step as their dataset, KVQA, is restricted to questions about person named entities. Facts from both textually- and visually-detected entities are retrieved from Wikidata<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.wikidata.org/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.wikidata.org/</a></span></span></span> and processed by a memory networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. In contrast, our work is in line withÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, who use unstructured text from Wikipedia as KB. UnlikeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, who follow a late fusion approach, searching the question and the image independently, we aim at a unified representation of the text and image, both on the visual question and KB sides.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first formalize our KVQAE framework, then describe the models before diving into the three training stages: (i) DPR for textual Question Answering; (ii) Multimodal Inverse Cloze Task, our main contribution; (iii) Fine-tuning for KVQAE. Finally, we discuss the inference mechanism and implementation details.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Information Retrieval Framework</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.11" class="ltx_p">In our multimodal setting, both visual questions (from the dataset) and visual passages (from the KB) consist of a text-image pair <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="(t,i)" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.2.3.2.1" xref="S3.SS1.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">t</mi><mo id="S3.SS1.p1.1.m1.2.3.2.2" xref="S3.SS1.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.2.3.2.3" xref="S3.SS1.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><interval closure="open" id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.2"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘¡</ci><ci id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">ğ‘–</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">(t,i)</annotation></semantics></math>, as in FigureÂ <a href="#S0.F1" title="Figure 1 â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our goal is to find the optimal model <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">E</annotation></semantics></math> to encode adequate representations <math id="S3.SS1.p1.3.m3.2" class="ltx_Math" alttext="\mathbf{q}=E(t_{q},i_{q})" display="inline"><semantics id="S3.SS1.p1.3.m3.2a"><mrow id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml"><mi id="S3.SS1.p1.3.m3.2.2.4" xref="S3.SS1.p1.3.m3.2.2.4.cmml">ğª</mi><mo id="S3.SS1.p1.3.m3.2.2.3" xref="S3.SS1.p1.3.m3.2.2.3.cmml">=</mo><mrow id="S3.SS1.p1.3.m3.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.cmml"><mi id="S3.SS1.p1.3.m3.2.2.2.4" xref="S3.SS1.p1.3.m3.2.2.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.2.2.2.3" xref="S3.SS1.p1.3.m3.2.2.2.3.cmml">â€‹</mo><mrow id="S3.SS1.p1.3.m3.2.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.2.2.2.2.2.3" xref="S3.SS1.p1.3.m3.2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml">t</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.SS1.p1.3.m3.2.2.2.2.2.4" xref="S3.SS1.p1.3.m3.2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.3.m3.2.2.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.3.m3.2.2.2.2.2.2.2" xref="S3.SS1.p1.3.m3.2.2.2.2.2.2.2.cmml">i</mi><mi id="S3.SS1.p1.3.m3.2.2.2.2.2.2.3" xref="S3.SS1.p1.3.m3.2.2.2.2.2.2.3.cmml">q</mi></msub><mo stretchy="false" id="S3.SS1.p1.3.m3.2.2.2.2.2.5" xref="S3.SS1.p1.3.m3.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.2b"><apply id="S3.SS1.p1.3.m3.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2"><eq id="S3.SS1.p1.3.m3.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.3"></eq><ci id="S3.SS1.p1.3.m3.2.2.4.cmml" xref="S3.SS1.p1.3.m3.2.2.4">ğª</ci><apply id="S3.SS1.p1.3.m3.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2"><times id="S3.SS1.p1.3.m3.2.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2.3"></times><ci id="S3.SS1.p1.3.m3.2.2.2.4.cmml" xref="S3.SS1.p1.3.m3.2.2.2.4">ğ¸</ci><interval closure="open" id="S3.SS1.p1.3.m3.2.2.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.2"><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.2">ğ‘¡</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS1.p1.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS1.p1.3.m3.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.3.m3.2.2.2.2.2.2.3">ğ‘</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.2c">\mathbf{q}=E(t_{q},i_{q})</annotation></semantics></math> and <math id="S3.SS1.p1.4.m4.2" class="ltx_Math" alttext="\mathbf{p}=E(t_{p},i_{p})" display="inline"><semantics id="S3.SS1.p1.4.m4.2a"><mrow id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml"><mi id="S3.SS1.p1.4.m4.2.2.4" xref="S3.SS1.p1.4.m4.2.2.4.cmml">ğ©</mi><mo id="S3.SS1.p1.4.m4.2.2.3" xref="S3.SS1.p1.4.m4.2.2.3.cmml">=</mo><mrow id="S3.SS1.p1.4.m4.2.2.2" xref="S3.SS1.p1.4.m4.2.2.2.cmml"><mi id="S3.SS1.p1.4.m4.2.2.2.4" xref="S3.SS1.p1.4.m4.2.2.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.4.m4.2.2.2.3" xref="S3.SS1.p1.4.m4.2.2.2.3.cmml">â€‹</mo><mrow id="S3.SS1.p1.4.m4.2.2.2.2.2" xref="S3.SS1.p1.4.m4.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.2.2.2.2.2.3" xref="S3.SS1.p1.4.m4.2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p1.4.m4.1.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.cmml">t</mi><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.SS1.p1.4.m4.2.2.2.2.2.4" xref="S3.SS1.p1.4.m4.2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.4.m4.2.2.2.2.2.2" xref="S3.SS1.p1.4.m4.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.4.m4.2.2.2.2.2.2.2" xref="S3.SS1.p1.4.m4.2.2.2.2.2.2.2.cmml">i</mi><mi id="S3.SS1.p1.4.m4.2.2.2.2.2.2.3" xref="S3.SS1.p1.4.m4.2.2.2.2.2.2.3.cmml">p</mi></msub><mo stretchy="false" id="S3.SS1.p1.4.m4.2.2.2.2.2.5" xref="S3.SS1.p1.4.m4.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><apply id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2"><eq id="S3.SS1.p1.4.m4.2.2.3.cmml" xref="S3.SS1.p1.4.m4.2.2.3"></eq><ci id="S3.SS1.p1.4.m4.2.2.4.cmml" xref="S3.SS1.p1.4.m4.2.2.4">ğ©</ci><apply id="S3.SS1.p1.4.m4.2.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2.2"><times id="S3.SS1.p1.4.m4.2.2.2.3.cmml" xref="S3.SS1.p1.4.m4.2.2.2.3"></times><ci id="S3.SS1.p1.4.m4.2.2.2.4.cmml" xref="S3.SS1.p1.4.m4.2.2.2.4">ğ¸</ci><interval closure="open" id="S3.SS1.p1.4.m4.2.2.2.2.3.cmml" xref="S3.SS1.p1.4.m4.2.2.2.2.2"><apply id="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2">ğ‘¡</ci><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS1.p1.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS1.p1.4.m4.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.4.m4.2.2.2.2.2.2.3">ğ‘</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">\mathbf{p}=E(t_{p},i_{p})</annotation></semantics></math> such that they are close if <math id="S3.SS1.p1.5.m5.2" class="ltx_Math" alttext="(t_{p},i_{p})" display="inline"><semantics id="S3.SS1.p1.5.m5.2a"><mrow id="S3.SS1.p1.5.m5.2.2.2" xref="S3.SS1.p1.5.m5.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.2.2.2.3" xref="S3.SS1.p1.5.m5.2.2.3.cmml">(</mo><msub id="S3.SS1.p1.5.m5.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.1.2.cmml">t</mi><mi id="S3.SS1.p1.5.m5.1.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.SS1.p1.5.m5.2.2.2.4" xref="S3.SS1.p1.5.m5.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.5.m5.2.2.2.2" xref="S3.SS1.p1.5.m5.2.2.2.2.cmml"><mi id="S3.SS1.p1.5.m5.2.2.2.2.2" xref="S3.SS1.p1.5.m5.2.2.2.2.2.cmml">i</mi><mi id="S3.SS1.p1.5.m5.2.2.2.2.3" xref="S3.SS1.p1.5.m5.2.2.2.2.3.cmml">p</mi></msub><mo stretchy="false" id="S3.SS1.p1.5.m5.2.2.2.5" xref="S3.SS1.p1.5.m5.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.2b"><interval closure="open" id="S3.SS1.p1.5.m5.2.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.2"><apply id="S3.SS1.p1.5.m5.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.2">ğ‘¡</ci><ci id="S3.SS1.p1.5.m5.1.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS1.p1.5.m5.2.2.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.2.2.2.2.1.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.5.m5.2.2.2.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS1.p1.5.m5.2.2.2.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.2c">(t_{p},i_{p})</annotation></semantics></math> is relevant for <math id="S3.SS1.p1.6.m6.2" class="ltx_Math" alttext="(t_{q},i_{q})" display="inline"><semantics id="S3.SS1.p1.6.m6.2a"><mrow id="S3.SS1.p1.6.m6.2.2.2" xref="S3.SS1.p1.6.m6.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m6.2.2.2.3" xref="S3.SS1.p1.6.m6.2.2.3.cmml">(</mo><msub id="S3.SS1.p1.6.m6.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1.2" xref="S3.SS1.p1.6.m6.1.1.1.1.2.cmml">t</mi><mi id="S3.SS1.p1.6.m6.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.SS1.p1.6.m6.2.2.2.4" xref="S3.SS1.p1.6.m6.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.6.m6.2.2.2.2" xref="S3.SS1.p1.6.m6.2.2.2.2.cmml"><mi id="S3.SS1.p1.6.m6.2.2.2.2.2" xref="S3.SS1.p1.6.m6.2.2.2.2.2.cmml">i</mi><mi id="S3.SS1.p1.6.m6.2.2.2.2.3" xref="S3.SS1.p1.6.m6.2.2.2.2.3.cmml">q</mi></msub><mo stretchy="false" id="S3.SS1.p1.6.m6.2.2.2.5" xref="S3.SS1.p1.6.m6.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.2b"><interval closure="open" id="S3.SS1.p1.6.m6.2.2.3.cmml" xref="S3.SS1.p1.6.m6.2.2.2"><apply id="S3.SS1.p1.6.m6.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.2">ğ‘¡</ci><ci id="S3.SS1.p1.6.m6.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS1.p1.6.m6.2.2.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.2.2.2.2.1.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.6.m6.2.2.2.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS1.p1.6.m6.2.2.2.2.3.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.2c">(t_{q},i_{q})</annotation></semantics></math> (denoted with the superscripts <math id="S3.SS1.p1.7.m7.1" class="ltx_math_unparsed" alttext="(^{+})" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mrow id="S3.SS1.p1.7.m7.1b"><msup id="S3.SS1.p1.7.m7.1.1"><mo stretchy="false" id="S3.SS1.p1.7.m7.1.1.2">(</mo><mo id="S3.SS1.p1.7.m7.1.1.3">+</mo></msup><mo stretchy="false" id="S3.SS1.p1.7.m7.1.2">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">(^{+})</annotation></semantics></math> and <math id="S3.SS1.p1.8.m8.1" class="ltx_math_unparsed" alttext="(^{-})" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mrow id="S3.SS1.p1.8.m8.1b"><msup id="S3.SS1.p1.8.m8.1.1"><mo stretchy="false" id="S3.SS1.p1.8.m8.1.1.2">(</mo><mo id="S3.SS1.p1.8.m8.1.1.3">âˆ’</mo></msup><mo stretchy="false" id="S3.SS1.p1.8.m8.1.2">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">(^{-})</annotation></semantics></math>). Search
then boils down to retrieving the <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">K</annotation></semantics></math> closest visual passages to the visual question.
When computing the similarity between two vectors, here with the dot product, the objective used throughout all the training stages (Â§<a href="#S3.SS3" title="3.3 Training stages â€£ 3 Methods â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) is to minimize the following negative log-likelihood loss for all visual questions in the datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>: <math id="S3.SS1.p1.10.m10.6" class="ltx_Math" alttext="-\log\frac{\exp{(\mathbf{q}\cdot\mathbf{p}^{+})}}{\exp{(\mathbf{q}\cdot\mathbf{p}^{+})}+\sum_{j}\exp{(\mathbf{q}\cdot\mathbf{p}_{j}^{-})}}" display="inline"><semantics id="S3.SS1.p1.10.m10.6a"><mrow id="S3.SS1.p1.10.m10.6.7" xref="S3.SS1.p1.10.m10.6.7.cmml"><mo rspace="0.167em" id="S3.SS1.p1.10.m10.6.7a" xref="S3.SS1.p1.10.m10.6.7.cmml">âˆ’</mo><mrow id="S3.SS1.p1.10.m10.6.7.2" xref="S3.SS1.p1.10.m10.6.7.2.cmml"><mi id="S3.SS1.p1.10.m10.6.7.2.1" xref="S3.SS1.p1.10.m10.6.7.2.1.cmml">log</mi><mo lspace="0.167em" id="S3.SS1.p1.10.m10.6.7.2a" xref="S3.SS1.p1.10.m10.6.7.2.cmml">â¡</mo><mfrac id="S3.SS1.p1.10.m10.6.6" xref="S3.SS1.p1.10.m10.6.6.cmml"><mrow id="S3.SS1.p1.10.m10.2.2.2.2" xref="S3.SS1.p1.10.m10.2.2.2.3.cmml"><mi id="S3.SS1.p1.10.m10.1.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.1.cmml">exp</mi><mo id="S3.SS1.p1.10.m10.2.2.2.2a" xref="S3.SS1.p1.10.m10.2.2.2.3.cmml">â¡</mo><mrow id="S3.SS1.p1.10.m10.2.2.2.2.1" xref="S3.SS1.p1.10.m10.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.10.m10.2.2.2.2.1.2" xref="S3.SS1.p1.10.m10.2.2.2.3.cmml">(</mo><mrow id="S3.SS1.p1.10.m10.2.2.2.2.1.1" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.cmml"><mi id="S3.SS1.p1.10.m10.2.2.2.2.1.1.2" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.2.cmml">ğª</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.10.m10.2.2.2.2.1.1.1" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.1.cmml">â‹…</mo><msup id="S3.SS1.p1.10.m10.2.2.2.2.1.1.3" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.cmml"><mi id="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.2" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.2.cmml">ğ©</mi><mo id="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.3" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.3.cmml">+</mo></msup></mrow><mo stretchy="false" id="S3.SS1.p1.10.m10.2.2.2.2.1.3" xref="S3.SS1.p1.10.m10.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.SS1.p1.10.m10.6.6.6" xref="S3.SS1.p1.10.m10.6.6.6.cmml"><mrow id="S3.SS1.p1.10.m10.5.5.5.3.1" xref="S3.SS1.p1.10.m10.5.5.5.3.2.cmml"><mi id="S3.SS1.p1.10.m10.3.3.3.1" xref="S3.SS1.p1.10.m10.3.3.3.1.cmml">exp</mi><mo id="S3.SS1.p1.10.m10.5.5.5.3.1a" xref="S3.SS1.p1.10.m10.5.5.5.3.2.cmml">â¡</mo><mrow id="S3.SS1.p1.10.m10.5.5.5.3.1.1" xref="S3.SS1.p1.10.m10.5.5.5.3.2.cmml"><mo stretchy="false" id="S3.SS1.p1.10.m10.5.5.5.3.1.1.2" xref="S3.SS1.p1.10.m10.5.5.5.3.2.cmml">(</mo><mrow id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.cmml"><mi id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.2" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.2.cmml">ğª</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.1" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.1.cmml">â‹…</mo><msup id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.cmml"><mi id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.2" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.2.cmml">ğ©</mi><mo id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.3" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.3.cmml">+</mo></msup></mrow><mo stretchy="false" id="S3.SS1.p1.10.m10.5.5.5.3.1.1.3" xref="S3.SS1.p1.10.m10.5.5.5.3.2.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.10.m10.6.6.6.5" xref="S3.SS1.p1.10.m10.6.6.6.5.cmml">+</mo><mrow id="S3.SS1.p1.10.m10.6.6.6.4" xref="S3.SS1.p1.10.m10.6.6.6.4.cmml"><mstyle displaystyle="false" id="S3.SS1.p1.10.m10.6.6.6.4.2" xref="S3.SS1.p1.10.m10.6.6.6.4.2.cmml"><msub id="S3.SS1.p1.10.m10.6.6.6.4.2a" xref="S3.SS1.p1.10.m10.6.6.6.4.2.cmml"><mo id="S3.SS1.p1.10.m10.6.6.6.4.2.2" xref="S3.SS1.p1.10.m10.6.6.6.4.2.2.cmml">âˆ‘</mo><mi id="S3.SS1.p1.10.m10.6.6.6.4.2.3" xref="S3.SS1.p1.10.m10.6.6.6.4.2.3.cmml">j</mi></msub></mstyle><mrow id="S3.SS1.p1.10.m10.6.6.6.4.1.1" xref="S3.SS1.p1.10.m10.6.6.6.4.1.2.cmml"><mi id="S3.SS1.p1.10.m10.4.4.4.2" xref="S3.SS1.p1.10.m10.4.4.4.2.cmml">exp</mi><mo id="S3.SS1.p1.10.m10.6.6.6.4.1.1a" xref="S3.SS1.p1.10.m10.6.6.6.4.1.2.cmml">â¡</mo><mrow id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1" xref="S3.SS1.p1.10.m10.6.6.6.4.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.2" xref="S3.SS1.p1.10.m10.6.6.6.4.1.2.cmml">(</mo><mrow id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.cmml"><mi id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.2" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.2.cmml">ğª</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.1" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.1.cmml">â‹…</mo><msubsup id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.2" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.2.cmml">ğ©</mi><mi id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.3" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.3.cmml">j</mi><mo id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.3" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.3.cmml">âˆ’</mo></msubsup></mrow><mo stretchy="false" id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.3" xref="S3.SS1.p1.10.m10.6.6.6.4.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.6b"><apply id="S3.SS1.p1.10.m10.6.7.cmml" xref="S3.SS1.p1.10.m10.6.7"><minus id="S3.SS1.p1.10.m10.6.7.1.cmml" xref="S3.SS1.p1.10.m10.6.7"></minus><apply id="S3.SS1.p1.10.m10.6.7.2.cmml" xref="S3.SS1.p1.10.m10.6.7.2"><log id="S3.SS1.p1.10.m10.6.7.2.1.cmml" xref="S3.SS1.p1.10.m10.6.7.2.1"></log><apply id="S3.SS1.p1.10.m10.6.6.cmml" xref="S3.SS1.p1.10.m10.6.6"><divide id="S3.SS1.p1.10.m10.6.6.7.cmml" xref="S3.SS1.p1.10.m10.6.6"></divide><apply id="S3.SS1.p1.10.m10.2.2.2.3.cmml" xref="S3.SS1.p1.10.m10.2.2.2.2"><exp id="S3.SS1.p1.10.m10.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1"></exp><apply id="S3.SS1.p1.10.m10.2.2.2.2.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1"><ci id="S3.SS1.p1.10.m10.2.2.2.2.1.1.1.cmml" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.1">â‹…</ci><ci id="S3.SS1.p1.10.m10.2.2.2.2.1.1.2.cmml" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.2">ğª</ci><apply id="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.cmml" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.1.cmml" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.2.cmml" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.2">ğ©</ci><plus id="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.3.cmml" xref="S3.SS1.p1.10.m10.2.2.2.2.1.1.3.3"></plus></apply></apply></apply><apply id="S3.SS1.p1.10.m10.6.6.6.cmml" xref="S3.SS1.p1.10.m10.6.6.6"><plus id="S3.SS1.p1.10.m10.6.6.6.5.cmml" xref="S3.SS1.p1.10.m10.6.6.6.5"></plus><apply id="S3.SS1.p1.10.m10.5.5.5.3.2.cmml" xref="S3.SS1.p1.10.m10.5.5.5.3.1"><exp id="S3.SS1.p1.10.m10.3.3.3.1.cmml" xref="S3.SS1.p1.10.m10.3.3.3.1"></exp><apply id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.cmml" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1"><ci id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.1">â‹…</ci><ci id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.2.cmml" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.2">ğª</ci><apply id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.cmml" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.1.cmml" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.2.cmml" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.2">ğ©</ci><plus id="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.3.cmml" xref="S3.SS1.p1.10.m10.5.5.5.3.1.1.1.3.3"></plus></apply></apply></apply><apply id="S3.SS1.p1.10.m10.6.6.6.4.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4"><apply id="S3.SS1.p1.10.m10.6.6.6.4.2.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.2"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.6.6.6.4.2.1.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.2">subscript</csymbol><sum id="S3.SS1.p1.10.m10.6.6.6.4.2.2.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.2.2"></sum><ci id="S3.SS1.p1.10.m10.6.6.6.4.2.3.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.2.3">ğ‘—</ci></apply><apply id="S3.SS1.p1.10.m10.6.6.6.4.1.2.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1"><exp id="S3.SS1.p1.10.m10.4.4.4.2.cmml" xref="S3.SS1.p1.10.m10.4.4.4.2"></exp><apply id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1"><ci id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.1">â‹…</ci><ci id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.2.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.2">ğª</ci><apply id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3">superscript</csymbol><apply id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.1.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.2.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.2">ğ©</ci><ci id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.3.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.2.3">ğ‘—</ci></apply><minus id="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.10.m10.6.6.6.4.1.1.1.1.3.3"></minus></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.6c">-\log\frac{\exp{(\mathbf{q}\cdot\mathbf{p}^{+})}}{\exp{(\mathbf{q}\cdot\mathbf{p}^{+})}+\sum_{j}\exp{(\mathbf{q}\cdot\mathbf{p}_{j}^{-})}}</annotation></semantics></math>.
This contrastive objective allows to efficiently utilize passages relevant to other questions in the batch as <span id="S3.SS1.p1.11.1" class="ltx_text ltx_font_italic">in-batch negatives</span>, since computing the similarity between two vectors is rather inexpensive compared to the forward pass of the whole model.
We present two different models <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><mi id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><ci id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">E</annotation></semantics></math> in the next section according to their fusion mechanism.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">All of our models take advantage of BERT<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Uncased â€œbaseâ€ 12-layers version available at <a target="_blank" href="https://huggingface.co" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://huggingface.co</a>.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and CLIP<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>With a ResNet-50 backboneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</span></span></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> as building blocks to represent text and image, respectively. BERT is trained for masked language modeling and next sentence prediction on Wikipedia and BooksCorpusÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. CLIP has been trained with a contrastive objective in a weakly-supervised manner over 400M image and caption pairs. It has demonstrated better generalization capacities than fully-supervised models and is efficient for KVQAE, as empirically demonstrated inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
We experiment with two different fusion techniques: ECA and ILF.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.3" class="ltx_p">Early Cross-Attention fusion (ECA) is carried out by a single-stream Transformer model like the multimodal BERTs described above (e.g. UNITERÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>). However, instead of relying on a fixed object detector such as Faster R-CNN, we take advantage of CLIP, as motivated above. To enable early fusion, the visual embedding produced by CLIP is projected in the same space as the text using a linear layer with <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{W}_{c}\in\mathbb{R}^{c\times d}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><msub id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.2.2.cmml">ğ–</mi><mi id="S3.SS2.p2.1.m1.1.1.2.3" xref="S3.SS2.p2.1.m1.1.1.2.3.cmml">c</mi></msub><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.cmml">c</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.3.3.1" xref="S3.SS2.p2.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><in id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></in><apply id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2.2">ğ–</ci><ci id="S3.SS2.p2.1.m1.1.1.2.3.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">â„</ci><apply id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"><times id="S3.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2">ğ‘</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathbf{W}_{c}\in\mathbb{R}^{c\times d}</annotation></semantics></math> parameters trained from scratch: <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{e}_{c}=\mathrm{CLIP}(i)\cdot\mathbf{W}_{c}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.2" xref="S3.SS2.p2.2.m2.1.2.cmml"><msub id="S3.SS2.p2.2.m2.1.2.2" xref="S3.SS2.p2.2.m2.1.2.2.cmml"><mi id="S3.SS2.p2.2.m2.1.2.2.2" xref="S3.SS2.p2.2.m2.1.2.2.2.cmml">ğ</mi><mi id="S3.SS2.p2.2.m2.1.2.2.3" xref="S3.SS2.p2.2.m2.1.2.2.3.cmml">c</mi></msub><mo id="S3.SS2.p2.2.m2.1.2.1" xref="S3.SS2.p2.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS2.p2.2.m2.1.2.3" xref="S3.SS2.p2.2.m2.1.2.3.cmml"><mrow id="S3.SS2.p2.2.m2.1.2.3.2" xref="S3.SS2.p2.2.m2.1.2.3.2.cmml"><mi id="S3.SS2.p2.2.m2.1.2.3.2.2" xref="S3.SS2.p2.2.m2.1.2.3.2.2.cmml">CLIP</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.2.3.2.1" xref="S3.SS2.p2.2.m2.1.2.3.2.1.cmml">â€‹</mo><mrow id="S3.SS2.p2.2.m2.1.2.3.2.3.2" xref="S3.SS2.p2.2.m2.1.2.3.2.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.2.3.2.3.2.1" xref="S3.SS2.p2.2.m2.1.2.3.2.cmml">(</mo><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">i</mi><mo rspace="0.055em" stretchy="false" id="S3.SS2.p2.2.m2.1.2.3.2.3.2.2" xref="S3.SS2.p2.2.m2.1.2.3.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.SS2.p2.2.m2.1.2.3.1" xref="S3.SS2.p2.2.m2.1.2.3.1.cmml">â‹…</mo><msub id="S3.SS2.p2.2.m2.1.2.3.3" xref="S3.SS2.p2.2.m2.1.2.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.2.3.3.2" xref="S3.SS2.p2.2.m2.1.2.3.3.2.cmml">ğ–</mi><mi id="S3.SS2.p2.2.m2.1.2.3.3.3" xref="S3.SS2.p2.2.m2.1.2.3.3.3.cmml">c</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.2.cmml" xref="S3.SS2.p2.2.m2.1.2"><eq id="S3.SS2.p2.2.m2.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.2.1"></eq><apply id="S3.SS2.p2.2.m2.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.2.2.1.cmml" xref="S3.SS2.p2.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.2.2.2.cmml" xref="S3.SS2.p2.2.m2.1.2.2.2">ğ</ci><ci id="S3.SS2.p2.2.m2.1.2.2.3.cmml" xref="S3.SS2.p2.2.m2.1.2.2.3">ğ‘</ci></apply><apply id="S3.SS2.p2.2.m2.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.2.3"><ci id="S3.SS2.p2.2.m2.1.2.3.1.cmml" xref="S3.SS2.p2.2.m2.1.2.3.1">â‹…</ci><apply id="S3.SS2.p2.2.m2.1.2.3.2.cmml" xref="S3.SS2.p2.2.m2.1.2.3.2"><times id="S3.SS2.p2.2.m2.1.2.3.2.1.cmml" xref="S3.SS2.p2.2.m2.1.2.3.2.1"></times><ci id="S3.SS2.p2.2.m2.1.2.3.2.2.cmml" xref="S3.SS2.p2.2.m2.1.2.3.2.2">CLIP</ci><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ğ‘–</ci></apply><apply id="S3.SS2.p2.2.m2.1.2.3.3.cmml" xref="S3.SS2.p2.2.m2.1.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.2.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.2.3.3">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.2.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.2.3.3.2">ğ–</ci><ci id="S3.SS2.p2.2.m2.1.2.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.2.3.3.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathbf{e}_{c}=\mathrm{CLIP}(i)\cdot\mathbf{W}_{c}</annotation></semantics></math>. The resulting embedding is then concatenated with the word embeddings in the sequence dimension, acting as a â€œvisual tokenâ€. Those embeddings are then fed to the Transformer model, where the attention mechanism should enable interaction between the modalities. The final embedding corresponds to the special <span id="S3.SS2.p2.3.1" class="ltx_text ltx_font_typewriter">[CLS]</span> token: <math id="S3.SS2.p2.3.m3.4" class="ltx_Math" alttext="\mathrm{ECA{}}(t,i)=\mathrm{BERT}([t;\mathbf{e}_{c}])_{\texttt{[CLS]}{}}" display="inline"><semantics id="S3.SS2.p2.3.m3.4a"><mrow id="S3.SS2.p2.3.m3.4.4" xref="S3.SS2.p2.3.m3.4.4.cmml"><mrow id="S3.SS2.p2.3.m3.4.4.3" xref="S3.SS2.p2.3.m3.4.4.3.cmml"><mi id="S3.SS2.p2.3.m3.4.4.3.2" xref="S3.SS2.p2.3.m3.4.4.3.2.cmml">ECA</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.4.4.3.1" xref="S3.SS2.p2.3.m3.4.4.3.1.cmml">â€‹</mo><mrow id="S3.SS2.p2.3.m3.4.4.3.3.2" xref="S3.SS2.p2.3.m3.4.4.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.4.4.3.3.2.1" xref="S3.SS2.p2.3.m3.4.4.3.3.1.cmml">(</mo><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">t</mi><mo id="S3.SS2.p2.3.m3.4.4.3.3.2.2" xref="S3.SS2.p2.3.m3.4.4.3.3.1.cmml">,</mo><mi id="S3.SS2.p2.3.m3.2.2" xref="S3.SS2.p2.3.m3.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS2.p2.3.m3.4.4.3.3.2.3" xref="S3.SS2.p2.3.m3.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.3.m3.4.4.2" xref="S3.SS2.p2.3.m3.4.4.2.cmml">=</mo><mrow id="S3.SS2.p2.3.m3.4.4.1" xref="S3.SS2.p2.3.m3.4.4.1.cmml"><mi id="S3.SS2.p2.3.m3.4.4.1.3" xref="S3.SS2.p2.3.m3.4.4.1.3.cmml">BERT</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.4.4.1.2" xref="S3.SS2.p2.3.m3.4.4.1.2.cmml">â€‹</mo><msub id="S3.SS2.p2.3.m3.4.4.1.1" xref="S3.SS2.p2.3.m3.4.4.1.1.cmml"><mrow id="S3.SS2.p2.3.m3.4.4.1.1.1.1" xref="S3.SS2.p2.3.m3.4.4.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.4.4.1.1.1.1.2" xref="S3.SS2.p2.3.m3.4.4.1.1.cmml">(</mo><mrow id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.2" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.2.cmml">[</mo><mi id="S3.SS2.p2.3.m3.3.3" xref="S3.SS2.p2.3.m3.3.3.cmml">t</mi><mo id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.3" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.2.cmml">;</mo><msub id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.2" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.2.cmml">ğ</mi><mi id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.3.cmml">c</mi></msub><mo stretchy="false" id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.4" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.2.cmml">]</mo></mrow><mo stretchy="false" id="S3.SS2.p2.3.m3.4.4.1.1.1.1.3" xref="S3.SS2.p2.3.m3.4.4.1.1.cmml">)</mo></mrow><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p2.3.m3.4.4.1.1.3" xref="S3.SS2.p2.3.m3.4.4.1.1.3a.cmml">[CLS]</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.4b"><apply id="S3.SS2.p2.3.m3.4.4.cmml" xref="S3.SS2.p2.3.m3.4.4"><eq id="S3.SS2.p2.3.m3.4.4.2.cmml" xref="S3.SS2.p2.3.m3.4.4.2"></eq><apply id="S3.SS2.p2.3.m3.4.4.3.cmml" xref="S3.SS2.p2.3.m3.4.4.3"><times id="S3.SS2.p2.3.m3.4.4.3.1.cmml" xref="S3.SS2.p2.3.m3.4.4.3.1"></times><ci id="S3.SS2.p2.3.m3.4.4.3.2.cmml" xref="S3.SS2.p2.3.m3.4.4.3.2">ECA</ci><interval closure="open" id="S3.SS2.p2.3.m3.4.4.3.3.1.cmml" xref="S3.SS2.p2.3.m3.4.4.3.3.2"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ‘¡</ci><ci id="S3.SS2.p2.3.m3.2.2.cmml" xref="S3.SS2.p2.3.m3.2.2">ğ‘–</ci></interval></apply><apply id="S3.SS2.p2.3.m3.4.4.1.cmml" xref="S3.SS2.p2.3.m3.4.4.1"><times id="S3.SS2.p2.3.m3.4.4.1.2.cmml" xref="S3.SS2.p2.3.m3.4.4.1.2"></times><ci id="S3.SS2.p2.3.m3.4.4.1.3.cmml" xref="S3.SS2.p2.3.m3.4.4.1.3">BERT</ci><apply id="S3.SS2.p2.3.m3.4.4.1.1.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.4.4.1.1.2.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1">subscript</csymbol><list id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1"><ci id="S3.SS2.p2.3.m3.3.3.cmml" xref="S3.SS2.p2.3.m3.3.3">ğ‘¡</ci><apply id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.2">ğ</ci><ci id="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1.1.1.1.1.1.3">ğ‘</ci></apply></list><ci id="S3.SS2.p2.3.m3.4.4.1.1.3a.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S3.SS2.p2.3.m3.4.4.1.1.3.cmml" xref="S3.SS2.p2.3.m3.4.4.1.1.3">[CLS]</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.4c">\mathrm{ECA{}}(t,i)=\mathrm{BERT}([t;\mathbf{e}_{c}])_{\texttt{[CLS]}{}}</annotation></semantics></math>. The Transformer model is first initialized from BERT.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">Intermediate Linear Fusion (ILF) introduces an additional <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{W}_{t}\in\mathbb{R}^{d\times d}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><msub id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2.2" xref="S3.SS2.p3.1.m1.1.1.2.2.cmml">ğ–</mi><mi id="S3.SS2.p3.1.m1.1.1.2.3" xref="S3.SS2.p3.1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.3.2" xref="S3.SS2.p3.1.m1.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.1.m1.1.1.3.3.1" xref="S3.SS2.p3.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><in id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></in><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2.2">ğ–</ci><ci id="S3.SS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3">ğ‘¡</ci></apply><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">â„</ci><apply id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3"><times id="S3.SS2.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.2">ğ‘‘</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathbf{W}_{t}\in\mathbb{R}^{d\times d}</annotation></semantics></math> parameters trained from scratch used to simply project the representation of the <span id="S3.SS2.p3.2.1" class="ltx_text ltx_font_typewriter">[CLS]</span> token in the same space as the CLIP embedding before summing the two<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Note that this is equivalent to concatenating both before projecting like <math id="footnote6.m1.6" class="ltx_Math" alttext="[\mathrm{BERT}(t)_{\texttt{[CLS]}{}};\mathrm{CLIP}(i)]\cdot[\mathbf{W}_{t};\mathbf{W}_{c}]" display="inline"><semantics id="footnote6.m1.6b"><mrow id="footnote6.m1.6.6" xref="footnote6.m1.6.6.cmml"><mrow id="footnote6.m1.4.4.2.2" xref="footnote6.m1.4.4.2.3.cmml"><mo stretchy="false" id="footnote6.m1.4.4.2.2.3" xref="footnote6.m1.4.4.2.3.cmml">[</mo><mrow id="footnote6.m1.3.3.1.1.1" xref="footnote6.m1.3.3.1.1.1.cmml"><mi id="footnote6.m1.3.3.1.1.1.2" xref="footnote6.m1.3.3.1.1.1.2.cmml">BERT</mi><mo lspace="0em" rspace="0em" id="footnote6.m1.3.3.1.1.1.1" xref="footnote6.m1.3.3.1.1.1.1.cmml">â€‹</mo><msub id="footnote6.m1.3.3.1.1.1.3" xref="footnote6.m1.3.3.1.1.1.3.cmml"><mrow id="footnote6.m1.3.3.1.1.1.3.2.2" xref="footnote6.m1.3.3.1.1.1.3.cmml"><mo stretchy="false" id="footnote6.m1.3.3.1.1.1.3.2.2.1" xref="footnote6.m1.3.3.1.1.1.3.cmml">(</mo><mi id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml">t</mi><mo stretchy="false" id="footnote6.m1.3.3.1.1.1.3.2.2.2" xref="footnote6.m1.3.3.1.1.1.3.cmml">)</mo></mrow><mtext class="ltx_mathvariant_monospace" id="footnote6.m1.3.3.1.1.1.3.3" xref="footnote6.m1.3.3.1.1.1.3.3a.cmml">[CLS]</mtext></msub></mrow><mo id="footnote6.m1.4.4.2.2.4" xref="footnote6.m1.4.4.2.3.cmml">;</mo><mrow id="footnote6.m1.4.4.2.2.2" xref="footnote6.m1.4.4.2.2.2.cmml"><mi id="footnote6.m1.4.4.2.2.2.2" xref="footnote6.m1.4.4.2.2.2.2.cmml">CLIP</mi><mo lspace="0em" rspace="0em" id="footnote6.m1.4.4.2.2.2.1" xref="footnote6.m1.4.4.2.2.2.1.cmml">â€‹</mo><mrow id="footnote6.m1.4.4.2.2.2.3.2" xref="footnote6.m1.4.4.2.2.2.cmml"><mo stretchy="false" id="footnote6.m1.4.4.2.2.2.3.2.1" xref="footnote6.m1.4.4.2.2.2.cmml">(</mo><mi id="footnote6.m1.2.2" xref="footnote6.m1.2.2.cmml">i</mi><mo stretchy="false" id="footnote6.m1.4.4.2.2.2.3.2.2" xref="footnote6.m1.4.4.2.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.055em" stretchy="false" id="footnote6.m1.4.4.2.2.5" xref="footnote6.m1.4.4.2.3.cmml">]</mo></mrow><mo rspace="0.222em" id="footnote6.m1.6.6.5" xref="footnote6.m1.6.6.5.cmml">â‹…</mo><mrow id="footnote6.m1.6.6.4.2" xref="footnote6.m1.6.6.4.3.cmml"><mo stretchy="false" id="footnote6.m1.6.6.4.2.3" xref="footnote6.m1.6.6.4.3.cmml">[</mo><msub id="footnote6.m1.5.5.3.1.1" xref="footnote6.m1.5.5.3.1.1.cmml"><mi id="footnote6.m1.5.5.3.1.1.2" xref="footnote6.m1.5.5.3.1.1.2.cmml">ğ–</mi><mi id="footnote6.m1.5.5.3.1.1.3" xref="footnote6.m1.5.5.3.1.1.3.cmml">t</mi></msub><mo id="footnote6.m1.6.6.4.2.4" xref="footnote6.m1.6.6.4.3.cmml">;</mo><msub id="footnote6.m1.6.6.4.2.2" xref="footnote6.m1.6.6.4.2.2.cmml"><mi id="footnote6.m1.6.6.4.2.2.2" xref="footnote6.m1.6.6.4.2.2.2.cmml">ğ–</mi><mi id="footnote6.m1.6.6.4.2.2.3" xref="footnote6.m1.6.6.4.2.2.3.cmml">c</mi></msub><mo stretchy="false" id="footnote6.m1.6.6.4.2.5" xref="footnote6.m1.6.6.4.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote6.m1.6c"><apply id="footnote6.m1.6.6.cmml" xref="footnote6.m1.6.6"><ci id="footnote6.m1.6.6.5.cmml" xref="footnote6.m1.6.6.5">â‹…</ci><list id="footnote6.m1.4.4.2.3.cmml" xref="footnote6.m1.4.4.2.2"><apply id="footnote6.m1.3.3.1.1.1.cmml" xref="footnote6.m1.3.3.1.1.1"><times id="footnote6.m1.3.3.1.1.1.1.cmml" xref="footnote6.m1.3.3.1.1.1.1"></times><ci id="footnote6.m1.3.3.1.1.1.2.cmml" xref="footnote6.m1.3.3.1.1.1.2">BERT</ci><apply id="footnote6.m1.3.3.1.1.1.3.cmml" xref="footnote6.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="footnote6.m1.3.3.1.1.1.3.1.cmml" xref="footnote6.m1.3.3.1.1.1.3">subscript</csymbol><ci id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1">ğ‘¡</ci><ci id="footnote6.m1.3.3.1.1.1.3.3a.cmml" xref="footnote6.m1.3.3.1.1.1.3.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="footnote6.m1.3.3.1.1.1.3.3.cmml" xref="footnote6.m1.3.3.1.1.1.3.3">[CLS]</mtext></ci></apply></apply><apply id="footnote6.m1.4.4.2.2.2.cmml" xref="footnote6.m1.4.4.2.2.2"><times id="footnote6.m1.4.4.2.2.2.1.cmml" xref="footnote6.m1.4.4.2.2.2.1"></times><ci id="footnote6.m1.4.4.2.2.2.2.cmml" xref="footnote6.m1.4.4.2.2.2.2">CLIP</ci><ci id="footnote6.m1.2.2.cmml" xref="footnote6.m1.2.2">ğ‘–</ci></apply></list><list id="footnote6.m1.6.6.4.3.cmml" xref="footnote6.m1.6.6.4.2"><apply id="footnote6.m1.5.5.3.1.1.cmml" xref="footnote6.m1.5.5.3.1.1"><csymbol cd="ambiguous" id="footnote6.m1.5.5.3.1.1.1.cmml" xref="footnote6.m1.5.5.3.1.1">subscript</csymbol><ci id="footnote6.m1.5.5.3.1.1.2.cmml" xref="footnote6.m1.5.5.3.1.1.2">ğ–</ci><ci id="footnote6.m1.5.5.3.1.1.3.cmml" xref="footnote6.m1.5.5.3.1.1.3">ğ‘¡</ci></apply><apply id="footnote6.m1.6.6.4.2.2.cmml" xref="footnote6.m1.6.6.4.2.2"><csymbol cd="ambiguous" id="footnote6.m1.6.6.4.2.2.1.cmml" xref="footnote6.m1.6.6.4.2.2">subscript</csymbol><ci id="footnote6.m1.6.6.4.2.2.2.cmml" xref="footnote6.m1.6.6.4.2.2.2">ğ–</ci><ci id="footnote6.m1.6.6.4.2.2.3.cmml" xref="footnote6.m1.6.6.4.2.2.3">ğ‘</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m1.6d">[\mathrm{BERT}(t)_{\texttt{[CLS]}{}};\mathrm{CLIP}(i)]\cdot[\mathbf{W}_{t};\mathbf{W}_{c}]</annotation></semantics></math>.</span></span></span>: <math id="S3.SS2.p3.2.m2.3" class="ltx_Math" alttext="\mathrm{ILF}(t,i)=\mathrm{BERT}(t)_{\texttt{[CLS]}{}}\cdot\mathbf{W}_{t}+\mathbf{e}_{c}" display="inline"><semantics id="S3.SS2.p3.2.m2.3a"><mrow id="S3.SS2.p3.2.m2.3.4" xref="S3.SS2.p3.2.m2.3.4.cmml"><mrow id="S3.SS2.p3.2.m2.3.4.2" xref="S3.SS2.p3.2.m2.3.4.2.cmml"><mi id="S3.SS2.p3.2.m2.3.4.2.2" xref="S3.SS2.p3.2.m2.3.4.2.2.cmml">ILF</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.3.4.2.1" xref="S3.SS2.p3.2.m2.3.4.2.1.cmml">â€‹</mo><mrow id="S3.SS2.p3.2.m2.3.4.2.3.2" xref="S3.SS2.p3.2.m2.3.4.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.3.4.2.3.2.1" xref="S3.SS2.p3.2.m2.3.4.2.3.1.cmml">(</mo><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">t</mi><mo id="S3.SS2.p3.2.m2.3.4.2.3.2.2" xref="S3.SS2.p3.2.m2.3.4.2.3.1.cmml">,</mo><mi id="S3.SS2.p3.2.m2.2.2" xref="S3.SS2.p3.2.m2.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS2.p3.2.m2.3.4.2.3.2.3" xref="S3.SS2.p3.2.m2.3.4.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p3.2.m2.3.4.1" xref="S3.SS2.p3.2.m2.3.4.1.cmml">=</mo><mrow id="S3.SS2.p3.2.m2.3.4.3" xref="S3.SS2.p3.2.m2.3.4.3.cmml"><mrow id="S3.SS2.p3.2.m2.3.4.3.2" xref="S3.SS2.p3.2.m2.3.4.3.2.cmml"><mrow id="S3.SS2.p3.2.m2.3.4.3.2.2" xref="S3.SS2.p3.2.m2.3.4.3.2.2.cmml"><mi id="S3.SS2.p3.2.m2.3.4.3.2.2.2" xref="S3.SS2.p3.2.m2.3.4.3.2.2.2.cmml">BERT</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.3.4.3.2.2.1" xref="S3.SS2.p3.2.m2.3.4.3.2.2.1.cmml">â€‹</mo><msub id="S3.SS2.p3.2.m2.3.4.3.2.2.3" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3.cmml"><mrow id="S3.SS2.p3.2.m2.3.4.3.2.2.3.2.2" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.3.4.3.2.2.3.2.2.1" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3.cmml">(</mo><mi id="S3.SS2.p3.2.m2.3.3" xref="S3.SS2.p3.2.m2.3.3.cmml">t</mi><mo rspace="0.055em" stretchy="false" id="S3.SS2.p3.2.m2.3.4.3.2.2.3.2.2.2" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3.cmml">)</mo></mrow><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p3.2.m2.3.4.3.2.2.3.3" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3.3a.cmml">[CLS]</mtext></msub></mrow><mo rspace="0.222em" id="S3.SS2.p3.2.m2.3.4.3.2.1" xref="S3.SS2.p3.2.m2.3.4.3.2.1.cmml">â‹…</mo><msub id="S3.SS2.p3.2.m2.3.4.3.2.3" xref="S3.SS2.p3.2.m2.3.4.3.2.3.cmml"><mi id="S3.SS2.p3.2.m2.3.4.3.2.3.2" xref="S3.SS2.p3.2.m2.3.4.3.2.3.2.cmml">ğ–</mi><mi id="S3.SS2.p3.2.m2.3.4.3.2.3.3" xref="S3.SS2.p3.2.m2.3.4.3.2.3.3.cmml">t</mi></msub></mrow><mo id="S3.SS2.p3.2.m2.3.4.3.1" xref="S3.SS2.p3.2.m2.3.4.3.1.cmml">+</mo><msub id="S3.SS2.p3.2.m2.3.4.3.3" xref="S3.SS2.p3.2.m2.3.4.3.3.cmml"><mi id="S3.SS2.p3.2.m2.3.4.3.3.2" xref="S3.SS2.p3.2.m2.3.4.3.3.2.cmml">ğ</mi><mi id="S3.SS2.p3.2.m2.3.4.3.3.3" xref="S3.SS2.p3.2.m2.3.4.3.3.3.cmml">c</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.3b"><apply id="S3.SS2.p3.2.m2.3.4.cmml" xref="S3.SS2.p3.2.m2.3.4"><eq id="S3.SS2.p3.2.m2.3.4.1.cmml" xref="S3.SS2.p3.2.m2.3.4.1"></eq><apply id="S3.SS2.p3.2.m2.3.4.2.cmml" xref="S3.SS2.p3.2.m2.3.4.2"><times id="S3.SS2.p3.2.m2.3.4.2.1.cmml" xref="S3.SS2.p3.2.m2.3.4.2.1"></times><ci id="S3.SS2.p3.2.m2.3.4.2.2.cmml" xref="S3.SS2.p3.2.m2.3.4.2.2">ILF</ci><interval closure="open" id="S3.SS2.p3.2.m2.3.4.2.3.1.cmml" xref="S3.SS2.p3.2.m2.3.4.2.3.2"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">ğ‘¡</ci><ci id="S3.SS2.p3.2.m2.2.2.cmml" xref="S3.SS2.p3.2.m2.2.2">ğ‘–</ci></interval></apply><apply id="S3.SS2.p3.2.m2.3.4.3.cmml" xref="S3.SS2.p3.2.m2.3.4.3"><plus id="S3.SS2.p3.2.m2.3.4.3.1.cmml" xref="S3.SS2.p3.2.m2.3.4.3.1"></plus><apply id="S3.SS2.p3.2.m2.3.4.3.2.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2"><ci id="S3.SS2.p3.2.m2.3.4.3.2.1.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.1">â‹…</ci><apply id="S3.SS2.p3.2.m2.3.4.3.2.2.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.2"><times id="S3.SS2.p3.2.m2.3.4.3.2.2.1.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.2.1"></times><ci id="S3.SS2.p3.2.m2.3.4.3.2.2.2.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.2.2">BERT</ci><apply id="S3.SS2.p3.2.m2.3.4.3.2.2.3.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.3.4.3.2.2.3.1.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3">subscript</csymbol><ci id="S3.SS2.p3.2.m2.3.3.cmml" xref="S3.SS2.p3.2.m2.3.3">ğ‘¡</ci><ci id="S3.SS2.p3.2.m2.3.4.3.2.2.3.3a.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S3.SS2.p3.2.m2.3.4.3.2.2.3.3.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.2.3.3">[CLS]</mtext></ci></apply></apply><apply id="S3.SS2.p3.2.m2.3.4.3.2.3.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.3.4.3.2.3.1.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.3">subscript</csymbol><ci id="S3.SS2.p3.2.m2.3.4.3.2.3.2.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.3.2">ğ–</ci><ci id="S3.SS2.p3.2.m2.3.4.3.2.3.3.cmml" xref="S3.SS2.p3.2.m2.3.4.3.2.3.3">ğ‘¡</ci></apply></apply><apply id="S3.SS2.p3.2.m2.3.4.3.3.cmml" xref="S3.SS2.p3.2.m2.3.4.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.3.4.3.3.1.cmml" xref="S3.SS2.p3.2.m2.3.4.3.3">subscript</csymbol><ci id="S3.SS2.p3.2.m2.3.4.3.3.2.cmml" xref="S3.SS2.p3.2.m2.3.4.3.3.2">ğ</ci><ci id="S3.SS2.p3.2.m2.3.4.3.3.3.cmml" xref="S3.SS2.p3.2.m2.3.4.3.3.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.3c">\mathrm{ILF}(t,i)=\mathrm{BERT}(t)_{\texttt{[CLS]}{}}\cdot\mathbf{W}_{t}+\mathbf{e}_{c}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.3" class="ltx_p">Because both ECA and ILF produce multimodal representations <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">ğª</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğª</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\mathbf{q}</annotation></semantics></math> and <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{p}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">ğ©</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">ğ©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\mathbf{p}</annotation></semantics></math>, ranking is done directly using their similarity <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{q}\cdot\mathbf{p}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">ğª</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.3.m3.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.cmml">â‹…</mo><mi id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">ğ©</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><ci id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1">â‹…</ci><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">ğª</ci><ci id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">ğ©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\mathbf{q}\cdot\mathbf{p}</annotation></semantics></math>. As baseline, we followÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and linearly combine text and image similarities after zero-mean and unit-variance normalization (omitted in the following equation):</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="\alpha\times\mathrm{BERT}(t_{q})_{\texttt{[CLS]}{}}\cdot\mathrm{BERT}(t_{p})_{\texttt{[CLS]}{}}+(1-\alpha)\times\cos(\mathrm{CLIP}(i_{q}),\mathrm{CLIP}(i_{p}))" display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mrow id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2.cmml">Î±</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.2.2.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.3.1.cmml">Ã—</mo><mi id="S3.E1.m1.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.cmml">BERT</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">â€‹</mo><msub id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">t</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">q</mi></msub><mo rspace="0.055em" stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mtext class="ltx_mathvariant_monospace" id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3a.cmml">[CLS]</mtext></msub></mrow><mo rspace="0.222em" id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">â‹…</mo><mi id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">BERT</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.2.3" xref="S3.E1.m1.3.3.2.3.cmml">â€‹</mo><msub id="S3.E1.m1.3.3.2.2" xref="S3.E1.m1.3.3.2.2.cmml"><mrow id="S3.E1.m1.3.3.2.2.1.1" xref="S3.E1.m1.3.3.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.2.2.1.1.2" xref="S3.E1.m1.3.3.2.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.3.3.2.2.1.1.1" xref="S3.E1.m1.3.3.2.2.1.1.1.cmml"><mi id="S3.E1.m1.3.3.2.2.1.1.1.2" xref="S3.E1.m1.3.3.2.2.1.1.1.2.cmml">t</mi><mi id="S3.E1.m1.3.3.2.2.1.1.1.3" xref="S3.E1.m1.3.3.2.2.1.1.1.3.cmml">p</mi></msub><mo stretchy="false" id="S3.E1.m1.3.3.2.2.1.1.3" xref="S3.E1.m1.3.3.2.2.1.1.1.cmml">)</mo></mrow><mtext class="ltx_mathvariant_monospace" id="S3.E1.m1.3.3.2.2.3" xref="S3.E1.m1.3.3.2.2.3a.cmml">[CLS]</mtext></msub></mrow><mo id="S3.E1.m1.6.6.6" xref="S3.E1.m1.6.6.6.cmml">+</mo><mrow id="S3.E1.m1.6.6.5" xref="S3.E1.m1.6.6.5.cmml"><mrow id="S3.E1.m1.4.4.3.1.1" xref="S3.E1.m1.4.4.3.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.3.1.1.2" xref="S3.E1.m1.4.4.3.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.3.1.1.1" xref="S3.E1.m1.4.4.3.1.1.1.cmml"><mn id="S3.E1.m1.4.4.3.1.1.1.2" xref="S3.E1.m1.4.4.3.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.4.4.3.1.1.1.1" xref="S3.E1.m1.4.4.3.1.1.1.1.cmml">âˆ’</mo><mi id="S3.E1.m1.4.4.3.1.1.1.3" xref="S3.E1.m1.4.4.3.1.1.1.3.cmml">Î±</mi></mrow><mo rspace="0.055em" stretchy="false" id="S3.E1.m1.4.4.3.1.1.3" xref="S3.E1.m1.4.4.3.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E1.m1.6.6.5.4" xref="S3.E1.m1.6.6.5.4.cmml">Ã—</mo><mrow id="S3.E1.m1.6.6.5.3.2" xref="S3.E1.m1.6.6.5.3.3.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">cos</mi><mo id="S3.E1.m1.6.6.5.3.2a" xref="S3.E1.m1.6.6.5.3.3.cmml">â¡</mo><mrow id="S3.E1.m1.6.6.5.3.2.2" xref="S3.E1.m1.6.6.5.3.3.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.5.3.2.2.3" xref="S3.E1.m1.6.6.5.3.3.cmml">(</mo><mrow id="S3.E1.m1.5.5.4.2.1.1.1" xref="S3.E1.m1.5.5.4.2.1.1.1.cmml"><mi id="S3.E1.m1.5.5.4.2.1.1.1.3" xref="S3.E1.m1.5.5.4.2.1.1.1.3.cmml">CLIP</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.4.2.1.1.1.2" xref="S3.E1.m1.5.5.4.2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.5.5.4.2.1.1.1.1.1" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.4.2.1.1.1.1.1.2" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.5.5.4.2.1.1.1.1.1.1" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.2.cmml">i</mi><mi id="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.3.cmml">q</mi></msub><mo stretchy="false" id="S3.E1.m1.5.5.4.2.1.1.1.1.1.3" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.5.3.2.2.4" xref="S3.E1.m1.6.6.5.3.3.cmml">,</mo><mrow id="S3.E1.m1.6.6.5.3.2.2.2" xref="S3.E1.m1.6.6.5.3.2.2.2.cmml"><mi id="S3.E1.m1.6.6.5.3.2.2.2.3" xref="S3.E1.m1.6.6.5.3.2.2.2.3.cmml">CLIP</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.5.3.2.2.2.2" xref="S3.E1.m1.6.6.5.3.2.2.2.2.cmml">â€‹</mo><mrow id="S3.E1.m1.6.6.5.3.2.2.2.1.1" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.5.3.2.2.2.1.1.2" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.6.6.5.3.2.2.2.1.1.1" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.2" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.2.cmml">i</mi><mi id="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.3" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.3.cmml">p</mi></msub><mo stretchy="false" id="S3.E1.m1.6.6.5.3.2.2.2.1.1.3" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.6.6.5.3.2.2.5" xref="S3.E1.m1.6.6.5.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><plus id="S3.E1.m1.6.6.6.cmml" xref="S3.E1.m1.6.6.6"></plus><apply id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"><times id="S3.E1.m1.3.3.2.3.cmml" xref="S3.E1.m1.3.3.2.3"></times><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2">â‹…</ci><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><times id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1"></times><ci id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">ğ›¼</ci><ci id="S3.E1.m1.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3">BERT</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2">ğ‘¡</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">ğ‘</ci></apply><ci id="S3.E1.m1.2.2.1.1.1.1.3a.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">[CLS]</mtext></ci></apply></apply><ci id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3">BERT</ci></apply><apply id="S3.E1.m1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2">subscript</csymbol><apply id="S3.E1.m1.3.3.2.2.1.1.1.cmml" xref="S3.E1.m1.3.3.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.2.2.1.1.1.2.cmml" xref="S3.E1.m1.3.3.2.2.1.1.1.2">ğ‘¡</ci><ci id="S3.E1.m1.3.3.2.2.1.1.1.3.cmml" xref="S3.E1.m1.3.3.2.2.1.1.1.3">ğ‘</ci></apply><ci id="S3.E1.m1.3.3.2.2.3a.cmml" xref="S3.E1.m1.3.3.2.2.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S3.E1.m1.3.3.2.2.3.cmml" xref="S3.E1.m1.3.3.2.2.3">[CLS]</mtext></ci></apply></apply><apply id="S3.E1.m1.6.6.5.cmml" xref="S3.E1.m1.6.6.5"><times id="S3.E1.m1.6.6.5.4.cmml" xref="S3.E1.m1.6.6.5.4"></times><apply id="S3.E1.m1.4.4.3.1.1.1.cmml" xref="S3.E1.m1.4.4.3.1.1"><minus id="S3.E1.m1.4.4.3.1.1.1.1.cmml" xref="S3.E1.m1.4.4.3.1.1.1.1"></minus><cn type="integer" id="S3.E1.m1.4.4.3.1.1.1.2.cmml" xref="S3.E1.m1.4.4.3.1.1.1.2">1</cn><ci id="S3.E1.m1.4.4.3.1.1.1.3.cmml" xref="S3.E1.m1.4.4.3.1.1.1.3">ğ›¼</ci></apply><apply id="S3.E1.m1.6.6.5.3.3.cmml" xref="S3.E1.m1.6.6.5.3.2"><cos id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"></cos><apply id="S3.E1.m1.5.5.4.2.1.1.1.cmml" xref="S3.E1.m1.5.5.4.2.1.1.1"><times id="S3.E1.m1.5.5.4.2.1.1.1.2.cmml" xref="S3.E1.m1.5.5.4.2.1.1.1.2"></times><ci id="S3.E1.m1.5.5.4.2.1.1.1.3.cmml" xref="S3.E1.m1.5.5.4.2.1.1.1.3">CLIP</ci><apply id="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.2">ğ‘–</ci><ci id="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.4.2.1.1.1.1.1.1.3">ğ‘</ci></apply></apply><apply id="S3.E1.m1.6.6.5.3.2.2.2.cmml" xref="S3.E1.m1.6.6.5.3.2.2.2"><times id="S3.E1.m1.6.6.5.3.2.2.2.2.cmml" xref="S3.E1.m1.6.6.5.3.2.2.2.2"></times><ci id="S3.E1.m1.6.6.5.3.2.2.2.3.cmml" xref="S3.E1.m1.6.6.5.3.2.2.2.3">CLIP</ci><apply id="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.cmml" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.2">ğ‘–</ci><ci id="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.6.6.5.3.2.2.2.1.1.1.3">ğ‘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\alpha\times\mathrm{BERT}(t_{q})_{\texttt{[CLS]}{}}\cdot\mathrm{BERT}(t_{p})_{\texttt{[CLS]}{}}+(1-\alpha)\times\cos(\mathrm{CLIP}(i_{q}),\mathrm{CLIP}(i_{p}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.4" class="ltx_p">The interpolation hyperparameter <math id="S3.SS2.p4.4.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p4.4.m1.1a"><mi id="S3.SS2.p4.4.m1.1.1" xref="S3.SS2.p4.4.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m1.1b"><ci id="S3.SS2.p4.4.m1.1.1.cmml" xref="S3.SS2.p4.4.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m1.1c">\alpha</annotation></semantics></math> is optimized on the validation set using grid search to maximize Mean Reciprocal Rank. The left term (text similarity) is referred to as DPR in the rest of the paper.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training stages</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The models are trained sequentially in three stages:</p>
</div>
<section id="S3.SS3.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Stage 1: DPR for textual Question Answering.</h4>

<div id="S3.SS3.SSSx1.p1" class="ltx_para">
<p id="S3.SS3.SSSx1.p1.3" class="ltx_p">Leaving visual representations aside, a DPR model is trained starting from the BERT initializationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. DPR consists of two BERT encoders: one for the question <math id="S3.SS3.SSSx1.p1.1.m1.1" class="ltx_Math" alttext="t_{q}" display="inline"><semantics id="S3.SS3.SSSx1.p1.1.m1.1a"><msub id="S3.SS3.SSSx1.p1.1.m1.1.1" xref="S3.SS3.SSSx1.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSSx1.p1.1.m1.1.1.2" xref="S3.SS3.SSSx1.p1.1.m1.1.1.2.cmml">t</mi><mi id="S3.SS3.SSSx1.p1.1.m1.1.1.3" xref="S3.SS3.SSSx1.p1.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx1.p1.1.m1.1b"><apply id="S3.SS3.SSSx1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSSx1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSSx1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSSx1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSSx1.p1.1.m1.1.1.2">ğ‘¡</ci><ci id="S3.SS3.SSSx1.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSSx1.p1.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx1.p1.1.m1.1c">t_{q}</annotation></semantics></math> and one for the text passage <math id="S3.SS3.SSSx1.p1.2.m2.1" class="ltx_Math" alttext="t_{p}" display="inline"><semantics id="S3.SS3.SSSx1.p1.2.m2.1a"><msub id="S3.SS3.SSSx1.p1.2.m2.1.1" xref="S3.SS3.SSSx1.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSSx1.p1.2.m2.1.1.2" xref="S3.SS3.SSSx1.p1.2.m2.1.1.2.cmml">t</mi><mi id="S3.SS3.SSSx1.p1.2.m2.1.1.3" xref="S3.SS3.SSSx1.p1.2.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx1.p1.2.m2.1b"><apply id="S3.SS3.SSSx1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSSx1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx1.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSSx1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSSx1.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSSx1.p1.2.m2.1.1.2">ğ‘¡</ci><ci id="S3.SS3.SSSx1.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSSx1.p1.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx1.p1.2.m2.1c">t_{p}</annotation></semantics></math>. We use the model pre-trained byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> on TriviaQA, filtered of all questions used in their dataset, ViQuAE.
They use the KILTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> version of TriviaQA and Wikipedia, which serves as KB in this stage. Each article is then split into disjoint passages of 100 words for text retrieval, while preserving sentence boundaries, and the title of the article is appended to the beginning of each passage. This yields 32M passages, that is <math id="S3.SS3.SSSx1.p1.3.m3.1" class="ltx_Math" alttext="\approx 5.4" display="inline"><semantics id="S3.SS3.SSSx1.p1.3.m3.1a"><mrow id="S3.SS3.SSSx1.p1.3.m3.1.1" xref="S3.SS3.SSSx1.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSSx1.p1.3.m3.1.1.2" xref="S3.SS3.SSSx1.p1.3.m3.1.1.2.cmml"></mi><mo id="S3.SS3.SSSx1.p1.3.m3.1.1.1" xref="S3.SS3.SSSx1.p1.3.m3.1.1.1.cmml">â‰ˆ</mo><mn id="S3.SS3.SSSx1.p1.3.m3.1.1.3" xref="S3.SS3.SSSx1.p1.3.m3.1.1.3.cmml">5.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx1.p1.3.m3.1b"><apply id="S3.SS3.SSSx1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSSx1.p1.3.m3.1.1"><approx id="S3.SS3.SSSx1.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSSx1.p1.3.m3.1.1.1"></approx><csymbol cd="latexml" id="S3.SS3.SSSx1.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSSx1.p1.3.m3.1.1.2">absent</csymbol><cn type="float" id="S3.SS3.SSSx1.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSSx1.p1.3.m3.1.1.3">5.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx1.p1.3.m3.1c">\approx 5.4</annotation></semantics></math> passages per article. FollowingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>,
irrelevant passages (i.e. hard negatives) are mined using BM25Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2301.04366/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="276" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Overview of Multimodal Inverse Cloze Task via Wikipedia/WIT.</span></figcaption>
</figure>
</section>
<section id="S3.SS3.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Stage 2: Multimodal Inverse Cloze Task.</h4>

<div id="S3.SS3.SSSx2.p1" class="ltx_para">
<p id="S3.SS3.SSSx2.p1.3" class="ltx_p">This is the main contribution of the paper. We propose to extend the ICT ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to multimodal documents. ICT consists in considering a sentence as a pseudo-question <math id="S3.SS3.SSSx2.p1.1.m1.1" class="ltx_Math" alttext="t_{q}" display="inline"><semantics id="S3.SS3.SSSx2.p1.1.m1.1a"><msub id="S3.SS3.SSSx2.p1.1.m1.1.1" xref="S3.SS3.SSSx2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSSx2.p1.1.m1.1.1.2" xref="S3.SS3.SSSx2.p1.1.m1.1.1.2.cmml">t</mi><mi id="S3.SS3.SSSx2.p1.1.m1.1.1.3" xref="S3.SS3.SSSx2.p1.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx2.p1.1.m1.1b"><apply id="S3.SS3.SSSx2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSSx2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSSx2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSSx2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSSx2.p1.1.m1.1.1.2">ğ‘¡</ci><ci id="S3.SS3.SSSx2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSSx2.p1.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx2.p1.1.m1.1c">t_{q}</annotation></semantics></math> and its context as a pseudo-relevant passage <math id="S3.SS3.SSSx2.p1.2.m2.1" class="ltx_Math" alttext="t_{p}^{+}" display="inline"><semantics id="S3.SS3.SSSx2.p1.2.m2.1a"><msubsup id="S3.SS3.SSSx2.p1.2.m2.1.1" xref="S3.SS3.SSSx2.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSSx2.p1.2.m2.1.1.2.2" xref="S3.SS3.SSSx2.p1.2.m2.1.1.2.2.cmml">t</mi><mi id="S3.SS3.SSSx2.p1.2.m2.1.1.2.3" xref="S3.SS3.SSSx2.p1.2.m2.1.1.2.3.cmml">p</mi><mo id="S3.SS3.SSSx2.p1.2.m2.1.1.3" xref="S3.SS3.SSSx2.p1.2.m2.1.1.3.cmml">+</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx2.p1.2.m2.1b"><apply id="S3.SS3.SSSx2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSSx2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSSx2.p1.2.m2.1.1">superscript</csymbol><apply id="S3.SS3.SSSx2.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSSx2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.SSSx2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSSx2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS3.SSSx2.p1.2.m2.1.1.2.2">ğ‘¡</ci><ci id="S3.SS3.SSSx2.p1.2.m2.1.1.2.3.cmml" xref="S3.SS3.SSSx2.p1.2.m2.1.1.2.3">ğ‘</ci></apply><plus id="S3.SS3.SSSx2.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSSx2.p1.2.m2.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx2.p1.2.m2.1c">t_{p}^{+}</annotation></semantics></math>. Note that the title of the article is appended to the beginning of each passage <math id="S3.SS3.SSSx2.p1.3.m3.1" class="ltx_Math" alttext="t_{p}" display="inline"><semantics id="S3.SS3.SSSx2.p1.3.m3.1a"><msub id="S3.SS3.SSSx2.p1.3.m3.1.1" xref="S3.SS3.SSSx2.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSSx2.p1.3.m3.1.1.2" xref="S3.SS3.SSSx2.p1.3.m3.1.1.2.cmml">t</mi><mi id="S3.SS3.SSSx2.p1.3.m3.1.1.3" xref="S3.SS3.SSSx2.p1.3.m3.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx2.p1.3.m3.1b"><apply id="S3.SS3.SSSx2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSSx2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSSx2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSSx2.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSSx2.p1.3.m3.1.1.2">ğ‘¡</ci><ci id="S3.SS3.SSSx2.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSSx2.p1.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx2.p1.3.m3.1c">t_{p}</annotation></semantics></math> (as in Stage 1). We extend it using the contextual images of Wikipedia paragraphs for the pseudo-question and the <span id="S3.SS3.SSSx2.p1.3.1" class="ltx_text ltx_font_italic">infobox</span> image for the passage (see FigureÂ <a href="#S3.F2" title="Figure 2 â€£ Stage 1: DPR for textual Question Answering. â€£ 3.3 Training stages â€£ 3 Methods â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> empirically demonstrated that a key success of their approach was to leave the pseudo-question in the relevant passage in 10% of the training samples so that the model will learn to perform word matching, as lexical overlap is ultimately a very useful feature for retrieval. In our case, however, we argue that it is neither necessary, as the model should be strongly initialized from StageÂ 1 training on TriviaQA, nor beneficial, as the model could then ignore the image modality. Question and passage encoders pre-trained in StageÂ 1 are used to initialize the visual question and visual passage encoders, respectively.</p>
</div>
<div id="S3.SS3.SSSx2.p2" class="ltx_para">
<p id="S3.SS3.SSSx2.p2.1" class="ltx_p">The process is eased thanks to the WIT datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. WIT consists of millions of images with associated text from Wikipedia and Wikimedia Commons in 108 different languages. We are, however, only interested in English for this work. WhileÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> have multiple strategies to find text related to a given Wikipedia image, such as its Commonsâ€™ caption, we use only the contextual paragraph as text source in order to mimic the downstream KVQAE setting. The resulting English subset of WIT yields 400K <span id="S3.SS3.SSSx2.p2.1.1" class="ltx_text ltx_font_italic">infobox</span> images/articles that correspond to 1.2M paragraphs/images. Those 1.2M paragraphs consist of 13.6M sentences, i.e. potential pseudo-questions, which are 26 words long on average. Therefore, to stick as close as possible to stages 1 and 3, where passages are up to 100 <span id="S3.SS3.SSSx2.p2.1.2" class="ltx_text ltx_font_italic">words</span> long, passages consist of <span id="S3.SS3.SSSx2.p2.1.3" class="ltx_text ltx_font_italic">four</span> sentences. This slightly differs fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> who consider passages of up to 288 <span id="S3.SS3.SSSx2.p2.1.4" class="ltx_text ltx_font_italic">wordpieces</span>, <span id="S3.SS3.SSSx2.p2.1.5" class="ltx_text ltx_font_italic">prior</span> to the pseudo-question masking.</p>
</div>
<div id="S3.SS3.SSSx2.p3" class="ltx_para">
<p id="S3.SS3.SSSx2.p3.1" class="ltx_p">Because both ViQuAE and WIT images are taken from Wikimedia Commons<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://commons.wikimedia.org/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://commons.wikimedia.org/</a></span></span></span>, we can estimate from the image URLs that 14% of ViQuAE images overlap with WIT. This might lead to a bias that we analyze in SectionÂ <a href="#S4.SS1" title="4.1 Information Retrieval â€£ 4 Results â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<div id="S3.SS3.SSSx2.p4" class="ltx_para">
<p id="S3.SS3.SSSx2.p4.2" class="ltx_p">Inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, to prevent <span id="S3.SS3.SSSx2.p4.2.1" class="ltx_text ltx_font_italic">catastrophic forgetting</span> and enforce a <span id="S3.SS3.SSSx2.p4.2.2" class="ltx_text ltx_font_italic">modality-invariant</span> representation of the entities, the last <math id="S3.SS3.SSSx2.p4.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS3.SSSx2.p4.1.m1.1a"><mi id="S3.SS3.SSSx2.p4.1.m1.1.1" xref="S3.SS3.SSSx2.p4.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx2.p4.1.m1.1b"><ci id="S3.SS3.SSSx2.p4.1.m1.1.1.cmml" xref="S3.SS3.SSSx2.p4.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx2.p4.1.m1.1c">l</annotation></semantics></math> layers of BERT are frozen during this stage. In this way, we tune only the first, modality-specific layers of ECA, the intuition being to â€œreplaceâ€ the text-named entities learned during StageÂ 1 with the â€œvisualâ€ entities present in the images. ILF fully freezes BERT during this stage, relying only on the <math id="S3.SS3.SSSx2.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{W}_{t}" display="inline"><semantics id="S3.SS3.SSSx2.p4.2.m2.1a"><msub id="S3.SS3.SSSx2.p4.2.m2.1.1" xref="S3.SS3.SSSx2.p4.2.m2.1.1.cmml"><mi id="S3.SS3.SSSx2.p4.2.m2.1.1.2" xref="S3.SS3.SSSx2.p4.2.m2.1.1.2.cmml">ğ–</mi><mi id="S3.SS3.SSSx2.p4.2.m2.1.1.3" xref="S3.SS3.SSSx2.p4.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx2.p4.2.m2.1b"><apply id="S3.SS3.SSSx2.p4.2.m2.1.1.cmml" xref="S3.SS3.SSSx2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p4.2.m2.1.1.1.cmml" xref="S3.SS3.SSSx2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSSx2.p4.2.m2.1.1.2.cmml" xref="S3.SS3.SSSx2.p4.2.m2.1.1.2">ğ–</ci><ci id="S3.SS3.SSSx2.p4.2.m2.1.1.3.cmml" xref="S3.SS3.SSSx2.p4.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx2.p4.2.m2.1c">\mathbf{W}_{t}</annotation></semantics></math> parameters to tune the text representation.
Furthermore, CLIP is systematically frozen throughout all stages.</p>
</div>
<div id="S3.SS3.SSSx2.p5" class="ltx_para">
<p id="S3.SS3.SSSx2.p5.1" class="ltx_p">We do not have a straightforward way of mining irrelevant visual passages in this stage. In early experiments, we tried to synthesize them by permuting images in the batch: <math id="S3.SS3.SSSx2.p5.1.m1.4" class="ltx_Math" alttext="(t_{p}^{+},i_{p}^{+})\leftarrow(t_{p}^{+},i_{p}^{-})" display="inline"><semantics id="S3.SS3.SSSx2.p5.1.m1.4a"><mrow id="S3.SS3.SSSx2.p5.1.m1.4.4" xref="S3.SS3.SSSx2.p5.1.m1.4.4.cmml"><mrow id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.3" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.3.cmml">(</mo><msubsup id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.2" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.2.cmml">t</mi><mi id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.3" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.3.cmml">p</mi><mo id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.3" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.3.cmml">+</mo></msubsup><mo id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.4" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.3.cmml">,</mo><msubsup id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.2" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.2.cmml">i</mi><mi id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.3" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.3.cmml">p</mi><mo id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.3" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.3.cmml">+</mo></msubsup><mo stretchy="false" id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.5" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.3.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS3.SSSx2.p5.1.m1.4.4.5" xref="S3.SS3.SSSx2.p5.1.m1.4.4.5.cmml">â†</mo><mrow id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.3.cmml"><mo stretchy="false" id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.3" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.3.cmml">(</mo><msubsup id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.cmml"><mi id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.2" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.2.cmml">t</mi><mi id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.3" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.3.cmml">p</mi><mo id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.3" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.3.cmml">+</mo></msubsup><mo id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.4" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.3.cmml">,</mo><msubsup id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.cmml"><mi id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.2" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.2.cmml">i</mi><mi id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.3" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.3.cmml">p</mi><mo id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.3" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.3.cmml">âˆ’</mo></msubsup><mo stretchy="false" id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.5" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx2.p5.1.m1.4b"><apply id="S3.SS3.SSSx2.p5.1.m1.4.4.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4"><ci id="S3.SS3.SSSx2.p5.1.m1.4.4.5.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.5">â†</ci><interval closure="open" id="S3.SS3.SSSx2.p5.1.m1.2.2.2.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2"><apply id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.2">ğ‘¡</ci><ci id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.2.3">ğ‘</ci></apply><plus id="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.1.1.1.1.1.3"></plus></apply><apply id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2">superscript</csymbol><apply id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.2.3">ğ‘</ci></apply><plus id="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.2.2.2.2.2.3"></plus></apply></interval><interval closure="open" id="S3.SS3.SSSx2.p5.1.m1.4.4.4.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2"><apply id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1">superscript</csymbol><apply id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1">subscript</csymbol><ci id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.2">ğ‘¡</ci><ci id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.2.3">ğ‘</ci></apply><plus id="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.3.3.3.1.1.3"></plus></apply><apply id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2">superscript</csymbol><apply id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.1.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2">subscript</csymbol><ci id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.2.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.2">ğ‘–</ci><ci id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.2.3">ğ‘</ci></apply><minus id="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.3.cmml" xref="S3.SS3.SSSx2.p5.1.m1.4.4.4.2.2.3"></minus></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx2.p5.1.m1.4c">(t_{p}^{+},i_{p}^{+})\leftarrow(t_{p}^{+},i_{p}^{-})</annotation></semantics></math>, but it did not improve the results.</p>
</div>
<div id="S3.SS3.SSSx2.p6" class="ltx_para">
<p id="S3.SS3.SSSx2.p6.1" class="ltx_p">After filtering corrupted images or images with inappropriate image formats (e.g. .svg) and paragraphs with a single sentence, we end up with 975K paragraphs/images. We refer to it as WIT in the rest of the paper. It is split into train (878K), validation (48K, to tune hyperparameters), and test (48K, as a sanity check) subsets such that there is no overlap between articles.</p>
</div>
</section>
<section id="S3.SS3.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Stage 3: Knowledge-based Visual Question Answering about Named Entities.</h4>

<div id="S3.SS3.SSSx3.p1" class="ltx_para">
<p id="S3.SS3.SSSx3.p1.2" class="ltx_p">This stage consists in fine-tuning the model on a downstream KVQAE dataset, which provides visual questions <math id="S3.SS3.SSSx3.p1.1.m1.2" class="ltx_Math" alttext="(t_{q},i_{q})" display="inline"><semantics id="S3.SS3.SSSx3.p1.1.m1.2a"><mrow id="S3.SS3.SSSx3.p1.1.m1.2.2.2" xref="S3.SS3.SSSx3.p1.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.SSSx3.p1.1.m1.2.2.2.3" xref="S3.SS3.SSSx3.p1.1.m1.2.2.3.cmml">(</mo><msub id="S3.SS3.SSSx3.p1.1.m1.1.1.1.1" xref="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.2" xref="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.2.cmml">t</mi><mi id="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.3" xref="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.SS3.SSSx3.p1.1.m1.2.2.2.4" xref="S3.SS3.SSSx3.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS3.SSSx3.p1.1.m1.2.2.2.2" xref="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.2" xref="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.2.cmml">i</mi><mi id="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.3" xref="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.3.cmml">q</mi></msub><mo stretchy="false" id="S3.SS3.SSSx3.p1.1.m1.2.2.2.5" xref="S3.SS3.SSSx3.p1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx3.p1.1.m1.2b"><interval closure="open" id="S3.SS3.SSSx3.p1.1.m1.2.2.3.cmml" xref="S3.SS3.SSSx3.p1.1.m1.2.2.2"><apply id="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.SSSx3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.SSSx3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.2">ğ‘¡</ci><ci id="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.SSSx3.p1.1.m1.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.cmml" xref="S3.SS3.SSSx3.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.SSSx3.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.SSSx3.p1.1.m1.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx3.p1.1.m1.2c">(t_{q},i_{q})</annotation></semantics></math> and relevant visual passages <math id="S3.SS3.SSSx3.p1.2.m2.2" class="ltx_Math" alttext="(t_{p}^{+},i_{p}^{+})" display="inline"><semantics id="S3.SS3.SSSx3.p1.2.m2.2a"><mrow id="S3.SS3.SSSx3.p1.2.m2.2.2.2" xref="S3.SS3.SSSx3.p1.2.m2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.SSSx3.p1.2.m2.2.2.2.3" xref="S3.SS3.SSSx3.p1.2.m2.2.2.3.cmml">(</mo><msubsup id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.2" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.2.cmml">t</mi><mi id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.3" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.3.cmml">p</mi><mo id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.3" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.3.cmml">+</mo></msubsup><mo id="S3.SS3.SSSx3.p1.2.m2.2.2.2.4" xref="S3.SS3.SSSx3.p1.2.m2.2.2.3.cmml">,</mo><msubsup id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.cmml"><mi id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.2" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.2.cmml">i</mi><mi id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.3" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.3.cmml">p</mi><mo id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.3" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.3.cmml">+</mo></msubsup><mo stretchy="false" id="S3.SS3.SSSx3.p1.2.m2.2.2.2.5" xref="S3.SS3.SSSx3.p1.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx3.p1.2.m2.2b"><interval closure="open" id="S3.SS3.SSSx3.p1.2.m2.2.2.3.cmml" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2"><apply id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1">superscript</csymbol><apply id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.1.cmml" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.2.cmml" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.2">ğ‘¡</ci><ci id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.3.cmml" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.2.3">ğ‘</ci></apply><plus id="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.SSSx3.p1.2.m2.1.1.1.1.3"></plus></apply><apply id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2">superscript</csymbol><apply id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.2.3">ğ‘</ci></apply><plus id="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.3.cmml" xref="S3.SS3.SSSx3.p1.2.m2.2.2.2.2.3"></plus></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx3.p1.2.m2.2c">(t_{p}^{+},i_{p}^{+})</annotation></semantics></math>. FollowingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, all layers of the model are tuned during this stage.</p>
</div>
<div id="S3.SS3.SSSx3.p2" class="ltx_para">
<p id="S3.SS3.SSSx3.p2.1" class="ltx_p">A subtlety of this stage is the selection of irrelevant visual passages <math id="S3.SS3.SSSx3.p2.1.m1.2" class="ltx_Math" alttext="(t_{p}^{-},i_{p}^{-})" display="inline"><semantics id="S3.SS3.SSSx3.p2.1.m1.2a"><mrow id="S3.SS3.SSSx3.p2.1.m1.2.2.2" xref="S3.SS3.SSSx3.p2.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.SSSx3.p2.1.m1.2.2.2.3" xref="S3.SS3.SSSx3.p2.1.m1.2.2.3.cmml">(</mo><msubsup id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.2" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.2.cmml">t</mi><mi id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.3" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.3.cmml">p</mi><mo id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.3" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.3.cmml">âˆ’</mo></msubsup><mo id="S3.SS3.SSSx3.p2.1.m1.2.2.2.4" xref="S3.SS3.SSSx3.p2.1.m1.2.2.3.cmml">,</mo><msubsup id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.2" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.2.cmml">i</mi><mi id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.3" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.3.cmml">p</mi><mo id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.3" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.3.cmml">âˆ’</mo></msubsup><mo stretchy="false" id="S3.SS3.SSSx3.p2.1.m1.2.2.2.5" xref="S3.SS3.SSSx3.p2.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSSx3.p2.1.m1.2b"><interval closure="open" id="S3.SS3.SSSx3.p2.1.m1.2.2.3.cmml" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2"><apply id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.cmml" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.2">ğ‘¡</ci><ci id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.2.3">ğ‘</ci></apply><minus id="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.SSSx3.p2.1.m1.1.1.1.1.3"></minus></apply><apply id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.cmml" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2">superscript</csymbol><apply id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.2.3">ğ‘</ci></apply><minus id="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.SSSx3.p2.1.m1.2.2.2.2.3"></minus></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSSx3.p2.1.m1.2c">(t_{p}^{-},i_{p}^{-})</annotation></semantics></math>. As mentioned in SectionÂ <a href="#S2" title="2 Related Work â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, it was shown to be essential to DPRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and it is more generally important for contrastive learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, irrelevant passages are mined with BM25 to train DPR. However, we suppose that this is suboptimal for ECA and ILF as BM25 will only mine textually-plausible passages but not visually-plausible ones. Therefore, we use the system provided byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> to mine irrelevant passages. It is a late-fusion of DPR, ArcFaceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, CLIP, and ImageNet-ResNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. This leads to different training setups between DPR (used as a baseline) and our models. However, we have experimented both for DPR and found no significant differences<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Evaluation methods are detailed in SectionÂ <a href="#S4" title="4 Results â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span></span></span>.</p>
</div>
<div id="S3.SS3.SSSx3.p3" class="ltx_para">
<p id="S3.SS3.SSSx3.p3.1" class="ltx_p">We use the same KB asÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, which is based upon KILTâ€™s Wikipedia and Wikidata images of the corresponding entities. It consists of 1.5M articles (thus images/entities) split into 12M passages of at most 100 words as in StageÂ 1.</p>
</div>
<div id="S3.SS3.SSSx3.p4" class="ltx_para">
<p id="S3.SS3.SSSx3.p4.1" class="ltx_p">Visual questions in ViQuAE are split into train (<span id="S3.SS3.SSSx3.p4.1.1" class="ltx_text ltx_number">1,190</span>), validation (<span id="S3.SS3.SSSx3.p4.1.2" class="ltx_text ltx_number">1,250</span>), and test (<span id="S3.SS3.SSSx3.p4.1.3" class="ltx_text ltx_number">1,257</span>) without overlap between imagesâ€™ URLsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
We do not experiment with KVQAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> for the following reasons: (i) it is generated automatically from Wikidata so our text-based KB has a poor coverage of the answers; (ii) it comprises yes/no questions for which passage relevance cannot be assessed automatically.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Inference</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">For efficient retrieval, every passage in the KB is embedded along with its corresponding image by the visual passage encoder beforehand. Given a question grounded in an image, both are embedded by the visual question encoder. Search is then carried out with maximum inner product search using FaissÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Implementation Details</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Our code is built upon PyTorchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, Hugging Faceâ€™s <span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_typewriter">transformers</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and <span id="S3.SS5.p1.1.2" class="ltx_text ltx_font_typewriter">datasets</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> (itself wrapping Faiss). It is freely available along with the data and trained models<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="https://github.com/PaulLerner/ViQuAE" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/PaulLerner/ViQuAE</a></span></span></span>.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.4" class="ltx_p">To train ECA, we use the same hyperparameters asÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> for DPR, themselves based uponÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. In particular, we use a learning rate of <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mrow id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml"><mn id="S3.SS5.p2.1.m1.1.1.2" xref="S3.SS5.p2.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p2.1.m1.1.1.1" xref="S3.SS5.p2.1.m1.1.1.1.cmml">Ã—</mo><msup id="S3.SS5.p2.1.m1.1.1.3" xref="S3.SS5.p2.1.m1.1.1.3.cmml"><mn id="S3.SS5.p2.1.m1.1.1.3.2" xref="S3.SS5.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S3.SS5.p2.1.m1.1.1.3.3" xref="S3.SS5.p2.1.m1.1.1.3.3.cmml"><mo id="S3.SS5.p2.1.m1.1.1.3.3a" xref="S3.SS5.p2.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S3.SS5.p2.1.m1.1.1.3.3.2" xref="S3.SS5.p2.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1"><times id="S3.SS5.p2.1.m1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS5.p2.1.m1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.1.2">2</cn><apply id="S3.SS5.p2.1.m1.1.1.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.1.3.1.cmml" xref="S3.SS5.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS5.p2.1.m1.1.1.3.2.cmml" xref="S3.SS5.p2.1.m1.1.1.3.2">10</cn><apply id="S3.SS5.p2.1.m1.1.1.3.3.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3"><minus id="S3.SS5.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S3.SS5.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS5.p2.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">2\times 10^{-5}</annotation></semantics></math> along with the Adam optimizerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. It is scheduled linearly with 100 and 4 warm-up steps for stages 2 and 3, respectively. However, for ILF, we found, based on the validation set, that it converged faster with a learning rate of <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="2\times 10^{-3}" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mrow id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mn id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p2.2.m2.1.1.1" xref="S3.SS5.p2.2.m2.1.1.1.cmml">Ã—</mo><msup id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml"><mn id="S3.SS5.p2.2.m2.1.1.3.2" xref="S3.SS5.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S3.SS5.p2.2.m2.1.1.3.3" xref="S3.SS5.p2.2.m2.1.1.3.3.cmml"><mo id="S3.SS5.p2.2.m2.1.1.3.3a" xref="S3.SS5.p2.2.m2.1.1.3.3.cmml">âˆ’</mo><mn id="S3.SS5.p2.2.m2.1.1.3.3.2" xref="S3.SS5.p2.2.m2.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><times id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2">2</cn><apply id="S3.SS5.p2.2.m2.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.3.1.cmml" xref="S3.SS5.p2.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS5.p2.2.m2.1.1.3.2.cmml" xref="S3.SS5.p2.2.m2.1.1.3.2">10</cn><apply id="S3.SS5.p2.2.m2.1.1.3.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3.3"><minus id="S3.SS5.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS5.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S3.SS5.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS5.p2.2.m2.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">2\times 10^{-3}</annotation></semantics></math> and a constant scheduler during StageÂ 2.
We believe this is because ILF fully freezes BERT in StageÂ 2, so it does not require careful scheduling or a small learning rate.
DropoutÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> is applied in BERT and after projecting embedding with <math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{W}_{c}" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><msub id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml"><mi id="S3.SS5.p2.3.m3.1.1.2" xref="S3.SS5.p2.3.m3.1.1.2.cmml">ğ–</mi><mi id="S3.SS5.p2.3.m3.1.1.3" xref="S3.SS5.p2.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><apply id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.3.m3.1.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p2.3.m3.1.1.2.cmml" xref="S3.SS5.p2.3.m3.1.1.2">ğ–</ci><ci id="S3.SS5.p2.3.m3.1.1.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">\mathbf{W}_{c}</annotation></semantics></math> and <math id="S3.SS5.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{W}_{t}" display="inline"><semantics id="S3.SS5.p2.4.m4.1a"><msub id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml"><mi id="S3.SS5.p2.4.m4.1.1.2" xref="S3.SS5.p2.4.m4.1.1.2.cmml">ğ–</mi><mi id="S3.SS5.p2.4.m4.1.1.3" xref="S3.SS5.p2.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><apply id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.4.m4.1.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS5.p2.4.m4.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1.2">ğ–</ci><ci id="S3.SS5.p2.4.m4.1.1.3.cmml" xref="S3.SS5.p2.4.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">\mathbf{W}_{t}</annotation></semantics></math> with a probability of 0.1 (as in the standard BERT configuration). Likewise, layer normalizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is applied in BERT and after summing the two embeddings in ILF. Gradientsâ€™ norms are clipped at 2.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Models in stages 2 and 3 are trained with a batch size of 512 and 298 visual questions, respectively. The success of contrastive learning partly relies on a large number of <span id="S3.SS5.p3.1.1" class="ltx_text ltx_font_italic">in-batch negatives</span> and, therefore, a large batch sizeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
We found that gradient checkpointingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> enables the use of much larger batch sizes for ECA<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>It is not necessary for ILF that fully freezes BERT.</span></span></span>. Instead ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> who use <span id="S3.SS5.p3.1.2" class="ltx_text ltx_font_italic">four</span> NVIDIA V100 GPUs with 32GB of RAM each for a total batch size of 128 questions, we are able to fit a batch of 298 questions (as stated above) in a <span id="S3.SS5.p3.1.3" class="ltx_text ltx_font_italic">single</span> V100 GPU. StageÂ 2 takes most of the compute budget, with most models converging after <math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="\approx 8" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mrow id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mi id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml"></mi><mo id="S3.SS5.p3.1.m1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.cmml">â‰ˆ</mo><mn id="S3.SS5.p3.1.m1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><approx id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.SS5.p3.1.m1.1.1.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">\approx 8</annotation></semantics></math>K steps, which takes around three days<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Jean Zay GPUs consume 0.482kW (or 0.259kW after heat recovery) in France, which has an average grid emission factor of 0.0569 kgCO2e/kWh according to <a target="_blank" href="https://bilans-ges.ademe.fr/en" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://bilans-ges.ademe.fr/en</a>.
</span></span></span>.
Checkpoint selection is made based on the validation <span id="S3.SS5.p3.1.4" class="ltx_text ltx_font_italic">in-batch</span> Mean Reciprocal Rank, for all stages. In-batch means that only the other visual passages in the batch are ranked and that each visual question is paired with only one relevant visual passage (as during training).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The retrieval models are evaluated in two different ways: (i) by computing standard IR metrics on visual passage retrieval; (ii) by feeding retrieved visual passages to a reader module that is tasked with extracting the concise answer to the question, thus achieving KVQAE. Put differently, either evaluate whether the system is able to retrieve a <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">relevant passage</span> for the question or whether it is able to <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">answer</span> the question. We find both metrics to correlate. Ablation studies are carried out with IR metrics.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">ViQuAE is based upon TriviaQA, so it is only distantly supervised: the answer is considered correct if it string-matches the ground truth and, likewise, a passage is deemed relevant if it contains the ground truth<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>After standard preprocessing (lowercasing, stripping articles, and punctuation).</span></span></span>. Moreover, Wikipedia aliases of the ground truth are considered to be valid answers.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Information Retrieval</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.46.3.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">
IR evaluation on ViQuAE. <math id="S4.T1.3.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.T1.3.1.m1.1b"><mi id="S4.T1.3.1.m1.1.1" xref="S4.T1.3.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.m1.1c"><ci id="S4.T1.3.1.m1.1.1.cmml" xref="S4.T1.3.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.m1.1d">l</annotation></semantics></math>: Number of frozen layers during Multimodal ICT. Superscripts denote significant differences in Fisherâ€™s randomization test with <math id="S4.T1.4.2.m2.1" class="ltx_Math" alttext="p\leq 0.01" display="inline"><semantics id="S4.T1.4.2.m2.1b"><mrow id="S4.T1.4.2.m2.1.1" xref="S4.T1.4.2.m2.1.1.cmml"><mi id="S4.T1.4.2.m2.1.1.2" xref="S4.T1.4.2.m2.1.1.2.cmml">p</mi><mo id="S4.T1.4.2.m2.1.1.1" xref="S4.T1.4.2.m2.1.1.1.cmml">â‰¤</mo><mn id="S4.T1.4.2.m2.1.1.3" xref="S4.T1.4.2.m2.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.m2.1c"><apply id="S4.T1.4.2.m2.1.1.cmml" xref="S4.T1.4.2.m2.1.1"><leq id="S4.T1.4.2.m2.1.1.1.cmml" xref="S4.T1.4.2.m2.1.1.1"></leq><ci id="S4.T1.4.2.m2.1.1.2.cmml" xref="S4.T1.4.2.m2.1.1.2">ğ‘</ci><cn type="float" id="S4.T1.4.2.m2.1.1.3.cmml" xref="S4.T1.4.2.m2.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.m2.1d">p\leq 0.01</annotation></semantics></math>. Hits@1 is omitted as it is equivalent to P@1.
</span></figcaption>
<table id="S4.T1.44" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.44.41.1" class="ltx_tr">
<th id="S4.T1.44.41.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.44.41.1.1.1" class="ltx_text ltx_font_bold">#</span></th>
<th id="S4.T1.44.41.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.44.41.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T1.44.41.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.44.41.1.3.1" class="ltx_text ltx_font_bold">Multimodal ICT</span></th>
<th id="S4.T1.44.41.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.44.41.1.4.1" class="ltx_text ltx_font_bold">MRR@100</span></th>
<th id="S4.T1.44.41.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.44.41.1.5.1" class="ltx_text ltx_font_bold">P@1</span></th>
<th id="S4.T1.44.41.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.44.41.1.6.1" class="ltx_text ltx_font_bold">P@20</span></th>
<th id="S4.T1.44.41.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.44.41.1.7.1" class="ltx_text ltx_font_bold">Hits@20</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.8.4" class="ltx_tr">
<th id="S4.T1.8.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">a</th>
<th id="S4.T1.8.4.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DPR</th>
<th id="S4.T1.8.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">NA</th>
<td id="S4.T1.5.1.1" class="ltx_td ltx_align_left ltx_border_t">32.8<span id="S4.T1.5.1.1.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.5.1.1.1.1" class="ltx_sup"><span id="S4.T1.5.1.1.1.1.1" class="ltx_text ltx_font_italic">bcdef</span></sup></span></span>
</td>
<td id="S4.T1.6.2.2" class="ltx_td ltx_align_left ltx_border_t">22.8<span id="S4.T1.6.2.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.6.2.2.1.1" class="ltx_sup"><span id="S4.T1.6.2.2.1.1.1" class="ltx_text ltx_font_italic">bcdef</span></sup></span></span>
</td>
<td id="S4.T1.7.3.3" class="ltx_td ltx_align_left ltx_border_t">16.4<span id="S4.T1.7.3.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.7.3.3.1.1" class="ltx_sup"><span id="S4.T1.7.3.3.1.1.1" class="ltx_text ltx_font_italic">bcdef</span></sup></span></span>
</td>
<td id="S4.T1.8.4.4" class="ltx_td ltx_align_left ltx_border_t">61.2<span id="S4.T1.8.4.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.8.4.4.1.1" class="ltx_sup"><span id="S4.T1.8.4.4.1.1.1" class="ltx_text ltx_font_italic">bcdef</span></sup></span></span>
</td>
</tr>
<tr id="S4.T1.14.10" class="ltx_tr">
<th id="S4.T1.14.10.7" class="ltx_td ltx_align_left ltx_th ltx_th_row">b</th>
<th id="S4.T1.14.10.8" class="ltx_td ltx_align_left ltx_th ltx_th_row">DPR + CLIP</th>
<th id="S4.T1.14.10.9" class="ltx_td ltx_align_center ltx_th ltx_th_row">NA</th>
<td id="S4.T1.10.6.2" class="ltx_td ltx_align_left">34.5<sup id="S4.T1.10.6.2.2" class="ltx_sup"><span id="S4.T1.10.6.2.2.1" class="ltx_text ltx_font_italic">a</span></sup><span id="S4.T1.10.6.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.10.6.2.1.1" class="ltx_sup"><span id="S4.T1.10.6.2.1.1.1" class="ltx_text ltx_font_italic">cdef</span></sup></span></span>
</td>
<td id="S4.T1.12.8.4" class="ltx_td ltx_align_left">24.8<sup id="S4.T1.12.8.4.2" class="ltx_sup"><span id="S4.T1.12.8.4.2.1" class="ltx_text ltx_font_italic">a</span></sup><span id="S4.T1.12.8.4.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.12.8.4.1.1" class="ltx_sup"><span id="S4.T1.12.8.4.1.1.1" class="ltx_text ltx_font_italic">cdef</span></sup></span></span>
</td>
<td id="S4.T1.13.9.5" class="ltx_td ltx_align_left">15.8<span id="S4.T1.13.9.5.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.13.9.5.1.1" class="ltx_sup"><span id="S4.T1.13.9.5.1.1.1" class="ltx_text ltx_font_italic">acdef</span></sup></span></span>
</td>
<td id="S4.T1.14.10.6" class="ltx_td ltx_align_left">61.8<span id="S4.T1.14.10.6.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.14.10.6.1.1" class="ltx_sup"><span id="S4.T1.14.10.6.1.1.1" class="ltx_text ltx_font_italic">acdef</span></sup></span></span>
</td>
</tr>
<tr id="S4.T1.20.16" class="ltx_tr">
<th id="S4.T1.20.16.7" class="ltx_td ltx_align_left ltx_th ltx_th_row">c</th>
<th id="S4.T1.20.16.8" class="ltx_td ltx_align_left ltx_th ltx_th_row">ECA</th>
<th id="S4.T1.20.16.9" class="ltx_td ltx_align_center ltx_th ltx_th_row">âœ—</th>
<td id="S4.T1.15.11.1" class="ltx_td ltx_align_left">34.6<span id="S4.T1.15.11.1.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.15.11.1.1.1" class="ltx_sup"><span id="S4.T1.15.11.1.1.1.1" class="ltx_text ltx_font_italic">abdef</span></sup></span></span>
</td>
<td id="S4.T1.17.13.3" class="ltx_td ltx_align_left">25.9<sup id="S4.T1.17.13.3.2" class="ltx_sup"><span id="S4.T1.17.13.3.2.1" class="ltx_text ltx_font_italic">a</span></sup><span id="S4.T1.17.13.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.17.13.3.1.1" class="ltx_sup"><span id="S4.T1.17.13.3.1.1.1" class="ltx_text ltx_font_italic">bdef</span></sup></span></span>
</td>
<td id="S4.T1.19.15.5" class="ltx_td ltx_align_left">17.2<sup id="S4.T1.19.15.5.2" class="ltx_sup"><span id="S4.T1.19.15.5.2.1" class="ltx_text ltx_font_italic">ab</span></sup><span id="S4.T1.19.15.5.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.19.15.5.1.1" class="ltx_sup"><span id="S4.T1.19.15.5.1.1.1" class="ltx_text ltx_font_italic">def</span></sup></span></span>
</td>
<td id="S4.T1.20.16.6" class="ltx_td ltx_align_left">61.6<span id="S4.T1.20.16.6.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.20.16.6.1.1" class="ltx_sup"><span id="S4.T1.20.16.6.1.1.1" class="ltx_text ltx_font_italic">abdef</span></sup></span></span>
</td>
</tr>
<tr id="S4.T1.29.25" class="ltx_tr">
<th id="S4.T1.29.25.10" class="ltx_td ltx_align_left ltx_th ltx_th_row">d</th>
<th id="S4.T1.21.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ECA (<math id="S4.T1.21.17.1.m1.1" class="ltx_Math" alttext="l=6" display="inline"><semantics id="S4.T1.21.17.1.m1.1a"><mrow id="S4.T1.21.17.1.m1.1.1" xref="S4.T1.21.17.1.m1.1.1.cmml"><mi id="S4.T1.21.17.1.m1.1.1.2" xref="S4.T1.21.17.1.m1.1.1.2.cmml">l</mi><mo id="S4.T1.21.17.1.m1.1.1.1" xref="S4.T1.21.17.1.m1.1.1.1.cmml">=</mo><mn id="S4.T1.21.17.1.m1.1.1.3" xref="S4.T1.21.17.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.21.17.1.m1.1b"><apply id="S4.T1.21.17.1.m1.1.1.cmml" xref="S4.T1.21.17.1.m1.1.1"><eq id="S4.T1.21.17.1.m1.1.1.1.cmml" xref="S4.T1.21.17.1.m1.1.1.1"></eq><ci id="S4.T1.21.17.1.m1.1.1.2.cmml" xref="S4.T1.21.17.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S4.T1.21.17.1.m1.1.1.3.cmml" xref="S4.T1.21.17.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.21.17.1.m1.1c">l=6</annotation></semantics></math>)</th>
<th id="S4.T1.29.25.11" class="ltx_td ltx_align_center ltx_th ltx_th_row">âœ“</th>
<td id="S4.T1.23.19.3" class="ltx_td ltx_align_left"><span id="S4.T1.23.19.3.2" class="ltx_text ltx_font_bold">37.8<sup id="S4.T1.23.19.3.2.2" class="ltx_sup"><span id="S4.T1.23.19.3.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">abce</span></sup><span id="S4.T1.23.19.3.2.1" class="ltx_text ltx_phantom ltx_font_medium"><span style="visibility:hidden"><sup id="S4.T1.23.19.3.2.1.1" class="ltx_sup"><span id="S4.T1.23.19.3.2.1.1.1" class="ltx_text ltx_font_italic">f</span></sup></span></span></span></td>
<td id="S4.T1.25.21.5" class="ltx_td ltx_align_left">26.7<sup id="S4.T1.25.21.5.2" class="ltx_sup"><span id="S4.T1.25.21.5.2.1" class="ltx_text ltx_font_italic">a</span></sup><span id="S4.T1.25.21.5.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.25.21.5.1.1" class="ltx_sup"><span id="S4.T1.25.21.5.1.1.1" class="ltx_text ltx_font_italic">bcef</span></sup></span></span>
</td>
<td id="S4.T1.27.23.7" class="ltx_td ltx_align_left"><span id="S4.T1.27.23.7.2" class="ltx_text ltx_font_bold">19.5<sup id="S4.T1.27.23.7.2.2" class="ltx_sup"><span id="S4.T1.27.23.7.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">abce</span></sup><span id="S4.T1.27.23.7.2.1" class="ltx_text ltx_phantom ltx_font_medium"><span style="visibility:hidden"><sup id="S4.T1.27.23.7.2.1.1" class="ltx_sup"><span id="S4.T1.27.23.7.2.1.1.1" class="ltx_text ltx_font_italic">f</span></sup></span></span></span></td>
<td id="S4.T1.29.25.9" class="ltx_td ltx_align_left"><span id="S4.T1.29.25.9.2" class="ltx_text ltx_font_bold">67.6<sup id="S4.T1.29.25.9.2.2" class="ltx_sup"><span id="S4.T1.29.25.9.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">abce</span></sup><span id="S4.T1.29.25.9.2.1" class="ltx_text ltx_phantom ltx_font_medium"><span style="visibility:hidden"><sup id="S4.T1.29.25.9.2.1.1" class="ltx_sup"><span id="S4.T1.29.25.9.2.1.1.1" class="ltx_text ltx_font_italic">f</span></sup></span></span></span></td>
</tr>
<tr id="S4.T1.35.31" class="ltx_tr">
<th id="S4.T1.35.31.7" class="ltx_td ltx_align_left ltx_th ltx_th_row">e</th>
<th id="S4.T1.30.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ECA (<math id="S4.T1.30.26.1.m1.1" class="ltx_Math" alttext="l=0" display="inline"><semantics id="S4.T1.30.26.1.m1.1a"><mrow id="S4.T1.30.26.1.m1.1.1" xref="S4.T1.30.26.1.m1.1.1.cmml"><mi id="S4.T1.30.26.1.m1.1.1.2" xref="S4.T1.30.26.1.m1.1.1.2.cmml">l</mi><mo id="S4.T1.30.26.1.m1.1.1.1" xref="S4.T1.30.26.1.m1.1.1.1.cmml">=</mo><mn id="S4.T1.30.26.1.m1.1.1.3" xref="S4.T1.30.26.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.30.26.1.m1.1b"><apply id="S4.T1.30.26.1.m1.1.1.cmml" xref="S4.T1.30.26.1.m1.1.1"><eq id="S4.T1.30.26.1.m1.1.1.1.cmml" xref="S4.T1.30.26.1.m1.1.1.1"></eq><ci id="S4.T1.30.26.1.m1.1.1.2.cmml" xref="S4.T1.30.26.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S4.T1.30.26.1.m1.1.1.3.cmml" xref="S4.T1.30.26.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.30.26.1.m1.1c">l=0</annotation></semantics></math>)</th>
<th id="S4.T1.35.31.8" class="ltx_td ltx_align_center ltx_th ltx_th_row">âœ“</th>
<td id="S4.T1.31.27.2" class="ltx_td ltx_align_left">35.1<span id="S4.T1.31.27.2.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.31.27.2.1.1" class="ltx_sup"><span id="S4.T1.31.27.2.1.1.1" class="ltx_text ltx_font_italic">abcdf</span></sup></span></span>
</td>
<td id="S4.T1.32.28.3" class="ltx_td ltx_align_left">24.7<span id="S4.T1.32.28.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.32.28.3.1.1" class="ltx_sup"><span id="S4.T1.32.28.3.1.1.1" class="ltx_text ltx_font_italic">abcdf</span></sup></span></span>
</td>
<td id="S4.T1.34.30.5" class="ltx_td ltx_align_left">17.6<sup id="S4.T1.34.30.5.2" class="ltx_sup"><span id="S4.T1.34.30.5.2.1" class="ltx_text ltx_font_italic">b</span></sup><span id="S4.T1.34.30.5.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.34.30.5.1.1" class="ltx_sup"><span id="S4.T1.34.30.5.1.1.1" class="ltx_text ltx_font_italic">acdf</span></sup></span></span>
</td>
<td id="S4.T1.35.31.6" class="ltx_td ltx_align_left">63.7<span id="S4.T1.35.31.6.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.35.31.6.1.1" class="ltx_sup"><span id="S4.T1.35.31.6.1.1.1" class="ltx_text ltx_font_italic">abcdf</span></sup></span></span>
</td>
</tr>
<tr id="S4.T1.44.40" class="ltx_tr">
<th id="S4.T1.44.40.10" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">f</th>
<th id="S4.T1.36.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">ILF (<math id="S4.T1.36.32.1.m1.1" class="ltx_Math" alttext="l=12" display="inline"><semantics id="S4.T1.36.32.1.m1.1a"><mrow id="S4.T1.36.32.1.m1.1.1" xref="S4.T1.36.32.1.m1.1.1.cmml"><mi id="S4.T1.36.32.1.m1.1.1.2" xref="S4.T1.36.32.1.m1.1.1.2.cmml">l</mi><mo id="S4.T1.36.32.1.m1.1.1.1" xref="S4.T1.36.32.1.m1.1.1.1.cmml">=</mo><mn id="S4.T1.36.32.1.m1.1.1.3" xref="S4.T1.36.32.1.m1.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.36.32.1.m1.1b"><apply id="S4.T1.36.32.1.m1.1.1.cmml" xref="S4.T1.36.32.1.m1.1.1"><eq id="S4.T1.36.32.1.m1.1.1.1.cmml" xref="S4.T1.36.32.1.m1.1.1.1"></eq><ci id="S4.T1.36.32.1.m1.1.1.2.cmml" xref="S4.T1.36.32.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S4.T1.36.32.1.m1.1.1.3.cmml" xref="S4.T1.36.32.1.m1.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.36.32.1.m1.1c">l=12</annotation></semantics></math>)</th>
<th id="S4.T1.44.40.11" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">âœ“</th>
<td id="S4.T1.38.34.3" class="ltx_td ltx_align_left ltx_border_bb">37.3<sup id="S4.T1.38.34.3.2" class="ltx_sup"><span id="S4.T1.38.34.3.2.1" class="ltx_text ltx_font_italic">a</span></sup><span id="S4.T1.38.34.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.38.34.3.1.1" class="ltx_sup"><span id="S4.T1.38.34.3.1.1.1" class="ltx_text ltx_font_italic">bcde</span></sup></span></span>
</td>
<td id="S4.T1.40.36.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.40.36.5.2" class="ltx_text ltx_font_bold">26.8<sup id="S4.T1.40.36.5.2.2" class="ltx_sup"><span id="S4.T1.40.36.5.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">a</span></sup><span id="S4.T1.40.36.5.2.1" class="ltx_text ltx_phantom ltx_font_medium"><span style="visibility:hidden"><sup id="S4.T1.40.36.5.2.1.1" class="ltx_sup"><span id="S4.T1.40.36.5.2.1.1.1" class="ltx_text ltx_font_italic">bcde</span></sup></span></span></span></td>
<td id="S4.T1.42.38.7" class="ltx_td ltx_align_left ltx_border_bb">19.1<sup id="S4.T1.42.38.7.2" class="ltx_sup"><span id="S4.T1.42.38.7.2.1" class="ltx_text ltx_font_italic">abce</span></sup><span id="S4.T1.42.38.7.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.42.38.7.1.1" class="ltx_sup"><span id="S4.T1.42.38.7.1.1.1" class="ltx_text ltx_font_italic">d</span></sup></span></span>
</td>
<td id="S4.T1.44.40.9" class="ltx_td ltx_align_left ltx_border_bb">66.9<sup id="S4.T1.44.40.9.2" class="ltx_sup"><span id="S4.T1.44.40.9.2.1" class="ltx_text ltx_font_italic">abc</span></sup><span id="S4.T1.44.40.9.1" class="ltx_text ltx_phantom"><span style="visibility:hidden"><sup id="S4.T1.44.40.9.1.1" class="ltx_sup"><span id="S4.T1.44.40.9.1.1.1" class="ltx_text ltx_font_italic">de</span></sup></span></span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2301.04366/assets/x2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="261" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.2.1" class="ltx_text" style="font-size:90%;">Qualitative examples where ECA (<math id="S4.F3.2.1.m1.1" class="ltx_Math" alttext="l=6" display="inline"><semantics id="S4.F3.2.1.m1.1b"><mrow id="S4.F3.2.1.m1.1.1" xref="S4.F3.2.1.m1.1.1.cmml"><mi id="S4.F3.2.1.m1.1.1.2" xref="S4.F3.2.1.m1.1.1.2.cmml">l</mi><mo id="S4.F3.2.1.m1.1.1.1" xref="S4.F3.2.1.m1.1.1.1.cmml">=</mo><mn id="S4.F3.2.1.m1.1.1.3" xref="S4.F3.2.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.2.1.m1.1c"><apply id="S4.F3.2.1.m1.1.1.cmml" xref="S4.F3.2.1.m1.1.1"><eq id="S4.F3.2.1.m1.1.1.1.cmml" xref="S4.F3.2.1.m1.1.1.1"></eq><ci id="S4.F3.2.1.m1.1.1.2.cmml" xref="S4.F3.2.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S4.F3.2.1.m1.1.1.3.cmml" xref="S4.F3.2.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.1.m1.1d">l=6</annotation></semantics></math>) finds a relevant visual passage in top-1 but late fusion falls behind.</span></figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Because of the setting of ViQuAE, it is impossible to get complete coverage of relevant passages. Therefore we do not use any metric based on recall (e.g. R-Precision, mAP, etc.). Instead, we evaluate the models with Precision@K (P@K), Mean Reciprocal Rank (MRR), and Hits@K. Hits@K is the proportion of questions for which IR retrieves <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">at least one</span> relevant passage in top-K. Statistical significance tests are conducted using Fisherâ€™s randomization testÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. Metrics and statistical tests are computed with <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">ranx</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and are reported in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.1 Information Retrieval â€£ 4 Results â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The best models pre-trained with Multimodal ICT (d and f) outperform the text-only (a) and late-fusion (b) baselines on all metrics. Some qualitative examples are shown in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.1 Information Retrieval â€£ 4 Results â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In the first row, we can see evidence of cross-modal interaction between the image depicting Winston Churchill and the passage that mentions him (while being illustrated by a totally different image). In contrast, the late fusion baseline exhibits textual bias by returning a passage that mentions several English palaces (highlighted in red). The same observation can be made for the second row, where St Paulâ€™s Cathedral is only mentioned in the relevant passage but not depicted in the contextual image. Cross-modal interactions prove useful in this case because of the heterogeneity of visual depictions: Winston Churchill is depicted by a statue in the visual question but by a photograph in the KB.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.2" class="ltx_p">We can see that Multimodal ICT is essential to ECA (c vs. d). Without it, it performs on par with late fusion. We believe this is because of overfitting on the small training set of ViQuAE.
However, we find that fine-tuning on ViQuAE is also essential to ECA, which exhibits catastrophic forgetting because of the sequential learning setup: indeed, after StageÂ 2, it falls behind DPR. We see that the freezing technique ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> helps to prevent catastrophic forgetting to some extent (d vs. e). It is also visible in the upstream WIT pre-training where ECA achieves 91.6 and 92.9 in-batch MRR on WITâ€™s test set with <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="l=6" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">l</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">l=6</annotation></semantics></math> and <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="l=0" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">l</mi><mo id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><eq id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1"></eq><ci id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">ğ‘™</ci><cn type="integer" id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">l=0</annotation></semantics></math>, respectively: fitting WIT better leads to further forgetting.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Unlike what is suggested by related work (Â§<a href="#S2.SS0.SSS2" title="2.0.2 Multimodal Fusion and Pre-Training. â€£ 2 Related Work â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.0.2</span></a>), we find that the linear fusion model performs on par with the more early, cross-attention based, fusion model (f vs. d). This suggests that the improvement over the late fusion baseline indeed comes from the Multimodal ICT pre-training, which is not very sensitive to the modelâ€™s architecture. Moreover, the architecture of ILF allows to fully freeze BERT during StageÂ 2, which circumvents catastrophic forgetting<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>ILF only achieves 87.1 in-batch MRR on WITâ€™s test set because of the freezing.</span></span></span>. We leave other training strategies (e.g. multi-tasking, using adaptersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) for future work.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Nothing suggests that ECA is better on the 14% of ViQuAE images that overlap with WIT. ECA is better on the out-of-WIT subset (38.0 vs. 36.5 MRR), but it is the other way around for DPR and late fusion.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Reading Comprehension</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.20.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.2.1" class="ltx_text" style="font-size:90%;">
Reading Comprehension evaluation on ViQuAE, averaged over 5 runs of the <span id="S4.T2.2.1.1" class="ltx_text ltx_font_italic">reader</span>. <math id="S4.T2.2.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.T2.2.1.m1.1b"><mi id="S4.T2.2.1.m1.1.1" xref="S4.T2.2.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.1.m1.1c"><ci id="S4.T2.2.1.m1.1.1.cmml" xref="S4.T2.2.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.1.m1.1d">l</annotation></semantics></math>: Number of frozen layers during Multimodal ICT.
</span></figcaption>
<table id="S4.T2.17" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.17.16.1" class="ltx_tr">
<th id="S4.T2.17.16.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.17.16.1.1.1" class="ltx_text ltx_font_bold">#</span></th>
<th id="S4.T2.17.16.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.17.16.1.2.1" class="ltx_text ltx_font_bold">IR Model</span></th>
<th id="S4.T2.17.16.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.17.16.1.3.1" class="ltx_text ltx_font_bold">Multimodal ICT</span></th>
<th id="S4.T2.17.16.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.17.16.1.4.1" class="ltx_text ltx_font_bold">Exact Match</span></th>
<th id="S4.T2.17.16.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.17.16.1.5.1" class="ltx_text ltx_font_bold">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.2" class="ltx_tr">
<td id="S4.T2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">a</td>
<td id="S4.T2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">DPR</td>
<td id="S4.T2.4.2.5" class="ltx_td ltx_align_center ltx_border_t">NA</td>
<td id="S4.T2.3.1.1" class="ltx_td ltx_align_center ltx_border_t">16.9 <math id="S4.T2.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.3.1.1.m1.1a"><mo id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.1c">\pm</annotation></semantics></math> 0.4</td>
<td id="S4.T2.4.2.2" class="ltx_td ltx_align_center ltx_border_t">20.1 <math id="S4.T2.4.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.4.2.2.m1.1a"><mo id="S4.T2.4.2.2.m1.1.1" xref="S4.T2.4.2.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.m1.1b"><csymbol cd="latexml" id="S4.T2.4.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.m1.1c">\pm</annotation></semantics></math> 0.5</td>
</tr>
<tr id="S4.T2.6.4" class="ltx_tr">
<td id="S4.T2.6.4.3" class="ltx_td ltx_align_center">b</td>
<td id="S4.T2.6.4.4" class="ltx_td ltx_align_center">DPR + CLIP</td>
<td id="S4.T2.6.4.5" class="ltx_td ltx_align_center">NA</td>
<td id="S4.T2.5.3.1" class="ltx_td ltx_align_center">19.0 <math id="S4.T2.5.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.5.3.1.m1.1a"><mo id="S4.T2.5.3.1.m1.1.1" xref="S4.T2.5.3.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.3.1.m1.1b"><csymbol cd="latexml" id="S4.T2.5.3.1.m1.1.1.cmml" xref="S4.T2.5.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.3.1.m1.1c">\pm</annotation></semantics></math> 0.4</td>
<td id="S4.T2.6.4.2" class="ltx_td ltx_align_center">22.3 <math id="S4.T2.6.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.6.4.2.m1.1a"><mo id="S4.T2.6.4.2.m1.1.1" xref="S4.T2.6.4.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.4.2.m1.1b"><csymbol cd="latexml" id="S4.T2.6.4.2.m1.1.1.cmml" xref="S4.T2.6.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.4.2.m1.1c">\pm</annotation></semantics></math> 0.4</td>
</tr>
<tr id="S4.T2.8.6" class="ltx_tr">
<td id="S4.T2.8.6.3" class="ltx_td ltx_align_center">c</td>
<td id="S4.T2.8.6.4" class="ltx_td ltx_align_center">ECA</td>
<td id="S4.T2.8.6.5" class="ltx_td ltx_align_center">âœ—</td>
<td id="S4.T2.7.5.1" class="ltx_td ltx_align_center">17.7 <math id="S4.T2.7.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.7.5.1.m1.1a"><mo id="S4.T2.7.5.1.m1.1.1" xref="S4.T2.7.5.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.5.1.m1.1b"><csymbol cd="latexml" id="S4.T2.7.5.1.m1.1.1.cmml" xref="S4.T2.7.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.5.1.m1.1c">\pm</annotation></semantics></math> 0.6</td>
<td id="S4.T2.8.6.2" class="ltx_td ltx_align_center">21.2 <math id="S4.T2.8.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.8.6.2.m1.1a"><mo id="S4.T2.8.6.2.m1.1.1" xref="S4.T2.8.6.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.6.2.m1.1b"><csymbol cd="latexml" id="S4.T2.8.6.2.m1.1.1.cmml" xref="S4.T2.8.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.6.2.m1.1c">\pm</annotation></semantics></math> 0.8</td>
</tr>
<tr id="S4.T2.11.9" class="ltx_tr">
<td id="S4.T2.11.9.4" class="ltx_td ltx_align_center">d</td>
<td id="S4.T2.9.7.1" class="ltx_td ltx_align_center">ECA (<math id="S4.T2.9.7.1.m1.1" class="ltx_Math" alttext="l=6" display="inline"><semantics id="S4.T2.9.7.1.m1.1a"><mrow id="S4.T2.9.7.1.m1.1.1" xref="S4.T2.9.7.1.m1.1.1.cmml"><mi id="S4.T2.9.7.1.m1.1.1.2" xref="S4.T2.9.7.1.m1.1.1.2.cmml">l</mi><mo id="S4.T2.9.7.1.m1.1.1.1" xref="S4.T2.9.7.1.m1.1.1.1.cmml">=</mo><mn id="S4.T2.9.7.1.m1.1.1.3" xref="S4.T2.9.7.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.7.1.m1.1b"><apply id="S4.T2.9.7.1.m1.1.1.cmml" xref="S4.T2.9.7.1.m1.1.1"><eq id="S4.T2.9.7.1.m1.1.1.1.cmml" xref="S4.T2.9.7.1.m1.1.1.1"></eq><ci id="S4.T2.9.7.1.m1.1.1.2.cmml" xref="S4.T2.9.7.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S4.T2.9.7.1.m1.1.1.3.cmml" xref="S4.T2.9.7.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.7.1.m1.1c">l=6</annotation></semantics></math>)</td>
<td id="S4.T2.11.9.5" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T2.10.8.2" class="ltx_td ltx_align_center">20.6 <math id="S4.T2.10.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.10.8.2.m1.1a"><mo id="S4.T2.10.8.2.m1.1.1" xref="S4.T2.10.8.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.8.2.m1.1b"><csymbol cd="latexml" id="S4.T2.10.8.2.m1.1.1.cmml" xref="S4.T2.10.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.8.2.m1.1c">\pm</annotation></semantics></math> 0.3</td>
<td id="S4.T2.11.9.3" class="ltx_td ltx_align_center">24.4 <math id="S4.T2.11.9.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.11.9.3.m1.1a"><mo id="S4.T2.11.9.3.m1.1.1" xref="S4.T2.11.9.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.9.3.m1.1b"><csymbol cd="latexml" id="S4.T2.11.9.3.m1.1.1.cmml" xref="S4.T2.11.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.9.3.m1.1c">\pm</annotation></semantics></math> 0.2</td>
</tr>
<tr id="S4.T2.14.12" class="ltx_tr">
<td id="S4.T2.14.12.4" class="ltx_td ltx_align_center">e</td>
<td id="S4.T2.12.10.1" class="ltx_td ltx_align_center">ECA (<math id="S4.T2.12.10.1.m1.1" class="ltx_Math" alttext="l=0" display="inline"><semantics id="S4.T2.12.10.1.m1.1a"><mrow id="S4.T2.12.10.1.m1.1.1" xref="S4.T2.12.10.1.m1.1.1.cmml"><mi id="S4.T2.12.10.1.m1.1.1.2" xref="S4.T2.12.10.1.m1.1.1.2.cmml">l</mi><mo id="S4.T2.12.10.1.m1.1.1.1" xref="S4.T2.12.10.1.m1.1.1.1.cmml">=</mo><mn id="S4.T2.12.10.1.m1.1.1.3" xref="S4.T2.12.10.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.10.1.m1.1b"><apply id="S4.T2.12.10.1.m1.1.1.cmml" xref="S4.T2.12.10.1.m1.1.1"><eq id="S4.T2.12.10.1.m1.1.1.1.cmml" xref="S4.T2.12.10.1.m1.1.1.1"></eq><ci id="S4.T2.12.10.1.m1.1.1.2.cmml" xref="S4.T2.12.10.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S4.T2.12.10.1.m1.1.1.3.cmml" xref="S4.T2.12.10.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.10.1.m1.1c">l=0</annotation></semantics></math>)</td>
<td id="S4.T2.14.12.5" class="ltx_td ltx_align_center">âœ“</td>
<td id="S4.T2.13.11.2" class="ltx_td ltx_align_center">20.8 <math id="S4.T2.13.11.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.13.11.2.m1.1a"><mo id="S4.T2.13.11.2.m1.1.1" xref="S4.T2.13.11.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.13.11.2.m1.1b"><csymbol cd="latexml" id="S4.T2.13.11.2.m1.1.1.cmml" xref="S4.T2.13.11.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.11.2.m1.1c">\pm</annotation></semantics></math> 0.8</td>
<td id="S4.T2.14.12.3" class="ltx_td ltx_align_center">24.3 <math id="S4.T2.14.12.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.14.12.3.m1.1a"><mo id="S4.T2.14.12.3.m1.1.1" xref="S4.T2.14.12.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.12.3.m1.1b"><csymbol cd="latexml" id="S4.T2.14.12.3.m1.1.1.cmml" xref="S4.T2.14.12.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.12.3.m1.1c">\pm</annotation></semantics></math> 0.9</td>
</tr>
<tr id="S4.T2.17.15" class="ltx_tr">
<td id="S4.T2.17.15.4" class="ltx_td ltx_align_center ltx_border_bb">f</td>
<td id="S4.T2.15.13.1" class="ltx_td ltx_align_center ltx_border_bb">ILF (<math id="S4.T2.15.13.1.m1.1" class="ltx_Math" alttext="l=12" display="inline"><semantics id="S4.T2.15.13.1.m1.1a"><mrow id="S4.T2.15.13.1.m1.1.1" xref="S4.T2.15.13.1.m1.1.1.cmml"><mi id="S4.T2.15.13.1.m1.1.1.2" xref="S4.T2.15.13.1.m1.1.1.2.cmml">l</mi><mo id="S4.T2.15.13.1.m1.1.1.1" xref="S4.T2.15.13.1.m1.1.1.1.cmml">=</mo><mn id="S4.T2.15.13.1.m1.1.1.3" xref="S4.T2.15.13.1.m1.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.15.13.1.m1.1b"><apply id="S4.T2.15.13.1.m1.1.1.cmml" xref="S4.T2.15.13.1.m1.1.1"><eq id="S4.T2.15.13.1.m1.1.1.1.cmml" xref="S4.T2.15.13.1.m1.1.1.1"></eq><ci id="S4.T2.15.13.1.m1.1.1.2.cmml" xref="S4.T2.15.13.1.m1.1.1.2">ğ‘™</ci><cn type="integer" id="S4.T2.15.13.1.m1.1.1.3.cmml" xref="S4.T2.15.13.1.m1.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.13.1.m1.1c">l=12</annotation></semantics></math>)</td>
<td id="S4.T2.17.15.5" class="ltx_td ltx_align_center ltx_border_bb">âœ“</td>
<td id="S4.T2.16.14.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.16.14.2.1" class="ltx_text ltx_font_bold">21.3</span> <math id="S4.T2.16.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.16.14.2.m1.1a"><mo id="S4.T2.16.14.2.m1.1.1" xref="S4.T2.16.14.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.16.14.2.m1.1b"><csymbol cd="latexml" id="S4.T2.16.14.2.m1.1.1.cmml" xref="S4.T2.16.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.14.2.m1.1c">\pm</annotation></semantics></math> 0.6</td>
<td id="S4.T2.17.15.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.17.15.3.1" class="ltx_text ltx_font_bold">25.4</span> <math id="S4.T2.17.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.17.15.3.m1.1a"><mo id="S4.T2.17.15.3.m1.1.1" xref="S4.T2.17.15.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.17.15.3.m1.1b"><csymbol cd="latexml" id="S4.T2.17.15.3.m1.1.1.cmml" xref="S4.T2.17.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.15.3.m1.1c">\pm</annotation></semantics></math> 0.3</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To extract the answers from the retrieved passages, we keep the same model asÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. It uses the Multi-passage BERT architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and is thus based on text only because <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">once the relevant passage has been retrieved</span>, the question may be answered without looking at the image. To limit the variations due to training and the number of experiments, we use the model trained byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> off-the-shelf and simply change its input passages. It takes the top-24 passages as input. The model was first trained on TriviaQA (filtered of all questions used in ViQuAE), then fine-tuned on ViQuAE, much like stages 1 and 3. The authors provide <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">five</span> different versions of the model that correspond to different random seeds.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We use Exact Match and F1-score (at the bag-of-words level) to evaluate the extracted answers.
In TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.2 Reading Comprehension â€£ 4 Results â€£ Multimodal Inverse Cloze Task for Knowledge-based Visual Question AnsweringThis work was supported by the ANR-19-CE23-0028 MEERQAT project. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012846 made by GENCI." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we can verify that more relevant passages indeed lead to better downstream answers. The only difference with the IR evaluation is the role of the freezing technique ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> (d vs. e), which is less clear here.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Generic vs. Specialized Image Representations</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Numbers reported in the previous section are actually on par with the best results ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. This is because the latter is based on ArcFace and ImageNet-ResNet, in addition to DPR and CLIP. In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> have a heuristic for taking advantage of the face representations provided by ArcFace: they use ArcFace if faces are detected and a combination of CLIP and ImageNet-ResNet otherwise. They show that this method improves retrieval precision for questions about persons (for which face representations are relevant). However, this approach is not scalable for two reasons: (i) there are near 1,000 different entity types in ViQuAE (according to Wikidataâ€™s ontology), and not all can benefit from specialized representations; (ii) combining several representations (e.g. CLIP and ImageNet-ResNet) for the same entity type is computationally expensive and quickly saturates.
To provide a comparable system to the late fusion ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we have tried integrating ArcFace and ImageNet-ResNet in ECA. However, we have failed to outperform the CLIP-only version of ECA. Intuitively, we think that ECA dilutes the specialized representations of ArcFace and is unable to preserve them throughout all twelve layers of BERT. Therefore, in this setting, ECA is overall on par with late fusion (37.7 vs. 37.9 MRR, not significant) but better on questions about non-persons (39.3 vs. 35.7 MRR), which again suggests that it is unable to exploit ArcFaceâ€™s representations.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Perspectives</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We have presented a new pre-training method, Multimodal Inverse Cloze Task, for Knowledge-based Visual Question Answering about Named Entities. Multimodal ICT leverages contextual images in multimodal documents to generate pseudo-visual questions. It enables the use of more complex multimodal fusion models than previously proposed late fusion methods. Consequently, our method improves retrieval accuracy over the latter by 10% relative-MRR, leading to a 9% relative-F1 improvement in downstream reading comprehension (i.e. answer extraction), on the recently introduced ViQuAE dataset. We believe it is thanks to cross-modal interactions, which are prohibited by late fusion. More precisely, we qualitatively observed that these interactions occurred between the image of the visual question and the text of the KB, which counteracts the heterogeneity of visual depictions.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We have experimented our pre-training method with two different neural networks architectures: (i) ECA, which follows recently proposed Multimodal BERTs by fusing modalities Early via Cross-Attention; (ii) ILF, a more standard model that fuses modalities through a linear projection. We found that both perform equally well, unlike in standard VQA and cross-modal retrieval. We argue that it might be because of their difference in training settings, which leads ECA to catastrophic forgetting. However, further investigations are required.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">While aiming for generic multimodal representations of named entities, we found that integrating specialized representations in our models, such as ArcFace for faces, was not beneficial. We hypothesize that the studied architectures may be inappropriate but we leave this issue for future studies.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.3" class="ltx_p">For future work, we think that generalizing Multimodal ICT for re-ranking (processing <math id="S6.p4.1.m1.2" class="ltx_Math" alttext="(t_{q},i_{q})" display="inline"><semantics id="S6.p4.1.m1.2a"><mrow id="S6.p4.1.m1.2.2.2" xref="S6.p4.1.m1.2.2.3.cmml"><mo stretchy="false" id="S6.p4.1.m1.2.2.2.3" xref="S6.p4.1.m1.2.2.3.cmml">(</mo><msub id="S6.p4.1.m1.1.1.1.1" xref="S6.p4.1.m1.1.1.1.1.cmml"><mi id="S6.p4.1.m1.1.1.1.1.2" xref="S6.p4.1.m1.1.1.1.1.2.cmml">t</mi><mi id="S6.p4.1.m1.1.1.1.1.3" xref="S6.p4.1.m1.1.1.1.1.3.cmml">q</mi></msub><mo id="S6.p4.1.m1.2.2.2.4" xref="S6.p4.1.m1.2.2.3.cmml">,</mo><msub id="S6.p4.1.m1.2.2.2.2" xref="S6.p4.1.m1.2.2.2.2.cmml"><mi id="S6.p4.1.m1.2.2.2.2.2" xref="S6.p4.1.m1.2.2.2.2.2.cmml">i</mi><mi id="S6.p4.1.m1.2.2.2.2.3" xref="S6.p4.1.m1.2.2.2.2.3.cmml">q</mi></msub><mo stretchy="false" id="S6.p4.1.m1.2.2.2.5" xref="S6.p4.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p4.1.m1.2b"><interval closure="open" id="S6.p4.1.m1.2.2.3.cmml" xref="S6.p4.1.m1.2.2.2"><apply id="S6.p4.1.m1.1.1.1.1.cmml" xref="S6.p4.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S6.p4.1.m1.1.1.1.1.1.cmml" xref="S6.p4.1.m1.1.1.1.1">subscript</csymbol><ci id="S6.p4.1.m1.1.1.1.1.2.cmml" xref="S6.p4.1.m1.1.1.1.1.2">ğ‘¡</ci><ci id="S6.p4.1.m1.1.1.1.1.3.cmml" xref="S6.p4.1.m1.1.1.1.1.3">ğ‘</ci></apply><apply id="S6.p4.1.m1.2.2.2.2.cmml" xref="S6.p4.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S6.p4.1.m1.2.2.2.2.1.cmml" xref="S6.p4.1.m1.2.2.2.2">subscript</csymbol><ci id="S6.p4.1.m1.2.2.2.2.2.cmml" xref="S6.p4.1.m1.2.2.2.2.2">ğ‘–</ci><ci id="S6.p4.1.m1.2.2.2.2.3.cmml" xref="S6.p4.1.m1.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.1.m1.2c">(t_{q},i_{q})</annotation></semantics></math> and <math id="S6.p4.2.m2.2" class="ltx_Math" alttext="(t_{p},i_{p})" display="inline"><semantics id="S6.p4.2.m2.2a"><mrow id="S6.p4.2.m2.2.2.2" xref="S6.p4.2.m2.2.2.3.cmml"><mo stretchy="false" id="S6.p4.2.m2.2.2.2.3" xref="S6.p4.2.m2.2.2.3.cmml">(</mo><msub id="S6.p4.2.m2.1.1.1.1" xref="S6.p4.2.m2.1.1.1.1.cmml"><mi id="S6.p4.2.m2.1.1.1.1.2" xref="S6.p4.2.m2.1.1.1.1.2.cmml">t</mi><mi id="S6.p4.2.m2.1.1.1.1.3" xref="S6.p4.2.m2.1.1.1.1.3.cmml">p</mi></msub><mo id="S6.p4.2.m2.2.2.2.4" xref="S6.p4.2.m2.2.2.3.cmml">,</mo><msub id="S6.p4.2.m2.2.2.2.2" xref="S6.p4.2.m2.2.2.2.2.cmml"><mi id="S6.p4.2.m2.2.2.2.2.2" xref="S6.p4.2.m2.2.2.2.2.2.cmml">i</mi><mi id="S6.p4.2.m2.2.2.2.2.3" xref="S6.p4.2.m2.2.2.2.2.3.cmml">p</mi></msub><mo stretchy="false" id="S6.p4.2.m2.2.2.2.5" xref="S6.p4.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p4.2.m2.2b"><interval closure="open" id="S6.p4.2.m2.2.2.3.cmml" xref="S6.p4.2.m2.2.2.2"><apply id="S6.p4.2.m2.1.1.1.1.cmml" xref="S6.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S6.p4.2.m2.1.1.1.1.1.cmml" xref="S6.p4.2.m2.1.1.1.1">subscript</csymbol><ci id="S6.p4.2.m2.1.1.1.1.2.cmml" xref="S6.p4.2.m2.1.1.1.1.2">ğ‘¡</ci><ci id="S6.p4.2.m2.1.1.1.1.3.cmml" xref="S6.p4.2.m2.1.1.1.1.3">ğ‘</ci></apply><apply id="S6.p4.2.m2.2.2.2.2.cmml" xref="S6.p4.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S6.p4.2.m2.2.2.2.2.1.cmml" xref="S6.p4.2.m2.2.2.2.2">subscript</csymbol><ci id="S6.p4.2.m2.2.2.2.2.2.cmml" xref="S6.p4.2.m2.2.2.2.2.2">ğ‘–</ci><ci id="S6.p4.2.m2.2.2.2.2.3.cmml" xref="S6.p4.2.m2.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.2.m2.2c">(t_{p},i_{p})</annotation></semantics></math> simultaneously) and reading comprehension (generating or extracting the answer from <math id="S6.p4.3.m3.2" class="ltx_Math" alttext="(t_{p},i_{p})" display="inline"><semantics id="S6.p4.3.m3.2a"><mrow id="S6.p4.3.m3.2.2.2" xref="S6.p4.3.m3.2.2.3.cmml"><mo stretchy="false" id="S6.p4.3.m3.2.2.2.3" xref="S6.p4.3.m3.2.2.3.cmml">(</mo><msub id="S6.p4.3.m3.1.1.1.1" xref="S6.p4.3.m3.1.1.1.1.cmml"><mi id="S6.p4.3.m3.1.1.1.1.2" xref="S6.p4.3.m3.1.1.1.1.2.cmml">t</mi><mi id="S6.p4.3.m3.1.1.1.1.3" xref="S6.p4.3.m3.1.1.1.1.3.cmml">p</mi></msub><mo id="S6.p4.3.m3.2.2.2.4" xref="S6.p4.3.m3.2.2.3.cmml">,</mo><msub id="S6.p4.3.m3.2.2.2.2" xref="S6.p4.3.m3.2.2.2.2.cmml"><mi id="S6.p4.3.m3.2.2.2.2.2" xref="S6.p4.3.m3.2.2.2.2.2.cmml">i</mi><mi id="S6.p4.3.m3.2.2.2.2.3" xref="S6.p4.3.m3.2.2.2.2.3.cmml">p</mi></msub><mo stretchy="false" id="S6.p4.3.m3.2.2.2.5" xref="S6.p4.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p4.3.m3.2b"><interval closure="open" id="S6.p4.3.m3.2.2.3.cmml" xref="S6.p4.3.m3.2.2.2"><apply id="S6.p4.3.m3.1.1.1.1.cmml" xref="S6.p4.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S6.p4.3.m3.1.1.1.1.1.cmml" xref="S6.p4.3.m3.1.1.1.1">subscript</csymbol><ci id="S6.p4.3.m3.1.1.1.1.2.cmml" xref="S6.p4.3.m3.1.1.1.1.2">ğ‘¡</ci><ci id="S6.p4.3.m3.1.1.1.1.3.cmml" xref="S6.p4.3.m3.1.1.1.1.3">ğ‘</ci></apply><apply id="S6.p4.3.m3.2.2.2.2.cmml" xref="S6.p4.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S6.p4.3.m3.2.2.2.2.1.cmml" xref="S6.p4.3.m3.2.2.2.2">subscript</csymbol><ci id="S6.p4.3.m3.2.2.2.2.2.cmml" xref="S6.p4.3.m3.2.2.2.2.2">ğ‘–</ci><ci id="S6.p4.3.m3.2.2.2.2.3.cmml" xref="S6.p4.3.m3.2.2.2.2.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.3.m3.2c">(t_{p},i_{p})</annotation></semantics></math>) is an exciting research lead. Indeed, there is evidence that sharing the same model for IR and reading comprehension, or IR and re-ranking, is beneficial for textual QAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and cross-modal retrievalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, respectively: two tasks that closely relate to KVQAE.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,
D.: VQA: Visual Question Answering. In: 2015 IEEE International
Conference on Computer Vision (ICCV). pp. 2425â€“2433. IEEE, Santiago,
Chile (Dec 2015). https://doi.org/10.1109/ICCV.2015.279,
<a target="_blank" href="http://ieeexplore.ieee.org/document/7410636/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://ieeexplore.ieee.org/document/7410636/</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Aytar, Y., Castrejon, L., Vondrick, C., Pirsiavash, H., Torralba, A.:
Cross-modal scene networks. IEEE transactions on pattern analysis and machine
intelligence <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">40</span>(10), 2303â€“2314 (2017), publisher: IEEE

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer Normalization. arXiv:1607.06450
[cs, stat] (Jul 2016), <a target="_blank" href="http://arxiv.org/abs/1607.06450" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://arxiv.org/abs/1607.06450</a>, arXiv:
1607.06450

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bassani, E.: ranx: A Blazing-Fast Python Library forÂ Ranking
Evaluation andÂ Comparison. In: Hagen, M., Verberne, S., Macdonald, C.,
Seifert, C., Balog, K., NÃ¸rvÃ¥g, K., Setty, V. (eds.) Advances in
Information Retrieval. pp. 259â€“264. Lecture Notes in Computer
Science, Springer International Publishing, Cham (2022).
https://doi.org/10.1007/978-3-030-99739-7_30

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Bugliarello, E., Cotterell, R., Okazaki, N., Elliott, D.: Multimodal
Pretraining Unmasked: A Meta-Analysis and a Unified Framework
of Vision-and-Language BERTs. Transactions of the Association for
Computational Linguistics <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">9</span>, 978â€“994 (Sep 2021).
https://doi.org/10.1162/tacl_a_00408, <a target="_blank" href="https://doi.org/10.1162/tacl_a_00408" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1162/taclË™aË™00408</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chen, T., Xu, B., Zhang, C., Guestrin, C.: Training Deep Nets with
Sublinear Memory Cost (Apr 2016). https://doi.org/10.48550/arXiv.1604.06174,
<a target="_blank" href="http://arxiv.org/abs/1604.06174" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://arxiv.org/abs/1604.06174</a>, number: arXiv:1604.06174
arXiv:1604.06174 [cs]

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chen, Y.C., Li, L., Yu, L., ElÂ Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu,
J.: Uniter: Universal image-text representation learning. In: European
Conference on Computer Vision. pp. 104â€“120. Springer (2020)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin
loss for deep face recognition. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (June 2019),
<a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://openaccess.thecvf.com/contentË™CVPRË™2019/html/DengË™ArcFaceË™AdditiveË™AngularË™MarginË™LossË™forË™DeepË™FaceË™RecognitionË™CVPRË™2019Ë™paper.html</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Depeursinge, A., MÃ¼ller, H.: Fusion Techniques for Combining Textual and
Visual Information Retrieval. In: MÃ¼ller, H., Clough, P., Deselaers,
T., Caputo, B. (eds.) ImageCLEF: Experimental Evaluation in Visual
Information Retrieval, pp. 95â€“114. The Information Retrieval
Series, Springer, Berlin, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-15181-1_6,
<a target="_blank" href="https://doi.org/10.1007/978-3-642-15181-1_6" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/978-3-642-15181-1Ë™6</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectional transformers for language understanding. In: Proceedings of the
2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). pp. 4171â€“4186. Association for Computational Linguistics,
Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423,
<a target="_blank" href="https://aclanthology.org/N19-1423" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/N19-1423</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Fan, Y., Xie, X., Cai, Y., Chen, J., Ma, X., Li, X., Zhang, R., Guo, J., Liu,
Y.: Pre-training Methods in Information Retrieval. arXiv:2111.13853
[cs] (Nov 2021), <a target="_blank" href="http://arxiv.org/abs/2111.13853" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://arxiv.org/abs/2111.13853</a>, arXiv: 2111.13853

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Fisher, R.A.: The design of experiments. The design of experiments. (2nd Ed)
(1937), <a target="_blank" href="https://www.cabdirect.org/cabdirect/abstract/19371601600" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.cabdirect.org/cabdirect/abstract/19371601600</a>,
publisher: Oliver &amp; Boyd, Edinburgh &amp; London.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Fun, H., Gandhi, S., Ravi, S.: Efficient Retrieval Optimized Multi-task
Learning. arXiv:2104.10129 [cs] (Apr 2021),
<a target="_blank" href="http://arxiv.org/abs/2104.10129" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://arxiv.org/abs/2104.10129</a>, arXiv: 2104.10129

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Gao, L., Callan, J.: Condenser: a Pre-training Architecture for Dense
Retrieval. In: Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing. pp. 981â€“993. Association for
Computational Linguistics, Online and Punta Cana, Dominican Republic (Nov
2021), <a target="_blank" href="https://aclanthology.org/2021.emnlp-main.75" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2021.emnlp-main.75</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Gao, L., Callan, J.: Unsupervised corpus aware language model pre-training for
dense passage retrieval. In: Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). pp.
2843â€“2853. Association for Computational Linguistics, Dublin, Ireland (May
2022). https://doi.org/10.18653/v1/2022.acl-long.203,
<a target="_blank" href="https://aclanthology.org/2022.acl-long.203" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2022.acl-long.203</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Garcia-Olano, D., Onoe, Y., Ghosh, J.: Improving and diagnosing knowledge-based
visual question answering via entity enhanced knowledge injection. In:
Companion Proceedings of the Web Conference 2022. p. 705â€“715. WWW â€™22,
Association for Computing Machinery, New York, NY, USA (2022).
https://doi.org/10.1145/3487553.3524648, <a target="_blank" href="https://doi.org/10.1145/3487553.3524648" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3487553.3524648</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
GardÃ¨res, F., Ziaeefard, M.: ConceptBert: Concept-Aware Representation
for Visual Question Answering. Findings of the Association for
Computational Linguistics: EMNLP 2020 p.Â 10 (2020),
<a target="_blank" href="https://aclanthology.org/2020.findings-emnlp.44/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2020.findings-emnlp.44/</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Geigle, G., Pfeiffer, J., Reimers, N., VuliÄ‡, I., Gurevych, I.: Retrieve
Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal
Retrieval. Transactions of the Association for Computational Linguistics
<span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">10</span>, 503â€“521 (05 2022). https://doi.org/10.1162/tacl_a_00473,
<a target="_blank" href="https://doi.org/10.1162/tacl_a_00473" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1162/tacl_a_00473</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Guo, Y., Nie, L., Wong, Y., Liu, Y., Cheng, Z., Kankanhalli, M.: A Unified
End-to-End Retriever-Reader Framework for Knowledge-based VQA
(Jun 2022), <a target="_blank" href="http://arxiv.org/abs/2206.14989" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://arxiv.org/abs/2206.14989</a>, number: arXiv:2206.14989
arXiv:2206.14989 [cs]

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 770â€“778 (2016),
<a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://openaccess.thecvf.com/contentË™cvprË™2016/papers/HeË™DeepË™ResidualË™LearningË™CVPRË™2016Ë™paper.pdf</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Heo, Y.J., Kim, E.S., Choi, W.S., Zhang, B.T.: Hypergraph Transformer:
Weakly-supervised multi-hop reasoning for knowledge-based visual question
answering. In: Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). pp. 373â€“390. Association
for Computational Linguistics, Dublin, Ireland (May 2022).
https://doi.org/10.18653/v1/2022.acl-long.29,
<a target="_blank" href="https://aclanthology.org/2022.acl-long.29" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2022.acl-long.29</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Hessel, J., Lee, L.: Does my multimodal model learn cross-modal interactions?
itâ€™s harder to tell than you might think! In: Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP). pp.
861â€“877. Association for Computational Linguistics, Online (Nov 2020).
https://doi.org/10.18653/v1/2020.emnlp-main.62,
<a target="_blank" href="https://aclanthology.org/2020.emnlp-main.62" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2020.emnlp-main.62</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
HofstÃ¤tter, S., Lin, S.C., Yang, J.H., Lin, J., Hanbury, A.: Efficiently
teaching an effective dense retriever with balanced topic aware sampling. In:
Proceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval. p. 113â€“122. SIGIR â€™21, Association
for Computing Machinery, New York, NY, USA (2021).
https://doi.org/10.1145/3404835.3462891, <a target="_blank" href="https://doi.org/10.1145/3404835.3462891" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3404835.3462891</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D.,
Gesmundo, A., Attariyan, M., Gelly, S.: Parameter-Efficient Transfer
Learning for NLP. In: Proceedings of the 36th International
Conference on Machine Learning. pp. 2790â€“2799. PMLR (May 2019),
<a target="_blank" href="https://proceedings.mlr.press/v97/houlsby19a.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://proceedings.mlr.press/v97/houlsby19a.html</a>, iSSN: 2640-3498

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Johnson, J., Douze, M., JÃ©gou, H.: Billion-scale similarity search with
GPUs. IEEE Transactions on Big Data <span id="bib.bib25.1.1" class="ltx_text ltx_font_bold">7</span>(3), 535â€“547 (2019).
https://doi.org/10.1109/TBDATA.2019.2921572

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kalantidis, Y., Sariyildiz, M.B., Pion, N., Weinzaepfel, P., Larlus, D.: Hard
Negative Mixing for Contrastive Learning. In: Advances in Neural
Information Processing Systems. vol.Â 33, pp. 21798â€“21809. Curran
Associates, Inc. (2020),
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/f7cade80b7cc92b991cf4d2806d6bd78-Abstract.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://proceedings.neurips.cc/paper/2020/hash/f7cade80b7cc92b991cf4d2806d6bd78-Abstract.html</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., Yih,
W.t.: Dense passage retrieval for open-domain question answering. In:
Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP). pp. 6769â€“6781. Association for Computational
Linguistics, Online (Nov 2020),
<a target="_blank" href="https://www.aclweb.org/anthology/2020.emnlp-main.550" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.aclweb.org/anthology/2020.emnlp-main.550</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.:
Transformers in vision: A survey. ACM Comput. Surv. (dec 2021).
https://doi.org/10.1145/3505244, <a target="_blank" href="https://doi.org/10.1145/3505244" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3505244</a>, just Accepted

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. In:
Proceedings of the 3rd International Conference for Learning Representations
(Jan 2015), <a target="_blank" href="http://arxiv.org/abs/1412.6980" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://arxiv.org/abs/1412.6980</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kludas, J., Bruno, E., Marchand-Maillet, S.: Information fusion in multimedia
information retrieval. In: International Workshop on Adaptive
Multimedia Retrieval. pp. 147â€“159. Springer (2007)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lee, K., Chang, M.W., Toutanova, K.: Latent Retrieval for Weakly
Supervised Open Domain Question Answering. In: Proceedings of the
57th Annual Meeting of the Association for Computational
Linguistics. pp. 6086â€“6096. Association for Computational Linguistics,
Florence, Italy (Jul 2019). https://doi.org/10.18653/v1/P19-1612,
<a target="_blank" href="https://aclanthology.org/P19-1612" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/P19-1612</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Lerner, P., Ferret, O., Guinaudeau, C., LeÂ Borgne, H., BesanÃ§on, R., Moreno,
J.G., LovÃ³nÂ Melgarejo, J.: ViQuAE, a dataset for knowledge-based visual
question answering about named entities. In: Proceedings of The 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. SIGIRâ€™22, Association for Computing Machinery, New York, NY, USA
(2022). https://doi.org/10.1145/3477495.3531753,
<a target="_blank" href="https://hal.archives-ouvertes.fr/hal-03650618" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://hal.archives-ouvertes.fr/hal-03650618</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Lhoest, Q., VillanovaÂ del Moral, A., Jernite, Y., Thakur, A., von Platen, P.,
Patil, S., Chaumond, J., Drame, M., Plu, J., Tunstall, L., Davison, J.,
Å aÅ¡ko, M., Chhablani, G., Malik, B., Brandeis, S., LeÂ Scao, T., Sanh, V.,
Xu, C., Patry, N., McMillan-Major, A., Schmid, P., Gugger, S., Delangue, C.,
MatussiÃ¨re, T., Debut, L., Bekman, S., Cistac, P., Goehringer, T., Mustar,
V., Lagunas, F., Rush, A., Wolf, T.: Datasets: A Community Library for
Natural Language Processing. In: Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing: System
Demonstrations. pp. 175â€“184. Association for Computational Linguistics,
Online and Punta Cana, Dominican Republic (Nov 2021),
<a target="_blank" href="https://aclanthology.org/2021.emnlp-demo.21" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2021.emnlp-demo.21</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Li, G., Duan, N., Fang, Y., Gong, M., Jiang, D.: Unicoder-VL: A Universal
Encoder for Vision and Language by Cross-Modal Pre-Training.
Proceedings of the AAAI Conference on Artificial Intelligence
<span id="bib.bib34.1.1" class="ltx_text ltx_font_bold">34</span>(07), 11336â€“11344 (Apr 2020). https://doi.org/10.1609/aaai.v34i07.6795,
<a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/6795" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://ojs.aaai.org/index.php/AAAI/article/view/6795</a>, number: 07

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: VisualBERT: A
Simple and Performant Baseline for Vision and Language (Jan 2019),
<a target="_blank" href="https://openreview.net/forum?id=OiixWIb7JpAg" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://openreview.net/forum?id=OiixWIb7JpAg</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Lin, J., Nogueira, R., Yates, A.: Pretrained transformers for text ranking:
Bert and beyond. Synthesis Lectures on Human Language Technologies
<span id="bib.bib36.1.1" class="ltx_text ltx_font_bold">14</span>(4), 1â€“325 (2021). https://doi.org/10.2200/S01123ED1V01Y202108HLT053

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r,
P., Zitnick, C.L.: Microsoft COCO: Common Objects in Context. In:
Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) Computer Vision
â€“ ECCV 2014. pp. 740â€“755. Lecture Notes in Computer Science,
Springer International Publishing, Cham (2014).
https://doi.org/10.1007/978-3-319-10602-1_48

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Lu, J., Batra, D., Parikh, D., Lee, S.: ViLBERT: Pretraining
Task-Agnostic Visiolinguistic Representations for
Vision-and-Language Tasks. Advances in Neural Information Processing
Systems <span id="bib.bib38.1.1" class="ltx_text ltx_font_bold">32</span>, 13â€“23 (2019),
<a target="_blank" href="https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Luo, M., Zeng, Y., Banerjee, P., Baral, C.: Weakly-supervised
visual-retriever-reader for knowledge-based question answering. In:
Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing. pp. 6417â€“6431. Association for Computational Linguistics, Online
and Punta Cana, Dominican Republic (Nov 2021).
https://doi.org/10.18653/v1/2021.emnlp-main.517,
<a target="_blank" href="https://aclanthology.org/2021.emnlp-main.517" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2021.emnlp-main.517</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: OK-VQA: A visual
question answering benchmark requiring external knowledge. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. pp.
3195â€“3204 (2019), <a target="_blank" href="https://ieeexplore.ieee.org/document/8953725/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://ieeexplore.ieee.org/document/8953725/</a>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed
Representations of Words and Phrases and their Compositionality.
Advances in Neural Information Processing Systems <span id="bib.bib41.1.1" class="ltx_text ltx_font_bold">26</span> (2013),
<a target="_blank" href="https://papers.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://papers.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., Chintala, S.: PyTorch: An Imperative Style,
High-Performance Deep Learning Library. Advances in Neural
Information Processing Systems <span id="bib.bib42.1.1" class="ltx_text ltx_font_bold">32</span> (2019),
<a target="_blank" href="https://papers.nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://papers.nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html</a>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., DeÂ Cao, N., Thorne,
J., Jernite, Y., Karpukhin, V., Maillard, J., Plachouras, V.,
RocktÃ¤schel, T., Riedel, S.: KILT: a benchmark for knowledge intensive
language tasks. In: Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies. pp. 2523â€“2544. Association for Computational Linguistics,
Online (Jun 2021). https://doi.org/10.18653/v1/2021.naacl-main.200,
<a target="_blank" href="https://aclanthology.org/2021.naacl-main.200" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2021.naacl-main.200</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Qu, C., Zamani, H., Yang, L., Croft, W.B., Learned-Miller, E.: Passage
retrieval for outside-knowledge visual question answering. In: Proceedings of
the 44th International ACM SIGIR Conference on Research and Development in
Information Retrieval. p. 1753â€“1757. SIGIR â€™21, Association for Computing
Machinery, New York, NY, USA (2021). https://doi.org/10.1145/3404835.3462987,
<a target="_blank" href="https://doi.org/10.1145/3404835.3462987" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3404835.3462987</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W.X., Dong, D., Wu, H.,
Wang, H.: RocketQA: An Optimized Training Approach to Dense
Passage Retrieval for Open-Domain Question Answering. In:
Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language
Technologies. pp. 5835â€“5847. Association for Computational Linguistics,
Online (Jun 2021). https://doi.org/10.18653/v1/2021.naacl-main.466,
<a target="_blank" href="https://aclanthology.org/2021.naacl-main.466" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2021.naacl-main.466</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., etÂ al.: Learning transferable visual
models from natural language supervision. In: International Conference on
Machine Learning. pp. 8748â€“8763. PMLR (2021)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Ram, O., Shachaf, G., Levy, O., Berant, J., Globerson, A.: Learning to
Retrieve Passages without Supervision. In: Proceedings of the 2022
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. pp.
2687â€“2700. Association for Computational Linguistics, Seattle, United States
(Jul 2022), <a target="_blank" href="https://aclanthology.org/2022.naacl-main.193" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2022.naacl-main.193</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards
Real-Time Object Detection with Region Proposal Networks.
Advances in Neural Information Processing Systems <span id="bib.bib48.1.1" class="ltx_text ltx_font_bold">28</span>, 91â€“99
(2015),
<a target="_blank" href="https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Robertson, S.E., Walker, S., Jones, S., Hancock-Beaulieu, M.M., Gatford, M.:
Okapi at TREC-3. In: Harman, D.K. (ed.) Third Text REtrieval Conference
(TREC-3). NIST Special Publication, vol. 500-225, pp. 109â€“126. National
Institute of Standards and Technology (NIST) (1995),
<a target="_blank" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.9922&amp;rep=rep1&amp;type=pdf" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.9922&amp;rep=rep1&amp;type=pdf</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Shah, S., Mishra, A., Yadati, N., Talukdar, P.P.: KVQA: Knowledge-Aware Visual
Question Answering. In: Proceedings of the AAAI Conference on Artificial
Intelligence. vol.Â 33, pp. 8876â€“8884 (2019),
<a target="_blank" href="https://144.208.67.177/ojs/index.php/AAAI/article/view/4915" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://144.208.67.177/ojs/index.php/AAAI/article/view/4915</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning.
In: Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). pp. 2556â€“2565
(2018)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Shevchenko, V., Teney, D., Dick, A., vanÂ den Hengel, A.: Reasoning over vision
and language: Exploring the benefits of supplemental knowledge. In:
Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating
Real-world kNowledge (LANTERN). pp. 1â€“18. Association for Computational
Linguistics, Kyiv, Ukraine (Apr 2021),
<a target="_blank" href="https://aclanthology.org/2021.lantern-1.1" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://aclanthology.org/2021.lantern-1.1</a>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Smucker, M.D., Allan, J., Carterette, B.: A comparison of statistical
significance tests for information retrieval evaluation. In: Proceedings of
the sixteenth ACM conference on Conference on information and knowledge
management. pp. 623â€“632. CIKM â€™07, Association for Computing Machinery,
New York, NY, USA (Nov 2007). https://doi.org/10.1145/1321440.1321528,
<a target="_blank" href="https://doi.org/10.1145/1321440.1321528" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/1321440.1321528</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Srinivasan, K., Raman, K., Chen, J., Bendersky, M., Najork, M.: Wit:
Wikipedia-based image text dataset for multimodal multilingual machine
learning. In: Proceedings of the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval. p. 2443â€“2449. SIGIR â€™21,
Association for Computing Machinery, New York, NY, USA (2021).
https://doi.org/10.1145/3404835.3463257, <a target="_blank" href="https://doi.org/10.1145/3404835.3463257" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3404835.3463257</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overfitting. The
journal of machine learning research <span id="bib.bib55.1.1" class="ltx_text ltx_font_bold">15</span>(1), 1929â€“1958 (2014),
publisher: JMLR. org

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: VL-BERT:
Pre-training of Generic Visual-Linguistic Representations. In:
Proceedings of ICLR 2020 (Feb 2020), <a target="_blank" href="http://arxiv.org/abs/1908.08530" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://arxiv.org/abs/1908.08530</a>,
arXiv: 1908.08530

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Tan, H., Bansal, M.: LXMERT: Learning Cross-Modality Encoder
Representations from Transformers. In: Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP). pp. 5100â€“5111. Association for
Computational Linguistics, Hong Kong, China (Nov 2019).
https://doi.org/10.18653/v1/D19-1514, <a target="_blank" href="https://www.aclweb.org/anthology/D19-1514" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.aclweb.org/anthology/D19-1514</a>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, Å., Polosukhin, I.: Attention is all you need. In: Advances in
neural information processing systems. pp. 5998â€“6008 (2017)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Wang, Z., Li, L., Li, Q., Zeng, D.: Multimodal Data Enhanced
Representation Learning for Knowledge Graphs. In: 2019
International Joint Conference on Neural Networks (IJCNN).
pp.Â 1â€“8 (Jul 2019). https://doi.org/10.1109/IJCNN.2019.8852079, iSSN: 2161-4407

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Weston, J., Chopra, S., Bordes, A.: Memory networks (2015)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame, M.,
Lhoest, Q., Rush, A.M.: HuggingFaceâ€™s Transformers: State-of-the-art
Natural Language Processing. arXiv:1910.03771 [cs] (Jul 2020),
<a target="_blank" href="http://arxiv.org/abs/1910.03771" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://arxiv.org/abs/1910.03771</a>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
Fidler, S.: Aligning books and movies: Towards story-like visual explanations
by watching movies and reading books. In: Proceedings of the IEEE
International Conference on Computer Vision (ICCV) (December 2015),
<a target="_blank" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Zhu_Aligning_Books_and_ICCV_2015_paper.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.cv-foundation.org/openaccess/contentË™iccvË™2015/html/ZhuË™AligningË™BooksË™andË™ICCVË™2015Ë™paper.html</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.04365" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.04366" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.04366">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.04366" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.04367" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 03:59:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
