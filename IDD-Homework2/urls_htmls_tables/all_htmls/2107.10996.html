<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.10996] Communication Efficiency in Federated Learning: Achievements and Challenges</title><meta property="og:description" content="Federated Learning (FL) is known to perform Machine Learning tasks in a distributed manner. Over the years, this has become an emerging technology especially with various data protection and privacy policies being impo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Communication Efficiency in Federated Learning: Achievements and Challenges">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Communication Efficiency in Federated Learning: Achievements and Challenges">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.10996">

<!--Generated on Wed Mar  6 23:06:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Security,  Privacy,  Federated learning,  Communication,  Machine Learning.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Communication Efficiency in Federated Learning: Achievements and Challenges</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Osama Shahid1, Seyedamin Pouriyeh1, Reza M. Parizi2, Quan Z. Sheng3, Gautam Srivastava4, Liang Zhao1
</span><span class="ltx_author_notes"><span id="id1.1.id1" class="ltx_text" style="font-size:70%;">Corresponding author: S. Pouriyeh (email: spouriye@kennesaw.edu).</span>
<span class="ltx_contact ltx_role_affiliation">1 Department of Information Technology, Kennesaw State University, Marietta, GA, USA

<br class="ltx_break">oshahid@students.kennesaw.edu, {spouriye, lzhao10}@kennesaw.edu
</span>
<span class="ltx_contact ltx_role_affiliation">2 Department of Software Engineering and Game Development, Kennesaw State University, Marietta, GA, USA 
<br class="ltx_break">rparizi1@kennesaw.edu
</span>
<span class="ltx_contact ltx_role_affiliation">3Department of Computing, Macquarie University, Sydney, Australia

<br class="ltx_break">michael.sheng@mq.edu.au 
</span>
<span class="ltx_contact ltx_role_affiliation">4Department of Math and Computer Science, Brandon University, Canada

<br class="ltx_break">
</span>
<span class="ltx_contact ltx_role_affiliation">4Research Centre for Interneural Computing, China Medical University, Taichung Taiwan

<br class="ltx_break">srivastavag@brandonu.ca
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Federated Learning (FL) is known to perform Machine Learning tasks in a distributed manner. Over the years, this has become an emerging technology especially with various data protection and privacy policies being imposed FL allows performing machine learning tasks whilst adhering to these challenges. As with the emerging of any new technology, there are going to be challenges and benefits. A challenge that exists in FL is the communication costs, as FL takes place in a distributed environment where devices connected over the network have to constantly share their updates this can create a communication bottleneck. In this paper, we present a survey of the research that is performed to overcome the communication constraints in an FL setting.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Security, Privacy, Federated learning, Communication, Machine Learning.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A Machine Learning (ML) model for a specific task is created utilizing the unprecedented amount of data that is generated from devices. This data can be used to achieve process optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, gain insight discovery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and aid in decision making <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Some examples of devices that generated data can be smartphones and wearable devices, smart homes, etc. Traditionally, to implement ML predictive models the data would need to be transferred to a central server where it could be stored and used for training and testing ML models that are designed for specific tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
However, the imposition of privacy and data-sharing policies like Global Data Protection Regulations (GDPR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> the traditional centralized method of transferring data to the server could present more challenges. In addition to this, other computational challenges are present with this traditional approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. An alternative solution for creating reliable ML models for tasks needs to be considered.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) is an innovative way to implement ML algorithms and models over decentralized data. First introduced by the research team at Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. FL attempts to answer the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">can we train machine learning models without needing to transfer user data onto a central server?</span> Since its’ introduction, there has been a growing interest in the research community to explore the opportunities and capabilities of FL. The technology enables a more collaborative approach of ML whilst also preserving user privacy by having the data decentralized over the device itself rather than have it over a central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This method of collaborative learning can be explained by using data generated from hospitals as an example. Each hospital generates data from its’ smart devices and equipment, using this data could be useful to create an ML model for a specific task.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, data from a single hospital may not generate enough data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Limitation of data could limit the overall knowledge and performance of the ML model. To create a more robust model that would be able to obtain a higher prediction accuracy access to a larger data set would be better. Other hospitals are also generating their data, however, due to data-sharing policies, it would not be possible to have access to that data. This is where FL and its collaborative nature can thrive where each hospital can train its’ own independent ML model that gets uploaded to a central server where all models average out as a global ML model for the specific task.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This is just one example of the capability of FL is being further researched by a range of industries to maximize this decentralized approach. Industries and applications such as transportation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, autonomous vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and a range of other Internet of Things (IoT) applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Its implementation can be seen on the mobile application Gboard by Google for predictive texting; the <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">FederatedAveraging</span> algorithm to train the ML model over the on-device decentralized data available on mobile devices can improve predictive texting results whilst also reducing the privacy and security risks of the sensitive user data compared to a central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2107.10996/assets/x1.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>How FL can be used to improve predictive healthcare ML using sensitive data across multiple hospitals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In addition to preserving data privacy and decentralizing the data, theoretically, the computational power is also split amongst parties that are within the federated network using the decentralized algorithms. Rather than having to rely on the centralized server for training the ML process, the training process can take place on end devices directly.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">FL and more of its capabilities are still being discovered, though, as with the introduction of any new technology the more benefits that are reaped of its capabilities the more challenges are exposed. Concerning FL, there are still some privacy, security, communication, and algorithmic challenges that still exist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In this survey paper, we solely discuss the challenges that are present in <span id="S1.p8.1.1" class="ltx_text ltx_font_bold">Communication</span> concerning FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The challenges that arise in an FL setting can come from multiple sources creating a bottleneck. Factors such as end-user network connections that operate at substantially lower rates when compared to network connections that are available at a data center. This form of unreliability could be potentially expensive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Therefore there has been some research done towards making the Communication factor of an FL environment more efficient. Quantization and Sparsification are two model compression techniques that can be integrated with the FL averaging algorithms to make communication more efficient, along with other techniques that are further discussed in this paper. There has been some research towards this but very limited surveys are found on the topic, therefore, this survey paper aims to present itself by providing a summary of recent research done on the subject so further researchers can benefit from it.
Here:
The remainder of the paper is structured as follows. Section <a href="#S2" title="II Problem Statement ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides an introduction to FL system and how it functions. Section <a href="#S3" title="III Research Questions &amp; Communication Efficient methods ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> introduces and answers the Research Questions (RSQs). Sections <a href="#S4" title="IV Discussion ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and <a href="#S5" title="V Conclusion &amp; Future Expectations ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> review the papers where we share our thoughts and navigate towards sharing the future expectations towards this topic of reasearch.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Problem Statement</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Background</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Federated learning was first introduced by the research team at Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, the team at Google had the motive to create machine learning models that would be applicable for the wealth of data that is available on mobile devices.
Federated Learning was introduced so users could retain hold on to their data and retain privacy. In FL the ML models can be directly trained on the device itself.
As the data can be sensitive and also large in quantity in many cases the approach of bringing the model to the data could suit better. This decentralized approach integrated with collaborative learning was given the term Federated Learning.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In this subsection, a general overview and the main components/entities that are available in an FL environment are introduced. Additionally, we will discuss the FL processes from the communication angel.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">The components of FL systems</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">FL system is a learning process where users/participants can collaboratively train the ML model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. As described by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> there are two main entities or components that are present in an FL environment (i) The data owners, or participants and (ii) The central server. The data owners produce the data on their end devices (e.g. mobile phones), this data generated by the owners is kept private to them and does not leave the device therefore each participant or device has their private dataset. The central server is where the global model stays. It is where the original model is stored and shared across with all the participants that are connected across the FL environment. This model is then a local model for each device i.e. being trained on that independent device’s dataset. Once all the device training is complete and model parameters are obtained each device uploads its local model back to the central server. Here at the central is also where each updated local model is uploaded by the participating device and the overall model aggregation takes place by applying aggregation algorithms takes place to generate a new global model.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.5.1.1" class="ltx_text">II-B</span>1 </span>The processes of an FL system</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">FL allows a promising approach towards collaborative machine learning whilst preserving privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The process of an FL has communication between the two main entities. The two entities, the participating devices and the central server communicate first when devices download a global model from the central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Each device trains the model based on their private dataset and improves the global model, the computation takes place directly on end devices. Each device then uploads its retrained version of the global model to the central server where an aggregation algorithm merges the model parameters from each participating device into one new generated global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2107.10996/assets/x2.jpg" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="178" height="107" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A general overview of how FL works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.5.1.1" class="ltx_text">II-B</span>2 </span>The different types of FL systems</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The base concept of FL is to be able to utilize data that is shared across multiple devices and domains. There are a few ways this data can be distributed across devices, ways such as partitioned by examples or partitioned by features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The categorization of this data can be a prerequisite step for building an FL environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Some characteristics of data distribution are factors such as heterogeneous data and the clients’ participation. In this subsection, a brief introduction is done about the few FL settings that can be applied based on the distribution of data and other characteristics.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">There are a few types of FL systems, based on the way the data is distributed across the environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, they can be categorized as:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Horizontal Federated Learning:</span> This type of FL system is when the data from various devices have a similar set of features in terms of the domain but with different instances. This is the original sense of FL learning where data from each domain is homogeneous, see in figure <a href="#S2.F3" title="Figure 3 ‣ 1st item ‣ II-B2 The different types of FL systems ‣ II-B The components of FL systems ‣ II Problem Statement ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and contributing together to train the global ML model together <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. This can be explained using the original example that is presented by Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> wherein the global model is an aggregate of locally trained multiple participating devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2107.10996/assets/x3.jpg" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="150" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Horizontal FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</figcaption>
</figure>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Vertical Federated Learning:</span> The data that is distributed across in a Vertical FL setting is data that is common between unrelated domains. This could perhaps be called a feature-based learning scheme as the datasets involved in the training process perhaps share the same sample ID space but may differ in feature space. An example could be where a bank and an e-commerce business in the same city have some form of the mutual user base, shown in figure <a href="#S2.F4" title="Figure 4 ‣ 2nd item ‣ II-B2 The different types of FL systems ‣ II-B The components of FL systems ‣ II Problem Statement ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The bank has user-sensitive data such as credit card rating, or revenue. Whereas, the e-commerce business has a purchase and browsing history. Here, two different domains can use their data to maybe create a prediction model based on the user and product information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2107.10996/assets/x4.jpg" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="155" height="115" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Vertical FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</figcaption>
</figure>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Federated Transfer Learning: </span> This type of system is different from the aforementioned systems where neither the samples nor the features have many similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. An example could be where two data sources such as a bank in the United States and an e-commerce business in Canada are restricted by geography but still have a small intersection with each other being different institutions similar to a vertical FL. However, this is just how the data is partitioned by the ML model is similar to the traditional ML method of transfer learning where the ML model is a pre-trained model on a similar dataset is used. This method can provide better results in some cases compared to a newly built ML model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, this is further shown in figure <a href="#S2.F5" title="Figure 5 ‣ II-B2 The different types of FL systems ‣ II-B The components of FL systems ‣ II Problem Statement ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2107.10996/assets/x5.jpg" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="277" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Federated Transfer Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Publication Analysis</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Over the recent years, Federated Learning has become a driving research topic for many researchers. Especially those who are interested in decentralized Machine Learning techniques and adhering to the privacy policies that have been imposed. The catalog of research can often times be presented in the form of survey papers. In this subsection we review and introduce the multiple survey papers that have been conducted recently and made available for this domain. Additionally, we highlight our contribution and how our work is different from others.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> provide a comprehensive and detailed survey on FL. This research done by authors is quite thorough with introducing FL and the different types of FL systems that exist. The authors provide a study with a focus on the software that allows FL to happen, the hardware that platforms, the protocol, and the possible applications for the emerging technology.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Similarly the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> introduce the implications and the practically of FL in healthcare providing general solutions. The authors discuss and introduce some open questions towards data quality, model precision, integrating expert knowledge, etc.
A survey that presents some challenges that can be present in the integration of FL is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The authors discuss on Mobile Edge Computing (MEC) and the implementation of FL and how it could act as a dependable technology for the collaborative machine learning approach over edge networks.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> conduct a survey on federated learning and the potential threats that might be available to this approach. As with any new discovery of research and advancements in technology there are always vulnerabilities and potential security threats that can provide a lack of robustness. The survey paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> introduces the risk of privacy and how that could potentially be exploited in a FL setting. Furthering, the paper the authors present a brief review on how Poisoning Attacks can interfere and manipulate the outcome of a FL training model, attacks such as data poisoning and model poisoning. Similarly how inference attacks which can also lead towards privacy leakage. Furthering this topic of research, another survey conducted by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> expands more on the threats towards FL as a survey by creating Research Questions with regards to security and privacy and answering them by providing a comprehensive list of threats and attacks, the authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> also surface some unique security threats exist in the FL environment and what defensive techniques can be implemented towards such vulnerabilities. Defense techniques such as proactive defenses that is a way to guess the threats and the potential risks associated with it. The authors present solutions like Sniper, Knowledge distillation, anomaly detection, and moving target defense.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">There are more surveys that are conducted on the topic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and even though they introduce the topic of communication and the challenges that may be present. There has not been a dedicated research or survey paper that is written towards the challenges that are present in communication and ways to make it efficient. This paper aims to bridge that gap by solely focusing on communication.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Publication Analysis of survey papers over the past couple of years.</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:74.6pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-259.0pt,44.3pt) scale(0.455641964695304,0.455641964695304) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Reference</span></th>
<th id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Objective</span></th>
<th id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Security</span></th>
<th id="S2.T1.1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Privacy</span></th>
<th id="S2.T1.1.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Communication</span></th>
<th id="S2.T1.1.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Challenges</span></th>
<th id="S2.T1.1.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.1.8.1" class="ltx_text ltx_font_bold">Future Direction</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></th>
<th id="S2.T1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S2.T1.1.1.2.1.2.1" class="ltx_text">2019</span></th>
<td id="S2.T1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">A comprehensive review of the FL systems, touching and introducing range of FL components.</td>
<td id="S2.T1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.2.1.8" class="ltx_td ltx_align_left ltx_border_t">✓</td>
</tr>
<tr id="S2.T1.1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></th>
<td id="S2.T1.1.1.3.2.2" class="ltx_td ltx_align_left">Providing a survey on existing works on FL discussing frameworks, concepts and applications.</td>
<td id="S2.T1.1.1.3.2.3" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.3.2.4" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.3.2.5" class="ltx_td ltx_align_left">✗</td>
<td id="S2.T1.1.1.3.2.6" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.3.2.7" class="ltx_td ltx_align_left">✓</td>
</tr>
<tr id="S2.T1.1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></th>
<th id="S2.T1.1.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S2.T1.1.1.4.3.2.1" class="ltx_text">2020</span></th>
<td id="S2.T1.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">Introduction of concept of FL, covering threat model attacks predominantly.</td>
<td id="S2.T1.1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.4.3.5" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.4.3.6" class="ltx_td ltx_align_left ltx_border_t">✗</td>
<td id="S2.T1.1.1.4.3.7" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.4.3.8" class="ltx_td ltx_align_left ltx_border_t">✓</td>
</tr>
<tr id="S2.T1.1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></th>
<td id="S2.T1.1.1.5.4.2" class="ltx_td ltx_align_left">A FL survey focusing on hardware, software and technologies and real-life applications</td>
<td id="S2.T1.1.1.5.4.3" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.5.4.4" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.5.4.5" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.5.4.6" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.5.4.7" class="ltx_td ltx_align_left">✓</td>
</tr>
<tr id="S2.T1.1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></th>
<td id="S2.T1.1.1.6.5.2" class="ltx_td ltx_align_left">Introduction of FL presenting some existing challenges and their solutions</td>
<td id="S2.T1.1.1.6.5.3" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.6.5.4" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.6.5.5" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.6.5.6" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.6.5.7" class="ltx_td ltx_align_left">✓</td>
</tr>
<tr id="S2.T1.1.1.7.6" class="ltx_tr">
<th id="S2.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></th>
<td id="S2.T1.1.1.7.6.2" class="ltx_td ltx_align_left">A detailed survey introducing FL and the challenges.</td>
<td id="S2.T1.1.1.7.6.3" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.7.6.4" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.7.6.5" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.7.6.6" class="ltx_td ltx_align_left">✓</td>
<td id="S2.T1.1.1.7.6.7" class="ltx_td ltx_align_left">✓</td>
</tr>
<tr id="S2.T1.1.1.8.7" class="ltx_tr">
<th id="S2.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></th>
<th id="S2.T1.1.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="2"><span id="S2.T1.1.1.8.7.2.1" class="ltx_text">2021</span></th>
<td id="S2.T1.1.1.8.7.3" class="ltx_td ltx_align_left ltx_border_t">A survey of FL in healthcare, covering common topics of introduction of technology, challenges, etc.</td>
<td id="S2.T1.1.1.8.7.4" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.8.7.5" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.8.7.6" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.8.7.7" class="ltx_td ltx_align_left ltx_border_t">✓</td>
<td id="S2.T1.1.1.8.7.8" class="ltx_td ltx_align_left ltx_border_t">✓</td>
</tr>
<tr id="S2.T1.1.1.9.8" class="ltx_tr">
<th id="S2.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></th>
<td id="S2.T1.1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_bb">A comprehensive survey posing Research Questions with regards to FL and privacy and security</td>
<td id="S2.T1.1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_bb">✓</td>
<td id="S2.T1.1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_bb">✓</td>
<td id="S2.T1.1.1.9.8.5" class="ltx_td ltx_align_left ltx_border_bb">✓</td>
<td id="S2.T1.1.1.9.8.6" class="ltx_td ltx_align_left ltx_border_bb">✓</td>
<td id="S2.T1.1.1.9.8.7" class="ltx_td ltx_align_left ltx_border_bb">✓</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">A list of recent surveys over the past few years are mentioned in table <a href="#S2.T1" title="TABLE I ‣ II-C Publication Analysis ‣ II Problem Statement ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. There is a common theme with most survey papers with survey papers with introducing the technology, presenting the applications and addressing the security and privacy benefits and concerns. In most papers the topic of communication is introduced too, though, it is only a brief surface level part of the paper. This survey paper aims to be the bridge in that gap and present a survey paper that focuses solely on the communication component for FL.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Research Questions &amp; Communication Efficient methods</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Communication between the participating devices and the central server is an essential step for an FL environment. The model is communicated over rounds of communication that download, upload and train the ML model. However, as mentioned, this comes with its’ challenges. To better gain more insight into the challenges that are present in an FL environment with regards to communication we pose two Research Questions (RSQs). The first <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">RSQ1</span> aims to provide a brief analysis of <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">What are some of the challenges that are presented in FL with regards to communication?</span> The second <span id="S3.p1.1.3" class="ltx_text ltx_font_bold">RSQ2</span> provides an analysis in-depth about various methods and approaches that can be implemented to answer <span id="S3.p1.1.4" class="ltx_text ltx_font_italic">How can we make communication more efficient in an FL environment?</span></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">RSQ1 - What are some of the challenges that are presented in FL with regards to communication?</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As aforementioned in Section <a href="#S2" title="II Problem Statement ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> about the general working of the FL environment the central server shares the global model across the network to all the devices that are participating. The total number of devices that are participating in this FL environment can sometimes be in millions and the bandwidth over which the devices are connected to the environment could be relatively slow or unstable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. In an FL training environment, there can be many rounds of communication that exist between the central server and all the participating devices. Over a singular communication round the global model is shared across all the devices in the FL environment and each participating device downloads the global model to train it on their local dataset. A version of that is uploaded back to the environment to the central server. Therefore there is a constant downlink and uplink during the communication rounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Though due to limited bandwidth, energy and power on the device end these rounds of communication can be slow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Even with these challenges, the overall communication cost of sharing model updates is relatively lower than sharing copious amounts of data from the devices to a central server; it is still important to preserve the communication bandwidth further to make it more efficient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. In this subsection, we introduce the overheads <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> or challenges that could create a communication bottleneck in an FL environment.

<br class="ltx_break"></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Number of Participating Devices:</span> Though having a high number of participating devices in an FL environment has its’ advantage wherein the ML model could be trained on more data and a possible increase in performance and accuracy. However, the large number of devices that are participating in multiple FL training rounds at the same time could create a communication bottleneck too. In some cases, a high number of clients could lead to an increase in the overall computational cost too <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Network Bandwidth:</span> Though in contrast to the traditional ML approach, the FL approach does reduce the cost substantially, however, the communication bandwidth still needs to be preserved <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The participating devices may not always have the bandwidth needed and could be participating under unreliable network conditions. Factors such as having a difference between upload speed and download speed that could result in delays such model uploads by participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to the central server which could lead to potential bottleneck leading to disrupting the FL environment.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Limited Edge Node Computation:</span> The computation is now dependant on edge devices rather than powerful GPUs and CPUs. The edge devices could have limitations towards computation, power resources, storage and as aforementioned limited link bandwidth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> give a comparison in training time between a central server and an edge device. They elaborate that an Image Classification model with over 60 million parameters can be trained in just a few minutes over a GPU reaching speeds of 56Gbps. However, even with a powerful smartphone connected over 5G it could take much longer reaching an average speed of 50Mbps.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Statistical Heterogeneity:</span> Another possible source for a communication bottleneck or where communication costs can rise could be statistical heterogeneity where the data is non independent and identically (non-i.i.d.) distributed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. In a FL environment, the data is only locally present on each participating device. It is gathered and collected by the participants on their independent devices based on their usage pattern and local environment. An individual participant’s dataset in an FL environment could not be representative of the population distribution of the other participants and their datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The size of data gathered and distributed amongst devices can typically vary heavily <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Therefore, this type of fluctuation in the size of the dataset could affect communication by causing a delay in model updates and other attributes. A device with a larger dataset could take longer to update, whereas a device with a smaller dataset could be done with updates. However, the global model might not be aggregated until all individual client models are trained and uploaded causing a bottleneck.

<br class="ltx_break"></p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The aforementioned constraints listed are just some of the discovered challenges that could be possible sources towards creating a communication bottleneck.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">RSQ2 - How can we make communication more efficient in an FL environment?</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To train the global ML model with decentralized data, the global model needs to be downloaded on the participating devices on that federated network. This allows the data generated by those remote devices to remain preserved on the device whilst subsequently also making it possible to improve the ML model. The steps that make this possible can be summarized into three communication steps (1) the global model needs to be shared across devices within the federated network, these can sometimes be millions of IoT devices of mobiles phone (2) The model is then downloaded by the devices and trained locally on-device on the private dataset that is generated by those devices (3) the ML model is uploaded back to the central server where it is pooled with numerous other models that have been uploaded to aggregate them all together and find a federated average to generate a new and updated global model. Considering the steps of communication that part-take to obtain an improvement in the global ML model it is important to seek out the most communication-efficient methods that could make the transfer of data from (1) to (2) and onto (3).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In this subsection, we present the recent findings and efforts that are made by the research community to improve the communication of the ML model over the federated network. Findings that are tackling constraints where devices can drop out due to poor or limited network bandwidth. In addition to this, other constraints too where data is sampled differently and how these could affect communication.
We list some research towards discovering different methods that could help with making communication more efficient. Methods such as Local Updating, Client Selection, Reduction in Model Updates, Decentralized Training &amp; Peer-to-Peer Learning, and various Compression Schemes.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2107.10996/assets/x6.jpg" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> illustrate a complete round of distributed Stochastic Gradient Descent (SGD) model. In (a) clients on the federated network synchronize with the server and download the global ML model. In (b) the global model is trained by each client on their local data, adjusting the weighted average of the ML model as per. In (c) the new average achieved by each client is updated onto the server where each update is used to obtain a new weighted average for the global model.</figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.5.1.1" class="ltx_text">III-B</span>1 </span>Local Updating</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Distributed ML that exists in data centers have become popular with integrating mini-batch optimization methods, however, there are still some constraints and limitation on flexibility. This form of batch optimization has constraints over a more federated setting that includes both communication and computation challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Though the overall objective of local updating is mostly focusing on the aforementioned point (2) i.e. to fit and train the ML model locally on-device using the data that is generated by those devices, however, having to compute locally over a singular communication round and then applying those updates to the central server is not that efficient due unexpected dropouts of devices or synchronization latency due to poor network connectivity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> also highlight that it is very important to have a form of flexibility when considering optimization method. Flexibility towards client participation and local updates. There are a few ways towards achieving this flexibility and in turn improving the overall communication efficiency. Techniques such as primal-dual methods can offer a form of flexibility where local devices can use local parameters to train the global model locally to find arbitrary approximations based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. This leveraging and breaking down the overall objective of the global model can allow for problems or training to be solved in parallel over each communicated round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. In this subsection, we review some of the methods that have come surface with regards to local updating techniques that can make communication more efficient.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">In an FL setting the data can be distributed unevenly across devices and may not always be identical. This can be considered that the distributed data across the FL environment is in a non-independent and identically distributed manner i.e. non-iid. Having the datasets in a non-id state could challenge the testing accuracy of an FL model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. The testing accuracy of a locally trained ML model is important as it will contribute to the global model. Therefore having an accuracy of a local model that performs poorly could also deteriorate the overall performance of the global model.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> the authors introduce their way of combining FL with Hierarchical Clustering (HC) techniques to reduce the overall communication rounds within the FL protocol. The clustering step introduced can cluster and separate the clients by the similarity of the local updates to the global model. These similarities are the weights that are achieved by locally updating the local model on each device. Upon testing and comparing their integrated FL + HC technique the authors concluded the communication rounds are reduced as per a comparative on the Manhattan distance metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">Due to the large number of connected devices that are attempting to communicate their locally updated model parameters with the central server, (2), this could cause a communication bottleneck due to bandwidth in some instances
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> introduce a new way to counter this problem, their FedPAQ framework i.e. Federated Periodic Averaging and Quantization framework tackles this problem with its’ features. Its feature of Periodic Averaging feature, in particular, helps reduce the number of communication rounds. In contrast to other training methods where each participating device sends their ML models to synchronize through the parameter servers over each iteration resulting in increasing the number of communication rounds between the devices and central sever. However, this method of Periodic Averaging method can allow a solution using the Stochastic Gradient Descent (SGD). The parameters and local updates of each device can be synchronized with the central server where a periodic average of models takes place.
This is done by adjusting the parameter that corresponds to the number of iterations that occur locally on the device itself. Other features such as Partial Node Participation and Quantized Message-Passing of the FedPAQ also reduce communication overhead.
The Quantized Message-Passing is further discussed in the Compression Schemes subsection.</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p">The aforementioned FedAvg algorithm is used for its simplicity and in turn, is about to reduce communication costs in an FL environment. The algorithm tackles the challenges presented over communication by performing numerous updates on available devices before communicating with the central server. However, in some cases such as the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> state that over heterogeneous data the FedAVG algorithm could introduce ’drift’ towards the updates of each client or device that is participating. This ’client drift’ if will result in slow and unstable convergence and could persist if all clients participate in training rounds with full batch gradients.
The authors run their analysis and determine that the FedAvg algorithm full batch gradients and matching lower bounds over no client sampling to conclude that it is slower than the Stochastic Gradient Descent (SGD) over some parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. To over this challenge the authors created their framework called Stochastic Controlled Averaging (SCAFFOLD) algorithm that can fix the drift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. For their analysis, the SCAFFOLD algorithm performs well and provides reliable convergence rates as SGD even for non-iid data. It also takes the advantage of the similarity that exists within the clients and reduce communication overload <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></p>
</div>
<div id="S3.SS2.SSS1.p6" class="ltx_para">
<p id="S3.SS2.SSS1.p6.1" class="ltx_p">An approach introduced by authors in the <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> called FedDANE tackles some of the practical constraints that are present with FL. The approach is a culmination of methods introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. FedDANE collects gradient updates from subset of devices during each of the communication rounds. FedDANE works well with low client participation settings too.</p>
</div>
<div id="S3.SS2.SSS1.p7" class="ltx_para">
<p id="S3.SS2.SSS1.p7.1" class="ltx_p">Local updating is an essential part of Federated Learning. Each device needs to compute local updates so a better overall global model can be generated and all participating devices can benefit from it. Making the process of local updating more efficient could indeed make communication more efficient.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.5.1.1" class="ltx_text">III-B</span>2 </span>Client Selection</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Client Selection is an approach that can be implemented to make communication more efficient by reducing the costs by restricting the number of participating devices so only a fraction of the parameters is updated over the communication round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. In this subsection, we introduce some of the research that is done with regards to this and how beneficial it could be towards easing communication overload.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">An approach introduced by authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> tackles the limitations that are present in communication over an FL setting. The authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> create a multi-criteria client selection model for IoT devices over an FL setting. Their FedMCCS approach considers all the specifications and network conditions of the IoT device for client selection. Things such as CPU, Memory, Energy and Time are all considered for the client resources to determine whether the client would be able to participate in the FL task. The FedMCCS approach considers more than one client and over each round, the number of clients participating is increased. From their analysis FedMCCS in comparison to other approaches can outperform by reducing the total number of communication rounds to achieve reasonable accuracy.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">Another factor that is a vital property of FL according to authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> is the varying significance of learning rounds. This comes to the surface when the authors realize that the learning rounds are temporally interdependent but also have varying significance towards achieving the final learning outcome. This conclusion comes from running numerous data-driven experiments, the authors create an algorithm that utilizes the wireless channel information but can achieve long-term performance guarantee; the algorithm results in providing a desired client selection pattern that is adapted to network environments.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> introduced a framework that specifically tackles the challenges when it comes to client selection. The authors called their framework FedCS. Their goal is that when it comes to Client Selection in a standard FL setting it can sometimes be a random selection of clients (or devices), however, with their approach of Client Selection they break into two steps. First, Resource Request, where client information such as state of the wireless channel and computational capacities, etc are requested and shared with the central server. Second, to estimate the time required Distribution and Scheduled Update and Upload steps are taken. With their framework, the overall approach of the Client Selection is to allow the server to aggregate as many clients within a certain time frame. Knowing these factors such as data, energy, and computational resources i.e. used by the devices could better meet the requirements of a training task and possibly affect energy consumption and bandwidth cost.</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<p id="S3.SS2.SSS2.p5.1" class="ltx_p">To better assess whether a client can sufficiently participate or not a resource management algorithm introduced by authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> adopts a deep-Q learning algorithm that could allow the servers to learn and make optimal decisions without having to know prior network knowledge. Their Mobile Crowd Machine Learning (MCML) algorithm addresses the constraints that are present in mobile devices reducing the energy consumed along with making the training and communication time more efficient too.</p>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para">
<p id="S3.SS2.SSS2.p6.1" class="ltx_p">Authors run their analysis providing a method that practices biased client selection strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. As per their analysis, the selection of bias can affect the convergence speed, the bias of their client selection works towards clients that have a higher local loss and are achieving faster error convergence. With this knowledge, the authors a framework that is a communication and computation efficient framework. Calling it Power-Of-Choice. Their framework has the agility to trade-off between the bias and the convergence speed. The results the authors achieve are computed relatively faster and provide higher accuracy of results than the baseline random selection methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS2.p7" class="ltx_para">
<p id="S3.SS2.SSS2.p7.1" class="ltx_p">Uplink and downlink communication with participating clients in a federated network is necessary. Having to communicate with clients that have dependable network bandwidth and energy sources could aid towards achieve a well-trained global model more efficiently. Client selection techniques implemented can aid in reducing the cost of overall communication that is needed to achieve a dependable global model.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.5.1.1" class="ltx_text">III-B</span>3 </span>Reducing Model Updates</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Once the global model is downloaded by connected devices in the FL environment each device starts to train the devices locally. As each device computes and updates the model these updates are communicated back to the central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The number of communication rounds between the devices and central server can be costly and perhaps having fewer but more efficient model updates could be a solution. In this section, we introduce some techniques that discuss a possible reduction in communication for these model updates and potentially reduce the cost.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> introduce an efficient way of training models, their proposed approach adapts to the concept of drifts trains models equally well through different phases of model training. Their approach leads to reducing the communication substantially without depreciating the predictive performance of the model.</p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">In another study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, authors introduce a Partitioned Variational Inference (PVI) for probabilistic models that work well over federated data. They train a Bayesian Neural Network (BNN) over an FL environment that is allowed for both synchronous or asynchronous model updates across many machines, their proposed approach along with the integration of other methods could allow a more communication-efficient training of BNN on non-iid federated data.</p>
</div>
<div id="S3.SS2.SSS3.p4" class="ltx_para">
<p id="S3.SS2.SSS3.p4.1" class="ltx_p">In contrast to most of the current FL methods that include iterative optimization techniques over numerous communication rounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> introduce a one-shot communication round approach where only a single round of communication is done between the central server and the number of connected devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. The authors do suggest as opposed to computing increments, each device trains a local model to completion and then applies ensemble methods to effectively capture information regarding device-specific models. Applying ensemble learning techniques could be better suited for global modelling than averaging techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS3.p5" class="ltx_para">
<p id="S3.SS2.SSS3.p5.1" class="ltx_p">In an FL setting the rate at which model convergence occurs can sometimes take a large number of communication rounds creating a delay towards model training whilst simultaneously increasing network resources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. An intelligent algorithm called FOLB is introduced by authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. The algorithm performs smart sampling of participating devices over each round to optimize the expected convergence speed. The algorithm can estimate the participating device’s capabilities, this is done by adapting to devices aggregations.</p>
</div>
<div id="S3.SS2.SSS3.p6" class="ltx_para">
<p id="S3.SS2.SSS3.p6.1" class="ltx_p">If the mere frequency in which the model updates are shared is reduced that means the overall communication between the devices and the central server is reduced too. Having an effective way of determining how these updates are computed and reducing the overall communication rounds that are needed. Fewer rounds could result in more efficiency.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS4.5.1.1" class="ltx_text">III-B</span>4 </span>Decentralized Training &amp; Peer-to-Peer Learning</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">FL in a way is like practicing ML in a decentralized manner. However, FL does allow a more peer-to-peer learning approach wherein each node that is trained can benefit from the other node that is trained on the FL network too. Even in decentralized training similar challenges of communication exist and to tackle it different methods like compression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> can be used. In this subsection how decentralized or peer-to-peer learning is utilized or integrated into an FL environment.</p>
</div>
<div id="S3.SS2.SSS4.p2" class="ltx_para">
<p id="S3.SS2.SSS4.p2.1" class="ltx_p">In an FL environment, as aforementioned, there is a central server that has the original model. The central server is where all the devices that are connected to the network update their model too. Essentially, all devices that are participating in the FL environment are connected to the central server. As stated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> the star network is the predominant communication topology in an FL setting shown in Figure <a href="#S3.F7" title="Figure 7 ‣ III-B4 Decentralized Training &amp; Peer-to-Peer Learning ‣ III-B RSQ2 - How can we make communication more efficient in an FL environment? ‣ III Research Questions &amp; Communication Efficient methods ‣ Communication Efficiency in Federated Learning: Achievements and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
However, there is some research on whether a different communication topology where the participating devices only communicate with one another. A Peer-to-Peer learning experience or decentralized training network, and whether this would be a more efficient way to communicate in an FL environment.
Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> state that in traditional data center environments decentralized training can appear to be faster than centralized training especially when constraints of low bandwidth and high latency are faced when operating on networks. Though this is not to say that decentralized training does not have its’ constraints authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> state that nodes computation time on nodes can slow down the convergence of a decentralized algorithm, in addition to this, the sometimes, large communication overhead could inalso further mitigate this. To overcome these, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> proposed QuanTimed-DSGD algorithm, a decentralized and gradient-based optimization, that imposes iteration deadlines for nodes and nodes exchange their quantized version of the models.</p>
</div>
<div id="S3.SS2.SSS4.p3" class="ltx_para">
<p id="S3.SS2.SSS4.p3.1" class="ltx_p">In the FL environment, peer-to-peer learning could be where each device only communicates with its neighbours and updates. The participating devices or clients updates their model on their dataset, and aggregates it along with the model updates from their neighbours <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. Furthering that, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> build a framework for an FL environment towards a generic social network scenario. Their Online Push-Sum (OPS) method handles the complex topology whilst simultaneously having optimal convergence rates.</p>
</div>
<div id="S3.SS2.SSS4.p4" class="ltx_para">
<p id="S3.SS2.SSS4.p4.1" class="ltx_p">Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> propose a distributed learning algorithm where there is no central server but instead, the participating devices practice peer-to-peer learning algorithm to iterate and aggregate model information with their neighbour to collaboratively estimate the global model parameters. The authors base an assumption suggesting that FL setting wherein a central server exists and communicates the global model can incur large communication costs. In their approach, the devices are already distributed over the network where communication occurs only with their one-hop neighbours.</p>
</div>
<div id="S3.SS2.SSS4.p5" class="ltx_para">
<p id="S3.SS2.SSS4.p5.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> that further provides another avenue for peer-to-peer learning and not depending on the central server as a single trusting authority. The authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> create their framework BrainTorrent that does not rely on a central server for training, their proposed peer-to-peer framework can is designed to motivate a collaborative environment. According to their analysis, the absence of a central server makes the environment resistant to failure but also precludes the need for a governing central body that every participant trusts.</p>
</div>
<div id="S3.SS2.SSS4.p6" class="ltx_para">
<p id="S3.SS2.SSS4.p6.1" class="ltx_p">Theoretically, the application of decentralized training in an FL setting could reduce the communication cost when compared with a central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> though there more research done on this and there are further avenues it can take too.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2107.10996/assets/x7.jpg" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="217" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>An overview of star-network topology. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Research done towards reducing the overall Communication costs and overheads in a FL.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:340.9pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-140.4pt,110.2pt) scale(0.60692023601737,0.60692023601737) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Reference</span></th>
<td id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Section</span></td>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Model and Technology</span></td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Remarks</span></td>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<th id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite></th>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="3">
<span id="S3.T2.1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.2.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T2.1.1.2.2.2.1.1.1" class="ltx_text">Local Updating</span></span>
</span>
</td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.2.3.1.1" class="ltx_p" style="width:113.8pt;">Hierarchical Clustering Technique</span>
</span>
</td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.2.4.1.1" class="ltx_p" style="width:341.4pt;">A FL+HC technique that separates client clusters similarity of local updates</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<th id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></th>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.3.3.2.1.1" class="ltx_p" style="width:113.8pt;">FedPAQ</span>
</span>
</td>
<td id="S3.T2.1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.3.3.3.1.1" class="ltx_p" style="width:341.4pt;">Using periodic averaging to aggregate and achieve global model updates</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.4.4" class="ltx_tr">
<th id="S3.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></th>
<td id="S3.T2.1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.4.4.2.1.1" class="ltx_p" style="width:113.8pt;">SCAFFOLD Algorithm</span>
</span>
</td>
<td id="S3.T2.1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.4.4.3.1.1" class="ltx_p" style="width:341.4pt;">An algorithm that provides better convergence rates over non-iid data</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.5.5" class="ltx_tr">
<th id="S3.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite></th>
<td id="S3.T2.1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="5">
<span id="S3.T2.1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.5.5.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T2.1.1.5.5.2.1.1.1" class="ltx_text">Compression Schemes - Sparsification</span></span>
</span>
</td>
<td id="S3.T2.1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.5.5.3.1.1" class="ltx_p" style="width:113.8pt;">STC method</span>
</span>
</td>
<td id="S3.T2.1.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.5.5.4.1.1" class="ltx_p" style="width:341.4pt;">Providing compression for both upstream and downstream communications</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.6.6" class="ltx_tr">
<th id="S3.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite></th>
<td id="S3.T2.1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.6.6.2.1.1" class="ltx_p" style="width:113.8pt;">FetchSGD</span>
</span>
</td>
<td id="S3.T2.1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.6.6.3.1.1" class="ltx_p" style="width:341.4pt;">Compresses the gradient based off of client’s local data</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.7.7" class="ltx_tr">
<th id="S3.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite></th>
<td id="S3.T2.1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.7.7.2.1.1" class="ltx_p" style="width:113.8pt;">General Gradient Sparsification (GSS)</span>
</span>
</td>
<td id="S3.T2.1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.7.7.3.1.1" class="ltx_p" style="width:341.4pt;">The batch normalization layer with local gradients achieved from GSS could mitigate the impact of delayed gradients and not increase the communication overhead</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.8.8" class="ltx_tr">
<th id="S3.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite></th>
<td id="S3.T2.1.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.8.8.2.1.1" class="ltx_p" style="width:113.8pt;">CPFed</span>
</span>
</td>
<td id="S3.T2.1.1.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.8.8.3.1.1" class="ltx_p" style="width:341.4pt;">A sparsified masking model that can provide compression and differential privacy</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.9.9" class="ltx_tr">
<th id="S3.T2.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite></th>
<td id="S3.T2.1.1.9.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.9.9.2.1.1" class="ltx_p" style="width:113.8pt;">Sparse Binary Compression (SBS)</span>
</span>
</td>
<td id="S3.T2.1.1.9.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.9.9.3.1.1" class="ltx_p" style="width:341.4pt;">Introducing temporal sparsity where gradients are not communicated after every local iteration</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.10.10" class="ltx_tr">
<th id="S3.T2.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite></th>
<td id="S3.T2.1.1.10.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="5">
<span id="S3.T2.1.1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.10.10.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T2.1.1.10.10.2.1.1.1" class="ltx_text">Compression Schemes - Quantization</span></span>
</span>
</td>
<td id="S3.T2.1.1.10.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.10.10.3.1.1" class="ltx_p" style="width:113.8pt;">FedPAQ</span>
</span>
</td>
<td id="S3.T2.1.1.10.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.10.10.4.1.1" class="ltx_p" style="width:341.4pt;">Using quantization techniques based upon model accuracy</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.11.11" class="ltx_tr">
<th id="S3.T2.1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite></th>
<td id="S3.T2.1.1.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.11.11.2.1.1" class="ltx_p" style="width:113.8pt;">Lossy FL algorithm (LFL)</span>
</span>
</td>
<td id="S3.T2.1.1.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.11.11.3.1.1" class="ltx_p" style="width:341.4pt;">Quantizing models before broadcasting</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.12.12" class="ltx_tr">
<th id="S3.T2.1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite></th>
<td id="S3.T2.1.1.12.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.12.12.2.1.1" class="ltx_p" style="width:113.8pt;">Hyper-Sphere Quantization (HSQ) framework</span>
</span>
</td>
<td id="S3.T2.1.1.12.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.12.12.3.1.1" class="ltx_p" style="width:341.4pt;">Ability to reduce the cost of communication per iteration</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.13.13" class="ltx_tr">
<th id="S3.T2.1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite></th>
<td id="S3.T2.1.1.13.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.13.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.13.13.2.1.1" class="ltx_p" style="width:113.8pt;">UVeQFed</span>
</span>
</td>
<td id="S3.T2.1.1.13.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.13.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.13.13.3.1.1" class="ltx_p" style="width:341.4pt;">With the algorithm convergence of model minimizes the loss function</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.14.14" class="ltx_tr">
<th id="S3.T2.1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite></th>
<td id="S3.T2.1.1.14.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.14.14.2.1.1" class="ltx_p" style="width:113.8pt;">Heir-Local-QSGD</span>
</span>
</td>
<td id="S3.T2.1.1.14.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.14.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.14.14.3.1.1" class="ltx_p" style="width:341.4pt;">A technique that naturally leverages client-edge-cloud network hierarchy and quantized models updates</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.15.15" class="ltx_tr">
<th id="S3.T2.1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite></th>
<td id="S3.T2.1.1.15.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2">
<span id="S3.T2.1.1.15.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.15.15.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T2.1.1.15.15.2.1.1.1" class="ltx_text">Decentralized Training or Peer-to-peer learning</span></span>
</span>
</td>
<td id="S3.T2.1.1.15.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.15.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.15.15.3.1.1" class="ltx_p" style="width:113.8pt;">BrainTorrent</span>
</span>
</td>
<td id="S3.T2.1.1.15.15.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.15.15.4.1.1" class="ltx_p" style="width:341.4pt;">A peer-to-peer learning framework where models converge faster and reach good accuracy</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.16.16" class="ltx_tr">
<th id="S3.T2.1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite></th>
<td id="S3.T2.1.1.16.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.16.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.16.16.2.1.1" class="ltx_p" style="width:113.8pt;">QuanTimed-DSGD</span>
</span>
</td>
<td id="S3.T2.1.1.16.16.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.16.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.16.16.3.1.1" class="ltx_p" style="width:341.4pt;">decentralized gradient-based optimization imposing iteration deadlines for devices</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.17.17" class="ltx_tr">
<th id="S3.T2.1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite></th>
<td id="S3.T2.1.1.17.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="4">
<span id="S3.T2.1.1.17.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.17.17.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T2.1.1.17.17.2.1.1.1" class="ltx_text">Client Selection</span></span>
</span>
</td>
<td id="S3.T2.1.1.17.17.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.17.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.17.17.3.1.1" class="ltx_p" style="width:113.8pt;">FedMCCS</span>
</span>
</td>
<td id="S3.T2.1.1.17.17.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.17.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.17.17.4.1.1" class="ltx_p" style="width:341.4pt;">A multi-criteria client selection that considers that considers IoT device specification and network condition</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.18.18" class="ltx_tr">
<th id="S3.T2.1.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></th>
<td id="S3.T2.1.1.18.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.18.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.18.18.2.1.1" class="ltx_p" style="width:113.8pt;">Resource Allocation Model</span>
</span>
</td>
<td id="S3.T2.1.1.18.18.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.18.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.18.18.3.1.1" class="ltx_p" style="width:341.4pt;">Optimizing learning performance on how clients are selected and how bandwidth is allocated</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.19.19" class="ltx_tr">
<th id="S3.T2.1.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></th>
<td id="S3.T2.1.1.19.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.19.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.19.19.2.1.1" class="ltx_p" style="width:113.8pt;">FedCS</span>
</span>
</td>
<td id="S3.T2.1.1.19.19.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.19.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.19.19.3.1.1" class="ltx_p" style="width:341.4pt;">The framework allows the server to aggregate as many clients within a certain time-frame</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.20.20" class="ltx_tr">
<th id="S3.T2.1.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite></th>
<td id="S3.T2.1.1.20.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.20.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.20.20.2.1.1" class="ltx_p" style="width:113.8pt;">Power-Of-Choice</span>
</span>
</td>
<td id="S3.T2.1.1.20.20.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.20.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.20.20.3.1.1" class="ltx_p" style="width:341.4pt;">A communication and computation efficient client selection framework</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.21.21" class="ltx_tr">
<th id="S3.T2.1.1.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite></th>
<td id="S3.T2.1.1.21.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="3">
<span id="S3.T2.1.1.21.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.21.21.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T2.1.1.21.21.2.1.1.1" class="ltx_text">Reduced Model Updates</span></span>
</span>
</td>
<td id="S3.T2.1.1.21.21.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.21.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.21.21.3.1.1" class="ltx_p" style="width:113.8pt;">A decentralized deep learning model</span>
</span>
</td>
<td id="S3.T2.1.1.21.21.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.21.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.21.21.4.1.1" class="ltx_p" style="width:341.4pt;">Ability to handle different phases of the model training well</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.22.22" class="ltx_tr">
<th id="S3.T2.1.1.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite></th>
<td id="S3.T2.1.1.22.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.22.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.22.22.2.1.1" class="ltx_p" style="width:113.8pt;">a Partitioned Variational Inference (PVI)</span>
</span>
</td>
<td id="S3.T2.1.1.22.22.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.22.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.22.22.3.1.1" class="ltx_p" style="width:341.4pt;">A Bayesian Neural Network over FL that is synchronous and asynchronous for model updates across machines</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.23.23" class="ltx_tr">
<th id="S3.T2.1.1.23.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite></th>
<td id="S3.T2.1.1.23.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.23.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.23.23.2.1.1" class="ltx_p" style="width:113.8pt;">One-shot federated learning</span>
</span>
</td>
<td id="S3.T2.1.1.23.23.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.23.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.23.23.3.1.1" class="ltx_p" style="width:341.4pt;">A single round of communication done between central server and connected devices</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.24.24" class="ltx_tr">
<th id="S3.T2.1.1.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite></th>
<td id="S3.T2.1.1.24.24.2" class="ltx_td ltx_align_top ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S3.T2.1.1.24.24.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.24.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.24.24.3.1.1" class="ltx_p" style="width:113.8pt;">FOLB</span>
</span>
</td>
<td id="S3.T2.1.1.24.24.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.24.24.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.24.24.4.1.1" class="ltx_p" style="width:341.4pt;">Intelligent sampling of devices in each round of model training to optimize the convergence speed</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS5.5.1.1" class="ltx_text">III-B</span>5 </span>Compression Schemes</h4>

<div id="S3.SS2.SSS5.p1" class="ltx_para">
<p id="S3.SS2.SSS5.p1.1" class="ltx_p">In addition to the local updating methods, some ML compression techniques may also be implemented towards reducing the total number of rounds between the centralized server and the devices that are connected on the federated network. Compression schemes like quantization and sparsification that are sometimes integrated with the aggregation algorithms can sometimes provide reasonable training accuracy results whilst at the same time also reducing the overall communication cost. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> list the objectives set for compression schemes and federated learning as such:</p>
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">a.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Reducing the size of the object / ML model from the clients to the server i.e. used to update the overall global model</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">b.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Reducing the size of the global model that shared with the clients on the network, the model on which the clients start local training using the available data</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">c.</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Any changes that are made to the overall training algorithm that makes training the global training model more computationally efficient</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.SSS5.p2" class="ltx_para">
<p id="S3.SS2.SSS5.p2.1" class="ltx_p">From the list of objectives <span id="S3.SS2.SSS5.p2.1.1" class="ltx_text ltx_font_bold">A</span> could have the highest effect on the overall running time of around therefore in reducing that would directly result in reducing the overall communication cost; the clients on a federated network generally have a slower upload bandwidth compared to a download. Therefore compressing the ML models and potentially reducing the uplink/downlink exchanges could result in reducing the communication cost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS5.p3" class="ltx_para">
<p id="S3.SS2.SSS5.p3.1" class="ltx_p">In this subsection, we review the research that is done by the community towards compression schemes to make communication more efficient in a federated network. We begin by sharing the research that is done towards compression schemes, then we further divide the research concerning methods of sparsification and quantization.</p>
</div>
<div id="S3.SS2.SSS5.p4" class="ltx_para">
<p id="S3.SS2.SSS5.p4.1" class="ltx_p"><span id="S3.SS2.SSS5.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Sparsification:</span> Sparsification techniques is a type of communication technique that can be implemented in an FL setting to compress the model when they are being communicated across the server, in this section, we review the research that has been conducted towards making integrating sparsification in an FL environment and making communication more efficient.</p>
</div>
<div id="S3.SS2.SSS5.p5" class="ltx_para">
<p id="S3.SS2.SSS5.p5.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> present a sparse ternary compression (STC) method that not only adheres to the compression requirements of an FL setting and environment but also provides compression for both upstream and downstream communications. They run an analysis over FL models over various datasets and architectures, in their analysis, they conclude that some factors of an FL environment are hugely dependant on the convergence rate of the averaging algorithm. From their analysis, the authors deduce that factors where training is done on non-iid small portions of data or when only a subset of clients participate in communication rounds can reduce the convergence rate.
However, the proposed model of STC is a protocol that compresses communication via sparsification, ternanization, error accumulation and the optimal Golomb encoding. The robust technique provided by authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> converges relatively faster when compared to other averaging algorithms like FedAVG over both factors of non-iid data and a lesser number of iterations that are communicated. The proposed method is also highly effective when the communication bandwidth is constrained.</p>
</div>
<div id="S3.SS2.SSS5.p6" class="ltx_para">
<p id="S3.SS2.SSS5.p6.1" class="ltx_p">Sparse client participation is another challenge that needs to be overcome in an FL environment, authors introduce a FetchSGD algorithm that can help towards achieving this <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. FetchSGD overall aids with communication constraints of an FL environment by compressing the gradient that is based on the client’s local data. The data structure Count Sketch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> is used to compress the gradient before it is uploaded to the central server. The Count Sketch is also used for error accumulation. According to authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, one key problem with regards to federated setting and communication is the communication overhead that is involved in parameters synchronization. The authors inform that this overhead wastes bandwidth and increases training time whilst impacting the overall model accuracy. To tackle this, the authors propose a General Gradient Sparsification (GGS) framework for adaptive optimizers. The framework consists of two important mechanisms that are batch normalization updates with local gradients (BN-LG), and gradient correction. The authors determine that updating the batch normalization layer with local gradients could mitigate the impact of delayed gradients and not increase the communication overhead. The authors run their analysis over several models such as AlexNet, DenseNet-121, CifarNet, etc, and achieve high accuracy results concluding gradient sparsification does have a significant impact in reducing the communication overhead.
The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> introduce a compression scheme that is both a communication efficient and a deferentially private federated learning scheme. The authors call it CPFed. The challenge the authors face when addressing both issues together is that one that rises from data compression. Techniques used for compression could sometimes lead to an increased number of training iterations required for achieving some desired training loss due to the compression errors, however, differential privacy could deteriorate with regards to the training iterations. To overcome this paradigm, their proposed CPFed is based on a sparsified privacy masking technique that adds random noise to model updates along with an unbiased random sparsifier before updating the model. The authors can achieve high communication efficiency through their proposed model.</p>
</div>
<div id="S3.SS2.SSS5.p7" class="ltx_para">
<p id="S3.SS2.SSS5.p7.1" class="ltx_p">Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> propose a compression technique that could drastically reduce the communication cost for a distributed training environment. The framework introduced by the authors is a Sparse Binary Compression (SBC) technique, their method integrated techniques that are already present in communication delay and gradient sparsification with a novel binarization method. According to their research, the authors find the current gradient information for training neural networks with SGD is redundant. Instead, the authors utilize communication delay methods introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to introduce temporal sparsity where gradients are not communicated after every local iteration. From their findings, the authors conclude that in a distributed SGD setting both communication delay and gradient sparsity can both be treated as independent types of sparsity techniques. These methods provide higher compression gains, though, in their findings the authors also find a slight decrease in Accuracy.</p>
</div>
<div id="S3.SS2.SSS5.p8" class="ltx_para">
<p id="S3.SS2.SSS5.p8.1" class="ltx_p"><span id="S3.SS2.SSS5.p8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Quantization:</span>
The communication between the devices and the central server includes sharing model updates and parameters that have occurred on the device end. This update of parameters can be strenuous when it comes to up-linking the model. To aid with this, another compression method referred to as quantization can bring the model parameters to a reasonable size without compromising much on model accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. In this subsection, we introduce different techniques and frameworks that have integrated quantization towards making communication more efficient for an FL setting.</p>
</div>
<div id="S3.SS2.SSS5.p9" class="ltx_para">
<p id="S3.SS2.SSS5.p9.1" class="ltx_p">The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> utilize their framework FedPAQ to reduce the overall communication rounds and the bearing cost. A feature of their framework is to Quantize Message-Passing. Communication bandwidth will mostly be limited in an FL setting, with limited uplink bandwidth on participating device end could increase the communication cost making it more expensive.
Authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> employ quantization techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> on the up-links; every local model is quantized before being uploaded hence reduce the overall communication overhead.</p>
</div>
<div id="S3.SS2.SSS5.p10" class="ltx_para">
<p id="S3.SS2.SSS5.p10.1" class="ltx_p">Furthering and employing quantization techniques authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> use them for their FL analysis. In contrast to typical FL models where the global model from a central server is downloaded by all devices and then subsequently updated and so and so forth. The authors from their analysis discovered that applying quantization techniques to the global model can help towards making communication more efficient. The Lossy FL algorithm (LFL) created by the authors quantizes the global model before it broadcasts and shares it across with the devices. The local updates that take place on the device are uploaded on the global central server are also quantized. The FL environment is hugely dependant on bandwidth and the authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> for their analysis study how well the quantized compressed global model performs to provide an estimate for the new global model from the local updates of the devices. They compare their results with another analysis of a lossless standard approach of FL to conclude that their LFL and the technique of quantizing global model updates provide a significant reduction in communication load.</p>
</div>
<div id="S3.SS2.SSS5.p11" class="ltx_para">
<p id="S3.SS2.SSS5.p11.1" class="ltx_p">Traditionally, compression algorithms are trained for a setting where there is a high-speed network, places such as data centers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> but in an FL setting these algorithms might not be directly very effective. To further tackle the communication bottleneck that is created due to the network and the other aforementioned reasons another quantization compression technique is proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. To achieve a trade-off between communication efficiency and accuracy authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> propose a Hyper-Sphere Quantization (HSQ) framework. HSQ can reduce communication cost low per-iteration which is ideal for an FL environment, the framework utilizes vector quantization techniques that shows an effective approach towards achieving gradient compression and simultaneously not comprising over on convergence accuracy.</p>
</div>
<div id="S3.SS2.SSS5.p12" class="ltx_para">
<p id="S3.SS2.SSS5.p12.1" class="ltx_p">In another study, the authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> suggest that the communication channel and transfer of model parameters between the users to the central server has a throughput that can be typically constrained. The authors whilst doing their research encountered that alternative methods to aid with this could provide dominant distortion of results. This lead <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> to create a quantization method that is more efficient towards facilitating the model transfer in an FL setting. Utilizing quantization theory methods, the authors design quantizers that are suitable for distributed deep network training. Understanding the requirements that are needed for quantization FL setting, the authors can propose an encoding-decoding strategy. Their proposed scheme shows potential for an FL setting as it performs relatively well compared to previously proposed methods. Furthering their research towards quantization theories the authors in their review approach it from identifying unique characteristics regarding the conveyed trained models over rate-constrained channels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. The authors propose a Universal Vector Quantization technique for FL; a quantization technique that would be suitable for such settings calling it UVeQFed. For their research, the authors demonstrate that combining universal vector quantization methods can yield a system where compression of trained models induces only a minimum distortion. Analyzing the distortion further the authors determine that the distortion reduces substantially as the number of users grows.</p>
</div>
<div id="S3.SS2.SSS5.p13" class="ltx_para">
<p id="S3.SS2.SSS5.p13.1" class="ltx_p">Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> introduce a Hierarchical Quantized Federated Learning technique that can leverage client-edge-cloud network hierarchy and quantized model updates. Their Heir-Local-QSGD algorithm performs partial edge aggregation and quantization on the model updates that can result in improving the communication efficiency in an FL environment.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This paper provides an introduction towards FL whilst retaining focus on the communication component of FL. Communication is quite an essential part of the Federated Learning environment. It is essentially how the global machine learning model is transmitted from the central server to all the participating devices. Similarly, the model is then trained on the local data that is available on the devices, and then uploaded it back to the central server. This constant communication requires a lot of download and upload using a reliable network bandwidth. As it is not always the case, limited bandwidth or poor client participation this can create a communication lag in completing the FL training. Techniques to make the rounds of communication more efficient are shared in this paper. For example a wide range of compression scheme methods mentioned in this paper can help towards reducing the communication overheads and reducing the costs to some factor. Even though the methods presented in this paper do make the communication front more efficient they are all implemented using a range of resources such as different computational power and datasets. There are dependable ways towards which communication rounds can be addressed, however, due to the range of solutions presented it could be concluded that a culmination of techniques implemented together could result in the most communication robust solution for a FL setting.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion &amp; Future Expectations</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This survey paper aimed to provide a bridge between the gap that was present on the topic of communication in FL. This paper presented the challenges and constraints that are present concerning this. Such as bandwidth and limited computation. We posed these challenges in the form of two RSQs that both introduced the problems and provided a list of solutions that have been applied towards making communication more efficient. We believe that communication is an important factor in an FL setting. It is after all how all the training of ML models occur. Despite its’ importance there is still limited research done in comparison to other aspects such as privacy, security and resources. For future expectations we hope to see research on this topic to further grow towards making the efforts making them more efficient.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D. Weichert, P. Link, A. Stoll, S. Rüping, S. Ihlenfeldt, and S. Wrobel,
“A review of machine learning for the optimization of production
processes,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">The International Journal of Advanced Manufacturing
Technology</em>, vol. 104, no. 5, pp. 1889–1902, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Pazzani, “Comprehensible knowledge discovery: gaining insight from data,”
in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">First Federal Data Mining Conference and Exposition</em>.   Citeseer, 1997, pp. 73–82.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
G. Meyer, G. Adomavicius, P. E. Johnson, M. Elidrisi, W. A. Rush, J. M.
Sperl-Hillen, and P. J. O’Connor, “A machine learning approach to improving
dynamic decision making,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Information Systems Research</em>, vol. 25,
no. 2, pp. 239–263, 2014.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. L’heureux, K. Grolinger, H. F. Elyamany, and M. A. Capretz, “Machine
learning with big data: Challenges and approaches,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Ieee Access</em>,
vol. 5, pp. 7776–7797, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. P. Albrecht, “How the gdpr will change the world,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Eur. Data Prot.
L. Rev.</em>, vol. 2, p. 287, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>.   PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Truex, N. Baracaldo, A. Anwar, T. Steinke, H. Ludwig, R. Zhang, and Y. Zhou,
“A hybrid approach to privacy-preserving federated learning,” in
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM Workshop on Artificial Intelligence and
Security</em>, 2019, pp. 1–11.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Aledhari, R. Razzak, R. M. Parizi, and F. Saeed, “Federated learning: A
survey on enabling technologies, protocols, and applications,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE
Access</em>, vol. 8, pp. 140 699–140 725, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Yazdinejad, G. Srivastava, R. M. Parizi, A. Dehghantanha, K.-K. R. Choo, and
M. Aledhari, “Decentralized authentication of distributed patients in
hospital networks using blockchain,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Biomedical and
Health Informatics</em>, vol. 24, no. 8, pp. 2146–2156, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Liu, J. James, J. Kang, D. Niyato, and S. Zhang, “Privacy-preserving
traffic flow prediction: A federated learning approach,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Internet
of Things Journal</em>, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. R. Pokhrel and J. Choi, “Federated learning with blockchain for autonomous
vehicles: Analysis and design challenges,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Communications</em>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Zhao, J. Zhao, L. Jiang, R. Tan, and D. Niyato, “Mobile edge computing,
blockchain and reputation-based crowdsourcing iot federated learning: A
secure, decentralized and privacy-preserving system,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1906.10893</em>, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
V. Mothukuri, P. Khare, R. M. Parizi, S. Pouriyeh, A. Dehghantanha, and
G. Srivastava, “Federated learning-based anomaly detection for iot security
attacks,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, pp. 1–1, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein,
H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for mobile
keyboard prediction,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Chen, R. Mathews, T. Ouyang, and F. Beaufays, “Federated learning of
out-of-vocabulary words,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.10635</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ramage, and
F. Beaufays, “Applied federated learning: Improving google keyboard query
suggestions,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.02903</em>, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays, “Federated learning for
emoji prediction in a mobile keyboard,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1906.04329</em>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Tian Li, “Federated learning: Challenges, methods and future directions,”
<a target="_blank" href="https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/</a>,
2019, Accessed November 13, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Niknam, H. S. Dhillon, and J. H. Reed, “Federated learning for wireless
communications: Motivation, opportunities, and challenges,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE
Communications Magazine</em>, vol. 58, no. 6, pp. 46–51, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advances
and open problems in federated learning,” <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1912.04977</em>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em>, vol. 10, no. 2, pp. 1–19, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks: A
comprehensive survey,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>,
vol. 22, no. 3, pp. 2031–2063, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Xu, N. Baracaldo, Y. Zhou, A. Anwar, and H. Ludwig, “Hybridalpha: An
efficient approach for privacy-preserving federated learning,” in
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM Workshop on Artificial Intelligence and
Security</em>, 2019, pp. 13–23.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Kang, Z. Xiong, D. Niyato, Y. Zou, Y. Zhang, and M. Guizani, “Reliable
federated learning for mobile networks,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Wireless
Communications</em>, vol. 27, no. 2, pp. 72–80, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, vol. 115, pp. 619–640, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. Zhu, H. Zhang, and Y. Jin, “From federated learning to federated neural
architecture search: a survey,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Complex &amp; Intelligent Systems</em>, pp.
1–19, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Z. Tian, R. Zhang, X. Hou, J. Liu, and K. Ren, “Federboost: Private federated
learning for gbdt,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.02796</em>, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Chen, X. Qin, J. Wang, C. Yu, and W. Gao, “Fedhealth: A federated transfer
learning framework for wearable healthcare,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Intelligent
Systems</em>, vol. 35, no. 4, pp. 83–93, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Xu, B. S. Glicksberg, C. Su, P. Walker, J. Bian, and F. Wang, “Federated
learning for healthcare informatics,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Journal of Healthcare
Informatics Research</em>, vol. 5, no. 1, pp. 1–19, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
L. Lyu, H. Yu, and Q. Yang, “Threats to federated learning: A survey,”
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.02133</em>, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing
Magazine</em>, vol. 37, no. 3, pp. 50–60, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Q. Li, Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He, “A survey on
federated learning systems: vision, hype and reality for data privacy and
protection,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.09693</em>, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh,
and D. Bacon, “Federated learning: Strategies for improving communication
efficiency,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Zheng, C. Shen, and X. Chen, “Design and analysis of uplink and downlink
communications for federated learning,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas
in Communications</em>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
W. Luping, W. Wei, and L. Bo, “Cmfl: Mitigating communication overhead for
federated learning,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 39th International Conference on
Distributed Computing Systems (ICDCS)</em>.   IEEE, 2019, pp. 954–964.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y. Chen, X. Sun, and Y. Jin, “Communication-efficient federated deep learning
with layerwise asynchronous model update and temporally weighted
aggregation,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning
systems</em>, vol. 31, no. 10, pp. 4229–4238, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief,
“Communication-efficient edge ai: Algorithms and systems,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE
Communications Surveys &amp; Tutorials</em>, vol. 22, no. 4, pp. 2167–2191, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
“Federated optimization in heterogeneous networks,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1812.06127</em>, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek, “Robust and
communication-efficient federated learning from non-iid data,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE
transactions on neural networks and learning systems</em>, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S. Zhang, A. E. Choromanska, and Y. LeCun, “Deep learning with elastic
averaging sgd,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
2015, pp. 685–693.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated learning
with non-iid data,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>, 2018.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
V. Smith, S. Forte, C. Ma, M. Takáč, M. I. Jordan, and M. Jaggi,
“Cocoa: A general framework for communication-efficient distributed
optimization,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, vol. 18,
no. 1, pp. 8590–8638, 2017.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
C. Briggs, Z. Fan, and P. Andras, “Federated learning with hierarchical
clustering of local updates to improve training on non-iid data,”
<em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.11791</em>, 2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
A. Singh, A. Yadav, and A. Rana, “K-means with three different distance
metrics,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Applications</em>, vol. 67,
no. 10, 2013.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A. Reisizadeh, A. Mokhtari, H. Hassani, A. Jadbabaie, and R. Pedarsani,
“Fedpaq: A communication-efficient federated learning method with periodic
averaging and quantization,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial
Intelligence and Statistics</em>.   PMLR,
2020, pp. 2021–2031.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh,
“Scaffold: Stochastic controlled averaging for federated learning,” in
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2020, pp. 5132–5143.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
O. Shamir, N. Srebro, and T. Zhang, “Communication-efficient distributed
optimization using an approximate newton-type method,” in
<em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2014, pp. 1000–1008.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
S. J. Reddi, J. Konečnỳ, P. Richtárik, B. Póczós, and
A. Smola, “Aide: Fast and communication efficient distributed
optimization,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1608.06879</em>, 2016.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smithy,
“Feddane: A federated newton-type method,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">2019 53rd Asilomar
Conference on Signals, Systems, and Computers</em>.   IEEE, 2019, pp. 1227–1231.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
S. AbdulRahman, H. Tout, A. Mourad, and C. Talhi, “Fedmccs: multicriteria
client selection model for optimal iot federated learning,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IEEE
Internet of Things Journal</em>, vol. 8, no. 6, pp. 4723–4735, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
J. Xu and H. Wang, “Client selection and bandwidth allocation in wireless
federated learning networks: A long-term perspective,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Wireless Communications</em>, 2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
T. Nishio and R. Yonetani, “Client selection for federated learning with
heterogeneous resources in mobile edge,” in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">ICC 2019-2019 IEEE
International Conference on Communications (ICC)</em>.   IEEE, 2019, pp. 1–7.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
T. T. Anh, N. C. Luong, D. Niyato, D. I. Kim, and L.-C. Wang, “Efficient
training management for mobile crowd-machine learning: A deep reinforcement
learning approach,” <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">IEEE Wireless Communications Letters</em>, vol. 8,
no. 5, pp. 1345–1348, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Y. J. Cho, J. Wang, and G. Joshi, “Client selection in federated learning:
Convergence analysis and power-of-choice selection strategies,” <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2010.01243</em>, 2020.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
M. Kamp, L. Adilova, J. Sicking, F. Hüger, P. Schlicht, T. Wirtz, and
S. Wrobel, “Efficient decentralized deep learning by dynamic model
averaging,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Joint European Conference on Machine Learning and
Knowledge Discovery in Databases</em>.   Springer, 2018, pp. 393–409.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
T. D. Bui, C. V. Nguyen, S. Swaroop, and R. E. Turner, “Partitioned
variational inference: A unified framework encompassing federated and
continual learning,” <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.11206</em>, 2018.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
V. Smith, C.-K. Chiang, M. Sanjabi, and A. Talwalkar, “Federated multi-task
learning,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.10467</em>, 2017.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
N. Guha, A. Talwalkar, and V. Smith, “One-shot federated learning,”
<em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.11175</em>, 2019.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
H. T. Nguyen, V. Sehwag, S. Hosseinalipour, C. G. Brinton, M. Chiang, and H. V.
Poor, “Fast-convergent federated learning,” <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected
Areas in Communications</em>, vol. 39, no. 1, pp. 201–218, 2020.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
H. Tang, S. Gan, C. Zhang, T. Zhang, and J. Liu, “Communication compression
for decentralized training,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.06443</em>, 2018.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
A. Reisizadeh, H. Taheri, A. Mokhtari, H. Hassani, and R. Pedarsani, “Robust
and communication-efficient collaborative learning,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1907.10595</em>, 2019.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
J. Xu, B. S. Glicksberg, C. Su, P. Walker, J. Bian, and F. Wang, “Federated
learning for healthcare informatics,” <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Journal of Healthcare
Informatics Research</em>, pp. 1–19, 2020.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
C. He, C. Tan, H. Tang, S. Qiu, and J. Liu, “Central server free federated
learning over single-sided trust social networks,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1910.04956</em>, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
A. Lalitha, O. C. Kilinc, T. Javidi, and F. Koushanfar, “Peer-to-peer
federated learning on graphs,” <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.11173</em>, 2019.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
A. G. Roy, S. Siddiqui, S. Pölsterl, N. Navab, and C. Wachinger,
“Braintorrent: A peer-to-peer environment for decentralized federated
learning,” <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.06731</em>, 2019.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
D. Rothchild, A. Panda, E. Ullah, N. Ivkin, I. Stoica, V. Braverman,
J. Gonzalez, and R. Arora, “Fetchsgd: Communication-efficient federated
learning with sketching,” in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>.   PMLR, 2020, pp. 8253–8265.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
S. Li, Q. Qi, J. Wang, H. Sun, Y. Li, and F. R. Yu, “Ggs: General gradient
sparsification for federated learning in edge computing,” in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">ICC
2020-2020 IEEE International Conference on Communications (ICC)</em>.   IEEE, 2020, pp. 1–7.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
R. Hu, Y. Gong, and Y. Guo, “Sparsified privacy-masking for
communication-efficient and privacy-preserving federated learning,”
<em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.01558</em>, 2020.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek, “Sparse binary
compression: Towards distributed deep learning with minimal communication,”
in <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">2019 International Joint Conference on Neural Networks
(IJCNN)</em>.   IEEE, 2019, pp. 1–8.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
M. M. Amiri, D. Gunduz, S. R. Kulkarni, and H. V. Poor, “Federated learning
with quantized global model updates,” <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2006.10672</em>, 2020.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
X. Dai, X. Yan, K. Zhou, H. Yang, K. K. Ng, J. Cheng, and Y. Fan,
“Hyper-sphere quantization: Communication-efficient sgd for federated
learning,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.04655</em>, 2019.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
N. Shlezinger, M. Chen, Y. C. Eldar, H. V. Poor, and S. Cui, “Federated
learning with quantization constraints,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.   IEEE, 2020, pp. 8851–8855.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
L. Liu, J. Zhang, S. Song, and K. B. Letaief, “Hierarchical quantized
federated learning: Convergence analysis and system design,” <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2103.14272</em>, 2021.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
R. Spring, A. Kyrillidis, V. Mohan, and A. Shrivastava, “Compressing gradient
optimizers via count-sketches,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.00179</em>,
2019.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
J. Nicholls, “Quantization in deep learning,”
<a target="_blank" href="https://medium.com/@joel_34050/quantization-in-deep-learning-478417eab72b/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://medium.com/@joel_34050/quantization-in-deep-learning-478417eab72b/</a>,
2018, Accessed April 28th, 2021.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
P. Jiang and G. Agrawal, “A linear speedup analysis of distributed deep
learning with sparse and quantized communication,” <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, vol. 31, pp. 2525–2536, 2018.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
A. Koloskova, T. Lin, S. U. Stich, and M. Jaggi, “Decentralized deep learning
with arbitrary communication compression,” <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1907.09356</em>, 2019.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
N. Shlezinger, M. Chen, Y. C. Eldar, H. V. Poor, and S. Cui, “Uveqfed:
Universal vector quantization for federated learning,” <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Signal Processing</em>, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.10995" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.10996" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.10996">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.10996" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.10997" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 23:06:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
