<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging</title>
<!--Generated on Sat Sep 14 16:32:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="PET/CT Lesion Segmentation Generalization." lang="en" name="keywords"/>
<base href="/html/2409.09478v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S1" title="In From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S2" title="In From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S2.SS1" title="In 2 Methods ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>nnUNet Configuration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S2.SS2" title="In 2 Methods ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S2.SS3" title="In 2 Methods ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Pretaining and Finetuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S2.SS4" title="In 2 Methods ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Organ Supervision</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S2.SS5" title="In 2 Methods ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Approaches we tried, that didn’t work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S3" title="In From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S3.SS1" title="In 3 Results ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Test Set Submission</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S4" title="In From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S4.SS0.SSS1" title="In 4 Conclusion ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.1 </span>Acknowledgements</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
German Cancer Research Center (DKFZ) Heidelberg,
<br class="ltx_break"/>Division of Medical Image Computing, Heidelberg, Germany
</span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Medical Faculty Heidelberg, Heidelberg University, Heidelberg, Germany</span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Faculty of Mathematics and Computer Science,
<br class="ltx_break"/>Heidelberg University, Heidelberg, Germany</span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Helmholtz Imaging, DKFZ, Heidelberg, Germany</span></span></span><span class="ltx_note ltx_role_institutetext" id="id5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany
<span class="ltx_note ltx_role_email" id="id5.1"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>maximilian.rokuss@dkfz-heidelberg.de</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">From FDG to PSMA: A Hitchhiker’s Guide
<br class="ltx_break"/>to Multitracer, Multicenter Lesion Segmentation
<br class="ltx_break"/>in PET/CT Imaging</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maximilian Rokuss<span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0009-0004-4560-0760
</span><span class="ltx_author_notes">1133</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Balint Kovacs<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0002-1191-0646
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yannick Kirchhoff<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0001-8124-8435
</span><span class="ltx_author_notes">1133</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Shuhan Xiao
</span><span class="ltx_author_notes">1133</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Constantin Ulrich<span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0000-0003-3002-8170
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Klaus H. Maier-Hein<span class="ltx_ERROR undefined" id="id5.1.id1">\orcidlink</span>0000-0002-6626-2463
</span><span class="ltx_author_notes">114455⋆⋆</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fabian Isensee<span class="ltx_ERROR undefined" id="id6.1.id1">\orcidlink</span>0000-0002-3519-5886
</span><span class="ltx_author_notes">1144⋆⋆</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Automated lesion segmentation in PET/CT scans is crucial for improving clinical workflows and advancing cancer diagnostics. However, the task is challenging due to physiological variability, different tracers used in PET imaging, and diverse imaging protocols across medical centers. To address this, the autoPET series was created to challenge researchers to develop algorithms that generalize across diverse PET/CT environments. This paper presents our solution for the autoPET III challenge, targeting multitracer, multicenter generalization using the nnU-Net framework with the ResEncL architecture. Key techniques include misalignment data augmentation and multi-modal pretraining across CT, MR, and PET datasets to provide an initial anatomical understanding. We incorporate organ supervision as a multitask approach, enabling the model to distinguish between physiological uptake and tracer-specific patterns, which is particularly beneficial in cases where no lesions are present. Compared to the default nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL (65.31) our model significantly improved performance with a Dice score of 68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative (FNvol: 10.35) volumes. These results underscore the effectiveness of combining advanced network design, augmentation, pretraining, and multitask learning for PET/CT lesion segmentation. Code is publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/MIC-DKFZ/autopet-3-submission" title="">https://github.com/MIC-DKFZ/autopet-3-submission</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>PET/CT Lesion Segmentation Generalization.
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>Shared Last Authorship</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Positron Emission Tomography (PET) combined with Computed Tomography (CT) is a powerful tool in modern medical diagnostics, particularly for detecting and monitoring cancer. PET/CT scans provide both metabolic and anatomical information, allowing clinicians to identify tumor lesions with high precision. However, manual segmentation of lesions in PET/CT images is time-consuming, and labor-intensive rendering it infeasible for patients with a multitude of lesions. Automated lesion segmentation offers a promising solution, enabling faster, more consistent analysis, which is crucial for clinical workflows and research.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, despite its potential, automated segmentation faces significant challenges. The complexity arises from the physiological variability between patients, the different tracers used in PET imaging (such as FDG and PSMA), and the variations in imaging protocols across different medical centers. Each tracer can highlight different metabolic activities, which leads to distinct patterns of uptake in non-tumor structures, complicating the task of distinguishing between normal physiological uptake and actual lesions. To accurately assess PET/CT images, a model must learn to interpret varying uptake patterns without explicit information about the specific tracer used. Instead, it must rely on the surrounding anatomical context to differentiate between physiological and cancerous uptake. These complexities, particularly the variability in tracer behavior, have made automated lesion segmentation in PET/CT imaging a highly challenging task for models to perform effectively.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address this, the autoPET challenge series was created, offering a platform for researchers to directly tackle these issues. Building on the insights from previous iterations, the <a class="ltx_ref ltx_href" href="https://autopet-iii.grand-challenge.org" title="">autoPET III challenge</a> broadens its scope to focus on multitracer, multicenter generalization. The publicly available dataset of 1014 FDG PET/CT studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib3" title="">3</a>]</cite> has been extended by 597 exams with a new PSMA tracer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib7" title="">7</a>]</cite>. By providing access to large, annotated datasets from different hospitals, participants are tasked with developing algorithms capable of accurate and robust segmentation across diverse PET/CT environments. This challenge represents a crucial step toward enhancing automated medical imaging for real-world clinical applications.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">This manuscript describes our participation in the <a class="ltx_ref ltx_href" href="https://autopet-iii.grand-challenge.org" title="">autoPET III challenge</a>. We base our solution on a strong foundation offered by nnU-Net<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib5" title="">5</a>]</cite> and address the aforementioned challenges of automated PET/CT lesion segmentation through data augmentation, pretraining, model design, and postprocessing techniques. Our approach aims to improve generalization across tracers and centers, tackling the complexities of physiological and cancerous uptake.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our method builds on the well-established nnU-Net framework<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib5" title="">5</a>]</cite>, specifically, we opt for a larger and more powerful network given by the recently introduced ResEncL architecture preset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>nnUNet Configuration</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We use the <code class="ltx_verbatim ltx_font_typewriter" id="S2.SS1.p1.1.1">3d_fullres</code> configuration, resample all images to a common spacing of <code class="ltx_verbatim ltx_font_typewriter" id="S2.SS1.p1.1.2">[3, 2.04, 2.04]</code> and normalize both modalities with the default CT normalization scheme. We train with a batch size of 2 for 1000 epochs and a uniform patch size of <code class="ltx_verbatim ltx_font_typewriter" id="S2.SS1.p1.1.3">192x192x192</code> for all trainings during method development. This large patch size has the advantage of providing the network with more context which is important, especially for the task at hand where the network needs to infer the tracer type and uptake rate from the surrounding organs. We also noticed that training without a smoothing term in the dice loss calculation the training becomes more stable, hence we omit it. The best model we use for the submission is retrained with a batch size of 3 for 1500 epochs.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data augmentation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">To account for potential misalignments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib10" title="">10</a>]</cite> between the CT and PET images, we extended the data augmentation (DA) scheme of nnU-Net with misalignment DA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib9" title="">9</a>]</cite>. Essentially, this augmentation shifts the PET and CT images relative to each other to make the network more robust to incorrect spatial alignment. This approach has the potential advantage of improving sensitivity for punctate lesions with small voxel segmentation volumes, which was indicated in the original study but remained unproven. The amplitude of the transformations used to generate misalignments was sampled randomly from a uniform distribution, constrained by a maximum amplitude in both positive and negative directions. The transformation included an initial rotation with a maximum angle of <math alttext="5\text{\,}\mathrm{\SIUnitSymbolDegree}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.3"><semantics id="S2.SS2.p1.1.m1.3a"><mrow id="S2.SS2.p1.1.m1.3.3" xref="S2.SS2.p1.1.m1.3.3.cmml"><mn id="S2.SS2.p1.1.m1.1.1.1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.1.1.1.cmml">5</mn><mtext id="S2.SS2.p1.1.m1.2.2.2.2.2.2" xref="S2.SS2.p1.1.m1.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S2.SS2.p1.1.m1.3.3.3.3.3.3" mathvariant="normal" xref="S2.SS2.p1.1.m1.3.3.3.3.3.3.cmml">°</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.3b"><apply id="S2.SS2.p1.1.m1.3.3.cmml" xref="S2.SS2.p1.1.m1.3.3"><csymbol cd="latexml" id="S2.SS2.p1.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS2.p1.1.m1.2.2.2.2.2.2">times</csymbol><cn id="S2.SS2.p1.1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.1.1.1.1">5</cn><csymbol cd="latexml" id="S2.SS2.p1.1.m1.3.3.3.3.3.3.cmml" xref="S2.SS2.p1.1.m1.3.3.3.3.3.3">degree</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.3c">5\text{\,}\mathrm{\SIUnitSymbolDegree}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.3d">start_ARG 5 end_ARG start_ARG times end_ARG start_ARG ° end_ARG</annotation></semantics></math>, followed by translations with maximum voxel shifts of <code class="ltx_verbatim ltx_font_typewriter" id="S2.SS2.p1.1.1">[2, 2, 0]</code> in the x, y, and z directions, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Pretaining and Finetuning</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To steer the model towards an anatomically relevant loss minimum, we first pretrain it on a large and diverse dataset of 3D medical images, combining a variety of public datasets in a MultiTalent-inspired fashion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib11" title="">11</a>]</cite>. Initially restricted to CT datasets, we later expanded this pretraining to include datasets from PET and Magnetic Resonance Imaging (MRI) modalities, allowing the model to learn general features and develop a universal understanding of anatomy and medical images. Employing separate segmentation heads for each dataset, the model was trained for 4000 epochs with a patch size of <code class="ltx_verbatim ltx_font_typewriter" id="S2.SS3.p1.1.1">[192,192,192]</code> and a batch size of 24, resampling all images to a cubic 1mm resolution and Z-score normalization. Dataset sampling was performed inversely proportional to the square root of the number of images per dataset, ensuring balanced training. The resulting foundation model, implemented using the nnU-Net framework and based on the ResEncL U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib6" title="">6</a>]</cite>, can effectively segment multiple datasets simultaneously and demonstrates strong potential for fine-tuning on specific downstream tasks. We have made the pretrained model weights publicly available to encourage further research<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/13753413" title="">https://zenodo.org/records/13753413</a></span></span></span>. After this pretraining, we fine-tune the model on the autoPET III dataset with the initial learning rate lowered to 1e-3, enhancing robustness and segmentation accuracy by leveraging weights optimized on a broad range of tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Organ Supervision</h3>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="569" id="S2.F1.sf1.g1" src="extracted/5855206/figures/autoPET_PETCT.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F1.sf1.3.2" style="font-size:90%;">PET/CT Image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="569" id="S2.F1.sf2.g1" src="extracted/5855206/figures/autoPET_organs.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F1.sf2.3.2" style="font-size:90%;">Organ annotation</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="569" id="S2.F1.sf3.g1" src="extracted/5855206/figures/autoPET_lesions.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F1.sf3.3.2" style="font-size:90%;">Lesion annotation</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.4.2" style="font-size:90%;">Examplary visualization of case <span class="ltx_text ltx_font_typewriter" id="S2.F1.4.2.1">fdg_0b98dbe00d_08-11-2002</span> shows the FDG PET image overlaid on the CT scan (a), along with corresponding annotations for organs (b) and lesions (c), used in the multitask training process.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">To improve further anatomical understanding and enhance segmentation performance, we introduce an additional prediction head focused on segmenting key organs. These organs often exhibit higher tracer uptake, which may not be the primary target but could complicate diagnosis. By addressing these areas, the network aims to reduce false positive volume (FPvol) and improve overall accuracy. We use TotalSegmentator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib12" title="">12</a>]</cite> to predict the spleen, kidneys, liver, urinary bladder, lung, brain, heart, stomach, prostate, and glands in the head region (parotid glands, submandibular glands) for all images as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S2.F1" title="Figure 1 ‣ 2.4 Organ Supervision ‣ 2 Methods ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_tag">1</span></a>. The selection of these structures was influenced by the uptake patterns of various tracers, reflecting the differing behaviors observed between them. This addition complements the lesion segmentation head, resulting in a dual-headed architecture where one head focuses on lesions and the other on organ structures. Each prediction head is then trained using a softmax activation function and with equal loss weighting during optimization.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">The rationale behind organ supervision is to provide additional guidance to the network by introducing anatomical priors and focusing on organs that exhibit uptake for specific tracers. This is particularly important given that approximately half of the training cases do not contain lesions, limiting the direct lesion-based learning signal. By including organ supervision, we ensure that the network learns meaningful representations of both healthy and abnormal anatomical structures, promoting better generalization. This approach helps improve segmentation performance, especially in cases without lesions, where the network can rely on organ structures for guidance. We also observed a faster convergence of lesion segmentation during training with the addition of organ supervision.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">We also experimented with a variant where the organ supervision loss was assigned a lower weight relative to the lesion segmentation loss. While this variant showed a slight improvement in Lesion Dice scores, it led to an increase in the false positive and false negative volume, suggesting a trade-off in performance. Consequently, we found that balancing the losses equally for both prediction heads yielded the most robust overall results.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Approaches we tried, that didn’t work</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">We also explored several ablation strategies that did not yield significant improvements during development. Resampling the images to an isotropic spacing of <code class="ltx_verbatim ltx_font_typewriter" id="S2.SS5.p1.1.1">[1, 1, 1]</code> resulted in a substantial performance decline, likely due to the reduced contextual information available within each patch. Additionally, pretraining on the TotalSegmentator dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib13" title="">13</a>]</cite> provided no notable advantage over training the model from scratch. Similarly, incorporating additional annotated FDG images from the HECKTOR challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#bib.bib2" title="">2</a>]</cite> into the training set did not enhance the performance metrics on the autoPET dataset.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.5.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.6.2" style="font-size:90%;">Five-fold cross-validation results. The table shows the metrics calculated in the official evaluation as well as the Dice values for the FDG and PSMA tracer separately.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.3.3" style="width:433.6pt;height:209.2pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.2pt,3.9pt) scale(0.96366,0.96366) ;">
<p class="ltx_p" id="S3.T1.3.3.3"><span class="ltx_text" id="S3.T1.3.3.3.3">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.3.3.3.3.3" style="width:450.0pt;height:217pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T1.3.3.3.3.3.3"><span class="ltx_text" id="S3.T1.3.3.3.3.3.3.3">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.3.3.3.3.3.3.3.3">
<span class="ltx_tr" id="S3.T1.1.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_border_r" id="S3.T1.1.1.1.1.1.1.1.1.1.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_colspan ltx_colspan_3" id="S3.T1.1.1.1.1.1.1.1.1.1.1">Dice<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.1.1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span>
<span class="ltx_td" id="S3.T1.1.1.1.1.1.1.1.1.1.3"></span>
<span class="ltx_td" id="S3.T1.1.1.1.1.1.1.1.1.1.4"></span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.3">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.3.3">Setting</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.3.4">All</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.3.5">FDG</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.3.6">PSMA</span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.2.2.2.2.1">FPvol<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1"><semantics id="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1a"><mo id="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1.1" stretchy="false" xref="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1b"><ci id="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.2.2.2.2.2.1.m1.1d">↓</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.3.2">FNvol<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1"><semantics id="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1a"><mo id="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1.1" stretchy="false" xref="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1b"><ci id="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1.1.cmml" xref="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.3.3.3.3.3.2.m1.1d">↓</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.4">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.4.1">nnU-Net</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.4.2">57.61</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.4.3">63.93</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.4.4">51.69</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.4.5">19.32</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.4.6">15.69</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.5">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.5.1">nnU-Net ResEnc L</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.5.2">65.31</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.5.3">72.87</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.5.4">58.25</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.5.5">10.47</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.5.6">13.63</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.6">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.6.1">+ misalDA</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.6.2">65.76</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.6.3">73.13</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.6.4">58.89</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.6.5">10.12</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.6.6">12.50</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.7">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.7.1">+ pretrain CT</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.7.2">66.08</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.7.3">73.13</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.7.4">59.46</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.7.5">7.82</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.7.6">15.34</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.8">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.8.1">+ pretrain CT, misalDA</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.8.2">66.37</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.8.3">73.06</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.8.4">59.99</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.8.5">7.99</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.8.6">14.46</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.9">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.9.1">+ pretrain CT, noSmooth</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.9.2">67.01</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.9.3">75.04</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.9.4">59.50</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.9.5">10.08</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.9.6">13.50</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.10">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.10.1">+ pretrain CT, misalDA, noSmooth</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.10.2">67.45</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.10.3">75.41</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.10.4">60.01</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.10.5">9.45</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.10.6">13.33</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.11">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.11.1">+ pretrain CT/MR/PET, misalDA, noSmooth</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.11.2">68.03</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.11.3">75.95</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.11.4">60.63</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.11.5">9.39</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.11.6">13.52</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.12">
<span class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.12.1">+ pretrain CT/MR/PET, misalDA, noSmooth, organs</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.12.2">68.33</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.12.3">76.30</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3.3.3.3.12.4"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.3.3.3.3.3.12.4.1">60.84</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.12.5">8.93</span>
<span class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3.3.3.3.3.12.6"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.3.3.3.3.3.12.6.1">10.15</span></span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3.3.3.3.13">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.13.1">+ batch size 3, 1500 epochs</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.13.2"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.3.3.3.3.3.13.2.1">68.40</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.13.3"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.3.3.3.3.3.13.3.1">77.28</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.13.4">60.01</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.13.5"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.3.3.3.3.3.13.5.1">7.82</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3.3.3.13.6">10.35</span></span>
</span></span></span>
</span></span></span></p>
</span></div>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Model development and evaluation were carried out using a five-fold cross-validation on the autoPET III training split. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09478v1#S3.T1" title="Table 1 ‣ 3 Results ‣ From FDG to PSMA: A Hitchhiker’s Guide to Multitracer, Multicenter Lesion Segmentation in PET/CT Imaging"><span class="ltx_text ltx_ref_tag">1</span></a>, the nnU-Net ResEnc L architecture outperformed the baseline nnU-Net across all metrics, serving as the backbone for further experimentation.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Initial improvements focused on data augmentation and pretraining strategies. Incorporating the misalignment data augmentation (misalDA) resulted in a marginal improvement in Dice score (from 65.31 to 65.76), particularly benefiting the PSMA tracer and achieving a low FNvol. Supervised pretraining the model on CT datasets boosted performance further, especially for PSMA, increasing the overall Dice to 66.08.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Combining CT pretraining with misalDA led to a more balanced improvement across both tracers compared to the ResEncL baseline, pushing the Dice to 66.37. Additional experiments with no smoothing term in the dice calculation yielded a significant leap, especially for FDG cases, with the overall Dice reaching 67.45. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Another gain was achieved by pretraining on multimodal datasets (CT/MR/PET) paired with misalDA. Lastly, the inclusion of organ segmentation as a multitask learning objective further enhanced the model, achieving a Dice score of 68.33 and notably reducing both false positive and false negative volumes. Retraining this model with a batch size of 3 for 1500 epochs yields the best overall model with a Dice score of 68.40 and the lowest combined FP and FN volume.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Test Set Submission</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For the final submission, we ensembled all 5 folds. To meet the 5-minute time constraint per case, we set a tile step size of 0.6, which controls the sliding window shift relative to the patch size. The first fold is processed without mirroring, and based on the time taken, one or two mirroring axes are added as test-time augmentation for the remaining folds.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this paper, we addressed the challenges of automated lesion segmentation in PET/CT imaging through a combination of data augmentation, pretraining, and multitask learning as well as a careful choice of the underlying network design and training. Building on the nnU-Net framework, we use misalignment data augmentation and multimodal pretraining, which improved generalization across different tracers and centers. Incorporating organ supervision as a secondary task further boosted performance by guiding the model with anatomical priors, especially in cases without lesions. This holistic approach resulted in significant improvements, achieving a top Dice score of 68.33 on the training set cross-validation, demonstrating the potential of these techniques in advancing multitracer, multicenter lesion segmentation.</p>
</div>
<div class="ltx_para" id="S4.p2">
<span class="ltx_ERROR undefined" id="S4.p2.1">{credits}</span>
</div>
<section class="ltx_subsubsection" id="S4.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Acknowledgements</h4>
<div class="ltx_para" id="S4.SS0.SSS1.p1">
<p class="ltx_p" id="S4.SS0.SSS1.p1.1">Part of this work was funded by Helmholtz Imaging (HI), a platform of the Helmholtz Incubator on Information and Data Science. The present contribution is supported by the Helmholtz Association under the joint research school "HIDSS4Health – Helmholtz Information and Data Science School for Health".</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alessio, A.M., Kinahan, P.E., Cheng, P.M., Vesselle, H., Karp, J.S.: PET/CT scanner instrumentation, challenges, and solutions. Radiologic Clinics <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">42</span>(6), 1017–1032 (2004). https://doi.org/doi:10.1016/j.rcl.2004.08.001

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Andrearczyk, V., Oreiller, V., Boughdad, S., Rest, C.C.L., Elhalawani, H., Jreige, M., Prior, J.O., Vallières, M., Visvikis, D., Hatt, M., et al.: Overview of the HECKTOR challenge at MICCAI 2021: automatic head and neck tumor segmentation and outcome prediction in PET/CT images. In: 3D head and neck tumor segmentation in PET/CT challenge, pp. 1–37. Springer (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Gatidis, S., Kuestner, T.: A whole-body fdg-pet/ct dataset with manually annotated tumor lesions (fdg-pet-ct-lesions). <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.7937/GKR0-XV29" title="">https://doi.org/10.7937/GKR0-XV29</a> (2022), the Cancer Imaging Archive

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Hunter, C.R., Klein, R., Beanlands, R.S., deKemp, R.A.: Patient motion effects on the quantification of regional myocardial blood flow with dynamic PET imaging. Medical physics <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">43</span>(4), 1829–1840 (2016). https://doi.org/DOI: 10.1118/1.4943565

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods <span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">18</span>(2), 203–211 (2021). https://doi.org/https://doi.org/10.1038/s41592-020-01008-z

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Isensee, F., Wald, T., Ulrich, C., Baumgartner, M., Roy, S., Maier-Hein, K., Jaeger, P.F.: nnU-Net revisited: A call for rigorous validation in 3d medical image segmentation. arXiv preprint arXiv:2404.09556 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jeblick, K., et al.: A whole-body psma-pet/ct dataset with manually annotated tumor lesions (psma-pet-ct-lesions). <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.7937/r7ep-3x37" title="">https://doi.org/10.7937/r7ep-3x37</a> (2024), the Cancer Imaging Archive

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Kaji, T., Osanai, K., Takahashi, A., Kinoshita, A., Satoh, D., Nakata, T., Tamaki, N.: Improvement of motion artifacts using dynamic whole-body 18F-FDG PET/CT imaging. Japanese Journal of Radiology <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">42</span>(4), 374–381 (2024). https://doi.org/https://doi.org/10.1007/s11604-023-01513-z

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kovacs, B., Netzer, N., Baumgartner, M., Schrader, A., Isensee, F., Weißer, C., Wolf, I., Görtz, M., Jaeger, P.F., Schütz, V., et al.: Addressing image misalignments in multi-parametric prostate MRI for enhanced computer-aided diagnosis of prostate cancer. Scientific Reports <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">13</span>(1), 19805 (2023). https://doi.org/https://doi.org/10.1038/s41598-023-46747-z

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Lodge, M.A., Mhlanga, J.C., Cho, S.Y., Wahl, R.L.: Effect of patient arm motion in whole-body PET/CT. Journal of Nuclear Medicine <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">52</span>(12), 1891–1897 (2011). https://doi.org/10.2967/jnumed.111.093583

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ulrich, C., Isensee, F., Wald, T., Zenk, M., Baumgartner, M., Maier-Hein, K.H.: Multitalent: A multi-dataset approach to medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 648–658. Springer (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Walter, A., Hoegen-Saßmannshausen, P., Stanic, G., Rodrigues, J.P., Adeberg, S., Jäkel, O., Frank, M., Giske, K.: Segmentation of 71 anatomical structures necessary for the evaluation of guideline-conforming clinical target volumes in head and neck cancers. Cancers <span class="ltx_text ltx_font_bold" id="bib.bib12.1.1">16</span>(2) (2024). https://doi.org/10.3390/cancers16020415

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Wasserthal, J., Breit, H.C., Meyer, M.T., Pradella, M., Hinck, D., Sauter, A.W., Heye, T., Boll, D.T., Cyriac, J., Yang, S., et al.: Totalsegmentator: robust segmentation of 104 anatomic structures in CT images. Radiology: Artificial Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">5</span>(5) (2023)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 14 16:32:41 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
