<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.11006] Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments</title><meta property="og:description" content="Federated learning (FL) is a privacy-friendly type of machine learning where devices locally train a model on their private data and typically communicate model updates with a server.
In decentralized FL (DFL), peers c…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.11006">

<!--Generated on Sat Mar  2 03:25:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Decentralized Federated learning,  Deep Transfer Learning,  Gradient Aggregation Rule,  Byzantine-resilience">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">
<span id="id1.id1" class="ltx_text ltx_font_smallcaps">Bristle</span>: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joost Verbraeken
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:j.verbraeken@student.tudelft.nl">j.verbraeken@student.tudelft.nl</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_affiliation_institution">Delft University of Technology</span><span id="id3.2.id2" class="ltx_text ltx_affiliation_city">Delft</span><span id="id4.3.id3" class="ltx_text ltx_affiliation_country">The Netherlands</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martijn de Vos
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:M.A.deVos-1@tudelft.nl">M.A.deVos-1@tudelft.nl</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Delft University of Technology</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_city">Delft</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_country">The Netherlands</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Johan Pouwelse
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:J.A.Pouwelse@tudelft.nl">J.A.Pouwelse@tudelft.nl</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id8.1.id1" class="ltx_text ltx_affiliation_institution">Delft University of Technology</span><span id="id9.2.id2" class="ltx_text ltx_affiliation_city">Delft</span><span id="id10.3.id3" class="ltx_text ltx_affiliation_country">The Netherlands</span>
</span></span></span>
</div>
<div class="ltx_dates">(2021)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id11.id1" class="ltx_p">Federated learning (FL) is a privacy-friendly type of machine learning where devices locally train a model on their private data and typically communicate model updates with a server.
In decentralized FL (DFL), peers communicate model updates with each other instead.
However, DFL is challenging since (1) the training data possessed by different peers is often non-i.i.d. (i.e., distributed differently between the peers) and (2) malicious, or Byzantine, attackers can share arbitrary model updates with other peers to subvert the training process.</p>
<p id="id12.id2" class="ltx_p">We address these two challenges and present <em id="id12.id2.1" class="ltx_emph ltx_font_italic">Bristle</em>, middleware between the learning application and the decentralized network layer.
Bristle leverages transfer learning to predetermine and freeze the non-output layers of a neural network, significantly speeding up model training and lowering communication costs.
To securely update the output layer with model updates from other peers, we design a fast distance-based prioritizer and a novel performance-based integrator.
Their combined effect results in high resilience to Byzantine attackers and the ability to handle non-i.i.d. classes.</p>
<p id="id13.id3" class="ltx_p">We empirically show that Bristle converges to a consistent 95% accuracy in Byzantine environments, outperforming all evaluated baselines. In non-Byzantine environments, Bristle requires 83% fewer iterations to achieve 90% accuracy compared to state-of-the-art methods. We show that when the training classes are non-i.i.d., Bristle significantly outperforms the accuracy of the most Byzantine-resilient baselines by 2.3x while reducing communication costs by 90%.</p>
</div>
<div class="ltx_keywords">Decentralized Federated learning, Deep Transfer Learning, Gradient Aggregation Rule, Byzantine-resilience
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2021</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Lifelong machine learning</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Transfer learning</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Supervised learning by classification</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Neural networks</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Machine learning applications have gained significant traction and are widely used for various purposes, such as understanding consumer preferences <cite class="ltx_cite ltx_citemacro_citep">(Chapelle and
Harchaoui, <a href="#bib.bib11" title="" class="ltx_ref">2005</a>)</cite>, recognizing pictures <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky
et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2012</a>)</cite>, predicting keystrokes <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite>, or translating texts <cite class="ltx_cite ltx_citemacro_citep">(Chui et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>.
Traditionally, these applications use neural networks trained on a single server using a tremendous amount of data generated by a large number of geo-distributed, heterogeneous edge devices such as smartphones, IoT devices, or autonomous vehicles <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>.
However, centralized training on data streams generated by such devices is limited by the following three factors.
First, transmitting training data over the Internet can pose a significant burden on backbone networks.
This burden is particularly problematic when media such as photos or videos are used as training data and is worsened by the fact that most learning applications communicate with cloud providers over wireless links <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2018</a>; Wang
et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2020</a>)</cite>.
Second, maintaining a central server architecture can quickly become expensive and time-consuming as the number of devices increases <cite class="ltx_cite ltx_citemacro_citep">(Durkee, <a href="#bib.bib25" title="" class="ltx_ref">2010</a>)</cite>.
Third, transmitting personal and sensitive information over the Internet, such as text conversations or photos, to cloud providers raises privacy concerns and is in certain jurisdictions not even allowed by regulations such as the US HIPAA <cite class="ltx_cite ltx_citemacro_citep">(for Disease Control et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2003</a>)</cite> and the European GDPR law <cite class="ltx_cite ltx_citemacro_citep">(Voigt and Von dem
Bussche, <a href="#bib.bib76" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.4" class="ltx_p"><em id="S1.p2.4.1" class="ltx_emph ltx_font_italic">Federated Learning (FL)</em> overcomes these three limitations.
With FL, edge devices do not send their data to the server training the model but instead communicate model updates to a so-called <em id="S1.p2.4.2" class="ltx_emph ltx_font_italic">parameter server</em>, also see the left side of Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020b</a>)</cite>.
The parameter server coordinates the learning process by pushing model updates to devices (step  <svg id="S1.p2.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.p2.1.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg>), and peers train the model with their private data on-device (step  <svg id="S1.p2.2.pic2" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.p2.2.pic2.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>).
Then they send the updated model back to the parameter server (step  <svg id="S1.p2.3.pic3" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.p2.3.pic3.1.1.1.1.1" class="ltx_text">3</span></foreignObject></g></g></svg>), after which the server <em id="S1.p2.4.3" class="ltx_emph ltx_font_italic">aggregates</em> the incoming model updates into a global model (step  <svg id="S1.p2.4.pic4" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.p2.4.pic4.1.1.1.1.1" class="ltx_text">4</span></foreignObject></g></g></svg>).
FL sidesteps the need for the data to leave the device, improving privacy, lowering the computational burden for the server, and decreasing the communication overhead when the model update is smaller than the data to be transmitted per iteration.
FL is nowadays used for various applications, including next-word prediction on keyboards <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2019a</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019b</a>; Hard et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>; RN9, <a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>, speech recognition <cite class="ltx_cite ltx_citemacro_citep">(Sim et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">[n.d.]</a>)</cite>, wireless communications <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020a</a>; Niknam
et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>, human activity recognition <cite class="ltx_cite ltx_citemacro_citep">(Sozinov
et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">[n.d.]</a>)</cite>, and health applications <cite class="ltx_cite ltx_citemacro_citep">(Brisimi et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2018</a>; Lu
et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">[n.d.]</a>; Rieke
et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2020</a>; Schneble and
Thamilarasu, <a href="#bib.bib64" title="" class="ltx_ref">[n.d.]</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The centralized architecture shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is typical for FL systems.
However, even though the parameter server synchronizes the learning process between remote edge devices, this approach comes with significant drawbacks <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz
et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2019</a>; Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">[n.d.]</a>; Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">[n.d.]b</a>)</cite>.
Notably, the parameter server not only poses a single point of failure susceptible to crashes or hacks, but it may also become a performance bottleneck as the number of devices pushing model updates increases <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">[n.d.]</a>)</cite>.
These issues motivate further research to remove dependency on the parameter server and train models in a decentralized manner <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">[n.d.]</a>; Dobbe
et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">[n.d.]</a>; Tang
et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">[n.d.]</a>; Lalitha et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2019</a>)</cite>.
With <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">decentralized federated learning</em> (DFL), each peer is connected to a subset of the other peers in the network from which it receives incoming models and to which it pushes its updated models <cite class="ltx_cite ltx_citemacro_citep">(Nedic and
Ozdaglar, <a href="#bib.bib59" title="" class="ltx_ref">2009</a>)</cite>.
We visualize this approach on the right side of Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">DFL has resulted in a new wave of learning methods that achieve comparable model accuracy as state-of-the-art FL approaches <cite class="ltx_cite ltx_citemacro_citep">(Hegedűs
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> while boasting several significant advantages.
First, decentralized networks are fault-tolerant by design since peers continuously update their knowledge about which other peers are online or stopped communicating<cite class="ltx_cite ltx_citemacro_citep">(Wallach, <a href="#bib.bib77" title="" class="ltx_ref">2002</a>)</cite>.
Second, decentralized networks are self-scaling. When a new peer joins the network, other peers can start communicating with the newly-joined peer seamlessly.
Because there is usually a maximum number of other peers with which each peer communicates, the connection ratio will drop, but the network will typically still be strongly connected <cite class="ltx_cite ltx_citemacro_citep">(Felber and
Biersack, <a href="#bib.bib28" title="" class="ltx_ref">2004</a>)</cite>.
Third, DFL unlocks AI with zero-cost infrastructure (from the developer’s perspective) since the computational power needed to train the network is delivered by all peers in the network working together instead of a parameter server under centralized control <cite class="ltx_cite ltx_citemacro_citep">(Endler
et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2011</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2110.11006/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="261" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Federated learning using a parameter server (left) and decentralized federated learning (right).</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Limitations of DFL.</span>
Despite the benefits of DFL, we identify three challenges that reduce its potential for practical applications.
The first barrier is that both existing FL and DFL architectures have to consider <em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">Byzantine attacks</em>, the situation where malicious peers aim to undermine the model’s training by purposefully sending specific model updates to the parameter server or other peers <cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2018</a>)</cite>.
Since malicious peers keep their data private, they can easily <span id="S1.p5.1.3" class="ltx_text ltx_inline-quote ltx_outerquote">“poison”</span> their model and disseminate this malicious model without repercussions.
Byzantine attacks are often addressed by the Gradient Aggregation Rule (GAR), which aggregates and combines incoming model updates <cite class="ltx_cite ltx_citemacro_citep">(Mhamdi
et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2018</a>; Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2018</a>; Blanchard
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">[n.d.]</a>)</cite>.
At the same time, state-of-the-art GARs typically assume that the majority of peers act honestly, which is hard to guarantee in networks with open participation <cite class="ltx_cite ltx_citemacro_citep">(Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The second challenge is that the performance of conventional FL systems degrades significantly when being deployed in an environment where the distribution of the training data differs between peers.
This is also known as <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">non-i.i.d.</em> (not independent and identically distributed) data.
Given that data is typically non-i.i.d. in an FL environment <cite class="ltx_cite ltx_citemacro_citep">(Blanchard
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">[n.d.]</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>; Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2018</a>)</cite>, dealing with non-i.i.d. data is considered a <span id="S1.p6.1.2" class="ltx_text ltx_font_italic">key challenge</span> in FL <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>.
The inability of most GARs to handle non-i.i.d. data results in Catastrophic Forgetting, a phenomenon where the peers overwrite model parameters that were important to predict a particular class distribution with parameters essential to predict another class distribution <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Yang, <a href="#bib.bib92" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The third challenge relates to the communication requirements to disseminate model updates among peers in the network.
In DFL, each peer disseminates a copy of the current model at every training iteration to several other peers.
However, modern neural networks may consist of millions of parameters <cite class="ltx_cite ltx_citemacro_citep">(He
et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2016</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2017</a>)</cite>, sometimes requiring gigabytes of data to be transferred for each model exchange.
Facilitating this amount of data quickly becomes infeasible when the number of devices and the frequency of model updates increase <cite class="ltx_cite ltx_citemacro_citep">(Sattler
et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Although some DFL systems are to a certain extent Byzantine-resilient <cite class="ltx_cite ltx_citemacro_citep">(Blanchard
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">[n.d.]</a>; Mhamdi
et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2018</a>; Yang and Bajwa, <a href="#bib.bib86" title="" class="ltx_ref">2019a</a>; Guo
et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>, able to deal with non-i.i.d. data <cite class="ltx_cite ltx_citemacro_citep">(Smith
et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">[n.d.]</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020b</a>; Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2018</a>)</cite>, or focus on reducing the communication costs <cite class="ltx_cite ltx_citemacro_citep">(De Sa
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">[n.d.]</a>; Alistarh
et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2016</a>; Wen
et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2017</a>)</cite>, there exists - to the best of the author’s knowledge - no DFL solution that addresses all three challenges simultaneously.
We argue that such a solution is a key requirement for a wide deployment of DFL systems.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p"><span id="S1.p9.1.1" class="ltx_text ltx_font_bold">Our Contributions. </span>
To the best of the author’s knowledge <cite class="ltx_cite ltx_citemacro_citep">(Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2020</a>; Yang and Bajwa, <a href="#bib.bib87" title="" class="ltx_ref">2019b</a>, <a href="#bib.bib86" title="" class="ltx_ref">a</a>; Guo
et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>, only four papers have ever been published about DFL that are Byzantine-resilient (first challenge), none of which are suitable for situations where the data is non-i.i.d. (second challenge) or decrease the communication requirements (third challenge).
Therefore, we present <em id="S1.p9.1.2" class="ltx_emph ltx_font_italic">Bristle</em>: the first Byzantine-resilient and communication-efficient approach for DFL in environments with non-i.i.d. classes.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text ltx_font_typewriter">Bristle</span> is an acronym for ”Byzantine-Resilient mIddleware for StochasTic federated LEarning”.</span></span></span>
Bristle is a pure edge solution and acts as a middleware between the machine learning application and the decentralized network layer by combining three techniques:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Using <em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">deep transfer learning</em>, model training with Bristle converges quickly and achieves high accuracy with low communication overhead and while requiring low amounts of on-device data (Section <a href="#S3.SS1" title="3.1. Bootstrapping Bristle with Transfer Learning ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">With a fast <em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">distance-based prioritizing step</em> during model aggregation based on an explore-exploit trade-off, Bristle pre-filters incoming model updates that are estimated to improve the current model (Section <a href="#S3.SS2" title="3.2. Distance-based prioritizer (DBP) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">With a novel <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">performance-based integrator</em>, Bristle provides state-of-the-art Byzantine-resilience, even when the classes are unevenly spread across peers (non-i.i.d.). Unlike related solutions, this component performs per-class performance measurements instead of per-model measurements (Section <a href="#S3.SS3" title="3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</li>
</ol>
<p id="S1.p9.2" class="ltx_p">Similar to related work in the FL domain, Bristle focuses on supervised learning problems that train a neural network for classification <cite class="ltx_cite ltx_citemacro_citep">(Mhamdi
et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2018</a>; McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">[n.d.]</a>; Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2018</a>)</cite>.
We implement all elements of Bristle as an Android library and open-source our implementation (see Section <a href="#S3.SS4" title="3.4. Implementation ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2110.11006/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_img_landscape" width="322" height="218" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">Our Bristle middleware for decentralized federated learning outperforms state-of-the-art solutions in a Byzantine and non-i.d.d. environment.</span></figcaption>
</figure>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">With an extensive set of experiments with the popular MNIST dataset, we evaluate the performance and resilience of Bristle.
We compare Bristle with five related approaches for FL and quantify the Byzantine-resilience of these approaches under four attacks in the domain of FL, aimed at reducing the model’s accuracy.
The experiments show that even in highly Byzantine environments where the classes are non-i.i.d., Bristle not only withstands all evaluated attacks but also outperforms all related approaches in terms of convergence speed, accuracy, consistency, and communication efficiency.
We highlight one of the key results from our experiments in Figure <a href="#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, showing model accuracy as peers train the model.
In this figure, the performance of Bristle and evaluated DFL approaches is illustrated in a Byzantine (40% of the peers perform a label-flip attack) and non-i.i.d. environment (the class overlap between the peers is just 40%).
Despite this challenging environment, Bristle quickly converges and outperforms all other approaches in terms of accuracy while reducing communication costs by over 90%.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2110.11006/assets/x3.png" id="S1.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="137" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S1.F3.3.2" class="ltx_text" style="font-size:90%;">High-level overview of Bristle, our middleware for decentralized federated learning.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>System and Threat model</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We now describe our system and threat model, and state the assumptions underlying our work.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Network Model.</span>
We consider an unstructured, strongly connected peer-to-peer network with <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">n</annotation></semantics></math> peers.
Each peer knows the network addresses of other peers in the network.
We assume unreliable and unordered network channels between peers.
In addition, we do not place any restriction on peers joining the system, and peers can join or leave the system at any time.
We consider the mitigation of attacks targeted at the network layer, e.g., the Eclipse Attack, beyond the scope of this work.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.5" class="ltx_p"><span id="S2.p3.5.1" class="ltx_text ltx_font_bold">Training Data.</span>
Each participating peer <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">i</annotation></semantics></math> acts on local training dataset <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.p3.2.m2.1a"><msub id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mi id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml">D</mi><mi id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1">subscript</csymbol><ci id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2">𝐷</ci><ci id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">D_{i}</annotation></semantics></math> which size is denoted by <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="|D_{i}|" display="inline"><semantics id="S2.p3.3.m3.1a"><mrow id="S2.p3.3.m3.1.1.1" xref="S2.p3.3.m3.1.1.2.cmml"><mo stretchy="false" id="S2.p3.3.m3.1.1.1.2" xref="S2.p3.3.m3.1.1.2.1.cmml">|</mo><msub id="S2.p3.3.m3.1.1.1.1" xref="S2.p3.3.m3.1.1.1.1.cmml"><mi id="S2.p3.3.m3.1.1.1.1.2" xref="S2.p3.3.m3.1.1.1.1.2.cmml">D</mi><mi id="S2.p3.3.m3.1.1.1.1.3" xref="S2.p3.3.m3.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.p3.3.m3.1.1.1.3" xref="S2.p3.3.m3.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><apply id="S2.p3.3.m3.1.1.2.cmml" xref="S2.p3.3.m3.1.1.1"><abs id="S2.p3.3.m3.1.1.2.1.cmml" xref="S2.p3.3.m3.1.1.1.2"></abs><apply id="S2.p3.3.m3.1.1.1.1.cmml" xref="S2.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.p3.3.m3.1.1.1.1.1.cmml" xref="S2.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.p3.3.m3.1.1.1.1.2.cmml" xref="S2.p3.3.m3.1.1.1.1.2">𝐷</ci><ci id="S2.p3.3.m3.1.1.1.1.3.cmml" xref="S2.p3.3.m3.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">|D_{i}|</annotation></semantics></math>.
Training data never leaves the device, and <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.p3.4.m4.1a"><msub id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml"><mi id="S2.p3.4.m4.1.1.2" xref="S2.p3.4.m4.1.1.2.cmml">D</mi><mi id="S2.p3.4.m4.1.1.3" xref="S2.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m4.1.1.1.cmml" xref="S2.p3.4.m4.1.1">subscript</csymbol><ci id="S2.p3.4.m4.1.1.2.cmml" xref="S2.p3.4.m4.1.1.2">𝐷</ci><ci id="S2.p3.4.m4.1.1.3.cmml" xref="S2.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">D_{i}</annotation></semantics></math> is only known to peer <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p3.5.m5.1a"><mi id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><ci id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">i</annotation></semantics></math>.
In contrast to most related work, we assume that the number of samples per class is not distributed uniformly among peers (non-i.i.d), which is a key characteristic of FL environments <cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2018</a>; Blanchard
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">[n.d.]</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite>.
However, we also assume that for each class the samples are distributed uniformly (i.i.d.) between peers as a prerequisite for the performance-based integrator (see Section <a href="#S3.SS3" title="3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Model Training.</span>
Our approach focuses on supervised classification problems where the dataset consists of a collection of input-output pairs.
The input is a chunk of data with a fixed size, and the output is a qualitative label associated with a class.
The devices aim to minimize the loss function by tuning the parameters in the neural network such that its predictive capabilities are maximized.
We use the negative log-likelihood of the ground truth class as the loss function.
Since this problem is intractable for complex models, we use, in line with most literature <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">[n.d.]</a>; Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2018</a>; Su and Vaidya, <a href="#bib.bib71" title="" class="ltx_ref">2015</a>; Sundaram and
Gharesifard, <a href="#bib.bib72" title="" class="ltx_ref">2018</a>)</cite>, a technique called Gradient Descent (GD) which iteratively takes the derivative of the loss function with respect to the training data and then moves the hyperparameters in the direction of the gradient.
However, because the local dataset can be large, it can take a long time for GD to converge <cite class="ltx_cite ltx_citemacro_citep">(Bottou, <a href="#bib.bib9" title="" class="ltx_ref">2012</a>)</cite>.
Therefore, we use the faster Stochastic Gradient Descent (SGD), where a subset (mini-batch) of five samples is stochastically sampled from the dataset to update the parameters in a particular iteration.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Threat Model.</span>
Our work assumes an environment with <em id="S2.p5.1.2" class="ltx_emph ltx_font_italic">an unconstrained number of Byzantine attackers</em> aiming to subvert the model’s performance.
This includes the Sybil Attack, an attack where a malicious peer joins the network under many different identities to prevent model convergence <cite class="ltx_cite ltx_citemacro_citep">(Douceur, <a href="#bib.bib24" title="" class="ltx_ref">[n.d.]</a>)</cite>.
It also includes collusion, the situation where malicious peers in concert attempt to undermine model convergence.
Byzantine attackers can send arbitrary model updates to any peer in the network, but have no access to the private samples of benign peers.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Bristle: Middleware for Decentralized Federated Learning</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Bristle enables DFL that can handle non-i.i.d. classes and thwart Byzantine attacks while improving communication costs compared to existing approaches.
We provide a high-level overview of Bristle and the actions performed during a training iteration in Figure <a href="#S1.F3" title="Figure 3 ‣ 1. Introduction ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
First, Bristle takes advantage of deep transfer learning to determine the non-output layers of each node before the first training iteration (step  <svg id="S3.p1.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.p1.1.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg>).
As we will experimentally demonstrate in Section <a href="#S5" title="5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, this significantly reduces communication costs compared to communicating the entire model, speeds up the model convergence rate, and acts as a prerequisite to learning on non-i.i.d. classes.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.7" class="ltx_p">The main innovation of Bristle is its two-phased GAR that integrates received models.
During each training iteration, a peer <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">i</annotation></semantics></math> trains the local model on a mini-batch of their private dataset <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">D</mi><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">𝐷</ci><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">D_{i}</annotation></semantics></math> and only updates the output layers (step  <svg id="S3.p2.3.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.p2.3.pic1.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>).
It then forwards the updated output layer<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We use the terms output layer and model interchangeably in this work.</span></span></span> and the output layers it has received from other peers to a distance-based prioritizer (step  <svg id="S3.p2.4.pic2" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.p2.4.pic2.1.1.1.1.1" class="ltx_text">3</span></foreignObject></g></g></svg>).
This fast distance-based prioritizer estimates the best candidates for integration based on an explore-exploit ratio and forwards them to the performance-based integrator (step  <svg id="S3.p2.5.pic3" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.p2.5.pic3.1.1.1.1.1" class="ltx_text">4</span></foreignObject></g></g></svg>).
The performance-based integration is more computationally demanding and integrates the prioritized output layers into the peer’s current output layer.
This integration process is both Byzantine-resilient and capable of continual learning, facilitated by selective updating of the output layer, per-class performance evaluations, and a carefully crafted weighted averaging function.
Our main motivation to use a two-staged approach is that a distance-based prioritizer on itself is ineffective and performance-based integration on all received models is too computationally expensive since it requires the evaluation of incoming models on private data.
Finally, Bristle transmits the output layer to a few connected peers (step  <svg id="S3.p2.6.pic4" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.p2.6.pic4.1.1.1.1.1" class="ltx_text">5</span></foreignObject></g></g></svg>) and uses the new output layer as input for the next training iteration (step  <svg id="S3.p2.7.pic5" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.p2.7.pic5.1.1.1.1.1" class="ltx_text">6</span></foreignObject></g></g></svg>).
In the remainder of this section, we elaborate on the model pre-training, the distance-based prioritizer, and the performance-based integrator.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Bootstrapping Bristle with Transfer Learning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In conventional FL settings, the parameters of the entire neural network are shared with the parameter server or, when using DFL, with other peers.
These parameters include all weights and biases of the input layer, hidden layers, and the output layer.
In Bristle, we avoid exchanging the full neural network between peers.
Instead, we leverage a popular ML technique called <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">deep transfer learning</em> which re-uses a neural network that has been trained on another dataset with comparable low-level features as the training data (step  <svg id="S3.SS1.p1.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS1.p1.1.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg> in Figure <a href="#S1.F3" title="Figure 3 ‣ 1. Introduction ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) <cite class="ltx_cite ltx_citemacro_citep">(Tan
et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2018</a>)</cite>.
More specifically, we copy and subsequently freeze the non-output layers from another model.
Bristle then only trains and exchanges the output layer, which has three major advantages:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Copying the non-output layers from a well-trained model significantly <em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">improves the convergence rate</em> when training the model in a decentralized fashion on devices with lower hardware capabilities.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Freezing non-output layers <em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">reduces communication overhead</em> since only the output layers have to be shared among peers.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Freezing the non-output layers is a key step towards <em id="S3.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">continual learning</em>, which further increases the performance and robustness of Bristle (see Section <a href="#S3.SS3" title="3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To bootstrap Bristle with a pre-trained model, we must assume that for the dataset we want to train on, there exists a vastly bigger dataset with roughly the same low-level features that we can use to pre-train a similar model.
Considering the extent to which transfer learning is nowadays used in practical learning problems, we argue that this assumption is realistic and does not prohibitively degrade the applicability of Bristle <cite class="ltx_cite ltx_citemacro_citep">(RN3, <a href="#bib.bib2" title="" class="ltx_ref">[n.d.]</a>; Pan, <a href="#bib.bib61" title="" class="ltx_ref">2010</a>)</cite>.
The training of the initial neural network can be performed offline by system designers or volunteers.
The pre-trained model can then be shared with peers by bundling them in the (mobile) application or be served by a trusted server that sends interested peers the pre-trained model upon request.
We assume that the developer knows the number of classes beforehand to determine the size of the output layer.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Distance-based prioritizer (DBP)</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We now elaborate on the distance-based prioritizer (DBP) in Bristle (step  <svg id="S3.SS2.p1.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS2.p1.1.pic1.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg> in Figure <a href="#S1.F3" title="Figure 3 ‣ 1. Introduction ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
The inputs to the DBP are the output layers received from other peers during the last training iteration.
To explain the motivation behind the DBP, we first examine Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2. Distance-based prioritizer (DBP) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> where the average Euclidean distance is shown from a non-i.i.d. model (configured as explained in Section <a href="#S5.SS3" title="5.3. Non-i.i.d. Classes ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>) to two benign models (shown in green), a benign model that is trained on an entirely different data distribution (shown in orange), and several Byzantine models (shown in red).
The distances in this figure are based on the models exchanged during our experiments (see Section <a href="#S5" title="5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
For each distance shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2. Distance-based prioritizer (DBP) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we took the average distance over 100 runs (with the current and the other model both trained ten times on different training data) and trained all models to convergence before comparing the distances.
The figure shows that based on the Euclidean distance alone, we cannot reliably determine if a certain model is benign because, in this example, the distance to a Trimmed Mean attack model is between the distances to two benign models.
However, the distance may clearly help prioritizing certain models since models with rather low or high distances appear to be more likely to be malicious.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2110.11006/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="323" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Average Euclidean distance from a given model to benign (green) and malicious (red) models, and models trained on an entirely different data distribution (orange).</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2110.11006/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_img_landscape" width="323" height="173" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.4.2.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S3.F5.2.1" class="ltx_text" style="font-size:90%;">Proportion of models sampled by the distance-based prioritizer from each of the three categories, based on the exploration-exploitation ratio <math id="S3.F5.2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F5.2.1.m1.1b"><mi id="S3.F5.2.1.m1.1.1" xref="S3.F5.2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F5.2.1.m1.1c"><ci id="S3.F5.2.1.m1.1.1.cmml" xref="S3.F5.2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.2.1.m1.1d">\alpha</annotation></semantics></math>.</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.13" class="ltx_p">Based on this insight, we segment the received models into three equally-sized categories with a low, medium, and high distance from our current model.
Then, we uniformly sample models from each of these categories.
The number of models sampled from each category depends on the exploration-exploitation ratio <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\alpha</annotation></semantics></math> where <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\alpha=0" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">α</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝛼</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\alpha=0</annotation></semantics></math> exclusively samples low-distance models (exploitation-dominant) and <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">α</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><eq id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></eq><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝛼</ci><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\alpha=1</annotation></semantics></math> exclusively samples high-distance models (exploration-dominant).
The optimal value of <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\alpha</annotation></semantics></math> depends on the maximum distance that benign models can reasonably have to each other, which is highly dependent on the degree of asynchrony (higher asynchrony <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mo stretchy="false" id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\rightarrow</annotation></semantics></math> higher <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\alpha</annotation></semantics></math>), the expected ratio of benign to Byzantine peers (more Byzantine peers <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mo stretchy="false" id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\rightarrow</annotation></semantics></math> lower <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mi id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><ci id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">\alpha</annotation></semantics></math>), and the degree of non-i.i.d.-ness (higher non-i.i.d.-ness <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><mo stretchy="false" id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><ci id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">\rightarrow</annotation></semantics></math> higher <math id="S3.SS2.p2.10.m10.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p2.10.m10.1a"><mi id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><ci id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">\alpha</annotation></semantics></math>).
The advantage of using three categories is that in situations where the accuracy is clearly sub-optimal and a significant number of Byzantine attackers are present, the developer may want to emphasize integrating models with a medium distance since these will often perform best.
<math id="S3.SS2.p2.11.m11.1" class="ltx_Math" alttext="f_{l}" display="inline"><semantics id="S3.SS2.p2.11.m11.1a"><msub id="S3.SS2.p2.11.m11.1.1" xref="S3.SS2.p2.11.m11.1.1.cmml"><mi id="S3.SS2.p2.11.m11.1.1.2" xref="S3.SS2.p2.11.m11.1.1.2.cmml">f</mi><mi id="S3.SS2.p2.11.m11.1.1.3" xref="S3.SS2.p2.11.m11.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m11.1b"><apply id="S3.SS2.p2.11.m11.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m11.1.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p2.11.m11.1.1.2.cmml" xref="S3.SS2.p2.11.m11.1.1.2">𝑓</ci><ci id="S3.SS2.p2.11.m11.1.1.3.cmml" xref="S3.SS2.p2.11.m11.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m11.1c">f_{l}</annotation></semantics></math>, <math id="S3.SS2.p2.12.m12.1" class="ltx_Math" alttext="f_{m}" display="inline"><semantics id="S3.SS2.p2.12.m12.1a"><msub id="S3.SS2.p2.12.m12.1.1" xref="S3.SS2.p2.12.m12.1.1.cmml"><mi id="S3.SS2.p2.12.m12.1.1.2" xref="S3.SS2.p2.12.m12.1.1.2.cmml">f</mi><mi id="S3.SS2.p2.12.m12.1.1.3" xref="S3.SS2.p2.12.m12.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m12.1b"><apply id="S3.SS2.p2.12.m12.1.1.cmml" xref="S3.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.12.m12.1.1.1.cmml" xref="S3.SS2.p2.12.m12.1.1">subscript</csymbol><ci id="S3.SS2.p2.12.m12.1.1.2.cmml" xref="S3.SS2.p2.12.m12.1.1.2">𝑓</ci><ci id="S3.SS2.p2.12.m12.1.1.3.cmml" xref="S3.SS2.p2.12.m12.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.12.m12.1c">f_{m}</annotation></semantics></math> and <math id="S3.SS2.p2.13.m13.1" class="ltx_Math" alttext="f_{h}" display="inline"><semantics id="S3.SS2.p2.13.m13.1a"><msub id="S3.SS2.p2.13.m13.1.1" xref="S3.SS2.p2.13.m13.1.1.cmml"><mi id="S3.SS2.p2.13.m13.1.1.2" xref="S3.SS2.p2.13.m13.1.1.2.cmml">f</mi><mi id="S3.SS2.p2.13.m13.1.1.3" xref="S3.SS2.p2.13.m13.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m13.1b"><apply id="S3.SS2.p2.13.m13.1.1.cmml" xref="S3.SS2.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.13.m13.1.1.1.cmml" xref="S3.SS2.p2.13.m13.1.1">subscript</csymbol><ci id="S3.SS2.p2.13.m13.1.1.2.cmml" xref="S3.SS2.p2.13.m13.1.1.2">𝑓</ci><ci id="S3.SS2.p2.13.m13.1.1.3.cmml" xref="S3.SS2.p2.13.m13.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.13.m13.1c">f_{h}</annotation></semantics></math> denote the fraction of samples from the low, medium and high distance categories, and are determined as follows:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.30" class="ltx_Math" alttext="\begin{gathered}f_{l}=(1-\alpha)^{2},f_{m}=-2\alpha^{2}+2\alpha,f_{h}=\alpha^{2}\\
\end{gathered}" display="block"><semantics id="S3.E1.m1.30a"><mtable displaystyle="true" id="S3.E1.m1.30.30.4"><mtr id="S3.E1.m1.30.30.4a"><mtd id="S3.E1.m1.30.30.4b"><mrow id="S3.E1.m1.30.30.4.28.28.28.28"><mrow id="S3.E1.m1.29.29.3.27.27.27.27.1"><msub id="S3.E1.m1.29.29.3.27.27.27.27.1.2"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">f</mi><mi id="S3.E1.m1.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.1.cmml">l</mi></msub><mo id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">=</mo><msup id="S3.E1.m1.29.29.3.27.27.27.27.1.1"><mrow id="S3.E1.m1.29.29.3.27.27.27.27.1.1.1.1"><mo stretchy="false" id="S3.E1.m1.4.4.4.4.4.4" xref="S3.E1.m1.28.28.2.3.cmml">(</mo><mrow id="S3.E1.m1.29.29.3.27.27.27.27.1.1.1.1.1"><mn id="S3.E1.m1.5.5.5.5.5.5" xref="S3.E1.m1.5.5.5.5.5.5.cmml">1</mn><mo id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.6.6.6.6.6.6.cmml">−</mo><mi id="S3.E1.m1.7.7.7.7.7.7" xref="S3.E1.m1.7.7.7.7.7.7.cmml">α</mi></mrow><mo stretchy="false" id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.28.28.2.3.cmml">)</mo></mrow><mn id="S3.E1.m1.9.9.9.9.9.9.1" xref="S3.E1.m1.9.9.9.9.9.9.1.cmml">2</mn></msup></mrow><mo id="S3.E1.m1.10.10.10.10.10.10" xref="S3.E1.m1.28.28.2.3.cmml">,</mo><mrow id="S3.E1.m1.30.30.4.28.28.28.28.2"><mrow id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1"><msub id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.1"><mi id="S3.E1.m1.11.11.11.11.11.11" xref="S3.E1.m1.11.11.11.11.11.11.cmml">f</mi><mi id="S3.E1.m1.12.12.12.12.12.12.1" xref="S3.E1.m1.12.12.12.12.12.12.1.cmml">m</mi></msub><mo id="S3.E1.m1.13.13.13.13.13.13" xref="S3.E1.m1.13.13.13.13.13.13.cmml">=</mo><mrow id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.2"><mrow id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.2.1"><mo id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.2.1a" xref="S3.E1.m1.28.28.2.3.cmml">−</mo><mrow id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.2.1.1"><mn id="S3.E1.m1.15.15.15.15.15.15" xref="S3.E1.m1.15.15.15.15.15.15.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.2.1.1.1" xref="S3.E1.m1.28.28.2.3.cmml">​</mo><msup id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.2.1.1.2"><mi id="S3.E1.m1.16.16.16.16.16.16" xref="S3.E1.m1.16.16.16.16.16.16.cmml">α</mi><mn id="S3.E1.m1.17.17.17.17.17.17.1" xref="S3.E1.m1.17.17.17.17.17.17.1.cmml">2</mn></msup></mrow></mrow><mo id="S3.E1.m1.18.18.18.18.18.18" xref="S3.E1.m1.18.18.18.18.18.18.cmml">+</mo><mrow id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.2.2"><mn id="S3.E1.m1.19.19.19.19.19.19" xref="S3.E1.m1.19.19.19.19.19.19.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.30.30.4.28.28.28.28.2.1.1.2.2.1" xref="S3.E1.m1.28.28.2.3.cmml">​</mo><mi id="S3.E1.m1.20.20.20.20.20.20" xref="S3.E1.m1.20.20.20.20.20.20.cmml">α</mi></mrow></mrow></mrow><mo id="S3.E1.m1.21.21.21.21.21.21" xref="S3.E1.m1.28.28.2.3.cmml">,</mo><mrow id="S3.E1.m1.30.30.4.28.28.28.28.2.2.2"><msub id="S3.E1.m1.30.30.4.28.28.28.28.2.2.2.1"><mi id="S3.E1.m1.22.22.22.22.22.22" xref="S3.E1.m1.22.22.22.22.22.22.cmml">f</mi><mi id="S3.E1.m1.23.23.23.23.23.23.1" xref="S3.E1.m1.23.23.23.23.23.23.1.cmml">h</mi></msub><mo id="S3.E1.m1.24.24.24.24.24.24" xref="S3.E1.m1.24.24.24.24.24.24.cmml">=</mo><msup id="S3.E1.m1.30.30.4.28.28.28.28.2.2.2.2"><mi id="S3.E1.m1.25.25.25.25.25.25" xref="S3.E1.m1.25.25.25.25.25.25.cmml">α</mi><mn id="S3.E1.m1.26.26.26.26.26.26.1" xref="S3.E1.m1.26.26.26.26.26.26.1.cmml">2</mn></msup></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.30b"><apply id="S3.E1.m1.28.28.2.3.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.2.3a.cmml" xref="S3.E1.m1.4.4.4.4.4.4">formulae-sequence</csymbol><apply id="S3.E1.m1.27.27.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><eq id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></eq><apply id="S3.E1.m1.27.27.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.27.27.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">𝑓</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1">𝑙</ci></apply><apply id="S3.E1.m1.27.27.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.27.27.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.4.4">superscript</csymbol><apply id="S3.E1.m1.27.27.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><minus id="S3.E1.m1.6.6.6.6.6.6.cmml" xref="S3.E1.m1.6.6.6.6.6.6"></minus><cn type="integer" id="S3.E1.m1.5.5.5.5.5.5.cmml" xref="S3.E1.m1.5.5.5.5.5.5">1</cn><ci id="S3.E1.m1.7.7.7.7.7.7.cmml" xref="S3.E1.m1.7.7.7.7.7.7">𝛼</ci></apply><cn type="integer" id="S3.E1.m1.9.9.9.9.9.9.1.cmml" xref="S3.E1.m1.9.9.9.9.9.9.1">2</cn></apply></apply><apply id="S3.E1.m1.28.28.2.2.2.3.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.2.2.2.3a.cmml" xref="S3.E1.m1.4.4.4.4.4.4">formulae-sequence</csymbol><apply id="S3.E1.m1.28.28.2.2.2.1.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><eq id="S3.E1.m1.13.13.13.13.13.13.cmml" xref="S3.E1.m1.13.13.13.13.13.13"></eq><apply id="S3.E1.m1.28.28.2.2.2.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.2.2.2.1.1.2.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4">subscript</csymbol><ci id="S3.E1.m1.11.11.11.11.11.11.cmml" xref="S3.E1.m1.11.11.11.11.11.11">𝑓</ci><ci id="S3.E1.m1.12.12.12.12.12.12.1.cmml" xref="S3.E1.m1.12.12.12.12.12.12.1">𝑚</ci></apply><apply id="S3.E1.m1.28.28.2.2.2.1.1.3.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><plus id="S3.E1.m1.18.18.18.18.18.18.cmml" xref="S3.E1.m1.18.18.18.18.18.18"></plus><apply id="S3.E1.m1.28.28.2.2.2.1.1.3.2.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><minus id="S3.E1.m1.14.14.14.14.14.14.cmml" xref="S3.E1.m1.4.4.4.4.4.4"></minus><apply id="S3.E1.m1.28.28.2.2.2.1.1.3.2.2.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><times id="S3.E1.m1.28.28.2.2.2.1.1.3.2.2.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4"></times><cn type="integer" id="S3.E1.m1.15.15.15.15.15.15.cmml" xref="S3.E1.m1.15.15.15.15.15.15">2</cn><apply id="S3.E1.m1.28.28.2.2.2.1.1.3.2.2.3.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.2.2.2.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4">superscript</csymbol><ci id="S3.E1.m1.16.16.16.16.16.16.cmml" xref="S3.E1.m1.16.16.16.16.16.16">𝛼</ci><cn type="integer" id="S3.E1.m1.17.17.17.17.17.17.1.cmml" xref="S3.E1.m1.17.17.17.17.17.17.1">2</cn></apply></apply></apply><apply id="S3.E1.m1.28.28.2.2.2.1.1.3.3.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><times id="S3.E1.m1.28.28.2.2.2.1.1.3.3.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4"></times><cn type="integer" id="S3.E1.m1.19.19.19.19.19.19.cmml" xref="S3.E1.m1.19.19.19.19.19.19">2</cn><ci id="S3.E1.m1.20.20.20.20.20.20.cmml" xref="S3.E1.m1.20.20.20.20.20.20">𝛼</ci></apply></apply></apply><apply id="S3.E1.m1.28.28.2.2.2.2.2.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><eq id="S3.E1.m1.24.24.24.24.24.24.cmml" xref="S3.E1.m1.24.24.24.24.24.24"></eq><apply id="S3.E1.m1.28.28.2.2.2.2.2.2.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4">subscript</csymbol><ci id="S3.E1.m1.22.22.22.22.22.22.cmml" xref="S3.E1.m1.22.22.22.22.22.22">𝑓</ci><ci id="S3.E1.m1.23.23.23.23.23.23.1.cmml" xref="S3.E1.m1.23.23.23.23.23.23.1">ℎ</ci></apply><apply id="S3.E1.m1.28.28.2.2.2.2.2.3.cmml" xref="S3.E1.m1.4.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.E1.m1.28.28.2.2.2.2.2.3.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4">superscript</csymbol><ci id="S3.E1.m1.25.25.25.25.25.25.cmml" xref="S3.E1.m1.25.25.25.25.25.25">𝛼</ci><cn type="integer" id="S3.E1.m1.26.26.26.26.26.26.1.cmml" xref="S3.E1.m1.26.26.26.26.26.26.1">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.30c">\begin{gathered}f_{l}=(1-\alpha)^{2},f_{m}=-2\alpha^{2}+2\alpha,f_{h}=\alpha^{2}\\
\end{gathered}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.4" class="ltx_p">Note that <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="f_{l}+f_{m}+f_{h}=1" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mrow id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml"><msub id="S3.SS2.p4.1.m1.1.1.2.2" xref="S3.SS2.p4.1.m1.1.1.2.2.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2.2.2" xref="S3.SS2.p4.1.m1.1.1.2.2.2.cmml">f</mi><mi id="S3.SS2.p4.1.m1.1.1.2.2.3" xref="S3.SS2.p4.1.m1.1.1.2.2.3.cmml">l</mi></msub><mo id="S3.SS2.p4.1.m1.1.1.2.1" xref="S3.SS2.p4.1.m1.1.1.2.1.cmml">+</mo><msub id="S3.SS2.p4.1.m1.1.1.2.3" xref="S3.SS2.p4.1.m1.1.1.2.3.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2.3.2" xref="S3.SS2.p4.1.m1.1.1.2.3.2.cmml">f</mi><mi id="S3.SS2.p4.1.m1.1.1.2.3.3" xref="S3.SS2.p4.1.m1.1.1.2.3.3.cmml">m</mi></msub><mo id="S3.SS2.p4.1.m1.1.1.2.1a" xref="S3.SS2.p4.1.m1.1.1.2.1.cmml">+</mo><msub id="S3.SS2.p4.1.m1.1.1.2.4" xref="S3.SS2.p4.1.m1.1.1.2.4.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2.4.2" xref="S3.SS2.p4.1.m1.1.1.2.4.2.cmml">f</mi><mi id="S3.SS2.p4.1.m1.1.1.2.4.3" xref="S3.SS2.p4.1.m1.1.1.2.4.3.cmml">h</mi></msub></mrow><mo id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><eq id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1"></eq><apply id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2"><plus id="S3.SS2.p4.1.m1.1.1.2.1.cmml" xref="S3.SS2.p4.1.m1.1.1.2.1"></plus><apply id="S3.SS2.p4.1.m1.1.1.2.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.2.2.1.cmml" xref="S3.SS2.p4.1.m1.1.1.2.2">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.2.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2.2.2">𝑓</ci><ci id="S3.SS2.p4.1.m1.1.1.2.2.3.cmml" xref="S3.SS2.p4.1.m1.1.1.2.2.3">𝑙</ci></apply><apply id="S3.SS2.p4.1.m1.1.1.2.3.cmml" xref="S3.SS2.p4.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.2.3">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2.3.2">𝑓</ci><ci id="S3.SS2.p4.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.p4.1.m1.1.1.2.3.3">𝑚</ci></apply><apply id="S3.SS2.p4.1.m1.1.1.2.4.cmml" xref="S3.SS2.p4.1.m1.1.1.2.4"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.2.4.1.cmml" xref="S3.SS2.p4.1.m1.1.1.2.4">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.4.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2.4.2">𝑓</ci><ci id="S3.SS2.p4.1.m1.1.1.2.4.3.cmml" xref="S3.SS2.p4.1.m1.1.1.2.4.3">ℎ</ci></apply></apply><cn type="integer" id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">f_{l}+f_{m}+f_{h}=1</annotation></semantics></math>.
Figure <a href="#S3.F5" title="Figure 5 ‣ 3.2. Distance-based prioritizer (DBP) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> visualizes the relation between the above values and <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\alpha</annotation></semantics></math>.
We also parameterize the maximum number of models that pass our DBP, denoted by <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\beta</annotation></semantics></math>.
Since the runtime of our performance-based integrator scales with the input size, devices with lower performance can opt for a lower value for <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">\beta</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Performance-based Integrator (PBI)</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The ability of our distance-based prioritizer to recognize Byzantine or <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">stale</em> models that are trained on a lower number of iterations is rather limited.
Byzantine attacks are easy to launch in open networks, and stale models are a regular phenomenon in FL systems <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020b</a>)</cite>.
To address these concerns, we assess the performance of each model that passes the distance-based prioritizer on a small <em id="S3.SS3.p1.1.2" class="ltx_emph ltx_font_italic">test dataset</em> (step  <svg id="S3.SS3.p1.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS3.p1.1.pic1.1.1.1.1.1" class="ltx_text">4</span></foreignObject></g></g></svg> in Figure <a href="#S1.F3" title="Figure 3 ‣ 1. Introduction ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) using a performance-based integrator (PBI).
This test dataset is a random subset of the full private dataset owned by the peer.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2110.11006/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_img_landscape" width="461" height="271" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">The per-class PBI in Bristle. Dashed boxes refer to values associated with foreign classes.</span></figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.7" class="ltx_p">Figure <a href="#S3.F6" title="Figure 6 ‣ 3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows how the PBI works, given a model with three classes (<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">A</annotation></semantics></math>, <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">B</annotation></semantics></math> and <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">C</annotation></semantics></math>).
In summary, the PBI selectively integrates the parameters of classes that perform well to achieve Byzantine-resilience and to handle non-i.i.d. classes properly.
This is different from existing approaches that integrate each received model as a whole instead.
We refer to the parameters in the output layer that are connected to a particular class as <em id="S3.SS3.p2.7.1" class="ltx_emph ltx_font_italic">Class-specific parameters (CSPs)</em>.
The integration process in Bristle first evaluates the performance of each class of each received model on a test dataset (step  <svg id="S3.SS3.p2.4.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS3.p2.4.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg>).
Based on this evaluation, we compute a <em id="S3.SS3.p2.7.2" class="ltx_emph ltx_font_italic">certainty</em> score that estimates how benign a received model is (Step  <svg id="S3.SS3.p2.5.pic2" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS3.p2.5.pic2.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>).
Then, we calculate for all CSPs of each model a weight (step  <svg id="S3.SS3.p2.6.pic3" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS3.p2.6.pic3.1.1.1.1.1" class="ltx_text">3</span></foreignObject></g></g></svg>).
Finally, we integrate the received models into our current model, based on the computed weights (step  <svg id="S3.SS3.p2.7.pic4" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.SS3.p2.7.pic4.1.1.1.1.1" class="ltx_text">4</span></foreignObject></g></g></svg>).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The selective integration of parameters is inspired by the continual learning algorithm CWR* <cite class="ltx_cite ltx_citemacro_citep">(Lomonaco, <a href="#bib.bib53" title="" class="ltx_ref">2019</a>)</cite> that enables non-i.i.d. learning by initializing the output layer to zero, applying a mean-shift, and replicating the hippocampus-cortex duality by selectively copying and resetting parts of the output layer.
Related work demonstrates that this approach provides excellent performance in regular non-FL environments and enables per-class updates <cite class="ltx_cite ltx_citemacro_citep">(Lomonaco, <a href="#bib.bib53" title="" class="ltx_ref">2019</a>)</cite>.
Applying continual learning requires that the non-output layers are frozen and equal between all peers.
We already addressed this by using transfer learning (see Section <a href="#S3.SS1" title="3.1. Bootstrapping Bristle with Transfer Learning ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>) to train these layers before the first iteration.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">We now elaborate on the PBI logic, which pseudocode is given in Listing <a href="#alg1" title="Algorithm 1 ‣ 3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The input for this algorithm is the current model of the peer (<em id="S3.SS3.p4.1.1" class="ltx_emph ltx_font_italic">myModel</em>), and the prioritized models that passed the DBP (<em id="S3.SS3.p4.1.2" class="ltx_emph ltx_font_italic">prioritizedModels</em>).</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_float"><span id="alg1.4.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Bristle’s performance-based integrator (PBI)</figcaption>
<div id="alg1.5" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="alg1.l1.2" class="ltx_text ltx_font_bold" style="font-size:80%;">procedure</span><span id="alg1.l1.3" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l1.4" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">PBI</span><span id="alg1.l1.5" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l1.6" class="ltx_emph ltx_font_italic" style="font-size:80%;">myModel</em><span id="alg1.l1.7" class="ltx_text" style="font-size:80%;">, </span><em id="alg1.l1.8" class="ltx_emph ltx_font_italic" style="font-size:80%;">prioritizedModels</em><span id="alg1.l1.9" class="ltx_text" style="font-size:80%;">)
</span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text" style="font-size:80%;">    </span><em id="alg1.l2.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">F1</em><span id="alg1.l2.4" class="ltx_text" style="font-size:80%;"> </span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l2.m1.1a"><mo mathsize="80%" stretchy="false" id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l2.5" class="ltx_text" style="font-size:80%;"> [], </span><em id="alg1.l2.6" class="ltx_emph ltx_font_italic" style="font-size:80%;">certainty</em><span id="alg1.l2.7" class="ltx_text" style="font-size:80%;"> </span><math id="alg1.l2.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l2.m2.1a"><mo mathsize="80%" stretchy="false" id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b"><ci id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m2.1c">\leftarrow</annotation></semantics></math><span id="alg1.l2.8" class="ltx_text" style="font-size:80%;"> [], </span><em id="alg1.l2.9" class="ltx_emph ltx_font_italic" style="font-size:80%;">disc</em><span id="alg1.l2.10" class="ltx_text" style="font-size:80%;"> </span><math id="alg1.l2.m3.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l2.m3.1a"><mo mathsize="80%" stretchy="false" id="alg1.l2.m3.1.1" xref="alg1.l2.m3.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l2.m3.1b"><ci id="alg1.l2.m3.1.1.cmml" xref="alg1.l2.m3.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m3.1c">\leftarrow</annotation></semantics></math><span id="alg1.l2.11" class="ltx_text" style="font-size:80%;"> []
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.2.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3.3" class="ltx_text" style="font-size:80%;">    </span><em id="alg1.l3.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">faWg</em><span id="alg1.l3.5" class="ltx_text" style="font-size:80%;"> </span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l3.m1.1a"><mo mathsize="80%" stretchy="false" id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l3.6" class="ltx_text" style="font-size:80%;"> [], </span><em id="alg1.l3.7" class="ltx_emph ltx_font_italic" style="font-size:80%;">foWg</em><span id="alg1.l3.8" class="ltx_text" style="font-size:80%;"> </span><math id="alg1.l3.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l3.m2.1a"><mo mathsize="80%" stretchy="false" id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b"><ci id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.1c">\leftarrow</annotation></semantics></math><span id="alg1.l3.9" class="ltx_text" style="font-size:80%;"> [] </span><span id="alg1.l3.1" class="ltx_text" style="font-size:80%;float:right;"><math id="alg1.l3.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l3.1.m1.1a"><mo id="alg1.l3.1.m1.1.1" xref="alg1.l3.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l3.1.m1.1b"><ci id="alg1.l3.1.m1.1.1.cmml" xref="alg1.l3.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.1.m1.1c">\triangleright</annotation></semantics></math> foreign/familiar class weights
</span>
</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.2.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><span id="alg1.l4.3" class="ltx_text" style="font-size:80%;">    </span><span id="alg1.l4.4" class="ltx_text ltx_font_bold" style="font-size:80%;">for</span><span id="alg1.l4.5" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l4.6" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l4.7" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l4.8" class="ltx_text ltx_font_bold" style="font-size:80%;">in</span><span id="alg1.l4.9" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l4.10" class="ltx_emph ltx_font_italic" style="font-size:80%;">myModel</em><span id="alg1.l4.11" class="ltx_text" style="font-size:80%;"> </span><math id="alg1.l4.m1.1" class="ltx_Math" alttext="\cup" display="inline"><semantics id="alg1.l4.m1.1a"><mo mathsize="80%" id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">∪</mo><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><union id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"></union></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">\cup</annotation></semantics></math><span id="alg1.l4.12" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l4.13" class="ltx_emph ltx_font_italic" style="font-size:80%;">prioritizedModels</em><span id="alg1.l4.14" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l4.15" class="ltx_text ltx_font_bold" style="font-size:80%;">do</span><span id="alg1.l4.16" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l4.1" class="ltx_text" style="font-size:80%;float:right;"><math id="alg1.l4.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l4.1.m1.1a"><mo id="alg1.l4.1.m1.1.1" xref="alg1.l4.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l4.1.m1.1b"><ci id="alg1.l4.1.m1.1.1.cmml" xref="alg1.l4.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.1.m1.1c">\triangleright</annotation></semantics></math> <span id="alg1.l4.1.1" class="ltx_text ltx_font_bold">Step 1</span>
</span>
</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><span id="alg1.l5.2" class="ltx_text" style="font-size:80%;">        </span><span id="alg1.l5.3" class="ltx_text ltx_font_bold" style="font-size:80%;">for</span><span id="alg1.l5.4" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l5.5" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l5.6" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l5.7" class="ltx_text ltx_font_bold" style="font-size:80%;">in</span><span id="alg1.l5.8" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l5.9" class="ltx_text ltx_font_typewriter" style="font-size:80%;">familiarClasses</span><span id="alg1.l5.10" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l5.11" class="ltx_emph ltx_font_italic" style="font-size:80%;">data</em><span id="alg1.l5.12" class="ltx_text" style="font-size:80%;">) </span><span id="alg1.l5.13" class="ltx_text ltx_font_bold" style="font-size:80%;">do</span><span id="alg1.l5.14" class="ltx_text" style="font-size:80%;">
</span>
</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg1.l6.2" class="ltx_text" style="font-size:80%;">           </span><em id="alg1.l6.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">F1</em><span id="alg1.l6.4" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l6.5" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l6.6" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l6.7" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l6.8" class="ltx_text" style="font-size:80%;">] </span><math id="alg1.l6.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l6.m1.1a"><mo mathsize="80%" stretchy="false" id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><ci id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l6.9" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l6.10" class="ltx_text ltx_font_typewriter" style="font-size:80%;">evaluate</span><span id="alg1.l6.11" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l6.12" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l6.13" class="ltx_text" style="font-size:80%;">, </span><em id="alg1.l6.14" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l6.15" class="ltx_text" style="font-size:80%;">, </span><em id="alg1.l6.16" class="ltx_emph ltx_font_italic" style="font-size:80%;">testData</em><span id="alg1.l6.17" class="ltx_text" style="font-size:80%;">)
</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><span id="alg1.l7.2" class="ltx_text" style="font-size:80%;">        </span><span id="alg1.l7.3" class="ltx_text ltx_font_bold" style="font-size:80%;">end</span><span id="alg1.l7.4" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l7.5" class="ltx_text ltx_font_bold" style="font-size:80%;">for</span>
</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg1.l8.2" class="ltx_text" style="font-size:80%;">    </span><span id="alg1.l8.3" class="ltx_text ltx_font_bold" style="font-size:80%;">end</span><span id="alg1.l8.4" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l8.5" class="ltx_text ltx_font_bold" style="font-size:80%;">for</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text" style="font-size:80%;">    </span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><span id="alg1.l10.2" class="ltx_text" style="font-size:80%;">    </span><span id="alg1.l10.3" class="ltx_text ltx_font_bold" style="font-size:80%;">for</span><span id="alg1.l10.4" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l10.5" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l10.6" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l10.7" class="ltx_text ltx_font_bold" style="font-size:80%;">in</span><span id="alg1.l10.8" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l10.9" class="ltx_emph ltx_font_italic" style="font-size:80%;">prioritizedModels</em><span id="alg1.l10.10" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l10.11" class="ltx_text ltx_font_bold" style="font-size:80%;">do</span><span id="alg1.l10.12" class="ltx_text" style="font-size:80%;">
</span>
</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span><span id="alg1.l11.2" class="ltx_text" style="font-size:80%;">        </span><em id="alg1.l11.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">best</em><span id="alg1.l11.4" class="ltx_text" style="font-size:80%;"> </span><math id="alg1.l11.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l11.m1.1a"><mo mathsize="80%" stretchy="false" id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l11.5" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l11.6" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sorted</span><span id="alg1.l11.7" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l11.8" class="ltx_emph ltx_font_italic" style="font-size:80%;">F1</em><span id="alg1.l11.9" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l11.10" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l11.11" class="ltx_text" style="font-size:80%;">]) [:</span><math id="alg1.l11.m2.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="alg1.l11.m2.1a"><mi mathsize="80%" id="alg1.l11.m2.1.1" xref="alg1.l11.m2.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="alg1.l11.m2.1b"><ci id="alg1.l11.m2.1.1.cmml" xref="alg1.l11.m2.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m2.1c">\phi</annotation></semantics></math><span id="alg1.l11.12" class="ltx_text" style="font-size:80%;">]
</span>
</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.2.1.1" class="ltx_text" style="font-size:80%;">12:</span></span><span id="alg1.l12.3" class="ltx_text" style="font-size:80%;">        </span><em id="alg1.l12.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">certainty</em><span id="alg1.l12.5" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l12.6" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l12.7" class="ltx_text" style="font-size:80%;">] </span><math id="alg1.l12.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l12.m1.1a"><mo mathsize="80%" stretchy="false" id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><ci id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l12.8" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l12.9" class="ltx_emph ltx_font_italic" style="font-size:80%;">max</em><span id="alg1.l12.10" class="ltx_text" style="font-size:80%;">(</span><span id="alg1.l12.11" class="ltx_text ltx_font_typewriter" style="font-size:80%;">avg</span><span id="alg1.l12.12" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l12.13" class="ltx_emph ltx_font_italic" style="font-size:80%;">best</em><span id="alg1.l12.14" class="ltx_text" style="font-size:80%;">) - </span><span id="alg1.l12.15" class="ltx_text ltx_font_typewriter" style="font-size:80%;">std</span><span id="alg1.l12.16" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l12.17" class="ltx_emph ltx_font_italic" style="font-size:80%;">best</em><span id="alg1.l12.18" class="ltx_text" style="font-size:80%;">), 0) </span><span id="alg1.l12.1" class="ltx_text" style="font-size:80%;float:right;"><math id="alg1.l12.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l12.1.m1.1a"><mo id="alg1.l12.1.m1.1.1" xref="alg1.l12.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l12.1.m1.1b"><ci id="alg1.l12.1.m1.1.1.cmml" xref="alg1.l12.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.1.m1.1c">\triangleright</annotation></semantics></math> <span id="alg1.l12.1.1" class="ltx_text ltx_font_bold">Step 2</span>
</span>
</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.2.1.1" class="ltx_text" style="font-size:80%;">13:</span></span><span id="alg1.l13.3" class="ltx_text" style="font-size:80%;">        </span><span id="alg1.l13.4" class="ltx_text ltx_font_bold" style="font-size:80%;">for</span><span id="alg1.l13.5" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l13.6" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l13.7" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l13.8" class="ltx_text ltx_font_bold" style="font-size:80%;">in</span><span id="alg1.l13.9" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l13.10" class="ltx_text ltx_font_typewriter" style="font-size:80%;">familiarClasses</span><span id="alg1.l13.11" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l13.12" class="ltx_emph ltx_font_italic" style="font-size:80%;">data</em><span id="alg1.l13.13" class="ltx_text" style="font-size:80%;">) </span><span id="alg1.l13.14" class="ltx_text ltx_font_bold" style="font-size:80%;">do</span><span id="alg1.l13.15" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l13.1" class="ltx_text" style="font-size:80%;float:right;"><math id="alg1.l13.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l13.1.m1.1a"><mo id="alg1.l13.1.m1.1.1" xref="alg1.l13.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l13.1.m1.1b"><ci id="alg1.l13.1.m1.1.1.cmml" xref="alg1.l13.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.1.m1.1c">\triangleright</annotation></semantics></math> <span id="alg1.l13.1.1" class="ltx_text ltx_font_bold">Step 3</span>
</span>
</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span><span id="alg1.l14.2" class="ltx_text" style="font-size:80%;">           </span><span id="alg1.l14.3" class="ltx_text ltx_font_bold" style="font-size:80%;">if</span><span id="alg1.l14.4" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l14.5" class="ltx_emph ltx_font_italic" style="font-size:80%;">F1</em><span id="alg1.l14.6" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l14.7" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l14.8" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l14.9" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l14.10" class="ltx_text" style="font-size:80%;">] </span><math id="alg1.l14.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="alg1.l14.m1.1a"><mo mathsize="80%" id="alg1.l14.m1.1.1" xref="alg1.l14.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="alg1.l14.m1.1b"><geq id="alg1.l14.m1.1.1.cmml" xref="alg1.l14.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m1.1c">\geq</annotation></semantics></math><span id="alg1.l14.11" class="ltx_text" style="font-size:80%;"> </span><em id="alg1.l14.12" class="ltx_emph ltx_font_italic" style="font-size:80%;">F1</em><span id="alg1.l14.13" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l14.14" class="ltx_emph ltx_font_italic" style="font-size:80%;">myModel</em><span id="alg1.l14.15" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l14.16" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l14.17" class="ltx_text" style="font-size:80%;">] </span><span id="alg1.l14.18" class="ltx_text ltx_font_bold" style="font-size:80%;">then</span><span id="alg1.l14.19" class="ltx_text" style="font-size:80%;">
</span>
</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span><span id="alg1.l15.2" class="ltx_text" style="font-size:80%;">               </span><em id="alg1.l15.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">disc</em><span id="alg1.l15.4" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l15.5" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l15.6" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l15.7" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l15.8" class="ltx_text" style="font-size:80%;">] </span><math id="alg1.l15.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l15.m1.1a"><mo mathsize="80%" stretchy="false" id="alg1.l15.m1.1.1" xref="alg1.l15.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l15.m1.1b"><ci id="alg1.l15.m1.1.1.cmml" xref="alg1.l15.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l15.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l15.9" class="ltx_text" style="font-size:80%;"> (—</span><em id="alg1.l15.10" class="ltx_emph ltx_font_italic" style="font-size:80%;">F1</em><span id="alg1.l15.11" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l15.12" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l15.13" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l15.14" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l15.15" class="ltx_text" style="font-size:80%;">] - </span><em id="alg1.l15.16" class="ltx_emph ltx_font_italic" style="font-size:80%;">F1</em><span id="alg1.l15.17" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l15.18" class="ltx_emph ltx_font_italic" style="font-size:80%;">myModel</em><span id="alg1.l15.19" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l15.20" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l15.21" class="ltx_text" style="font-size:80%;">]— * </span><math id="alg1.l15.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg1.l15.m2.1a"><mi mathsize="80%" id="alg1.l15.m2.1.1" xref="alg1.l15.m2.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="alg1.l15.m2.1b"><ci id="alg1.l15.m2.1.1.cmml" xref="alg1.l15.m2.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l15.m2.1c">\eta</annotation></semantics></math><span id="alg1.l15.22" class="ltx_text" style="font-size:80%;">)</span><sup id="alg1.l15.23" class="ltx_sup"><span id="alg1.l15.23.1" class="ltx_text" style="font-size:80%;">3</span></sup><span id="alg1.l15.24" class="ltx_text" style="font-size:80%;">
</span>
</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span><span id="alg1.l16.2" class="ltx_text" style="font-size:80%;">           </span><span id="alg1.l16.3" class="ltx_text ltx_font_bold" style="font-size:80%;">else</span>
</div>
<div id="alg1.l17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l17.1.1.1" class="ltx_text" style="font-size:80%;">17:</span></span><span id="alg1.l17.2" class="ltx_text" style="font-size:80%;">               </span><em id="alg1.l17.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">disc</em><span id="alg1.l17.4" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l17.5" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l17.6" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l17.7" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l17.8" class="ltx_text" style="font-size:80%;">] </span><math id="alg1.l17.m1.1" class="ltx_Math" alttext="\leftarrow-\infty" display="inline"><semantics id="alg1.l17.m1.1a"><mrow id="alg1.l17.m1.1.1" xref="alg1.l17.m1.1.1.cmml"><mi id="alg1.l17.m1.1.1.2" xref="alg1.l17.m1.1.1.2.cmml"></mi><mo mathsize="80%" stretchy="false" id="alg1.l17.m1.1.1.1" xref="alg1.l17.m1.1.1.1.cmml">←</mo><mrow id="alg1.l17.m1.1.1.3" xref="alg1.l17.m1.1.1.3.cmml"><mo mathsize="80%" id="alg1.l17.m1.1.1.3a" xref="alg1.l17.m1.1.1.3.cmml">−</mo><mi mathsize="80%" mathvariant="normal" id="alg1.l17.m1.1.1.3.2" xref="alg1.l17.m1.1.1.3.2.cmml">∞</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l17.m1.1b"><apply id="alg1.l17.m1.1.1.cmml" xref="alg1.l17.m1.1.1"><ci id="alg1.l17.m1.1.1.1.cmml" xref="alg1.l17.m1.1.1.1">←</ci><csymbol cd="latexml" id="alg1.l17.m1.1.1.2.cmml" xref="alg1.l17.m1.1.1.2">absent</csymbol><apply id="alg1.l17.m1.1.1.3.cmml" xref="alg1.l17.m1.1.1.3"><minus id="alg1.l17.m1.1.1.3.1.cmml" xref="alg1.l17.m1.1.1.3"></minus><infinity id="alg1.l17.m1.1.1.3.2.cmml" xref="alg1.l17.m1.1.1.3.2"></infinity></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l17.m1.1c">\leftarrow-\infty</annotation></semantics></math><span id="alg1.l17.9" class="ltx_text" style="font-size:80%;">
</span>
</div>
<div id="alg1.l18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l18.1.1.1" class="ltx_text" style="font-size:80%;">18:</span></span><span id="alg1.l18.2" class="ltx_text" style="font-size:80%;">           </span><span id="alg1.l18.3" class="ltx_text ltx_font_bold" style="font-size:80%;">end</span><span id="alg1.l18.4" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l18.5" class="ltx_text ltx_font_bold" style="font-size:80%;">if</span>
</div>
<div id="alg1.l19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l19.1.1.1" class="ltx_text" style="font-size:80%;">19:</span></span><span id="alg1.l19.2" class="ltx_text" style="font-size:80%;">           </span><em id="alg1.l19.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">faWg</em><span id="alg1.l19.4" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l19.5" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l19.6" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l19.7" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l19.8" class="ltx_text" style="font-size:80%;">] </span><math id="alg1.l19.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l19.m1.1a"><mo mathsize="80%" stretchy="false" id="alg1.l19.m1.1.1" xref="alg1.l19.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l19.m1.1b"><ci id="alg1.l19.m1.1.1.cmml" xref="alg1.l19.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l19.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l19.9" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l19.10" class="ltx_text ltx_font_typewriter" style="font-size:80%;">computeFaWg</span><span id="alg1.l19.11" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l19.12" class="ltx_emph ltx_font_italic" style="font-size:80%;">disc</em><span id="alg1.l19.13" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l19.14" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l19.15" class="ltx_text" style="font-size:80%;">][</span><em id="alg1.l19.16" class="ltx_emph ltx_font_italic" style="font-size:80%;">c</em><span id="alg1.l19.17" class="ltx_text" style="font-size:80%;">], </span><em id="alg1.l19.18" class="ltx_emph ltx_font_italic" style="font-size:80%;">certainty[<em id="alg1.l19.18.1" class="ltx_emph ltx_font_upright">m</em>]</em><span id="alg1.l19.19" class="ltx_text" style="font-size:80%;">)
</span>
</div>
<div id="alg1.l20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l20.1.1.1" class="ltx_text" style="font-size:80%;">20:</span></span><span id="alg1.l20.2" class="ltx_text" style="font-size:80%;">        </span><span id="alg1.l20.3" class="ltx_text ltx_font_bold" style="font-size:80%;">end</span><span id="alg1.l20.4" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l20.5" class="ltx_text ltx_font_bold" style="font-size:80%;">for</span>
</div>
<div id="alg1.l21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l21.1.1.1" class="ltx_text" style="font-size:80%;">21:</span></span><span id="alg1.l21.2" class="ltx_text" style="font-size:80%;">        </span><em id="alg1.l21.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">foWg</em><span id="alg1.l21.4" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l21.5" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l21.6" class="ltx_text" style="font-size:80%;">] </span><math id="alg1.l21.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l21.m1.1a"><mo mathsize="80%" stretchy="false" id="alg1.l21.m1.1.1" xref="alg1.l21.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l21.m1.1b"><ci id="alg1.l21.m1.1.1.cmml" xref="alg1.l21.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l21.7" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l21.8" class="ltx_text ltx_font_typewriter" style="font-size:80%;">computeFoWg</span><span id="alg1.l21.9" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l21.10" class="ltx_emph ltx_font_italic" style="font-size:80%;">disc</em><span id="alg1.l21.11" class="ltx_text" style="font-size:80%;">[</span><em id="alg1.l21.12" class="ltx_emph ltx_font_italic" style="font-size:80%;">m</em><span id="alg1.l21.13" class="ltx_text" style="font-size:80%;">], </span><em id="alg1.l21.14" class="ltx_emph ltx_font_italic" style="font-size:80%;">certainty[<em id="alg1.l21.14.1" class="ltx_emph ltx_font_upright">m</em>]</em><span id="alg1.l21.15" class="ltx_text" style="font-size:80%;">)
</span>
</div>
<div id="alg1.l22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l22.1.1.1" class="ltx_text" style="font-size:80%;">22:</span></span><span id="alg1.l22.2" class="ltx_text" style="font-size:80%;">    </span><span id="alg1.l22.3" class="ltx_text ltx_font_bold" style="font-size:80%;">end</span><span id="alg1.l22.4" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l22.5" class="ltx_text ltx_font_bold" style="font-size:80%;">for</span>
</div>
<div id="alg1.l23" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l23.2.1.1" class="ltx_text" style="font-size:80%;">23:</span></span><span id="alg1.l23.3" class="ltx_text" style="font-size:80%;">    </span><span id="alg1.l23.1" class="ltx_text" style="font-size:80%;float:right;"><math id="alg1.l23.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l23.1.m1.1a"><mo id="alg1.l23.1.m1.1.1" xref="alg1.l23.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l23.1.m1.1b"><ci id="alg1.l23.1.m1.1.1.cmml" xref="alg1.l23.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l23.1.m1.1c">\triangleright</annotation></semantics></math> <span id="alg1.l23.1.1" class="ltx_text ltx_font_bold">Step 4</span>
</span>
</div>
<div id="alg1.l24" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l24.1.1.1" class="ltx_text" style="font-size:80%;">24:</span></span><span id="alg1.l24.2" class="ltx_text" style="font-size:80%;">    </span><em id="alg1.l24.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">myModel</em><span id="alg1.l24.4" class="ltx_text" style="font-size:80%;">.</span><span id="alg1.l24.5" class="ltx_text ltx_font_typewriter" style="font-size:80%;">integrateFamiliarClasses</span><span id="alg1.l24.6" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l24.7" class="ltx_emph ltx_font_italic" style="font-size:80%;">prioritizedModels</em><span id="alg1.l24.8" class="ltx_text" style="font-size:80%;">, </span><em id="alg1.l24.9" class="ltx_emph ltx_font_italic" style="font-size:80%;">faWg</em><span id="alg1.l24.10" class="ltx_text" style="font-size:80%;">)
</span>
</div>
<div id="alg1.l25" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l25.1.1.1" class="ltx_text" style="font-size:80%;">25:</span></span><span id="alg1.l25.2" class="ltx_text" style="font-size:80%;">    </span><em id="alg1.l25.3" class="ltx_emph ltx_font_italic" style="font-size:80%;">myModel</em><span id="alg1.l25.4" class="ltx_text" style="font-size:80%;">.</span><span id="alg1.l25.5" class="ltx_text ltx_font_typewriter" style="font-size:80%;">integrateForeignClasses</span><span id="alg1.l25.6" class="ltx_text" style="font-size:80%;">(</span><em id="alg1.l25.7" class="ltx_emph ltx_font_italic" style="font-size:80%;">prioritizedModels</em><span id="alg1.l25.8" class="ltx_text" style="font-size:80%;">, </span><em id="alg1.l25.9" class="ltx_emph ltx_font_italic" style="font-size:80%;">foWg</em><span id="alg1.l25.10" class="ltx_text" style="font-size:80%;">)
</span>
</div>
<div id="alg1.l26" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l26.1.1.1" class="ltx_text" style="font-size:80%;">26:</span></span><span id="alg1.l26.2" class="ltx_text ltx_font_bold" style="font-size:80%;">end</span><span id="alg1.l26.3" class="ltx_text" style="font-size:80%;"> </span><span id="alg1.l26.4" class="ltx_text ltx_font_bold" style="font-size:80%;">procedure</span>
</div>
</div>
</figure>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.2" class="ltx_p"><span id="S3.SS3.p5.2.1" class="ltx_text ltx_font_bold">Step 1 (per-class performance measurements).</span>
Based on a subset of the local dataset, we first calculate the F1-score for the CSPs of all <em id="S3.SS3.p5.2.2" class="ltx_emph ltx_font_italic">familiar classes</em> of both the peer’s own output layer and the prioritized output layers (line 4-8).
We refer to a class as familiar when the peer has a sufficient number of samples, denoted by <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="\kappa" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\kappa</annotation></semantics></math>, to reliably estimate the performance of that class of a given model.
This threshold is determined by the developer.
With a higher value of <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="\kappa" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">\kappa</annotation></semantics></math>, the PBI can more reliably determine F1-scores, but the number of classes for which insufficient samples are available to evaluate, also known as <em id="S3.SS3.p5.2.3" class="ltx_emph ltx_font_italic">foreign classes</em>, might decrease.
We chose to measure F1-scores instead of the accuracy as a proxy for the performance since the former is more suited for imbalanced datasets <cite class="ltx_cite ltx_citemacro_citep">(Chicco and Jurman, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>.
The data subset used by the PBI is never used to train the peer’s own model since this would result in an overestimation of the performance of the peer’s own model.
Since these measurements depend on the availability of sufficient private data samples, Bristle avoids integrating models from other peers until sufficient private data samples are available to test them reliably.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.2" class="ltx_p"><span id="S3.SS3.p6.2.1" class="ltx_text ltx_font_bold">Step 2 (certainty computation).</span>
Then, we calculate for each prioritized model a <em id="S3.SS3.p6.2.2" class="ltx_emph ltx_font_italic">certainty</em> score, which is a (rough) estimate of the degree to which this model is benign (line 10-12).
This certainty score is determined by subtracting the standard deviation from the average F1-score of the <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mi id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><ci id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">\phi</annotation></semantics></math> best-performing familiar classes (line 11-12), thus rewarding high class performance and punishing high variation among the class performance.
We do not consider all classes when computing the certainty score since it is not realistic to assume that all classes of each benign received model perform well in a non-i.i.d. environment.
The larger <math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mi id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><ci id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">\phi</annotation></semantics></math> is, the more robust the algorithm is against Byzantine attacks, but the less able it is to learn about foreign classes in the case of a non-i.i.d. setting.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.2" class="ltx_p"><span id="S3.SS3.p7.2.1" class="ltx_text ltx_font_bold">Step 3 (weight computation).</span>
Furthermore, we estimate for each familiar class of each prioritized model if integrating its corresponding CSP improves the model’s performance by simply checking if the F1-score of that class of the received model exceeds the respective F1-score of the peer’s own model (line 14).
We call the extent to which it does the <em id="S3.SS3.p7.2.2" class="ltx_emph ltx_font_italic">F1-discrepancy</em>, and we store this value in a two-dimensional array named <em id="S3.SS3.p7.2.3" class="ltx_emph ltx_font_italic">disc</em>.
If the class-specific F1-score of the model being evaluated exceeds that of our own model, we calculated the F1-discrepancy as in line 15.
Otherwise, we set the F1-discrepancy to <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="-\infty" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><mrow id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml"><mo id="S3.SS3.p7.1.m1.1.1a" xref="S3.SS3.p7.1.m1.1.1.cmml">−</mo><mi mathvariant="normal" id="S3.SS3.p7.1.m1.1.1.2" xref="S3.SS3.p7.1.m1.1.1.2.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><apply id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1"><minus id="S3.SS3.p7.1.m1.1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1"></minus><infinity id="S3.SS3.p7.1.m1.1.1.2.cmml" xref="S3.SS3.p7.1.m1.1.1.2"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">-\infty</annotation></semantics></math> (line 17).
The parameter <math id="S3.SS3.p7.2.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S3.SS3.p7.2.m2.1a"><mi id="S3.SS3.p7.2.m2.1.1" xref="S3.SS3.p7.2.m2.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.2.m2.1b"><ci id="S3.SS3.p7.2.m2.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.2.m2.1c">\eta</annotation></semantics></math> increases the degree to which high-performing classes are integrated.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<p id="S3.SS3.p8.4" class="ltx_p">The <span id="S3.SS3.p8.4.1" class="ltx_text ltx_font_typewriter">computeFaWg</span> function then computes the weights for the familiar classes for each model, taking the prior computed scores and certainty as input (line 19).
This function computes the following Sigmoid function, where <math id="S3.SS3.p8.1.m1.1" class="ltx_Math" alttext="w_{c}" display="inline"><semantics id="S3.SS3.p8.1.m1.1a"><msub id="S3.SS3.p8.1.m1.1.1" xref="S3.SS3.p8.1.m1.1.1.cmml"><mi id="S3.SS3.p8.1.m1.1.1.2" xref="S3.SS3.p8.1.m1.1.1.2.cmml">w</mi><mi id="S3.SS3.p8.1.m1.1.1.3" xref="S3.SS3.p8.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.1.m1.1b"><apply id="S3.SS3.p8.1.m1.1.1.cmml" xref="S3.SS3.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p8.1.m1.1.1.1.cmml" xref="S3.SS3.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p8.1.m1.1.1.2.cmml" xref="S3.SS3.p8.1.m1.1.1.2">𝑤</ci><ci id="S3.SS3.p8.1.m1.1.1.3.cmml" xref="S3.SS3.p8.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.1.m1.1c">w_{c}</annotation></semantics></math> is the weight of familiar class <math id="S3.SS3.p8.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.p8.2.m2.1a"><mi id="S3.SS3.p8.2.m2.1.1" xref="S3.SS3.p8.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.2.m2.1b"><ci id="S3.SS3.p8.2.m2.1.1.cmml" xref="S3.SS3.p8.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.2.m2.1c">c</annotation></semantics></math>, <math id="S3.SS3.p8.3.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS3.p8.3.m3.1a"><mi id="S3.SS3.p8.3.m3.1.1" xref="S3.SS3.p8.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.3.m3.1b"><ci id="S3.SS3.p8.3.m3.1.1.cmml" xref="S3.SS3.p8.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.3.m3.1c">s</annotation></semantics></math> is the F1-discrepancy, and <math id="S3.SS3.p8.4.m4.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS3.p8.4.m4.1a"><mi id="S3.SS3.p8.4.m4.1.1" xref="S3.SS3.p8.4.m4.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.4.m4.1b"><ci id="S3.SS3.p8.4.m4.1.1.cmml" xref="S3.SS3.p8.4.m4.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.4.m4.1c">r</annotation></semantics></math> is the certainty of the model being considered:</p>
</div>
<div id="S3.SS3.p9" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="w_{c}=max(0,\frac{\omega_{fa}^{1}}{1+e^{-\frac{s}{100}}}-\omega_{fa}^{2})*r" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msub id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml"><mi id="S3.E2.m1.2.2.3.2" xref="S3.E2.m1.2.2.3.2.cmml">w</mi><mi id="S3.E2.m1.2.2.3.3" xref="S3.E2.m1.2.2.3.3.cmml">c</mi></msub><mo id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.4" xref="S3.E2.m1.2.2.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.2a" xref="S3.E2.m1.2.2.1.1.2.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.5" xref="S3.E2.m1.2.2.1.1.5.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.2b" xref="S3.E2.m1.2.2.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">(</mo><mn id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">0</mn><mo id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.2.cmml">,</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml"><mfrac id="S3.E2.m1.2.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.cmml"><msubsup id="S3.E2.m1.2.2.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.2.cmml">ω</mi><mrow id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.3.cmml">a</mi></mrow><mn id="S3.E2.m1.2.2.1.1.1.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.3.cmml">1</mn></msubsup><mrow id="S3.E2.m1.2.2.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.cmml"><mn id="S3.E2.m1.2.2.1.1.1.1.1.2.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.2.cmml">1</mn><mo id="S3.E2.m1.2.2.1.1.1.1.1.2.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.1.cmml">+</mo><msup id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.2.cmml">e</mi><mrow id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3a" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.cmml">−</mo><mfrac id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.2.cmml">s</mi><mn id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.3.cmml">100</mn></mfrac></mrow></msup></mrow></mfrac><mo id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E2.m1.2.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.2.cmml">ω</mi><mrow id="S3.E2.m1.2.2.1.1.1.1.1.3.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.3.cmml">a</mi></mrow><mn id="S3.E2.m1.2.2.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.3.3.cmml">2</mn></msubsup></mrow><mo rspace="0.055em" stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.4" xref="S3.E2.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">∗</mo><mi id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"></eq><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.3">subscript</csymbol><ci id="S3.E2.m1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2">𝑤</ci><ci id="S3.E2.m1.2.2.3.3.cmml" xref="S3.E2.m1.2.2.3.3">𝑐</ci></apply><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></times><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><times id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3">𝑚</ci><ci id="S3.E2.m1.2.2.1.1.4.cmml" xref="S3.E2.m1.2.2.1.1.4">𝑎</ci><ci id="S3.E2.m1.2.2.1.1.5.cmml" xref="S3.E2.m1.2.2.1.1.5">𝑥</ci><interval closure="open" id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><cn type="integer" id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">0</cn><apply id="S3.E2.m1.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2"><divide id="S3.E2.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2"></divide><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.2">𝜔</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3"><times id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.1"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.2">𝑓</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.2.3.3">𝑎</ci></apply></apply><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.3">1</cn></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3"><plus id="S3.E2.m1.2.2.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.1"></plus><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.2">1</cn><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.2">𝑒</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3"><minus id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3"></minus><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2"><divide id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2"></divide><ci id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.2">𝑠</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.3.3.3.2.3">100</cn></apply></apply></apply></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.2">𝜔</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.3"><times id="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.1"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.2">𝑓</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.3.3">𝑎</ci></apply></apply><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.3">2</cn></apply></apply></interval></apply><ci id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">w_{c}=max(0,\frac{\omega_{fa}^{1}}{1+e^{-\frac{s}{100}}}-\omega_{fa}^{2})*r</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p10" class="ltx_para">
<p id="S3.SS3.p10.5" class="ltx_p">This function assigns a weight of one to equally performing classes and an increasingly higher weight to classes that show excellent performance, dependent on the values of <math id="S3.SS3.p10.1.m1.1" class="ltx_Math" alttext="\omega_{fa}^{1}" display="inline"><semantics id="S3.SS3.p10.1.m1.1a"><msubsup id="S3.SS3.p10.1.m1.1.1" xref="S3.SS3.p10.1.m1.1.1.cmml"><mi id="S3.SS3.p10.1.m1.1.1.2.2" xref="S3.SS3.p10.1.m1.1.1.2.2.cmml">ω</mi><mrow id="S3.SS3.p10.1.m1.1.1.2.3" xref="S3.SS3.p10.1.m1.1.1.2.3.cmml"><mi id="S3.SS3.p10.1.m1.1.1.2.3.2" xref="S3.SS3.p10.1.m1.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p10.1.m1.1.1.2.3.1" xref="S3.SS3.p10.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p10.1.m1.1.1.2.3.3" xref="S3.SS3.p10.1.m1.1.1.2.3.3.cmml">a</mi></mrow><mn id="S3.SS3.p10.1.m1.1.1.3" xref="S3.SS3.p10.1.m1.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p10.1.m1.1b"><apply id="S3.SS3.p10.1.m1.1.1.cmml" xref="S3.SS3.p10.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p10.1.m1.1.1.1.cmml" xref="S3.SS3.p10.1.m1.1.1">superscript</csymbol><apply id="S3.SS3.p10.1.m1.1.1.2.cmml" xref="S3.SS3.p10.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p10.1.m1.1.1.2.1.cmml" xref="S3.SS3.p10.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p10.1.m1.1.1.2.2.cmml" xref="S3.SS3.p10.1.m1.1.1.2.2">𝜔</ci><apply id="S3.SS3.p10.1.m1.1.1.2.3.cmml" xref="S3.SS3.p10.1.m1.1.1.2.3"><times id="S3.SS3.p10.1.m1.1.1.2.3.1.cmml" xref="S3.SS3.p10.1.m1.1.1.2.3.1"></times><ci id="S3.SS3.p10.1.m1.1.1.2.3.2.cmml" xref="S3.SS3.p10.1.m1.1.1.2.3.2">𝑓</ci><ci id="S3.SS3.p10.1.m1.1.1.2.3.3.cmml" xref="S3.SS3.p10.1.m1.1.1.2.3.3">𝑎</ci></apply></apply><cn type="integer" id="S3.SS3.p10.1.m1.1.1.3.cmml" xref="S3.SS3.p10.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p10.1.m1.1c">\omega_{fa}^{1}</annotation></semantics></math> and <math id="S3.SS3.p10.2.m2.1" class="ltx_Math" alttext="\omega_{fa}^{2}" display="inline"><semantics id="S3.SS3.p10.2.m2.1a"><msubsup id="S3.SS3.p10.2.m2.1.1" xref="S3.SS3.p10.2.m2.1.1.cmml"><mi id="S3.SS3.p10.2.m2.1.1.2.2" xref="S3.SS3.p10.2.m2.1.1.2.2.cmml">ω</mi><mrow id="S3.SS3.p10.2.m2.1.1.2.3" xref="S3.SS3.p10.2.m2.1.1.2.3.cmml"><mi id="S3.SS3.p10.2.m2.1.1.2.3.2" xref="S3.SS3.p10.2.m2.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p10.2.m2.1.1.2.3.1" xref="S3.SS3.p10.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p10.2.m2.1.1.2.3.3" xref="S3.SS3.p10.2.m2.1.1.2.3.3.cmml">a</mi></mrow><mn id="S3.SS3.p10.2.m2.1.1.3" xref="S3.SS3.p10.2.m2.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p10.2.m2.1b"><apply id="S3.SS3.p10.2.m2.1.1.cmml" xref="S3.SS3.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p10.2.m2.1.1.1.cmml" xref="S3.SS3.p10.2.m2.1.1">superscript</csymbol><apply id="S3.SS3.p10.2.m2.1.1.2.cmml" xref="S3.SS3.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p10.2.m2.1.1.2.1.cmml" xref="S3.SS3.p10.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p10.2.m2.1.1.2.2.cmml" xref="S3.SS3.p10.2.m2.1.1.2.2">𝜔</ci><apply id="S3.SS3.p10.2.m2.1.1.2.3.cmml" xref="S3.SS3.p10.2.m2.1.1.2.3"><times id="S3.SS3.p10.2.m2.1.1.2.3.1.cmml" xref="S3.SS3.p10.2.m2.1.1.2.3.1"></times><ci id="S3.SS3.p10.2.m2.1.1.2.3.2.cmml" xref="S3.SS3.p10.2.m2.1.1.2.3.2">𝑓</ci><ci id="S3.SS3.p10.2.m2.1.1.2.3.3.cmml" xref="S3.SS3.p10.2.m2.1.1.2.3.3">𝑎</ci></apply></apply><cn type="integer" id="S3.SS3.p10.2.m2.1.1.3.cmml" xref="S3.SS3.p10.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p10.2.m2.1c">\omega_{fa}^{2}</annotation></semantics></math>.
<math id="S3.SS3.p10.3.m3.1" class="ltx_Math" alttext="\omega_{fa}" display="inline"><semantics id="S3.SS3.p10.3.m3.1a"><msub id="S3.SS3.p10.3.m3.1.1" xref="S3.SS3.p10.3.m3.1.1.cmml"><mi id="S3.SS3.p10.3.m3.1.1.2" xref="S3.SS3.p10.3.m3.1.1.2.cmml">ω</mi><mrow id="S3.SS3.p10.3.m3.1.1.3" xref="S3.SS3.p10.3.m3.1.1.3.cmml"><mi id="S3.SS3.p10.3.m3.1.1.3.2" xref="S3.SS3.p10.3.m3.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p10.3.m3.1.1.3.1" xref="S3.SS3.p10.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p10.3.m3.1.1.3.3" xref="S3.SS3.p10.3.m3.1.1.3.3.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p10.3.m3.1b"><apply id="S3.SS3.p10.3.m3.1.1.cmml" xref="S3.SS3.p10.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p10.3.m3.1.1.1.cmml" xref="S3.SS3.p10.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p10.3.m3.1.1.2.cmml" xref="S3.SS3.p10.3.m3.1.1.2">𝜔</ci><apply id="S3.SS3.p10.3.m3.1.1.3.cmml" xref="S3.SS3.p10.3.m3.1.1.3"><times id="S3.SS3.p10.3.m3.1.1.3.1.cmml" xref="S3.SS3.p10.3.m3.1.1.3.1"></times><ci id="S3.SS3.p10.3.m3.1.1.3.2.cmml" xref="S3.SS3.p10.3.m3.1.1.3.2">𝑓</ci><ci id="S3.SS3.p10.3.m3.1.1.3.3.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p10.3.m3.1c">\omega_{fa}</annotation></semantics></math> represents the boost for above-average performing models and the bounty for below-average performing models. The bigger this discrepancy, the faster the model can catch up with other better-performing models, but the bigger the impact of a malicious model that performs well on familiar classes and bad on foreign classes.
We multiply the outcome of the Sigmoid function by the certainty score calculated earlier because the parameters of a class might be sub-optimal even when it has a perfect F1-score when other classes perform poorly.
Figure <a href="#S3.F7" title="Figure 7 ‣ 3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates how the weight of CSPs is affected by their F1-score on a test dataset, assuming that the peer’s current model yields a F1-score of <math id="S3.SS3.p10.4.m4.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.SS3.p10.4.m4.1a"><mn id="S3.SS3.p10.4.m4.1.1" xref="S3.SS3.p10.4.m4.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p10.4.m4.1b"><cn type="float" id="S3.SS3.p10.4.m4.1.1.cmml" xref="S3.SS3.p10.4.m4.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p10.4.m4.1c">0.5</annotation></semantics></math> on that class and that the calculated certainty is <math id="S3.SS3.p10.5.m5.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS3.p10.5.m5.1a"><mn id="S3.SS3.p10.5.m5.1.1" xref="S3.SS3.p10.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p10.5.m5.1b"><cn type="integer" id="S3.SS3.p10.5.m5.1.1.cmml" xref="S3.SS3.p10.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p10.5.m5.1c">1</annotation></semantics></math>.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2110.11006/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_img_landscape" width="346" height="135" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S3.F7.3.2" class="ltx_text" style="font-size:90%;">Weight assigned to the CSPs given an F1-score of 0.5 and a certainty score of 1.0.</span></figcaption>
</figure>
<div id="S3.SS3.p11" class="ltx_para">
<p id="S3.SS3.p11.4" class="ltx_p">Determining a weight for the CSPs of the foreign classes is challenging because we cannot directly measure their performance.
Instead, we take the sum of the scores of all familiar classes and feed this into the same Sigmoid function as used for the familiar classes (Equation <a href="#S3.E2" title="In 3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), albeit parameterized with separate variables <math id="S3.SS3.p11.1.m1.1" class="ltx_Math" alttext="\omega_{fo}^{1}" display="inline"><semantics id="S3.SS3.p11.1.m1.1a"><msubsup id="S3.SS3.p11.1.m1.1.1" xref="S3.SS3.p11.1.m1.1.1.cmml"><mi id="S3.SS3.p11.1.m1.1.1.2.2" xref="S3.SS3.p11.1.m1.1.1.2.2.cmml">ω</mi><mrow id="S3.SS3.p11.1.m1.1.1.2.3" xref="S3.SS3.p11.1.m1.1.1.2.3.cmml"><mi id="S3.SS3.p11.1.m1.1.1.2.3.2" xref="S3.SS3.p11.1.m1.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p11.1.m1.1.1.2.3.1" xref="S3.SS3.p11.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p11.1.m1.1.1.2.3.3" xref="S3.SS3.p11.1.m1.1.1.2.3.3.cmml">o</mi></mrow><mn id="S3.SS3.p11.1.m1.1.1.3" xref="S3.SS3.p11.1.m1.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p11.1.m1.1b"><apply id="S3.SS3.p11.1.m1.1.1.cmml" xref="S3.SS3.p11.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p11.1.m1.1.1.1.cmml" xref="S3.SS3.p11.1.m1.1.1">superscript</csymbol><apply id="S3.SS3.p11.1.m1.1.1.2.cmml" xref="S3.SS3.p11.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p11.1.m1.1.1.2.1.cmml" xref="S3.SS3.p11.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p11.1.m1.1.1.2.2.cmml" xref="S3.SS3.p11.1.m1.1.1.2.2">𝜔</ci><apply id="S3.SS3.p11.1.m1.1.1.2.3.cmml" xref="S3.SS3.p11.1.m1.1.1.2.3"><times id="S3.SS3.p11.1.m1.1.1.2.3.1.cmml" xref="S3.SS3.p11.1.m1.1.1.2.3.1"></times><ci id="S3.SS3.p11.1.m1.1.1.2.3.2.cmml" xref="S3.SS3.p11.1.m1.1.1.2.3.2">𝑓</ci><ci id="S3.SS3.p11.1.m1.1.1.2.3.3.cmml" xref="S3.SS3.p11.1.m1.1.1.2.3.3">𝑜</ci></apply></apply><cn type="integer" id="S3.SS3.p11.1.m1.1.1.3.cmml" xref="S3.SS3.p11.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p11.1.m1.1c">\omega_{fo}^{1}</annotation></semantics></math> and <math id="S3.SS3.p11.2.m2.1" class="ltx_Math" alttext="\omega_{fo}^{2}" display="inline"><semantics id="S3.SS3.p11.2.m2.1a"><msubsup id="S3.SS3.p11.2.m2.1.1" xref="S3.SS3.p11.2.m2.1.1.cmml"><mi id="S3.SS3.p11.2.m2.1.1.2.2" xref="S3.SS3.p11.2.m2.1.1.2.2.cmml">ω</mi><mrow id="S3.SS3.p11.2.m2.1.1.2.3" xref="S3.SS3.p11.2.m2.1.1.2.3.cmml"><mi id="S3.SS3.p11.2.m2.1.1.2.3.2" xref="S3.SS3.p11.2.m2.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p11.2.m2.1.1.2.3.1" xref="S3.SS3.p11.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p11.2.m2.1.1.2.3.3" xref="S3.SS3.p11.2.m2.1.1.2.3.3.cmml">o</mi></mrow><mn id="S3.SS3.p11.2.m2.1.1.3" xref="S3.SS3.p11.2.m2.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p11.2.m2.1b"><apply id="S3.SS3.p11.2.m2.1.1.cmml" xref="S3.SS3.p11.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p11.2.m2.1.1.1.cmml" xref="S3.SS3.p11.2.m2.1.1">superscript</csymbol><apply id="S3.SS3.p11.2.m2.1.1.2.cmml" xref="S3.SS3.p11.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p11.2.m2.1.1.2.1.cmml" xref="S3.SS3.p11.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p11.2.m2.1.1.2.2.cmml" xref="S3.SS3.p11.2.m2.1.1.2.2">𝜔</ci><apply id="S3.SS3.p11.2.m2.1.1.2.3.cmml" xref="S3.SS3.p11.2.m2.1.1.2.3"><times id="S3.SS3.p11.2.m2.1.1.2.3.1.cmml" xref="S3.SS3.p11.2.m2.1.1.2.3.1"></times><ci id="S3.SS3.p11.2.m2.1.1.2.3.2.cmml" xref="S3.SS3.p11.2.m2.1.1.2.3.2">𝑓</ci><ci id="S3.SS3.p11.2.m2.1.1.2.3.3.cmml" xref="S3.SS3.p11.2.m2.1.1.2.3.3">𝑜</ci></apply></apply><cn type="integer" id="S3.SS3.p11.2.m2.1.1.3.cmml" xref="S3.SS3.p11.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p11.2.m2.1c">\omega_{fo}^{2}</annotation></semantics></math>.
This is done by the <span id="S3.SS3.p11.4.1" class="ltx_text ltx_font_typewriter">computeFoWg</span> function (line 21).
<math id="S3.SS3.p11.3.m3.1" class="ltx_Math" alttext="\omega_{fo}" display="inline"><semantics id="S3.SS3.p11.3.m3.1a"><msub id="S3.SS3.p11.3.m3.1.1" xref="S3.SS3.p11.3.m3.1.1.cmml"><mi id="S3.SS3.p11.3.m3.1.1.2" xref="S3.SS3.p11.3.m3.1.1.2.cmml">ω</mi><mrow id="S3.SS3.p11.3.m3.1.1.3" xref="S3.SS3.p11.3.m3.1.1.3.cmml"><mi id="S3.SS3.p11.3.m3.1.1.3.2" xref="S3.SS3.p11.3.m3.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p11.3.m3.1.1.3.1" xref="S3.SS3.p11.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p11.3.m3.1.1.3.3" xref="S3.SS3.p11.3.m3.1.1.3.3.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p11.3.m3.1b"><apply id="S3.SS3.p11.3.m3.1.1.cmml" xref="S3.SS3.p11.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p11.3.m3.1.1.1.cmml" xref="S3.SS3.p11.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p11.3.m3.1.1.2.cmml" xref="S3.SS3.p11.3.m3.1.1.2">𝜔</ci><apply id="S3.SS3.p11.3.m3.1.1.3.cmml" xref="S3.SS3.p11.3.m3.1.1.3"><times id="S3.SS3.p11.3.m3.1.1.3.1.cmml" xref="S3.SS3.p11.3.m3.1.1.3.1"></times><ci id="S3.SS3.p11.3.m3.1.1.3.2.cmml" xref="S3.SS3.p11.3.m3.1.1.3.2">𝑓</ci><ci id="S3.SS3.p11.3.m3.1.1.3.3.cmml" xref="S3.SS3.p11.3.m3.1.1.3.3">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p11.3.m3.1c">\omega_{fo}</annotation></semantics></math> is the extent to which foreign classes are integrated into the model. The higher this value, the better the model can be when the peer wants to use the model to classify formerly foreign classes, but also the higher the impact when a potentially malicious model is integrated.
If the user is uninterested in achieving high performance on foreign classes, <math id="S3.SS3.p11.4.m4.1" class="ltx_Math" alttext="\omega_{fo}^{1}" display="inline"><semantics id="S3.SS3.p11.4.m4.1a"><msubsup id="S3.SS3.p11.4.m4.1.1" xref="S3.SS3.p11.4.m4.1.1.cmml"><mi id="S3.SS3.p11.4.m4.1.1.2.2" xref="S3.SS3.p11.4.m4.1.1.2.2.cmml">ω</mi><mrow id="S3.SS3.p11.4.m4.1.1.2.3" xref="S3.SS3.p11.4.m4.1.1.2.3.cmml"><mi id="S3.SS3.p11.4.m4.1.1.2.3.2" xref="S3.SS3.p11.4.m4.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p11.4.m4.1.1.2.3.1" xref="S3.SS3.p11.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p11.4.m4.1.1.2.3.3" xref="S3.SS3.p11.4.m4.1.1.2.3.3.cmml">o</mi></mrow><mn id="S3.SS3.p11.4.m4.1.1.3" xref="S3.SS3.p11.4.m4.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p11.4.m4.1b"><apply id="S3.SS3.p11.4.m4.1.1.cmml" xref="S3.SS3.p11.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p11.4.m4.1.1.1.cmml" xref="S3.SS3.p11.4.m4.1.1">superscript</csymbol><apply id="S3.SS3.p11.4.m4.1.1.2.cmml" xref="S3.SS3.p11.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p11.4.m4.1.1.2.1.cmml" xref="S3.SS3.p11.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p11.4.m4.1.1.2.2.cmml" xref="S3.SS3.p11.4.m4.1.1.2.2">𝜔</ci><apply id="S3.SS3.p11.4.m4.1.1.2.3.cmml" xref="S3.SS3.p11.4.m4.1.1.2.3"><times id="S3.SS3.p11.4.m4.1.1.2.3.1.cmml" xref="S3.SS3.p11.4.m4.1.1.2.3.1"></times><ci id="S3.SS3.p11.4.m4.1.1.2.3.2.cmml" xref="S3.SS3.p11.4.m4.1.1.2.3.2">𝑓</ci><ci id="S3.SS3.p11.4.m4.1.1.2.3.3.cmml" xref="S3.SS3.p11.4.m4.1.1.2.3.3">𝑜</ci></apply></apply><cn type="integer" id="S3.SS3.p11.4.m4.1.1.3.cmml" xref="S3.SS3.p11.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p11.4.m4.1c">\omega_{fo}^{1}</annotation></semantics></math> can be set to 0.</p>
</div>
<div id="S3.SS3.p12" class="ltx_para">
<p id="S3.SS3.p12.1" class="ltx_p"><span id="S3.SS3.p12.1.1" class="ltx_text ltx_font_bold">Step 4 (model integration).</span>
Finally, we replace the CSPs of familiar and foreign classes with the weighted average of all CSPs by using the calculated weights (line 24-25).</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Implementation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Bristle is implemented on top of an existing network library that provides support for peer discovery, decentralized overlay creation, and authenticated messaging <cite class="ltx_cite ltx_citemacro_citep">(Stokkink
et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2020</a>; Stokkink and
Pouwelse, <a href="#bib.bib69" title="" class="ltx_ref">[n.d.]</a>)</cite>.
Since we envision the usage of Bristle mostly in a mobile environment, our middleware is written in the Kotlin programming language (the default language for Android applications).
We envision that Bristle runs as a background service on the end-users’ devices to periodically receive or transmit models from and to peers when the device is connected to Wi-Fi.
Peers transmit model updates using the UDP protocol and network messages are compressed with Gzip.
Model training is facilitated by the DeepLearning4j library (version 1.0.0-beta7) to enjoy compatibility with a wide range of advanced ML algorithms.
We have published the Bristle source code and developer documentation in a GitHub repository.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Source code can be requested through the Program Committee (to comply with the double-blind review requirements).</span></span></span></p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">Using Bristle.</span>
Developers can leverage the Bristle middleware by feeding the peer’s current model and all received models into Bristle’s GAR after each iteration and subsequently replacing the peer’s model with the result.
The moment a model is updated can be decided by the developer, for example, model training can take place when the device is charging to minimize the impact on end users.
The peers should be able to selectively receive and integrate only models relevant to the current ML application.
To this end, Bristle uses the functionality to form communication groups as provided by its network library.
Deciding on the variables listed under Section <a href="#S4" title="4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is a key part of machine learning.
We recommend the developer to use heuristics, optimize them on a related dataset, or use A/B-testing to find the best values to obtain the desired accuracy on the models.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We now describe our experimental setup, datasets, and parameters.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Testbed.</span>
All experiments are run on an HPE DL385 Gen10 server.
This server is equipped with 128 AMD EPYC 7452 CPUs, has 512 GB of DDR4 memory, and runs Debian 10.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.2" class="ltx_p"><span id="S4.p3.2.1" class="ltx_text ltx_font_bold">Datasets.</span>
In line with related research <cite class="ltx_cite ltx_citemacro_citep">(Mhamdi
et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2018</a>; McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">[n.d.]</a>; Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2018</a>)</cite>, we consider an image classification application that applies Convolutional Neural Networks (CNNs) to classify images.
We use the popular MNIST <cite class="ltx_cite ltx_citemacro_citep">(Lecun, <a href="#bib.bib46" title="" class="ltx_ref">[n.d.]</a>)</cite> dataset, consisting of 60,000 gray-scale training images and 10,000 test images of 28x28 pixels representing handwritten digits.
To achieve better performance, we standardize the dataset by applying Z-score normalization such that the features are re-scaled to a normal distribution with <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.p3.1.m1.1a"><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\mu</annotation></semantics></math> = 0 and <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\sigma</annotation></semantics></math> = 1.
We pre-train - until convergence - a neural network with the same configuration on the EMNIST-Letters  <cite class="ltx_cite ltx_citemacro_citep">(Cohen
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> dataset, which features resemble MNIST, but contains characters instead of digits.
We then include the resulting neural network in the Bristle software.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.12" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.12.13.1" class="ltx_tr">
<td id="S4.T1.12.13.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.12.13.1.1.1" class="ltx_text ltx_font_bold">Experiment parameter</span></td>
<td id="S4.T1.12.13.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T1.12.13.1.2.1" class="ltx_text ltx_font_bold">Default value</span></td>
</tr>
<tr id="S4.T1.12.14.2" class="ltx_tr">
<td id="S4.T1.12.14.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><em id="S4.T1.12.14.2.1.1" class="ltx_emph ltx_font_italic">Environment</em></td>
<td id="S4.T1.12.14.2.2" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Peers (<math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mi id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">n</annotation></semantics></math>)</td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_left ltx_border_r">10</td>
</tr>
<tr id="S4.T1.12.15.3" class="ltx_tr">
<td id="S4.T1.12.15.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Connection ratio</td>
<td id="S4.T1.12.15.3.2" class="ltx_td ltx_align_left ltx_border_r">100%</td>
</tr>
<tr id="S4.T1.12.16.4" class="ltx_tr">
<td id="S4.T1.12.16.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Fraction Byzantine peers</td>
<td id="S4.T1.12.16.4.2" class="ltx_td ltx_align_left ltx_border_r">50%</td>
</tr>
<tr id="S4.T1.12.17.5" class="ltx_tr">
<td id="S4.T1.12.17.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><em id="S4.T1.12.17.5.1.1" class="ltx_emph ltx_font_italic">Machine learning</em></td>
<td id="S4.T1.12.17.5.2" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Mini-batch size (<math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mi id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><ci id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">b</annotation></semantics></math>)</td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_r">5</td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Learning rate (<math id="S4.T1.3.3.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mi id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><ci id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\lambda</annotation></semantics></math>)</td>
<td id="S4.T1.3.3.2" class="ltx_td ltx_align_left ltx_border_r">0.001</td>
</tr>
<tr id="S4.T1.12.18.6" class="ltx_tr">
<td id="S4.T1.12.18.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">L2-value</td>
<td id="S4.T1.12.18.6.2" class="ltx_td ltx_align_left ltx_border_r">0.005</td>
</tr>
<tr id="S4.T1.12.19.7" class="ltx_tr">
<td id="S4.T1.12.19.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Max. iterations</td>
<td id="S4.T1.12.19.7.2" class="ltx_td ltx_align_left ltx_border_r">300</td>
</tr>
<tr id="S4.T1.12.20.8" class="ltx_tr">
<td id="S4.T1.12.20.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><em id="S4.T1.12.20.8.1.1" class="ltx_emph ltx_font_italic">Bristle</em></td>
<td id="S4.T1.12.20.8.2" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Exploration-exploitation rate (<math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mi id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><ci id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">\alpha</annotation></semantics></math>)</td>
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_left ltx_border_r">0.4</td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr">
<td id="S4.T1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Max. PBI input size (<math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mi id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><ci id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">\beta</annotation></semantics></math>)</td>
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_left ltx_border_r">30</td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<td id="S4.T1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">#familiar class selection size (<math id="S4.T1.6.6.1.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S4.T1.6.6.1.m1.1a"><mi id="S4.T1.6.6.1.m1.1.1" xref="S4.T1.6.6.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.m1.1b"><ci id="S4.T1.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.m1.1c">\phi</annotation></semantics></math>)</td>
<td id="S4.T1.6.6.2" class="ltx_td ltx_align_left ltx_border_r">3</td>
</tr>
<tr id="S4.T1.7.7" class="ltx_tr">
<td id="S4.T1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">#test samples per class (<math id="S4.T1.7.7.1.m1.1" class="ltx_Math" alttext="\kappa" display="inline"><semantics id="S4.T1.7.7.1.m1.1a"><mi id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><ci id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">\kappa</annotation></semantics></math>)</td>
<td id="S4.T1.7.7.2" class="ltx_td ltx_align_left ltx_border_r">10</td>
</tr>
<tr id="S4.T1.8.8" class="ltx_tr">
<td id="S4.T1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><math id="S4.T1.8.8.1.m1.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S4.T1.8.8.1.m1.1a"><mi id="S4.T1.8.8.1.m1.1.1" xref="S4.T1.8.8.1.m1.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.1.m1.1b"><ci id="S4.T1.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.1.m1.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.1.m1.1c">\eta</annotation></semantics></math></td>
<td id="S4.T1.8.8.2" class="ltx_td ltx_align_left ltx_border_r">10</td>
</tr>
<tr id="S4.T1.10.10" class="ltx_tr">
<td id="S4.T1.10.10.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">
<math id="S4.T1.9.9.1.m1.1" class="ltx_Math" alttext="\omega_{fa}^{1}" display="inline"><semantics id="S4.T1.9.9.1.m1.1a"><msubsup id="S4.T1.9.9.1.m1.1.1" xref="S4.T1.9.9.1.m1.1.1.cmml"><mi id="S4.T1.9.9.1.m1.1.1.2.2" xref="S4.T1.9.9.1.m1.1.1.2.2.cmml">ω</mi><mrow id="S4.T1.9.9.1.m1.1.1.2.3" xref="S4.T1.9.9.1.m1.1.1.2.3.cmml"><mi id="S4.T1.9.9.1.m1.1.1.2.3.2" xref="S4.T1.9.9.1.m1.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.T1.9.9.1.m1.1.1.2.3.1" xref="S4.T1.9.9.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.T1.9.9.1.m1.1.1.2.3.3" xref="S4.T1.9.9.1.m1.1.1.2.3.3.cmml">a</mi></mrow><mn id="S4.T1.9.9.1.m1.1.1.3" xref="S4.T1.9.9.1.m1.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b"><apply id="S4.T1.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.9.9.1.m1.1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1">superscript</csymbol><apply id="S4.T1.9.9.1.m1.1.1.2.cmml" xref="S4.T1.9.9.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.9.9.1.m1.1.1.2.1.cmml" xref="S4.T1.9.9.1.m1.1.1">subscript</csymbol><ci id="S4.T1.9.9.1.m1.1.1.2.2.cmml" xref="S4.T1.9.9.1.m1.1.1.2.2">𝜔</ci><apply id="S4.T1.9.9.1.m1.1.1.2.3.cmml" xref="S4.T1.9.9.1.m1.1.1.2.3"><times id="S4.T1.9.9.1.m1.1.1.2.3.1.cmml" xref="S4.T1.9.9.1.m1.1.1.2.3.1"></times><ci id="S4.T1.9.9.1.m1.1.1.2.3.2.cmml" xref="S4.T1.9.9.1.m1.1.1.2.3.2">𝑓</ci><ci id="S4.T1.9.9.1.m1.1.1.2.3.3.cmml" xref="S4.T1.9.9.1.m1.1.1.2.3.3">𝑎</ci></apply></apply><cn type="integer" id="S4.T1.9.9.1.m1.1.1.3.cmml" xref="S4.T1.9.9.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">\omega_{fa}^{1}</annotation></semantics></math>, <math id="S4.T1.10.10.2.m2.1" class="ltx_Math" alttext="\omega_{fo}^{1}" display="inline"><semantics id="S4.T1.10.10.2.m2.1a"><msubsup id="S4.T1.10.10.2.m2.1.1" xref="S4.T1.10.10.2.m2.1.1.cmml"><mi id="S4.T1.10.10.2.m2.1.1.2.2" xref="S4.T1.10.10.2.m2.1.1.2.2.cmml">ω</mi><mrow id="S4.T1.10.10.2.m2.1.1.2.3" xref="S4.T1.10.10.2.m2.1.1.2.3.cmml"><mi id="S4.T1.10.10.2.m2.1.1.2.3.2" xref="S4.T1.10.10.2.m2.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.T1.10.10.2.m2.1.1.2.3.1" xref="S4.T1.10.10.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S4.T1.10.10.2.m2.1.1.2.3.3" xref="S4.T1.10.10.2.m2.1.1.2.3.3.cmml">o</mi></mrow><mn id="S4.T1.10.10.2.m2.1.1.3" xref="S4.T1.10.10.2.m2.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.2.m2.1b"><apply id="S4.T1.10.10.2.m2.1.1.cmml" xref="S4.T1.10.10.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T1.10.10.2.m2.1.1.1.cmml" xref="S4.T1.10.10.2.m2.1.1">superscript</csymbol><apply id="S4.T1.10.10.2.m2.1.1.2.cmml" xref="S4.T1.10.10.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T1.10.10.2.m2.1.1.2.1.cmml" xref="S4.T1.10.10.2.m2.1.1">subscript</csymbol><ci id="S4.T1.10.10.2.m2.1.1.2.2.cmml" xref="S4.T1.10.10.2.m2.1.1.2.2">𝜔</ci><apply id="S4.T1.10.10.2.m2.1.1.2.3.cmml" xref="S4.T1.10.10.2.m2.1.1.2.3"><times id="S4.T1.10.10.2.m2.1.1.2.3.1.cmml" xref="S4.T1.10.10.2.m2.1.1.2.3.1"></times><ci id="S4.T1.10.10.2.m2.1.1.2.3.2.cmml" xref="S4.T1.10.10.2.m2.1.1.2.3.2">𝑓</ci><ci id="S4.T1.10.10.2.m2.1.1.2.3.3.cmml" xref="S4.T1.10.10.2.m2.1.1.2.3.3">𝑜</ci></apply></apply><cn type="integer" id="S4.T1.10.10.2.m2.1.1.3.cmml" xref="S4.T1.10.10.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.2.m2.1c">\omega_{fo}^{1}</annotation></semantics></math>
</td>
<td id="S4.T1.10.10.3" class="ltx_td ltx_align_left ltx_border_r">10</td>
</tr>
<tr id="S4.T1.12.12" class="ltx_tr">
<td id="S4.T1.12.12.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">
<math id="S4.T1.11.11.1.m1.1" class="ltx_Math" alttext="\omega_{fa}^{2}" display="inline"><semantics id="S4.T1.11.11.1.m1.1a"><msubsup id="S4.T1.11.11.1.m1.1.1" xref="S4.T1.11.11.1.m1.1.1.cmml"><mi id="S4.T1.11.11.1.m1.1.1.2.2" xref="S4.T1.11.11.1.m1.1.1.2.2.cmml">ω</mi><mrow id="S4.T1.11.11.1.m1.1.1.2.3" xref="S4.T1.11.11.1.m1.1.1.2.3.cmml"><mi id="S4.T1.11.11.1.m1.1.1.2.3.2" xref="S4.T1.11.11.1.m1.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.T1.11.11.1.m1.1.1.2.3.1" xref="S4.T1.11.11.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.T1.11.11.1.m1.1.1.2.3.3" xref="S4.T1.11.11.1.m1.1.1.2.3.3.cmml">a</mi></mrow><mn id="S4.T1.11.11.1.m1.1.1.3" xref="S4.T1.11.11.1.m1.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.1.m1.1b"><apply id="S4.T1.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.11.11.1.m1.1.1.1.cmml" xref="S4.T1.11.11.1.m1.1.1">superscript</csymbol><apply id="S4.T1.11.11.1.m1.1.1.2.cmml" xref="S4.T1.11.11.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.11.11.1.m1.1.1.2.1.cmml" xref="S4.T1.11.11.1.m1.1.1">subscript</csymbol><ci id="S4.T1.11.11.1.m1.1.1.2.2.cmml" xref="S4.T1.11.11.1.m1.1.1.2.2">𝜔</ci><apply id="S4.T1.11.11.1.m1.1.1.2.3.cmml" xref="S4.T1.11.11.1.m1.1.1.2.3"><times id="S4.T1.11.11.1.m1.1.1.2.3.1.cmml" xref="S4.T1.11.11.1.m1.1.1.2.3.1"></times><ci id="S4.T1.11.11.1.m1.1.1.2.3.2.cmml" xref="S4.T1.11.11.1.m1.1.1.2.3.2">𝑓</ci><ci id="S4.T1.11.11.1.m1.1.1.2.3.3.cmml" xref="S4.T1.11.11.1.m1.1.1.2.3.3">𝑎</ci></apply></apply><cn type="integer" id="S4.T1.11.11.1.m1.1.1.3.cmml" xref="S4.T1.11.11.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.1.m1.1c">\omega_{fa}^{2}</annotation></semantics></math>, <math id="S4.T1.12.12.2.m2.1" class="ltx_Math" alttext="\omega_{fo}^{2}" display="inline"><semantics id="S4.T1.12.12.2.m2.1a"><msubsup id="S4.T1.12.12.2.m2.1.1" xref="S4.T1.12.12.2.m2.1.1.cmml"><mi id="S4.T1.12.12.2.m2.1.1.2.2" xref="S4.T1.12.12.2.m2.1.1.2.2.cmml">ω</mi><mrow id="S4.T1.12.12.2.m2.1.1.2.3" xref="S4.T1.12.12.2.m2.1.1.2.3.cmml"><mi id="S4.T1.12.12.2.m2.1.1.2.3.2" xref="S4.T1.12.12.2.m2.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.T1.12.12.2.m2.1.1.2.3.1" xref="S4.T1.12.12.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S4.T1.12.12.2.m2.1.1.2.3.3" xref="S4.T1.12.12.2.m2.1.1.2.3.3.cmml">o</mi></mrow><mn id="S4.T1.12.12.2.m2.1.1.3" xref="S4.T1.12.12.2.m2.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.2.m2.1b"><apply id="S4.T1.12.12.2.m2.1.1.cmml" xref="S4.T1.12.12.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T1.12.12.2.m2.1.1.1.cmml" xref="S4.T1.12.12.2.m2.1.1">superscript</csymbol><apply id="S4.T1.12.12.2.m2.1.1.2.cmml" xref="S4.T1.12.12.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T1.12.12.2.m2.1.1.2.1.cmml" xref="S4.T1.12.12.2.m2.1.1">subscript</csymbol><ci id="S4.T1.12.12.2.m2.1.1.2.2.cmml" xref="S4.T1.12.12.2.m2.1.1.2.2">𝜔</ci><apply id="S4.T1.12.12.2.m2.1.1.2.3.cmml" xref="S4.T1.12.12.2.m2.1.1.2.3"><times id="S4.T1.12.12.2.m2.1.1.2.3.1.cmml" xref="S4.T1.12.12.2.m2.1.1.2.3.1"></times><ci id="S4.T1.12.12.2.m2.1.1.2.3.2.cmml" xref="S4.T1.12.12.2.m2.1.1.2.3.2">𝑓</ci><ci id="S4.T1.12.12.2.m2.1.1.2.3.3.cmml" xref="S4.T1.12.12.2.m2.1.1.2.3.3">𝑜</ci></apply></apply><cn type="integer" id="S4.T1.12.12.2.m2.1.1.3.cmml" xref="S4.T1.12.12.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.2.m2.1c">\omega_{fo}^{2}</annotation></semantics></math>
</td>
<td id="S4.T1.12.12.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.14.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S4.T1.15.2" class="ltx_text" style="font-size:90%;">Default parameters and values used during the experiments.</span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Experiment parameters.</span>
We list all default parameters used during our experiments in Table <a href="#S4.T1" title="Table 1 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
An exploration-exploitation ratio <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.p4.1.m1.1a"><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\alpha</annotation></semantics></math> of 0.4 slightly prioritizes model with lower distance.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Non-i.d.d. data. </span>
To evaluate Bristle with non-i.d.d. data, we first sort the data per class, divide the data into equally-sized shards, and then assign to every peer several shards, which is also the approach taken by related work <cite class="ltx_cite ltx_citemacro_citep">(Damaskinos et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020a</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2019</a>; Wang
et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">[n.d.]</a>)</cite>.
We note that the more shards we assign to every peer, the better every peer can recognize and defend against Byzantine attacks (see Section <a href="#S3.SS3" title="3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>), but the less non-i.i.d. the classes are.
We opt to assign - unless specified otherwise - without loss of generality to every peer four shards which cover 40% of the classes to balance between the ability to learn non-i.i.d. classes and to recognize Byzantine attacks.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Model training.</span>
We train the dataset on the same CNN architecture used by McMahan et al. <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">[n.d.]</a>)</cite>, except that we use Leaky ReLu instead of the regular ReLu as the activation function for the hidden layers since the former seems to give slightly better performance (specifically, suffers less from the vanishing gradients problem).
The model consists of twice a convolutional layer (kernel size = 5, stride = 1, padding = 0) followed by a max pooling layer (kernel size = 2, stride = 2, padding = 0), and finally the output layer (with 800 hidden nodes).
For the output function, we use the softmax function, and as the loss function, we use negative log-likelihood.
To train the model, we use the Adam optimizer, parameterized as shown in Table <a href="#S4.T1" title="Table 1 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Since these values significantly impact the model’s performance, we used a grid search with typically 7-9 values for each parameter and ensured that the optimal values were approximately in the middle.
Although the baselines were originally not developed to be used in combination with transfer learning, we decided to use transfer learning for all baselines for all experiments anyway since the performance increase is so significant that otherwise any comparison with Bristle would be meaningless (also see Section <a href="#S5.SS1" title="5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>).</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.3" class="ltx_p"><span id="S4.p7.3.1" class="ltx_text ltx_font_bold">Baselines.</span>
To compare Bristle with existing methods, we implemented five other GARs commonly used in FL:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><em id="S4.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">FedAvg</em> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">[n.d.]</a>)</cite> aggregates all models by coordinate-wise averaging of parameters. It is commonly used as a baseline to compare the performance of FL systems.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><em id="S4.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Median</em> <cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2018</a>)</cite> aggregates all models by taking the coordinate-wise mean of parameters. As demonstrated in the literature, it is already a particularly effective Byzantine-resilient GAR <cite class="ltx_cite ltx_citemacro_citep">(Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><em id="S4.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Krum</em> <cite class="ltx_cite ltx_citemacro_citep">(Blanchard
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">[n.d.]</a>)</cite> integrates the model that most closely resembles (in terms of Euclidean distance) all other models as the new global model. Even if the selected model is malicious, in theory, the performance should not degrade too much as it is close to all other models.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><em id="S4.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">BRIDGE</em> <cite class="ltx_cite ltx_citemacro_citep">(Yang and Bajwa, <a href="#bib.bib86" title="" class="ltx_ref">2019a</a>)</cite> is specifically designed for Byzantine-resilient model aggregation in decentralized settings. It cyclically updates every coordinate one by one and subsequently applies trimmed-mean screening to obtain the final coordinate for each dimension.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><em id="S4.I1.i5.p1.1.1" class="ltx_emph ltx_font_italic">MOZI</em> <cite class="ltx_cite ltx_citemacro_citep">(Guo
et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> uses a combination of a fast distance-based and accurate performance-based filter to aggregate model updates in a Byzantine-resilient manner.</p>
</div>
</li>
</ol>
<p id="S4.p7.2" class="ltx_p">We initialized Krum and BRIDGE, which are dependent on a-priori knowledge of the number of attackers, with <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="b=4" display="inline"><semantics id="S4.p7.1.m1.1a"><mrow id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">b</mi><mo id="S4.p7.1.m1.1.1.1" xref="S4.p7.1.m1.1.1.1.cmml">=</mo><mn id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><eq id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1.1"></eq><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">𝑏</ci><cn type="integer" id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">b=4</annotation></semantics></math> (the maximum number of attackers) and Mozi with <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="\rho=0.5" display="inline"><semantics id="S4.p7.2.m2.1a"><mrow id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml"><mi id="S4.p7.2.m2.1.1.2" xref="S4.p7.2.m2.1.1.2.cmml">ρ</mi><mo id="S4.p7.2.m2.1.1.1" xref="S4.p7.2.m2.1.1.1.cmml">=</mo><mn id="S4.p7.2.m2.1.1.3" xref="S4.p7.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><apply id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1"><eq id="S4.p7.2.m2.1.1.1.cmml" xref="S4.p7.2.m2.1.1.1"></eq><ci id="S4.p7.2.m2.1.1.2.cmml" xref="S4.p7.2.m2.1.1.2">𝜌</ci><cn type="float" id="S4.p7.2.m2.1.1.3.cmml" xref="S4.p7.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">\rho=0.5</annotation></semantics></math> (the ratio of benign to Byzantine peers).</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x8.png" id="S4.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F8.sf1.3.2" class="ltx_text" style="font-size:90%;">Regular learning</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x9.png" id="S4.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F8.sf2.3.2" class="ltx_text" style="font-size:90%;">Transfer learning</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">The accuracy of the model while training, for regular and transfer learning.</span></figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x10.png" id="S4.F9.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F9.sf1.3.2" class="ltx_text" style="font-size:90%;">Label-flip attack</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x11.png" id="S4.F9.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F9.sf2.3.2" class="ltx_text" style="font-size:90%;">Additive noise attack</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F9.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x12.png" id="S4.F9.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F9.sf3.3.2" class="ltx_text" style="font-size:90%;">Krum attack</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F9.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x13.png" id="S4.F9.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F9.sf4.3.2" class="ltx_text" style="font-size:90%;">Trimmed Mean attack</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>. </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">The resilience of Bristle and other GARs against various Byzantine attacks, with 50% of all peers being malicious and i.i.d. data.</span></figcaption>
</figure>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p"><span id="S4.p8.1.1" class="ltx_text ltx_font_bold">Byzantine attacks. </span>
We also evaluate Bristle under the following four Byzantine attacks, which are commonly considered in the domain.</p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.2" class="ltx_p">The <em id="S4.I2.i1.p1.2.1" class="ltx_emph ltx_font_italic">Label-flip attack</em> <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">[n.d.]</a>)</cite> assigns an incorrect label to each input <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">[n.d.]</a>; Fung
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>; Tolpegin
et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">[n.d.]</a>)</cite>. We implement this attack by numbering all labels and reassigning each sample with label <math id="S4.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.I2.i1.p1.1.m1.1a"><mi id="S4.I2.i1.p1.1.m1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.1.m1.1b"><ci id="S4.I2.i1.p1.1.m1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.1.m1.1c">x</annotation></semantics></math> to label <math id="S4.I2.i1.p1.2.m2.2" class="ltx_Math" alttext="(x+1)\ \%\ |x|" display="inline"><semantics id="S4.I2.i1.p1.2.m2.2a"><mrow id="S4.I2.i1.p1.2.m2.2.2" xref="S4.I2.i1.p1.2.m2.2.2.cmml"><mrow id="S4.I2.i1.p1.2.m2.2.2.1" xref="S4.I2.i1.p1.2.m2.2.2.1.cmml"><mrow id="S4.I2.i1.p1.2.m2.2.2.1.1.1" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.I2.i1.p1.2.m2.2.2.1.1.1.2" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.I2.i1.p1.2.m2.2.2.1.1.1.1" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.cmml"><mi id="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.2" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.2.cmml">x</mi><mo id="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.1" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.1.cmml">+</mo><mn id="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.3" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.3.cmml">1</mn></mrow><mo rspace="0.500em" stretchy="false" id="S4.I2.i1.p1.2.m2.2.2.1.1.1.3" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow><mo id="S4.I2.i1.p1.2.m2.2.2.1.2" xref="S4.I2.i1.p1.2.m2.2.2.1.2.cmml">%</mo></mrow><mo lspace="0.500em" rspace="0em" id="S4.I2.i1.p1.2.m2.2.2.2" xref="S4.I2.i1.p1.2.m2.2.2.2.cmml">​</mo><mrow id="S4.I2.i1.p1.2.m2.2.2.3.2" xref="S4.I2.i1.p1.2.m2.2.2.3.1.cmml"><mo stretchy="false" id="S4.I2.i1.p1.2.m2.2.2.3.2.1" xref="S4.I2.i1.p1.2.m2.2.2.3.1.1.cmml">|</mo><mi id="S4.I2.i1.p1.2.m2.1.1" xref="S4.I2.i1.p1.2.m2.1.1.cmml">x</mi><mo stretchy="false" id="S4.I2.i1.p1.2.m2.2.2.3.2.2" xref="S4.I2.i1.p1.2.m2.2.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.2.m2.2b"><apply id="S4.I2.i1.p1.2.m2.2.2.cmml" xref="S4.I2.i1.p1.2.m2.2.2"><times id="S4.I2.i1.p1.2.m2.2.2.2.cmml" xref="S4.I2.i1.p1.2.m2.2.2.2"></times><apply id="S4.I2.i1.p1.2.m2.2.2.1.cmml" xref="S4.I2.i1.p1.2.m2.2.2.1"><csymbol cd="latexml" id="S4.I2.i1.p1.2.m2.2.2.1.2.cmml" xref="S4.I2.i1.p1.2.m2.2.2.1.2">percent</csymbol><apply id="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1"><plus id="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.1.cmml" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.1"></plus><ci id="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.2.cmml" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.2">𝑥</ci><cn type="integer" id="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.3.cmml" xref="S4.I2.i1.p1.2.m2.2.2.1.1.1.1.3">1</cn></apply></apply><apply id="S4.I2.i1.p1.2.m2.2.2.3.1.cmml" xref="S4.I2.i1.p1.2.m2.2.2.3.2"><abs id="S4.I2.i1.p1.2.m2.2.2.3.1.1.cmml" xref="S4.I2.i1.p1.2.m2.2.2.3.2.1"></abs><ci id="S4.I2.i1.p1.2.m2.1.1.cmml" xref="S4.I2.i1.p1.2.m2.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.2.m2.2c">(x+1)\ \%\ |x|</annotation></semantics></math>.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">The <em id="S4.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Additive noise attack</em> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2020a</a>)</cite> adds some noise to the parameters of outgoing models. When the noise has a larger variance, it can indeed prevent convergence but also makes the noise attack easier to detect <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2020a</a>)</cite>. Centering the noise around a value slightly different from 0 allows the attack to prevent convergence despite low variance, but since the mean of benign updates is always centered around 0, this attack can be easily detected. We consider a variant where each half of the parameters are set to noise centered around a value just below and above 0, respectively.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">The <em id="S4.I2.i3.p1.1.1" class="ltx_emph ltx_font_italic">Krum attack</em> <cite class="ltx_cite ltx_citemacro_citep">(Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">[n.d.]</a>)</cite> specifically targets the Krum aggregation rule. It is an effective, state-of-the-art attack by iteratively sending attack vectors that will be accepted by Krum whilst inflicting maximum damage to the peer’s model.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">The <em id="S4.I2.i4.p1.1.1" class="ltx_emph ltx_font_italic">Trimmed Mean attack</em> <cite class="ltx_cite ltx_citemacro_citep">(Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">[n.d.]</a>)</cite> targets the trimmed mean GAR (Bridge in our experiments). It determines the gradient direction for each parameter of the model and then creates an attack vector that points exactly in the opposite direction, scaled per parameter depending on the values of the other benign peers.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experimental Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We now evaluate the performance of Bristle.
Our evaluation answers the following questions: <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">(1) what is the achieved training speedup when applying transfer learning to DFL? (2) How does Bristle perform in the presence of Byzantine attackers in terms of model accuracy? (3) How does Bristle perform when classes are not uniformly distributed over peers (non-i.i.d.) in terms of model accuracy? (4) How does Bristle perform in an environment with both Byzantine attackers and non-i.i.d. classes? And (5) What are the communication and computational costs of Bristle?</em></p>
</div>
<figure id="S5.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x14.png" id="S5.F10.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F10.sf1.3.2" class="ltx_text" style="font-size:90%;">All peers own samples of all classes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x15.png" id="S5.F10.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F10.sf2.3.2" class="ltx_text" style="font-size:90%;">Each peer owns samples of 40% of the classes</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>. </span><span id="S5.F10.3.2" class="ltx_text" style="font-size:90%;">The performance of Bristle and other GARs when the data is i.i.d. and non-i.i.d.</span></figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Performance Gains of Transfer Learning</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We first evaluate the performance gains of transfer learning when all peers are honest, and the data is i.i.d.
For each experiment, we measure after every 10 iterations the performance of all peers and then take the average accuracy, defined as <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\frac{\#correct\ predictions}{\#test\ samples}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mfrac id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mrow id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S5.SS1.p1.1.m1.1.1.2.2" xref="S5.SS1.p1.1.m1.1.1.2.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.3" xref="S5.SS1.p1.1.m1.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1a" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.4" xref="S5.SS1.p1.1.m1.1.1.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1b" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.5" xref="S5.SS1.p1.1.m1.1.1.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1c" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.6" xref="S5.SS1.p1.1.m1.1.1.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1d" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.7" xref="S5.SS1.p1.1.m1.1.1.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1e" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.8" xref="S5.SS1.p1.1.m1.1.1.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1f" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.9" xref="S5.SS1.p1.1.m1.1.1.2.9.cmml">t</mi><mo lspace="0.350em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1g" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.10" xref="S5.SS1.p1.1.m1.1.1.2.10.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1h" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.11" xref="S5.SS1.p1.1.m1.1.1.2.11.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1i" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.12" xref="S5.SS1.p1.1.m1.1.1.2.12.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1j" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.13" xref="S5.SS1.p1.1.m1.1.1.2.13.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1k" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.14" xref="S5.SS1.p1.1.m1.1.1.2.14.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1l" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.15" xref="S5.SS1.p1.1.m1.1.1.2.15.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1m" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.16" xref="S5.SS1.p1.1.m1.1.1.2.16.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1n" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.17" xref="S5.SS1.p1.1.m1.1.1.2.17.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1o" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.18" xref="S5.SS1.p1.1.m1.1.1.2.18.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1p" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.19" xref="S5.SS1.p1.1.m1.1.1.2.19.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.2.1q" xref="S5.SS1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.2.20" xref="S5.SS1.p1.1.m1.1.1.2.20.cmml">s</mi></mrow><mrow id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml"><mi mathvariant="normal" id="S5.SS1.p1.1.m1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.3.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.3" xref="S5.SS1.p1.1.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1a" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.4" xref="S5.SS1.p1.1.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1b" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.5" xref="S5.SS1.p1.1.m1.1.1.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1c" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.6" xref="S5.SS1.p1.1.m1.1.1.3.6.cmml">t</mi><mo lspace="0.350em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1d" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.7" xref="S5.SS1.p1.1.m1.1.1.3.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1e" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.8" xref="S5.SS1.p1.1.m1.1.1.3.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1f" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.9" xref="S5.SS1.p1.1.m1.1.1.3.9.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1g" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.10" xref="S5.SS1.p1.1.m1.1.1.3.10.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1h" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.11" xref="S5.SS1.p1.1.m1.1.1.3.11.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1i" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.12" xref="S5.SS1.p1.1.m1.1.1.3.12.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.1.m1.1.1.3.1j" xref="S5.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.1.m1.1.1.3.13" xref="S5.SS1.p1.1.m1.1.1.3.13.cmml">s</mi></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><divide id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"></divide><apply id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2"><times id="S5.SS1.p1.1.m1.1.1.2.1.cmml" xref="S5.SS1.p1.1.m1.1.1.2.1"></times><ci id="S5.SS1.p1.1.m1.1.1.2.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2.2">#</ci><ci id="S5.SS1.p1.1.m1.1.1.2.3.cmml" xref="S5.SS1.p1.1.m1.1.1.2.3">𝑐</ci><ci id="S5.SS1.p1.1.m1.1.1.2.4.cmml" xref="S5.SS1.p1.1.m1.1.1.2.4">𝑜</ci><ci id="S5.SS1.p1.1.m1.1.1.2.5.cmml" xref="S5.SS1.p1.1.m1.1.1.2.5">𝑟</ci><ci id="S5.SS1.p1.1.m1.1.1.2.6.cmml" xref="S5.SS1.p1.1.m1.1.1.2.6">𝑟</ci><ci id="S5.SS1.p1.1.m1.1.1.2.7.cmml" xref="S5.SS1.p1.1.m1.1.1.2.7">𝑒</ci><ci id="S5.SS1.p1.1.m1.1.1.2.8.cmml" xref="S5.SS1.p1.1.m1.1.1.2.8">𝑐</ci><ci id="S5.SS1.p1.1.m1.1.1.2.9.cmml" xref="S5.SS1.p1.1.m1.1.1.2.9">𝑡</ci><ci id="S5.SS1.p1.1.m1.1.1.2.10.cmml" xref="S5.SS1.p1.1.m1.1.1.2.10">𝑝</ci><ci id="S5.SS1.p1.1.m1.1.1.2.11.cmml" xref="S5.SS1.p1.1.m1.1.1.2.11">𝑟</ci><ci id="S5.SS1.p1.1.m1.1.1.2.12.cmml" xref="S5.SS1.p1.1.m1.1.1.2.12">𝑒</ci><ci id="S5.SS1.p1.1.m1.1.1.2.13.cmml" xref="S5.SS1.p1.1.m1.1.1.2.13">𝑑</ci><ci id="S5.SS1.p1.1.m1.1.1.2.14.cmml" xref="S5.SS1.p1.1.m1.1.1.2.14">𝑖</ci><ci id="S5.SS1.p1.1.m1.1.1.2.15.cmml" xref="S5.SS1.p1.1.m1.1.1.2.15">𝑐</ci><ci id="S5.SS1.p1.1.m1.1.1.2.16.cmml" xref="S5.SS1.p1.1.m1.1.1.2.16">𝑡</ci><ci id="S5.SS1.p1.1.m1.1.1.2.17.cmml" xref="S5.SS1.p1.1.m1.1.1.2.17">𝑖</ci><ci id="S5.SS1.p1.1.m1.1.1.2.18.cmml" xref="S5.SS1.p1.1.m1.1.1.2.18">𝑜</ci><ci id="S5.SS1.p1.1.m1.1.1.2.19.cmml" xref="S5.SS1.p1.1.m1.1.1.2.19">𝑛</ci><ci id="S5.SS1.p1.1.m1.1.1.2.20.cmml" xref="S5.SS1.p1.1.m1.1.1.2.20">𝑠</ci></apply><apply id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3"><times id="S5.SS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.3.1"></times><ci id="S5.SS1.p1.1.m1.1.1.3.2.cmml" xref="S5.SS1.p1.1.m1.1.1.3.2">#</ci><ci id="S5.SS1.p1.1.m1.1.1.3.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3.3">𝑡</ci><ci id="S5.SS1.p1.1.m1.1.1.3.4.cmml" xref="S5.SS1.p1.1.m1.1.1.3.4">𝑒</ci><ci id="S5.SS1.p1.1.m1.1.1.3.5.cmml" xref="S5.SS1.p1.1.m1.1.1.3.5">𝑠</ci><ci id="S5.SS1.p1.1.m1.1.1.3.6.cmml" xref="S5.SS1.p1.1.m1.1.1.3.6">𝑡</ci><ci id="S5.SS1.p1.1.m1.1.1.3.7.cmml" xref="S5.SS1.p1.1.m1.1.1.3.7">𝑠</ci><ci id="S5.SS1.p1.1.m1.1.1.3.8.cmml" xref="S5.SS1.p1.1.m1.1.1.3.8">𝑎</ci><ci id="S5.SS1.p1.1.m1.1.1.3.9.cmml" xref="S5.SS1.p1.1.m1.1.1.3.9">𝑚</ci><ci id="S5.SS1.p1.1.m1.1.1.3.10.cmml" xref="S5.SS1.p1.1.m1.1.1.3.10">𝑝</ci><ci id="S5.SS1.p1.1.m1.1.1.3.11.cmml" xref="S5.SS1.p1.1.m1.1.1.3.11">𝑙</ci><ci id="S5.SS1.p1.1.m1.1.1.3.12.cmml" xref="S5.SS1.p1.1.m1.1.1.3.12">𝑒</ci><ci id="S5.SS1.p1.1.m1.1.1.3.13.cmml" xref="S5.SS1.p1.1.m1.1.1.3.13">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\frac{\#correct\ predictions}{\#test\ samples}</annotation></semantics></math>.
Figure <a href="#S4.F8" title="Figure 8 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the evolution of model accuracy for the baseline GARs and Bristle, both without transfer learning (Figure <a href="#S4.F8.sf1" title="In Figure 8 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a>) and with transfer learning (Figure <a href="#S4.F8.sf2" title="In Figure 8 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(b)</span></a>).
We note that Bristle requires a pre-trained model and therefore is not included in Figure <a href="#S4.F8.sf1" title="In Figure 8 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a>.
Figure <a href="#S4.F8.sf1" title="In Figure 8 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a> shows that the evaluated GARs perform quite similarly, although Mozi has a slower convergence rate since it assigns lower weights to benign models that have not been trained sufficiently yet to perform well.
Transfer learning dramatically improves both the convergence rate and the maximum accuracy after 300 iterations of all GARs, which also supports literature <cite class="ltx_cite ltx_citemacro_citep">(Joy
et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>.
More specifically, whereas reaching 70% accuracy takes on average 55 iterations with regular training, it takes merely four iterations with transfer learning, a reduction of 93%.
Reaching 90% accuracy takes on average 180 iterations with regular learning, whereas it only takes 30 iterations with transfer learning, a reduction of 83%.
This reduction in iterations to reach the desired level of model accuracy can significantly reduce the load on the network.</p>
</div>
<figure id="S5.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x16.png" id="S5.F11.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F11.sf1.3.2" class="ltx_text" style="font-size:90%;">Label-flip attack</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x17.png" id="S5.F11.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F11.sf2.3.2" class="ltx_text" style="font-size:90%;">Additive noise attack</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F11.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x18.png" id="S5.F11.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F11.sf3.3.2" class="ltx_text" style="font-size:90%;">Krum attack</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F11.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x19.png" id="S5.F11.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F11.sf4.3.2" class="ltx_text" style="font-size:90%;">Trimmed Mean attack</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>. </span><span id="S5.F11.3.2" class="ltx_text" style="font-size:90%;">The resilience of Bristle and other GARs against various Byzantine attacks when the data is non-i.i.d.</span></figcaption>
</figure>
<figure id="S5.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F12.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2110.11006/assets/x20.png" id="S5.F12.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F12.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F12.sf1.3.2" class="ltx_text" style="font-size:90%;">10% of the peers is malicious</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F12.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2110.11006/assets/x21.png" id="S5.F12.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F12.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F12.sf2.3.2" class="ltx_text" style="font-size:90%;">30% of the peers is malicious</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F12.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x22.png" id="S5.F12.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F12.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F12.sf3.3.2" class="ltx_text" style="font-size:90%;">50% of the peers is malicious</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F12.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x23.png" id="S5.F12.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F12.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F12.sf4.3.2" class="ltx_text" style="font-size:90%;">70% of the peers is malicious</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>. </span><span id="S5.F12.3.2" class="ltx_text" style="font-size:90%;">The resilience of Bristle and other GARs under a label-flip-attack, with a varying percentage of all peers being malicious, where the data is non-i.i.d.</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Byzantine Attacks</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We now evaluate the Byzantine-resilience of Bristle under different attacks when data is i.i.d.
Figure <a href="#S4.F9" title="Figure 9 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the effect of different attacks.
Bristle withstands all evaluated Byzantine attacks and quickly achieves high model accuracy.
Figure <a href="#S4.F9.sf1" title="In Figure 9 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(a)</span></a> shows that the label-flip attack prevents model convergence when using the FedAvg GAR, but is, despite its relatively small influence on the model’s parameters, successfully mitigated by all other GARs.
Figure <a href="#S4.F9.sf2" title="In Figure 9 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(b)</span></a> shows that the FedAvg and Median GARs are susceptible to the additive noise attack.
The Krum attack, shown in Figure <a href="#S4.F9.sf3" title="In Figure 9 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(c)</span></a>, is clearly very effective against Krum but has only a minor effect on the convergence rate of the other GARs.
The Trimmed Mean attack, see Figure <a href="#S4.F9.sf4" title="In Figure 9 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(d)</span></a>, is relatively ineffective against any Byzantine-resilient GAR. This is because all benign models are very close to each other in this scenario, making it hard for this attack to steer the model in another direction without clearly being an outlier.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Non-i.i.d. Classes</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.2" class="ltx_p">We now consider the scenario where the classes are non-i.i.d., see Figure <a href="#S5.F10" title="Figure 10 ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.
In this experiment, there are 10 peers, each of which has access to two random samples of four consecutive classes.
Thus, each peer <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">i</annotation></semantics></math> has access to classes <math id="S5.SS3.p1.2.m2.4" class="ltx_Math" alttext="\{i,(i+1)\%10,(i+2)\%10,(i+3)\%10\}" display="inline"><semantics id="S5.SS3.p1.2.m2.4a"><mrow id="S5.SS3.p1.2.m2.4.4.3" xref="S5.SS3.p1.2.m2.4.4.4.cmml"><mo stretchy="false" id="S5.SS3.p1.2.m2.4.4.3.4" xref="S5.SS3.p1.2.m2.4.4.4.cmml">{</mo><mi id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml">i</mi><mo id="S5.SS3.p1.2.m2.4.4.3.5" xref="S5.SS3.p1.2.m2.4.4.4.cmml">,</mo><mrow id="S5.SS3.p1.2.m2.2.2.1.1" xref="S5.SS3.p1.2.m2.2.2.1.1.cmml"><mrow id="S5.SS3.p1.2.m2.2.2.1.1.1" xref="S5.SS3.p1.2.m2.2.2.1.1.1.cmml"><mrow id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.2" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.cmml"><mi id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.2" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.2.cmml">i</mi><mo id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.1" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.1.cmml">+</mo><mn id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.3" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.3" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.SS3.p1.2.m2.2.2.1.1.1.2" xref="S5.SS3.p1.2.m2.2.2.1.1.1.2.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S5.SS3.p1.2.m2.2.2.1.1.2" xref="S5.SS3.p1.2.m2.2.2.1.1.2.cmml">​</mo><mn id="S5.SS3.p1.2.m2.2.2.1.1.3" xref="S5.SS3.p1.2.m2.2.2.1.1.3.cmml">10</mn></mrow><mo id="S5.SS3.p1.2.m2.4.4.3.6" xref="S5.SS3.p1.2.m2.4.4.4.cmml">,</mo><mrow id="S5.SS3.p1.2.m2.3.3.2.2" xref="S5.SS3.p1.2.m2.3.3.2.2.cmml"><mrow id="S5.SS3.p1.2.m2.3.3.2.2.1" xref="S5.SS3.p1.2.m2.3.3.2.2.1.cmml"><mrow id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.2" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.cmml"><mi id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.2" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.2.cmml">i</mi><mo id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.1" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.1.cmml">+</mo><mn id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.3" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.3.cmml">2</mn></mrow><mo stretchy="false" id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.3" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.cmml">)</mo></mrow><mo id="S5.SS3.p1.2.m2.3.3.2.2.1.2" xref="S5.SS3.p1.2.m2.3.3.2.2.1.2.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S5.SS3.p1.2.m2.3.3.2.2.2" xref="S5.SS3.p1.2.m2.3.3.2.2.2.cmml">​</mo><mn id="S5.SS3.p1.2.m2.3.3.2.2.3" xref="S5.SS3.p1.2.m2.3.3.2.2.3.cmml">10</mn></mrow><mo id="S5.SS3.p1.2.m2.4.4.3.7" xref="S5.SS3.p1.2.m2.4.4.4.cmml">,</mo><mrow id="S5.SS3.p1.2.m2.4.4.3.3" xref="S5.SS3.p1.2.m2.4.4.3.3.cmml"><mrow id="S5.SS3.p1.2.m2.4.4.3.3.1" xref="S5.SS3.p1.2.m2.4.4.3.3.1.cmml"><mrow id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.2" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.cmml"><mi id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.2" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.2.cmml">i</mi><mo id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.1" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.1.cmml">+</mo><mn id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.3" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.3.cmml">3</mn></mrow><mo stretchy="false" id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.3" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.cmml">)</mo></mrow><mo id="S5.SS3.p1.2.m2.4.4.3.3.1.2" xref="S5.SS3.p1.2.m2.4.4.3.3.1.2.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S5.SS3.p1.2.m2.4.4.3.3.2" xref="S5.SS3.p1.2.m2.4.4.3.3.2.cmml">​</mo><mn id="S5.SS3.p1.2.m2.4.4.3.3.3" xref="S5.SS3.p1.2.m2.4.4.3.3.3.cmml">10</mn></mrow><mo stretchy="false" id="S5.SS3.p1.2.m2.4.4.3.8" xref="S5.SS3.p1.2.m2.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.4b"><set id="S5.SS3.p1.2.m2.4.4.4.cmml" xref="S5.SS3.p1.2.m2.4.4.3"><ci id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">𝑖</ci><apply id="S5.SS3.p1.2.m2.2.2.1.1.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1"><times id="S5.SS3.p1.2.m2.2.2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1.2"></times><apply id="S5.SS3.p1.2.m2.2.2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1.1"><csymbol cd="latexml" id="S5.SS3.p1.2.m2.2.2.1.1.1.2.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1.1.2">percent</csymbol><apply id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1"><plus id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.1.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.1"></plus><ci id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1.1.1.1.1.3">1</cn></apply></apply><cn type="integer" id="S5.SS3.p1.2.m2.2.2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.2.2.1.1.3">10</cn></apply><apply id="S5.SS3.p1.2.m2.3.3.2.2.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2"><times id="S5.SS3.p1.2.m2.3.3.2.2.2.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2.2"></times><apply id="S5.SS3.p1.2.m2.3.3.2.2.1.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2.1"><csymbol cd="latexml" id="S5.SS3.p1.2.m2.3.3.2.2.1.2.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2.1.2">percent</csymbol><apply id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1"><plus id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.1.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.1"></plus><ci id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.2.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.2">𝑖</ci><cn type="integer" id="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.3.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2.1.1.1.1.3">2</cn></apply></apply><cn type="integer" id="S5.SS3.p1.2.m2.3.3.2.2.3.cmml" xref="S5.SS3.p1.2.m2.3.3.2.2.3">10</cn></apply><apply id="S5.SS3.p1.2.m2.4.4.3.3.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3"><times id="S5.SS3.p1.2.m2.4.4.3.3.2.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3.2"></times><apply id="S5.SS3.p1.2.m2.4.4.3.3.1.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3.1"><csymbol cd="latexml" id="S5.SS3.p1.2.m2.4.4.3.3.1.2.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3.1.2">percent</csymbol><apply id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1"><plus id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.1.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.1"></plus><ci id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.2.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.2">𝑖</ci><cn type="integer" id="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.3.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3.1.1.1.1.3">3</cn></apply></apply><cn type="integer" id="S5.SS3.p1.2.m2.4.4.3.3.3.cmml" xref="S5.SS3.p1.2.m2.4.4.3.3.3">10</cn></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.4c">\{i,(i+1)\%10,(i+2)\%10,(i+3)\%10\}</annotation></semantics></math>.
When we compare Figure <a href="#S5.F10.sf2" title="In Figure 10 ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a> with Figure <a href="#S5.F10.sf1" title="In Figure 10 ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(a)</span></a>, it is clear that Krum and Mozi fail to achieve desired model accuracy with non-i.i.d. classes, and we also observe that Bridge and Median converge significantly slower.
FedAvg and Bristle show excellent performance and achieve 90% model accuracy in 65 and 55 iterations, respectively.
This is because these methods (eventually) combine the information learned by every peer, while the other methods disregard a part of the received models under the incorrect assumption that those are Byzantine.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Combining Byzantine Attacks and Non-i.i.d. Classes</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We now evaluate Bristle and other GARs in an environment containing both Byzantine attackers and where the classes are non-i.i.d. (see Figure <a href="#S5.F11" title="Figure 11 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>).
We use the label-flip attack in all experiments because this one is effective (see Figure <a href="#S5.F11.sf1" title="In Figure 11 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(a)</span></a>) and very popular in related work <cite class="ltx_cite ltx_citemacro_citep">(Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">[n.d.]a</a>; Muñoz-González et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2019</a>; Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">[n.d.]</a>)</cite>.
Except for Bristle, all baselines fare poorly in the Byzantine experiments with non-i.i.d. classes.
Specifically, we observe that FedAvg is unable to defend against Byzantine attacks, although it achieves an accuracy of 87% in the first few iterations of the Krum attack (see Figure <a href="#S5.F11.sf3" title="In Figure 11 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(c)</span></a>).
Although Bridge performs well in the experiments presented in Section <a href="#S5.SS2" title="5.2. Byzantine Attacks ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, it performs poorly under the threat of Byzantine attacks when the classes are non-i.i.d.
Krum is, as expected, unable to defend against the Krum attack but performs under the threat of the other attacks roughly on par with Mozi.
Mozi shows the same accuracy as in the benign non-i.i.d. experiments (see Figure <a href="#S4.F9" title="Figure 9 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>) because it successfully defends against Byzantine attacks but is also maxed out on the poor maximum accuracy that it can attain on the node’s own dataset.
The performance of the Median baseline varies significantly depending on the attack and may be quite inconsistent (as illustrated in Figure <a href="#S5.F11.sf2" title="In Figure 11 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(b)</span></a> and Figure <a href="#S5.F11.sf4" title="In Figure 11 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(d)</span></a>).
An interesting finding is that the Trimmed Mean attack is relatively ineffective in the i.i.d. experiment (see Figure <a href="#S4.F9.sf4" title="In Figure 9 ‣ 4. Experimental Setup ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(d)</span></a>) but impacts the performance of the same baselines significantly in the non-i.i.d. experiment (see Figure <a href="#S5.F11.sf4" title="In Figure 11 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(d)</span></a>).
This results from the greater distance between the benign models in the non-i.i.d. experiment, giving the attack more leeway to steer the model in a different direction without being considered an outlier.
Since the PBI assigns a weight of 0 to the malicious CSPs, Bristle consistently outperforms all other GARs and defends well against all evaluated Byzantine attacks.</p>
</div>
<figure id="S5.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x24.png" id="S5.F13.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F13.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F13.sf1.3.2" class="ltx_text" style="font-size:90%;">Each peer has data of 20% of the classes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x25.png" id="S5.F13.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F13.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F13.sf2.3.2" class="ltx_text" style="font-size:90%;">Each peer has data of 60% of the classes</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>. </span><span id="S5.F13.3.2" class="ltx_text" style="font-size:90%;">The resilience of Bristle and other GARs under a label-flip-attack, with 30% of all peers being malicious, where the classes are to a varying extent non-i.i.d.</span></figcaption>
</figure>
<figure id="S5.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x26.png" id="S5.F14.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F14.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F14.sf1.3.2" class="ltx_text" style="font-size:90%;">Each peer is connected to 2% of the other peers</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2110.11006/assets/x27.png" id="S5.F14.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F14.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F14.sf2.3.2" class="ltx_text" style="font-size:90%;">Each peer is connected to 5% of the other peers</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F14.2.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>. </span><span id="S5.F14.3.2" class="ltx_text" style="font-size:90%;">The resilience of Bristle and other GARs under a label-flip-attack, with a varying percentage of the peers connected, where the classes are non-i.i.d.</span></figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Varying the Number of Byzantine Attackers.</span>
We now vary the number of Byzantine attackers, see Figure <a href="#S5.F12" title="Figure 12 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.
Figure <a href="#S5.F12.sf1" title="In Figure 12 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(a)</span></a> shows that even with 10% Byzantine attackers, the performance of the baselines already degrades significantly.
Bristle manages to maintain a quick increase in accuracy for all considered attack scenarios.
With 10% and 30% Byzantine attackers, Mozi manages to keep a stable performance but is limited to predicting only the peer’s familiar classes correctly.
Figure <a href="#S5.F12.sf4" title="In Figure 12 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12(d)</span></a> shows that the performance of Krum is inconsistent and that only Bristle achieves a consistent high performance.
Bristle’s excellent performance results from the fact that (a) in contrast to Mozi, Bristle evaluates and integrates the parameters per class instead of per model, and (b) in contrast to the other GARs, Bristle uses performance evaluations instead of just distance comparisons.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">Varying the Degree of Non-i.i.d.-ness.</span>
We now compare the performance of the GARs in three situations where the classes are to a varying extent non-i.i.d. (see Figure <a href="#S5.F13" title="Figure 13 ‣ 5.4. Combining Byzantine Attacks and Non-i.i.d. Classes ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>).
FedAvg and Bridge are, regardless of the degree of non-i.i.d.-ness, unable to provide any resilience to the label-flip attack.
Mozi defends, similarly to the experiments in Figure <a href="#S5.F11" title="Figure 11 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, perfectly well against the label-flip attack but is limited to the maximum accuracy that can be obtained by training on its own dataset.
The performance of Krum increases with the number of classes owned by the peer.
The Median rule performs relatively well, even when the classes are highly non-i.i.d.
Bristle, however, clearly performs best compared to all baselines, although its performance decreases when the peers have only 20% of the data (see Figure <a href="#S5.F13.sf1" title="In Figure 13 ‣ 5.4. Combining Byzantine Attacks and Non-i.i.d. Classes ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13(a)</span></a>).</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p"><span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_bold">Varying the Connection Ratio.</span>
In larger networks, it is unrealistic to assume all-to-all dissemination of model updates.
To test the impact of the connection rate on the convergence rate more accurately, we set up an environment with 100 peers where the classes are again non-i.i.d. similarly to the previous experiments.
We setup two experiments, connect each peer to a random subset of 2% and 5% of the other peers respectively, add an equal number of label-flip attackers that are connected to each benign peer, and measure the average accuracy over time.
From Figure <a href="#S5.F14.sf1" title="In Figure 14 ‣ 5.4. Combining Byzantine Attacks and Non-i.i.d. Classes ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(a)</span></a>, we observe that when the connection rate is only 2%, FedAvg and Median perform very poorly, but also the other GARs are unable to achieve satisfactory accuracy.
When the connection rate increases to 5% in Figure <a href="#S5.F14.sf2" title="In Figure 14 ‣ 5.4. Combining Byzantine Attacks and Non-i.i.d. Classes ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(b)</span></a>, Bristle and Krum perform quite well in contrast to the other baselines.
Thus, it seems that in a setting with 100 peers, a connection rate of only 5% is already almost enough to reach the maximum accuracy.
The number of iterations required to obtain this accuracy is relatively high compared to a connection rate of 100% (see Figure <a href="#S5.F11.sf1" title="In Figure 11 ‣ 5.1. Performance Gains of Transfer Learning ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11(a)</span></a>).</p>
</div>
<figure id="S5.F15" class="ltx_figure"><img src="/html/2110.11006/assets/x28.png" id="S5.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="345" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F15.2.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>. </span><span id="S5.F15.3.2" class="ltx_text" style="font-size:90%;">Average time to complete a single training iteration on the non-i.i.d. label-flip experiment with a varying number of connected peers.</span></figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Bristle Efficiency and Scalability</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.2" class="ltx_p">Figure <a href="#S5.F15" title="Figure 15 ‣ 5.4. Combining Byzantine Attacks and Non-i.i.d. Classes ‣ 5. Experimental Evaluation ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> shows for each GAR the average time it takes for a peer to finish a single iteration depending on the number of connected peers.
For evaluation purposes we assume that each peer receives from every other peer exactly one model at every iteration.
The running time of Average is clearly the fastest, followed closely by Median, Krum, and Bridge.
The fast execution of these GARs results from the usage of relatively inexpensive operations, such as calculating the distance between models and sorting the parameters of each dimension.
Mozi is relatively slow because it has to evaluate the accuracy of each model.
Bristle is initially by far the slowest GAR caused by the performance-based integrator that evaluates the performance not just for each model but for each familiar class of each model.
However, when the number of connected peers exceeds <math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mi id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><ci id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">\beta</annotation></semantics></math> (see Section <a href="#S3.SS2" title="3.2. Distance-based prioritizer (DBP) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>), adding more peers has a negligible impact on the speed of Bristle because the distance-based prioritizer is efficient in reducing the number of models to <math id="S5.SS5.p1.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S5.SS5.p1.2.m2.1a"><mi id="S5.SS5.p1.2.m2.1.1" xref="S5.SS5.p1.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.2.m2.1b"><ci id="S5.SS5.p1.2.m2.1.1.cmml" xref="S5.SS5.p1.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.2.m2.1c">\beta</annotation></semantics></math>.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">All baselines consume an equal amount of network traffic (roughly 3 MB per iteration per peer for our neural network), but because Bristle transmits only the output layer (roughly 300 KB), the bandwidth requirements of Bristle are reduced by 90%.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Related work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In the last five years, a considerable amount of literature has been devoted to creating more effective FL systems.
Most work focuses on achieving Byzantine-resilience but also introduces several methods to reduce the bandwidth requirements and to improve learning on non-i.i.d. classes.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Byzantine-resilience. </span>
Several well-known Byzantine-resilient GARs are Coordinate-wise Median (CM) <cite class="ltx_cite ltx_citemacro_citep">(Muñoz-González et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2019</a>)</cite> which takes the median across all models for each parameter, Krum <cite class="ltx_cite ltx_citemacro_citep">(Blanchard
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">[n.d.]</a>)</cite> which selects the model that most closely resembles all other models, and Bulyan <cite class="ltx_cite ltx_citemacro_citep">(Mhamdi
et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2018</a>)</cite> which iteratively applies Krum followed by a variant of CM.
However, these GARs use distances as a proxy for benignness, which it not reliable as we have seen in Section <a href="#S3.SS2" title="3.2. Distance-based prioritizer (DBP) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
In contrast, performance-based methods reject or accept received models based on their performance on a test dataset.
Examples include RONI (Reject On Negative Influence) <cite class="ltx_cite ltx_citemacro_citep">(Barreno
et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2010</a>)</cite> which simply discards all models with a negative impact on the model, and Zeno <cite class="ltx_cite ltx_citemacro_citep">(Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">[n.d.]a</a>)</cite> / Zeno++ <cite class="ltx_cite ltx_citemacro_citep">(Xie
et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2019</a>)</cite> which use a central oracle to estimate the true gradient and only keep the gradients most similar to this estimation.
Bristle uses distances only to prioritize the received models and uses per-class performance measurements to integrate models.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Communication efficiency. </span>
Because of the bandwidth limitations of cellular networks, numerous methods were proposed to improve the communication efficiency of FL.
A popular method is to quantize the gradients to low-precision values <cite class="ltx_cite ltx_citemacro_citep">(Wen
et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2017</a>; Alistarh
et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2016</a>; De Sa
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">[n.d.]</a>)</cite> or only to transmit the most important parameters (sparse matrix methods) <cite class="ltx_cite ltx_citemacro_citep">(Strom, <a href="#bib.bib70" title="" class="ltx_ref">[n.d.]</a>; Konečný and
Richtárik, <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>. Bristle only updates the output layer, which works well together with existing techniques to reduce the communication overhead.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold">Non-i.i.d. learning. </span>
Several methods enable learning on non-i.i.d. classes, such as by sharing a small i.i.d. training dataset across all peers <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2018</a>)</cite> or by reusing non-federated continual learning techniques <cite class="ltx_cite ltx_citemacro_citep">(Kopparapu and Lin, <a href="#bib.bib42" title="" class="ltx_ref">2020</a>; Kumar
et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">[n.d.]</a>; Yao and Sun, <a href="#bib.bib90" title="" class="ltx_ref">[n.d.]</a>; Gonzalez
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>.
Bristle also lends several concepts from a non-federated continual learning technique, namely CWR* <cite class="ltx_cite ltx_citemacro_citep">(Lomonaco
et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite> (see Section <a href="#S3.SS3" title="3.3. Performance-based Integrator (PBI) ‣ 3. Bristle: Middleware for Decentralized Federated Learning ‣ Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d. Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p"><span id="S6.p5.1.1" class="ltx_text ltx_font_bold">FL systems. </span>
Several FL systems try to combine Byzantine-resilience with learning on non-i.i.d. data.
RSA <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">[n.d.]</a>)</cite> uses a regularization-based strategy and although it performs relatively well when the data is non-i.i.d., it fares poorly against Byzantine attackers.
FoolsGold <cite class="ltx_cite ltx_citemacro_citep">(Fung
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite> detects and rejects attacks executed by multiple sybils working together and works well with non-i.i.d. data.
However, Zhao et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2020</a>)</cite> show that the Byzantine-resilience of FoolsGold is quite limited.
FLeet <cite class="ltx_cite ltx_citemacro_citep">(Damaskinos et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020b</a>)</cite> uses past observed staleness values and similarities with past learning tasks to achieve learning on non-i.i.d. data for a specific type of ”soft” Byzantine attacks (namely the presence of stale models) but is unsuitable for other types.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Concluding Remarks</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We have presented Bristle, middleware for decentralized, federated learning that tolerates Byzantine attacks, even when the classes of the training data are non-i.i.d.
By leveraging deep transfer learning, Bristle achieves a high convergence rate despite having a low communication overhead.
Through the combination of a fast distance-based prioritizer and a per-class performance integrator, Bristle is able to withstand attacks targeted at subverting the model accuracy.
Our experimental evaluation using the popular MNIST dataset has demonstrated these desirable properties and shows that Bristle exhibits superior performance compared to related solutions.
In this evaluation, we did not focus on privacy, communication compression, or other aspects that are supplementary to Bristle and addressed in the existing literature.
We have implemented and open-sourced Bristle on GitHub.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Although we believe that Bristle is a significant step forward for DFL, we identify two directions for further work.
First, communication costs could be further reduced by selectively sending parameters to peers with high class overlap. This can be estimated using Private Set Intersection Cardinality (PSI-CA).
One can prevent peers from learning too much about the class distribution of others, e.g., by adding noise to the cardinality and by rate-limiting PSI-CA requests.
Second, the accuracy could be increased even more when the non-output layers can also be fine-tuned rather than being fixed.
Several popular continual learning algorithms are suitable for this purpose, such as LWF <cite class="ltx_cite ltx_citemacro_citep">(Li and Hoiem, <a href="#bib.bib51" title="" class="ltx_ref">2017</a>)</cite>, AR1 <cite class="ltx_cite ltx_citemacro_citep">(Lomonaco
et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite>, or EWC <cite class="ltx_cite ltx_citemacro_citep">(Kirkpatrick et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2017</a>)</cite>.
However, tuning also the non-output layers requires the transmission of information about these non-output layers, significantly increasing the communication costs.
Additionally, it is non-trivial to maintain the Byzantine-resilience property because the parameters in the hidden layers do not correspond directly with the classes to be predicted.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RN3 ([n.d.])</span>
<span class="ltx_bibblock">
[n.d.].

</span>
<span class="ltx_bibblock">Google Trends - transfer learning popularity
worldwide.

</span>
<span class="ltx_bibblock">([n. d.]).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://trends.google.com/trends/explore?date=all" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://trends.google.com/trends/explore?date=all</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RN9 (2017)</span>
<span class="ltx_bibblock">
2017.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.googleblog.com/2017/04/federated-learning-collaborative.html</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alistarh
et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Dan Alistarh, Jerry Li,
Ryota Tomioka, and Milan Vojnovic.
2016.

</span>
<span class="ltx_bibblock">Qsgd: Randomized quantization for
communication-optimal stochastic gradient descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02132</em>
1 (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan,
Andreas Veit, Yiqing Hua,
Deborah Estrin, and Vitaly Shmatikov.
[n.d.].

</span>
<span class="ltx_bibblock">How to backdoor federated learning. In
<em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence
and Statistics</em>. PMLR, 2938–2948.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barreno
et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Marco Barreno, Blaine
Nelson, Anthony D Joseph, and J Doug
Tygar. 2010.

</span>
<span class="ltx_bibblock">The security of machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Machine Learning</em> 81,
2 (2010), 121–148.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard
et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Peva Blanchard, Rachid
Guerraoui, and Julien Stainer.
[n.d.].

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine
tolerant gradient descent. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>. 119–129.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz
et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert
Eichner, Wolfgang Grieskamp, Dzmitry
Huba, Alex Ingerman, Vladimir Ivanov,
Chloe Kiddon, Jakub Konečný,
Stefano Mazzocchi, and H Brendan
McMahan. 2019.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System
design.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.01046</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bottou (2012)</span>
<span class="ltx_bibblock">
Léon Bottou.
2012.

</span>
<span class="ltx_bibblock">Stochastic gradient descent tricks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Neural networks: Tricks of the trade</em>.
Springer, 421–436.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brisimi et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Theodora S Brisimi, Ruidi
Chen, Theofanie Mela, Alex Olshevsky,
Ioannis Ch Paschalidis, and Wei Shi.
2018.

</span>
<span class="ltx_bibblock">Federated learning of predictive models from
federated electronic health records.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">International journal of medical
informatics</em> 112 (2018),
59–67.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chapelle and
Harchaoui (2005)</span>
<span class="ltx_bibblock">
Olivier Chapelle and
Zaıd Harchaoui. 2005.

</span>
<span class="ltx_bibblock">A machine learning approach to conjoint analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 17 (2005),
257–264.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Mingqing Chen, Rajiv
Mathews, Tom Ouyang, and Françoise
Beaufays. 2019a.

</span>
<span class="ltx_bibblock">Federated learning of out-of-vocabulary words.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.10635</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Mingzhe Chen, H Vincent
Poor, Walid Saad, and Shuguang Cui.
2020a.

</span>
<span class="ltx_bibblock">Wireless Communications for Collaborative Federated
Learning in the Internet of Things.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.02499</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Mingqing Chen,
Ananda Theertha Suresh, Rajiv Mathews,
Adeline Wong, Cyril Allauzen,
Françoise Beaufays, and Michael
Riley. 2019b.

</span>
<span class="ltx_bibblock">Federated learning of N-gram language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03432</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yudong Chen, Lili Su,
and Jiaming Xu. 2017.

</span>
<span class="ltx_bibblock">Distributed statistical machine learning in
adversarial settings: Byzantine gradient descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Measurement and
Analysis of Computing Systems</em> 1, 2
(2017), 1–25.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Zheyi Chen, Pu Tian,
Weixian Liao, and Wei Yu.
2020b.

</span>
<span class="ltx_bibblock">Zero Knowledge Clustering Based Adversarial
Mitigation in Heterogeneous Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and
Engineering</em> (2020).

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chicco and Jurman (2020)</span>
<span class="ltx_bibblock">
Davide Chicco and
Giuseppe Jurman. 2020.

</span>
<span class="ltx_bibblock">The advantages of the Matthews correlation
coefficient (MCC) over F1 score and accuracy in binary classification
evaluation.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">BMC genomics</em> 21,
1 (2020), 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chui et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Michael Chui, James
Manyika, Mehdi Miremadi, Nicolaus Henke,
Rita Chung, Pieter Nel, and
Sankalp Malhotra. 2018.

</span>
<span class="ltx_bibblock">Notes from the AI frontier: Insights from hundreds
of use cases.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">McKinsey Global Institute</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Gregory Cohen, Saeed
Afshar, Jonathan Tapson, and Andre
Van Schaik. 2017.

</span>
<span class="ltx_bibblock">EMNIST: Extending MNIST to handwritten letters. In
<em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">2017 International Joint Conference on Neural
Networks (IJCNN)</em>. IEEE, 2921–2926.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damaskinos et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Georgios Damaskinos,
Rachid Guerraoui, Anne-Marie Kermarrec,
Vlad Nitu, Rhicheek Patra, and
Francois Taiani. 2020a.

</span>
<span class="ltx_bibblock">Fleet: Online federated learning via staleness
awareness and performance prediction. In
<em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st International Middleware
Conference</em>. 163–177.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damaskinos et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Georgios Damaskinos,
Rachid Guerraoui, Anne-Marie Kermarrec,
Vlad Nitu, Rhicheek Patra, and
Francois Taiani. 2020b.

</span>
<span class="ltx_bibblock">Fleet: Online federated learning via staleness
awareness and performance prediction. In
<em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st International Middleware
Conference</em>. 163–177.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Sa
et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Christopher De Sa, Matthew
Feldman, Christopher Ré, and Kunle
Olukotun. [n.d.].

</span>
<span class="ltx_bibblock">Understanding and optimizing asynchronous
low-precision stochastic gradient descent. In
<em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 44th Annual International
Symposium on Computer Architecture</em>. 561–574.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dobbe
et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Roel Dobbe, David
Fridovich-Keil, and Claire Tomlin.
[n.d.].

</span>
<span class="ltx_bibblock">Fully decentralized policies for multi-agent
systems: An information theoretic approach. In
<em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>. 2941–2950.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douceur ([n.d.])</span>
<span class="ltx_bibblock">
John R Douceur.
[n.d.].

</span>
<span class="ltx_bibblock">The sybil attack. In
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International workshop on peer-to-peer systems</em>.
Springer, 251–260.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Durkee (2010)</span>
<span class="ltx_bibblock">
Dave Durkee.
2010.

</span>
<span class="ltx_bibblock">Why cloud computing will never be free.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em> 53,
5 (2010), 62–69.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Endler
et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Markus Endler, Alexandre
Skyrme, Daniel Schuster, and Thomas
Springer. 2011.

</span>
<span class="ltx_bibblock">Defining situated social context for pervasive
social computing. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">2011 IEEE International
Conference on Pervasive Computing and Communications Workshops (PERCOM
Workshops)</em>. IEEE, 519–524.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang
et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu
Cao, Jinyuan Jia, and Neil Gong.
[n.d.].

</span>
<span class="ltx_bibblock">Local model poisoning attacks to Byzantine-robust
federated learning. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">29th USENIX Security
Symposium (USENIX Security 20)</em>. 1605–1622.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Felber and
Biersack (2004)</span>
<span class="ltx_bibblock">
P Felber and Ernst W
Biersack. 2004.

</span>
<span class="ltx_bibblock">Self-scaling networks for content distribution. In
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. International Workshop on Self-* Properties
in Complex Information Systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">for Disease Control et al<span id="bib.bib29.3.3.1" class="ltx_text">.</span> (2003)</span>
<span class="ltx_bibblock">
Centers for Disease Control,
Prevention, et al<span id="bib.bib29.4.1" class="ltx_text">.</span>
2003.

</span>
<span class="ltx_bibblock">HIPAA privacy rule and public health. Guidance from
CDC and the US Department of Health and Human Services.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.5.1" class="ltx_emph ltx_font_italic">MMWR: Morbidity and mortality weekly report</em>
52, Suppl 1 (2003),
1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fung
et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Clement Fung, Chris JM
Yoon, and Ivan Beschastnikh.
2018.

</span>
<span class="ltx_bibblock">Mitigating sybils in federated learning poisoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.04866</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonzalez
et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Camila Gonzalez, Georgios
Sakas, and Anirban Mukhopadhyay.
2020.

</span>
<span class="ltx_bibblock">What is Wrong with Continual Learning in Medical
Image Segmentation?

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11008</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo
et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Shangwei Guo, Tianwei
Zhang, Xiaofei Xie, Lei Ma,
Tao Xiang, and Yang Liu.
2020.

</span>
<span class="ltx_bibblock">Towards Byzantine-resilient Learning in
Decentralized Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.08569</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka
Rao, Rajiv Mathews, Swaroop Ramaswamy,
Françoise Beaufays, Sean Augenstein,
Hubert Eichner, Chloé Kiddon, and
Daniel Ramage. 2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He
et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu
Zhang, Shaoqing Ren, and Jian Sun.
2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. In
<em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>. 770–778.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hegedűs
et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
István Hegedűs, Gábor
Danner, and Márk Jelasity.
2021.

</span>
<span class="ltx_bibblock">Decentralized learning works: An empirical
comparison of gossip learning and federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">J. Parallel and Distrib. Comput.</em>
148 (2021), 109–124.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Hanpeng Hu, Dan Wang,
and Chuan Wu. 2020.

</span>
<span class="ltx_bibblock">Distributed machine learning through heterogeneous
edge systems. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, Vol. 34.
7179–7186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Gao Huang, Zhuang Liu,
Laurens Van Der Maaten, and Kilian Q
Weinberger. 2017.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks. In
<em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>. 4700–4708.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joy
et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Tinu Theckel Joy, Santu
Rana, Sunil Gupta, and Svetha
Venkatesh. 2019.

</span>
<span class="ltx_bibblock">A flexible transfer learning framework for Bayesian
optimization with convergence guarantee.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>
115 (2019), 656–672.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz
et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary
Charles, Graham Cormode, and Rachel
Cummings. 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.04977</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick et al<span id="bib.bib40.3.3.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan
Pascanu, Neil Rabinowitz, Joel Veness,
Guillaume Desjardins, Andrei A Rusu,
Kieran Milan, John Quan,
Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al<span id="bib.bib40.4.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.5.1" class="ltx_emph ltx_font_italic">Proceedings of the national academy of
sciences</em> 114, 13
(2017), 3521–3526.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečný and
Richtárik (2018)</span>
<span class="ltx_bibblock">
Jakub Konečný and
Peter Richtárik. 2018.

</span>
<span class="ltx_bibblock">Randomized distributed mean estimation: Accuracy
vs. communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Frontiers in Applied Mathematics and
Statistics</em> 4 (2018),
62.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopparapu and Lin (2020)</span>
<span class="ltx_bibblock">
Kavya Kopparapu and Eric
Lin. 2020.

</span>
<span class="ltx_bibblock">FedFMC: Sequential Efficient Federated Learning on
Non-iid Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.10937</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky
et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya
Sutskever, and Geoffrey E Hinton.
2012.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional
neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 25 (2012),
1097–1105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar
et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Swaraj Kumar, Sandipan
Dutta, Shaurya Chatturvedi, and MPS
Bhatia. [n.d.].

</span>
<span class="ltx_bibblock">Strategies for Enhancing Training and Privacy in
Blockchain Enabled Federated Learning. In <em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">2020
IEEE Sixth International Conference on Multimedia Big Data (BigMM)</em>.
IEEE, 333–340.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lalitha et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Anusha Lalitha, Xinghan
Wang, Osman Kilinc, Yongxi Lu,
Tara Javidi, and Farinaz Koushanfar.
2019.

</span>
<span class="ltx_bibblock">Decentralized bayesian learning over graphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10466</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lecun ([n.d.])</span>
<span class="ltx_bibblock">
Yann Lecun.
[n.d.].

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://yann.lecun.com/exdb/mnist" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://yann.lecun.com/exdb/mnist</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
He Li, Kaoru Ota, and
Mianxiong Dong. 2018.

</span>
<span class="ltx_bibblock">Learning IoT in edge: Deep learning for the
Internet of Things with edge computing.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">IEEE network</em> 32,
1 (2018), 96–101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Liping Li, Wei Xu,
Tianyi Chen, Georgios B Giannakis, and
Qing Ling. [n.d.].

</span>
<span class="ltx_bibblock">RSA: Byzantine-robust stochastic aggregation
methods for distributed learning from heterogeneous datasets. In
<em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, Vol. 33. 1544–1551.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Suyi Li, Yong Cheng,
Wei Wang, Yang Liu, and
Tianjian Chen. 2020a.

</span>
<span class="ltx_bibblock">Learning to Detect Malicious Clients for Robust
Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.00211</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu,
Ameet Talwalkar, and Virginia Smith.
2020b.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future
directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>
37, 3 (2020),
50–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Hoiem (2017)</span>
<span class="ltx_bibblock">
Zhizhong Li and Derek
Hoiem. 2017.

</span>
<span class="ltx_bibblock">Learning without forgetting.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and
machine intelligence</em> 40, 12
(2017), 2935–2947.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Xiangru Lian, Ce Zhang,
Huan Zhang, Cho-Jui Hsieh,
Wei Zhang, and Ji Liu.
[n.d.].

</span>
<span class="ltx_bibblock">Can decentralized algorithms outperform centralized
algorithms? a case study for decentralized parallel stochastic gradient
descent. In <em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>. 5330–5340.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lomonaco (2019)</span>
<span class="ltx_bibblock">
Vincenzo Lomonaco.
2019.

</span>
<span class="ltx_bibblock">Continual Learning with Deep Architectures.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lomonaco
et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Vincenzo Lomonaco, Davide
Maltoni, and Lorenzo Pellegrini.
2019.

</span>
<span class="ltx_bibblock">Rehearsal-Free Continual Learning over Small
Non-IID Batches.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.03799</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu
et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Songtao Lu, Yawen Zhang,
and Yunlong Wang. [n.d.].

</span>
<span class="ltx_bibblock">Decentralized federated learning for electronic
health records. In <em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">2020 54th Annual Conference on
Information Sciences and Systems (CISS)</em>. IEEE,
1–5.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas.
[n.d.].

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence and Statistics</em>. PMLR,
1273–1282.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mhamdi
et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
El Mahdi El Mhamdi, Rachid
Guerraoui, and Sébastien Rouault.
2018.

</span>
<span class="ltx_bibblock">The hidden vulnerability of distributed learning in
byzantium.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.07927</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muñoz-González et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Luis Muñoz-González,
Kenneth T Co, and Emil C Lupu.
2019.

</span>
<span class="ltx_bibblock">Byzantine-robust federated machine learning through
adaptive model averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.05125</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nedic and
Ozdaglar (2009)</span>
<span class="ltx_bibblock">
Angelia Nedic and Asuman
Ozdaglar. 2009.

</span>
<span class="ltx_bibblock">Distributed subgradient methods for multi-agent
optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Automat. Control</em>
54, 1 (2009),
48–61.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niknam
et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Solmaz Niknam, Harpreet S
Dhillon, and Jeffrey H Reed.
2020.

</span>
<span class="ltx_bibblock">Federated learning for wireless communications:
Motivation, opportunities, and challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>
58, 6 (2020),
46–51.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan (2010)</span>
<span class="ltx_bibblock">
Jialin Pan.
2010.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Feature-based transfer learning with real-world
applications</em>.

</span>
<span class="ltx_bibblock">Ph.D. Dissertation. Citeseer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rieke
et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Nicola Rieke, Jonny
Hancox, Wenqi Li, Fausto Milletari,
Holger Roth, Shadi Albarqouni,
Spyridon Bakas, Mathieu N Galtier,
Bennett Landman, and Klaus Maier-Hein.
2020.

</span>
<span class="ltx_bibblock">The future of digital health with federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.08119</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler
et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Felix Sattler, Thomas
Wiegand, and Wojciech Samek.
2020.

</span>
<span class="ltx_bibblock">Trends and advancements in deep neural network
communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.03320</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schneble and
Thamilarasu ([n.d.])</span>
<span class="ltx_bibblock">
William Schneble and
Geethapriya Thamilarasu.
[n.d.].

</span>
<span class="ltx_bibblock">Attack Detection Using Federated Learning in
Medical Cyber-Physical Systems.

</span>
<span class="ltx_bibblock">([n. d.]).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sim et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Khe Chai Sim, Françoise
Beaufays, Arnaud Benard, Dhruv Guliani,
Andreas Kabel, Nikhil Khare,
Tamar Lucassen, Petr Zadrazil,
Harry Zhang, and Leif Johnson.
[n.d.].

</span>
<span class="ltx_bibblock">Personalization of end-to-end speech recognition on
mobile devices for named entities. In <em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">2019 IEEE
Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.
IEEE, 23–30.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith
et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Virginia Smith, Chao-Kai
Chiang, Maziar Sanjabi, and Ameet S
Talwalkar. [n.d.].

</span>
<span class="ltx_bibblock">Federated multi-task learning. In
<em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>. 4424–4434.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sozinov
et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Konstantin Sozinov,
Vladimir Vlassov, and Sarunas
Girdzijauskas. [n.d.].

</span>
<span class="ltx_bibblock">Human Activity Recognition Using Federated
Learning. In <em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">2018 IEEE Intl Conf on Parallel &amp;
Distributed Processing with Applications, Ubiquitous Computing &amp;
Communications, Big Data &amp; Cloud Computing, Social Computing &amp; Networking,
Sustainable Computing &amp; Communications
(ISPA/IUCC/BDCloud/SocialCom/SustainCom)</em>. IEEE,
1103–1111.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stokkink
et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Quinten Stokkink, Dick
Epema, and Johan Pouwelse.
2020.

</span>
<span class="ltx_bibblock">A Truly Self-Sovereign Identity System.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.00415</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stokkink and
Pouwelse ([n.d.])</span>
<span class="ltx_bibblock">
Quinten Stokkink and
Johan Pouwelse. [n.d.].

</span>
<span class="ltx_bibblock">Deployment of a blockchain-based self-sovereign
identity. In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference on
Internet of Things (iThings) and IEEE green computing and communications
(GreenCom) and IEEE cyber, physical and social computing (CPSCom) and IEEE
smart data (SmartData)</em>. IEEE,
1336–1342.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strom ([n.d.])</span>
<span class="ltx_bibblock">
Nikko Strom.
[n.d.].

</span>
<span class="ltx_bibblock">Scalable distributed DNN training using commodity
GPU cloud computing. In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Sixteenth Annual
Conference of the International Speech Communication Association</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su and Vaidya (2015)</span>
<span class="ltx_bibblock">
Lili Su and Nitin H
Vaidya. 2015.

</span>
<span class="ltx_bibblock">Fault-tolerant distributed optimization (Part IV):
Constrained optimization with arbitrary directed networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1511.01821</em>
(2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundaram and
Gharesifard (2018)</span>
<span class="ltx_bibblock">
Shreyas Sundaram and
Bahman Gharesifard. 2018.

</span>
<span class="ltx_bibblock">Distributed optimization under adversarial nodes.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Automat. Control</em>
64, 3 (2018),
1063–1076.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan
et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Chuanqi Tan, Fuchun Sun,
Tao Kong, Wenchang Zhang,
Chao Yang, and Chunfang Liu.
2018.

</span>
<span class="ltx_bibblock">A survey on deep transfer learning. In
<em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">International conference on artificial neural
networks</em>. Springer, 270–279.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang
et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Hanlin Tang, Shaoduo Gan,
Ce Zhang, Tong Zhang, and
Ji Liu. [n.d.].

</span>
<span class="ltx_bibblock">Communication compression for decentralized
training. In <em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>. 7652–7662.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tolpegin
et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Vale Tolpegin, Stacey
Truex, Mehmet Emre Gursoy, and Ling
Liu. [n.d.].

</span>
<span class="ltx_bibblock">Data poisoning attacks against federated learning
systems. In <em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">European Symposium on Research in
Computer Security</em>. Springer,
480–501.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voigt and Von dem
Bussche (2017)</span>
<span class="ltx_bibblock">
Paul Voigt and Axel
Von dem Bussche. 2017.

</span>
<span class="ltx_bibblock">The eu general data protection regulation (gdpr).

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer
International Publishing</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallach (2002)</span>
<span class="ltx_bibblock">
Dan S Wallach.
2002.

</span>
<span class="ltx_bibblock">A survey of peer-to-peer security issues. In
<em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">International symposium on software security</em>.
Springer, 42–57.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Shiqiang Wang, Tiffany
Tuor, Theodoros Salonidis, Kin K Leung,
Christian Makaya, Ting He, and
Kevin Chan. 2019.

</span>
<span class="ltx_bibblock">Adaptive federated learning in resource constrained
edge computing systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.3.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in
Communications</em> 37, 6
(2019), 1205–1221.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiaofei Wang, Yiwen Han,
Victor CM Leung, Dusit Niyato,
Xueqiang Yan, and Xu Chen.
2020.

</span>
<span class="ltx_bibblock">Convergence of edge computing and deep learning: A
comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>
22, 2 (2020),
869–904.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> ([n.d.])</span>
<span class="ltx_bibblock">
Zhibo Wang, Mengkai Song,
Zhifei Zhang, Yang Song,
Qian Wang, and Hairong Qi.
[n.d.].

</span>
<span class="ltx_bibblock">Beyond inferring class representatives: User-level
privacy leakage from federated learning. In <em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">IEEE
INFOCOM 2019-IEEE Conference on Computer Communications</em>.
IEEE, 2512–2520.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen
et al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Wei Wen, Cong Xu,
Feng Yan, Chunpeng Wu,
Yandan Wang, Yiran Chen, and
Hai Li. 2017.

</span>
<span class="ltx_bibblock">Terngrad: Ternary gradients to reduce communication
in distributed deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.07878</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Cong Xie, Oluwasanmi
Koyejo, and Indranil Gupta.
2018.

</span>
<span class="ltx_bibblock">Generalized byzantine-tolerant sgd.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.10116</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> ([n.d.]a)</span>
<span class="ltx_bibblock">
Cong Xie, Sanmi Koyejo,
and Indranil Gupta. [n.d.]a.

</span>
<span class="ltx_bibblock">Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In <em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">International
Conference on Machine Learning</em>. PMLR,
6893–6901.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
et al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Cong Xie, Sanmi Koyejo,
and Indranil Gupta. 2019.

</span>
<span class="ltx_bibblock">Zeno++: Robust Fully Asynchronous SGD.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.07020</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie
et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> ([n.d.]b)</span>
<span class="ltx_bibblock">
Xiaofei Xie, Lei Ma,
Haijun Wang, Yuekang Li,
Yang Liu, and Xiaohong Li.
[n.d.]b.

</span>
<span class="ltx_bibblock">DiffChaser: Detecting Disagreements for Deep Neural
Networks. In <em id="bib.bib85.3.1" class="ltx_emph ltx_font_italic">IJCAI</em>. 5772–5778.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Bajwa (2019a)</span>
<span class="ltx_bibblock">
Zhixiong Yang and
Waheed U Bajwa. 2019a.

</span>
<span class="ltx_bibblock">BRIDGE: Byzantine-resilient decentralized gradient
descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.08098</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Bajwa (2019b)</span>
<span class="ltx_bibblock">
Zhixiong Yang and
Waheed U Bajwa. 2019b.

</span>
<span class="ltx_bibblock">ByRDiE: Byzantine-resilient distributed coordinate
descent for decentralized learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal and Information
Processing over Networks</em> 5, 4
(2019), 611–627.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
et al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhixiong Yang, Arpita
Gang, and Waheed U Bajwa.
2019.

</span>
<span class="ltx_bibblock">Adversary-resilient inference and machine learning:
From distributed to decentralized.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">stat</em> 1050
(2019), 23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
et al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhixiong Yang, Arpita
Gang, and Waheed U Bajwa.
2020.

</span>
<span class="ltx_bibblock">Adversary-resilient distributed and decentralized
statistical inference and machine learning: An overview of recent advances
under the Byzantine threat model.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.3.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>
37, 3 (2020),
146–159.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao and Sun ([n.d.])</span>
<span class="ltx_bibblock">
Xin Yao and Lifeng
Sun. [n.d.].

</span>
<span class="ltx_bibblock">Continual Local Training For Better Initialization
Of Federated Models. In <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International
Conference on Image Processing (ICIP)</em>. IEEE,
1736–1740.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin
et al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Dong Yin, Yudong Chen,
Kannan Ramchandran, and Peter
Bartlett. 2018.

</span>
<span class="ltx_bibblock">Byzantine-robust distributed learning: Towards
optimal statistical rates.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.01498</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Yang (2017)</span>
<span class="ltx_bibblock">
Yu Zhang and Qiang
Yang. 2017.

</span>
<span class="ltx_bibblock">A survey on multi-task learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.08114</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Lingchen Zhao, Shengshan
Hu, Qian Wang, Jianlin Jiang,
Shen Chao, Xiangyang Luo, and
Pengfei Hu. 2020.

</span>
<span class="ltx_bibblock">Shielding Collaborative Learning: Mitigating
Poisoning Attacks through Client-Side Detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Dependable and Secure
Computing</em> (2020).

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li,
Liangzhen Lai, Naveen Suda,
Damon Civin, and Vikas Chandra.
2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
et al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sicong Zhou, Huawei
Huang, Wuhui Chen, Pan Zhou,
Zibin Zheng, and Song Guo.
2020.

</span>
<span class="ltx_bibblock">Pirate: A blockchain-based secure framework of
distributed machine learning in 5g networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.3.1" class="ltx_emph ltx_font_italic">IEEE Network</em> (2020).

</span>
<span class="ltx_bibblock">

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.11005" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.11006" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.11006">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.11006" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.11007" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 03:25:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
