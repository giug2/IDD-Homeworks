<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.16575] Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking</title><meta property="og:description" content="The dominant probing approaches rely on the zero-shot performance of image-text matching tasks to gain a finer-grained understanding of the representations learned by recent multimodal image-language transformer models…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.16575">

<!--Generated on Tue Feb 27 07:30:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ivana Beňová<sup id="id10.2.id1" class="ltx_sup"><span id="id10.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jana Košecká<sup id="id11.2.id1" class="ltx_sup"><span id="id11.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michal Gregor<sup id="id12.2.id1" class="ltx_sup"><span id="id12.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup> 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id4.1.1" class="ltx_text ltx_font_bold">Martin Tamajka<sup id="id4.1.1.1" class="ltx_sup"><span id="id4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.1" class="ltx_text ltx_font_bold">Marcel Veselý<sup id="id5.1.1.1" class="ltx_sup"><span id="id5.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id6.1.1" class="ltx_text ltx_font_bold">Marián Šimko<sup id="id6.1.1.1" class="ltx_sup"><span id="id6.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span> 
<br class="ltx_break"><sup id="id13.5.id1" class="ltx_sup"><span id="id13.5.id1.1" class="ltx_text ltx_font_italic">1</span></sup> Faculty of Information Technology, Brno University of Technology, Brno, Czech republic 
<br class="ltx_break"><sup id="id14.6.id2" class="ltx_sup"><span id="id14.6.id2.1" class="ltx_text ltx_font_italic">2</span></sup> Kempelen Institute of Intelligent Technologies, Bratislava, Slovakia 
<br class="ltx_break"><sup id="id15.7.id3" class="ltx_sup"><span id="id15.7.id3.1" class="ltx_text ltx_font_italic">3</span></sup> George Mason University, USA 
<br class="ltx_break"><span id="id16.8.id4" class="ltx_text ltx_font_typewriter"> {ivana.benova, michal.gregor, martin.tamajka, marcel.vesely, marian.simko}@kinit.sk</span> 
<br class="ltx_break"><span id="id17.9.id5" class="ltx_text ltx_font_typewriter">kosecka@gmu.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id18.id1" class="ltx_p">The dominant probing approaches rely on the zero-shot performance of image-text matching tasks to gain a finer-grained understanding of the representations learned by recent multimodal image-language transformer models. The evaluation is carried out on carefully curated datasets focusing on counting, relations, attributes, and others. This work introduces an alternative probing strategy called <span id="id18.id1.1" class="ltx_text ltx_font_italic">guided masking</span>. The proposed approach ablates different modalities using masking and assesses the model’s ability to predict the masked word with high accuracy. We focus on studying multimodal models that consider regions of interest (ROI) features obtained by object detectors as input tokens. We probe the understanding of verbs using guided masking on ViLBERT, LXMERT, UNITER, and VisualBERT and show that these models can predict the correct verb with high accuracy. This contrasts with previous conclusions drawn from image-text matching probing techniques that frequently fail in situations requiring verb understanding. The code for all experiments will be publicly available <a target="_blank" href="https://github.com/ivana-13/guided_masking" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ivana-13/guided_masking</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent years have witnessed notable progress in developing and training multimodal transformers that integrate self-attention, cross-attention, and self-supervised learning for fusing vision and language modalities. These strategies were initially introduced in natural language processing <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite>, specifically in BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>, and were later extended to include visual modality. Diverse multimodal vision-language transformers, such as LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>, ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>, ALBEF <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>, OmniVL <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>, CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite>, BLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, and FLAVA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite>, utilize large datasets for self-supervised training, vary in architecture, pre-training loss functions, and dataset size.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Multimodal image-language transformers undergo pre-training using tasks such as masked language modeling, masked region prediction, and others to capture fine-grained correlations between image and text tokens. For coarse-grained alignment of image and caption, image-text matching and cross-modal contrastive learning are employed, enabling the utilization of vast web data without requiring additional supervision.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2401.16575/assets/images/walking.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="381" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Image from the SVO-Probes dataset <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. It consists of image-caption pairs, where the sentence either correctly describes the image (positive example) or one aspect of the sentence (subject, verb, or object) does not match the image (negative example). These pairs are used to probe models through zero-shot image-text matching. Example of a positive caption: <span id="S1.F1.3.1" class="ltx_text ltx_font_italic">A person walking on a trail.</span> Example of a verb-negative caption: <span id="S1.F1.4.2" class="ltx_text ltx_font_italic">A person runs on the trail.</span> </figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Several probing methods have been introduced to better understand the representations and capabilities of these multimodal transformers. The method that appears most often uses carefully curated datasets designed to test the model’s understanding of different linguistic aspects such as attributes, objects, counting, or word order. Minimal controlled edits of the original caption are used to create the negative image-caption pairs, and these positive and negative pairs are then used to formulate the probing as binary classification. The testing is done by measuring the model’s performance on image-caption matching. Examples of these include research works that probe object understanding <cite class="ltx_cite ltx_citemacro_citep">(Shekhar et al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>, counting <cite class="ltx_cite ltx_citemacro_citep">(Parcalabescu et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>, verb understanding <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>, word order <cite class="ltx_cite ltx_citemacro_citep">(Thrush et al., <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, spatial relations understanding <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> or all-at-once analysis <cite class="ltx_cite ltx_citemacro_citep">(Bugliarello et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>. For example, the probing results in <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> led authors to conclude that understanding verbs is very challenging as the image-text matching task is on the edge of randomness.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Although the image-text matching is a straightforward training objective and is easy to evaluate, it was found to be insufficient for fine-grained understanding by multiple researchers <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Yuksekgonul et al. (<a href="#bib.bib34" title="" class="ltx_ref">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Bi et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Herzig et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>. Probing using image-text matching also has several shortcomings that affect the conclusions drawn by previous works. It uses the fusion of holistic representations (<span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">[IMG]</span> and <span id="S1.p4.1.2" class="ltx_text ltx_font_typewriter">[CLS]</span> tokens) as its multimodal representation, making fine-grained understanding and analysis of the impact of local change challenging. Even minor phrasing alterations in captions can lead to misclassification. For instance, in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a change of a single word in a caption, like <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">“A person runs on a trail”</span> to <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">“A person runs on a pathway”</span>, leads to a change in the representations of both modalities and the classification.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Additionally, during image-text matching pre-training, the negative captions are selected randomly and often have little in common with the image. This makes it easier for the classifier to identify non-matching pairs during pre-training. Finally, creating foiled (minimally edited captions in the specific linguistic aspect) captions in curated datasets is time-consuming and prone to ambiguity, with instances where negative pairs are not genuinely negative, indicating the need for a more precise probing approach.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To address these challenges, we propose using the guided masking probing technique, which involves masking tokens representing specific linguistic aspects of language that we want to probe and quantify the model’s ability to predict the masked token.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The main contributions of our work are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">guided masking</span> as a probing technique to enable more detailed probing and evaluation of pre-trained vision-language models. This method can study the understanding of attributes, objects, or subjects, counting, spatial relations, or verbs.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present a quantitative analysis of verb understanding on a pre-selected group of pre-trained vision-language models. However, we probe ViLBERT, LXMERT, UNITER, and VisualBERT on the carefully curated SVO-Probes dataset <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> and V-COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Gupta and Malik, <a href="#bib.bib11" title="" class="ltx_ref">2015</a>)</cite>. The results obtained using guided masking show that the models predicted the correct verb in more than <math id="S1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S1.I1.i2.p1.1.m1.1a"><mrow id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml"><mn id="S1.I1.i2.p1.1.m1.1.1.2" xref="S1.I1.i2.p1.1.m1.1.1.2.cmml">75</mn><mo id="S1.I1.i2.p1.1.m1.1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><apply id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I1.i2.p1.1.m1.1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S1.I1.i2.p1.1.m1.1.1.2.cmml" xref="S1.I1.i2.p1.1.m1.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">75\%</annotation></semantics></math> of the captions, suggesting a significantly higher understanding than those obtained using image-text matching.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We perform sensitivity analysis of the probing technique and the pre-trained models using ablation of visual tokens to study the impact of vision on the output. We focus our study on multimodal models that consider regions of interest (ROI) of the image obtained with object detectors as inputs to enable guided masking of the vision modality. While these models have some biases from the pre-trained object detectors, they offer more control for experiments in this case. For all models, we perform ablations of visual tokens, and the results indicate that the visual inputs have an impact on the prediction of words.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Existing probing strategies for multimodal transformers depend on the methodology, linguistic phenomenon, and type of model being evaluated.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Vision-Language Transformers.</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Multimodal vision-language transformer models vary in architecture, pre-training objectives, and datasets used for training. They also differ in the way they fuse the vision and language modalities. This work focuses on architectures where language and vision models are trained jointly, with the visual input tokenized into region of interest (ROI) features obtained using pre-trained object detectors. Examples of such models include ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>, LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>, VisualBERT <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>, and UNITER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">To pretrain these models, datasets such as Conceptual Captions (CC) <cite class="ltx_cite ltx_citemacro_citep">(Sharma et al., <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite> with <math id="S2.SS0.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S2.SS0.SSS0.Px1.p2.1.m1.1a"><mo id="S2.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p2.1.m1.1b"><approx id="S2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p2.1.m1.1c">\approx</annotation></semantics></math> 3.3 million image-caption pairs are used. The training objectives include masked language modeling (MLM), masked region modeling (MRM), and image-text matching (ITM).</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p">The pre-training of fusion encoders with image-text matching is performed on holistic image-text pair representation. In LXMERT <cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>, UNITER <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite> and VisualBERT <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>,
it is a final hidden state of the <span id="S2.SS0.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_typewriter">[CLS]</span> token, in ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>, it is obtained by using element-wise multiplication on the holistic visual token <span id="S2.SS0.SSS0.Px1.p3.1.2" class="ltx_text ltx_font_typewriter">[IMG]</span> and language <span id="S2.SS0.SSS0.Px1.p3.1.3" class="ltx_text ltx_font_typewriter">[CLS]</span>. More recently additional pre-training objectives were introduced to improve fine-grain understanding of the VLMs. For example, in <cite class="ltx_cite ltx_citemacro_citep">(Yuksekgonul et al., <a href="#bib.bib34" title="" class="ltx_ref">2022</a>)</cite> authors introduced composition-aware hard negative mining to improve the strategy of finding the negative captions for pre-training image-text matching to improve compositional and order understanding. In <cite class="ltx_cite ltx_citemacro_citep">(Bi et al., <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>, to further enhance vision-language matching, the authors introduced new pre-training tasks, namely vision-language replaced token detection and fine-grained image-text matching.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p4.1" class="ltx_p">The dual encoders or the combination of fusion and dual encoder, e.g. by CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite>, FLAVA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite>, and BLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, use patch-based representations and are pre-trained on larger datasets using contrastive learning and language generation as part of the pre-training. While they often achieve better performance on some downstream tasks, they make the more controlled probing of fine-grained representations more challenging. To study the understanding of these models, explainability methods such as gScoreCAM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite> are used.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Probing of Understanding.</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">The first in-depth general analysis of vision-language transformers was presented in <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>. The authors introduced the Value (Vision-And-Language Understanding Evaluation) framework, which consists of a set of probing tasks focused on the explanation of individual layers, heads, or fusion techniques. While this analysis led to essential conclusions about vision-language models, the authors did not perform fine-grained probing of different linguistic aspects.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Another class of probing methods focuses on particular linguistic aspects. These methods rely on specially curated datasets with foiled captions and use image-text matching evaluation in a zero-shot setting. A foil caption was created for every image for verb understanding <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. In this caption, only the part representing the studied linguistic aspect was changed; in this case, the verb (see Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for an example). Other studied aspects include probing of counting <cite class="ltx_cite ltx_citemacro_citep">(Parcalabescu et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>, spatial relationships <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, word order <cite class="ltx_cite ltx_citemacro_citep">(Thrush et al., <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, color, size, position, and adversarial captions <cite class="ltx_cite ltx_citemacro_citep">(Salin et al., <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Entailment.</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Visual entailment is a task that aims to predict whether an image semantically entails a given text. This task is similar to image-text matching but with three classes: entailment, contradiction, and neutral. SeeTrue benchmark has been introduced for this task <cite class="ltx_cite ltx_citemacro_citep">(Yarom et al., <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Image-Text Retrieval.</h5>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">The exploration of word order, attribution, and relations has been investigated by <cite class="ltx_cite ltx_citemacro_citep">(Yuksekgonul et al., <a href="#bib.bib34" title="" class="ltx_ref">2022</a>)</cite>, employing an image retrieval approach and holistic representations of text and image. Utilizing models that adopt patch-based visual input tokenization, including CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite>, BLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, and FLAVA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite>, the authors showcased the efficacy of data augmentations and fine-tuning within these models.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Ablation Study.</h5>

<div id="S2.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px5.p1.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_citep">(Frank et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>, the authors carried out more general probing without focusing on token prediction or particular linguistic aspects using cross-modal input ablation to quantify to what extent vision-language models use cross-modal information. The authors used Flickr30k Entities, an extension of the Flickr30k general captions dataset with additional annotations of bounding boxes corresponding to entity mentions in the captions. The authors studied how models predict masked language tokens (class of masked bounding box), given ablated inputs in the other modality. Instead of focusing on performance on downstream tasks (e.g., image-text matching or retrieval), they quantified the models using the value of cross-entropy loss for masked language modeling and Kullback-Leibler divergence loss for masked region modeling and its effect (increase or decrease) in the presence of ablation. The results revealed an asymmetry in pre-trained vision-language models. The prediction of masked phrases was strongly affected by ablated visual inputs. At the same time, text ablation had almost no effect on the prediction of masked image regions. While this study revealed the role of the visual modality, it did not provide more detailed insight related to the grounding of different parts of the language (nouns, adjectives, verbs, spatial relationships).</p>
</div>
<div id="S2.SS0.SSS0.Px5.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px5.p2.1" class="ltx_p">Our work is closest to <cite class="ltx_cite ltx_citemacro_citep">(Frank et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>, but instead of masking general phrases, we focus on a careful analysis of verbs, as in <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>, and study the model’s ability to predict the correct words in the masked caption with or without ablated visual input.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Probing with Masking</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Guided Masking</h5>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.9" class="ltx_p">To better understand the effect of local caption changes on cross-modal representations, we propose the following probing technique:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\vec{P}=f_{lang}(I,C^{\prime})," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mover accent="true" id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml">P</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.1" xref="S3.E1.m1.2.2.1.1.3.1.cmml">→</mo></mover><mo id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2.cmml">f</mi><mrow id="S3.E1.m1.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.1.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.3.3.1" xref="S3.E1.m1.2.2.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.1.3.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.3.3.1a" xref="S3.E1.m1.2.2.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.1.3.3.4" xref="S3.E1.m1.2.2.1.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.3.3.1b" xref="S3.E1.m1.2.2.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.1.3.3.5" xref="S3.E1.m1.2.2.1.1.1.3.3.5.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">I</mi><mo id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">,</mo><msup id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">C</mi><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.4" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"></eq><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><ci id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1">→</ci><ci id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2">𝑃</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">𝑓</ci><apply id="S3.E1.m1.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3"><times id="S3.E1.m1.2.2.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.1"></times><ci id="S3.E1.m1.2.2.1.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.2">𝑙</ci><ci id="S3.E1.m1.2.2.1.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.3">𝑎</ci><ci id="S3.E1.m1.2.2.1.1.1.3.3.4.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.4">𝑛</ci><ci id="S3.E1.m1.2.2.1.1.1.3.3.5.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3.5">𝑔</ci></apply></apply><interval closure="open" id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐼</ci><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">𝐶</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3">′</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\vec{P}=f_{lang}(I,C^{\prime}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS0.SSS0.Px1.p1.8" class="ltx_p">where <math id="S3.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">I</annotation></semantics></math> is input image, <math id="S3.SS0.SSS0.Px1.p1.2.m2.4" class="ltx_Math" alttext="C=\{w_{1},w_{2},\dots w_{i},\dots w_{n}\}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.4a"><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.4.4" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.cmml">C</mi><mo id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.5" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.5.cmml">=</mo><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.5" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">{</mo><msub id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2.cmml">w</mi><mn id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.6" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.2.cmml">w</mi><mn id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.7" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">,</mo><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.cmml"><mi mathvariant="normal" id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.1.cmml">​</mo><msub id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.2.cmml">w</mi><mi id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.3.cmml">i</mi></msub></mrow><mo id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.8" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">,</mo><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.cmml"><mi mathvariant="normal" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.1.cmml">​</mo><msub id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.2.cmml">w</mi><mi id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.3" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.3.cmml">n</mi></msub></mrow><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.9" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.4b"><apply id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4"><eq id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.5.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.5"></eq><ci id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.6">𝐶</ci><set id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.5.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4"><apply id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.2">𝑤</ci><cn type="integer" id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.2">𝑤</ci><cn type="integer" id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.2.2.2.3">2</cn></apply><apply id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3"><times id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.2">…</ci><apply id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.2">𝑤</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.3.3.3.3.3.3.3">𝑖</ci></apply></apply><apply id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4"><times id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.1"></times><ci id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.2">…</ci><apply id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.2">𝑤</ci><ci id="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.4.4.4.4.4.3.3">𝑛</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.4c">C=\{w_{1},w_{2},\dots w_{i},\dots w_{n}\}</annotation></semantics></math> is a matching caption containing <math id="S3.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.3.m3.1c">n</annotation></semantics></math> words <math id="S3.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.4.m4.1a"><mi id="S3.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.4.m4.1b"><ci id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.4.m4.1c">w</annotation></semantics></math>, <math id="S3.SS0.SSS0.Px1.p1.5.m5.4" class="ltx_Math" alttext="C^{\prime}=\{w_{1},w_{2},\dots[MASK]_{i},\dots w_{n}\}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.5.m5.4a"><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.4.4" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.cmml"><msup id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.cmml"><mi id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.2.cmml">C</mi><mo id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.3.cmml">′</mo></msup><mo id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.5" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.5.cmml">=</mo><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.5.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.5" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.5.cmml">{</mo><msub id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2.cmml">w</mi><mn id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.6" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.5.cmml">,</mo><msub id="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.cmml"><mi id="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.2.cmml">w</mi><mn id="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.7" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.5.cmml">,</mo><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.cmml"><mi mathvariant="normal" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.3.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.2.cmml">​</mo><msub id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.cmml"><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.2.1.cmml">[</mo><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.1a" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.4" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.1b" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.5" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.5.cmml">K</mi></mrow><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.2.1.cmml">]</mo></mrow><mi id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.3.cmml">i</mi></msub></mrow><mo id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.8" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.5.cmml">,</mo><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.cmml"><mi mathvariant="normal" id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.1.cmml">​</mo><msub id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.2.cmml">w</mi><mi id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.3" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.3.cmml">n</mi></msub></mrow><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.9" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.5.m5.4b"><apply id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4"><eq id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.5.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.5"></eq><apply id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6">superscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.2">𝐶</ci><ci id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.6.3">′</ci></apply><set id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.5.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4"><apply id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2">𝑤</ci><cn type="integer" id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.2">𝑤</ci><cn type="integer" id="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.3">2</cn></apply><apply id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3"><times id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.2"></times><ci id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.3">…</ci><apply id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1">subscript</csymbol><apply id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1"><csymbol cd="latexml" id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1"><times id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.1"></times><ci id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.2">𝑀</ci><ci id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.3">𝐴</ci><ci id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.4.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.4">𝑆</ci><ci id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.5.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.1.1.1.5">𝐾</ci></apply></apply><ci id="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.3.3.3.3.3.1.3">𝑖</ci></apply></apply><apply id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4"><times id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.1"></times><ci id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.2">…</ci><apply id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.2">𝑤</ci><ci id="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.4.4.4.4.4.3.3">𝑛</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.5.m5.4c">C^{\prime}=\{w_{1},w_{2},\dots[MASK]_{i},\dots w_{n}\}</annotation></semantics></math> is created by masking a word <math id="S3.SS0.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.6.m6.1a"><msub id="S3.SS0.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml">w</mi><mi id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.2">𝑤</ci><ci id="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.6.m6.1c">w_{i}</annotation></semantics></math> we want to probe, <math id="S3.SS0.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="f_{lang}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.7.m7.1a"><msub id="S3.SS0.SSS0.Px1.p1.7.m7.1.1" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.2" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.2.cmml">f</mi><mrow id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.2" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.1" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.3" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.1a" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.4" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.1b" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.5" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.5.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.7.m7.1b"><apply id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.2">𝑓</ci><apply id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3"><times id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.1"></times><ci id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.2">𝑙</ci><ci id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.3">𝑎</ci><ci id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.4.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.4">𝑛</ci><ci id="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.5.cmml" xref="S3.SS0.SSS0.Px1.p1.7.m7.1.1.3.5">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.7.m7.1c">f_{lang}</annotation></semantics></math> is representing the pre-trained language head of the multimodal model which predicts masked word(s), and <math id="S3.SS0.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="\vec{P}" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.8.m8.1a"><mover accent="true" id="S3.SS0.SSS0.Px1.p1.8.m8.1.1" xref="S3.SS0.SSS0.Px1.p1.8.m8.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.8.m8.1.1.2" xref="S3.SS0.SSS0.Px1.p1.8.m8.1.1.2.cmml">P</mi><mo stretchy="false" id="S3.SS0.SSS0.Px1.p1.8.m8.1.1.1" xref="S3.SS0.SSS0.Px1.p1.8.m8.1.1.1.cmml">→</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.8.m8.1b"><apply id="S3.SS0.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.8.m8.1.1"><ci id="S3.SS0.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.8.m8.1.1.1">→</ci><ci id="S3.SS0.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.8.m8.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.8.m8.1c">\vec{P}</annotation></semantics></math> is probability distribution across all the tokens in the vocabulary.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p">This technique presents a compelling advantage as it obviates the need to generate a new dataset replete with foiled captions. Instead of relying solely on a binary match versus non-match score characteristic of image-text matching, our approach delves into the exploration of the model’s most probable token predictions, thereby offering a richer understanding of potential alternatives considered by the model. Additionally, token-level masking allows a nuanced examination of local connections between vision and language tokens. The discernible variance in performance, both with and without incorporating the visual modality, elucidates the model’s proficiency in predicting the correct verbs. This nuanced evaluation underscores the method’s efficacy in providing a more comprehensive and insightful analysis.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Guided Masking Evaluation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As the prediction of only the most probable word in the caption could lead to many false negatives, we suggest following a robust evaluation approach. By doing that, predictions of words in varying grammatical forms and even synonyms, to some extent, are considered.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Lemmatization:</span> The challenge was addressing different grammatical tenses in the caption. For instance, if the positive caption is <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">“Girl sitting in grass.”</span>, the model might predict <span id="S3.I1.i1.p1.1.3" class="ltx_text ltx_font_italic">“Girl sits in grass.”</span>. We lemmatize the original and predicted words using Lemmatizer in <span id="S3.I1.i1.p1.1.4" class="ltx_text ltx_font_italic">nltk</span> library to handle this.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.4" class="ltx_p"><span id="S3.I1.i2.p1.4.1" class="ltx_text ltx_font_bold">Synonyms:</span> Images often allow multiple accurate verbs. If the caption <span id="S3.I1.i2.p1.4.2" class="ltx_text ltx_font_italic">“Woman jogging in the forest."</span> has the most likely prediction <span id="S3.I1.i2.p1.4.3" class="ltx_text ltx_font_italic">“running”</span>, the model still understands the visual context. We compare the top <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">n</annotation></semantics></math> lemmatized predictions to the lemmatized masked token to handle this. We use <math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="n=5" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mrow id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml"><mi id="S3.I1.i2.p1.2.m2.1.1.2" xref="S3.I1.i2.p1.2.m2.1.1.2.cmml">n</mi><mo id="S3.I1.i2.p1.2.m2.1.1.1" xref="S3.I1.i2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.I1.i2.p1.2.m2.1.1.3" xref="S3.I1.i2.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><apply id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1"><eq id="S3.I1.i2.p1.2.m2.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1.1"></eq><ci id="S3.I1.i2.p1.2.m2.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.1.1.2">𝑛</ci><cn type="integer" id="S3.I1.i2.p1.2.m2.1.1.3.cmml" xref="S3.I1.i2.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">n=5</annotation></semantics></math> in our experiments. This parameter was selected as the first five words are usually predictions of synonyms or have interchangeable meanings. Moreover, the average probability of the fourth word is <math id="S3.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="S3.I1.i2.p1.3.m3.1a"><mrow id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><mn id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml">5</mn><mo id="S3.I1.i2.p1.3.m3.1.1.1" xref="S3.I1.i2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">5\%</annotation></semantics></math>, and the average probability of the fifth word is <math id="S3.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S3.I1.i2.p1.4.m4.1a"><mrow id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml"><mn id="S3.I1.i2.p1.4.m4.1.1.2" xref="S3.I1.i2.p1.4.m4.1.1.2.cmml">3</mn><mo id="S3.I1.i2.p1.4.m4.1.1.1" xref="S3.I1.i2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.1b"><apply id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.I1.i2.p1.4.m4.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S3.I1.i2.p1.4.m4.1.1.2.cmml" xref="S3.I1.i2.p1.4.m4.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.1c">3\%</annotation></semantics></math>, which we still consider high. More predictions are unnecessary due to low probabilities.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Semantic Variation:</span> Predicting a fitting but different word for an image, like changing <span id="S3.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">“laying”</span> to <span id="S3.I1.i3.p1.1.3" class="ltx_text ltx_font_italic">“resting”</span> poses a challenge. Although comparing the five most likely lemmatized predictions to the lemmatized masked token can solve semantic variation in some cases, none of the top 5 predictions may match the original caption’s word, even though the top predictions could still be applicable.</p>
</div>
</li>
</ul>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Evaluation of Cross-Modal Grounding</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">We evaluated cross-modal grounding by testing the model’s understanding through vision ablation to explore verb and visual input connections. It has been observed in <cite class="ltx_cite ltx_citemacro_citep">(Aflalo et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite> that alignment between language tokens and visual tokens results in high transformer attention. We assessed this relationship through ablation. If the performance diminishes upon removing visual inputs, it suggests that the model has acquired knowledge of alignments between phrases and objects.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">We employed vision ablation to evaluate verb grounding in visual inputs of subjects engaged in activities. For ViLBERT, LXMERT, UNITER, and VisualBERT, images are processed with Faster R-CNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite>, using ROI features of detected regions and position embeddings as input. In visual ablation, we determine the caption’s subject through Trankit’s part-of-speech tagging and dependency parsing <cite class="ltx_cite ltx_citemacro_citep">(Van Nguyen et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>.
We find the subject in the image using the WordNet graph (approximating semantic similarity). The caption’s subject is compared with all the object labels predicted for each visual token by Faster R-CNN. The label closest to the caption’s subject is considered the subject, and the bounding box assigned to this object is considered the image’s subject. The features of this object are then masked to zero, together with all features whose bounding boxes intercede with the image’s subject. The model’s ability to predict the masked word should drop when such vital visual information is ablated.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this work, we used datasets designed for studying verbs, which are described in more detail below. All experiments were carried out using implementations of the ViLBERT, LXMERT, UNITER, and VisualBERT models<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The number of parameters of these models is around 240 million.</span></span></span> provided by VOLTA (Visiolinguistic Transformer Architectures) <cite class="ltx_cite ltx_citemacro_citep">(Bugliarello et al., <a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>, a PyTorch implementation of a unified mathematical framework of currently proposed V<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><and id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\&amp;</annotation></semantics></math>L BERTs. We used guided masking to probe the understanding of verbs on the SVO-Probes dataset. We compared our findings and conclusions with those of a published paper <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SVO-Probes Dataset.</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">This dataset was created to study the understanding of subjects, verbs, and objects. The dataset contains simple captions created only from triplets of words <math id="S4.SS0.SSS0.Px1.p1.1.m1.3" class="ltx_Math" alttext="\langle subject,verb,object\rangle" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.1.m1.3a"><mrow id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.4.cmml"><mo stretchy="false" id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.4" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.4.cmml">⟨</mo><mrow id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1a" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.4" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1b" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.5" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.5.cmml">j</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1c" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.6" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1d" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.7" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1e" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.8" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.8.cmml">t</mi></mrow><mo id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.5" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.4.cmml">,</mo><mrow id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.cmml"><mi id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1a" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.4" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1b" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.5" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.5.cmml">b</mi></mrow><mo id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.6" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.4.cmml">,</mo><mrow id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.cmml"><mi id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.2" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.3" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1a" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.4" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.4.cmml">j</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1b" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.5" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1c" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.6" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1d" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.7" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.7.cmml">t</mi></mrow><mo stretchy="false" id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.7" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.4.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.m1.3b"><list id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.4.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3"><apply id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1"><times id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1"></times><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2">𝑠</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3">𝑢</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.4.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.4">𝑏</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.5.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.5">𝑗</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.6.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.6">𝑒</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.7.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.7">𝑐</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.8.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.8">𝑡</ci></apply><apply id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2"><times id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1"></times><ci id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2">𝑣</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3">𝑒</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.4.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.4">𝑟</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.5.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.5">𝑏</ci></apply><apply id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3"><times id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.1"></times><ci id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.2.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.2">𝑜</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.3.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.3">𝑏</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.4.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.4">𝑗</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.5.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.5">𝑒</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.6.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.6">𝑐</ci><ci id="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.7.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.3.3.3.3.7">𝑡</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.m1.3c">\langle subject,verb,object\rangle</annotation></semantics></math>, where the verb should be visually recognizable. Two images were connected to each caption. One of the images created a positive image-caption pair, and the other created a subject-negative, verb-negative, or object-negative pair based on which aspect of the caption was foiled. An example of a verb-negative pair is shown in Figure <a href="#S4.F2" title="Figure 2 ‣ SVO-Probes Dataset. ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.16575/assets/images/SVOphoto.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="275" height="183" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.16575/assets/images/SVOphoto3.png" id="S4.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="275" height="183" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Images from the SVO-Probes dataset <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. Caption: <span id="S4.F2.2.1" class="ltx_text ltx_font_italic">"A girl sitting on grass."</span>. The image on the left creates a positive pair, while the image on the right creates a negative pair.</figcaption>
</figure>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">This dataset was created with the help of Amazon Mechanical Turk annotators. The SVO triplets were collected from the Conceptual Captions (CC) dataset, while images were downloaded from the web to prevent overlap with the CC dataset.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Image-text matching baseline.</h5>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">In the first row of Table <a href="#S4.T1" title="Table 1 ‣ Image-text matching baseline. ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can see the results reported in <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> for probing verb understanding with a base multimodal transformer (MMT) that closely replicates the ViLBERT architecture. The results represent the accuracy of the average prediction of positive and negative pairs. The authors concluded that the model fails more in situations requiring verb understanding than other speech parts. The difference in accuracy we measured (reported bellow MMT in Table <a href="#S4.T1" title="Table 1 ‣ Image-text matching baseline. ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) could have been influenced by various factors, including the distinct implementation of ViLBERT and the slightly lower number of samples we were able to obtain (some links had become broken by the time we downloaded the data). Since the average accuracy for image-text matching is around <math id="S4.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="46\%" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">46</mn><mo id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2">46</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">46\%</annotation></semantics></math>, below random guessing, the conclusion about understanding verbs was negative.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">Additionally, we computed the results for image-text matching while performing vision ablation on activity subjects and by masking the entire image. The outcomes in the third and fourth parts of the table reveal that ablating the vision causes the models to predict the negative label more frequently. Strikingly, this leads to an improvement in average accuracy due to the imbalanced dataset. This suggests that this needs to be improved in the image-text matching evaluation method.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Average</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Positive</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Negative</td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">#Examples</th>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">34k</td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">11k</td>
<td id="S4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">23k</td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MMT</th>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_center">60.8%</td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_center">93.8%</td>
<td id="S4.T1.1.3.3.4" class="ltx_td ltx_align_center">27.8%</td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">#Examples</th>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">33893</td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">11571</td>
<td id="S4.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">22322</td>
</tr>
<tr id="S4.T1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViLBERT</th>
<td id="S4.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">46.3%</td>
<td id="S4.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">95.4%</td>
<td id="S4.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">20.9%</td>
</tr>
<tr id="S4.T1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LXMERT</th>
<td id="S4.T1.1.6.6.2" class="ltx_td ltx_align_center">45.0%</td>
<td id="S4.T1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.6.3.1" class="ltx_text ltx_font_bold">95.5%</span></td>
<td id="S4.T1.1.6.6.4" class="ltx_td ltx_align_center">18.8%</td>
</tr>
<tr id="S4.T1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UNITER</th>
<td id="S4.T1.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.7.7.2.1" class="ltx_text ltx_font_bold">47.2%</span></td>
<td id="S4.T1.1.7.7.3" class="ltx_td ltx_align_center">94.5%</td>
<td id="S4.T1.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.7.7.4.1" class="ltx_text ltx_font_bold">22.7%</span></td>
</tr>
<tr id="S4.T1.1.8.8" class="ltx_tr">
<th id="S4.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VisualBERT</th>
<td id="S4.T1.1.8.8.2" class="ltx_td ltx_align_center">47.1%</td>
<td id="S4.T1.1.8.8.3" class="ltx_td ltx_align_center">94.4%</td>
<td id="S4.T1.1.8.8.4" class="ltx_td ltx_align_center">22.6%</td>
</tr>
<tr id="S4.T1.1.9.9" class="ltx_tr">
<th id="S4.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4">Vision ablation</th>
</tr>
<tr id="S4.T1.1.10.10" class="ltx_tr">
<th id="S4.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViLBERT</th>
<td id="S4.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.10.10.2.1" class="ltx_text ltx_font_bold">56.5%</span></td>
<td id="S4.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">63.4%</td>
<td id="S4.T1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.10.10.4.1" class="ltx_text ltx_font_bold">53.0%</span></td>
</tr>
<tr id="S4.T1.1.11.11" class="ltx_tr">
<th id="S4.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LXMERT</th>
<td id="S4.T1.1.11.11.2" class="ltx_td ltx_align_center">47.5%</td>
<td id="S4.T1.1.11.11.3" class="ltx_td ltx_align_center">86.5%</td>
<td id="S4.T1.1.11.11.4" class="ltx_td ltx_align_center">27.2%</td>
</tr>
<tr id="S4.T1.1.12.12" class="ltx_tr">
<th id="S4.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UNITER</th>
<td id="S4.T1.1.12.12.2" class="ltx_td ltx_align_center">44.2%</td>
<td id="S4.T1.1.12.12.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.12.12.3.1" class="ltx_text ltx_font_bold">95.6%</span></td>
<td id="S4.T1.1.12.12.4" class="ltx_td ltx_align_center">17.5%</td>
</tr>
<tr id="S4.T1.1.13.13" class="ltx_tr">
<th id="S4.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VisualBERT</th>
<td id="S4.T1.1.13.13.2" class="ltx_td ltx_align_center">46.3%</td>
<td id="S4.T1.1.13.13.3" class="ltx_td ltx_align_center">91.6%</td>
<td id="S4.T1.1.13.13.4" class="ltx_td ltx_align_center">22.9%</td>
</tr>
<tr id="S4.T1.1.14.14" class="ltx_tr">
<th id="S4.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4">Masking whole image</th>
</tr>
<tr id="S4.T1.1.15.15" class="ltx_tr">
<th id="S4.T1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViLBERT</th>
<td id="S4.T1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.15.15.2.1" class="ltx_text ltx_font_bold">65.2%</span></td>
<td id="S4.T1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_t">8.3%</td>
<td id="S4.T1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.15.15.4.1" class="ltx_text ltx_font_bold">94.7%</span></td>
</tr>
<tr id="S4.T1.1.16.16" class="ltx_tr">
<th id="S4.T1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LXMERT</th>
<td id="S4.T1.1.16.16.2" class="ltx_td ltx_align_center">55.3%</td>
<td id="S4.T1.1.16.16.3" class="ltx_td ltx_align_center">45.5%</td>
<td id="S4.T1.1.16.16.4" class="ltx_td ltx_align_center">60.4%</td>
</tr>
<tr id="S4.T1.1.17.17" class="ltx_tr">
<th id="S4.T1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UNITER</th>
<td id="S4.T1.1.17.17.2" class="ltx_td ltx_align_center">43.1%</td>
<td id="S4.T1.1.17.17.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.17.17.3.1" class="ltx_text ltx_font_bold">85.2%</span></td>
<td id="S4.T1.1.17.17.4" class="ltx_td ltx_align_center">21.2%</td>
</tr>
<tr id="S4.T1.1.18.18" class="ltx_tr">
<th id="S4.T1.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">VisualBERT</th>
<td id="S4.T1.1.18.18.2" class="ltx_td ltx_align_center ltx_border_b">58.3%</td>
<td id="S4.T1.1.18.18.3" class="ltx_td ltx_align_center ltx_border_b">40.3%</td>
<td id="S4.T1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_b">67.7%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance on probing for verb understanding with image-text matching using SVO-Probes dataset averaged over all (Average), positive (Positive), and negative (Negative) pairs. The results in the first row (MMT) were published in <cite class="ltx_cite ltx_citemacro_citep">(Hendricks and Nematzadeh, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>, and we obtained the results in the rest of the table using VOLTA’s model implementation.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Top 5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.2.1" class="ltx_tr">
<th id="S4.T2.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViLBERT</th>
<td id="S4.T2.3.2.1.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T2.3.3.2" class="ltx_tr">
<th id="S4.T2.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Guided masking</th>
<td id="S4.T2.3.3.2.2" class="ltx_td ltx_align_center ltx_border_t">73.9%</td>
</tr>
<tr id="S4.T2.3.4.3" class="ltx_tr">
<th id="S4.T2.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vision ablation</th>
<td id="S4.T2.3.4.3.2" class="ltx_td ltx_align_center">71.5%</td>
</tr>
<tr id="S4.T2.3.5.4" class="ltx_tr">
<th id="S4.T2.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Masking whole image</th>
<td id="S4.T2.3.5.4.2" class="ltx_td ltx_align_center">62.9%</td>
</tr>
<tr id="S4.T2.3.6.5" class="ltx_tr">
<th id="S4.T2.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERT</th>
<td id="S4.T2.3.6.5.2" class="ltx_td ltx_align_center">36.1%</td>
</tr>
<tr id="S4.T2.3.7.6" class="ltx_tr">
<th id="S4.T2.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LXMERT</th>
<td id="S4.T2.3.7.6.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T2.3.8.7" class="ltx_tr">
<th id="S4.T2.3.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Guided masking</th>
<td id="S4.T2.3.8.7.2" class="ltx_td ltx_align_center ltx_border_t">74.6%</td>
</tr>
<tr id="S4.T2.3.9.8" class="ltx_tr">
<th id="S4.T2.3.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vision ablation</th>
<td id="S4.T2.3.9.8.2" class="ltx_td ltx_align_center">71.6%</td>
</tr>
<tr id="S4.T2.3.10.9" class="ltx_tr">
<th id="S4.T2.3.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Masking whole image</th>
<td id="S4.T2.3.10.9.2" class="ltx_td ltx_align_center">59.6%</td>
</tr>
<tr id="S4.T2.3.11.10" class="ltx_tr">
<th id="S4.T2.3.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERT</th>
<td id="S4.T2.3.11.10.2" class="ltx_td ltx_align_center">36.1%</td>
</tr>
<tr id="S4.T2.3.12.11" class="ltx_tr">
<th id="S4.T2.3.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">UNITER</th>
<td id="S4.T2.3.12.11.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T2.3.13.12" class="ltx_tr">
<th id="S4.T2.3.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Guided masking</th>
<td id="S4.T2.3.13.12.2" class="ltx_td ltx_align_center ltx_border_t">74.4%</td>
</tr>
<tr id="S4.T2.3.14.13" class="ltx_tr">
<th id="S4.T2.3.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vision ablation</th>
<td id="S4.T2.3.14.13.2" class="ltx_td ltx_align_center">72.2%</td>
</tr>
<tr id="S4.T2.3.15.14" class="ltx_tr">
<th id="S4.T2.3.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Masking whole image</th>
<td id="S4.T2.3.15.14.2" class="ltx_td ltx_align_center">62.5%</td>
</tr>
<tr id="S4.T2.3.16.15" class="ltx_tr">
<th id="S4.T2.3.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERT</th>
<td id="S4.T2.3.16.15.2" class="ltx_td ltx_align_center">36.1%</td>
</tr>
<tr id="S4.T2.3.17.16" class="ltx_tr">
<th id="S4.T2.3.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VisualBERT</th>
<td id="S4.T2.3.17.16.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T2.3.18.17" class="ltx_tr">
<th id="S4.T2.3.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Guided masking</th>
<td id="S4.T2.3.18.17.2" class="ltx_td ltx_align_center ltx_border_t">74.3%</td>
</tr>
<tr id="S4.T2.3.19.18" class="ltx_tr">
<th id="S4.T2.3.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vision ablation</th>
<td id="S4.T2.3.19.18.2" class="ltx_td ltx_align_center">72.2%</td>
</tr>
<tr id="S4.T2.3.20.19" class="ltx_tr">
<th id="S4.T2.3.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Masking whole image</th>
<td id="S4.T2.3.20.19.2" class="ltx_td ltx_align_center">59.9%</td>
</tr>
<tr id="S4.T2.3.21.20" class="ltx_tr">
<th id="S4.T2.3.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">BERT</th>
<td id="S4.T2.3.21.20.2" class="ltx_td ltx_align_center ltx_border_b">36.1%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Probing on positive image-caption pairs of the SVO-Probes dataset for verb understanding with guided masking probing technique. BERT - using the guided masking technique with BERT. Of all 11,571 samples, 44 (<math id="S4.T2.2.m1.1" class="ltx_Math" alttext="0.4\%" display="inline"><semantics id="S4.T2.2.m1.1b"><mrow id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml"><mn id="S4.T2.2.m1.1.1.2" xref="S4.T2.2.m1.1.1.2.cmml">0.4</mn><mo id="S4.T2.2.m1.1.1.1" xref="S4.T2.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><apply id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.2.m1.1.1.1.cmml" xref="S4.T2.2.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T2.2.m1.1.1.2.cmml" xref="S4.T2.2.m1.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">0.4\%</annotation></semantics></math>) were not evaluated.</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Guided Masking.</h5>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">The guided masking probing technique results regarding the top 5 accuracy predicting the masked verb are presented in Table <a href="#S4.T2" title="Table 2 ‣ Image-text matching baseline. ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Only positive image-caption pairs from the SVO-Probes dataset were used for this experiment<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>By masking the verb in the verb-negative image-caption pair, we would obtain the same caption as by masking the correct verb in the positive image-caption.</span></span></span>. The accuracy of the masked word in the first five predictions is around <math id="S4.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="74\%" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">74</mn><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2">74</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.m1.1c">74\%</annotation></semantics></math> for all models, suggesting that the understanding of verbs in these models could be better than previously thought.</p>
</div>
<div id="S4.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p2.2" class="ltx_p">We argue that for verbs describing an activity, the attention weights between the visual token representing the subject and the text token representing the verb are higher. Ablation of the visual token interrupts the connection and affects the result. In that case, the performance decreases, mainly when the activity is only connected to one bounding box containing the entity performing the activity. The row “vision ablation” in Table <a href="#S4.T2" title="Table 2 ‣ Image-text matching baseline. ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> refers to the ablation of the visual token associated with the subject—the accuracy of correct prediction after ablation of the subject in the image dropped by around <math id="S4.SS0.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="2.7\%" display="inline"><semantics id="S4.SS0.SSS0.Px3.p2.1.m1.1a"><mrow id="S4.SS0.SSS0.Px3.p2.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p2.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p2.1.m1.1.1.2" xref="S4.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml">2.7</mn><mo id="S4.SS0.SSS0.Px3.p2.1.m1.1.1.1" xref="S4.SS0.SSS0.Px3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p2.1.m1.1b"><apply id="S4.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px3.p2.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p2.1.m1.1.1.2">2.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p2.1.m1.1c">2.7\%</annotation></semantics></math>. The ablation of the whole image leads to a drop in performance by around <math id="S4.SS0.SSS0.Px3.p2.2.m2.1" class="ltx_Math" alttext="13\%" display="inline"><semantics id="S4.SS0.SSS0.Px3.p2.2.m2.1a"><mrow id="S4.SS0.SSS0.Px3.p2.2.m2.1.1" xref="S4.SS0.SSS0.Px3.p2.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p2.2.m2.1.1.2" xref="S4.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml">13</mn><mo id="S4.SS0.SSS0.Px3.p2.2.m2.1.1.1" xref="S4.SS0.SSS0.Px3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p2.2.m2.1b"><apply id="S4.SS0.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px3.p2.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p2.2.m2.1.1.2">13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p2.2.m2.1c">13\%</annotation></semantics></math>. It is essential to state here that the masking of visual tokens assumes that the Faster R-CNN prediction of the subject was correct. This, however, is not always the case, causing errors in masking the tokens representing the subject.</p>
</div>
<div id="S4.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p3.3" class="ltx_p">To better understand to what extent the observed token predictions are due to simply language priors vs the result of multimodal pre-training, we compare the VLMs with BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> only. BERT’s top 5 accuracy was only <math id="S4.SS0.SSS0.Px3.p3.1.m1.1" class="ltx_Math" alttext="36.1\%" display="inline"><semantics id="S4.SS0.SSS0.Px3.p3.1.m1.1a"><mrow id="S4.SS0.SSS0.Px3.p3.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p3.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p3.1.m1.1.1.2" xref="S4.SS0.SSS0.Px3.p3.1.m1.1.1.2.cmml">36.1</mn><mo id="S4.SS0.SSS0.Px3.p3.1.m1.1.1.1" xref="S4.SS0.SSS0.Px3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p3.1.m1.1b"><apply id="S4.SS0.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px3.p3.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS0.SSS0.Px3.p3.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p3.1.m1.1.1.2">36.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p3.1.m1.1c">36.1\%</annotation></semantics></math>. Since the VL models are initialized with BERT, comparing the results of complete image ablation and BERT can also suggest how over-fitting BERT on CC captions boosts performance. This baseline clarifies why adding visual data improves verb prediction by only <math id="S4.SS0.SSS0.Px3.p3.2.m2.1" class="ltx_Math" alttext="11\%" display="inline"><semantics id="S4.SS0.SSS0.Px3.p3.2.m2.1a"><mrow id="S4.SS0.SSS0.Px3.p3.2.m2.1.1" xref="S4.SS0.SSS0.Px3.p3.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p3.2.m2.1.1.2" xref="S4.SS0.SSS0.Px3.p3.2.m2.1.1.2.cmml">11</mn><mo id="S4.SS0.SSS0.Px3.p3.2.m2.1.1.1" xref="S4.SS0.SSS0.Px3.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p3.2.m2.1b"><apply id="S4.SS0.SSS0.Px3.p3.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px3.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px3.p3.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px3.p3.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p3.2.m2.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p3.2.m2.1c">11\%</annotation></semantics></math> or <math id="S4.SS0.SSS0.Px3.p3.3.m3.1" class="ltx_Math" alttext="15\%" display="inline"><semantics id="S4.SS0.SSS0.Px3.p3.3.m3.1a"><mrow id="S4.SS0.SSS0.Px3.p3.3.m3.1.1" xref="S4.SS0.SSS0.Px3.p3.3.m3.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p3.3.m3.1.1.2" xref="S4.SS0.SSS0.Px3.p3.3.m3.1.1.2.cmml">15</mn><mo id="S4.SS0.SSS0.Px3.p3.3.m3.1.1.1" xref="S4.SS0.SSS0.Px3.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p3.3.m3.1b"><apply id="S4.SS0.SSS0.Px3.p3.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px3.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px3.p3.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p3.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px3.p3.3.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p3.3.m3.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p3.3.m3.1c">15\%</annotation></semantics></math>. The language model and its fine-tuned versions in image-language models are adept at predicting verbs in many instances. We also believe that a more comprehensive evaluation beyond the top 5 predictions (considering caption semantic variety) could yield even more substantial improvements with added visual input.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">V-COCO Dataset</h5>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">The V-COCO dataset is a subset of images from MS-COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib17" title="" class="ltx_ref">2014</a>)</cite>. It was created to study coarse activity recognition and the complete visual understanding of the activity, together with the ability to associate objects in the scene with the semantic roles of the action.</p>
</div>
<div id="S4.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p2.1" class="ltx_p">For example, in caption <span id="S4.SS0.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_italic">“Player hitting a ball with a baseball bat”</span>, <span id="S4.SS0.SSS0.Px4.p2.1.2" class="ltx_text ltx_font_italic">“player”</span> is the agent of the action, <span id="S4.SS0.SSS0.Px4.p2.1.3" class="ltx_text ltx_font_italic">“bat”</span> is the instrument, and <span id="S4.SS0.SSS0.Px4.p2.1.4" class="ltx_text ltx_font_italic">“ball”</span> is the object. This leads to the realization that there are different types of activities, depending on the number of visual tokens affecting them. Grounding of activities such as <span id="S4.SS0.SSS0.Px4.p2.1.5" class="ltx_text ltx_font_italic">“sitting”</span>, <span id="S4.SS0.SSS0.Px4.p2.1.6" class="ltx_text ltx_font_italic">“standing”</span>, and <span id="S4.SS0.SSS0.Px4.p2.1.7" class="ltx_text ltx_font_italic">“running”</span> is affected only by a single image token containing the entity. However, if activities such as <span id="S4.SS0.SSS0.Px4.p2.1.8" class="ltx_text ltx_font_italic">“playing guitar”</span>, <span id="S4.SS0.SSS0.Px4.p2.1.9" class="ltx_text ltx_font_italic">“kicking football”</span>, and <span id="S4.SS0.SSS0.Px4.p2.1.10" class="ltx_text ltx_font_italic">“hitting the ball with baseball bat”</span> are grounded, they should be connected with multiple image tokens. Since the full captions were not part of V-COCO, for this experiment, we used the captions from the MS-COCO dataset for training, validation, and testing. Using captions from MS-COCO sometimes led to a change in the verb being probed. An example of a caption in MS-COCO aligned with the activity in V-COCO and a caption not aligned with the activity can be found in Figure <a href="#S4.F3" title="Figure 3 ‣ V-COCO Dataset ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.16575/assets/images/SVOProbessurfboard.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="179" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.16575/assets/images/SVOProbesdonut.png" id="S4.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="204" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example of two images in the MS-COCO dataset. The action assigned to an image in the V-COCO dataset is not guaranteed to be contained in the MS-COCO caption used with our guided masking technique. The image on the left exemplifies where the action and the masked verb are identical. The action names associated with the image in the V-COCO dataset are <span id="S4.F3.13.1" class="ltx_text ltx_font_italic">"hold"</span>, <span id="S4.F3.14.2" class="ltx_text ltx_font_italic">"stand"</span>, <span id="S4.F3.15.3" class="ltx_text ltx_font_italic">"walk"</span>, <span id="S4.F3.16.4" class="ltx_text ltx_font_italic">"look"</span>, and <span id="S4.F3.17.5" class="ltx_text ltx_font_italic">"carry"</span>. The description in MS-COCO is <span id="S4.F3.18.6" class="ltx_text ltx_font_italic">"A man walks with his surfboard on the sand."</span> The masked verb is <span id="S4.F3.19.7" class="ltx_text ltx_font_italic">"walks"</span>. The image on the right exemplifies where the action and the verb differ. The action names associated with the image in the V-COCO dataset are <span id="S4.F3.20.8" class="ltx_text ltx_font_italic">"hold"</span>, <span id="S4.F3.21.9" class="ltx_text ltx_font_italic">"sit"</span>, and <span id="S4.F3.22.10" class="ltx_text ltx_font_italic">"drink"</span>. The description in MS-COCO is <span id="S4.F3.23.11" class="ltx_text ltx_font_italic">"An older person with a child, both eating donuts."</span> The masked verb is <span id="S4.F3.24.12" class="ltx_text ltx_font_italic">"eating"</span>.</figcaption>
</figure>
<div id="S4.SS0.SSS0.Px4.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p3.1" class="ltx_p">The results of the masked language modeling probing technique on the V-COCO dataset can be seen in Table <a href="#S4.T3" title="Table 3 ‣ V-COCO Dataset ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The accuracy of predicting the correct verb in the caption is relatively high for this dataset. However, the ablation of visual tokens containing the activity’s subject or the ablation of the whole image impacts the prediction of verbs. The ablation leads to almost a <math id="S4.SS0.SSS0.Px4.p3.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.SS0.SSS0.Px4.p3.1.m1.1a"><mrow id="S4.SS0.SSS0.Px4.p3.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p3.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px4.p3.1.m1.1.1.2" xref="S4.SS0.SSS0.Px4.p3.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS0.SSS0.Px4.p3.1.m1.1.1.1" xref="S4.SS0.SSS0.Px4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p3.1.m1.1b"><apply id="S4.SS0.SSS0.Px4.p3.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px4.p3.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px4.p3.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p3.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p3.1.m1.1c">10\%</annotation></semantics></math> performance decrease, further supporting the claim of grounding the verb token in image tokens. Compared with SVO-Probes, the accuracy of BERT’s only predictions is higher. People generated captions in MS-COCO, contain context, and their vocabulary is not restricted in the same way as in SVO-Probes. This nature of captions influenced BERT’s results.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Top 5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.2.1" class="ltx_tr">
<th id="S4.T3.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViLBERT</th>
<td id="S4.T3.3.2.1.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T3.3.3.2" class="ltx_tr">
<th id="S4.T3.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Guided masking</th>
<td id="S4.T3.3.3.2.2" class="ltx_td ltx_align_center ltx_border_t">81.1%</td>
</tr>
<tr id="S4.T3.3.4.3" class="ltx_tr">
<th id="S4.T3.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vision ablation</th>
<td id="S4.T3.3.4.3.2" class="ltx_td ltx_align_center">79.8%</td>
</tr>
<tr id="S4.T3.3.5.4" class="ltx_tr">
<th id="S4.T3.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Masking whole image</th>
<td id="S4.T3.3.5.4.2" class="ltx_td ltx_align_center">72.5%</td>
</tr>
<tr id="S4.T3.3.6.5" class="ltx_tr">
<th id="S4.T3.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERT</th>
<td id="S4.T3.3.6.5.2" class="ltx_td ltx_align_center">58.5%</td>
</tr>
<tr id="S4.T3.3.7.6" class="ltx_tr">
<th id="S4.T3.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LXMERT</th>
<td id="S4.T3.3.7.6.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T3.3.8.7" class="ltx_tr">
<th id="S4.T3.3.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Guided masking</th>
<td id="S4.T3.3.8.7.2" class="ltx_td ltx_align_center ltx_border_t">80.5%</td>
</tr>
<tr id="S4.T3.3.9.8" class="ltx_tr">
<th id="S4.T3.3.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vision ablation</th>
<td id="S4.T3.3.9.8.2" class="ltx_td ltx_align_center">79.2%</td>
</tr>
<tr id="S4.T3.3.10.9" class="ltx_tr">
<th id="S4.T3.3.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Masking whole image</th>
<td id="S4.T3.3.10.9.2" class="ltx_td ltx_align_center">73.5%</td>
</tr>
<tr id="S4.T3.3.11.10" class="ltx_tr">
<th id="S4.T3.3.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERT</th>
<td id="S4.T3.3.11.10.2" class="ltx_td ltx_align_center">58.5%</td>
</tr>
<tr id="S4.T3.3.12.11" class="ltx_tr">
<th id="S4.T3.3.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">UNITER</th>
<td id="S4.T3.3.12.11.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T3.3.13.12" class="ltx_tr">
<th id="S4.T3.3.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Guided masking</th>
<td id="S4.T3.3.13.12.2" class="ltx_td ltx_align_center ltx_border_t">81.3%</td>
</tr>
<tr id="S4.T3.3.14.13" class="ltx_tr">
<th id="S4.T3.3.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vision ablation</th>
<td id="S4.T3.3.14.13.2" class="ltx_td ltx_align_center">79.5%</td>
</tr>
<tr id="S4.T3.3.15.14" class="ltx_tr">
<th id="S4.T3.3.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Masking whole image</th>
<td id="S4.T3.3.15.14.2" class="ltx_td ltx_align_center">76.3%</td>
</tr>
<tr id="S4.T3.3.16.15" class="ltx_tr">
<th id="S4.T3.3.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERT</th>
<td id="S4.T3.3.16.15.2" class="ltx_td ltx_align_center">58.5%</td>
</tr>
<tr id="S4.T3.3.17.16" class="ltx_tr">
<th id="S4.T3.3.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VisualBERT</th>
<td id="S4.T3.3.17.16.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T3.3.18.17" class="ltx_tr">
<th id="S4.T3.3.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Guided masking</th>
<td id="S4.T3.3.18.17.2" class="ltx_td ltx_align_center ltx_border_t">80.2%</td>
</tr>
<tr id="S4.T3.3.19.18" class="ltx_tr">
<th id="S4.T3.3.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vision ablation</th>
<td id="S4.T3.3.19.18.2" class="ltx_td ltx_align_center">78.5%</td>
</tr>
<tr id="S4.T3.3.20.19" class="ltx_tr">
<th id="S4.T3.3.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Masking whole image</th>
<td id="S4.T3.3.20.19.2" class="ltx_td ltx_align_center">74.8%</td>
</tr>
<tr id="S4.T3.3.21.20" class="ltx_tr">
<th id="S4.T3.3.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">BERT</th>
<td id="S4.T3.3.21.20.2" class="ltx_td ltx_align_center ltx_border_b">58.5%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Probing the V-COCO dataset with captions from MS-COCO for verb understanding with guided masking probing technique. BERT - using the guided masking technique with BERT. Out of all 10345 samples, three (<math id="S4.T3.2.m1.1" class="ltx_Math" alttext="0.03\%" display="inline"><semantics id="S4.T3.2.m1.1b"><mrow id="S4.T3.2.m1.1.1" xref="S4.T3.2.m1.1.1.cmml"><mn id="S4.T3.2.m1.1.1.2" xref="S4.T3.2.m1.1.1.2.cmml">0.03</mn><mo id="S4.T3.2.m1.1.1.1" xref="S4.T3.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><apply id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.2.m1.1.1.1.cmml" xref="S4.T3.2.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T3.2.m1.1.1.2.cmml" xref="S4.T3.2.m1.1.1.2">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">0.03\%</annotation></semantics></math>) were not evaluated.</figcaption>
</figure>
</section>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Image-Text Matching and Explainability</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To better understand the difference between the image-text matching probing method and guided masking, we demonstrated the limitations of the image-text matching methodology using the relevancy-based explainability tool from <cite class="ltx_cite ltx_citemacro_cite">Chefer et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>. This method uses the model’s attention layers to produce relevancy maps for the interactions between the input modalities in the network. Due to the tool’s limitations, we only examined explanations for the LXMERT architecture, and the created relevancy maps contain only relevancy that is positive w.r.t. the prediction.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Positive Example</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">The SVO-Probes image in Figure <a href="#S4.F4" title="Figure 4 ‣ Positive Example ‣ 4.1 Image-Text Matching and Explainability ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (left) is associated with the negative verb caption <span id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">"A woman lies on a beach."</span>. The LXMERT model correctly classifies this pair as not matching with a probability of <math id="S4.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="97.83\%" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">97.83</mn><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">97.83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">97.83\%</annotation></semantics></math>.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.16575/assets/images/jumping.jpg" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="180" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.16575/assets/images/jumping_explanation_image.png" id="S4.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="174" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An SVO-Probes image studied when paired with the negative verb caption <span id="S4.F4.2.1" class="ltx_text ltx_font_italic">"A woman lies on a beach."</span> The image is shown without and with the visualization of the relevancy map on the left and the right respectively. The woman’s region of interest (ROI) is the most relevant for the model’s prediction that the pair is not a match.</figcaption>
</figure>
<div id="S4.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p2.1" class="ltx_p">The relevancy map for the input text in Figure <a href="#S4.F5" title="Figure 5 ‣ Positive Example ‣ 4.1 Image-Text Matching and Explainability ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that the most relevant part of the caption for the prediction that the pair is not a match is the verb <span id="S4.SS1.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">"lies"</span>. This explanation is understandable for humans because it reveals that the most relevant token is exactly the word that is foiled. The interpretation could be that the model found the word in the caption, compared it with the image, and used it as the most relevant for the correct prediction because this word is the reason the pair does not match. The relevancy map of the image in Figure <a href="#S4.F4" title="Figure 4 ‣ Positive Example ‣ 4.1 Image-Text Matching and Explainability ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (right) also suggests that the ROI of the woman in the image is the most relevant to the classification.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2401.16575/assets/images/jumping_explanation.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The visualization of the relevancy map on the input caption <span id="S4.F5.2.1" class="ltx_text ltx_font_italic">"A woman lies on a beach."</span> with Figure <a href="#S4.F4" title="Figure 4 ‣ Positive Example ‣ 4.1 Image-Text Matching and Explainability ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for LXMERT’s image-text matching correct prediction that this caption and image do not match.</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Negative Example</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">The SVO-Probes image in Figure <a href="#S4.F6" title="Figure 6 ‣ Negative Example ‣ 4.1 Image-Text Matching and Explainability ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (left) is associated with the negative verb caption <span id="S4.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">"The person ran on the trail."</span>. LXMERT correctly classifies this pair as not matching with a probability of <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="98.72\%" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">98.72</mn><mo id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2">98.72</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">98.72\%</annotation></semantics></math>.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.16575/assets/images/walking.jpg" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="181" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2401.16575/assets/images/walking_explanation_image.png" id="S4.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="185" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An SVO-Probes image studied when paired with the negative verb caption <span id="S4.F6.2.1" class="ltx_text ltx_font_italic">"The person ran on the trail."</span> The image is shown without and with the visualization of the relevancy map on the left and the right respectively. The region of interest (ROI) of the man is the most relevant to the classification.</figcaption>
</figure>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p2.1" class="ltx_p">However, after checking the relevancy map in Figure <a href="#S4.F7" title="Figure 7 ‣ Negative Example ‣ 4.1 Image-Text Matching and Explainability ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for the input text, it can be seen that the most relevant parts of the caption are the words <span id="S4.SS1.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">"person"</span> and <span id="S4.SS1.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_italic">"trail"</span>. Looking at the relevancy of visual tokens in Figure <a href="#S4.F6" title="Figure 6 ‣ Negative Example ‣ 4.1 Image-Text Matching and Explainability ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (right), the model clearly focuses primarily on the person and partially on the trail. This means the model focuses more on the specific words <span id="S4.SS1.SSS0.Px2.p2.1.3" class="ltx_text ltx_font_italic">"person"</span> and <span id="S4.SS1.SSS0.Px2.p2.1.4" class="ltx_text ltx_font_italic">"trail"</span> in the caption than actually focusing on the mismatch of the foiled verb and the activity in the image, which is the real reason for the non-matching label.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2401.16575/assets/images/walking_explanation.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualization of the relevancy map on the input caption <span id="S4.F7.2.1" class="ltx_text ltx_font_italic">"The person ran on the trail."</span> with Figure <a href="#S4.F6" title="Figure 6 ‣ Negative Example ‣ 4.1 Image-Text Matching and Explainability ‣ 4 Experimental Results ‣ Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for LXMERT’s image-text matching correct prediction that this caption and image do not match.</figcaption>
</figure>
<div id="S4.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p3.1" class="ltx_p">After using guided masking with caption <span id="S4.SS1.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_italic">“A person walking on a trail.”</span>, where the word <span id="S4.SS1.SSS0.Px2.p3.1.2" class="ltx_text ltx_font_italic">“walking”</span> is being masked, LXMERT predicts it as the most probable word with a probability of <math id="S4.SS1.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="66\%" display="inline"><semantics id="S4.SS1.SSS0.Px2.p3.1.m1.1a"><mrow id="S4.SS1.SSS0.Px2.p3.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p3.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p3.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p3.1.m1.1.1.2.cmml">66</mn><mo id="S4.SS1.SSS0.Px2.p3.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p3.1.m1.1b"><apply id="S4.SS1.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p3.1.m1.1.1.2">66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p3.1.m1.1c">66\%</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.SSS0.Px2.p4" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p4.1" class="ltx_p">The key objective of this experiment was to emphasize the imperative of employing the explainability tool to thoroughly analyze all instances involved in image-text matching, ensuring a comprehensive verification of the model’s performance across all samples and predictions. Such analysis ensures the model indeed emphasizes verbs and activities in class prediction. Due to the manual nature of this analysis, it is not feasible for the entire dataset, which consists of over 10,000 images. This exposes the limitations of the image-text matching methodology for probing. In contrast, guided masking offers insights through its five considered predictions.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Ethical Policy</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section examines the prospective benefits and potential hazards associated with this paper. Although the introduced probing technique contributes to the advancement of interpretable deep learning models, it is crucial to acknowledge the limited scope of this study, which is centered solely around English image-caption datasets characterized by North American and Western European biases. It is essential to recognize that the quality of datasets significantly influences the outcomes and the implications that can be generalized for other models adopting the guided masking probing technique. This underlines the need for an ethical and inclusive approach to dataset selection and analysis in future research endeavors.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Computing Infrastructure and Budget</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">All results were calculated on a local Linux server (Ubuntu 20.04.3 LTS) with 4 NVIDIA RTX 3090 GPUs, AMD Ryzen Threadripper 3970X 32-Core CPU, and 128GB DDR4. From the available resources, we used 1 GPU. Replicating all experiments with guided masking, vision ablation, and comparison to BERT on all three datasets would take approximately 8 GPU days. Additionally, roughly 15 GPU days were spent on other experiments or attempts at probing that were removed and are not reported in this paper.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">While multimodal vision and language transformers achieve impressive results on downstream tasks, the complexity of the tasks and model makes it difficult to ascertain the fine-grained understanding enabled by the resulting representations.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">This paper proposes a new method for probing and evaluating different aspects of multimodal transformers called <span id="S7.p2.1.1" class="ltx_text ltx_font_italic">guided masking</span>. Instead of image-text matching, vision, and language modalities are ablated using masking. The model’s performance is evaluated by its ability to predict the masked word with high probability. This technique has notable advantages compared to frequently used image-text matching. It does not require the creation of a dataset with foiled captions, and it is better aligned with the pre-training objectives. The guided ablation of visual tokens further reveals the role of grounding in vision and language models and can be compared on the same footing with language-only transformer models (e.g., BERT).</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Our analysis focuses on multimodal transformers with ROI features obtained with a Faster R-CNN object detector as the input on the vision side. We studied ViLBERT’s, LXMERT’s, UNITER’s, and VisualBERT’s ability to understand verbs. However, the proposed method is agnostic when studying specific aspects of language, such as subjects, objects, attributes, or counting, which can be studied with this method. This method could also be extended to work with multimodal transformers that use ViT patch features to represent images such as ALBEF, VLMo, or X-VLM
using a different method for vision ablation. In conclusion, any model that has masked language modeling as a pre-training objective can be studied with guided masking.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.2" class="ltx_p">The second contribution of this paper is a quantitative analysis of verb understanding on the pre-selected group of four pre-trained vision-language models on the carefully curated SVO-Probes dataset and V-COCO dataset. The guided masking results show that the models predicted the
correct verb in more than <math id="S7.p4.1.m1.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S7.p4.1.m1.1a"><mrow id="S7.p4.1.m1.1.1" xref="S7.p4.1.m1.1.1.cmml"><mn id="S7.p4.1.m1.1.1.2" xref="S7.p4.1.m1.1.1.2.cmml">75</mn><mo id="S7.p4.1.m1.1.1.1" xref="S7.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p4.1.m1.1b"><apply id="S7.p4.1.m1.1.1.cmml" xref="S7.p4.1.m1.1.1"><csymbol cd="latexml" id="S7.p4.1.m1.1.1.1.cmml" xref="S7.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S7.p4.1.m1.1.1.2.cmml" xref="S7.p4.1.m1.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.1.m1.1c">75\%</annotation></semantics></math> (in SVO-Probes) and <math id="S7.p4.2.m2.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S7.p4.2.m2.1a"><mrow id="S7.p4.2.m2.1.1" xref="S7.p4.2.m2.1.1.cmml"><mn id="S7.p4.2.m2.1.1.2" xref="S7.p4.2.m2.1.1.2.cmml">80</mn><mo id="S7.p4.2.m2.1.1.1" xref="S7.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p4.2.m2.1b"><apply id="S7.p4.2.m2.1.1.cmml" xref="S7.p4.2.m2.1.1"><csymbol cd="latexml" id="S7.p4.2.m2.1.1.1.cmml" xref="S7.p4.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S7.p4.2.m2.1.1.2.cmml" xref="S7.p4.2.m2.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.2.m2.1c">80\%</annotation></semantics></math> (in V-COCO) of the captions. This leads us to the conclusion that the verb understanding of these multimodal models is better than previously documented.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was partially supported by <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">DisAI - Improving scientific excellence and creativity in combating disinformation with artificial intelligence and language technologies</span>, a project funded by Horizon Europe under <a target="_blank" href="https://doi.org/10.3030/101079164" title="" class="ltx_ref ltx_href">GA No. 101079164</a>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aflalo et al. (2022)</span>
<span class="ltx_bibblock">
Estelle Aflalo, Meng Du, Shao-Yen Tseng, Yongfei Liu, Chenfei Wu, Nan Duan, and Vasudev Lal. 2022.

</span>
<span class="ltx_bibblock">Vl-interpret: An interactive visualization tool for interpreting vision-language transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 21406–21415.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bi et al. (2023)</span>
<span class="ltx_bibblock">
Junyu Bi, Daixuan Cheng, Ping Yao, Bochen Pang, Yuefeng Zhan, Chuanguang Yang, Yujing Wang, Hao Sun, Weiwei Deng, and Qi Zhang. 2023.

</span>
<span class="ltx_bibblock">Vl-match: Enhancing vision-language pretraining with token-level and instance-level matching.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 2584–2593.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bugliarello et al. (2021)</span>
<span class="ltx_bibblock">
Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, and Desmond Elliott. 2021.

</span>
<span class="ltx_bibblock">Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language berts.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 9:978–994.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bugliarello et al. (2023)</span>
<span class="ltx_bibblock">
Emanuele Bugliarello, Laurent Sartran, Aishwarya Agrawal, Lisa Anne Hendricks, and Aida Nematzadeh. 2023.

</span>
<span class="ltx_bibblock">Measuring progress in fine-grained vision-and-language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07558</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2020)</span>
<span class="ltx_bibblock">
Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. 2020.

</span>
<span class="ltx_bibblock">Behind the scene: Revealing the secrets of pre-trained vision-and-language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 565–580. Springer.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chefer et al. (2021)</span>
<span class="ltx_bibblock">
Hila Chefer, Shir Gur, and Lior Wolf. 2021.

</span>
<span class="ltx_bibblock">Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 397–406.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Peijie Chen, Qi Li, Saad Biaz, Trung Bui, and Anh Nguyen. 2022.

</span>
<span class="ltx_bibblock">gscorecam: What objects is clip looking at?

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Asian Conference on Computer Vision</em>, pages 1959–1975.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019.

</span>
<span class="ltx_bibblock">Uniter: Learning universal image-text representations.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frank et al. (2021)</span>
<span class="ltx_bibblock">
Stella Frank, Emanuele Bugliarello, and Desmond Elliott. 2021.

</span>
<span class="ltx_bibblock">Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.04448</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta and Malik (2015)</span>
<span class="ltx_bibblock">
Saurabh Gupta and Jitendra Malik. 2015.

</span>
<span class="ltx_bibblock">Visual semantic role labeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1505.04474</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendricks and Nematzadeh (2021)</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks and Aida Nematzadeh. 2021.

</span>
<span class="ltx_bibblock">Probing image-language transformers for verb understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09141</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herzig et al. (2023)</span>
<span class="ltx_bibblock">
Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogerio Feris, Trevor Darrell, and Amir Globerson. 2023.

</span>
<span class="ltx_bibblock">Incorporating structured representations into pretrained vision &amp; language models using scene graphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06343</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 12888–12900. PMLR.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021.

</span>
<span class="ltx_bibblock">Align before fuse: Vision and language representation learning with momentum distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 34:9694–9705.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for vision and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.03557</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2014</em>, pages 740–755. Springer.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Fangyu Liu, Guy Emerson, and Nigel Collier. 2022.

</span>
<span class="ltx_bibblock">Visual spatial reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.00363</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parcalabescu et al. (2020)</span>
<span class="ltx_bibblock">
Letitia Parcalabescu, Albert Gatt, Anette Frank, and Iacer Calixto. 2020.

</span>
<span class="ltx_bibblock">Seeing past words: Testing the cross-modal capabilities of pretrained v&amp;l models on counting tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.12352</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 8748–8763. PMLR.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salin et al. (2022)</span>
<span class="ltx_bibblock">
Emmanuelle Salin, Badreddine Farah, Stéphane Ayache, and Benoit Favre. 2022.

</span>
<span class="ltx_bibblock">Are vision-language transformers learning multimodal representations? a probing perspective.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">AAAI 2022</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018.

</span>
<span class="ltx_bibblock">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2556–2565.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shekhar et al. (2017)</span>
<span class="ltx_bibblock">
Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurélie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017.

</span>
<span class="ltx_bibblock">Foil it! find one mismatch between image and language caption.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.01359</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2022)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022.

</span>
<span class="ltx_bibblock">Flava: A foundational language and vision alignment model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 15638–15650.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07490</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thrush et al. (2022)</span>
<span class="ltx_bibblock">
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022.

</span>
<span class="ltx_bibblock">Winoground: Probing vision and language models for visio-linguistic compositionality.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 5238–5248.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Nguyen et al. (2021)</span>
<span class="ltx_bibblock">
Minh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, and Thien Huu Nguyen. 2021.

</span>
<span class="ltx_bibblock">Trankit: A light-weight transformer-based toolkit for multilingual natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.03289</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. 2022.

</span>
<span class="ltx_bibblock">Omnivl: One foundation model for image-language and video-language tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 35:5696–5710.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Ziyan Yang, Kushal Kafle, Franck Dernoncourt, and Vicente Ordonez. 2023.

</span>
<span class="ltx_bibblock">Improving visual grounding by encouraging consistent gradient-based explanations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 19165–19174.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yarom et al. (2023)</span>
<span class="ltx_bibblock">
Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. 2023.

</span>
<span class="ltx_bibblock">What you see is what you read? improving text-image alignment evaluation.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10400</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuksekgonul et al. (2022)</span>
<span class="ltx_bibblock">
Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2022.

</span>
<span class="ltx_bibblock">When and why vision-language models behave like bags-of-words, and what to do about it?

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, pages arXiv–2210.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2021)</span>
<span class="ltx_bibblock">
Yan Zeng, Xinsong Zhang, and Hang Li. 2021.

</span>
<span class="ltx_bibblock">Multi-grained vision language pre-training: Aligning texts with visual concepts.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.08276</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.16574" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.16575" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.16575">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.16575" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.16576" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 07:30:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
