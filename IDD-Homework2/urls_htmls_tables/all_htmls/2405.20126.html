<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.20126] Federated and Transfer Learning for Cancer Detection Based on Image Analysis</title><meta property="og:description" content="This review article discusses the roles of federated learning (FL) and transfer learning (TL) in cancer detection based on image analysis. These two strategies powered by machine learning have drawn a lot of attention …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated and Transfer Learning for Cancer Detection Based on Image Analysis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated and Transfer Learning for Cancer Detection Based on Image Analysis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.20126">

<!--Generated on Wed Jun  5 17:46:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on Received: date / Accepted: date.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated learning Deep transfer learning Image analysis Cancer detection ">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">∎



</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>A. Bechar </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Laboratoire LITAN École supérieure en Sciences et Technologies de l’Informatique et du Numérique, RN 75, 06300, Amizour, Bejaia, Algérie
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Y. Elmir</span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Laboratoire LITAN École supérieure en Sciences et Technologies de l’Informatique et du Numérique, RN 75, 06300, Amizour, Bejaia, Algérie; SGRE-Lab, University Tahri Mohammed of Bechar, 08000, Bechar, Algeria
</span></span></span><span id="id5" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>Y. Himeur
College of Engineering and Information Technology, University of Dubai, Dubai, UAE
</span></span></span><span id="id6" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_note_type">institutetext: </span>R. Medjoudj</span></span></span><span id="id7" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_note_type">institutetext: </span>Laboratoire LITAN École supérieure en Sciences et Technologies de l’Informatique et du Numérique, RN 75, 06300, Amizour, Bejaia, Algérie
</span></span></span><span id="id8" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_note_type">institutetext: </span>A. Amira</span></span></span><span id="id9" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_note_type">institutetext: </span>Department of Computer Science, University of Sharjah, Sharjah, UAE
; Institute of Artificial Intelligence, De Montfort University, Leicester, United Kingdom
<br class="ltx_break"><span id="id9.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_note_type">email: </span>bechar@estin.dz; elmir@estin.dz; yhimeur@ud.ac.ae; medjoudj@estin.dz; aamira@sharjah.ac.ae;</span></span></span> <em id="id9.2" class="ltx_emph ltx_font_italic">Present address:</em> of F. Author </span></span></span>
<h1 class="ltx_title ltx_title_document">Federated and Transfer Learning for Cancer Detection Based on Image Analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amine Bechar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Youssef Elmir
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yassine Himeur
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rafik Medjoudj
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abbes Amira
</span></span>
</div>
<div class="ltx_dates">(Received: date / Accepted: date)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This review article discusses the roles of federated learning (FL) and transfer learning (TL) in cancer detection based on image analysis. These two strategies powered by machine learning have drawn a lot of attention due to their potential to increase the precision and effectiveness of cancer diagnosis in light of the growing importance of machine learning techniques in cancer detection. FL enables the training of machine learning models on data distributed across multiple sites without the need for centralized data sharing, while TL allows for the transfer of knowledge from one task to another. A comprehensive assessment of the two methods, including their strengths, and weaknesses is presented. Moving on, their applications in cancer detection are discussed, including potential directions for the future. Finally, this article offers a thorough description of the functions of TL and FL in image-based cancer detection. The authors also make insightful suggestions for additional study in this rapidly developing area.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Federated learning Deep transfer learning Image analysis Cancer detection 
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Preliminary</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Cancer detection (CD) remains a pivotal topic in the contemporary health sector, largely due to cancer being the world’s second most lethal disease following cardiac conditions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">anukriti2019investigation</span> </a></cite>. The predominant cause of this affliction is unrestrained cell growth, which can generate malicious tumors and adversely affect surrounding cells <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">shrivastava2020bone</span> </a></cite>. Prompt CD not only significantly increases survival rates but also minimizes reliance on the visual examination of medical imagery by healthcare professionals, thereby reducing human error <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">hamza2023hybrid</span> </a></cite>. A vital component of CD involved discerning between benign and malignant tumors, as they each require distinct treatment strategies <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">tahmooresi2018early</span> </a></cite>. In light of the considerable strides made in digital image processing and artificial intelligence, numerous tools and frameworks have been developed to improve disease classification, detection, and even prediction, including cancer <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">bechar2023harnessing</span> </a></cite>. Machine learning (ML), in particular, has emerged as a highly promising method for CD, owing to its ability to analyze complex patterns and make accurate predictions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">pradhan2020medical</span> </a></cite>.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Domains within ML, notably Deep learning (DL), have emerged as potent forces within the healthcare sector <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">farrelly2023current</span> </a></cite>. The remarkable evolution of DL has facilitated the analysis of vast amounts of medical data, such as medical imaging, making the work of radiologists more efficient and effective <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wu2017small</span> </a></cite>. DL aids in extracting crucial insights, patterns, and anomalies, thereby enhancing the ability to detect and predict potential future diseases. However, ML and DL come with a set of limitations. The foremost among these is the challenge of determining which model best suits a specific use case within the healthcare system, particularly in terms of delivering optimal results. Additionally, the complexity of model training often necessitates considerable computational resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">iman2023review</span> </a></cite>. Another significant constraint pertains to the dependency on a large set of labeled training data. Access to patient medical data, such as medical imaging, is often restricted due to privacy concerns, leading to issues related to the size and comprehensiveness of the dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wittkopp2021decentralized</span> </a></cite>. This has implications for the accuracy and diversity of the models developed.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Addressing the inherent limitations of ML applications. Techniques such as Federated Learning (FL) and Transfer Learning (TL) have been introduced <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">himeur2023video</span> </a>; <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kheddar2023deep</span> </a></cite>. FL is a decentralized ML approach that allows multiple parties to collaboratively train a model without the necessity of sharing sensitive medical data, ensuring data security and privacy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">bousbiat2023crossing</span> </a></cite>. On the other hand, TL, a subset of ML, facilitates knowledge transfer from multiple related datasets, thereby reducing an ML model’s dependency on labeled training sets <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">razavi2022introduction</span> </a></cite>. Nevertheless, training a model with limited datasets remains a challenging task. TL also aids in conserving time and resources during the model training phase. Therefore, a combination of FL and TL enables comprehensive learning from medical data while preserving user privacy.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Brief overview of CD using image analysis</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">According to research by the World Health Organization (WHO) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ferlay2021cancer</span> </a></cite>, cancer is a major cause of death worldwide, with predictions indicating that cancer rates in people are set to double in the near future. Early detection and treatment of cancer can significantly reduce the risk of mortality.
CD using image analysis involves the use of medical imaging techniques such as X-rays, computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound to generate images of internal body structures. These images are then analyzed using various image analysis techniques to detect cancerous cells or tumors. Image analysis algorithms can be used to identify changes in the shape, size, and texture of cells and tissues that may be indicative of cancer. For example, computer aided detection (CAD) software can be used to analyze mammograms and detect early signs of breast cancer. Other techniques such as ML and DL can also be used to analyze medical images and identify patterns or anomalies that are indicative of cancer <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jeyaraj2019computer</span> </a></cite>. These algorithms can be trained on large datasets of medical images to improve their accuracy and sensitivity in detecting cancer <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jiang2023deep</span> </a></cite>.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Importance of ML techniques in CD</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">ML is the process of training a machine using a large amount of data in order to extract patterns and insights to make decisions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ebert2016machine</span> </a></cite>. In recent years, ML has made significant improvements in the development of healthcare systems, especially in CD.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">Cancer diagnosis and the healing process are assisted using ML, with supervised, unsupervised, and DL techniques (Neural Networks) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">saba2020recent</span> </a></cite>. Much research has been done using ML for various types of CD and diagnosis. For example, in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">xie2021early</span> </a></cite> the authors combined metabolomics and ML methods for early detection of lung cancer. In another study <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">vaka2020breast</span> </a></cite> the authors used a deep neural network to improve the performance and the quality of breast cancer images.</p>
</div>
<div id="S1.SS3.p3" class="ltx_para">
<p id="S1.SS3.p3.1" class="ltx_p">In general, the methodology and steps of using ML algorithms in healthcare systems are the preprocessing, segmentation, feature extraction, training and classification, and performance evaluation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mohammed2020analysis</span> </a></cite>. Recently, with the development of ML, many techniques and concepts have emerged to improve the performance of AI systems, such as FL and TL. These concepts address challenges in ML by improving accuracy and data privacy.</p>
</div>
</section>
<section id="S1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Roles of FL and TL</h3>

<div id="S1.SS4.p1" class="ltx_para">
<p id="S1.SS4.p1.1" class="ltx_p">With the recent and significant advancements of healthcare systems and medical Internet of Things (IoT) devices, medical data has become available and easier to get and extract, this medical data aids in the detection of patient anomalies, and health issues and helps to monitor a patient’s state.</p>
</div>
<div id="S1.SS4.p2" class="ltx_para">
<p id="S1.SS4.p2.1" class="ltx_p">Unfortunately, the availability of this data in centralized ML systems causes processing delays, and more crucially, it raises privacy concerns, which is a significant issue that has to be rectified. Another issue is that even if the data was safeguarded and made unavailable, it still presents a challenge. The issue of dataset limitations is what leads to overfitting problems if there are very limited labeled training datasets (difficult task to annotate medical data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mehmood2021transfer</span> </a></cite>). As a result, the model’s performance and quality of anomaly detection caused it to degrade and become unreliable.</p>
</div>
<div id="S1.SS4.p3" class="ltx_para">
<p id="S1.SS4.p3.1" class="ltx_p">To address these challenges, a multitude of concepts and methodologies have emerged in recent times. This study investigates FL and TL. The importance of TL is its powerful benefit in producing a medical model and using it in another identical model, which allows the problem of limited labeled training datasets and overfitting to be resolved. TL allows also improving the quality, precision, and stability of an anomaly detection model, and it reduces the computing rate and improve considerably the time of training a model in comparison with a model built from the ground up <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zheng2022application</span> </a></cite>.</p>
</div>
<div id="S1.SS4.p4" class="ltx_para">
<p id="S1.SS4.p4.1" class="ltx_p">On the other hand, the role of FL is to allow the training of ML models without the need to share the dataset. This is done using a federated server and federated clients instead of one centralized model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">alam2022federated</span> </a></cite>. The model was trained by each client using its datasets and sending the parameters and metrics back to the server through secured connections, this approach has drastically improved dataset security and integrity.</p>
</div>
<div id="S1.SS4.p5" class="ltx_para">
<p id="S1.SS4.p5.1" class="ltx_p">Although FL decreases the accuracy of the model because of the different distribution of data for every FL client, TL resolves this issue by improving the quality and precision of the model to be ready to use <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">weiss2016survey</span> </a></cite>.</p>
</div>
</section>
<section id="S1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.5 </span>Paper’s Contributions</h3>

<div id="S1.SS5.p1" class="ltx_para">
<p id="S1.SS5.p1.1" class="ltx_p">This review article delves into the utilization of FL and TL in CD through image analysis, highlighting their roles in enhancing ML applications in this domain. It offers a detailed examination of FL and TL, covering their definitions, advantages such as improved privacy, scalability, efficiency, and reduced training times, as well as their challenges like hardware limitations, communication issues, data distribution, and domain shift problems. The paper sets itself apart from existing literature by providing a comparative analysis of FL and TL, discussing their applicability across various types of cancer, and addressing specific considerations in choosing between the two methods. It also explores future directions and potential applications of these technologies in CD, while acknowledging the challenges related to data privacy, computational demands, and the heterogeneity of cancer. The review concludes with final thoughts and recommendations, aiming to guide future research in this rapidly evolving field. The main contributions of this review are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Providing the first comprehensive review of FL and TL for CD based on image analysis.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Introducing a comprehensive taxonomy for FL and TL techniques for CD based on image analysis.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Detailed comparison of FL and TL in terms of models, advantages, limitations, etc.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Review of algorithms and models for applying FL and TL to CD.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">Identifying key challenges and future directions for these methods in CD.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.6 </span>Comparison with other existing reviews</h3>

<div id="S1.SS6.p1" class="ltx_para">
<p id="S1.SS6.p1.1" class="ltx_p">This paper provides a comprehensive review of FL and TL techniques for CD based on medical image analysis. This paper is compared to existing survey articles along several key dimensions in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Definition and concept of FL ‣ 2 Federated Learning (FL) ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The survey provides the most comprehensive coverage across FL, TL, model analysis, applications, and future outlook. Multiple cancer domains are spanned and the two techniques are examined. Other reviews usually only look at FL or TL separately. This review looks at how both methods work together to improve CD from medical images. A complete overview is provided of the roles and combined benefits of FL and TL. This makes this review unique and useful for researchers and healthcare professionals working in this field.</p>
</div>
</section>
<section id="S1.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.7 </span>Organization of the paper</h3>

<div id="S1.SS7.p1" class="ltx_para">
<p id="S1.SS7.p1.1" class="ltx_p">The remaining sections of the paper are structured as follows: Section 2 provides an overview of FL, including its definition, concept, advantages in CD, and challenges. Section 3 defines TL, and it discusses its advantages, limitations, and their applications in CD. Section 4 compares FL and TL based on image analysis, providing a taxonomy and models of each, along with a discussion of their advantages and disadvantages. Section 5 explores some applications and future directions of FL and TL for CD based on image analysis. Section 6 concludes with a summary of the key roles of FL and TL, thoughts on the future outlook for ML in CD, and final recommendations.</p>
</div>
<div id="S1.SS7.p2" class="ltx_para">
<p id="S1.SS7.p2.1" class="ltx_p">The paper aims to provide a comprehensive review of these two important ML techniques and their applications in improving CD based on medical image analysis. The organization begins by introducing each approach, then compares them, considers use cases, and looks toward the future.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Learning (FL)</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Definition and concept of FL</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">FL is a new approach of ML that allows a machine to train a model without sharing its sensitive data. The ultimate goal for each client is to share to a federated server weights, parameters, and even the metrics of the local model to make constant evaluation of the global model. The client-side established communications with the federated server and shared the weights, so it was aggregated to create a new global model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">konevcny2016federated</span> </a></cite>. This process happens and occurs in multiple rounds.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.7" class="ltx_p"><span id="S2.SS1.p2.7.1" class="ltx_text" style="color:#000000;">The fundamental objective in FL is to minimize a global objective function that is typically the weighted sum of local objective functions, formulated in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yu2022survey</span> </a></cite>:</span></p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\min_{\theta}F(\theta)=\sum_{k=1}^{K}p_{k}F_{k}(\theta)" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.3" xref="S2.E1.m1.2.3.cmml"><mrow id="S2.E1.m1.2.3.2" xref="S2.E1.m1.2.3.2.cmml"><mrow id="S2.E1.m1.2.3.2.2" xref="S2.E1.m1.2.3.2.2.cmml"><munder id="S2.E1.m1.2.3.2.2.1" xref="S2.E1.m1.2.3.2.2.1.cmml"><mi mathcolor="#000000" id="S2.E1.m1.2.3.2.2.1.2" xref="S2.E1.m1.2.3.2.2.1.2.cmml">min</mi><mi mathcolor="#000000" id="S2.E1.m1.2.3.2.2.1.3" xref="S2.E1.m1.2.3.2.2.1.3.cmml">θ</mi></munder><mo lspace="0.167em" id="S2.E1.m1.2.3.2.2a" xref="S2.E1.m1.2.3.2.2.cmml">⁡</mo><mi mathcolor="#000000" id="S2.E1.m1.2.3.2.2.2" xref="S2.E1.m1.2.3.2.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.3.2.1" xref="S2.E1.m1.2.3.2.1.cmml">​</mo><mrow id="S2.E1.m1.2.3.2.3.2" xref="S2.E1.m1.2.3.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E1.m1.2.3.2.3.2.1" xref="S2.E1.m1.2.3.2.cmml">(</mo><mi mathcolor="#000000" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E1.m1.2.3.2.3.2.2" xref="S2.E1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" rspace="0.111em" id="S2.E1.m1.2.3.1" xref="S2.E1.m1.2.3.1.cmml">=</mo><mrow id="S2.E1.m1.2.3.3" xref="S2.E1.m1.2.3.3.cmml"><munderover id="S2.E1.m1.2.3.3.1" xref="S2.E1.m1.2.3.3.1.cmml"><mo mathcolor="#000000" movablelimits="false" id="S2.E1.m1.2.3.3.1.2.2" xref="S2.E1.m1.2.3.3.1.2.2.cmml">∑</mo><mrow id="S2.E1.m1.2.3.3.1.2.3" xref="S2.E1.m1.2.3.3.1.2.3.cmml"><mi mathcolor="#000000" id="S2.E1.m1.2.3.3.1.2.3.2" xref="S2.E1.m1.2.3.3.1.2.3.2.cmml">k</mi><mo mathcolor="#000000" id="S2.E1.m1.2.3.3.1.2.3.1" xref="S2.E1.m1.2.3.3.1.2.3.1.cmml">=</mo><mn mathcolor="#000000" id="S2.E1.m1.2.3.3.1.2.3.3" xref="S2.E1.m1.2.3.3.1.2.3.3.cmml">1</mn></mrow><mi mathcolor="#000000" id="S2.E1.m1.2.3.3.1.3" xref="S2.E1.m1.2.3.3.1.3.cmml">K</mi></munderover><mrow id="S2.E1.m1.2.3.3.2" xref="S2.E1.m1.2.3.3.2.cmml"><msub id="S2.E1.m1.2.3.3.2.2" xref="S2.E1.m1.2.3.3.2.2.cmml"><mi mathcolor="#000000" id="S2.E1.m1.2.3.3.2.2.2" xref="S2.E1.m1.2.3.3.2.2.2.cmml">p</mi><mi mathcolor="#000000" id="S2.E1.m1.2.3.3.2.2.3" xref="S2.E1.m1.2.3.3.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.3.3.2.1" xref="S2.E1.m1.2.3.3.2.1.cmml">​</mo><msub id="S2.E1.m1.2.3.3.2.3" xref="S2.E1.m1.2.3.3.2.3.cmml"><mi mathcolor="#000000" id="S2.E1.m1.2.3.3.2.3.2" xref="S2.E1.m1.2.3.3.2.3.2.cmml">F</mi><mi mathcolor="#000000" id="S2.E1.m1.2.3.3.2.3.3" xref="S2.E1.m1.2.3.3.2.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.3.3.2.1a" xref="S2.E1.m1.2.3.3.2.1.cmml">​</mo><mrow id="S2.E1.m1.2.3.3.2.4.2" xref="S2.E1.m1.2.3.3.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E1.m1.2.3.3.2.4.2.1" xref="S2.E1.m1.2.3.3.2.cmml">(</mo><mi mathcolor="#000000" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E1.m1.2.3.3.2.4.2.2" xref="S2.E1.m1.2.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.3.cmml" xref="S2.E1.m1.2.3"><eq id="S2.E1.m1.2.3.1.cmml" xref="S2.E1.m1.2.3.1"></eq><apply id="S2.E1.m1.2.3.2.cmml" xref="S2.E1.m1.2.3.2"><times id="S2.E1.m1.2.3.2.1.cmml" xref="S2.E1.m1.2.3.2.1"></times><apply id="S2.E1.m1.2.3.2.2.cmml" xref="S2.E1.m1.2.3.2.2"><apply id="S2.E1.m1.2.3.2.2.1.cmml" xref="S2.E1.m1.2.3.2.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.3.2.2.1.1.cmml" xref="S2.E1.m1.2.3.2.2.1">subscript</csymbol><min id="S2.E1.m1.2.3.2.2.1.2.cmml" xref="S2.E1.m1.2.3.2.2.1.2"></min><ci id="S2.E1.m1.2.3.2.2.1.3.cmml" xref="S2.E1.m1.2.3.2.2.1.3">𝜃</ci></apply><ci id="S2.E1.m1.2.3.2.2.2.cmml" xref="S2.E1.m1.2.3.2.2.2">𝐹</ci></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝜃</ci></apply><apply id="S2.E1.m1.2.3.3.cmml" xref="S2.E1.m1.2.3.3"><apply id="S2.E1.m1.2.3.3.1.cmml" xref="S2.E1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.3.3.1.1.cmml" xref="S2.E1.m1.2.3.3.1">superscript</csymbol><apply id="S2.E1.m1.2.3.3.1.2.cmml" xref="S2.E1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.3.3.1.2.1.cmml" xref="S2.E1.m1.2.3.3.1">subscript</csymbol><sum id="S2.E1.m1.2.3.3.1.2.2.cmml" xref="S2.E1.m1.2.3.3.1.2.2"></sum><apply id="S2.E1.m1.2.3.3.1.2.3.cmml" xref="S2.E1.m1.2.3.3.1.2.3"><eq id="S2.E1.m1.2.3.3.1.2.3.1.cmml" xref="S2.E1.m1.2.3.3.1.2.3.1"></eq><ci id="S2.E1.m1.2.3.3.1.2.3.2.cmml" xref="S2.E1.m1.2.3.3.1.2.3.2">𝑘</ci><cn type="integer" id="S2.E1.m1.2.3.3.1.2.3.3.cmml" xref="S2.E1.m1.2.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.2.3.3.1.3.cmml" xref="S2.E1.m1.2.3.3.1.3">𝐾</ci></apply><apply id="S2.E1.m1.2.3.3.2.cmml" xref="S2.E1.m1.2.3.3.2"><times id="S2.E1.m1.2.3.3.2.1.cmml" xref="S2.E1.m1.2.3.3.2.1"></times><apply id="S2.E1.m1.2.3.3.2.2.cmml" xref="S2.E1.m1.2.3.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.3.3.2.2.1.cmml" xref="S2.E1.m1.2.3.3.2.2">subscript</csymbol><ci id="S2.E1.m1.2.3.3.2.2.2.cmml" xref="S2.E1.m1.2.3.3.2.2.2">𝑝</ci><ci id="S2.E1.m1.2.3.3.2.2.3.cmml" xref="S2.E1.m1.2.3.3.2.2.3">𝑘</ci></apply><apply id="S2.E1.m1.2.3.3.2.3.cmml" xref="S2.E1.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.3.3.2.3.1.cmml" xref="S2.E1.m1.2.3.3.2.3">subscript</csymbol><ci id="S2.E1.m1.2.3.3.2.3.2.cmml" xref="S2.E1.m1.2.3.3.2.3.2">𝐹</ci><ci id="S2.E1.m1.2.3.3.2.3.3.cmml" xref="S2.E1.m1.2.3.3.2.3.3">𝑘</ci></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\min_{\theta}F(\theta)=\sum_{k=1}^{K}p_{k}F_{k}(\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.6" class="ltx_p"><span id="S2.SS1.p2.6.6" class="ltx_text" style="color:#000000;">where <math id="S2.SS1.p2.1.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p2.1.1.m1.1a"><mi mathcolor="#000000" id="S2.SS1.p2.1.1.m1.1.1" xref="S2.SS1.p2.1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.1.m1.1b"><ci id="S2.SS1.p2.1.1.m1.1.1.cmml" xref="S2.SS1.p2.1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.1.m1.1c">\theta</annotation></semantics></math> represents the global model parameters to be learned, <math id="S2.SS1.p2.2.2.m2.1" class="ltx_Math" alttext="F_{k}(\theta)" display="inline"><semantics id="S2.SS1.p2.2.2.m2.1a"><mrow id="S2.SS1.p2.2.2.m2.1.2" xref="S2.SS1.p2.2.2.m2.1.2.cmml"><msub id="S2.SS1.p2.2.2.m2.1.2.2" xref="S2.SS1.p2.2.2.m2.1.2.2.cmml"><mi mathcolor="#000000" id="S2.SS1.p2.2.2.m2.1.2.2.2" xref="S2.SS1.p2.2.2.m2.1.2.2.2.cmml">F</mi><mi mathcolor="#000000" id="S2.SS1.p2.2.2.m2.1.2.2.3" xref="S2.SS1.p2.2.2.m2.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.2.m2.1.2.1" xref="S2.SS1.p2.2.2.m2.1.2.1.cmml">​</mo><mrow id="S2.SS1.p2.2.2.m2.1.2.3.2" xref="S2.SS1.p2.2.2.m2.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.SS1.p2.2.2.m2.1.2.3.2.1" xref="S2.SS1.p2.2.2.m2.1.2.cmml">(</mo><mi mathcolor="#000000" id="S2.SS1.p2.2.2.m2.1.1" xref="S2.SS1.p2.2.2.m2.1.1.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.SS1.p2.2.2.m2.1.2.3.2.2" xref="S2.SS1.p2.2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.2.m2.1b"><apply id="S2.SS1.p2.2.2.m2.1.2.cmml" xref="S2.SS1.p2.2.2.m2.1.2"><times id="S2.SS1.p2.2.2.m2.1.2.1.cmml" xref="S2.SS1.p2.2.2.m2.1.2.1"></times><apply id="S2.SS1.p2.2.2.m2.1.2.2.cmml" xref="S2.SS1.p2.2.2.m2.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.2.2.m2.1.2.2.1.cmml" xref="S2.SS1.p2.2.2.m2.1.2.2">subscript</csymbol><ci id="S2.SS1.p2.2.2.m2.1.2.2.2.cmml" xref="S2.SS1.p2.2.2.m2.1.2.2.2">𝐹</ci><ci id="S2.SS1.p2.2.2.m2.1.2.2.3.cmml" xref="S2.SS1.p2.2.2.m2.1.2.2.3">𝑘</ci></apply><ci id="S2.SS1.p2.2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.2.m2.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.2.m2.1c">F_{k}(\theta)</annotation></semantics></math> is the local objective function of the <math id="S2.SS1.p2.3.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p2.3.3.m3.1a"><mi mathcolor="#000000" id="S2.SS1.p2.3.3.m3.1.1" xref="S2.SS1.p2.3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.3.m3.1b"><ci id="S2.SS1.p2.3.3.m3.1.1.cmml" xref="S2.SS1.p2.3.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.3.m3.1c">k</annotation></semantics></math>-th participant (out of <math id="S2.SS1.p2.4.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS1.p2.4.4.m4.1a"><mi mathcolor="#000000" id="S2.SS1.p2.4.4.m4.1.1" xref="S2.SS1.p2.4.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.4.m4.1b"><ci id="S2.SS1.p2.4.4.m4.1.1.cmml" xref="S2.SS1.p2.4.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.4.m4.1c">K</annotation></semantics></math> total), and <math id="S2.SS1.p2.5.5.m5.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="S2.SS1.p2.5.5.m5.1a"><msub id="S2.SS1.p2.5.5.m5.1.1" xref="S2.SS1.p2.5.5.m5.1.1.cmml"><mi mathcolor="#000000" id="S2.SS1.p2.5.5.m5.1.1.2" xref="S2.SS1.p2.5.5.m5.1.1.2.cmml">p</mi><mi mathcolor="#000000" id="S2.SS1.p2.5.5.m5.1.1.3" xref="S2.SS1.p2.5.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.5.m5.1b"><apply id="S2.SS1.p2.5.5.m5.1.1.cmml" xref="S2.SS1.p2.5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.5.5.m5.1.1.1.cmml" xref="S2.SS1.p2.5.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p2.5.5.m5.1.1.2.cmml" xref="S2.SS1.p2.5.5.m5.1.1.2">𝑝</ci><ci id="S2.SS1.p2.5.5.m5.1.1.3.cmml" xref="S2.SS1.p2.5.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.5.m5.1c">p_{k}</annotation></semantics></math> is the weight of the <math id="S2.SS1.p2.6.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p2.6.6.m6.1a"><mi mathcolor="#000000" id="S2.SS1.p2.6.6.m6.1.1" xref="S2.SS1.p2.6.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.6.m6.1b"><ci id="S2.SS1.p2.6.6.m6.1.1.cmml" xref="S2.SS1.p2.6.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.6.m6.1c">k</annotation></semantics></math>-th participant’s local dataset, often chosen based on the dataset size or importance.</span></p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.4" class="ltx_p"><span id="S2.SS1.p3.4.1" class="ltx_text" style="color:#000000;">FL often uses gradient-based optimization methods, where the local updates can be computed using stochastic gradient descent (SGD) or its variants. Each participant computes the gradient of the local objective function with respect to the model parameters <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2020accelerating</span> </a></cite>:</span></p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.8" class="ltx_Math" alttext="\nabla F_{k}(\theta)=\frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\nabla f(x_{k,i},y_{k,i};\theta)" display="block"><semantics id="S2.E2.m1.8a"><mrow id="S2.E2.m1.8.8" xref="S2.E2.m1.8.8.cmml"><mrow id="S2.E2.m1.8.8.4" xref="S2.E2.m1.8.8.4.cmml"><mrow id="S2.E2.m1.8.8.4.2" xref="S2.E2.m1.8.8.4.2.cmml"><mo mathcolor="#000000" rspace="0.167em" id="S2.E2.m1.8.8.4.2.1" xref="S2.E2.m1.8.8.4.2.1.cmml">∇</mo><msub id="S2.E2.m1.8.8.4.2.2" xref="S2.E2.m1.8.8.4.2.2.cmml"><mi mathcolor="#000000" id="S2.E2.m1.8.8.4.2.2.2" xref="S2.E2.m1.8.8.4.2.2.2.cmml">F</mi><mi mathcolor="#000000" id="S2.E2.m1.8.8.4.2.2.3" xref="S2.E2.m1.8.8.4.2.2.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.8.8.4.1" xref="S2.E2.m1.8.8.4.1.cmml">​</mo><mrow id="S2.E2.m1.8.8.4.3.2" xref="S2.E2.m1.8.8.4.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E2.m1.8.8.4.3.2.1" xref="S2.E2.m1.8.8.4.cmml">(</mo><mi mathcolor="#000000" id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E2.m1.8.8.4.3.2.2" xref="S2.E2.m1.8.8.4.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" id="S2.E2.m1.8.8.3" xref="S2.E2.m1.8.8.3.cmml">=</mo><mrow id="S2.E2.m1.8.8.2" xref="S2.E2.m1.8.8.2.cmml"><mfrac mathcolor="#000000" id="S2.E2.m1.8.8.2.4" xref="S2.E2.m1.8.8.2.4.cmml"><mn mathcolor="#000000" id="S2.E2.m1.8.8.2.4.2" xref="S2.E2.m1.8.8.2.4.2.cmml">1</mn><msub id="S2.E2.m1.8.8.2.4.3" xref="S2.E2.m1.8.8.2.4.3.cmml"><mi mathcolor="#000000" id="S2.E2.m1.8.8.2.4.3.2" xref="S2.E2.m1.8.8.2.4.3.2.cmml">N</mi><mi mathcolor="#000000" id="S2.E2.m1.8.8.2.4.3.3" xref="S2.E2.m1.8.8.2.4.3.3.cmml">k</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S2.E2.m1.8.8.2.3" xref="S2.E2.m1.8.8.2.3.cmml">​</mo><mrow id="S2.E2.m1.8.8.2.2" xref="S2.E2.m1.8.8.2.2.cmml"><munderover id="S2.E2.m1.8.8.2.2.3" xref="S2.E2.m1.8.8.2.2.3.cmml"><mo mathcolor="#000000" movablelimits="false" id="S2.E2.m1.8.8.2.2.3.2.2" xref="S2.E2.m1.8.8.2.2.3.2.2.cmml">∑</mo><mrow id="S2.E2.m1.8.8.2.2.3.2.3" xref="S2.E2.m1.8.8.2.2.3.2.3.cmml"><mi mathcolor="#000000" id="S2.E2.m1.8.8.2.2.3.2.3.2" xref="S2.E2.m1.8.8.2.2.3.2.3.2.cmml">i</mi><mo mathcolor="#000000" id="S2.E2.m1.8.8.2.2.3.2.3.1" xref="S2.E2.m1.8.8.2.2.3.2.3.1.cmml">=</mo><mn mathcolor="#000000" id="S2.E2.m1.8.8.2.2.3.2.3.3" xref="S2.E2.m1.8.8.2.2.3.2.3.3.cmml">1</mn></mrow><msub id="S2.E2.m1.8.8.2.2.3.3" xref="S2.E2.m1.8.8.2.2.3.3.cmml"><mi mathcolor="#000000" id="S2.E2.m1.8.8.2.2.3.3.2" xref="S2.E2.m1.8.8.2.2.3.3.2.cmml">N</mi><mi mathcolor="#000000" id="S2.E2.m1.8.8.2.2.3.3.3" xref="S2.E2.m1.8.8.2.2.3.3.3.cmml">k</mi></msub></munderover><mrow id="S2.E2.m1.8.8.2.2.2" xref="S2.E2.m1.8.8.2.2.2.cmml"><mrow id="S2.E2.m1.8.8.2.2.2.4" xref="S2.E2.m1.8.8.2.2.2.4.cmml"><mo mathcolor="#000000" rspace="0.167em" id="S2.E2.m1.8.8.2.2.2.4.1" xref="S2.E2.m1.8.8.2.2.2.4.1.cmml">∇</mo><mi mathcolor="#000000" id="S2.E2.m1.8.8.2.2.2.4.2" xref="S2.E2.m1.8.8.2.2.2.4.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.8.8.2.2.2.3" xref="S2.E2.m1.8.8.2.2.2.3.cmml">​</mo><mrow id="S2.E2.m1.8.8.2.2.2.2.2" xref="S2.E2.m1.8.8.2.2.2.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E2.m1.8.8.2.2.2.2.2.3" xref="S2.E2.m1.8.8.2.2.2.2.3.cmml">(</mo><msub id="S2.E2.m1.7.7.1.1.1.1.1.1" xref="S2.E2.m1.7.7.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E2.m1.7.7.1.1.1.1.1.1.2" xref="S2.E2.m1.7.7.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E2.m1.2.2.2.4" xref="S2.E2.m1.2.2.2.3.cmml"><mi mathcolor="#000000" id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml">k</mi><mo mathcolor="#000000" id="S2.E2.m1.2.2.2.4.1" xref="S2.E2.m1.2.2.2.3.cmml">,</mo><mi mathcolor="#000000" id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.2.cmml">i</mi></mrow></msub><mo mathcolor="#000000" id="S2.E2.m1.8.8.2.2.2.2.2.4" xref="S2.E2.m1.8.8.2.2.2.2.3.cmml">,</mo><msub id="S2.E2.m1.8.8.2.2.2.2.2.2" xref="S2.E2.m1.8.8.2.2.2.2.2.2.cmml"><mi mathcolor="#000000" id="S2.E2.m1.8.8.2.2.2.2.2.2.2" xref="S2.E2.m1.8.8.2.2.2.2.2.2.2.cmml">y</mi><mrow id="S2.E2.m1.4.4.2.4" xref="S2.E2.m1.4.4.2.3.cmml"><mi mathcolor="#000000" id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml">k</mi><mo mathcolor="#000000" id="S2.E2.m1.4.4.2.4.1" xref="S2.E2.m1.4.4.2.3.cmml">,</mo><mi mathcolor="#000000" id="S2.E2.m1.4.4.2.2" xref="S2.E2.m1.4.4.2.2.cmml">i</mi></mrow></msub><mo mathcolor="#000000" id="S2.E2.m1.8.8.2.2.2.2.2.5" xref="S2.E2.m1.8.8.2.2.2.2.3.cmml">;</mo><mi mathcolor="#000000" id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E2.m1.8.8.2.2.2.2.2.6" xref="S2.E2.m1.8.8.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.8b"><apply id="S2.E2.m1.8.8.cmml" xref="S2.E2.m1.8.8"><eq id="S2.E2.m1.8.8.3.cmml" xref="S2.E2.m1.8.8.3"></eq><apply id="S2.E2.m1.8.8.4.cmml" xref="S2.E2.m1.8.8.4"><times id="S2.E2.m1.8.8.4.1.cmml" xref="S2.E2.m1.8.8.4.1"></times><apply id="S2.E2.m1.8.8.4.2.cmml" xref="S2.E2.m1.8.8.4.2"><ci id="S2.E2.m1.8.8.4.2.1.cmml" xref="S2.E2.m1.8.8.4.2.1">∇</ci><apply id="S2.E2.m1.8.8.4.2.2.cmml" xref="S2.E2.m1.8.8.4.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.8.8.4.2.2.1.cmml" xref="S2.E2.m1.8.8.4.2.2">subscript</csymbol><ci id="S2.E2.m1.8.8.4.2.2.2.cmml" xref="S2.E2.m1.8.8.4.2.2.2">𝐹</ci><ci id="S2.E2.m1.8.8.4.2.2.3.cmml" xref="S2.E2.m1.8.8.4.2.2.3">𝑘</ci></apply></apply><ci id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5">𝜃</ci></apply><apply id="S2.E2.m1.8.8.2.cmml" xref="S2.E2.m1.8.8.2"><times id="S2.E2.m1.8.8.2.3.cmml" xref="S2.E2.m1.8.8.2.3"></times><apply id="S2.E2.m1.8.8.2.4.cmml" xref="S2.E2.m1.8.8.2.4"><divide id="S2.E2.m1.8.8.2.4.1.cmml" xref="S2.E2.m1.8.8.2.4"></divide><cn type="integer" id="S2.E2.m1.8.8.2.4.2.cmml" xref="S2.E2.m1.8.8.2.4.2">1</cn><apply id="S2.E2.m1.8.8.2.4.3.cmml" xref="S2.E2.m1.8.8.2.4.3"><csymbol cd="ambiguous" id="S2.E2.m1.8.8.2.4.3.1.cmml" xref="S2.E2.m1.8.8.2.4.3">subscript</csymbol><ci id="S2.E2.m1.8.8.2.4.3.2.cmml" xref="S2.E2.m1.8.8.2.4.3.2">𝑁</ci><ci id="S2.E2.m1.8.8.2.4.3.3.cmml" xref="S2.E2.m1.8.8.2.4.3.3">𝑘</ci></apply></apply><apply id="S2.E2.m1.8.8.2.2.cmml" xref="S2.E2.m1.8.8.2.2"><apply id="S2.E2.m1.8.8.2.2.3.cmml" xref="S2.E2.m1.8.8.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.8.8.2.2.3.1.cmml" xref="S2.E2.m1.8.8.2.2.3">superscript</csymbol><apply id="S2.E2.m1.8.8.2.2.3.2.cmml" xref="S2.E2.m1.8.8.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.8.8.2.2.3.2.1.cmml" xref="S2.E2.m1.8.8.2.2.3">subscript</csymbol><sum id="S2.E2.m1.8.8.2.2.3.2.2.cmml" xref="S2.E2.m1.8.8.2.2.3.2.2"></sum><apply id="S2.E2.m1.8.8.2.2.3.2.3.cmml" xref="S2.E2.m1.8.8.2.2.3.2.3"><eq id="S2.E2.m1.8.8.2.2.3.2.3.1.cmml" xref="S2.E2.m1.8.8.2.2.3.2.3.1"></eq><ci id="S2.E2.m1.8.8.2.2.3.2.3.2.cmml" xref="S2.E2.m1.8.8.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S2.E2.m1.8.8.2.2.3.2.3.3.cmml" xref="S2.E2.m1.8.8.2.2.3.2.3.3">1</cn></apply></apply><apply id="S2.E2.m1.8.8.2.2.3.3.cmml" xref="S2.E2.m1.8.8.2.2.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.8.8.2.2.3.3.1.cmml" xref="S2.E2.m1.8.8.2.2.3.3">subscript</csymbol><ci id="S2.E2.m1.8.8.2.2.3.3.2.cmml" xref="S2.E2.m1.8.8.2.2.3.3.2">𝑁</ci><ci id="S2.E2.m1.8.8.2.2.3.3.3.cmml" xref="S2.E2.m1.8.8.2.2.3.3.3">𝑘</ci></apply></apply><apply id="S2.E2.m1.8.8.2.2.2.cmml" xref="S2.E2.m1.8.8.2.2.2"><times id="S2.E2.m1.8.8.2.2.2.3.cmml" xref="S2.E2.m1.8.8.2.2.2.3"></times><apply id="S2.E2.m1.8.8.2.2.2.4.cmml" xref="S2.E2.m1.8.8.2.2.2.4"><ci id="S2.E2.m1.8.8.2.2.2.4.1.cmml" xref="S2.E2.m1.8.8.2.2.2.4.1">∇</ci><ci id="S2.E2.m1.8.8.2.2.2.4.2.cmml" xref="S2.E2.m1.8.8.2.2.2.4.2">𝑓</ci></apply><vector id="S2.E2.m1.8.8.2.2.2.2.3.cmml" xref="S2.E2.m1.8.8.2.2.2.2.2"><apply id="S2.E2.m1.7.7.1.1.1.1.1.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.2">𝑥</ci><list id="S2.E2.m1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.4"><ci id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1">𝑘</ci><ci id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2">𝑖</ci></list></apply><apply id="S2.E2.m1.8.8.2.2.2.2.2.2.cmml" xref="S2.E2.m1.8.8.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.8.8.2.2.2.2.2.2.1.cmml" xref="S2.E2.m1.8.8.2.2.2.2.2.2">subscript</csymbol><ci id="S2.E2.m1.8.8.2.2.2.2.2.2.2.cmml" xref="S2.E2.m1.8.8.2.2.2.2.2.2.2">𝑦</ci><list id="S2.E2.m1.4.4.2.3.cmml" xref="S2.E2.m1.4.4.2.4"><ci id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1.1">𝑘</ci><ci id="S2.E2.m1.4.4.2.2.cmml" xref="S2.E2.m1.4.4.2.2">𝑖</ci></list></apply><ci id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6">𝜃</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.8c">\nabla F_{k}(\theta)=\frac{1}{N_{k}}\sum_{i=1}^{N_{k}}\nabla f(x_{k,i},y_{k,i};\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.3" class="ltx_p"><span id="S2.SS1.p3.3.3" class="ltx_text" style="color:#000000;">where <math id="S2.SS1.p3.1.1.m1.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="S2.SS1.p3.1.1.m1.1a"><msub id="S2.SS1.p3.1.1.m1.1.1" xref="S2.SS1.p3.1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S2.SS1.p3.1.1.m1.1.1.2" xref="S2.SS1.p3.1.1.m1.1.1.2.cmml">N</mi><mi mathcolor="#000000" id="S2.SS1.p3.1.1.m1.1.1.3" xref="S2.SS1.p3.1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.1.m1.1b"><apply id="S2.SS1.p3.1.1.m1.1.1.cmml" xref="S2.SS1.p3.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.1.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p3.1.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.1.m1.1.1.2">𝑁</ci><ci id="S2.SS1.p3.1.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.1.m1.1c">N_{k}</annotation></semantics></math> is the number of samples at the <math id="S2.SS1.p3.2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p3.2.2.m2.1a"><mi mathcolor="#000000" id="S2.SS1.p3.2.2.m2.1.1" xref="S2.SS1.p3.2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.2.m2.1b"><ci id="S2.SS1.p3.2.2.m2.1.1.cmml" xref="S2.SS1.p3.2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.2.m2.1c">k</annotation></semantics></math>-th participant, and <math id="S2.SS1.p3.3.3.m3.6" class="ltx_Math" alttext="x_{k,i},y_{k,i}" display="inline"><semantics id="S2.SS1.p3.3.3.m3.6a"><mrow id="S2.SS1.p3.3.3.m3.6.6.2" xref="S2.SS1.p3.3.3.m3.6.6.3.cmml"><msub id="S2.SS1.p3.3.3.m3.5.5.1.1" xref="S2.SS1.p3.3.3.m3.5.5.1.1.cmml"><mi mathcolor="#000000" id="S2.SS1.p3.3.3.m3.5.5.1.1.2" xref="S2.SS1.p3.3.3.m3.5.5.1.1.2.cmml">x</mi><mrow id="S2.SS1.p3.3.3.m3.2.2.2.4" xref="S2.SS1.p3.3.3.m3.2.2.2.3.cmml"><mi mathcolor="#000000" id="S2.SS1.p3.3.3.m3.1.1.1.1" xref="S2.SS1.p3.3.3.m3.1.1.1.1.cmml">k</mi><mo mathcolor="#000000" id="S2.SS1.p3.3.3.m3.2.2.2.4.1" xref="S2.SS1.p3.3.3.m3.2.2.2.3.cmml">,</mo><mi mathcolor="#000000" id="S2.SS1.p3.3.3.m3.2.2.2.2" xref="S2.SS1.p3.3.3.m3.2.2.2.2.cmml">i</mi></mrow></msub><mo mathcolor="#000000" id="S2.SS1.p3.3.3.m3.6.6.2.3" xref="S2.SS1.p3.3.3.m3.6.6.3.cmml">,</mo><msub id="S2.SS1.p3.3.3.m3.6.6.2.2" xref="S2.SS1.p3.3.3.m3.6.6.2.2.cmml"><mi mathcolor="#000000" id="S2.SS1.p3.3.3.m3.6.6.2.2.2" xref="S2.SS1.p3.3.3.m3.6.6.2.2.2.cmml">y</mi><mrow id="S2.SS1.p3.3.3.m3.4.4.2.4" xref="S2.SS1.p3.3.3.m3.4.4.2.3.cmml"><mi mathcolor="#000000" id="S2.SS1.p3.3.3.m3.3.3.1.1" xref="S2.SS1.p3.3.3.m3.3.3.1.1.cmml">k</mi><mo mathcolor="#000000" id="S2.SS1.p3.3.3.m3.4.4.2.4.1" xref="S2.SS1.p3.3.3.m3.4.4.2.3.cmml">,</mo><mi mathcolor="#000000" id="S2.SS1.p3.3.3.m3.4.4.2.2" xref="S2.SS1.p3.3.3.m3.4.4.2.2.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.3.m3.6b"><list id="S2.SS1.p3.3.3.m3.6.6.3.cmml" xref="S2.SS1.p3.3.3.m3.6.6.2"><apply id="S2.SS1.p3.3.3.m3.5.5.1.1.cmml" xref="S2.SS1.p3.3.3.m3.5.5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.3.3.m3.5.5.1.1.1.cmml" xref="S2.SS1.p3.3.3.m3.5.5.1.1">subscript</csymbol><ci id="S2.SS1.p3.3.3.m3.5.5.1.1.2.cmml" xref="S2.SS1.p3.3.3.m3.5.5.1.1.2">𝑥</ci><list id="S2.SS1.p3.3.3.m3.2.2.2.3.cmml" xref="S2.SS1.p3.3.3.m3.2.2.2.4"><ci id="S2.SS1.p3.3.3.m3.1.1.1.1.cmml" xref="S2.SS1.p3.3.3.m3.1.1.1.1">𝑘</ci><ci id="S2.SS1.p3.3.3.m3.2.2.2.2.cmml" xref="S2.SS1.p3.3.3.m3.2.2.2.2">𝑖</ci></list></apply><apply id="S2.SS1.p3.3.3.m3.6.6.2.2.cmml" xref="S2.SS1.p3.3.3.m3.6.6.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.3.3.m3.6.6.2.2.1.cmml" xref="S2.SS1.p3.3.3.m3.6.6.2.2">subscript</csymbol><ci id="S2.SS1.p3.3.3.m3.6.6.2.2.2.cmml" xref="S2.SS1.p3.3.3.m3.6.6.2.2.2">𝑦</ci><list id="S2.SS1.p3.3.3.m3.4.4.2.3.cmml" xref="S2.SS1.p3.3.3.m3.4.4.2.4"><ci id="S2.SS1.p3.3.3.m3.3.3.1.1.cmml" xref="S2.SS1.p3.3.3.m3.3.3.1.1">𝑘</ci><ci id="S2.SS1.p3.3.3.m3.4.4.2.2.cmml" xref="S2.SS1.p3.3.3.m3.4.4.2.2">𝑖</ci></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.3.m3.6c">x_{k,i},y_{k,i}</annotation></semantics></math> are the local data samples and their corresponding labels.</span></p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.3" class="ltx_p"><span id="S2.SS1.p4.3.1" class="ltx_text" style="color:#000000;">After computing local updates, these are sent to a central server (or aggregated in a decentralized manner) to update the global model. The simplest aggregation method is federated averaging (FedAvg), where the global model is updated as <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">konevcny2016federated</span> </a></cite>:</span></p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_Math" alttext="\theta\leftarrow\theta-\eta\sum_{k=1}^{K}\frac{N_{k}}{N}\nabla F_{k}(\theta)" display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.2" xref="S2.E3.m1.1.2.cmml"><mi mathcolor="#000000" id="S2.E3.m1.1.2.2" xref="S2.E3.m1.1.2.2.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E3.m1.1.2.1" xref="S2.E3.m1.1.2.1.cmml">←</mo><mrow id="S2.E3.m1.1.2.3" xref="S2.E3.m1.1.2.3.cmml"><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.2" xref="S2.E3.m1.1.2.3.2.cmml">θ</mi><mo mathcolor="#000000" id="S2.E3.m1.1.2.3.1" xref="S2.E3.m1.1.2.3.1.cmml">−</mo><mrow id="S2.E3.m1.1.2.3.3" xref="S2.E3.m1.1.2.3.3.cmml"><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.3.2" xref="S2.E3.m1.1.2.3.3.2.cmml">η</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.2.3.3.1" xref="S2.E3.m1.1.2.3.3.1.cmml">​</mo><mrow id="S2.E3.m1.1.2.3.3.3" xref="S2.E3.m1.1.2.3.3.3.cmml"><munderover id="S2.E3.m1.1.2.3.3.3.1" xref="S2.E3.m1.1.2.3.3.3.1.cmml"><mo mathcolor="#000000" movablelimits="false" id="S2.E3.m1.1.2.3.3.3.1.2.2" xref="S2.E3.m1.1.2.3.3.3.1.2.2.cmml">∑</mo><mrow id="S2.E3.m1.1.2.3.3.3.1.2.3" xref="S2.E3.m1.1.2.3.3.3.1.2.3.cmml"><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.1.2.3.2" xref="S2.E3.m1.1.2.3.3.3.1.2.3.2.cmml">k</mi><mo mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.1.2.3.1" xref="S2.E3.m1.1.2.3.3.3.1.2.3.1.cmml">=</mo><mn mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.1.2.3.3" xref="S2.E3.m1.1.2.3.3.3.1.2.3.3.cmml">1</mn></mrow><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.1.3" xref="S2.E3.m1.1.2.3.3.3.1.3.cmml">K</mi></munderover><mrow id="S2.E3.m1.1.2.3.3.3.2" xref="S2.E3.m1.1.2.3.3.3.2.cmml"><mfrac mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.2.2" xref="S2.E3.m1.1.2.3.3.3.2.2.cmml"><msub id="S2.E3.m1.1.2.3.3.3.2.2.2" xref="S2.E3.m1.1.2.3.3.3.2.2.2.cmml"><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.2.2.2.2" xref="S2.E3.m1.1.2.3.3.3.2.2.2.2.cmml">N</mi><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.2.2.2.3" xref="S2.E3.m1.1.2.3.3.3.2.2.2.3.cmml">k</mi></msub><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.2.2.3" xref="S2.E3.m1.1.2.3.3.3.2.2.3.cmml">N</mi></mfrac><mo lspace="0.167em" rspace="0em" id="S2.E3.m1.1.2.3.3.3.2.1" xref="S2.E3.m1.1.2.3.3.3.2.1.cmml">​</mo><mrow id="S2.E3.m1.1.2.3.3.3.2.3" xref="S2.E3.m1.1.2.3.3.3.2.3.cmml"><mo mathcolor="#000000" rspace="0.167em" id="S2.E3.m1.1.2.3.3.3.2.3.1" xref="S2.E3.m1.1.2.3.3.3.2.3.1.cmml">∇</mo><msub id="S2.E3.m1.1.2.3.3.3.2.3.2" xref="S2.E3.m1.1.2.3.3.3.2.3.2.cmml"><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.2.3.2.2" xref="S2.E3.m1.1.2.3.3.3.2.3.2.2.cmml">F</mi><mi mathcolor="#000000" id="S2.E3.m1.1.2.3.3.3.2.3.2.3" xref="S2.E3.m1.1.2.3.3.3.2.3.2.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.2.3.3.3.2.1a" xref="S2.E3.m1.1.2.3.3.3.2.1.cmml">​</mo><mrow id="S2.E3.m1.1.2.3.3.3.2.4.2" xref="S2.E3.m1.1.2.3.3.3.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E3.m1.1.2.3.3.3.2.4.2.1" xref="S2.E3.m1.1.2.3.3.3.2.cmml">(</mo><mi mathcolor="#000000" id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E3.m1.1.2.3.3.3.2.4.2.2" xref="S2.E3.m1.1.2.3.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.2.cmml" xref="S2.E3.m1.1.2"><ci id="S2.E3.m1.1.2.1.cmml" xref="S2.E3.m1.1.2.1">←</ci><ci id="S2.E3.m1.1.2.2.cmml" xref="S2.E3.m1.1.2.2">𝜃</ci><apply id="S2.E3.m1.1.2.3.cmml" xref="S2.E3.m1.1.2.3"><minus id="S2.E3.m1.1.2.3.1.cmml" xref="S2.E3.m1.1.2.3.1"></minus><ci id="S2.E3.m1.1.2.3.2.cmml" xref="S2.E3.m1.1.2.3.2">𝜃</ci><apply id="S2.E3.m1.1.2.3.3.cmml" xref="S2.E3.m1.1.2.3.3"><times id="S2.E3.m1.1.2.3.3.1.cmml" xref="S2.E3.m1.1.2.3.3.1"></times><ci id="S2.E3.m1.1.2.3.3.2.cmml" xref="S2.E3.m1.1.2.3.3.2">𝜂</ci><apply id="S2.E3.m1.1.2.3.3.3.cmml" xref="S2.E3.m1.1.2.3.3.3"><apply id="S2.E3.m1.1.2.3.3.3.1.cmml" xref="S2.E3.m1.1.2.3.3.3.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.2.3.3.3.1.1.cmml" xref="S2.E3.m1.1.2.3.3.3.1">superscript</csymbol><apply id="S2.E3.m1.1.2.3.3.3.1.2.cmml" xref="S2.E3.m1.1.2.3.3.3.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.2.3.3.3.1.2.1.cmml" xref="S2.E3.m1.1.2.3.3.3.1">subscript</csymbol><sum id="S2.E3.m1.1.2.3.3.3.1.2.2.cmml" xref="S2.E3.m1.1.2.3.3.3.1.2.2"></sum><apply id="S2.E3.m1.1.2.3.3.3.1.2.3.cmml" xref="S2.E3.m1.1.2.3.3.3.1.2.3"><eq id="S2.E3.m1.1.2.3.3.3.1.2.3.1.cmml" xref="S2.E3.m1.1.2.3.3.3.1.2.3.1"></eq><ci id="S2.E3.m1.1.2.3.3.3.1.2.3.2.cmml" xref="S2.E3.m1.1.2.3.3.3.1.2.3.2">𝑘</ci><cn type="integer" id="S2.E3.m1.1.2.3.3.3.1.2.3.3.cmml" xref="S2.E3.m1.1.2.3.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E3.m1.1.2.3.3.3.1.3.cmml" xref="S2.E3.m1.1.2.3.3.3.1.3">𝐾</ci></apply><apply id="S2.E3.m1.1.2.3.3.3.2.cmml" xref="S2.E3.m1.1.2.3.3.3.2"><times id="S2.E3.m1.1.2.3.3.3.2.1.cmml" xref="S2.E3.m1.1.2.3.3.3.2.1"></times><apply id="S2.E3.m1.1.2.3.3.3.2.2.cmml" xref="S2.E3.m1.1.2.3.3.3.2.2"><divide id="S2.E3.m1.1.2.3.3.3.2.2.1.cmml" xref="S2.E3.m1.1.2.3.3.3.2.2"></divide><apply id="S2.E3.m1.1.2.3.3.3.2.2.2.cmml" xref="S2.E3.m1.1.2.3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.2.3.3.3.2.2.2.1.cmml" xref="S2.E3.m1.1.2.3.3.3.2.2.2">subscript</csymbol><ci id="S2.E3.m1.1.2.3.3.3.2.2.2.2.cmml" xref="S2.E3.m1.1.2.3.3.3.2.2.2.2">𝑁</ci><ci id="S2.E3.m1.1.2.3.3.3.2.2.2.3.cmml" xref="S2.E3.m1.1.2.3.3.3.2.2.2.3">𝑘</ci></apply><ci id="S2.E3.m1.1.2.3.3.3.2.2.3.cmml" xref="S2.E3.m1.1.2.3.3.3.2.2.3">𝑁</ci></apply><apply id="S2.E3.m1.1.2.3.3.3.2.3.cmml" xref="S2.E3.m1.1.2.3.3.3.2.3"><ci id="S2.E3.m1.1.2.3.3.3.2.3.1.cmml" xref="S2.E3.m1.1.2.3.3.3.2.3.1">∇</ci><apply id="S2.E3.m1.1.2.3.3.3.2.3.2.cmml" xref="S2.E3.m1.1.2.3.3.3.2.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.2.3.3.3.2.3.2.1.cmml" xref="S2.E3.m1.1.2.3.3.3.2.3.2">subscript</csymbol><ci id="S2.E3.m1.1.2.3.3.3.2.3.2.2.cmml" xref="S2.E3.m1.1.2.3.3.3.2.3.2.2">𝐹</ci><ci id="S2.E3.m1.1.2.3.3.3.2.3.2.3.cmml" xref="S2.E3.m1.1.2.3.3.3.2.3.2.3">𝑘</ci></apply></apply><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">𝜃</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\theta\leftarrow\theta-\eta\sum_{k=1}^{K}\frac{N_{k}}{N}\nabla F_{k}(\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p4.2" class="ltx_p"><span id="S2.SS1.p4.2.2" class="ltx_text" style="color:#000000;">with <math id="S2.SS1.p4.1.1.m1.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.SS1.p4.1.1.m1.1a"><mi mathcolor="#000000" id="S2.SS1.p4.1.1.m1.1.1" xref="S2.SS1.p4.1.1.m1.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.1.m1.1b"><ci id="S2.SS1.p4.1.1.m1.1.1.cmml" xref="S2.SS1.p4.1.1.m1.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.1.m1.1c">\eta</annotation></semantics></math> being the learning rate and <math id="S2.SS1.p4.2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p4.2.2.m2.1a"><mi mathcolor="#000000" id="S2.SS1.p4.2.2.m2.1.1" xref="S2.SS1.p4.2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.2.m2.1b"><ci id="S2.SS1.p4.2.2.m2.1.1.cmml" xref="S2.SS1.p4.2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.2.m2.1c">N</annotation></semantics></math> the total number of samples across all participants.</span></p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text" style="color:#000000;">FL can be applied to almost any edge device, this has revolutionized key fields such as medical healthcare applications <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2023heterogeneity</span> </a></cite>. One of the key advantages of FL in medical systems is that it enables securing very sensitive and clinical medical data (CT, X-ray, MRI…) while maintaining the quality and performance of the model, as long as reducing execution time in comparison to centralized models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2022decentralized</span> </a></cite>.</span></p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.6.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.7.2" class="ltx_text" style="font-size:90%;">Contribution comparison of the proposed study against other FL and TL surveys. The tick mark (✓) indicates that the specific field has been addressed, whereas the cross mark (<span id="S2.T1.7.2.1" class="ltx_text">✗</span>) means addressing the specific fields has been missed </span></figcaption>
<table id="S2.T1.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.8.1" class="ltx_tr">
<td id="S2.T1.8.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.1.1" class="ltx_text" style="color:#000000;">Review</span></td>
<td id="S2.T1.8.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.2.1" class="ltx_text" style="color:#000000;">Application</span></td>
<td id="S2.T1.8.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.3.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S2.T1.8.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S2.T1.8.1.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.5.1" class="ltx_text" style="color:#000000;">Research</span></td>
<td id="S2.T1.8.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.6.1" class="ltx_text" style="color:#000000;">Public</span></td>
<td id="S2.T1.8.1.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.7.1" class="ltx_text" style="color:#000000;">Model</span></td>
<td id="S2.T1.8.1.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.8.1" class="ltx_text" style="color:#000000;">Open</span></td>
<td id="S2.T1.8.1.9" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.1.9.1" class="ltx_text" style="color:#000000;">Future</span></td>
</tr>
<tr id="S2.T1.8.2" class="ltx_tr">
<td id="S2.T1.8.2.1" class="ltx_td"></td>
<td id="S2.T1.8.2.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.2.2.1" class="ltx_text" style="color:#000000;">domain</span></td>
<td id="S2.T1.8.2.3" class="ltx_td"></td>
<td id="S2.T1.8.2.4" class="ltx_td"></td>
<td id="S2.T1.8.2.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.2.5.1" class="ltx_text" style="color:#000000;">Questions</span></td>
<td id="S2.T1.8.2.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.2.6.1" class="ltx_text" style="color:#000000;">Dataset</span></td>
<td id="S2.T1.8.2.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.2.7.1" class="ltx_text" style="color:#000000;">Analysis</span></td>
<td id="S2.T1.8.2.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.2.8.1" class="ltx_text" style="color:#000000;">Challenges</span></td>
<td id="S2.T1.8.2.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.2.9.1" class="ltx_text" style="color:#000000;">Direction</span></td>
</tr>
<tr id="S2.T1.8.3" class="ltx_tr">
<td id="S2.T1.8.3.1" class="ltx_td ltx_align_left ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">meghana2023breast</span> </a></cite></td>
<td id="S2.T1.8.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.3.2.1" class="ltx_text" style="color:#000000;">Breast cancer</span></td>
<td id="S2.T1.8.3.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.3.3.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.3.4.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.3.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.3.5.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.3.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.3.6.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.3.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.3.7.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.3.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.3.8.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.3.9" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.8.3.9.1" class="ltx_text" style="color:#000000;">✗</span></td>
</tr>
<tr id="S2.T1.8.4" class="ltx_tr">
<td id="S2.T1.8.4.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rani2023application</span> </a></cite></td>
<td id="S2.T1.8.4.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.4.2.1" class="ltx_text" style="color:#000000;">Lung cancer</span></td>
<td id="S2.T1.8.4.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.4.3.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.4.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.4.4.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.4.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.4.5.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.4.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.4.6.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.4.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.4.7.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.4.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.4.8.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.4.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.4.9.1" class="ltx_text" style="color:#000000;">✗</span></td>
</tr>
<tr id="S2.T1.8.5" class="ltx_tr">
<td id="S2.T1.8.5.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">coelho2023survey</span> </a></cite></td>
<td id="S2.T1.8.5.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.5.2.1" class="ltx_text" style="color:#000000;">Healthcare</span></td>
<td id="S2.T1.8.5.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.5.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.5.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.5.4.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.5.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.5.5.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.5.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.5.6.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.5.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.5.7.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.5.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.5.8.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.5.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.5.9.1" class="ltx_text" style="color:#000000;">✗</span></td>
</tr>
<tr id="S2.T1.8.6" class="ltx_tr">
<td id="S2.T1.8.6.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rahman2023federated</span> </a></cite></td>
<td id="S2.T1.8.6.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.6.2.1" class="ltx_text" style="color:#000000;">Healthcare</span></td>
<td id="S2.T1.8.6.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.6.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.6.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.6.4.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.6.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.6.5.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.6.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.6.6.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.6.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.6.7.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.6.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.6.8.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.6.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.6.9.1" class="ltx_text" style="color:#000000;">✓</span></td>
</tr>
<tr id="S2.T1.8.7" class="ltx_tr">
<td id="S2.T1.8.7.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chowdhury2021review</span> </a></cite></td>
<td id="S2.T1.8.7.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.7.2.1" class="ltx_text" style="color:#000000;">Healthcare</span></td>
<td id="S2.T1.8.7.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.7.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.7.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.7.4.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.7.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.7.5.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.7.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.7.6.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.7.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.7.7.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.7.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.7.8.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.7.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.7.9.1" class="ltx_text" style="color:#000000;">✓</span></td>
</tr>
<tr id="S2.T1.8.8" class="ltx_tr">
<td id="S2.T1.8.8.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ayana2021transfer</span> </a></cite></td>
<td id="S2.T1.8.8.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.2.1" class="ltx_text" style="color:#000000;">Breast cancer</span></td>
<td id="S2.T1.8.8.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.3.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.8.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.4.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.8.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.5.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.8.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.6.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.8.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.7.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.8.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.8.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.8.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.9.1" class="ltx_text" style="color:#000000;">✓</span></td>
</tr>
<tr id="S2.T1.8.9" class="ltx_tr">
<td id="S2.T1.8.9.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rauniyar2023federated</span> </a></cite></td>
<td id="S2.T1.8.9.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.9.2.1" class="ltx_text" style="color:#000000;">Healthcare</span></td>
<td id="S2.T1.8.9.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.9.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.9.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.9.4.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.9.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.9.5.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.9.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.9.6.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.9.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.9.7.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.9.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.9.8.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.9.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.9.9.1" class="ltx_text" style="color:#000000;">✓</span></td>
</tr>
<tr id="S2.T1.8.10" class="ltx_tr">
<td id="S2.T1.8.10.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">hasan2022dermoexpert</span> </a></cite></td>
<td id="S2.T1.8.10.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.10.2.1" class="ltx_text" style="color:#000000;">Skin lesion</span></td>
<td id="S2.T1.8.10.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.10.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.10.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.10.4.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.10.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.10.5.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.10.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.10.6.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.10.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.10.7.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.10.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.10.8.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.10.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.10.9.1" class="ltx_text" style="color:#000000;">✓</span></td>
</tr>
<tr id="S2.T1.8.11" class="ltx_tr">
<td id="S2.T1.8.11.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kumar2021federated</span> </a></cite></td>
<td id="S2.T1.8.11.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.11.2.1" class="ltx_text" style="color:#000000;">Healthcare</span></td>
<td id="S2.T1.8.11.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.11.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.11.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.11.4.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.11.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.11.5.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.11.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.11.6.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.11.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.11.7.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.11.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.11.8.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.11.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.11.9.1" class="ltx_text" style="color:#000000;">✓</span></td>
</tr>
<tr id="S2.T1.8.12" class="ltx_tr">
<td id="S2.T1.8.12.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">joshi2022federated</span> </a></cite></td>
<td id="S2.T1.8.12.2" class="ltx_td ltx_align_left"><span id="S2.T1.8.12.2.1" class="ltx_text" style="color:#000000;">Healthcare</span></td>
<td id="S2.T1.8.12.3" class="ltx_td ltx_align_left"><span id="S2.T1.8.12.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.12.4" class="ltx_td ltx_align_left"><span id="S2.T1.8.12.4.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.12.5" class="ltx_td ltx_align_left"><span id="S2.T1.8.12.5.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.12.6" class="ltx_td ltx_align_left"><span id="S2.T1.8.12.6.1" class="ltx_text" style="color:#000000;">✗</span></td>
<td id="S2.T1.8.12.7" class="ltx_td ltx_align_left"><span id="S2.T1.8.12.7.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.12.8" class="ltx_td ltx_align_left"><span id="S2.T1.8.12.8.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.12.9" class="ltx_td ltx_align_left"><span id="S2.T1.8.12.9.1" class="ltx_text" style="color:#000000;">✓</span></td>
</tr>
<tr id="S2.T1.8.13" class="ltx_tr">
<td id="S2.T1.8.13.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.1.1" class="ltx_text" style="color:#000000;">Ours</span></td>
<td id="S2.T1.8.13.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.2.1" class="ltx_text" style="color:#000000;">Multidisciplinary</span></td>
<td id="S2.T1.8.13.3" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.3.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.13.4" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.4.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.13.5" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.5.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.13.6" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.6.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.13.7" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.7.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.13.8" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.8.1" class="ltx_text" style="color:#000000;">✓</span></td>
<td id="S2.T1.8.13.9" class="ltx_td ltx_align_left ltx_border_b"><span id="S2.T1.8.13.9.1" class="ltx_text" style="color:#000000;">✓</span></td>
</tr>
</table>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>FL Types</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Horizontal FL (HFL)</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p"><span id="S2.SS2.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">In HFL, the objective is to learn a global model </span><math id="S2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><ci id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">\theta</annotation></semantics></math><span id="S2.SS2.SSS1.p1.1.2" class="ltx_text" style="color:#000000;"> by minimizing the sum of local loss functions computed on local datasets that share the same feature space but differ in samples. Mathematically, the objective can be formulated as </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">huang2022fairness</span> </a></cite><span id="S2.SS2.SSS1.p1.1.5" class="ltx_text" style="color:#000000;">:</span></p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.2" class="ltx_Math" alttext="\min_{\theta}F(\theta)=\sum_{k=1}^{K}\frac{N_{k}}{N}F_{k}(\theta)" display="block"><semantics id="S2.E4.m1.2a"><mrow id="S2.E4.m1.2.3" xref="S2.E4.m1.2.3.cmml"><mrow id="S2.E4.m1.2.3.2" xref="S2.E4.m1.2.3.2.cmml"><mrow id="S2.E4.m1.2.3.2.2" xref="S2.E4.m1.2.3.2.2.cmml"><munder id="S2.E4.m1.2.3.2.2.1" xref="S2.E4.m1.2.3.2.2.1.cmml"><mi mathcolor="#000000" id="S2.E4.m1.2.3.2.2.1.2" xref="S2.E4.m1.2.3.2.2.1.2.cmml">min</mi><mi mathcolor="#000000" id="S2.E4.m1.2.3.2.2.1.3" xref="S2.E4.m1.2.3.2.2.1.3.cmml">θ</mi></munder><mo lspace="0.167em" id="S2.E4.m1.2.3.2.2a" xref="S2.E4.m1.2.3.2.2.cmml">⁡</mo><mi mathcolor="#000000" id="S2.E4.m1.2.3.2.2.2" xref="S2.E4.m1.2.3.2.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.3.2.1" xref="S2.E4.m1.2.3.2.1.cmml">​</mo><mrow id="S2.E4.m1.2.3.2.3.2" xref="S2.E4.m1.2.3.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.2.3.2.3.2.1" xref="S2.E4.m1.2.3.2.cmml">(</mo><mi mathcolor="#000000" id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.2.3.2.3.2.2" xref="S2.E4.m1.2.3.2.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" rspace="0.111em" id="S2.E4.m1.2.3.1" xref="S2.E4.m1.2.3.1.cmml">=</mo><mrow id="S2.E4.m1.2.3.3" xref="S2.E4.m1.2.3.3.cmml"><munderover id="S2.E4.m1.2.3.3.1" xref="S2.E4.m1.2.3.3.1.cmml"><mo mathcolor="#000000" movablelimits="false" id="S2.E4.m1.2.3.3.1.2.2" xref="S2.E4.m1.2.3.3.1.2.2.cmml">∑</mo><mrow id="S2.E4.m1.2.3.3.1.2.3" xref="S2.E4.m1.2.3.3.1.2.3.cmml"><mi mathcolor="#000000" id="S2.E4.m1.2.3.3.1.2.3.2" xref="S2.E4.m1.2.3.3.1.2.3.2.cmml">k</mi><mo mathcolor="#000000" id="S2.E4.m1.2.3.3.1.2.3.1" xref="S2.E4.m1.2.3.3.1.2.3.1.cmml">=</mo><mn mathcolor="#000000" id="S2.E4.m1.2.3.3.1.2.3.3" xref="S2.E4.m1.2.3.3.1.2.3.3.cmml">1</mn></mrow><mi mathcolor="#000000" id="S2.E4.m1.2.3.3.1.3" xref="S2.E4.m1.2.3.3.1.3.cmml">K</mi></munderover><mrow id="S2.E4.m1.2.3.3.2" xref="S2.E4.m1.2.3.3.2.cmml"><mfrac mathcolor="#000000" id="S2.E4.m1.2.3.3.2.2" xref="S2.E4.m1.2.3.3.2.2.cmml"><msub id="S2.E4.m1.2.3.3.2.2.2" xref="S2.E4.m1.2.3.3.2.2.2.cmml"><mi mathcolor="#000000" id="S2.E4.m1.2.3.3.2.2.2.2" xref="S2.E4.m1.2.3.3.2.2.2.2.cmml">N</mi><mi mathcolor="#000000" id="S2.E4.m1.2.3.3.2.2.2.3" xref="S2.E4.m1.2.3.3.2.2.2.3.cmml">k</mi></msub><mi mathcolor="#000000" id="S2.E4.m1.2.3.3.2.2.3" xref="S2.E4.m1.2.3.3.2.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.3.3.2.1" xref="S2.E4.m1.2.3.3.2.1.cmml">​</mo><msub id="S2.E4.m1.2.3.3.2.3" xref="S2.E4.m1.2.3.3.2.3.cmml"><mi mathcolor="#000000" id="S2.E4.m1.2.3.3.2.3.2" xref="S2.E4.m1.2.3.3.2.3.2.cmml">F</mi><mi mathcolor="#000000" id="S2.E4.m1.2.3.3.2.3.3" xref="S2.E4.m1.2.3.3.2.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.3.3.2.1a" xref="S2.E4.m1.2.3.3.2.1.cmml">​</mo><mrow id="S2.E4.m1.2.3.3.2.4.2" xref="S2.E4.m1.2.3.3.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.2.3.3.2.4.2.1" xref="S2.E4.m1.2.3.3.2.cmml">(</mo><mi mathcolor="#000000" id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.2.3.3.2.4.2.2" xref="S2.E4.m1.2.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.2b"><apply id="S2.E4.m1.2.3.cmml" xref="S2.E4.m1.2.3"><eq id="S2.E4.m1.2.3.1.cmml" xref="S2.E4.m1.2.3.1"></eq><apply id="S2.E4.m1.2.3.2.cmml" xref="S2.E4.m1.2.3.2"><times id="S2.E4.m1.2.3.2.1.cmml" xref="S2.E4.m1.2.3.2.1"></times><apply id="S2.E4.m1.2.3.2.2.cmml" xref="S2.E4.m1.2.3.2.2"><apply id="S2.E4.m1.2.3.2.2.1.cmml" xref="S2.E4.m1.2.3.2.2.1"><csymbol cd="ambiguous" id="S2.E4.m1.2.3.2.2.1.1.cmml" xref="S2.E4.m1.2.3.2.2.1">subscript</csymbol><min id="S2.E4.m1.2.3.2.2.1.2.cmml" xref="S2.E4.m1.2.3.2.2.1.2"></min><ci id="S2.E4.m1.2.3.2.2.1.3.cmml" xref="S2.E4.m1.2.3.2.2.1.3">𝜃</ci></apply><ci id="S2.E4.m1.2.3.2.2.2.cmml" xref="S2.E4.m1.2.3.2.2.2">𝐹</ci></apply><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝜃</ci></apply><apply id="S2.E4.m1.2.3.3.cmml" xref="S2.E4.m1.2.3.3"><apply id="S2.E4.m1.2.3.3.1.cmml" xref="S2.E4.m1.2.3.3.1"><csymbol cd="ambiguous" id="S2.E4.m1.2.3.3.1.1.cmml" xref="S2.E4.m1.2.3.3.1">superscript</csymbol><apply id="S2.E4.m1.2.3.3.1.2.cmml" xref="S2.E4.m1.2.3.3.1"><csymbol cd="ambiguous" id="S2.E4.m1.2.3.3.1.2.1.cmml" xref="S2.E4.m1.2.3.3.1">subscript</csymbol><sum id="S2.E4.m1.2.3.3.1.2.2.cmml" xref="S2.E4.m1.2.3.3.1.2.2"></sum><apply id="S2.E4.m1.2.3.3.1.2.3.cmml" xref="S2.E4.m1.2.3.3.1.2.3"><eq id="S2.E4.m1.2.3.3.1.2.3.1.cmml" xref="S2.E4.m1.2.3.3.1.2.3.1"></eq><ci id="S2.E4.m1.2.3.3.1.2.3.2.cmml" xref="S2.E4.m1.2.3.3.1.2.3.2">𝑘</ci><cn type="integer" id="S2.E4.m1.2.3.3.1.2.3.3.cmml" xref="S2.E4.m1.2.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.2.3.3.1.3.cmml" xref="S2.E4.m1.2.3.3.1.3">𝐾</ci></apply><apply id="S2.E4.m1.2.3.3.2.cmml" xref="S2.E4.m1.2.3.3.2"><times id="S2.E4.m1.2.3.3.2.1.cmml" xref="S2.E4.m1.2.3.3.2.1"></times><apply id="S2.E4.m1.2.3.3.2.2.cmml" xref="S2.E4.m1.2.3.3.2.2"><divide id="S2.E4.m1.2.3.3.2.2.1.cmml" xref="S2.E4.m1.2.3.3.2.2"></divide><apply id="S2.E4.m1.2.3.3.2.2.2.cmml" xref="S2.E4.m1.2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.3.3.2.2.2.1.cmml" xref="S2.E4.m1.2.3.3.2.2.2">subscript</csymbol><ci id="S2.E4.m1.2.3.3.2.2.2.2.cmml" xref="S2.E4.m1.2.3.3.2.2.2.2">𝑁</ci><ci id="S2.E4.m1.2.3.3.2.2.2.3.cmml" xref="S2.E4.m1.2.3.3.2.2.2.3">𝑘</ci></apply><ci id="S2.E4.m1.2.3.3.2.2.3.cmml" xref="S2.E4.m1.2.3.3.2.2.3">𝑁</ci></apply><apply id="S2.E4.m1.2.3.3.2.3.cmml" xref="S2.E4.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.3.3.2.3.1.cmml" xref="S2.E4.m1.2.3.3.2.3">subscript</csymbol><ci id="S2.E4.m1.2.3.3.2.3.2.cmml" xref="S2.E4.m1.2.3.3.2.3.2">𝐹</ci><ci id="S2.E4.m1.2.3.3.2.3.3.cmml" xref="S2.E4.m1.2.3.3.2.3.3">𝑘</ci></apply><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.2c">\min_{\theta}F(\theta)=\sum_{k=1}^{K}\frac{N_{k}}{N}F_{k}(\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS1.p1.7" class="ltx_p"><span id="S2.SS2.SSS1.p1.7.1" class="ltx_text" style="color:#000000;">where </span><math id="S2.SS2.SSS1.p1.2.m1.1" class="ltx_Math" alttext="F_{k}(\theta)" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m1.1a"><mrow id="S2.SS2.SSS1.p1.2.m1.1.2" xref="S2.SS2.SSS1.p1.2.m1.1.2.cmml"><msub id="S2.SS2.SSS1.p1.2.m1.1.2.2" xref="S2.SS2.SSS1.p1.2.m1.1.2.2.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.2.m1.1.2.2.2" xref="S2.SS2.SSS1.p1.2.m1.1.2.2.2.cmml">F</mi><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.2.m1.1.2.2.3" xref="S2.SS2.SSS1.p1.2.m1.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.p1.2.m1.1.2.1" xref="S2.SS2.SSS1.p1.2.m1.1.2.1.cmml">​</mo><mrow id="S2.SS2.SSS1.p1.2.m1.1.2.3.2" xref="S2.SS2.SSS1.p1.2.m1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.SS2.SSS1.p1.2.m1.1.2.3.2.1" xref="S2.SS2.SSS1.p1.2.m1.1.2.cmml">(</mo><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.2.m1.1.1" xref="S2.SS2.SSS1.p1.2.m1.1.1.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.SS2.SSS1.p1.2.m1.1.2.3.2.2" xref="S2.SS2.SSS1.p1.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m1.1b"><apply id="S2.SS2.SSS1.p1.2.m1.1.2.cmml" xref="S2.SS2.SSS1.p1.2.m1.1.2"><times id="S2.SS2.SSS1.p1.2.m1.1.2.1.cmml" xref="S2.SS2.SSS1.p1.2.m1.1.2.1"></times><apply id="S2.SS2.SSS1.p1.2.m1.1.2.2.cmml" xref="S2.SS2.SSS1.p1.2.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.2.m1.1.2.2.1.cmml" xref="S2.SS2.SSS1.p1.2.m1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.2.m1.1.2.2.2.cmml" xref="S2.SS2.SSS1.p1.2.m1.1.2.2.2">𝐹</ci><ci id="S2.SS2.SSS1.p1.2.m1.1.2.2.3.cmml" xref="S2.SS2.SSS1.p1.2.m1.1.2.2.3">𝑘</ci></apply><ci id="S2.SS2.SSS1.p1.2.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m1.1c">F_{k}(\theta)</annotation></semantics></math><span id="S2.SS2.SSS1.p1.7.2" class="ltx_text" style="color:#000000;"> is the local loss function of the </span><math id="S2.SS2.SSS1.p1.3.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.SSS1.p1.3.m2.1a"><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.3.m2.1.1" xref="S2.SS2.SSS1.p1.3.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m2.1b"><ci id="S2.SS2.SSS1.p1.3.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m2.1c">k</annotation></semantics></math><span id="S2.SS2.SSS1.p1.7.3" class="ltx_text" style="color:#000000;">-th participant, </span><math id="S2.SS2.SSS1.p1.4.m3.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="S2.SS2.SSS1.p1.4.m3.1a"><msub id="S2.SS2.SSS1.p1.4.m3.1.1" xref="S2.SS2.SSS1.p1.4.m3.1.1.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.4.m3.1.1.2" xref="S2.SS2.SSS1.p1.4.m3.1.1.2.cmml">N</mi><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.4.m3.1.1.3" xref="S2.SS2.SSS1.p1.4.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.4.m3.1b"><apply id="S2.SS2.SSS1.p1.4.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.4.m3.1.1.1.cmml" xref="S2.SS2.SSS1.p1.4.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.4.m3.1.1.2.cmml" xref="S2.SS2.SSS1.p1.4.m3.1.1.2">𝑁</ci><ci id="S2.SS2.SSS1.p1.4.m3.1.1.3.cmml" xref="S2.SS2.SSS1.p1.4.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.4.m3.1c">N_{k}</annotation></semantics></math><span id="S2.SS2.SSS1.p1.7.4" class="ltx_text" style="color:#000000;"> is the number of samples of the </span><math id="S2.SS2.SSS1.p1.5.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.SSS1.p1.5.m4.1a"><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.5.m4.1.1" xref="S2.SS2.SSS1.p1.5.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.5.m4.1b"><ci id="S2.SS2.SSS1.p1.5.m4.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.5.m4.1c">k</annotation></semantics></math><span id="S2.SS2.SSS1.p1.7.5" class="ltx_text" style="color:#000000;">-th participant, </span><math id="S2.SS2.SSS1.p1.6.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.SSS1.p1.6.m5.1a"><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.6.m5.1.1" xref="S2.SS2.SSS1.p1.6.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.6.m5.1b"><ci id="S2.SS2.SSS1.p1.6.m5.1.1.cmml" xref="S2.SS2.SSS1.p1.6.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.6.m5.1c">N</annotation></semantics></math><span id="S2.SS2.SSS1.p1.7.6" class="ltx_text" style="color:#000000;"> is the total number of samples across all participants, and </span><math id="S2.SS2.SSS1.p1.7.m6.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS2.SSS1.p1.7.m6.1a"><mi mathcolor="#000000" id="S2.SS2.SSS1.p1.7.m6.1.1" xref="S2.SS2.SSS1.p1.7.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.7.m6.1b"><ci id="S2.SS2.SSS1.p1.7.m6.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m6.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.7.m6.1c">K</annotation></semantics></math><span id="S2.SS2.SSS1.p1.7.7" class="ltx_text" style="color:#000000;"> is the total number of participants.</span></p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Vertical FL (VFL)</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.2" class="ltx_p"><span id="S2.SS2.SSS2.p1.2.1" class="ltx_text" style="color:#000000;">In VFL, participants aim to learn a global model by leveraging datasets that have the same sample space but different features. The goal is to optimize a global objective function that may involve joining features from different participants to predict a common target. The mathematical formulation involves a coordinated optimization problem where each participant contributes a different part of the feature vector </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhu2021pivodl</span> </a></cite><span id="S2.SS2.SSS2.p1.2.4" class="ltx_text" style="color:#000000;">:</span></p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.2" class="ltx_Math" alttext="\min_{\theta}F(\theta)=\sum_{k=1}^{K}F_{k}(\theta_{k})" display="block"><semantics id="S2.E5.m1.2a"><mrow id="S2.E5.m1.2.2" xref="S2.E5.m1.2.2.cmml"><mrow id="S2.E5.m1.2.2.3" xref="S2.E5.m1.2.2.3.cmml"><mrow id="S2.E5.m1.2.2.3.2" xref="S2.E5.m1.2.2.3.2.cmml"><munder id="S2.E5.m1.2.2.3.2.1" xref="S2.E5.m1.2.2.3.2.1.cmml"><mi mathcolor="#000000" id="S2.E5.m1.2.2.3.2.1.2" xref="S2.E5.m1.2.2.3.2.1.2.cmml">min</mi><mi mathcolor="#000000" id="S2.E5.m1.2.2.3.2.1.3" xref="S2.E5.m1.2.2.3.2.1.3.cmml">θ</mi></munder><mo lspace="0.167em" id="S2.E5.m1.2.2.3.2a" xref="S2.E5.m1.2.2.3.2.cmml">⁡</mo><mi mathcolor="#000000" id="S2.E5.m1.2.2.3.2.2" xref="S2.E5.m1.2.2.3.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.3.1" xref="S2.E5.m1.2.2.3.1.cmml">​</mo><mrow id="S2.E5.m1.2.2.3.3.2" xref="S2.E5.m1.2.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.2.2.3.3.2.1" xref="S2.E5.m1.2.2.3.cmml">(</mo><mi mathcolor="#000000" id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.2.2.3.3.2.2" xref="S2.E5.m1.2.2.3.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" rspace="0.111em" id="S2.E5.m1.2.2.2" xref="S2.E5.m1.2.2.2.cmml">=</mo><mrow id="S2.E5.m1.2.2.1" xref="S2.E5.m1.2.2.1.cmml"><munderover id="S2.E5.m1.2.2.1.2" xref="S2.E5.m1.2.2.1.2.cmml"><mo mathcolor="#000000" movablelimits="false" id="S2.E5.m1.2.2.1.2.2.2" xref="S2.E5.m1.2.2.1.2.2.2.cmml">∑</mo><mrow id="S2.E5.m1.2.2.1.2.2.3" xref="S2.E5.m1.2.2.1.2.2.3.cmml"><mi mathcolor="#000000" id="S2.E5.m1.2.2.1.2.2.3.2" xref="S2.E5.m1.2.2.1.2.2.3.2.cmml">k</mi><mo mathcolor="#000000" id="S2.E5.m1.2.2.1.2.2.3.1" xref="S2.E5.m1.2.2.1.2.2.3.1.cmml">=</mo><mn mathcolor="#000000" id="S2.E5.m1.2.2.1.2.2.3.3" xref="S2.E5.m1.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi mathcolor="#000000" id="S2.E5.m1.2.2.1.2.3" xref="S2.E5.m1.2.2.1.2.3.cmml">K</mi></munderover><mrow id="S2.E5.m1.2.2.1.1" xref="S2.E5.m1.2.2.1.1.cmml"><msub id="S2.E5.m1.2.2.1.1.3" xref="S2.E5.m1.2.2.1.1.3.cmml"><mi mathcolor="#000000" id="S2.E5.m1.2.2.1.1.3.2" xref="S2.E5.m1.2.2.1.1.3.2.cmml">F</mi><mi mathcolor="#000000" id="S2.E5.m1.2.2.1.1.3.3" xref="S2.E5.m1.2.2.1.1.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.1.2" xref="S2.E5.m1.2.2.1.1.2.cmml">​</mo><mrow id="S2.E5.m1.2.2.1.1.1.1" xref="S2.E5.m1.2.2.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.2.2.1.1.1.1.2" xref="S2.E5.m1.2.2.1.1.1.1.1.cmml">(</mo><msub id="S2.E5.m1.2.2.1.1.1.1.1" xref="S2.E5.m1.2.2.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E5.m1.2.2.1.1.1.1.1.2" xref="S2.E5.m1.2.2.1.1.1.1.1.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E5.m1.2.2.1.1.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.1.1.3.cmml">k</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.2.2.1.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.2b"><apply id="S2.E5.m1.2.2.cmml" xref="S2.E5.m1.2.2"><eq id="S2.E5.m1.2.2.2.cmml" xref="S2.E5.m1.2.2.2"></eq><apply id="S2.E5.m1.2.2.3.cmml" xref="S2.E5.m1.2.2.3"><times id="S2.E5.m1.2.2.3.1.cmml" xref="S2.E5.m1.2.2.3.1"></times><apply id="S2.E5.m1.2.2.3.2.cmml" xref="S2.E5.m1.2.2.3.2"><apply id="S2.E5.m1.2.2.3.2.1.cmml" xref="S2.E5.m1.2.2.3.2.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.3.2.1.1.cmml" xref="S2.E5.m1.2.2.3.2.1">subscript</csymbol><min id="S2.E5.m1.2.2.3.2.1.2.cmml" xref="S2.E5.m1.2.2.3.2.1.2"></min><ci id="S2.E5.m1.2.2.3.2.1.3.cmml" xref="S2.E5.m1.2.2.3.2.1.3">𝜃</ci></apply><ci id="S2.E5.m1.2.2.3.2.2.cmml" xref="S2.E5.m1.2.2.3.2.2">𝐹</ci></apply><ci id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1">𝜃</ci></apply><apply id="S2.E5.m1.2.2.1.cmml" xref="S2.E5.m1.2.2.1"><apply id="S2.E5.m1.2.2.1.2.cmml" xref="S2.E5.m1.2.2.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.2.1.cmml" xref="S2.E5.m1.2.2.1.2">superscript</csymbol><apply id="S2.E5.m1.2.2.1.2.2.cmml" xref="S2.E5.m1.2.2.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.2.2.1.cmml" xref="S2.E5.m1.2.2.1.2">subscript</csymbol><sum id="S2.E5.m1.2.2.1.2.2.2.cmml" xref="S2.E5.m1.2.2.1.2.2.2"></sum><apply id="S2.E5.m1.2.2.1.2.2.3.cmml" xref="S2.E5.m1.2.2.1.2.2.3"><eq id="S2.E5.m1.2.2.1.2.2.3.1.cmml" xref="S2.E5.m1.2.2.1.2.2.3.1"></eq><ci id="S2.E5.m1.2.2.1.2.2.3.2.cmml" xref="S2.E5.m1.2.2.1.2.2.3.2">𝑘</ci><cn type="integer" id="S2.E5.m1.2.2.1.2.2.3.3.cmml" xref="S2.E5.m1.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E5.m1.2.2.1.2.3.cmml" xref="S2.E5.m1.2.2.1.2.3">𝐾</ci></apply><apply id="S2.E5.m1.2.2.1.1.cmml" xref="S2.E5.m1.2.2.1.1"><times id="S2.E5.m1.2.2.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.2"></times><apply id="S2.E5.m1.2.2.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.3.1.cmml" xref="S2.E5.m1.2.2.1.1.3">subscript</csymbol><ci id="S2.E5.m1.2.2.1.1.3.2.cmml" xref="S2.E5.m1.2.2.1.1.3.2">𝐹</ci><ci id="S2.E5.m1.2.2.1.1.3.3.cmml" xref="S2.E5.m1.2.2.1.1.3.3">𝑘</ci></apply><apply id="S2.E5.m1.2.2.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.2">𝜃</ci><ci id="S2.E5.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.1.1.1.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.2c">\min_{\theta}F(\theta)=\sum_{k=1}^{K}F_{k}(\theta_{k})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS2.p1.1" class="ltx_p"><span id="S2.SS2.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">with </span><math id="S2.SS2.SSS2.p1.1.m1.4" class="ltx_Math" alttext="\theta=(\theta_{1},\theta_{2},...,\theta_{K})" display="inline"><semantics id="S2.SS2.SSS2.p1.1.m1.4a"><mrow id="S2.SS2.SSS2.p1.1.m1.4.4" xref="S2.SS2.SSS2.p1.1.m1.4.4.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.4.4.5" xref="S2.SS2.SSS2.p1.1.m1.4.4.5.cmml">θ</mi><mo mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.4.4.4" xref="S2.SS2.SSS2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.SS2.SSS2.p1.1.m1.4.4.3.3" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.4.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.4" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">(</mo><msub id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.2.cmml">θ</mi><mn mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.5" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.2" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.2.cmml">θ</mi><mn mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.3" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.6" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathcolor="#000000" mathvariant="normal" id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml">…</mi><mo mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.7" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.2" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.3" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.cmml">K</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.8" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.4b"><apply id="S2.SS2.SSS2.p1.1.m1.4.4.cmml" xref="S2.SS2.SSS2.p1.1.m1.4.4"><eq id="S2.SS2.SSS2.p1.1.m1.4.4.4.cmml" xref="S2.SS2.SSS2.p1.1.m1.4.4.4"></eq><ci id="S2.SS2.SSS2.p1.1.m1.4.4.5.cmml" xref="S2.SS2.SSS2.p1.1.m1.4.4.5">𝜃</ci><vector id="S2.SS2.SSS2.p1.1.m1.4.4.3.4.cmml" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.3"><apply id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.2">𝜃</ci><cn type="integer" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.2">𝜃</ci><cn type="integer" id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">…</ci><apply id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.2">𝜃</ci><ci id="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.4.4.3.3.3.3">𝐾</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.4c">\theta=(\theta_{1},\theta_{2},...,\theta_{K})</annotation></semantics></math><span id="S2.SS2.SSS2.p1.1.2" class="ltx_text" style="color:#000000;"> representing the parts of the model parameters corresponding to the features held by each participant.</span></p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Federated Transfer Learning (FTL)</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.5" class="ltx_p"><span id="S2.SS2.SSS3.p1.5.1" class="ltx_text" style="color:#000000;">FTL seeks to transfer knowledge from one domain to another. Mathematically, this involves optimizing local models on their respective datasets and then transferring some aspects of these models (e.g., model parameters, representations) to improve learning in another domain with possibly different feature and sample spaces </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2020secure</span> </a></cite><span id="S2.SS2.SSS3.p1.5.4" class="ltx_text" style="color:#000000;">:</span></p>
<table id="S2.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E6.m1.5" class="ltx_Math" alttext="\min_{\theta_{s},\theta_{t}}F_{s}(\theta_{s})+F_{t}(\theta_{t},\theta_{s})" display="block"><semantics id="S2.E6.m1.5a"><mrow id="S2.E6.m1.5.5" xref="S2.E6.m1.5.5.cmml"><mrow id="S2.E6.m1.3.3.1" xref="S2.E6.m1.3.3.1.cmml"><mrow id="S2.E6.m1.3.3.1.3" xref="S2.E6.m1.3.3.1.3.cmml"><munder id="S2.E6.m1.3.3.1.3.1" xref="S2.E6.m1.3.3.1.3.1.cmml"><mi mathcolor="#000000" id="S2.E6.m1.3.3.1.3.1.2" xref="S2.E6.m1.3.3.1.3.1.2.cmml">min</mi><mrow id="S2.E6.m1.2.2.2.2" xref="S2.E6.m1.2.2.2.3.cmml"><msub id="S2.E6.m1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E6.m1.1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.1.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E6.m1.1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.1.3.cmml">s</mi></msub><mo mathcolor="#000000" id="S2.E6.m1.2.2.2.2.3" xref="S2.E6.m1.2.2.2.3.cmml">,</mo><msub id="S2.E6.m1.2.2.2.2.2" xref="S2.E6.m1.2.2.2.2.2.cmml"><mi mathcolor="#000000" id="S2.E6.m1.2.2.2.2.2.2" xref="S2.E6.m1.2.2.2.2.2.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E6.m1.2.2.2.2.2.3" xref="S2.E6.m1.2.2.2.2.2.3.cmml">t</mi></msub></mrow></munder><mo lspace="0.167em" id="S2.E6.m1.3.3.1.3a" xref="S2.E6.m1.3.3.1.3.cmml">⁡</mo><msub id="S2.E6.m1.3.3.1.3.2" xref="S2.E6.m1.3.3.1.3.2.cmml"><mi mathcolor="#000000" id="S2.E6.m1.3.3.1.3.2.2" xref="S2.E6.m1.3.3.1.3.2.2.cmml">F</mi><mi mathcolor="#000000" id="S2.E6.m1.3.3.1.3.2.3" xref="S2.E6.m1.3.3.1.3.2.3.cmml">s</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E6.m1.3.3.1.2" xref="S2.E6.m1.3.3.1.2.cmml">​</mo><mrow id="S2.E6.m1.3.3.1.1.1" xref="S2.E6.m1.3.3.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E6.m1.3.3.1.1.1.2" xref="S2.E6.m1.3.3.1.1.1.1.cmml">(</mo><msub id="S2.E6.m1.3.3.1.1.1.1" xref="S2.E6.m1.3.3.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E6.m1.3.3.1.1.1.1.2" xref="S2.E6.m1.3.3.1.1.1.1.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E6.m1.3.3.1.1.1.1.3" xref="S2.E6.m1.3.3.1.1.1.1.3.cmml">s</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.E6.m1.3.3.1.1.1.3" xref="S2.E6.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" id="S2.E6.m1.5.5.4" xref="S2.E6.m1.5.5.4.cmml">+</mo><mrow id="S2.E6.m1.5.5.3" xref="S2.E6.m1.5.5.3.cmml"><msub id="S2.E6.m1.5.5.3.4" xref="S2.E6.m1.5.5.3.4.cmml"><mi mathcolor="#000000" id="S2.E6.m1.5.5.3.4.2" xref="S2.E6.m1.5.5.3.4.2.cmml">F</mi><mi mathcolor="#000000" id="S2.E6.m1.5.5.3.4.3" xref="S2.E6.m1.5.5.3.4.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.E6.m1.5.5.3.3" xref="S2.E6.m1.5.5.3.3.cmml">​</mo><mrow id="S2.E6.m1.5.5.3.2.2" xref="S2.E6.m1.5.5.3.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E6.m1.5.5.3.2.2.3" xref="S2.E6.m1.5.5.3.2.3.cmml">(</mo><msub id="S2.E6.m1.4.4.2.1.1.1" xref="S2.E6.m1.4.4.2.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E6.m1.4.4.2.1.1.1.2" xref="S2.E6.m1.4.4.2.1.1.1.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E6.m1.4.4.2.1.1.1.3" xref="S2.E6.m1.4.4.2.1.1.1.3.cmml">t</mi></msub><mo mathcolor="#000000" id="S2.E6.m1.5.5.3.2.2.4" xref="S2.E6.m1.5.5.3.2.3.cmml">,</mo><msub id="S2.E6.m1.5.5.3.2.2.2" xref="S2.E6.m1.5.5.3.2.2.2.cmml"><mi mathcolor="#000000" id="S2.E6.m1.5.5.3.2.2.2.2" xref="S2.E6.m1.5.5.3.2.2.2.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E6.m1.5.5.3.2.2.2.3" xref="S2.E6.m1.5.5.3.2.2.2.3.cmml">s</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.E6.m1.5.5.3.2.2.5" xref="S2.E6.m1.5.5.3.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E6.m1.5b"><apply id="S2.E6.m1.5.5.cmml" xref="S2.E6.m1.5.5"><plus id="S2.E6.m1.5.5.4.cmml" xref="S2.E6.m1.5.5.4"></plus><apply id="S2.E6.m1.3.3.1.cmml" xref="S2.E6.m1.3.3.1"><times id="S2.E6.m1.3.3.1.2.cmml" xref="S2.E6.m1.3.3.1.2"></times><apply id="S2.E6.m1.3.3.1.3.cmml" xref="S2.E6.m1.3.3.1.3"><apply id="S2.E6.m1.3.3.1.3.1.cmml" xref="S2.E6.m1.3.3.1.3.1"><csymbol cd="ambiguous" id="S2.E6.m1.3.3.1.3.1.1.cmml" xref="S2.E6.m1.3.3.1.3.1">subscript</csymbol><min id="S2.E6.m1.3.3.1.3.1.2.cmml" xref="S2.E6.m1.3.3.1.3.1.2"></min><list id="S2.E6.m1.2.2.2.3.cmml" xref="S2.E6.m1.2.2.2.2"><apply id="S2.E6.m1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E6.m1.1.1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.1.2">𝜃</ci><ci id="S2.E6.m1.1.1.1.1.1.3.cmml" xref="S2.E6.m1.1.1.1.1.1.3">𝑠</ci></apply><apply id="S2.E6.m1.2.2.2.2.2.cmml" xref="S2.E6.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E6.m1.2.2.2.2.2.1.cmml" xref="S2.E6.m1.2.2.2.2.2">subscript</csymbol><ci id="S2.E6.m1.2.2.2.2.2.2.cmml" xref="S2.E6.m1.2.2.2.2.2.2">𝜃</ci><ci id="S2.E6.m1.2.2.2.2.2.3.cmml" xref="S2.E6.m1.2.2.2.2.2.3">𝑡</ci></apply></list></apply><apply id="S2.E6.m1.3.3.1.3.2.cmml" xref="S2.E6.m1.3.3.1.3.2"><csymbol cd="ambiguous" id="S2.E6.m1.3.3.1.3.2.1.cmml" xref="S2.E6.m1.3.3.1.3.2">subscript</csymbol><ci id="S2.E6.m1.3.3.1.3.2.2.cmml" xref="S2.E6.m1.3.3.1.3.2.2">𝐹</ci><ci id="S2.E6.m1.3.3.1.3.2.3.cmml" xref="S2.E6.m1.3.3.1.3.2.3">𝑠</ci></apply></apply><apply id="S2.E6.m1.3.3.1.1.1.1.cmml" xref="S2.E6.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.E6.m1.3.3.1.1.1.1.1.cmml" xref="S2.E6.m1.3.3.1.1.1">subscript</csymbol><ci id="S2.E6.m1.3.3.1.1.1.1.2.cmml" xref="S2.E6.m1.3.3.1.1.1.1.2">𝜃</ci><ci id="S2.E6.m1.3.3.1.1.1.1.3.cmml" xref="S2.E6.m1.3.3.1.1.1.1.3">𝑠</ci></apply></apply><apply id="S2.E6.m1.5.5.3.cmml" xref="S2.E6.m1.5.5.3"><times id="S2.E6.m1.5.5.3.3.cmml" xref="S2.E6.m1.5.5.3.3"></times><apply id="S2.E6.m1.5.5.3.4.cmml" xref="S2.E6.m1.5.5.3.4"><csymbol cd="ambiguous" id="S2.E6.m1.5.5.3.4.1.cmml" xref="S2.E6.m1.5.5.3.4">subscript</csymbol><ci id="S2.E6.m1.5.5.3.4.2.cmml" xref="S2.E6.m1.5.5.3.4.2">𝐹</ci><ci id="S2.E6.m1.5.5.3.4.3.cmml" xref="S2.E6.m1.5.5.3.4.3">𝑡</ci></apply><interval closure="open" id="S2.E6.m1.5.5.3.2.3.cmml" xref="S2.E6.m1.5.5.3.2.2"><apply id="S2.E6.m1.4.4.2.1.1.1.cmml" xref="S2.E6.m1.4.4.2.1.1.1"><csymbol cd="ambiguous" id="S2.E6.m1.4.4.2.1.1.1.1.cmml" xref="S2.E6.m1.4.4.2.1.1.1">subscript</csymbol><ci id="S2.E6.m1.4.4.2.1.1.1.2.cmml" xref="S2.E6.m1.4.4.2.1.1.1.2">𝜃</ci><ci id="S2.E6.m1.4.4.2.1.1.1.3.cmml" xref="S2.E6.m1.4.4.2.1.1.1.3">𝑡</ci></apply><apply id="S2.E6.m1.5.5.3.2.2.2.cmml" xref="S2.E6.m1.5.5.3.2.2.2"><csymbol cd="ambiguous" id="S2.E6.m1.5.5.3.2.2.2.1.cmml" xref="S2.E6.m1.5.5.3.2.2.2">subscript</csymbol><ci id="S2.E6.m1.5.5.3.2.2.2.2.cmml" xref="S2.E6.m1.5.5.3.2.2.2.2">𝜃</ci><ci id="S2.E6.m1.5.5.3.2.2.2.3.cmml" xref="S2.E6.m1.5.5.3.2.2.2.3">𝑠</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m1.5c">\min_{\theta_{s},\theta_{t}}F_{s}(\theta_{s})+F_{t}(\theta_{t},\theta_{s})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS3.p1.4" class="ltx_p"><span id="S2.SS2.SSS3.p1.4.1" class="ltx_text" style="color:#000000;">where </span><math id="S2.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="F_{s}" display="inline"><semantics id="S2.SS2.SSS3.p1.1.m1.1a"><msub id="S2.SS2.SSS3.p1.1.m1.1.1" xref="S2.SS2.SSS3.p1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS3.p1.1.m1.1.1.2" xref="S2.SS2.SSS3.p1.1.m1.1.1.2.cmml">F</mi><mi mathcolor="#000000" id="S2.SS2.SSS3.p1.1.m1.1.1.3" xref="S2.SS2.SSS3.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.1.m1.1b"><apply id="S2.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1.2">𝐹</ci><ci id="S2.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.1.m1.1c">F_{s}</annotation></semantics></math><span id="S2.SS2.SSS3.p1.4.2" class="ltx_text" style="color:#000000;"> and </span><math id="S2.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="F_{t}" display="inline"><semantics id="S2.SS2.SSS3.p1.2.m2.1a"><msub id="S2.SS2.SSS3.p1.2.m2.1.1" xref="S2.SS2.SSS3.p1.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS3.p1.2.m2.1.1.2" xref="S2.SS2.SSS3.p1.2.m2.1.1.2.cmml">F</mi><mi mathcolor="#000000" id="S2.SS2.SSS3.p1.2.m2.1.1.3" xref="S2.SS2.SSS3.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.2.m2.1b"><apply id="S2.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1.2">𝐹</ci><ci id="S2.SS2.SSS3.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.2.m2.1c">F_{t}</annotation></semantics></math><span id="S2.SS2.SSS3.p1.4.3" class="ltx_text" style="color:#000000;"> are the source and target domain objective functions, respectively, and </span><math id="S2.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="\theta_{s}" display="inline"><semantics id="S2.SS2.SSS3.p1.3.m3.1a"><msub id="S2.SS2.SSS3.p1.3.m3.1.1" xref="S2.SS2.SSS3.p1.3.m3.1.1.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS3.p1.3.m3.1.1.2" xref="S2.SS2.SSS3.p1.3.m3.1.1.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.SS2.SSS3.p1.3.m3.1.1.3" xref="S2.SS2.SSS3.p1.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.3.m3.1b"><apply id="S2.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1.2">𝜃</ci><ci id="S2.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.3.m3.1c">\theta_{s}</annotation></semantics></math><span id="S2.SS2.SSS3.p1.4.4" class="ltx_text" style="color:#000000;">, </span><math id="S2.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="\theta_{t}" display="inline"><semantics id="S2.SS2.SSS3.p1.4.m4.1a"><msub id="S2.SS2.SSS3.p1.4.m4.1.1" xref="S2.SS2.SSS3.p1.4.m4.1.1.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS3.p1.4.m4.1.1.2" xref="S2.SS2.SSS3.p1.4.m4.1.1.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.SS2.SSS3.p1.4.m4.1.1.3" xref="S2.SS2.SSS3.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.4.m4.1b"><apply id="S2.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p1.4.m4.1.1.1.cmml" xref="S2.SS2.SSS3.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p1.4.m4.1.1.2.cmml" xref="S2.SS2.SSS3.p1.4.m4.1.1.2">𝜃</ci><ci id="S2.SS2.SSS3.p1.4.m4.1.1.3.cmml" xref="S2.SS2.SSS3.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.4.m4.1c">\theta_{t}</annotation></semantics></math><span id="S2.SS2.SSS3.p1.4.5" class="ltx_text" style="color:#000000;"> are the model parameters for the source and target domains.</span></p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Decentralized FL (DFL)</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.9" class="ltx_p"><span id="S2.SS2.SSS4.p1.9.1" class="ltx_text" style="color:#000000;">DFL optimizes the global model without a central server, relying on local updates and peer-to-peer communication. Each participant updates its model based on local data and then aggregates updates from neighbors, which can be mathematically represented as </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">beltran2023decentralized</span> </a></cite><span id="S2.SS2.SSS4.p1.9.4" class="ltx_text" style="color:#000000;">:</span></p>
<table id="S2.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E7.m1.6" class="ltx_Math" alttext="\theta_{i}^{(t+1)}=\theta_{i}^{(t)}-\eta\left(\nabla F_{i}(\theta_{i}^{(t)})+\sum_{j\in\mathcal{N}_{i}}w_{ij}(\theta_{j}^{(t)}-\theta_{i}^{(t)})\right)" display="block"><semantics id="S2.E7.m1.6a"><mrow id="S2.E7.m1.6.6" xref="S2.E7.m1.6.6.cmml"><msubsup id="S2.E7.m1.6.6.3" xref="S2.E7.m1.6.6.3.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.3.2.2" xref="S2.E7.m1.6.6.3.2.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E7.m1.6.6.3.2.3" xref="S2.E7.m1.6.6.3.2.3.cmml">i</mi><mrow id="S2.E7.m1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.1.1.1.1.2" xref="S2.E7.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E7.m1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E7.m1.1.1.1.1.1.2" xref="S2.E7.m1.1.1.1.1.1.2.cmml">t</mi><mo mathcolor="#000000" id="S2.E7.m1.1.1.1.1.1.1" xref="S2.E7.m1.1.1.1.1.1.1.cmml">+</mo><mn mathcolor="#000000" id="S2.E7.m1.1.1.1.1.1.3" xref="S2.E7.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.1.1.1.1.3" xref="S2.E7.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo mathcolor="#000000" id="S2.E7.m1.6.6.2" xref="S2.E7.m1.6.6.2.cmml">=</mo><mrow id="S2.E7.m1.6.6.1" xref="S2.E7.m1.6.6.1.cmml"><msubsup id="S2.E7.m1.6.6.1.3" xref="S2.E7.m1.6.6.1.3.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.3.2.2" xref="S2.E7.m1.6.6.1.3.2.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.3.2.3" xref="S2.E7.m1.6.6.1.3.2.3.cmml">i</mi><mrow id="S2.E7.m1.2.2.1.3" xref="S2.E7.m1.6.6.1.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.2.2.1.3.1" xref="S2.E7.m1.6.6.1.3.cmml">(</mo><mi mathcolor="#000000" id="S2.E7.m1.2.2.1.1" xref="S2.E7.m1.2.2.1.1.cmml">t</mi><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.2.2.1.3.2" xref="S2.E7.m1.6.6.1.3.cmml">)</mo></mrow></msubsup><mo mathcolor="#000000" id="S2.E7.m1.6.6.1.2" xref="S2.E7.m1.6.6.1.2.cmml">−</mo><mrow id="S2.E7.m1.6.6.1.1" xref="S2.E7.m1.6.6.1.1.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.3" xref="S2.E7.m1.6.6.1.1.3.cmml">η</mi><mo lspace="0em" rspace="0em" id="S2.E7.m1.6.6.1.1.2" xref="S2.E7.m1.6.6.1.1.2.cmml">​</mo><mrow id="S2.E7.m1.6.6.1.1.1.1" xref="S2.E7.m1.6.6.1.1.1.1.1.cmml"><mo mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.2" xref="S2.E7.m1.6.6.1.1.1.1.1.cmml">(</mo><mrow id="S2.E7.m1.6.6.1.1.1.1.1" xref="S2.E7.m1.6.6.1.1.1.1.1.cmml"><mrow id="S2.E7.m1.6.6.1.1.1.1.1.1" xref="S2.E7.m1.6.6.1.1.1.1.1.1.cmml"><mrow id="S2.E7.m1.6.6.1.1.1.1.1.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.cmml"><mo mathcolor="#000000" rspace="0.167em" id="S2.E7.m1.6.6.1.1.1.1.1.1.3.1" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.1.cmml">∇</mo><msub id="S2.E7.m1.6.6.1.1.1.1.1.1.3.2" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.2" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.2.cmml">F</mi><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.3" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E7.m1.6.6.1.1.1.1.1.1.2" xref="S2.E7.m1.6.6.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.2" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi><mrow id="S2.E7.m1.3.3.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.3.3.1.3.1" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi mathcolor="#000000" id="S2.E7.m1.3.3.1.1" xref="S2.E7.m1.3.3.1.1.cmml">t</mi><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.3.3.1.3.2" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" rspace="0.055em" id="S2.E7.m1.6.6.1.1.1.1.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.3.cmml">+</mo><mrow id="S2.E7.m1.6.6.1.1.1.1.1.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.cmml"><munder id="S2.E7.m1.6.6.1.1.1.1.1.2.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.cmml"><mo mathcolor="#000000" movablelimits="false" id="S2.E7.m1.6.6.1.1.1.1.1.2.2.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.2.cmml">j</mi><mo mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.1" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.1.cmml">∈</mo><msub id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.2.cmml">𝒩</mi><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.3.cmml">i</mi></msub></mrow></munder><mrow id="S2.E7.m1.6.6.1.1.1.1.1.2.1" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.cmml"><msub id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.2.cmml">w</mi><mrow id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.1" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.1.cmml">​</mo><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.2.cmml">​</mo><mrow id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.cmml"><msubsup id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.3.cmml">j</mi><mrow id="S2.E7.m1.4.4.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.4.4.1.3.1" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.cmml">(</mo><mi mathcolor="#000000" id="S2.E7.m1.4.4.1.1" xref="S2.E7.m1.4.4.1.1.cmml">t</mi><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.4.4.1.3.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.cmml">)</mo></mrow></msubsup><mo mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.1" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.1.cmml">−</mo><msubsup id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.cmml"><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.3.cmml">i</mi><mrow id="S2.E7.m1.5.5.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.5.5.1.3.1" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.cmml">(</mo><mi mathcolor="#000000" id="S2.E7.m1.5.5.1.1" xref="S2.E7.m1.5.5.1.1.cmml">t</mi><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.5.5.1.3.2" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.cmml">)</mo></mrow></msubsup></mrow><mo mathcolor="#000000" stretchy="false" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo mathcolor="#000000" id="S2.E7.m1.6.6.1.1.1.1.3" xref="S2.E7.m1.6.6.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E7.m1.6b"><apply id="S2.E7.m1.6.6.cmml" xref="S2.E7.m1.6.6"><eq id="S2.E7.m1.6.6.2.cmml" xref="S2.E7.m1.6.6.2"></eq><apply id="S2.E7.m1.6.6.3.cmml" xref="S2.E7.m1.6.6.3"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.3.1.cmml" xref="S2.E7.m1.6.6.3">superscript</csymbol><apply id="S2.E7.m1.6.6.3.2.cmml" xref="S2.E7.m1.6.6.3"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.3.2.1.cmml" xref="S2.E7.m1.6.6.3">subscript</csymbol><ci id="S2.E7.m1.6.6.3.2.2.cmml" xref="S2.E7.m1.6.6.3.2.2">𝜃</ci><ci id="S2.E7.m1.6.6.3.2.3.cmml" xref="S2.E7.m1.6.6.3.2.3">𝑖</ci></apply><apply id="S2.E7.m1.1.1.1.1.1.cmml" xref="S2.E7.m1.1.1.1.1"><plus id="S2.E7.m1.1.1.1.1.1.1.cmml" xref="S2.E7.m1.1.1.1.1.1.1"></plus><ci id="S2.E7.m1.1.1.1.1.1.2.cmml" xref="S2.E7.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S2.E7.m1.1.1.1.1.1.3.cmml" xref="S2.E7.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.E7.m1.6.6.1.cmml" xref="S2.E7.m1.6.6.1"><minus id="S2.E7.m1.6.6.1.2.cmml" xref="S2.E7.m1.6.6.1.2"></minus><apply id="S2.E7.m1.6.6.1.3.cmml" xref="S2.E7.m1.6.6.1.3"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.3.1.cmml" xref="S2.E7.m1.6.6.1.3">superscript</csymbol><apply id="S2.E7.m1.6.6.1.3.2.cmml" xref="S2.E7.m1.6.6.1.3"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.3.2.1.cmml" xref="S2.E7.m1.6.6.1.3">subscript</csymbol><ci id="S2.E7.m1.6.6.1.3.2.2.cmml" xref="S2.E7.m1.6.6.1.3.2.2">𝜃</ci><ci id="S2.E7.m1.6.6.1.3.2.3.cmml" xref="S2.E7.m1.6.6.1.3.2.3">𝑖</ci></apply><ci id="S2.E7.m1.2.2.1.1.cmml" xref="S2.E7.m1.2.2.1.1">𝑡</ci></apply><apply id="S2.E7.m1.6.6.1.1.cmml" xref="S2.E7.m1.6.6.1.1"><times id="S2.E7.m1.6.6.1.1.2.cmml" xref="S2.E7.m1.6.6.1.1.2"></times><ci id="S2.E7.m1.6.6.1.1.3.cmml" xref="S2.E7.m1.6.6.1.1.3">𝜂</ci><apply id="S2.E7.m1.6.6.1.1.1.1.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1"><plus id="S2.E7.m1.6.6.1.1.1.1.1.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.3"></plus><apply id="S2.E7.m1.6.6.1.1.1.1.1.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1"><times id="S2.E7.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.2"></times><apply id="S2.E7.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3"><ci id="S2.E7.m1.6.6.1.1.1.1.1.1.3.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.1">∇</ci><apply id="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.2">𝐹</ci><ci id="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.3.2.3">𝑖</ci></apply></apply><apply id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.2">𝜃</ci><ci id="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.E7.m1.3.3.1.1.cmml" xref="S2.E7.m1.3.3.1.1">𝑡</ci></apply></apply><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2"><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.2.2.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2">subscript</csymbol><sum id="S2.E7.m1.6.6.1.1.1.1.1.2.2.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.2"></sum><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3"><in id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.1"></in><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.2">𝑗</ci><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3">subscript</csymbol><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.2">𝒩</ci><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1"><times id="S2.E7.m1.6.6.1.1.1.1.1.2.1.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.2"></times><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3">subscript</csymbol><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.2">𝑤</ci><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3"><times id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.1"></times><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.2">𝑖</ci><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.3.3.3">𝑗</ci></apply></apply><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1"><minus id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.1"></minus><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2">superscript</csymbol><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2">subscript</csymbol><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.2">𝜃</ci><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.2.2.3">𝑗</ci></apply><ci id="S2.E7.m1.4.4.1.1.cmml" xref="S2.E7.m1.4.4.1.1">𝑡</ci></apply><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3">superscript</csymbol><apply id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.1.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3">subscript</csymbol><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.2.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.2">𝜃</ci><ci id="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.3.cmml" xref="S2.E7.m1.6.6.1.1.1.1.1.2.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S2.E7.m1.5.5.1.1.cmml" xref="S2.E7.m1.5.5.1.1">𝑡</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E7.m1.6c">\theta_{i}^{(t+1)}=\theta_{i}^{(t)}-\eta\left(\nabla F_{i}(\theta_{i}^{(t)})+\sum_{j\in\mathcal{N}_{i}}w_{ij}(\theta_{j}^{(t)}-\theta_{i}^{(t)})\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS4.p1.8" class="ltx_p"><span id="S2.SS2.SSS4.p1.8.1" class="ltx_text" style="color:#000000;">where </span><math id="S2.SS2.SSS4.p1.1.m1.1" class="ltx_Math" alttext="\theta_{i}^{(t)}" display="inline"><semantics id="S2.SS2.SSS4.p1.1.m1.1a"><msubsup id="S2.SS2.SSS4.p1.1.m1.1.2" xref="S2.SS2.SSS4.p1.1.m1.1.2.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.1.m1.1.2.2.2" xref="S2.SS2.SSS4.p1.1.m1.1.2.2.2.cmml">θ</mi><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.1.m1.1.2.2.3" xref="S2.SS2.SSS4.p1.1.m1.1.2.2.3.cmml">i</mi><mrow id="S2.SS2.SSS4.p1.1.m1.1.1.1.3" xref="S2.SS2.SSS4.p1.1.m1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.SS2.SSS4.p1.1.m1.1.1.1.3.1" xref="S2.SS2.SSS4.p1.1.m1.1.2.cmml">(</mo><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.1.m1.1.1.1.1" xref="S2.SS2.SSS4.p1.1.m1.1.1.1.1.cmml">t</mi><mo mathcolor="#000000" stretchy="false" id="S2.SS2.SSS4.p1.1.m1.1.1.1.3.2" xref="S2.SS2.SSS4.p1.1.m1.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.1.m1.1b"><apply id="S2.SS2.SSS4.p1.1.m1.1.2.cmml" xref="S2.SS2.SSS4.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS4.p1.1.m1.1.2.1.cmml" xref="S2.SS2.SSS4.p1.1.m1.1.2">superscript</csymbol><apply id="S2.SS2.SSS4.p1.1.m1.1.2.2.cmml" xref="S2.SS2.SSS4.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS4.p1.1.m1.1.2.2.1.cmml" xref="S2.SS2.SSS4.p1.1.m1.1.2">subscript</csymbol><ci id="S2.SS2.SSS4.p1.1.m1.1.2.2.2.cmml" xref="S2.SS2.SSS4.p1.1.m1.1.2.2.2">𝜃</ci><ci id="S2.SS2.SSS4.p1.1.m1.1.2.2.3.cmml" xref="S2.SS2.SSS4.p1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S2.SS2.SSS4.p1.1.m1.1.1.1.1.cmml" xref="S2.SS2.SSS4.p1.1.m1.1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.1.m1.1c">\theta_{i}^{(t)}</annotation></semantics></math><span id="S2.SS2.SSS4.p1.8.2" class="ltx_text" style="color:#000000;"> is the model parameters of the </span><math id="S2.SS2.SSS4.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.SSS4.p1.2.m2.1a"><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.2.m2.1.1" xref="S2.SS2.SSS4.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.2.m2.1b"><ci id="S2.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS4.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.2.m2.1c">i</annotation></semantics></math><span id="S2.SS2.SSS4.p1.8.3" class="ltx_text" style="color:#000000;">-th participant at iteration </span><math id="S2.SS2.SSS4.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS2.SSS4.p1.3.m3.1a"><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.3.m3.1.1" xref="S2.SS2.SSS4.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.3.m3.1b"><ci id="S2.SS2.SSS4.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS4.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.3.m3.1c">t</annotation></semantics></math><span id="S2.SS2.SSS4.p1.8.4" class="ltx_text" style="color:#000000;">, </span><math id="S2.SS2.SSS4.p1.4.m4.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.SS2.SSS4.p1.4.m4.1a"><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.4.m4.1.1" xref="S2.SS2.SSS4.p1.4.m4.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.4.m4.1b"><ci id="S2.SS2.SSS4.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS4.p1.4.m4.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.4.m4.1c">\eta</annotation></semantics></math><span id="S2.SS2.SSS4.p1.8.5" class="ltx_text" style="color:#000000;"> is the learning rate, </span><math id="S2.SS2.SSS4.p1.5.m5.1" class="ltx_Math" alttext="\nabla F_{i}" display="inline"><semantics id="S2.SS2.SSS4.p1.5.m5.1a"><mrow id="S2.SS2.SSS4.p1.5.m5.1.1" xref="S2.SS2.SSS4.p1.5.m5.1.1.cmml"><mo mathcolor="#000000" rspace="0.167em" id="S2.SS2.SSS4.p1.5.m5.1.1.1" xref="S2.SS2.SSS4.p1.5.m5.1.1.1.cmml">∇</mo><msub id="S2.SS2.SSS4.p1.5.m5.1.1.2" xref="S2.SS2.SSS4.p1.5.m5.1.1.2.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.5.m5.1.1.2.2" xref="S2.SS2.SSS4.p1.5.m5.1.1.2.2.cmml">F</mi><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.5.m5.1.1.2.3" xref="S2.SS2.SSS4.p1.5.m5.1.1.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.5.m5.1b"><apply id="S2.SS2.SSS4.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS4.p1.5.m5.1.1"><ci id="S2.SS2.SSS4.p1.5.m5.1.1.1.cmml" xref="S2.SS2.SSS4.p1.5.m5.1.1.1">∇</ci><apply id="S2.SS2.SSS4.p1.5.m5.1.1.2.cmml" xref="S2.SS2.SSS4.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS4.p1.5.m5.1.1.2.1.cmml" xref="S2.SS2.SSS4.p1.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS4.p1.5.m5.1.1.2.2.cmml" xref="S2.SS2.SSS4.p1.5.m5.1.1.2.2">𝐹</ci><ci id="S2.SS2.SSS4.p1.5.m5.1.1.2.3.cmml" xref="S2.SS2.SSS4.p1.5.m5.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.5.m5.1c">\nabla F_{i}</annotation></semantics></math><span id="S2.SS2.SSS4.p1.8.6" class="ltx_text" style="color:#000000;"> is the gradient of the local loss function, and </span><math id="S2.SS2.SSS4.p1.6.m6.1" class="ltx_Math" alttext="w_{ij}" display="inline"><semantics id="S2.SS2.SSS4.p1.6.m6.1a"><msub id="S2.SS2.SSS4.p1.6.m6.1.1" xref="S2.SS2.SSS4.p1.6.m6.1.1.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.6.m6.1.1.2" xref="S2.SS2.SSS4.p1.6.m6.1.1.2.cmml">w</mi><mrow id="S2.SS2.SSS4.p1.6.m6.1.1.3" xref="S2.SS2.SSS4.p1.6.m6.1.1.3.cmml"><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.6.m6.1.1.3.2" xref="S2.SS2.SSS4.p1.6.m6.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS4.p1.6.m6.1.1.3.1" xref="S2.SS2.SSS4.p1.6.m6.1.1.3.1.cmml">​</mo><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.6.m6.1.1.3.3" xref="S2.SS2.SSS4.p1.6.m6.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.6.m6.1b"><apply id="S2.SS2.SSS4.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS4.p1.6.m6.1.1.1.cmml" xref="S2.SS2.SSS4.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS4.p1.6.m6.1.1.2.cmml" xref="S2.SS2.SSS4.p1.6.m6.1.1.2">𝑤</ci><apply id="S2.SS2.SSS4.p1.6.m6.1.1.3.cmml" xref="S2.SS2.SSS4.p1.6.m6.1.1.3"><times id="S2.SS2.SSS4.p1.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS4.p1.6.m6.1.1.3.1"></times><ci id="S2.SS2.SSS4.p1.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS4.p1.6.m6.1.1.3.2">𝑖</ci><ci id="S2.SS2.SSS4.p1.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS4.p1.6.m6.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.6.m6.1c">w_{ij}</annotation></semantics></math><span id="S2.SS2.SSS4.p1.8.7" class="ltx_text" style="color:#000000;"> are the weights denoting the influence between the </span><math id="S2.SS2.SSS4.p1.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.SSS4.p1.7.m7.1a"><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.7.m7.1.1" xref="S2.SS2.SSS4.p1.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.7.m7.1b"><ci id="S2.SS2.SSS4.p1.7.m7.1.1.cmml" xref="S2.SS2.SSS4.p1.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.7.m7.1c">i</annotation></semantics></math><span id="S2.SS2.SSS4.p1.8.8" class="ltx_text" style="color:#000000;">-th participant and its neighbors </span><math id="S2.SS2.SSS4.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{N}_{i}" display="inline"><semantics id="S2.SS2.SSS4.p1.8.m8.1a"><msub id="S2.SS2.SSS4.p1.8.m8.1.1" xref="S2.SS2.SSS4.p1.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathcolor="#000000" id="S2.SS2.SSS4.p1.8.m8.1.1.2" xref="S2.SS2.SSS4.p1.8.m8.1.1.2.cmml">𝒩</mi><mi mathcolor="#000000" id="S2.SS2.SSS4.p1.8.m8.1.1.3" xref="S2.SS2.SSS4.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.8.m8.1b"><apply id="S2.SS2.SSS4.p1.8.m8.1.1.cmml" xref="S2.SS2.SSS4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS4.p1.8.m8.1.1.1.cmml" xref="S2.SS2.SSS4.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.SSS4.p1.8.m8.1.1.2.cmml" xref="S2.SS2.SSS4.p1.8.m8.1.1.2">𝒩</ci><ci id="S2.SS2.SSS4.p1.8.m8.1.1.3.cmml" xref="S2.SS2.SSS4.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.8.m8.1c">\mathcal{N}_{i}</annotation></semantics></math><span id="S2.SS2.SSS4.p1.8.9" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S2.SS2.SSS4.p2" class="ltx_para">
<p id="S2.SS2.SSS4.p2.1" class="ltx_p"><span id="S2.SS2.SSS4.p2.1.1" class="ltx_text" style="color:#000000;">The key difference among these types of FL lies in the formulation of the objective function, data partitioning, and model parameter sharing strategies. HFL and VFL differ primarily in the structure of the data they are designed to work with, while FTL focuses on transferring knowledge between domains. DFL distinguishes itself by the decentralization of the model update and aggregation process.</span></p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Advantages of using FL for CD based on image analysis</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p"><span id="S2.SS3.p1.1.1" class="ltx_text" style="color:#000000;">The great evolution of IoT devices (edge devices and wearable sensors), has helped in improving medical monitoring and anomaly detection. This is done by generating real-time medical data such as biometrics, blood tests, and medical images. These types of datasets, especially the medical images, have generated a huge overflow of data which has caused performance and security issues. So for that FL has been inaugurated to resolve these issues and make other benefits and advantages such as:
Improving privacy, especially with the availability of enormous and sensitive medical data, it has become more and more easily accessed and attacked. Security and privacy have emerged as a focal and very point to address and improve, FL has introduced a new approach that allows hospitals and medical patient not to send or share their sensitive data. On the other hand, the client only shares weights to the server so that it will be aggregated </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2022review</span> </a></cite><span id="S2.SS3.p1.1.4" class="ltx_text" style="color:#000000;">. So the data has not been stored in one place and was not intercepted in the communication process, which is a huge boost to the security and privacy of the datasets.</span></p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text" style="color:#000000;">Furthermore, FL approach needs the collection of datasets from various hospitals from different locations, the diversity of data allows for building a more robust model which improves the quality of the detection of anomalies and potential symptoms that lead to cancer. The diversity of the datasets allow also the reduction of the bias, this improved the prediction and the performance of the model </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">darzidehkalani2022federated</span> </a></cite><span id="S2.SS3.p2.1.4" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text" style="color:#000000;">In addition, one of the advantages of FL is that it improves drastically time execution of the training model and reduces the necessity of a very large space in the server (to save all the hospitals’ data) because the clients’ machines send only weights and parameters, also the federated server does not require having computational power because the training was run on different clients. The scalability of the FL server has not been affected by adding new hospitals or patients. Because the training was done on the client machines and shared only the data, this decentralization improved the scalability of FL which is a very important advantage </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">diaz2023study</span> </a></cite><span id="S2.SS3.p3.1.4" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">3 </span>Transfer Learning</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Definition and concept of transfer learning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="color:#000000;">TL is a new technique that allows utilizing knowledge and insights that is acquired and learned from data and use this knowledge it in different but related domains, this allows ML algorithms sharing their experience to improve the performance and reduce the dependency on labeled datasets </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zheng2022application</span> </a></cite><span id="S3.SS1.p1.1.4" class="ltx_text" style="color:#000000;">. This approach is very useful if there is difficulty in collecting labeled medical data, which is a problematic in healthcare department.</span></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.8" class="ltx_p"><span id="S3.SS1.p2.8.1" class="ltx_text" style="color:#000000;">TL aims to improve learning in a new target task through the transfer of knowledge from a related source task. It involves two domains: the source domain </span><math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝐷</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">D_{S}</annotation></semantics></math><span id="S3.SS1.p2.8.2" class="ltx_text" style="color:#000000;"> with data distribution </span><math id="S3.SS1.p2.2.m2.2" class="ltx_Math" alttext="P(X_{S},Y_{S})" display="inline"><semantics id="S3.SS1.p2.2.m2.2a"><mrow id="S3.SS1.p2.2.m2.2.2" xref="S3.SS1.p2.2.m2.2.2.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.2.m2.2.2.4" xref="S3.SS1.p2.2.m2.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.2.2.3" xref="S3.SS1.p2.2.m2.2.2.3.cmml">​</mo><mrow id="S3.SS1.p2.2.m2.2.2.2.2" xref="S3.SS1.p2.2.m2.2.2.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p2.2.m2.2.2.2.2.3" xref="S3.SS1.p2.2.m2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.2.m2.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml">X</mi><mi mathcolor="#000000" id="S3.SS1.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml">S</mi></msub><mo mathcolor="#000000" id="S3.SS1.p2.2.m2.2.2.2.2.4" xref="S3.SS1.p2.2.m2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.2.m2.2.2.2.2.2" xref="S3.SS1.p2.2.m2.2.2.2.2.2.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.2.m2.2.2.2.2.2.2" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.cmml">Y</mi><mi mathcolor="#000000" id="S3.SS1.p2.2.m2.2.2.2.2.2.3" xref="S3.SS1.p2.2.m2.2.2.2.2.2.3.cmml">S</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p2.2.m2.2.2.2.2.5" xref="S3.SS1.p2.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.2b"><apply id="S3.SS1.p2.2.m2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2"><times id="S3.SS1.p2.2.m2.2.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.3"></times><ci id="S3.SS1.p2.2.m2.2.2.4.cmml" xref="S3.SS1.p2.2.m2.2.2.4">𝑃</ci><interval closure="open" id="S3.SS1.p2.2.m2.2.2.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2"><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2">𝑋</ci><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3">𝑆</ci></apply><apply id="S3.SS1.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2">𝑌</ci><ci id="S3.SS1.p2.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2.3">𝑆</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.2c">P(X_{S},Y_{S})</annotation></semantics></math><span id="S3.SS1.p2.8.3" class="ltx_text" style="color:#000000;"> and the target domain </span><math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="D_{T}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝐷</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">D_{T}</annotation></semantics></math><span id="S3.SS1.p2.8.4" class="ltx_text" style="color:#000000;"> with data distribution </span><math id="S3.SS1.p2.4.m4.2" class="ltx_Math" alttext="P(X_{T},Y_{T})" display="inline"><semantics id="S3.SS1.p2.4.m4.2a"><mrow id="S3.SS1.p2.4.m4.2.2" xref="S3.SS1.p2.4.m4.2.2.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.4.m4.2.2.4" xref="S3.SS1.p2.4.m4.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.2.2.3" xref="S3.SS1.p2.4.m4.2.2.3.cmml">​</mo><mrow id="S3.SS1.p2.4.m4.2.2.2.2" xref="S3.SS1.p2.4.m4.2.2.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p2.4.m4.2.2.2.2.3" xref="S3.SS1.p2.4.m4.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p2.4.m4.1.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.4.m4.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml">X</mi><mi mathcolor="#000000" id="S3.SS1.p2.4.m4.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml">T</mi></msub><mo mathcolor="#000000" id="S3.SS1.p2.4.m4.2.2.2.2.4" xref="S3.SS1.p2.4.m4.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.4.m4.2.2.2.2.2" xref="S3.SS1.p2.4.m4.2.2.2.2.2.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.4.m4.2.2.2.2.2.2" xref="S3.SS1.p2.4.m4.2.2.2.2.2.2.cmml">Y</mi><mi mathcolor="#000000" id="S3.SS1.p2.4.m4.2.2.2.2.2.3" xref="S3.SS1.p2.4.m4.2.2.2.2.2.3.cmml">T</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p2.4.m4.2.2.2.2.5" xref="S3.SS1.p2.4.m4.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.2b"><apply id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2"><times id="S3.SS1.p2.4.m4.2.2.3.cmml" xref="S3.SS1.p2.4.m4.2.2.3"></times><ci id="S3.SS1.p2.4.m4.2.2.4.cmml" xref="S3.SS1.p2.4.m4.2.2.4">𝑃</ci><interval closure="open" id="S3.SS1.p2.4.m4.2.2.2.3.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2"><apply id="S3.SS1.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.2">𝑋</ci><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.3">𝑇</ci></apply><apply id="S3.SS1.p2.4.m4.2.2.2.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2.2.2">𝑌</ci><ci id="S3.SS1.p2.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2.2.3">𝑇</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.2c">P(X_{T},Y_{T})</annotation></semantics></math><span id="S3.SS1.p2.8.5" class="ltx_text" style="color:#000000;">. The goal is to use knowledge from </span><math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝐷</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">D_{S}</annotation></semantics></math><span id="S3.SS1.p2.8.6" class="ltx_text" style="color:#000000;"> to improve the learning of the target predictive function </span><math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="f_{T}(\cdot)" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.2" xref="S3.SS1.p2.6.m6.1.2.cmml"><msub id="S3.SS1.p2.6.m6.1.2.2" xref="S3.SS1.p2.6.m6.1.2.2.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.6.m6.1.2.2.2" xref="S3.SS1.p2.6.m6.1.2.2.2.cmml">f</mi><mi mathcolor="#000000" id="S3.SS1.p2.6.m6.1.2.2.3" xref="S3.SS1.p2.6.m6.1.2.2.3.cmml">T</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p2.6.m6.1.2.1" xref="S3.SS1.p2.6.m6.1.2.1.cmml">​</mo><mrow id="S3.SS1.p2.6.m6.1.2.3.2" xref="S3.SS1.p2.6.m6.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p2.6.m6.1.2.3.2.1" xref="S3.SS1.p2.6.m6.1.2.cmml">(</mo><mo lspace="0em" mathcolor="#000000" rspace="0em" id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">⋅</mo><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p2.6.m6.1.2.3.2.2" xref="S3.SS1.p2.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.2.cmml" xref="S3.SS1.p2.6.m6.1.2"><times id="S3.SS1.p2.6.m6.1.2.1.cmml" xref="S3.SS1.p2.6.m6.1.2.1"></times><apply id="S3.SS1.p2.6.m6.1.2.2.cmml" xref="S3.SS1.p2.6.m6.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.2.2.1.cmml" xref="S3.SS1.p2.6.m6.1.2.2">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.2.2.2.cmml" xref="S3.SS1.p2.6.m6.1.2.2.2">𝑓</ci><ci id="S3.SS1.p2.6.m6.1.2.2.3.cmml" xref="S3.SS1.p2.6.m6.1.2.2.3">𝑇</ci></apply><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">f_{T}(\cdot)</annotation></semantics></math><span id="S3.SS1.p2.8.7" class="ltx_text" style="color:#000000;"> in </span><math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="D_{T}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝐷</ci><ci id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">D_{T}</annotation></semantics></math><span id="S3.SS1.p2.8.8" class="ltx_text" style="color:#000000;">.
Typically, TL uses observations from multiple source domains and tasks to learn a decision function that can be applied to the target domain and task, with the aim of increasing diversity and improving the performance of the target class </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhuang2020comprehensive</span> </a></cite><span id="S3.SS1.p2.8.11" class="ltx_text" style="color:#000000;">.
This is doable by minimizing the loss on the target domain, </span><math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="L_{T}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><msub id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">L</mi><mi mathcolor="#000000" id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝐿</ci><ci id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">L_{T}</annotation></semantics></math><span id="S3.SS1.p2.8.12" class="ltx_text" style="color:#000000;">, using knowledge from the source domain. Fig. </span><a href="#S3.F1" title="Figure 1 ‣ 3.1.2 Domain Adaptation ‣ 3.1 Definition and concept of transfer learning ‣ 3 Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS1.p2.8.13" class="ltx_text" style="color:#000000;"> provides a visual representation of the types and the subtypes of TL methods.</span></p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Fine-tuning</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p"><span id="S3.SS1.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">Fine-tuning adjusts a pre-trained model to perform a new target task, involving minor modifications to the model architecture and re-training the model on the target task data.</span></p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.3" class="ltx_Math" alttext="\theta^{*}=\arg\min_{\theta}L_{T}(\theta;D_{T})+\lambda R(\theta)" display="block"><semantics id="S3.E8.m1.3a"><mrow id="S3.E8.m1.3.3" xref="S3.E8.m1.3.3.cmml"><msup id="S3.E8.m1.3.3.3" xref="S3.E8.m1.3.3.3.cmml"><mi mathcolor="#000000" id="S3.E8.m1.3.3.3.2" xref="S3.E8.m1.3.3.3.2.cmml">θ</mi><mo mathcolor="#000000" id="S3.E8.m1.3.3.3.3" xref="S3.E8.m1.3.3.3.3.cmml">∗</mo></msup><mo mathcolor="#000000" id="S3.E8.m1.3.3.2" xref="S3.E8.m1.3.3.2.cmml">=</mo><mrow id="S3.E8.m1.3.3.1" xref="S3.E8.m1.3.3.1.cmml"><mrow id="S3.E8.m1.3.3.1.1" xref="S3.E8.m1.3.3.1.1.cmml"><mrow id="S3.E8.m1.3.3.1.1.3" xref="S3.E8.m1.3.3.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.1.3.1" xref="S3.E8.m1.3.3.1.1.3.1.cmml">arg</mi><mo lspace="0.167em" id="S3.E8.m1.3.3.1.1.3a" xref="S3.E8.m1.3.3.1.1.3.cmml">⁡</mo><mrow id="S3.E8.m1.3.3.1.1.3.2" xref="S3.E8.m1.3.3.1.1.3.2.cmml"><munder id="S3.E8.m1.3.3.1.1.3.2.1" xref="S3.E8.m1.3.3.1.1.3.2.1.cmml"><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.1.3.2.1.2" xref="S3.E8.m1.3.3.1.1.3.2.1.2.cmml">min</mi><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.1.3.2.1.3" xref="S3.E8.m1.3.3.1.1.3.2.1.3.cmml">θ</mi></munder><mo lspace="0.167em" id="S3.E8.m1.3.3.1.1.3.2a" xref="S3.E8.m1.3.3.1.1.3.2.cmml">⁡</mo><msub id="S3.E8.m1.3.3.1.1.3.2.2" xref="S3.E8.m1.3.3.1.1.3.2.2.cmml"><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.1.3.2.2.2" xref="S3.E8.m1.3.3.1.1.3.2.2.2.cmml">L</mi><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.1.3.2.2.3" xref="S3.E8.m1.3.3.1.1.3.2.2.3.cmml">T</mi></msub></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.1.1.2" xref="S3.E8.m1.3.3.1.1.2.cmml">​</mo><mrow id="S3.E8.m1.3.3.1.1.1.1" xref="S3.E8.m1.3.3.1.1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E8.m1.3.3.1.1.1.1.2" xref="S3.E8.m1.3.3.1.1.1.2.cmml">(</mo><mi mathcolor="#000000" id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml">θ</mi><mo mathcolor="#000000" id="S3.E8.m1.3.3.1.1.1.1.3" xref="S3.E8.m1.3.3.1.1.1.2.cmml">;</mo><msub id="S3.E8.m1.3.3.1.1.1.1.1" xref="S3.E8.m1.3.3.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.1.1.1.1.2" xref="S3.E8.m1.3.3.1.1.1.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.1.1.1.1.3" xref="S3.E8.m1.3.3.1.1.1.1.1.3.cmml">T</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E8.m1.3.3.1.1.1.1.4" xref="S3.E8.m1.3.3.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" id="S3.E8.m1.3.3.1.2" xref="S3.E8.m1.3.3.1.2.cmml">+</mo><mrow id="S3.E8.m1.3.3.1.3" xref="S3.E8.m1.3.3.1.3.cmml"><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.3.2" xref="S3.E8.m1.3.3.1.3.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.1.3.1" xref="S3.E8.m1.3.3.1.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.E8.m1.3.3.1.3.3" xref="S3.E8.m1.3.3.1.3.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.3.1.3.1a" xref="S3.E8.m1.3.3.1.3.1.cmml">​</mo><mrow id="S3.E8.m1.3.3.1.3.4.2" xref="S3.E8.m1.3.3.1.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E8.m1.3.3.1.3.4.2.1" xref="S3.E8.m1.3.3.1.3.cmml">(</mo><mi mathcolor="#000000" id="S3.E8.m1.2.2" xref="S3.E8.m1.2.2.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S3.E8.m1.3.3.1.3.4.2.2" xref="S3.E8.m1.3.3.1.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.3b"><apply id="S3.E8.m1.3.3.cmml" xref="S3.E8.m1.3.3"><eq id="S3.E8.m1.3.3.2.cmml" xref="S3.E8.m1.3.3.2"></eq><apply id="S3.E8.m1.3.3.3.cmml" xref="S3.E8.m1.3.3.3"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.3.1.cmml" xref="S3.E8.m1.3.3.3">superscript</csymbol><ci id="S3.E8.m1.3.3.3.2.cmml" xref="S3.E8.m1.3.3.3.2">𝜃</ci><times id="S3.E8.m1.3.3.3.3.cmml" xref="S3.E8.m1.3.3.3.3"></times></apply><apply id="S3.E8.m1.3.3.1.cmml" xref="S3.E8.m1.3.3.1"><plus id="S3.E8.m1.3.3.1.2.cmml" xref="S3.E8.m1.3.3.1.2"></plus><apply id="S3.E8.m1.3.3.1.1.cmml" xref="S3.E8.m1.3.3.1.1"><times id="S3.E8.m1.3.3.1.1.2.cmml" xref="S3.E8.m1.3.3.1.1.2"></times><apply id="S3.E8.m1.3.3.1.1.3.cmml" xref="S3.E8.m1.3.3.1.1.3"><arg id="S3.E8.m1.3.3.1.1.3.1.cmml" xref="S3.E8.m1.3.3.1.1.3.1"></arg><apply id="S3.E8.m1.3.3.1.1.3.2.cmml" xref="S3.E8.m1.3.3.1.1.3.2"><apply id="S3.E8.m1.3.3.1.1.3.2.1.cmml" xref="S3.E8.m1.3.3.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.3.2.1.1.cmml" xref="S3.E8.m1.3.3.1.1.3.2.1">subscript</csymbol><min id="S3.E8.m1.3.3.1.1.3.2.1.2.cmml" xref="S3.E8.m1.3.3.1.1.3.2.1.2"></min><ci id="S3.E8.m1.3.3.1.1.3.2.1.3.cmml" xref="S3.E8.m1.3.3.1.1.3.2.1.3">𝜃</ci></apply><apply id="S3.E8.m1.3.3.1.1.3.2.2.cmml" xref="S3.E8.m1.3.3.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.3.2.2.1.cmml" xref="S3.E8.m1.3.3.1.1.3.2.2">subscript</csymbol><ci id="S3.E8.m1.3.3.1.1.3.2.2.2.cmml" xref="S3.E8.m1.3.3.1.1.3.2.2.2">𝐿</ci><ci id="S3.E8.m1.3.3.1.1.3.2.2.3.cmml" xref="S3.E8.m1.3.3.1.1.3.2.2.3">𝑇</ci></apply></apply></apply><list id="S3.E8.m1.3.3.1.1.1.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1"><ci id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1">𝜃</ci><apply id="S3.E8.m1.3.3.1.1.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E8.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.2">𝐷</ci><ci id="S3.E8.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E8.m1.3.3.1.1.1.1.1.3">𝑇</ci></apply></list></apply><apply id="S3.E8.m1.3.3.1.3.cmml" xref="S3.E8.m1.3.3.1.3"><times id="S3.E8.m1.3.3.1.3.1.cmml" xref="S3.E8.m1.3.3.1.3.1"></times><ci id="S3.E8.m1.3.3.1.3.2.cmml" xref="S3.E8.m1.3.3.1.3.2">𝜆</ci><ci id="S3.E8.m1.3.3.1.3.3.cmml" xref="S3.E8.m1.3.3.1.3.3">𝑅</ci><ci id="S3.E8.m1.2.2.cmml" xref="S3.E8.m1.2.2">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.3c">\theta^{*}=\arg\min_{\theta}L_{T}(\theta;D_{T})+\lambda R(\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS1.p2.3" class="ltx_p"><span id="S3.SS1.SSS1.p2.3.1" class="ltx_text" style="color:#000000;">where </span><math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="L_{T}" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><msub id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.SSS1.p2.1.m1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml">L</mi><mi mathcolor="#000000" id="S3.SS1.SSS1.p2.1.m1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2">𝐿</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">L_{T}</annotation></semantics></math><span id="S3.SS1.SSS1.p2.3.2" class="ltx_text" style="color:#000000;"> is the target domain loss, </span><math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="R(\theta)" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mrow id="S3.SS1.SSS1.p2.2.m2.1.2" xref="S3.SS1.SSS1.p2.2.m2.1.2.cmml"><mi mathcolor="#000000" id="S3.SS1.SSS1.p2.2.m2.1.2.2" xref="S3.SS1.SSS1.p2.2.m2.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p2.2.m2.1.2.1" xref="S3.SS1.SSS1.p2.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS1.SSS1.p2.2.m2.1.2.3.2" xref="S3.SS1.SSS1.p2.2.m2.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS1.SSS1.p2.2.m2.1.2.3.2.1" xref="S3.SS1.SSS1.p2.2.m2.1.2.cmml">(</mo><mi mathcolor="#000000" id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">θ</mi><mo mathcolor="#000000" stretchy="false" id="S3.SS1.SSS1.p2.2.m2.1.2.3.2.2" xref="S3.SS1.SSS1.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><apply id="S3.SS1.SSS1.p2.2.m2.1.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2"><times id="S3.SS1.SSS1.p2.2.m2.1.2.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2.1"></times><ci id="S3.SS1.SSS1.p2.2.m2.1.2.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.2.2">𝑅</ci><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">R(\theta)</annotation></semantics></math><span id="S3.SS1.SSS1.p2.3.3" class="ltx_text" style="color:#000000;"> is a regularization term, and </span><math id="S3.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><mi mathcolor="#000000" id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.1b"><ci id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.1c">\lambda</annotation></semantics></math><span id="S3.SS1.SSS1.p2.3.4" class="ltx_text" style="color:#000000;"> is a regularization parameter.</span></p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Domain Adaptation</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.3" class="ltx_p"><span id="S3.SS1.SSS2.p1.3.1" class="ltx_text" style="color:#000000;">Domain adaptation is a technique in machine learning that aims to enable a model trained on a source domain to perform well on a different but related target domain, especially when there is abundant labeled data in the source domain and limited or no labeled data in the target domain </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2018deep</span> </a></cite><span id="S3.SS1.SSS2.p1.3.4" class="ltx_text" style="color:#000000;">.
Domain adaptation seeks to transfer knowledge from a well-labeled source domain to a less-labeled or unlabeled target domain by minimizing both the domain discrepancy and the predictive loss on the target domain, using techniques such as discrepancy measures, adversarial training, and covariate shift adjustment.
Domain adaptation aims at minimizing the domain discrepancy between </span><math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="D_{S}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><msub id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">D_{S}</annotation></semantics></math><span id="S3.SS1.SSS2.p1.3.5" class="ltx_text" style="color:#000000;"> and </span><math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="D_{T}" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><msub id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS1.SSS2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2">𝐷</ci><ci id="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">D_{T}</annotation></semantics></math><span id="S3.SS1.SSS2.p1.3.6" class="ltx_text" style="color:#000000;"> and the predictive loss on the target domain </span><math id="S3.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="D_{T}" display="inline"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><msub id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.SSS2.p1.3.m3.1.1.2" xref="S3.SS1.SSS2.p1.3.m3.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS1.SSS2.p1.3.m3.1.1.3" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><apply id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.2">𝐷</ci><ci id="S3.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">D_{T}</annotation></semantics></math><span id="S3.SS1.SSS2.p1.3.7" class="ltx_text" style="color:#000000;"> </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">you2019universal</span> </a></cite><span id="S3.SS1.SSS2.p1.3.10" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.3" class="ltx_p"><span id="S3.I1.i1.p1.3.1" class="ltx_text" style="color:#000000;">Discrepancy Measures: A common approach is to use discrepancy measures such as the maximum mean discrepancy (MMD) defined as </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">lee2019sliced</span> </a></cite><span id="S3.I1.i1.p1.3.4" class="ltx_text" style="color:#000000;">:</span></p>
<table id="S3.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E9.m1.6" class="ltx_Math" alttext="\text{MMD}^{2}(F,X_{S},X_{T})=\sup_{f\in F}\left(E_{x_{s}\sim P(X_{S})}[f(x_{s})]-E_{x_{t}\sim P(X_{T})}[f(x_{t})]\right)^{2}" display="block"><semantics id="S3.E9.m1.6a"><mrow id="S3.E9.m1.6.6" xref="S3.E9.m1.6.6.cmml"><mrow id="S3.E9.m1.5.5.2" xref="S3.E9.m1.5.5.2.cmml"><msup id="S3.E9.m1.5.5.2.4" xref="S3.E9.m1.5.5.2.4.cmml"><mtext mathcolor="#000000" id="S3.E9.m1.5.5.2.4.2" xref="S3.E9.m1.5.5.2.4.2a.cmml">MMD</mtext><mn mathcolor="#000000" id="S3.E9.m1.5.5.2.4.3" xref="S3.E9.m1.5.5.2.4.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.E9.m1.5.5.2.3" xref="S3.E9.m1.5.5.2.3.cmml">​</mo><mrow id="S3.E9.m1.5.5.2.2.2" xref="S3.E9.m1.5.5.2.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.5.5.2.2.2.3" xref="S3.E9.m1.5.5.2.2.3.cmml">(</mo><mi mathcolor="#000000" id="S3.E9.m1.3.3" xref="S3.E9.m1.3.3.cmml">F</mi><mo mathcolor="#000000" id="S3.E9.m1.5.5.2.2.2.4" xref="S3.E9.m1.5.5.2.2.3.cmml">,</mo><msub id="S3.E9.m1.4.4.1.1.1.1" xref="S3.E9.m1.4.4.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.4.4.1.1.1.1.2" xref="S3.E9.m1.4.4.1.1.1.1.2.cmml">X</mi><mi mathcolor="#000000" id="S3.E9.m1.4.4.1.1.1.1.3" xref="S3.E9.m1.4.4.1.1.1.1.3.cmml">S</mi></msub><mo mathcolor="#000000" id="S3.E9.m1.5.5.2.2.2.5" xref="S3.E9.m1.5.5.2.2.3.cmml">,</mo><msub id="S3.E9.m1.5.5.2.2.2.2" xref="S3.E9.m1.5.5.2.2.2.2.cmml"><mi mathcolor="#000000" id="S3.E9.m1.5.5.2.2.2.2.2" xref="S3.E9.m1.5.5.2.2.2.2.2.cmml">X</mi><mi mathcolor="#000000" id="S3.E9.m1.5.5.2.2.2.2.3" xref="S3.E9.m1.5.5.2.2.2.2.3.cmml">T</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.5.5.2.2.2.6" xref="S3.E9.m1.5.5.2.2.3.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" rspace="0.1389em" id="S3.E9.m1.6.6.4" xref="S3.E9.m1.6.6.4.cmml">=</mo><mrow id="S3.E9.m1.6.6.3" xref="S3.E9.m1.6.6.3.cmml"><munder id="S3.E9.m1.6.6.3.2" xref="S3.E9.m1.6.6.3.2.cmml"><mo lspace="0.1389em" mathcolor="#000000" movablelimits="false" rspace="0em" id="S3.E9.m1.6.6.3.2.2" xref="S3.E9.m1.6.6.3.2.2.cmml">sup</mo><mrow id="S3.E9.m1.6.6.3.2.3" xref="S3.E9.m1.6.6.3.2.3.cmml"><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.2.3.2" xref="S3.E9.m1.6.6.3.2.3.2.cmml">f</mi><mo mathcolor="#000000" id="S3.E9.m1.6.6.3.2.3.1" xref="S3.E9.m1.6.6.3.2.3.1.cmml">∈</mo><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.2.3.3" xref="S3.E9.m1.6.6.3.2.3.3.cmml">F</mi></mrow></munder><msup id="S3.E9.m1.6.6.3.1" xref="S3.E9.m1.6.6.3.1.cmml"><mrow id="S3.E9.m1.6.6.3.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.cmml"><mo mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.cmml">(</mo><mrow id="S3.E9.m1.6.6.3.1.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.cmml"><mrow id="S3.E9.m1.6.6.3.1.1.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.1.cmml"><msub id="S3.E9.m1.6.6.3.1.1.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.1.3.2" xref="S3.E9.m1.6.6.3.1.1.1.1.1.3.2.cmml">E</mi><mrow id="S3.E9.m1.1.1.1" xref="S3.E9.m1.1.1.1.cmml"><msub id="S3.E9.m1.1.1.1.3" xref="S3.E9.m1.1.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E9.m1.1.1.1.3.2" xref="S3.E9.m1.1.1.1.3.2.cmml">x</mi><mi mathcolor="#000000" id="S3.E9.m1.1.1.1.3.3" xref="S3.E9.m1.1.1.1.3.3.cmml">s</mi></msub><mo mathcolor="#000000" id="S3.E9.m1.1.1.1.2" xref="S3.E9.m1.1.1.1.2.cmml">∼</mo><mrow id="S3.E9.m1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E9.m1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E9.m1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.1.2.cmml">X</mi><mi mathcolor="#000000" id="S3.E9.m1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.1.3.cmml">S</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E9.m1.6.6.3.1.1.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">s</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.3.cmml">−</mo><mrow id="S3.E9.m1.6.6.3.1.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.2.cmml"><msub id="S3.E9.m1.6.6.3.1.1.1.1.2.3" xref="S3.E9.m1.6.6.3.1.1.1.1.2.3.cmml"><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.2.3.2" xref="S3.E9.m1.6.6.3.1.1.1.1.2.3.2.cmml">E</mi><mrow id="S3.E9.m1.2.2.1" xref="S3.E9.m1.2.2.1.cmml"><msub id="S3.E9.m1.2.2.1.3" xref="S3.E9.m1.2.2.1.3.cmml"><mi mathcolor="#000000" id="S3.E9.m1.2.2.1.3.2" xref="S3.E9.m1.2.2.1.3.2.cmml">x</mi><mi mathcolor="#000000" id="S3.E9.m1.2.2.1.3.3" xref="S3.E9.m1.2.2.1.3.3.cmml">t</mi></msub><mo mathcolor="#000000" id="S3.E9.m1.2.2.1.2" xref="S3.E9.m1.2.2.1.2.cmml">∼</mo><mrow id="S3.E9.m1.2.2.1.1" xref="S3.E9.m1.2.2.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.2.2.1.1.3" xref="S3.E9.m1.2.2.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.1.1.2" xref="S3.E9.m1.2.2.1.1.2.cmml">​</mo><mrow id="S3.E9.m1.2.2.1.1.1.1" xref="S3.E9.m1.2.2.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.2.2.1.1.1.1.2" xref="S3.E9.m1.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.E9.m1.2.2.1.1.1.1.1" xref="S3.E9.m1.2.2.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.2.2.1.1.1.1.1.2" xref="S3.E9.m1.2.2.1.1.1.1.1.2.cmml">X</mi><mi mathcolor="#000000" id="S3.E9.m1.2.2.1.1.1.1.1.3" xref="S3.E9.m1.2.2.1.1.1.1.1.3.cmml">T</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.2.2.1.1.1.1.3" xref="S3.E9.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E9.m1.6.6.3.1.1.1.1.2.2" xref="S3.E9.m1.6.6.3.1.1.1.1.2.2.cmml">​</mo><mrow id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.2.1.cmml">[</mo><mrow id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.2.cmml">​</mo><mrow id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.2" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.2.cmml">x</mi><mi mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.3.cmml">t</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" stretchy="false" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo mathcolor="#000000" id="S3.E9.m1.6.6.3.1.1.1.3" xref="S3.E9.m1.6.6.3.1.1.1.1.cmml">)</mo></mrow><mn mathcolor="#000000" id="S3.E9.m1.6.6.3.1.3" xref="S3.E9.m1.6.6.3.1.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.6b"><apply id="S3.E9.m1.6.6.cmml" xref="S3.E9.m1.6.6"><eq id="S3.E9.m1.6.6.4.cmml" xref="S3.E9.m1.6.6.4"></eq><apply id="S3.E9.m1.5.5.2.cmml" xref="S3.E9.m1.5.5.2"><times id="S3.E9.m1.5.5.2.3.cmml" xref="S3.E9.m1.5.5.2.3"></times><apply id="S3.E9.m1.5.5.2.4.cmml" xref="S3.E9.m1.5.5.2.4"><csymbol cd="ambiguous" id="S3.E9.m1.5.5.2.4.1.cmml" xref="S3.E9.m1.5.5.2.4">superscript</csymbol><ci id="S3.E9.m1.5.5.2.4.2a.cmml" xref="S3.E9.m1.5.5.2.4.2"><mtext id="S3.E9.m1.5.5.2.4.2.cmml" xref="S3.E9.m1.5.5.2.4.2">MMD</mtext></ci><cn type="integer" id="S3.E9.m1.5.5.2.4.3.cmml" xref="S3.E9.m1.5.5.2.4.3">2</cn></apply><vector id="S3.E9.m1.5.5.2.2.3.cmml" xref="S3.E9.m1.5.5.2.2.2"><ci id="S3.E9.m1.3.3.cmml" xref="S3.E9.m1.3.3">𝐹</ci><apply id="S3.E9.m1.4.4.1.1.1.1.cmml" xref="S3.E9.m1.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.4.4.1.1.1.1.1.cmml" xref="S3.E9.m1.4.4.1.1.1.1">subscript</csymbol><ci id="S3.E9.m1.4.4.1.1.1.1.2.cmml" xref="S3.E9.m1.4.4.1.1.1.1.2">𝑋</ci><ci id="S3.E9.m1.4.4.1.1.1.1.3.cmml" xref="S3.E9.m1.4.4.1.1.1.1.3">𝑆</ci></apply><apply id="S3.E9.m1.5.5.2.2.2.2.cmml" xref="S3.E9.m1.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S3.E9.m1.5.5.2.2.2.2.1.cmml" xref="S3.E9.m1.5.5.2.2.2.2">subscript</csymbol><ci id="S3.E9.m1.5.5.2.2.2.2.2.cmml" xref="S3.E9.m1.5.5.2.2.2.2.2">𝑋</ci><ci id="S3.E9.m1.5.5.2.2.2.2.3.cmml" xref="S3.E9.m1.5.5.2.2.2.2.3">𝑇</ci></apply></vector></apply><apply id="S3.E9.m1.6.6.3.cmml" xref="S3.E9.m1.6.6.3"><apply id="S3.E9.m1.6.6.3.2.cmml" xref="S3.E9.m1.6.6.3.2"><csymbol cd="ambiguous" id="S3.E9.m1.6.6.3.2.1.cmml" xref="S3.E9.m1.6.6.3.2">subscript</csymbol><csymbol cd="latexml" id="S3.E9.m1.6.6.3.2.2.cmml" xref="S3.E9.m1.6.6.3.2.2">supremum</csymbol><apply id="S3.E9.m1.6.6.3.2.3.cmml" xref="S3.E9.m1.6.6.3.2.3"><in id="S3.E9.m1.6.6.3.2.3.1.cmml" xref="S3.E9.m1.6.6.3.2.3.1"></in><ci id="S3.E9.m1.6.6.3.2.3.2.cmml" xref="S3.E9.m1.6.6.3.2.3.2">𝑓</ci><ci id="S3.E9.m1.6.6.3.2.3.3.cmml" xref="S3.E9.m1.6.6.3.2.3.3">𝐹</ci></apply></apply><apply id="S3.E9.m1.6.6.3.1.cmml" xref="S3.E9.m1.6.6.3.1"><csymbol cd="ambiguous" id="S3.E9.m1.6.6.3.1.2.cmml" xref="S3.E9.m1.6.6.3.1">superscript</csymbol><apply id="S3.E9.m1.6.6.3.1.1.1.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1"><minus id="S3.E9.m1.6.6.3.1.1.1.1.3.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.3"></minus><apply id="S3.E9.m1.6.6.3.1.1.1.1.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1"><times id="S3.E9.m1.6.6.3.1.1.1.1.1.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.2"></times><apply id="S3.E9.m1.6.6.3.1.1.1.1.1.3.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.6.6.3.1.1.1.1.1.3.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E9.m1.6.6.3.1.1.1.1.1.3.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.3.2">𝐸</ci><apply id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.1.1"><csymbol cd="latexml" id="S3.E9.m1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.2">similar-to</csymbol><apply id="S3.E9.m1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.1.3">subscript</csymbol><ci id="S3.E9.m1.1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.1.3.2">𝑥</ci><ci id="S3.E9.m1.1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.1.3.3">𝑠</ci></apply><apply id="S3.E9.m1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1"><times id="S3.E9.m1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.2"></times><ci id="S3.E9.m1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.3">𝑃</ci><apply id="S3.E9.m1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E9.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E9.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1.3">𝑆</ci></apply></apply></apply></apply><apply id="S3.E9.m1.6.6.3.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1"><times id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.3">𝑓</ci><apply id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑠</ci></apply></apply></apply></apply><apply id="S3.E9.m1.6.6.3.1.1.1.1.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2"><times id="S3.E9.m1.6.6.3.1.1.1.1.2.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.2"></times><apply id="S3.E9.m1.6.6.3.1.1.1.1.2.3.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E9.m1.6.6.3.1.1.1.1.2.3.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E9.m1.6.6.3.1.1.1.1.2.3.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.3.2">𝐸</ci><apply id="S3.E9.m1.2.2.1.cmml" xref="S3.E9.m1.2.2.1"><csymbol cd="latexml" id="S3.E9.m1.2.2.1.2.cmml" xref="S3.E9.m1.2.2.1.2">similar-to</csymbol><apply id="S3.E9.m1.2.2.1.3.cmml" xref="S3.E9.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.3.1.cmml" xref="S3.E9.m1.2.2.1.3">subscript</csymbol><ci id="S3.E9.m1.2.2.1.3.2.cmml" xref="S3.E9.m1.2.2.1.3.2">𝑥</ci><ci id="S3.E9.m1.2.2.1.3.3.cmml" xref="S3.E9.m1.2.2.1.3.3">𝑡</ci></apply><apply id="S3.E9.m1.2.2.1.1.cmml" xref="S3.E9.m1.2.2.1.1"><times id="S3.E9.m1.2.2.1.1.2.cmml" xref="S3.E9.m1.2.2.1.1.2"></times><ci id="S3.E9.m1.2.2.1.1.3.cmml" xref="S3.E9.m1.2.2.1.1.3">𝑃</ci><apply id="S3.E9.m1.2.2.1.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E9.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E9.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E9.m1.2.2.1.1.1.1.1.2">𝑋</ci><ci id="S3.E9.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E9.m1.2.2.1.1.1.1.1.3">𝑇</ci></apply></apply></apply></apply><apply id="S3.E9.m1.6.6.3.1.1.1.1.2.1.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1"><csymbol cd="latexml" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.2.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.2">delimited-[]</csymbol><apply id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1"><times id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.2"></times><ci id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.3">𝑓</ci><apply id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.6.6.3.1.1.1.1.2.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></apply></apply><cn type="integer" id="S3.E9.m1.6.6.3.1.3.cmml" xref="S3.E9.m1.6.6.3.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.6c">\text{MMD}^{2}(F,X_{S},X_{T})=\sup_{f\in F}\left(E_{x_{s}\sim P(X_{S})}[f(x_{s})]-E_{x_{t}\sim P(X_{T})}[f(x_{t})]\right)^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p id="S3.I1.i1.p1.2" class="ltx_p"><span id="S3.I1.i1.p1.2.1" class="ltx_text" style="color:#000000;">where </span><math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi mathcolor="#000000" id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">F</annotation></semantics></math><span id="S3.I1.i1.p1.2.2" class="ltx_text" style="color:#000000;"> is a class of functions, and </span><math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><mi mathcolor="#000000" id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">E</annotation></semantics></math><span id="S3.I1.i1.p1.2.3" class="ltx_text" style="color:#000000;"> denotes the expectation.</span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.4" class="ltx_p"><span id="S3.I1.i2.p1.4.1" class="ltx_text" style="color:#000000;">Adversarial Training: Adversarial training can be formulated as a min-max problem </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">su2020active</span> </a></cite><span id="S3.I1.i2.p1.4.4" class="ltx_text" style="color:#000000;">:</span></p>
<table id="S3.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E10.m1.5" class="ltx_Math" alttext="\min_{\theta_{f}}\max_{\theta_{d}}\left(E_{x_{s}\sim P(X_{S})}[\log D(x_{s})]+E_{x_{t}\sim P(X_{T})}[\log(1-D(x_{t}))]\right)" display="block"><semantics id="S3.E10.m1.5a"><mrow id="S3.E10.m1.5.5" xref="S3.E10.m1.5.5.cmml"><munder id="S3.E10.m1.5.5.3" xref="S3.E10.m1.5.5.3.cmml"><mi mathcolor="#000000" id="S3.E10.m1.5.5.3.2" xref="S3.E10.m1.5.5.3.2.cmml">min</mi><msub id="S3.E10.m1.5.5.3.3" xref="S3.E10.m1.5.5.3.3.cmml"><mi mathcolor="#000000" id="S3.E10.m1.5.5.3.3.2" xref="S3.E10.m1.5.5.3.3.2.cmml">θ</mi><mi mathcolor="#000000" id="S3.E10.m1.5.5.3.3.3" xref="S3.E10.m1.5.5.3.3.3.cmml">f</mi></msub></munder><mo lspace="0.167em" id="S3.E10.m1.5.5a" xref="S3.E10.m1.5.5.cmml">⁡</mo><mrow id="S3.E10.m1.5.5.2.2" xref="S3.E10.m1.5.5.2.3.cmml"><munder id="S3.E10.m1.4.4.1.1.1" xref="S3.E10.m1.4.4.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E10.m1.4.4.1.1.1.2" xref="S3.E10.m1.4.4.1.1.1.2.cmml">max</mi><msub id="S3.E10.m1.4.4.1.1.1.3" xref="S3.E10.m1.4.4.1.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E10.m1.4.4.1.1.1.3.2" xref="S3.E10.m1.4.4.1.1.1.3.2.cmml">θ</mi><mi mathcolor="#000000" id="S3.E10.m1.4.4.1.1.1.3.3" xref="S3.E10.m1.4.4.1.1.1.3.3.cmml">d</mi></msub></munder><mo id="S3.E10.m1.5.5.2.2a" xref="S3.E10.m1.5.5.2.3.cmml">⁡</mo><mrow id="S3.E10.m1.5.5.2.2.2" xref="S3.E10.m1.5.5.2.3.cmml"><mo mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.2" xref="S3.E10.m1.5.5.2.3.cmml">(</mo><mrow id="S3.E10.m1.5.5.2.2.2.1" xref="S3.E10.m1.5.5.2.2.2.1.cmml"><mrow id="S3.E10.m1.5.5.2.2.2.1.1" xref="S3.E10.m1.5.5.2.2.2.1.1.cmml"><msub id="S3.E10.m1.5.5.2.2.2.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.1.3.2" xref="S3.E10.m1.5.5.2.2.2.1.1.3.2.cmml">E</mi><mrow id="S3.E10.m1.1.1.1" xref="S3.E10.m1.1.1.1.cmml"><msub id="S3.E10.m1.1.1.1.3" xref="S3.E10.m1.1.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E10.m1.1.1.1.3.2" xref="S3.E10.m1.1.1.1.3.2.cmml">x</mi><mi mathcolor="#000000" id="S3.E10.m1.1.1.1.3.3" xref="S3.E10.m1.1.1.1.3.3.cmml">s</mi></msub><mo mathcolor="#000000" id="S3.E10.m1.1.1.1.2" xref="S3.E10.m1.1.1.1.2.cmml">∼</mo><mrow id="S3.E10.m1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E10.m1.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.1.1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E10.m1.1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E10.m1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.1.1.1.2.cmml">X</mi><mi mathcolor="#000000" id="S3.E10.m1.1.1.1.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.1.1.1.3.cmml">S</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.1.1.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E10.m1.5.5.2.2.2.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.1.1.2.1.cmml">[</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.cmml"><mrow id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.1" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3a" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.cmml">⁡</mo><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.2" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.2.cmml">D</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.3.cmml">s</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.3" xref="S3.E10.m1.5.5.2.2.2.1.3.cmml">+</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.2" xref="S3.E10.m1.5.5.2.2.2.1.2.cmml"><msub id="S3.E10.m1.5.5.2.2.2.1.2.3" xref="S3.E10.m1.5.5.2.2.2.1.2.3.cmml"><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.2.3.2" xref="S3.E10.m1.5.5.2.2.2.1.2.3.2.cmml">E</mi><mrow id="S3.E10.m1.2.2.1" xref="S3.E10.m1.2.2.1.cmml"><msub id="S3.E10.m1.2.2.1.3" xref="S3.E10.m1.2.2.1.3.cmml"><mi mathcolor="#000000" id="S3.E10.m1.2.2.1.3.2" xref="S3.E10.m1.2.2.1.3.2.cmml">x</mi><mi mathcolor="#000000" id="S3.E10.m1.2.2.1.3.3" xref="S3.E10.m1.2.2.1.3.3.cmml">t</mi></msub><mo mathcolor="#000000" id="S3.E10.m1.2.2.1.2" xref="S3.E10.m1.2.2.1.2.cmml">∼</mo><mrow id="S3.E10.m1.2.2.1.1" xref="S3.E10.m1.2.2.1.1.cmml"><mi mathcolor="#000000" id="S3.E10.m1.2.2.1.1.3" xref="S3.E10.m1.2.2.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.2.2.1.1.2" xref="S3.E10.m1.2.2.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.2.2.1.1.1.1" xref="S3.E10.m1.2.2.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.2.2.1.1.1.1.2" xref="S3.E10.m1.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.E10.m1.2.2.1.1.1.1.1" xref="S3.E10.m1.2.2.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E10.m1.2.2.1.1.1.1.1.2" xref="S3.E10.m1.2.2.1.1.1.1.1.2.cmml">X</mi><mi mathcolor="#000000" id="S3.E10.m1.2.2.1.1.1.1.1.3" xref="S3.E10.m1.2.2.1.1.1.1.1.3.cmml">T</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.2.2.1.1.1.1.3" xref="S3.E10.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E10.m1.5.5.2.2.2.1.2.2" xref="S3.E10.m1.5.5.2.2.2.1.2.2.cmml">​</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.2.1.1" xref="S3.E10.m1.5.5.2.2.2.1.2.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.2.1.2.1.cmml">[</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.2.cmml"><mi mathcolor="#000000" id="S3.E10.m1.3.3" xref="S3.E10.m1.3.3.cmml">log</mi><mo id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1a" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.2.cmml">⁡</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.2.cmml">(</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.cmml"><mn mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.3.cmml">1</mn><mo mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" stretchy="false" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.3" xref="S3.E10.m1.5.5.2.2.2.1.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo mathcolor="#000000" id="S3.E10.m1.5.5.2.2.2.3" xref="S3.E10.m1.5.5.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.5b"><apply id="S3.E10.m1.5.5.cmml" xref="S3.E10.m1.5.5"><apply id="S3.E10.m1.5.5.3.cmml" xref="S3.E10.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E10.m1.5.5.3.1.cmml" xref="S3.E10.m1.5.5.3">subscript</csymbol><min id="S3.E10.m1.5.5.3.2.cmml" xref="S3.E10.m1.5.5.3.2"></min><apply id="S3.E10.m1.5.5.3.3.cmml" xref="S3.E10.m1.5.5.3.3"><csymbol cd="ambiguous" id="S3.E10.m1.5.5.3.3.1.cmml" xref="S3.E10.m1.5.5.3.3">subscript</csymbol><ci id="S3.E10.m1.5.5.3.3.2.cmml" xref="S3.E10.m1.5.5.3.3.2">𝜃</ci><ci id="S3.E10.m1.5.5.3.3.3.cmml" xref="S3.E10.m1.5.5.3.3.3">𝑓</ci></apply></apply><apply id="S3.E10.m1.5.5.2.3.cmml" xref="S3.E10.m1.5.5.2.2"><apply id="S3.E10.m1.4.4.1.1.1.cmml" xref="S3.E10.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m1.4.4.1.1.1.1.cmml" xref="S3.E10.m1.4.4.1.1.1">subscript</csymbol><max id="S3.E10.m1.4.4.1.1.1.2.cmml" xref="S3.E10.m1.4.4.1.1.1.2"></max><apply id="S3.E10.m1.4.4.1.1.1.3.cmml" xref="S3.E10.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.4.4.1.1.1.3.1.cmml" xref="S3.E10.m1.4.4.1.1.1.3">subscript</csymbol><ci id="S3.E10.m1.4.4.1.1.1.3.2.cmml" xref="S3.E10.m1.4.4.1.1.1.3.2">𝜃</ci><ci id="S3.E10.m1.4.4.1.1.1.3.3.cmml" xref="S3.E10.m1.4.4.1.1.1.3.3">𝑑</ci></apply></apply><apply id="S3.E10.m1.5.5.2.2.2.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1"><plus id="S3.E10.m1.5.5.2.2.2.1.3.cmml" xref="S3.E10.m1.5.5.2.2.2.1.3"></plus><apply id="S3.E10.m1.5.5.2.2.2.1.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1"><times id="S3.E10.m1.5.5.2.2.2.1.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.2"></times><apply id="S3.E10.m1.5.5.2.2.2.1.1.3.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.5.5.2.2.2.1.1.3.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E10.m1.5.5.2.2.2.1.1.3.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.3.2">𝐸</ci><apply id="S3.E10.m1.1.1.1.cmml" xref="S3.E10.m1.1.1.1"><csymbol cd="latexml" id="S3.E10.m1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.2">similar-to</csymbol><apply id="S3.E10.m1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.3.1.cmml" xref="S3.E10.m1.1.1.1.3">subscript</csymbol><ci id="S3.E10.m1.1.1.1.3.2.cmml" xref="S3.E10.m1.1.1.1.3.2">𝑥</ci><ci id="S3.E10.m1.1.1.1.3.3.cmml" xref="S3.E10.m1.1.1.1.3.3">𝑠</ci></apply><apply id="S3.E10.m1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1"><times id="S3.E10.m1.1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.1.2"></times><ci id="S3.E10.m1.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.3">𝑃</ci><apply id="S3.E10.m1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E10.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E10.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1.3">𝑆</ci></apply></apply></apply></apply><apply id="S3.E10.m1.5.5.2.2.2.1.1.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.E10.m1.5.5.2.2.2.1.1.1.2.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1"><times id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.2"></times><apply id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3"><log id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.1"></log><ci id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.3.2">𝐷</ci></apply><apply id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.5.5.2.2.2.1.1.1.1.1.1.1.1.3">𝑠</ci></apply></apply></apply></apply><apply id="S3.E10.m1.5.5.2.2.2.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2"><times id="S3.E10.m1.5.5.2.2.2.1.2.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.2"></times><apply id="S3.E10.m1.5.5.2.2.2.1.2.3.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.3"><csymbol cd="ambiguous" id="S3.E10.m1.5.5.2.2.2.1.2.3.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.3">subscript</csymbol><ci id="S3.E10.m1.5.5.2.2.2.1.2.3.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.3.2">𝐸</ci><apply id="S3.E10.m1.2.2.1.cmml" xref="S3.E10.m1.2.2.1"><csymbol cd="latexml" id="S3.E10.m1.2.2.1.2.cmml" xref="S3.E10.m1.2.2.1.2">similar-to</csymbol><apply id="S3.E10.m1.2.2.1.3.cmml" xref="S3.E10.m1.2.2.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.2.2.1.3.1.cmml" xref="S3.E10.m1.2.2.1.3">subscript</csymbol><ci id="S3.E10.m1.2.2.1.3.2.cmml" xref="S3.E10.m1.2.2.1.3.2">𝑥</ci><ci id="S3.E10.m1.2.2.1.3.3.cmml" xref="S3.E10.m1.2.2.1.3.3">𝑡</ci></apply><apply id="S3.E10.m1.2.2.1.1.cmml" xref="S3.E10.m1.2.2.1.1"><times id="S3.E10.m1.2.2.1.1.2.cmml" xref="S3.E10.m1.2.2.1.1.2"></times><ci id="S3.E10.m1.2.2.1.1.3.cmml" xref="S3.E10.m1.2.2.1.1.3">𝑃</ci><apply id="S3.E10.m1.2.2.1.1.1.1.1.cmml" xref="S3.E10.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E10.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E10.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E10.m1.2.2.1.1.1.1.1.2">𝑋</ci><ci id="S3.E10.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E10.m1.2.2.1.1.1.1.1.3">𝑇</ci></apply></apply></apply></apply><apply id="S3.E10.m1.5.5.2.2.2.1.2.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1"><csymbol cd="latexml" id="S3.E10.m1.5.5.2.2.2.1.2.1.2.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.2">delimited-[]</csymbol><apply id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1"><log id="S3.E10.m1.3.3.cmml" xref="S3.E10.m1.3.3"></log><apply id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1"><minus id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.2"></minus><cn type="integer" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.3">1</cn><apply id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1"><times id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.3">𝐷</ci><apply id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.5.5.2.2.2.1.2.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.5c">\min_{\theta_{f}}\max_{\theta_{d}}\left(E_{x_{s}\sim P(X_{S})}[\log D(x_{s})]+E_{x_{t}\sim P(X_{T})}[\log(1-D(x_{t}))]\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S3.I1.i2.p1.3" class="ltx_p"><span id="S3.I1.i2.p1.3.1" class="ltx_text" style="color:#000000;">where </span><math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi mathcolor="#000000" id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">D</annotation></semantics></math><span id="S3.I1.i2.p1.3.2" class="ltx_text" style="color:#000000;"> is the domain classifier, </span><math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\theta_{f}" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><msub id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S3.I1.i2.p1.2.m2.1.1.2" xref="S3.I1.i2.p1.2.m2.1.1.2.cmml">θ</mi><mi mathcolor="#000000" id="S3.I1.i2.p1.2.m2.1.1.3" xref="S3.I1.i2.p1.2.m2.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><apply id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.2.m2.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.1.1.2">𝜃</ci><ci id="S3.I1.i2.p1.2.m2.1.1.3.cmml" xref="S3.I1.i2.p1.2.m2.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">\theta_{f}</annotation></semantics></math><span id="S3.I1.i2.p1.3.3" class="ltx_text" style="color:#000000;"> are the parameters of the feature extractor, and </span><math id="S3.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="\theta_{d}" display="inline"><semantics id="S3.I1.i2.p1.3.m3.1a"><msub id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><mi mathcolor="#000000" id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml">θ</mi><mi mathcolor="#000000" id="S3.I1.i2.p1.3.m3.1.1.3" xref="S3.I1.i2.p1.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">𝜃</ci><ci id="S3.I1.i2.p1.3.m3.1.1.3.cmml" xref="S3.I1.i2.p1.3.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">\theta_{d}</annotation></semantics></math><span id="S3.I1.i2.p1.3.4" class="ltx_text" style="color:#000000;"> are the parameters of the domain classifier.</span></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text" style="color:#000000;">Covariate Shift Adjustment: Covariate shift adjustment involves re-weighting the loss function on the source domain samples as </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhou2021bayesian</span> </a></cite><span id="S3.I1.i3.p1.1.4" class="ltx_text" style="color:#000000;">:</span></p>
<table id="S3.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E11.m1.3" class="ltx_Math" alttext="w(x)=\frac{P(X_{T}=x)}{P(X_{S}=x)}" display="block"><semantics id="S3.E11.m1.3a"><mrow id="S3.E11.m1.3.4" xref="S3.E11.m1.3.4.cmml"><mrow id="S3.E11.m1.3.4.2" xref="S3.E11.m1.3.4.2.cmml"><mi mathcolor="#000000" id="S3.E11.m1.3.4.2.2" xref="S3.E11.m1.3.4.2.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.3.4.2.1" xref="S3.E11.m1.3.4.2.1.cmml">​</mo><mrow id="S3.E11.m1.3.4.2.3.2" xref="S3.E11.m1.3.4.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E11.m1.3.4.2.3.2.1" xref="S3.E11.m1.3.4.2.cmml">(</mo><mi mathcolor="#000000" id="S3.E11.m1.3.3" xref="S3.E11.m1.3.3.cmml">x</mi><mo mathcolor="#000000" stretchy="false" id="S3.E11.m1.3.4.2.3.2.2" xref="S3.E11.m1.3.4.2.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" id="S3.E11.m1.3.4.1" xref="S3.E11.m1.3.4.1.cmml">=</mo><mfrac mathcolor="#000000" id="S3.E11.m1.2.2" xref="S3.E11.m1.2.2.cmml"><mrow id="S3.E11.m1.1.1.1" xref="S3.E11.m1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.E11.m1.1.1.1.3" xref="S3.E11.m1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.1.1.1.2" xref="S3.E11.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E11.m1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E11.m1.1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E11.m1.1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.cmml"><msub id="S3.E11.m1.1.1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.1.1.2.cmml"><mi mathcolor="#000000" id="S3.E11.m1.1.1.1.1.1.1.2.2" xref="S3.E11.m1.1.1.1.1.1.1.2.2.cmml">X</mi><mi mathcolor="#000000" id="S3.E11.m1.1.1.1.1.1.1.2.3" xref="S3.E11.m1.1.1.1.1.1.1.2.3.cmml">T</mi></msub><mo mathcolor="#000000" id="S3.E11.m1.1.1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.1.cmml">=</mo><mi mathcolor="#000000" id="S3.E11.m1.1.1.1.1.1.1.3" xref="S3.E11.m1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo mathcolor="#000000" stretchy="false" id="S3.E11.m1.1.1.1.1.1.3" xref="S3.E11.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E11.m1.2.2.2" xref="S3.E11.m1.2.2.2.cmml"><mi mathcolor="#000000" id="S3.E11.m1.2.2.2.3" xref="S3.E11.m1.2.2.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.2.2.2.2" xref="S3.E11.m1.2.2.2.2.cmml">​</mo><mrow id="S3.E11.m1.2.2.2.1.1" xref="S3.E11.m1.2.2.2.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E11.m1.2.2.2.1.1.2" xref="S3.E11.m1.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E11.m1.2.2.2.1.1.1" xref="S3.E11.m1.2.2.2.1.1.1.cmml"><msub id="S3.E11.m1.2.2.2.1.1.1.2" xref="S3.E11.m1.2.2.2.1.1.1.2.cmml"><mi mathcolor="#000000" id="S3.E11.m1.2.2.2.1.1.1.2.2" xref="S3.E11.m1.2.2.2.1.1.1.2.2.cmml">X</mi><mi mathcolor="#000000" id="S3.E11.m1.2.2.2.1.1.1.2.3" xref="S3.E11.m1.2.2.2.1.1.1.2.3.cmml">S</mi></msub><mo mathcolor="#000000" id="S3.E11.m1.2.2.2.1.1.1.1" xref="S3.E11.m1.2.2.2.1.1.1.1.cmml">=</mo><mi mathcolor="#000000" id="S3.E11.m1.2.2.2.1.1.1.3" xref="S3.E11.m1.2.2.2.1.1.1.3.cmml">x</mi></mrow><mo mathcolor="#000000" stretchy="false" id="S3.E11.m1.2.2.2.1.1.3" xref="S3.E11.m1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E11.m1.3b"><apply id="S3.E11.m1.3.4.cmml" xref="S3.E11.m1.3.4"><eq id="S3.E11.m1.3.4.1.cmml" xref="S3.E11.m1.3.4.1"></eq><apply id="S3.E11.m1.3.4.2.cmml" xref="S3.E11.m1.3.4.2"><times id="S3.E11.m1.3.4.2.1.cmml" xref="S3.E11.m1.3.4.2.1"></times><ci id="S3.E11.m1.3.4.2.2.cmml" xref="S3.E11.m1.3.4.2.2">𝑤</ci><ci id="S3.E11.m1.3.3.cmml" xref="S3.E11.m1.3.3">𝑥</ci></apply><apply id="S3.E11.m1.2.2.cmml" xref="S3.E11.m1.2.2"><divide id="S3.E11.m1.2.2.3.cmml" xref="S3.E11.m1.2.2"></divide><apply id="S3.E11.m1.1.1.1.cmml" xref="S3.E11.m1.1.1.1"><times id="S3.E11.m1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.2"></times><ci id="S3.E11.m1.1.1.1.3.cmml" xref="S3.E11.m1.1.1.1.3">𝑃</ci><apply id="S3.E11.m1.1.1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1.1"><eq id="S3.E11.m1.1.1.1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1"></eq><apply id="S3.E11.m1.1.1.1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E11.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E11.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2.2">𝑋</ci><ci id="S3.E11.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2.3">𝑇</ci></apply><ci id="S3.E11.m1.1.1.1.1.1.1.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.3">𝑥</ci></apply></apply><apply id="S3.E11.m1.2.2.2.cmml" xref="S3.E11.m1.2.2.2"><times id="S3.E11.m1.2.2.2.2.cmml" xref="S3.E11.m1.2.2.2.2"></times><ci id="S3.E11.m1.2.2.2.3.cmml" xref="S3.E11.m1.2.2.2.3">𝑃</ci><apply id="S3.E11.m1.2.2.2.1.1.1.cmml" xref="S3.E11.m1.2.2.2.1.1"><eq id="S3.E11.m1.2.2.2.1.1.1.1.cmml" xref="S3.E11.m1.2.2.2.1.1.1.1"></eq><apply id="S3.E11.m1.2.2.2.1.1.1.2.cmml" xref="S3.E11.m1.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E11.m1.2.2.2.1.1.1.2.1.cmml" xref="S3.E11.m1.2.2.2.1.1.1.2">subscript</csymbol><ci id="S3.E11.m1.2.2.2.1.1.1.2.2.cmml" xref="S3.E11.m1.2.2.2.1.1.1.2.2">𝑋</ci><ci id="S3.E11.m1.2.2.2.1.1.1.2.3.cmml" xref="S3.E11.m1.2.2.2.1.1.1.2.3">𝑆</ci></apply><ci id="S3.E11.m1.2.2.2.1.1.1.3.cmml" xref="S3.E11.m1.2.2.2.1.1.1.3">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E11.m1.3c">w(x)=\frac{P(X_{T}=x)}{P(X_{S}=x)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p id="S3.I1.i3.p1.2" class="ltx_p"><span id="S3.I1.i3.p1.2.1" class="ltx_text" style="color:#000000;">and modifying the source domain loss function accordingly.</span></p>
</div>
</li>
</ul>
<p id="S3.SS1.SSS2.p2.1" class="ltx_p"><span id="S3.SS1.SSS2.p2.1.1" class="ltx_text" style="color:#000000;">The optimization problem typically involves minimizing a combined loss function that includes the predictive loss on the source domain, the domain discrepancy measure, and the predictive loss on the target domain (if labeled data is available).</span></p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2405.20126/assets/x1.png" id="S3.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="202" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.7.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.8.2" class="ltx_text" style="font-size:90%;">A comprehensive examination of different types and subtypes of TL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2019federated</span> </a></cite></span></figcaption>
</figure>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Knowledge Distillation</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p"><span id="S3.SS1.SSS3.p1.1.1" class="ltx_text" style="color:#000000;">Knowledge distillation transfers knowledge from a complex model (teacher) to a simpler model (student), aiming to retain much of the teacher’s performance.</span></p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<table id="S3.E12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E12.m1.1" class="ltx_Math" alttext="L=(1-\alpha)L_{\text{hard}}+\alpha T^{2}L_{\text{soft}}" display="block"><semantics id="S3.E12.m1.1a"><mrow id="S3.E12.m1.1.1" xref="S3.E12.m1.1.1.cmml"><mi mathcolor="#000000" id="S3.E12.m1.1.1.3" xref="S3.E12.m1.1.1.3.cmml">L</mi><mo mathcolor="#000000" id="S3.E12.m1.1.1.2" xref="S3.E12.m1.1.1.2.cmml">=</mo><mrow id="S3.E12.m1.1.1.1" xref="S3.E12.m1.1.1.1.cmml"><mrow id="S3.E12.m1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.cmml"><mrow id="S3.E12.m1.1.1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.E12.m1.1.1.1.1.1.1.2" xref="S3.E12.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E12.m1.1.1.1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.1.1.1.cmml"><mn mathcolor="#000000" id="S3.E12.m1.1.1.1.1.1.1.1.2" xref="S3.E12.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo mathcolor="#000000" id="S3.E12.m1.1.1.1.1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi mathcolor="#000000" id="S3.E12.m1.1.1.1.1.1.1.1.3" xref="S3.E12.m1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo mathcolor="#000000" stretchy="false" id="S3.E12.m1.1.1.1.1.1.1.3" xref="S3.E12.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.2" xref="S3.E12.m1.1.1.1.1.2.cmml">​</mo><msub id="S3.E12.m1.1.1.1.1.3" xref="S3.E12.m1.1.1.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E12.m1.1.1.1.1.3.2" xref="S3.E12.m1.1.1.1.1.3.2.cmml">L</mi><mtext mathcolor="#000000" id="S3.E12.m1.1.1.1.1.3.3" xref="S3.E12.m1.1.1.1.1.3.3a.cmml">hard</mtext></msub></mrow><mo mathcolor="#000000" id="S3.E12.m1.1.1.1.2" xref="S3.E12.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E12.m1.1.1.1.3" xref="S3.E12.m1.1.1.1.3.cmml"><mi mathcolor="#000000" id="S3.E12.m1.1.1.1.3.2" xref="S3.E12.m1.1.1.1.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.3.1" xref="S3.E12.m1.1.1.1.3.1.cmml">​</mo><msup id="S3.E12.m1.1.1.1.3.3" xref="S3.E12.m1.1.1.1.3.3.cmml"><mi mathcolor="#000000" id="S3.E12.m1.1.1.1.3.3.2" xref="S3.E12.m1.1.1.1.3.3.2.cmml">T</mi><mn mathcolor="#000000" id="S3.E12.m1.1.1.1.3.3.3" xref="S3.E12.m1.1.1.1.3.3.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.3.1a" xref="S3.E12.m1.1.1.1.3.1.cmml">​</mo><msub id="S3.E12.m1.1.1.1.3.4" xref="S3.E12.m1.1.1.1.3.4.cmml"><mi mathcolor="#000000" id="S3.E12.m1.1.1.1.3.4.2" xref="S3.E12.m1.1.1.1.3.4.2.cmml">L</mi><mtext mathcolor="#000000" id="S3.E12.m1.1.1.1.3.4.3" xref="S3.E12.m1.1.1.1.3.4.3a.cmml">soft</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E12.m1.1b"><apply id="S3.E12.m1.1.1.cmml" xref="S3.E12.m1.1.1"><eq id="S3.E12.m1.1.1.2.cmml" xref="S3.E12.m1.1.1.2"></eq><ci id="S3.E12.m1.1.1.3.cmml" xref="S3.E12.m1.1.1.3">𝐿</ci><apply id="S3.E12.m1.1.1.1.cmml" xref="S3.E12.m1.1.1.1"><plus id="S3.E12.m1.1.1.1.2.cmml" xref="S3.E12.m1.1.1.1.2"></plus><apply id="S3.E12.m1.1.1.1.1.cmml" xref="S3.E12.m1.1.1.1.1"><times id="S3.E12.m1.1.1.1.1.2.cmml" xref="S3.E12.m1.1.1.1.1.2"></times><apply id="S3.E12.m1.1.1.1.1.1.1.1.cmml" xref="S3.E12.m1.1.1.1.1.1.1"><minus id="S3.E12.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E12.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E12.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E12.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E12.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E12.m1.1.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="S3.E12.m1.1.1.1.1.3.cmml" xref="S3.E12.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E12.m1.1.1.1.1.3.1.cmml" xref="S3.E12.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E12.m1.1.1.1.1.3.2.cmml" xref="S3.E12.m1.1.1.1.1.3.2">𝐿</ci><ci id="S3.E12.m1.1.1.1.1.3.3a.cmml" xref="S3.E12.m1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E12.m1.1.1.1.1.3.3.cmml" xref="S3.E12.m1.1.1.1.1.3.3">hard</mtext></ci></apply></apply><apply id="S3.E12.m1.1.1.1.3.cmml" xref="S3.E12.m1.1.1.1.3"><times id="S3.E12.m1.1.1.1.3.1.cmml" xref="S3.E12.m1.1.1.1.3.1"></times><ci id="S3.E12.m1.1.1.1.3.2.cmml" xref="S3.E12.m1.1.1.1.3.2">𝛼</ci><apply id="S3.E12.m1.1.1.1.3.3.cmml" xref="S3.E12.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E12.m1.1.1.1.3.3.1.cmml" xref="S3.E12.m1.1.1.1.3.3">superscript</csymbol><ci id="S3.E12.m1.1.1.1.3.3.2.cmml" xref="S3.E12.m1.1.1.1.3.3.2">𝑇</ci><cn type="integer" id="S3.E12.m1.1.1.1.3.3.3.cmml" xref="S3.E12.m1.1.1.1.3.3.3">2</cn></apply><apply id="S3.E12.m1.1.1.1.3.4.cmml" xref="S3.E12.m1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E12.m1.1.1.1.3.4.1.cmml" xref="S3.E12.m1.1.1.1.3.4">subscript</csymbol><ci id="S3.E12.m1.1.1.1.3.4.2.cmml" xref="S3.E12.m1.1.1.1.3.4.2">𝐿</ci><ci id="S3.E12.m1.1.1.1.3.4.3a.cmml" xref="S3.E12.m1.1.1.1.3.4.3"><mtext mathsize="70%" id="S3.E12.m1.1.1.1.3.4.3.cmml" xref="S3.E12.m1.1.1.1.3.4.3">soft</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E12.m1.1c">L=(1-\alpha)L_{\text{hard}}+\alpha T^{2}L_{\text{soft}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS3.p2.4" class="ltx_p"><span id="S3.SS1.SSS3.p2.4.1" class="ltx_text" style="color:#000000;">where </span><math id="S3.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="L_{\text{hard}}" display="inline"><semantics id="S3.SS1.SSS3.p2.1.m1.1a"><msub id="S3.SS1.SSS3.p2.1.m1.1.1" xref="S3.SS1.SSS3.p2.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.SSS3.p2.1.m1.1.1.2" xref="S3.SS1.SSS3.p2.1.m1.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S3.SS1.SSS3.p2.1.m1.1.1.3" xref="S3.SS1.SSS3.p2.1.m1.1.1.3a.cmml">hard</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.1.m1.1b"><apply id="S3.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.2">𝐿</ci><ci id="S3.SS1.SSS3.p2.1.m1.1.1.3a.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS3.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.3">hard</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.1.m1.1c">L_{\text{hard}}</annotation></semantics></math><span id="S3.SS1.SSS3.p2.4.2" class="ltx_text" style="color:#000000;"> is the traditional loss (e.g., cross-entropy against true labels), </span><math id="S3.SS1.SSS3.p2.2.m2.1" class="ltx_Math" alttext="L_{\text{soft}}" display="inline"><semantics id="S3.SS1.SSS3.p2.2.m2.1a"><msub id="S3.SS1.SSS3.p2.2.m2.1.1" xref="S3.SS1.SSS3.p2.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.SSS3.p2.2.m2.1.1.2" xref="S3.SS1.SSS3.p2.2.m2.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S3.SS1.SSS3.p2.2.m2.1.1.3" xref="S3.SS1.SSS3.p2.2.m2.1.1.3a.cmml">soft</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.2.m2.1b"><apply id="S3.SS1.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.2">𝐿</ci><ci id="S3.SS1.SSS3.p2.2.m2.1.1.3a.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS3.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.3">soft</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.2.m2.1c">L_{\text{soft}}</annotation></semantics></math><span id="S3.SS1.SSS3.p2.4.3" class="ltx_text" style="color:#000000;"> is the distillation loss (e.g., KL divergence between the teacher’s and student’s predictions), </span><math id="S3.SS1.SSS3.p2.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.SSS3.p2.3.m3.1a"><mi mathcolor="#000000" id="S3.SS1.SSS3.p2.3.m3.1.1" xref="S3.SS1.SSS3.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.3.m3.1b"><ci id="S3.SS1.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.3.m3.1c">\alpha</annotation></semantics></math><span id="S3.SS1.SSS3.p2.4.4" class="ltx_text" style="color:#000000;"> is a hyperparameter balancing the two losses, and </span><math id="S3.SS1.SSS3.p2.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.SSS3.p2.4.m4.1a"><mi mathcolor="#000000" id="S3.SS1.SSS3.p2.4.m4.1.1" xref="S3.SS1.SSS3.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.4.m4.1b"><ci id="S3.SS1.SSS3.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p2.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.4.m4.1c">T</annotation></semantics></math><span id="S3.SS1.SSS3.p2.4.5" class="ltx_text" style="color:#000000;"> is the temperature scaling the logits before applying softmax.</span></p>
</div>
<div id="S3.SS1.SSS3.p3" class="ltx_para">
<p id="S3.SS1.SSS3.p3.1" class="ltx_p"><span id="S3.SS1.SSS3.p3.1.1" class="ltx_text" style="color:#000000;">Overall, TL provides a broad framework for leveraging knowledge from related tasks while Fine-Tuning focuses on adjustments to a pre-trained model for a new task. On the other hand, domain adaptation addresses the discrepancy between source and target domain distributions. Moving on, knowledge distillation focuses on transferring knowledge from a complex model to a simpler one.</span></p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Advantages of using transfer learning for CD based on image analysis</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="color:#000000;">In medical image analysis, TL has emerged as one of the most important techniques to improve the quality and the performances of DL models, and it comes with other advantages such as:
TL saves time by using a pre-trained system to learn new data. Instead of training a model from the ground up, which can be time-consuming and computationally expensive, as TL allows reusing pre-trained models that have already learned useful features. Reducing training time is very important, especially because many hospitals possess limited resources to train these models. TL emerges as an efficient way to resolve this major issue </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhu2022super</span> </a></cite><span id="S3.SS2.p1.1.4" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text" style="color:#000000;">Moving on, TL can improve medical image analysis by using pre-trained models on large datasets like convolutional neural network (CNN) models such as VGG16 and ResNet-50 to extract features from medical images and improve classification accuracy. It can also reduce the need for large amounts of labeled data, which can be time-consuming and expensive to obtain because it is a difficult task to find hospitals that possess labeled and clean datasets that help improve the quality of the prediction and help the model to extract insights and anomalies from this data.</span></p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text" style="color:#000000;">General applicability and diversity are crucial in medical image analysis since it is frequently challenging to collect extensive and diverse datasets that accurately reflect the spectrum of variability in actual clinical situations. TL can improve the generalization performance of models on new and unknown data by utilizing pre-trained models that have already learned useful features from substantial and varied datasets. Additionally, TL can help overcome issues with overfitting, which is essential in improving the diversity of the model and the performance </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">morid2021scoping</span> </a></cite><span id="S3.SS2.p3.1.4" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Comparison of FL and TL in CD based on image analysis</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text" style="color:#000000;">FL and TL have complementary strengths and limitations, FL improves the scalability and efficiency of ML systems, protects privacy, and is adapted to different situations. However, to create a more robust model, datasets from different hospitals in different locations are required and there are hardware limitations, communication, scheduling, and data distribution issues. TL improves the accuracy and sensitivity of ML models in detecting cancer, reduces the dependence of ML models on labeled training sets, and improves the quality and precision of anomaly detection models based on clinical data. However, the challenge is to select the most suitable model for a specific healthcare use case. Finally, it is noted that FL and TL can be used together, where FL has enabled distributed private training while TL has improved performance through the transfer of knowledge and learned functions. The choice depends on several factors: specific use case, data availability, privacy requirements, problem similarity, and computational limitations. Table </span><a href="#S4.T2" title="Table 2 ‣ 1st item ‣ item C1. ‣ 4.1.1 Classification (C) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS3.p1.1.2" class="ltx_text" style="color:#000000;"> shows the key differences between FL and TL based on several factors.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">4 </span>Federated Learning vs Transfer Learning</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overview</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="color:#000000;">This section provides a visual taxonomy of the key components of federated and TL techniques for CD based on medical image analysis. Fig. </span><a href="#S4.F2" title="Figure 2 ‣ 1st item ‣ item C1. ‣ 4.1.1 Classification (C) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS1.p1.1.2" class="ltx_text" style="color:#000000;"> provides a visual representation of the taxonomy that categorizes these methods into different sections to compare their role in improving cancer diagnosis while maintaining privacy. Table </span><a href="#S4.T3" title="Table 3 ‣ 1st item ‣ item C1. ‣ 4.1.1 Classification (C) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS1.p1.1.3" class="ltx_text" style="color:#000000;"> summarizes research studies using FL and TL for CD through image analysis. The studies are compared based on parameters such as cancer type, DL model, learning method, aggregation strategy, dataset and best performance.</span></p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Classification (C)</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p"><span id="S4.SS1.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">This classification provides a general overview of the different categories and techniques for TL and FL applied in CD tasks:</span></p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i1.1.1.1" class="ltx_text ltx_font_bold">C1.</span></span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Classification of TL: </span><span id="S4.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;">
This section provides an overview of the different classes and types of TL approaches and techniques developed to address the challenges of knowledge transfer from one domain to another:</span></p>
<ul id="S4.I1.i1.I1" class="ltx_itemize">
<li id="S4.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Data homogeneity: </span><span id="S4.I1.i1.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;">is very important in generating high quality and performant models. The similarity of data distributions between the source domain and the target domain allows increasing the efficiency of TL, especially in healthcare where the data is complex. These are types of data homogeneity:</span></p>
</div>
<div id="S4.I1.i1.I1.i1.p2" class="ltx_para">
<ul id="S4.I1.i1.I1.i1.I1" class="ltx_itemize">
<li id="S4.I1.i1.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i1.I1.i1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i1.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Homogenous Data: </span><span id="S4.I1.i1.I1.i1.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;">Homogeneous datasets can be beneficial for TL as they contain similar data from the same domain, which improved the efficiency and quality of the model. Chui et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chui2023facilitating</span> </a></cite><span id="S4.I1.i1.I1.i1.I1.i1.p1.1.5" class="ltx_text" style="color:#000000;"> points out that homogeneous medical datasets may not be available or provide enough diversity to build robust models, which is why TL with heterogeneous datasets has become an emerging research initiative.</span></p>
</div>
</li>
<li id="S4.I1.i1.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i1.I1.i1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i1.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Heterogeneous data: </span><span id="S4.I1.i1.I1.i1.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;">The presence of dissimilarity in data from different domains poses a challenge for TL when using heterogeneous datasets. This dissimilarity can lead to negative transfer, where the performance of the target model decreases after TL. Many researches have been conducted to solve this problem, such as Chui et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chui2023facilitating</span> </a></cite><span id="S4.I1.i1.I1.i1.I1.i2.p1.1.5" class="ltx_text" style="color:#000000;"> proposed a generic incremental TL approach to address the challenges of TL with heterogeneous datasets. This approach can be useful for small datasets and for applications that may not have similar datasets.</span></p>
</div>
</li>
</ul>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2405.20126/assets/x2.png" id="S4.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="326" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.6.2" class="ltx_text" style="font-size:90%;">Taxonomy of Federated Transfer Learning based on Image Analysis</span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text" style="font-size:90%;">Key differences between FL and TL for CD</span></figcaption>
<table id="S4.T2.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.6.1" class="ltx_tr">
<td id="S4.T2.6.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T2.6.1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Key Differences</span></td>
<td id="S4.T2.6.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.1.2.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.1.2.1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Federated Learning</span></span>
</span>
</td>
<td id="S4.T2.6.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.1.3.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.1.3.1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Transfer Learning</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.2" class="ltx_tr">
<td id="S4.T2.6.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T2.6.2.1.1" class="ltx_text" style="color:#000000;">Technique</span></td>
<td id="S4.T2.6.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.2.2.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.2.2.1.1.1" class="ltx_text" style="color:#000000;">Decentralized ML approach that allows multiple parties to collaboratively train a model without sharing sensitive medical data, ensuring data security and privacy.</span></span>
</span>
</td>
<td id="S4.T2.6.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.2.3.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.2.3.1.1.1" class="ltx_text" style="color:#000000;">Transfer knowledge from multiple related datasets, reducing an ML model’s dependence on labeled training sets.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.3" class="ltx_tr">
<td id="S4.T2.6.3.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T2.6.3.1.1" class="ltx_text" style="color:#000000;">Data requirements</span></td>
<td id="S4.T2.6.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.3.2.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.3.2.1.1.1" class="ltx_text" style="color:#000000;">Requires datasets from various hospitals in different locations to build a more robust model.</span></span>
</span>
</td>
<td id="S4.T2.6.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.3.3.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.3.3.1.1.1" class="ltx_text" style="color:#000000;">Allows for the transfer of knowledge from one task to another.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.4" class="ltx_tr">
<td id="S4.T2.6.4.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T2.6.4.1.1" class="ltx_text" style="color:#000000;">Use cases</span></td>
<td id="S4.T2.6.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.4.2.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.4.2.1.1.1" class="ltx_text" style="color:#000000;">Used to improve the quality and performance of anomaly detection models while maintaining user privacy.</span></span>
</span>
</td>
<td id="S4.T2.6.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.4.3.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.4.3.1.1.1" class="ltx_text" style="color:#000000;">Used to improve the quality and precision of an anomaly detection model and reduce the computing resources required during the model training phase.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.5" class="ltx_tr">
<td id="S4.T2.6.5.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T2.6.5.1.1" class="ltx_text" style="color:#000000;">Performance</span></td>
<td id="S4.T2.6.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.5.2.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.5.2.1.1.1" class="ltx_text" style="color:#000000;">Improves the scalability and efficiency of ML systems.</span></span>
</span>
</td>
<td id="S4.T2.6.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.5.3.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.5.3.1.1.1" class="ltx_text" style="color:#000000;">Improves the accuracy and sensitivity of ML models in CD.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.6" class="ltx_tr">
<td id="S4.T2.6.6.1" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T2.6.6.1.1" class="ltx_text" style="color:#000000;">Privacy</span></td>
<td id="S4.T2.6.6.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.2.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.6.2.1.1.1" class="ltx_text" style="color:#000000;">Protect privacy by allowing hospitals and medical patients not to send or share their sensitive information.</span></span>
</span>
</td>
<td id="S4.T2.6.6.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.6.3.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.6.3.1.1.1" class="ltx_text" style="color:#000000;">Reduces the dependence of ML models on labeled training sets, thereby reducing the risk of privacy leaks.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.6.7" class="ltx_tr">
<td id="S4.T2.6.7.1" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T2.6.7.1.1" class="ltx_text" style="color:#000000;">Challenges</span></td>
<td id="S4.T2.6.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.7.2.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.7.2.1.1.1" class="ltx_text" style="color:#000000;">Hardware limitations, communication and scheduling problems, and data distribution.</span></span>
</span>
</td>
<td id="S4.T2.6.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.6.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.6.7.3.1.1" class="ltx_p" style="width:179.3pt;"><span id="S4.T2.6.7.3.1.1.1" class="ltx_text" style="color:#000000;">Depends on a large set of labeled training data and determining which model is best suited for a specific healthcare use case.</span></span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text" style="font-size:90%;">Summary of research studies conducted in FL and TL on CD based on image analysis</span></figcaption>
<table id="S4.T3.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.6.1" class="ltx_tr">
<td id="S4.T3.6.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Ref</span></td>
<td id="S4.T3.6.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.1.2.1" class="ltx_text"></span><span id="S4.T3.6.1.2.2" class="ltx_text ltx_font_bold" style="color:#000000;"> <span id="S4.T3.6.1.2.2.1" class="ltx_text">
<span id="S4.T3.6.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.1.2.2.1.1.1" class="ltx_tr">
<span id="S4.T3.6.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Cancer</span></span>
<span id="S4.T3.6.1.2.2.1.1.2" class="ltx_tr">
<span id="S4.T3.6.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Type</span></span>
</span></span><span id="S4.T3.6.1.2.2.2" class="ltx_text"></span></span>
</td>
<td id="S4.T3.6.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.1.3.1" class="ltx_text ltx_font_bold" style="color:#000000;">DL Model</span></td>
<td id="S4.T3.6.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.1.4.1" class="ltx_text"></span><span id="S4.T3.6.1.4.2" class="ltx_text ltx_font_bold" style="color:#000000;"> <span id="S4.T3.6.1.4.2.1" class="ltx_text">
<span id="S4.T3.6.1.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.1.4.2.1.1.1" class="ltx_tr">
<span id="S4.T3.6.1.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Learning</span></span>
<span id="S4.T3.6.1.4.2.1.1.2" class="ltx_tr">
<span id="S4.T3.6.1.4.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Type</span></span>
</span></span><span id="S4.T3.6.1.4.2.2" class="ltx_text"></span></span>
</td>
<td id="S4.T3.6.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.1.5.1" class="ltx_text"></span><span id="S4.T3.6.1.5.2" class="ltx_text ltx_font_bold" style="color:#000000;"> <span id="S4.T3.6.1.5.2.1" class="ltx_text">
<span id="S4.T3.6.1.5.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.1.5.2.1.1.1" class="ltx_tr">
<span id="S4.T3.6.1.5.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Method</span></span>
</span></span><span id="S4.T3.6.1.5.2.2" class="ltx_text"></span></span>
</td>
<td id="S4.T3.6.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.1.6.1" class="ltx_text"></span><span id="S4.T3.6.1.6.2" class="ltx_text ltx_font_bold" style="color:#000000;"> <span id="S4.T3.6.1.6.2.1" class="ltx_text">
<span id="S4.T3.6.1.6.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.1.6.2.1.1.1" class="ltx_tr">
<span id="S4.T3.6.1.6.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Aggregation</span></span>
<span id="S4.T3.6.1.6.2.1.1.2" class="ltx_tr">
<span id="S4.T3.6.1.6.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Strategy</span></span>
</span></span><span id="S4.T3.6.1.6.2.2" class="ltx_text"></span></span>
</td>
<td id="S4.T3.6.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.1.7.1" class="ltx_text ltx_font_bold" style="color:#000000;">Dataset</span></td>
<td id="S4.T3.6.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.1.8.1" class="ltx_text"></span><span id="S4.T3.6.1.8.2" class="ltx_text ltx_font_bold" style="color:#000000;"> <span id="S4.T3.6.1.8.2.1" class="ltx_text">
<span id="S4.T3.6.1.8.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.1.8.2.1.1.1" class="ltx_tr">
<span id="S4.T3.6.1.8.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Best</span></span>
<span id="S4.T3.6.1.8.2.1.1.2" class="ltx_tr">
<span id="S4.T3.6.1.8.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Performance</span></span>
</span></span><span id="S4.T3.6.1.8.2.2" class="ltx_text"></span></span>
</td>
</tr>
<tr id="S4.T3.6.2" class="ltx_tr">
<td id="S4.T3.6.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">shamshiri2023compatible</span> </a></cite></td>
<td id="S4.T3.6.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.2.2.1" class="ltx_text"></span><span id="S4.T3.6.2.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.2.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.2.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.2.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Breast</span></span>
</span></span><span id="S4.T3.6.2.2.4" class="ltx_text"></span><span id="S4.T3.6.2.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.2.3.1" class="ltx_text"></span><span id="S4.T3.6.2.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.2.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.2.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.2.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.2.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">DenseNet-169</span></span>
</span></span><span id="S4.T3.6.2.3.4" class="ltx_text"></span><span id="S4.T3.6.2.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.2.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.2.5.1" class="ltx_text" style="color:#000000;">Inductive</span></td>
<td id="S4.T3.6.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.2.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.2.7.1" class="ltx_text" style="color:#000000;">BreakHis</span></td>
<td id="S4.T3.6.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.2.8.1" class="ltx_text"></span><span id="S4.T3.6.2.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.2.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.2.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.2.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.2.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy=98.73%</span></span>
</span></span><span id="S4.T3.6.2.8.4" class="ltx_text"></span><span id="S4.T3.6.2.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.3" class="ltx_tr">
<td id="S4.T3.6.3.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.3.1.1" class="ltx_text"></span><span id="S4.T3.6.3.1.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.3.1.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.3.1.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.3.1.3.1.1" class="ltx_tr">
<span id="S4.T3.6.3.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rehman2020deep</span> </a></cite></span></span>
</span></span><span id="S4.T3.6.3.1.4" class="ltx_text"></span><span id="S4.T3.6.3.1.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.3.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.3.2.1" class="ltx_text"></span><span id="S4.T3.6.3.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.3.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.3.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.3.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.3.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Brain</span></span>
<span id="S4.T3.6.3.2.3.1.2" class="ltx_tr">
<span id="S4.T3.6.3.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">tumor</span></span>
</span></span><span id="S4.T3.6.3.2.4" class="ltx_text"></span><span id="S4.T3.6.3.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.3.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.3.3.1" class="ltx_text"></span><span id="S4.T3.6.3.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.3.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.3.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.3.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.3.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">VGG16</span></span>
</span></span><span id="S4.T3.6.3.3.4" class="ltx_text"></span><span id="S4.T3.6.3.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.3.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.3.4.1" class="ltx_text"></span><span id="S4.T3.6.3.4.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.3.4.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.3.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.3.4.3.1.1" class="ltx_tr">
<span id="S4.T3.6.3.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">TL</span></span>
</span></span><span id="S4.T3.6.3.4.4" class="ltx_text"></span><span id="S4.T3.6.3.4.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.3.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.3.5.1" class="ltx_text"></span><span id="S4.T3.6.3.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.3.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.3.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.3.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.3.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.3.5.4" class="ltx_text"></span><span id="S4.T3.6.3.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.3.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.3.6.1" class="ltx_text"></span><span id="S4.T3.6.3.6.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.3.6.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.3.6.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.3.6.3.1.1" class="ltx_tr">
<span id="S4.T3.6.3.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">N/A</span></span>
</span></span><span id="S4.T3.6.3.6.4" class="ltx_text"></span><span id="S4.T3.6.3.6.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.3.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.3.7.1" class="ltx_text"></span><span id="S4.T3.6.3.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.3.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.3.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.3.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.3.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Figshare brain</span></span>
<span id="S4.T3.6.3.7.3.1.2" class="ltx_tr">
<span id="S4.T3.6.3.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">tumor</span></span>
</span></span><span id="S4.T3.6.3.7.4" class="ltx_text"></span><span id="S4.T3.6.3.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.3.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.3.8.1" class="ltx_text"></span><span id="S4.T3.6.3.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.3.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.3.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.3.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.3.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy=98.69%</span></span>
</span></span><span id="S4.T3.6.3.8.4" class="ltx_text"></span><span id="S4.T3.6.3.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.4" class="ltx_tr">
<td id="S4.T3.6.4.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.4.1.1" class="ltx_text"></span><span id="S4.T3.6.4.1.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.4.1.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.4.1.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.4.1.3.1.1" class="ltx_tr">
<span id="S4.T3.6.4.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">talukder2023efficient</span> </a></cite></span></span>
</span></span><span id="S4.T3.6.4.1.4" class="ltx_text"></span><span id="S4.T3.6.4.1.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.4.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.4.2.1" class="ltx_text"></span><span id="S4.T3.6.4.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.4.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.4.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.4.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.4.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Brain</span></span>
<span id="S4.T3.6.4.2.3.1.2" class="ltx_tr">
<span id="S4.T3.6.4.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">tumor</span></span>
</span></span><span id="S4.T3.6.4.2.4" class="ltx_text"></span><span id="S4.T3.6.4.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.4.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.4.3.1" class="ltx_text"></span><span id="S4.T3.6.4.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.4.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.4.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.4.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.4.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">ResNet50V2</span></span>
</span></span><span id="S4.T3.6.4.3.4" class="ltx_text"></span><span id="S4.T3.6.4.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.4.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.4.4.1" class="ltx_text"></span><span id="S4.T3.6.4.4.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.4.4.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.4.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.4.4.3.1.1" class="ltx_tr">
<span id="S4.T3.6.4.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">TL</span></span>
</span></span><span id="S4.T3.6.4.4.4" class="ltx_text"></span><span id="S4.T3.6.4.4.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.4.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.4.5.1" class="ltx_text"></span><span id="S4.T3.6.4.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.4.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.4.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.4.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.4.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.4.5.4" class="ltx_text"></span><span id="S4.T3.6.4.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.4.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.4.6.1" class="ltx_text"></span><span id="S4.T3.6.4.6.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.4.6.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.4.6.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.4.6.3.1.1" class="ltx_tr">
<span id="S4.T3.6.4.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">N/A</span></span>
</span></span><span id="S4.T3.6.4.6.4" class="ltx_text"></span><span id="S4.T3.6.4.6.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.4.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.4.7.1" class="ltx_text"></span><span id="S4.T3.6.4.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.4.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.4.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.4.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.4.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Figshare brain</span></span>
<span id="S4.T3.6.4.7.3.1.2" class="ltx_tr">
<span id="S4.T3.6.4.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">tumor</span></span>
</span></span><span id="S4.T3.6.4.7.4" class="ltx_text"></span><span id="S4.T3.6.4.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.4.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.4.8.1" class="ltx_text"></span><span id="S4.T3.6.4.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.4.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.4.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.4.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.4.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy=99.68%</span></span>
</span></span><span id="S4.T3.6.4.8.4" class="ltx_text"></span><span id="S4.T3.6.4.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.5" class="ltx_tr">
<td id="S4.T3.6.5.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.5.1.1" class="ltx_text"></span><span id="S4.T3.6.5.1.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.5.1.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.5.1.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.5.1.3.1.1" class="ltx_tr">
<span id="S4.T3.6.5.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">lyu2022transformer</span> </a></cite></span></span>
</span></span><span id="S4.T3.6.5.1.4" class="ltx_text"></span><span id="S4.T3.6.5.1.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.5.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.5.2.1" class="ltx_text"></span><span id="S4.T3.6.5.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.5.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.5.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.5.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.5.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Brain</span></span>
<span id="S4.T3.6.5.2.3.1.2" class="ltx_tr">
<span id="S4.T3.6.5.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">metastases</span></span>
</span></span><span id="S4.T3.6.5.2.4" class="ltx_text"></span><span id="S4.T3.6.5.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.5.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.5.3.1" class="ltx_text"></span><span id="S4.T3.6.5.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.5.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.5.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.5.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.5.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Custom</span></span>
<span id="S4.T3.6.5.3.3.1.2" class="ltx_tr">
<span id="S4.T3.6.5.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">transformer</span></span>
<span id="S4.T3.6.5.3.3.1.3" class="ltx_tr">
<span id="S4.T3.6.5.3.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">model</span></span>
</span></span><span id="S4.T3.6.5.3.4" class="ltx_text"></span><span id="S4.T3.6.5.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.5.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.5.4.1" class="ltx_text"></span><span id="S4.T3.6.5.4.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.5.4.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.5.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.5.4.3.1.1" class="ltx_tr">
<span id="S4.T3.6.5.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">TL</span></span>
</span></span><span id="S4.T3.6.5.4.4" class="ltx_text"></span><span id="S4.T3.6.5.4.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.5.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.5.5.1" class="ltx_text"></span><span id="S4.T3.6.5.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.5.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.5.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.5.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.5.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.5.5.4" class="ltx_text"></span><span id="S4.T3.6.5.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.5.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.5.6.1" class="ltx_text"></span><span id="S4.T3.6.5.6.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.5.6.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.5.6.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.5.6.3.1.1" class="ltx_tr">
<span id="S4.T3.6.5.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">N/A</span></span>
</span></span><span id="S4.T3.6.5.6.4" class="ltx_text"></span><span id="S4.T3.6.5.6.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.5.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.5.7.1" class="ltx_text"></span><span id="S4.T3.6.5.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.5.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.5.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.5.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.5.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">In-house brain</span></span>
<span id="S4.T3.6.5.7.3.1.2" class="ltx_tr">
<span id="S4.T3.6.5.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">MRI</span></span>
</span></span><span id="S4.T3.6.5.7.4" class="ltx_text"></span><span id="S4.T3.6.5.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.5.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.5.8.1" class="ltx_text"></span><span id="S4.T3.6.5.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.5.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.5.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.5.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.5.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">AUC=0.878</span></span>
</span></span><span id="S4.T3.6.5.8.4" class="ltx_text"></span><span id="S4.T3.6.5.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.6" class="ltx_tr">
<td id="S4.T3.6.6.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jimenez2023memory</span> </a></cite></td>
<td id="S4.T3.6.6.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.6.2.1" class="ltx_text"></span><span id="S4.T3.6.6.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.6.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.6.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.6.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Breast</span></span>
</span></span><span id="S4.T3.6.6.2.4" class="ltx_text"></span><span id="S4.T3.6.6.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.6.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.6.3.1" class="ltx_text" style="color:#000000;">CNN</span></td>
<td id="S4.T3.6.6.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.6.4.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T3.6.6.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.6.5.1" class="ltx_text" style="color:#000000;">Vertical</span></td>
<td id="S4.T3.6.6.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.6.6.1" class="ltx_text" style="color:#000000;">FedAvg</span></td>
<td id="S4.T3.6.6.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.6.7.1" class="ltx_text" style="color:#000000;">3 clinical datasets</span></td>
<td id="S4.T3.6.6.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.6.8.1" class="ltx_text"></span><span id="S4.T3.6.6.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.6.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.6.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">ROC= 5%</span></span>
</span></span><span id="S4.T3.6.6.8.4" class="ltx_text"></span><span id="S4.T3.6.6.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.7" class="ltx_tr">
<td id="S4.T3.6.7.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kumari2023magnification</span> </a></cite></td>
<td id="S4.T3.6.7.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.7.2.1" class="ltx_text"></span><span id="S4.T3.6.7.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.7.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.7.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.7.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.7.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Breast</span></span>
</span></span><span id="S4.T3.6.7.2.4" class="ltx_text"></span><span id="S4.T3.6.7.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.7.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.7.3.1" class="ltx_text"></span><span id="S4.T3.6.7.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.7.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.7.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.7.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.7.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">VGG-16, Xception,</span></span>
<span id="S4.T3.6.7.3.3.1.2" class="ltx_tr">
<span id="S4.T3.6.7.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Densenet-201</span></span>
</span></span><span id="S4.T3.6.7.3.4" class="ltx_text"></span><span id="S4.T3.6.7.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.7.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.7.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.7.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.7.5.1" class="ltx_text" style="color:#000000;">Inductive</span></td>
<td id="S4.T3.6.7.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.7.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.7.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.7.7.1" class="ltx_text"></span><span id="S4.T3.6.7.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.7.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.7.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.7.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.7.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">IDC,</span></span>
<span id="S4.T3.6.7.7.3.1.2" class="ltx_tr">
<span id="S4.T3.6.7.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">BreaKHis</span></span>
</span></span><span id="S4.T3.6.7.7.4" class="ltx_text"></span><span id="S4.T3.6.7.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.7.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.7.8.1" class="ltx_text"></span><span id="S4.T3.6.7.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.7.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.7.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.7.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.7.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy=99.42%</span></span>
</span></span><span id="S4.T3.6.7.8.4" class="ltx_text"></span><span id="S4.T3.6.7.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.8" class="ltx_tr">
<td id="S4.T3.6.8.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kumbhare2023federated</span> </a></cite></td>
<td id="S4.T3.6.8.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.8.2.1" class="ltx_text"></span><span id="S4.T3.6.8.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.8.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.8.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.8.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.8.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Breast</span></span>
</span></span><span id="S4.T3.6.8.2.4" class="ltx_text"></span><span id="S4.T3.6.8.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.8.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.8.3.1" class="ltx_text" style="color:#000000;">DenseNet, E-RNN</span></td>
<td id="S4.T3.6.8.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.8.4.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T3.6.8.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.8.5.1" class="ltx_text" style="color:#000000;">Vertical</span></td>
<td id="S4.T3.6.8.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.8.6.1" class="ltx_text" style="color:#000000;">FedAvg</span></td>
<td id="S4.T3.6.8.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.8.7.1" class="ltx_text" style="color:#000000;">CBIS-DDSM</span></td>
<td id="S4.T3.6.8.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.8.8.1" class="ltx_text"></span><span id="S4.T3.6.8.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.8.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.8.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.8.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.8.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy=95.73%</span></span>
</span></span><span id="S4.T3.6.8.8.4" class="ltx_text"></span><span id="S4.T3.6.8.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.9" class="ltx_tr">
<td id="S4.T3.6.9.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">talukder2022machine</span> </a></cite></td>
<td id="S4.T3.6.9.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.9.2.1" class="ltx_text"></span><span id="S4.T3.6.9.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.9.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.9.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.9.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.9.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Colon</span></span>
</span></span><span id="S4.T3.6.9.2.4" class="ltx_text"></span><span id="S4.T3.6.9.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.9.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.9.3.1" class="ltx_text" style="color:#000000;">YOLOv3</span></td>
<td id="S4.T3.6.9.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.9.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.9.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.9.5.1" class="ltx_text"></span><span id="S4.T3.6.9.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.9.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.9.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.9.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.9.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.9.5.4" class="ltx_text"></span><span id="S4.T3.6.9.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.9.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.9.6.1" class="ltx_text"></span><span id="S4.T3.6.9.6.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.9.6.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.9.6.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.9.6.3.1.1" class="ltx_tr">
<span id="S4.T3.6.9.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">N/A</span></span>
</span></span><span id="S4.T3.6.9.6.4" class="ltx_text"></span><span id="S4.T3.6.9.6.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.9.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.9.7.1" class="ltx_text"></span><span id="S4.T3.6.9.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.9.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.9.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.9.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.9.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">CVC,</span></span>
<span id="S4.T3.6.9.7.3.1.2" class="ltx_tr">
<span id="S4.T3.6.9.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">colonDB</span></span>
</span></span><span id="S4.T3.6.9.7.4" class="ltx_text"></span><span id="S4.T3.6.9.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.9.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.9.8.1" class="ltx_text" style="color:#000000;">Accuracy=96.04%</span></td>
</tr>
<tr id="S4.T3.6.10" class="ltx_tr">
<td id="S4.T3.6.10.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mehmood2022malignancy</span> </a></cite></td>
<td id="S4.T3.6.10.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.10.2.1" class="ltx_text"></span><span id="S4.T3.6.10.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.10.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.10.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.10.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.10.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Lung and</span></span>
<span id="S4.T3.6.10.2.3.1.2" class="ltx_tr">
<span id="S4.T3.6.10.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Colon</span></span>
</span></span><span id="S4.T3.6.10.2.4" class="ltx_text"></span><span id="S4.T3.6.10.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.10.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.10.3.1" class="ltx_text"></span><span id="S4.T3.6.10.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.10.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.10.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.10.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.10.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">VGG16 and</span></span>
<span id="S4.T3.6.10.3.3.1.2" class="ltx_tr">
<span id="S4.T3.6.10.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">VGG19</span></span>
</span></span><span id="S4.T3.6.10.3.4" class="ltx_text"></span><span id="S4.T3.6.10.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.10.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.10.4.1" class="ltx_text"></span><span id="S4.T3.6.10.4.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.10.4.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.10.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.10.4.3.1.1" class="ltx_tr">
<span id="S4.T3.6.10.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">TL</span></span>
</span></span><span id="S4.T3.6.10.4.4" class="ltx_text"></span><span id="S4.T3.6.10.4.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.10.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.10.5.1" class="ltx_text"></span><span id="S4.T3.6.10.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.10.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.10.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.10.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.10.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.10.5.4" class="ltx_text"></span><span id="S4.T3.6.10.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.10.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.10.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.10.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.10.7.1" class="ltx_text" style="color:#000000;">LC25000</span></td>
<td id="S4.T3.6.10.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.10.8.1" class="ltx_text"></span><span id="S4.T3.6.10.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.10.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.10.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.10.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.10.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy=99.05%</span></span>
</span></span><span id="S4.T3.6.10.8.4" class="ltx_text"></span><span id="S4.T3.6.10.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.11" class="ltx_tr">
<td id="S4.T3.6.11.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2023federated</span> </a></cite></td>
<td id="S4.T3.6.11.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.11.2.1" class="ltx_text" style="color:#000000;">Lung</span></td>
<td id="S4.T3.6.11.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.11.3.1" class="ltx_text" style="color:#000000;">ResNet18</span></td>
<td id="S4.T3.6.11.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.11.4.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T3.6.11.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.11.5.1" class="ltx_text" style="color:#000000;">Horizontal</span></td>
<td id="S4.T3.6.11.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.11.6.1" class="ltx_text" style="color:#000000;">FedAvg</span></td>
<td id="S4.T3.6.11.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.11.7.1" class="ltx_text" style="color:#000000;">Luna16</span></td>
<td id="S4.T3.6.11.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.11.8.1" class="ltx_text" style="color:#000000;">Accuracy=83.4%</span></td>
</tr>
<tr id="S4.T3.6.12" class="ltx_tr">
<td id="S4.T3.6.12.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">fang2018novel</span> </a></cite></td>
<td id="S4.T3.6.12.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.12.2.1" class="ltx_text" style="color:#000000;">Lung</span></td>
<td id="S4.T3.6.12.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.12.3.1" class="ltx_text" style="color:#000000;">GoogLeNet</span></td>
<td id="S4.T3.6.12.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.12.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.12.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.12.5.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.12.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.12.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.12.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.12.7.1" class="ltx_text" style="color:#000000;">LIDC</span></td>
<td id="S4.T3.6.12.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.12.8.1" class="ltx_text" style="color:#000000;">Accuracy=81%</span></td>
</tr>
<tr id="S4.T3.6.13" class="ltx_tr">
<td id="S4.T3.6.13.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sajja2019lung</span> </a></cite></td>
<td id="S4.T3.6.13.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.13.2.1" class="ltx_text" style="color:#000000;">Lung</span></td>
<td id="S4.T3.6.13.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.13.3.1" class="ltx_text"></span><span id="S4.T3.6.13.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.13.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.13.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.13.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.13.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Modified</span></span>
<span id="S4.T3.6.13.3.3.1.2" class="ltx_tr">
<span id="S4.T3.6.13.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">GoogleNet</span></span>
</span></span><span id="S4.T3.6.13.3.4" class="ltx_text"></span><span id="S4.T3.6.13.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.13.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.13.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.13.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.13.5.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.13.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.13.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.13.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.13.7.1" class="ltx_text" style="color:#000000;">LIDC</span></td>
<td id="S4.T3.6.13.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.13.8.1" class="ltx_text" style="color:#000000;">Accuracy=99.03%</span></td>
</tr>
<tr id="S4.T3.6.14" class="ltx_tr">
<td id="S4.T3.6.14.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">heidari2023new</span> </a></cite></td>
<td id="S4.T3.6.14.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.14.2.1" class="ltx_text" style="color:#000000;">Lung</span></td>
<td id="S4.T3.6.14.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.14.3.1" class="ltx_text" style="color:#000000;">CapsNet</span></td>
<td id="S4.T3.6.14.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.14.4.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T3.6.14.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.14.5.1" class="ltx_text" style="color:#000000;">Horizontal</span></td>
<td id="S4.T3.6.14.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.14.6.1" class="ltx_text"></span><span id="S4.T3.6.14.6.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.14.6.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.14.6.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.14.6.3.1.1" class="ltx_tr">
<span id="S4.T3.6.14.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Model</span></span>
<span id="S4.T3.6.14.6.3.1.2" class="ltx_tr">
<span id="S4.T3.6.14.6.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Aggregation</span></span>
</span></span><span id="S4.T3.6.14.6.4" class="ltx_text"></span><span id="S4.T3.6.14.6.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.14.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.14.7.1" class="ltx_text"></span><span id="S4.T3.6.14.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.14.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.14.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.14.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.14.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">CIA, KDSB,</span></span>
<span id="S4.T3.6.14.7.3.1.2" class="ltx_tr">
<span id="S4.T3.6.14.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">LUNA16, Local</span></span>
</span></span><span id="S4.T3.6.14.7.4" class="ltx_text"></span><span id="S4.T3.6.14.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.14.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.14.8.1" class="ltx_text" style="color:#000000;">Accuracy=99.69%</span></td>
</tr>
<tr id="S4.T3.6.15" class="ltx_tr">
<td id="S4.T3.6.15.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rajagopal2023federated</span> </a></cite></td>
<td id="S4.T3.6.15.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.15.2.1" class="ltx_text" style="color:#000000;">Prostate</span></td>
<td id="S4.T3.6.15.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.15.3.1" class="ltx_text" style="color:#000000;">3D UNet</span></td>
<td id="S4.T3.6.15.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.15.4.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T3.6.15.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.15.5.1" class="ltx_text" style="color:#000000;">Vertical</span></td>
<td id="S4.T3.6.15.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.15.6.1" class="ltx_text" style="color:#000000;">FedSGD</span></td>
<td id="S4.T3.6.15.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.15.7.1" class="ltx_text"></span><span id="S4.T3.6.15.7.2" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.15.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.15.7.2.1.1" class="ltx_tr">
<span id="S4.T3.6.15.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">UCS, UCLA</span></span>
</span></span><span id="S4.T3.6.15.7.3" class="ltx_text"></span><span id="S4.T3.6.15.7.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.15.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.15.8.1" class="ltx_text"></span><span id="S4.T3.6.15.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.15.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.15.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.15.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.15.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">IoU improved 100%</span></span>
</span></span><span id="S4.T3.6.15.8.4" class="ltx_text"></span><span id="S4.T3.6.15.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.16" class="ltx_tr">
<td id="S4.T3.6.16.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2020federated</span> </a></cite></td>
<td id="S4.T3.6.16.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.16.2.1" class="ltx_text" style="color:#000000;">Various</span></td>
<td id="S4.T3.6.16.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.16.3.1" class="ltx_text"></span><span id="S4.T3.6.16.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.16.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.16.3.2.1.1" class="ltx_tr">
<span id="S4.T3.6.16.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">LeNet, VGG</span></span>
</span></span><span id="S4.T3.6.16.3.3" class="ltx_text"></span><span id="S4.T3.6.16.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.16.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.16.4.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T3.6.16.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.16.5.1" class="ltx_text" style="color:#000000;">Vertical</span></td>
<td id="S4.T3.6.16.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.16.6.1" class="ltx_text" style="color:#000000;">FedMA</span></td>
<td id="S4.T3.6.16.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.16.7.1" class="ltx_text"></span><span id="S4.T3.6.16.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.16.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.16.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.16.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.16.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">MNIST, CIFAR-10</span></span>
</span></span><span id="S4.T3.6.16.7.4" class="ltx_text"></span><span id="S4.T3.6.16.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.16.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.16.8.1" class="ltx_text" style="color:#000000;">N/A</span></td>
</tr>
<tr id="S4.T3.6.17" class="ltx_tr">
<td id="S4.T3.6.17.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2022transunet</span> </a></cite></td>
<td id="S4.T3.6.17.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.17.2.1" class="ltx_text"></span><span id="S4.T3.6.17.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.17.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.17.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.17.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.17.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Brain</span></span>
<span id="S4.T3.6.17.2.3.1.2" class="ltx_tr">
<span id="S4.T3.6.17.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">tumor</span></span>
</span></span><span id="S4.T3.6.17.2.4" class="ltx_text"></span><span id="S4.T3.6.17.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.17.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.17.3.1" class="ltx_text" style="color:#000000;">TransUNet</span></td>
<td id="S4.T3.6.17.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.17.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.17.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.17.5.1" class="ltx_text"></span><span id="S4.T3.6.17.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.17.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.17.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.17.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.17.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.17.5.4" class="ltx_text"></span><span id="S4.T3.6.17.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.17.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.17.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.17.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.17.7.1" class="ltx_text" style="color:#000000;">BraTS</span></td>
<td id="S4.T3.6.17.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.17.8.1" class="ltx_text"></span><span id="S4.T3.6.17.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.17.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.17.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.17.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.17.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Dice=0.864</span></span>
</span></span><span id="S4.T3.6.17.8.4" class="ltx_text"></span><span id="S4.T3.6.17.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.18" class="ltx_tr">
<td id="S4.T3.6.18.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">masood2023multi</span> </a></cite></td>
<td id="S4.T3.6.18.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.18.2.1" class="ltx_text" style="color:#000000;">Lung</span></td>
<td id="S4.T3.6.18.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.18.3.1" class="ltx_text"></span><span id="S4.T3.6.18.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.18.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.18.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.18.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.18.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Swin</span></span>
<span id="S4.T3.6.18.3.3.1.2" class="ltx_tr">
<span id="S4.T3.6.18.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Transformer</span></span>
</span></span><span id="S4.T3.6.18.3.4" class="ltx_text"></span><span id="S4.T3.6.18.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.18.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.18.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.18.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.18.5.1" class="ltx_text"></span><span id="S4.T3.6.18.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.18.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.18.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.18.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.18.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.18.5.4" class="ltx_text"></span><span id="S4.T3.6.18.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.18.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.18.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.18.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.18.7.1" class="ltx_text"></span><span id="S4.T3.6.18.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.18.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.18.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.18.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.18.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">LIDC-IDRI and ILD</span></span>
</span></span><span id="S4.T3.6.18.7.4" class="ltx_text"></span><span id="S4.T3.6.18.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.18.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.18.8.1" class="ltx_text"></span><span id="S4.T3.6.18.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.18.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.18.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.18.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.18.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Dice=0.9672</span></span>
</span></span><span id="S4.T3.6.18.8.4" class="ltx_text"></span><span id="S4.T3.6.18.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.19" class="ltx_tr">
<td id="S4.T3.6.19.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">hosny2018skin</span> </a></cite></td>
<td id="S4.T3.6.19.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.19.2.1" class="ltx_text"></span><span id="S4.T3.6.19.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.19.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.19.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.19.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.19.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Skin</span></span>
</span></span><span id="S4.T3.6.19.2.4" class="ltx_text"></span><span id="S4.T3.6.19.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.19.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.19.3.1" class="ltx_text" style="color:#000000;">AlexNet</span></td>
<td id="S4.T3.6.19.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.19.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.19.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.19.5.1" class="ltx_text" style="color:#000000;">Inductive</span></td>
<td id="S4.T3.6.19.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.19.6.1" class="ltx_text"></span><span id="S4.T3.6.19.6.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.19.6.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.19.6.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.19.6.3.1.1" class="ltx_tr">
<span id="S4.T3.6.19.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">N/A</span></span>
</span></span><span id="S4.T3.6.19.6.4" class="ltx_text"></span><span id="S4.T3.6.19.6.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.19.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.19.7.1" class="ltx_text"></span><span id="S4.T3.6.19.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.19.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.19.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.19.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.19.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Ph2</span></span>
</span></span><span id="S4.T3.6.19.7.4" class="ltx_text"></span><span id="S4.T3.6.19.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.19.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.19.8.1" class="ltx_text" style="color:#000000;">Accuracy=98.61%</span></td>
</tr>
<tr id="S4.T3.6.20" class="ltx_tr">
<td id="S4.T3.6.20.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">cai2021many</span> </a></cite></td>
<td id="S4.T3.6.20.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.20.2.1" class="ltx_text" style="color:#000000;">Skin</span></td>
<td id="S4.T3.6.20.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.20.3.1" class="ltx_text" style="color:#000000;">CNN</span></td>
<td id="S4.T3.6.20.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.20.4.1" class="ltx_text"></span><span id="S4.T3.6.20.4.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.20.4.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.20.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.20.4.3.1.1" class="ltx_tr">
<span id="S4.T3.6.20.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">FTL</span></span>
</span></span><span id="S4.T3.6.20.4.4" class="ltx_text"></span><span id="S4.T3.6.20.4.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.20.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.20.5.1" class="ltx_text" style="color:#000000;">Horizontal</span></td>
<td id="S4.T3.6.20.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.20.6.1" class="ltx_text"></span><span id="S4.T3.6.20.6.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.20.6.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.20.6.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.20.6.3.1.1" class="ltx_tr">
<span id="S4.T3.6.20.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Model</span></span>
<span id="S4.T3.6.20.6.3.1.2" class="ltx_tr">
<span id="S4.T3.6.20.6.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Averaging</span></span>
</span></span><span id="S4.T3.6.20.6.4" class="ltx_text"></span><span id="S4.T3.6.20.6.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.20.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.20.7.1" class="ltx_text" style="color:#000000;">ISIC 2018</span></td>
<td id="S4.T3.6.20.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.20.8.1" class="ltx_text" style="color:#000000;">Accuracy=91%</span></td>
</tr>
<tr id="S4.T3.6.21" class="ltx_tr">
<td id="S4.T3.6.21.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">tyagi2023amalgamation</span> </a></cite></td>
<td id="S4.T3.6.21.2" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.21.2.1" class="ltx_text"></span><span id="S4.T3.6.21.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.21.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.21.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.21.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.21.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Lung</span></span>
<span id="S4.T3.6.21.2.3.1.2" class="ltx_tr">
<span id="S4.T3.6.21.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">tumor</span></span>
</span></span><span id="S4.T3.6.21.2.4" class="ltx_text"></span><span id="S4.T3.6.21.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.21.3" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.21.3.1" class="ltx_text"></span><span id="S4.T3.6.21.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.21.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.21.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.21.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.21.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">ViT and</span></span>
<span id="S4.T3.6.21.3.3.1.2" class="ltx_tr">
<span id="S4.T3.6.21.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">CNN</span></span>
</span></span><span id="S4.T3.6.21.3.4" class="ltx_text"></span><span id="S4.T3.6.21.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.21.4" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.21.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.21.5" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.21.5.1" class="ltx_text"></span><span id="S4.T3.6.21.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.21.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.21.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.21.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.21.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.21.5.4" class="ltx_text"></span><span id="S4.T3.6.21.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.21.6" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.21.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.21.7" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.21.7.1" class="ltx_text"></span><span id="S4.T3.6.21.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.21.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.21.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.21.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.21.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">NSCLC-Radiomics,</span></span>
<span id="S4.T3.6.21.7.3.1.2" class="ltx_tr">
<span id="S4.T3.6.21.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Local hospital</span></span>
</span></span><span id="S4.T3.6.21.7.4" class="ltx_text"></span><span id="S4.T3.6.21.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.21.8" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.21.8.1" class="ltx_text"></span><span id="S4.T3.6.21.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.21.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.21.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.21.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.21.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Dice=0.7468</span></span>
</span></span><span id="S4.T3.6.21.8.4" class="ltx_text"></span><span id="S4.T3.6.21.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T3.6.22" class="ltx_tr">
<td id="S4.T3.6.22.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2023vision</span> </a></cite></td>
<td id="S4.T3.6.22.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.22.2.1" class="ltx_text"></span><span id="S4.T3.6.22.2.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.22.2.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.22.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.22.2.3.1.1" class="ltx_tr">
<span id="S4.T3.6.22.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Cell</span></span>
<span id="S4.T3.6.22.2.3.1.2" class="ltx_tr">
<span id="S4.T3.6.22.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">tumor</span></span>
</span></span><span id="S4.T3.6.22.2.4" class="ltx_text"></span><span id="S4.T3.6.22.2.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.22.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.22.3.1" class="ltx_text"></span><span id="S4.T3.6.22.3.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.22.3.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.22.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.22.3.3.1.1" class="ltx_tr">
<span id="S4.T3.6.22.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">ViT</span></span>
</span></span><span id="S4.T3.6.22.3.4" class="ltx_text"></span><span id="S4.T3.6.22.3.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.22.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.22.4.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T3.6.22.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.22.5.1" class="ltx_text"></span><span id="S4.T3.6.22.5.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.22.5.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.22.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.22.5.3.1.1" class="ltx_tr">
<span id="S4.T3.6.22.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Inductive</span></span>
</span></span><span id="S4.T3.6.22.5.4" class="ltx_text"></span><span id="S4.T3.6.22.5.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.22.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T3.6.22.6.1" class="ltx_text" style="color:#000000;">N/A</span></td>
<td id="S4.T3.6.22.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.22.7.1" class="ltx_text"></span><span id="S4.T3.6.22.7.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.22.7.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.22.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.22.7.3.1.1" class="ltx_tr">
<span id="S4.T3.6.22.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">BreakHis and</span></span>
<span id="S4.T3.6.22.7.3.1.2" class="ltx_tr">
<span id="S4.T3.6.22.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">PatchCamelyon</span></span>
</span></span><span id="S4.T3.6.22.7.4" class="ltx_text"></span><span id="S4.T3.6.22.7.5" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T3.6.22.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T3.6.22.8.1" class="ltx_text"></span><span id="S4.T3.6.22.8.2" class="ltx_text" style="color:#000000;"> </span><span id="S4.T3.6.22.8.3" class="ltx_text" style="color:#000000;">
<span id="S4.T3.6.22.8.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.22.8.3.1.1" class="ltx_tr">
<span id="S4.T3.6.22.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">AUC=0.96</span></span>
</span></span><span id="S4.T3.6.22.8.4" class="ltx_text"></span><span id="S4.T3.6.22.8.5" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
</table>
</figure>
</li>
<li id="S4.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Categories: </span><span id="S4.I1.i1.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;">Consists of three main types (inductive, transductive and unsupervised). The type depends on the availability and type of data. The types are:</span></p>
<ul id="S4.I1.i1.I1.i2.I1" class="ltx_itemize">
<li id="S4.I1.i1.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i1.I1.i2.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i1.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i2.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Transductive:</span><span id="S4.I1.i1.I1.i2.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;"> The transductive approach refers to a type of TL in which the source and target tasks are similar but their domains are different </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">himeur2022next</span> </a></cite><span id="S4.I1.i1.I1.i2.I1.i1.p1.1.5" class="ltx_text" style="color:#000000;">. The goal of transductive TL is to improve the performance of a specific task in a target domain through the transfer of knowledge acquired from a source domain. This approach estimates the probability distribution of the target dataset given the source dataset and uses this data to adjust the source dataset to improve the performance of the child model </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">slim2022improving</span> </a></cite><span id="S4.I1.i1.I1.i2.I1.i1.p1.1.8" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</li>
<li id="S4.I1.i1.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i1.I1.i2.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i1.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i2.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Inductive:</span><span id="S4.I1.i1.I1.i2.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;"> According to </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">paya2022automatic</span> </a></cite><span id="S4.I1.i1.I1.i2.I1.i2.p1.1.5" class="ltx_text" style="color:#000000;">, inductive transfer learning (ITL) is used to transfer knowledge from a source dataset to a smaller target dataset to improve the prediction efficiency of the given dataset. ITL is generally used in prior knowledge (neural networks and probabilistic generative processes) to detect unseen classes or clusters that are not seen or recognized by the original model.</span></p>
</div>
</li>
<li id="S4.I1.i1.I1.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i1.I1.i2.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i1.I1.i2.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i1.I1.i2.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i1.I1.i2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Unsupervised Transfer Learning (UTL): </span><span id="S4.I1.i1.I1.i2.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;">UTL is similar to inductive TL, with the main difference being the lack of labeled data in both the source and target domains </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">agarwal2021transfer</span> </a></cite><span id="S4.I1.i1.I1.i2.I1.i3.p1.1.5" class="ltx_text" style="color:#000000;">, the method involved weighting the subspace-aligned features from the source users based on their agreement with the target user and subspace alignment to adapt the features from the source domain to the target domain </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chen2022multi</span> </a></cite><span id="S4.I1.i1.I1.i2.I1.i3.p1.1.8" class="ltx_text" style="color:#000000;">. There are many types of UTL such as unsupervised domain adaptation </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2023unsupervised</span> </a></cite><span id="S4.I1.i1.I1.i2.I1.i3.p1.1.11" class="ltx_text" style="color:#000000;">, Unsupervised domain generalization </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhou2022asymmetrical</span> </a></cite><span id="S4.I1.i1.I1.i2.I1.i3.p1.1.14" class="ltx_text" style="color:#000000;"> and Unsupervised Clustering </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2021survey</span> </a></cite><span id="S4.I1.i1.I1.i2.I1.i3.p1.1.17" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.20126/assets/x3.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.5.2" class="ltx_text" style="font-size:90%;">Centralized FL with one aggregation server</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.20126/assets/x4.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.5.2" class="ltx_text" style="font-size:90%;">Decentralized FL with one peer to peer communication</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.20126/assets/x5.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="238" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F3.sf3.5.2" class="ltx_text" style="font-size:90%;">Hybrid FL with three centralized FL connected with decentralized communication</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.6.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.7.2" class="ltx_text" style="font-size:90%;">Difference between centralized, decentralized and hybrid communication in FL environment for CD based on image analysis <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">roy2019braintorrent</span> </a>; <a href="#bib.bib91" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhang2020hybrid</span> </a></cite></span></figcaption>
</figure>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.1.1.1" class="ltx_text ltx_font_bold">C2.</span></span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Classification of FL: </span><span id="S4.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;">This section focuses on the communication types, data partitioning and aggregation strategies used in FL. By providing an overview of the different techniques used in each of these categories, including their advantages and disadvantages:</span></p>
<ul id="S4.I1.i2.I1" class="ltx_itemize">
<li id="S4.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Communication Types: </span><span id="S4.I1.i2.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;">Many types of ML models have been used in federated architecture to process and train medical data to extract insights, anomalies, and patterns. Fig. </span><a href="#S4.F3" title="Figure 3 ‣ item C1. ‣ 4.1.1 Classification (C) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.I1.i2.I1.i1.p1.1.3" class="ltx_text" style="color:#000000;"> illustrates the three main communication styles, including centralized, decentralized and hybrid. These are the most commonly used ML types and models:</span></p>
<ul id="S4.I1.i2.I1.i1.I1" class="ltx_itemize">
<li id="S4.I1.i2.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Centralized Communication: </span><span id="S4.I1.i2.I1.i1.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;">This is the most common type of FL communication. It refers to a classic approach of using a central server to carry out operations and coordinate communications in a federated architecture, for example: planning the agenda of training rounds, aggregation strategy of weights, exchanging parameters with local customers and selecting customers, who took part in the training. The disadvantage of this communication is that the architecture is dependent on the server. If the server crashes for any reason, then the system collapses. Additionally, the number of maximum clients may be limited, leading to scalability issues. Therefore, decentralized communication was introduced to solve these problems </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">abdulrahman2020survey</span> </a></cite><span id="S4.I1.i2.I1.i1.I1.i1.p1.1.5" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</li>
<li id="S4.I1.i2.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Decentralized Communication:</span><span id="S4.I1.i2.I1.i1.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;"> refers to a type of communication in which two clients share model weight updates directly in a peer-to-peer communication protocol. The sharing of the weights covered all clients present in the federated network until the general model was formed, which allowed increasing the scalability of the federated model and ensuring that the federated model does not collapse </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">beltran2023decentralized</span> </a>; <a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">roy2019braintorrent</span> </a></cite><span id="S4.I1.i2.I1.i1.I1.i2.p1.1.5" class="ltx_text" style="color:#000000;">. There are different types of federated decentralized communication, such as: gossip-based protocols, peer-to-peer protocols and blockchain-based protocols.</span></p>
</div>
</li>
<li id="S4.I1.i2.I1.i1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i1.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i1.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i1.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Hybrid Communication:</span><span id="S4.I1.i2.I1.i1.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;"> is a combination of centralized and peer-to-peer communication in a federated architecture. It is designed to explore the benefits of both centralized and decentralized communications, e.g. the control of the processes and strategies of the centralized model as well as their benefit from the scalability and security of the decentralized approach. Guo et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">guo2022hybrid</span> </a></cite><span id="S4.I1.i2.I1.i1.I1.i3.p1.1.5" class="ltx_text" style="color:#000000;"> proposed a new algorithm called Hybrid Local SGD (HL-SGD) that uses both device-to-device (D2D) and device-to-server (D2S) communication. The authors concluded that this approach has accelerated the FL. However, the disadvantage of hybrid communication lies in the complex conception and implementation of these types of communication, as it combines many complex and different communication protocols.</span></p>
</div>
</li>
</ul>
</div>
</li>
<li id="S4.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Data Partition:</span><span id="S4.I1.i2.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;"> is a key aspect of FL, and it depends on the feature and sample space of the data parties involved. There are three categories: horizontal, vertical and federated transfer learning. Here is a brief explanation of each category:</span></p>
</div>
<div id="S4.I1.i2.I1.i2.p2" class="ltx_para">
<ul id="S4.I1.i2.I1.i2.I1" class="ltx_itemize">
<li id="S4.I1.i2.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i2.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i2.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Horizontal: </span><span id="S4.I1.i2.I1.i2.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;">is an approach to training ML models on distributed datasets. The idea is to split the dataset by rows and store it in each participating client. For example, in this case, 03 hospitals were created. Each hospital’s data has the same feature space but with different patients (even the same patient can have different IDs). Google Keyboard used this type of learning as the first use case of FL, where the participating mobile phones have the same functions with different training data </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mammen2021federated</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i1.p1.1.5" class="ltx_text" style="color:#000000;">. The main advantage of this method is that it can help address privacy concerns by storing the data at local hospitals. This can also reduce the amount of data to be sent, which is beneficial for hospitals with little computing power.</span></p>
</div>
</li>
<li id="S4.I1.i2.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i2.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i2.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Vertical:</span><span id="S4.I1.i2.I1.i2.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;"> is a type of data partition that enables collaborative ML on vertically partitioned data while protecting privacy </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">gu2021privacy</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i2.p1.1.5" class="ltx_text" style="color:#000000;">. The goal is to train a model on the features of multiple parties’ data without explicitly sharing the raw data of parties </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">das2021multi</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i2.p1.1.8" class="ltx_text" style="color:#000000;">. VFL divides a neural network between different parties and a server. Each party trains its local models based on their respective functions, and the server coordinates communication between the parties to generate a global model. The parties exchange encrypted model updates between each other and the server aggregates the updates to generate a global model </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2019parallel</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i2.p1.1.11" class="ltx_text" style="color:#000000;">. The biggest challenges of vertical FL include: Security and privacy; system heterogeneity and selection of important participants </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wei2022vertical</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i2.p1.1.14" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S4.I1.i2.I1.i2.I1.i2.p2" class="ltx_para">
<p id="S4.I1.i2.I1.i2.I1.i2.p2.1" class="ltx_p"><span id="S4.I1.i2.I1.i2.I1.i2.p2.1.1" class="ltx_text" style="color:#000000;">An example of the vertical architecture is shown in Fig. </span><a href="#S4.F4" title="Figure 4 ‣ 2nd item ‣ item C2. ‣ 4.1.1 Classification (C) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.I1.i2.I1.i2.I1.i2.p2.1.2" class="ltx_text" style="color:#000000;">, where the data of cancer patients is divided vertically as the features are divided based on the patient’s personal data, details of the cancer disease (breast, melanoma, etc.), medical images (MRI, CT scans…) and finally medical analysis of the patients (Anatomical pathology, blood sugar…)</span></p>
</div>
</li>
<li id="S4.I1.i2.I1.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i2.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i2.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i2.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Federated Transfer Learning:</span><span id="S4.I1.i2.I1.i2.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;"> is an approach in FL that uses TL mechanism for learning aggregation sequentially </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib97" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ambesange2023simulating</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i3.p1.1.5" class="ltx_text" style="color:#000000;">. Fig. </span><a href="#S4.F5" title="Figure 5 ‣ 2nd item ‣ item C2. ‣ 4.1.1 Classification (C) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.I1.i2.I1.i2.I1.i3.p1.1.6" class="ltx_text" style="color:#000000;"> represent a visual representation of FTL. This involved transferring knowledge from a pre-trained model to a new model trained on a distributed dataset. The model of the target domain client is built using the diagnostic knowledge of the source domain clients and uploaded to the central server </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib98" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2023federated</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i3.p1.1.9" class="ltx_text" style="color:#000000;">. The steps to realize FTL includes : simulation of distributed data; simulation of FTL mechanism; selection and pre-training of the proposed model; local data processing for model training and aggregating model learning at the master node </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib97" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ambesange2023simulating</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i3.p1.1.12" class="ltx_text" style="color:#000000;">. There are many advantages of FTL, some of them are: facilitating knowledge transfer with data protection, simultaneously addressing collaborative training, data protection and domain movement problems </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2023federated</span> </a></cite><span id="S4.I1.i2.I1.i2.I1.i3.p1.1.15" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</li>
</ul>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2405.20126/assets/x6.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="391" height="265" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.7.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.8.2" class="ltx_text" style="font-size:90%;">The process of a round of FL based on a vertical partition architecture for cancer detection <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2019federated</span> </a></cite></span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2405.20126/assets/x7.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="223" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.5.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.6.2" class="ltx_text" style="font-size:90%;">The process of making a FTL for cancer detection</span></figcaption>
</figure>
</li>
<li id="S4.I1.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Aggregation strategies:</span><span id="S4.I1.i2.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;"> are a crucial aspect of FL and involve combining different customers’ local models to create a global model. There are different aggregation strategies in FL, some of them are:</span></p>
<ul id="S4.I1.i2.I1.i3.I1" class="ltx_itemize">
<li id="S4.I1.i2.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i3.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i3.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i3.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">FedMA: </span><span id="S4.I1.i2.I1.i3.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;">is a type of ML algorithm that helps build a common global model by matching and averaging hidden elements in each layer. This algorithm aims to address the heterogeneity of data and reduce communication congestion </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2020federated</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i1.p1.1.5" class="ltx_text" style="color:#000000;">. The FedMA approach aims to achieve device customization and high customer accuracy of one’s own data while maintaining a high level of diversity </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib103" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ek2020evaluation</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i1.p1.1.8" class="ltx_text" style="color:#000000;">. The FedMA algorithm has several advantages over other FL approaches. Here are two key advantages: Efficient use of local models and addressing data bias </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2020federated</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i1.p1.1.11" class="ltx_text" style="color:#000000;">. One of the key limitations of FedMA is its reliance on well-trained local models. FedMA does not perform well in scenarios where customers have limited computing resources or insufficient training data. Another limitation is the limited applicability to certain neural network architectures (Long short term memory (LSTM) and CNN).</span></p>
</div>
</li>
<li id="S4.I1.i2.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i3.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i3.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i3.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">FedSGD:</span><span id="S4.I1.i2.I1.i3.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;"> is a federated averaging algorithm that combines local Stochastic Gradient Descent (SGD) on each client with a server that performs model averaging </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib101" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mcmahan2017communication</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i2.p1.1.5" class="ltx_text" style="color:#000000;">. The algorithm selects clients and calculates the loss gradient over their local data using SGD. The gradients are then sent to the server where they are averaged to produce a global gradient update. This process is repeated over several rounds until convergence is achieved </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib101" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mcmahan2017communication</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i2.p1.1.8" class="ltx_text" style="color:#000000;">. Shin et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib105" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">shin2022fedvar</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i2.p1.1.11" class="ltx_text" style="color:#000000;"> concluded that FedSGD works reasonably well with independent and identically d istributed (IID) or semi-IID data, but its performance degrades as data heterogeneity increases. Thonglek et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">thonglek2020federated</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i2.p1.1.14" class="ltx_text" style="color:#000000;"> noted that FedSGD enables decentralized private training, but suffers from high communication costs and difficulties in dealing with heterogeneous devices/data. So, FedSGD is a federated averaging framework that aims to address the challenges of heterogeneity in federated networks.</span></p>
</div>
</li>
<li id="S4.I1.i2.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I1.i2.I1.i3.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I1.i2.I1.i3.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i3.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">FedAvg: </span><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;">is a synchronous-distributed optimization algorithm for FL to address the problem of communication efficiency in FL </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib100" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2019convergence</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.5" class="ltx_text" style="color:#000000;">. FedAvg’s process is to divide the training process into multiple rounds. In each round, the algorithm selects the clients who participated in the training. These clients then downloaded the model from the server and applied the training to their local data. Once training is finished, the weights and metrics are sent back to the server so that they are summarized in a global model. The same process is carried out in the new round of the algorithm until the maximum number of rounds is reached </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib101" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mcmahan2017communication</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.8" class="ltx_text" style="color:#000000;">. This aggregation process averages the weights of the local models, giving each device or node an equal say in the final model. One of the main advantages of FedAvg is that it allows training ML models on distributed datasets without the need to transfer data to a central server. This helps address privacy concerns and can also reduce the amount of data that needs to be transferred. Fig. </span><a href="#S4.F6" title="Figure 6 ‣ 4.1.1 Classification (C) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.9" class="ltx_text" style="color:#000000;"> demonstrates the process of FL using FedAvg algorithm. Additionally, FedAvg addresses issues related to data heterogeneity </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">darzidehkalani2022federated</span> </a>; <a href="#bib.bib102" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">casella2023benchmarking</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.12" class="ltx_text" style="color:#000000;">. Fig. </span><a href="#S4.F7" title="Figure 7 ‣ 4.1.1 Classification (C) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">7</span></a><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.13" class="ltx_text" style="color:#000000;"> is the result of applying different aggregation strategies to REALWORLD datasets, where FedAvg performed the best </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib103" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ek2020evaluation</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.16" class="ltx_text" style="color:#000000;">. However, one limitation of the FedAvg algorithm is that it can be affected by slow or unresponsive devices. Especially if there are many client devices with weak computing power </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib104" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">gu2021fast</span> </a></cite><span id="S4.I1.i2.I1.i3.I1.i3.p1.1.19" class="ltx_text" style="color:#000000;">, this can affect the functionality of the algorithm.</span></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2405.20126/assets/x8.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="200" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.7.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.8.2" class="ltx_text" style="font-size:90%;">The process of FL using the FedAvg averaging algorithm <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib100" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2019convergence</span> </a></cite></span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2405.20126/assets/x9.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="185" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.7.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.8.2" class="ltx_text" style="font-size:90%;">Comparative study between centralized and FL accuracy results on the REALWORLD dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib103" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ek2020evaluation</span> </a></cite></span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Deep Learning models (D) </h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p"><span id="S4.SS1.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">DL has become a game changer in computer vision, enabling computers to analyze and understand visual data with extreme accuracy. DL involves training neural networks on large datasets with a large number of features. There are two important categories of DL in computer vision:</span></p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I2.i1.1.1.1" class="ltx_text ltx_font_bold">D1.</span></span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Convolutional Neural Networks (CNN)</span><span id="S4.I2.i1.p1.1.2" class="ltx_text" style="color:#000000;">: CNNs are among the most commonly used neural network types in DL for images. These models are often used to teach computers to analyze images in a similar way to humans. This section examines the most widely used computer vision CNN models for medical imaging:</span></p>
<ul id="S4.I2.i1.I1" class="ltx_itemize">
<li id="S4.I2.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.I1.i1.p1" class="ltx_para">
<p id="S4.I2.i1.I1.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Inception-V3: </span><span id="S4.I2.i1.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;">is a CNN model used for image classification tasks developed by Google researchers. Inception-V3 is based on the Inception architecture, which uses conventional layers having different filter sizes to extract image features (tumors for CD) </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib107" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ahmed2023inception</span> </a></cite><span id="S4.I2.i1.I1.i1.p1.1.5" class="ltx_text" style="color:#000000;">. Inception-V3 has been integrated in developing various DL models for CD. Li et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib108" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2021deep</span> </a></cite><span id="S4.I2.i1.I1.i1.p1.1.8" class="ltx_text" style="color:#000000;"> have built a model based on Inception-V3, R-CNN and S-Mask to improve the segmentation and classification of prostate ultrasound images, which helped in cancer diagnosis. Dong et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib109" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">dong2020inception</span> </a></cite><span id="S4.I2.i1.I1.i1.p1.1.11" class="ltx_text" style="color:#000000;"> proposed a cell recognition algorithm that combined Inception-V3 and artificial features extraction for the classification of cervical cancer cell. This model provided effective methods for the diagnosis of the cervical cancer methods. Demir et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib110" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">demir2019early</span> </a></cite><span id="S4.I2.i1.I1.i1.p1.1.14" class="ltx_text" style="color:#000000;"> used Inception-V3 and ResNet-101 for skin CD. The authors trained the models on 2437 training images, 600 test images and 200 validation images. The accuracy of the Inception-V3 model was 87%. All this researches mentioned before showed that Inception model have given great results in CD based on image analysis, which ultimately improves the probability of successful therapy.</span></p>
</div>
</li>
<li id="S4.I2.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.I1.i2.p1" class="ltx_para">
<p id="S4.I2.i1.I1.i2.p1.1" class="ltx_p"><span id="S4.I2.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">VGG-16: </span><span id="S4.I2.i1.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;">is a pre-trained DL model used for medical image classification tasks. This architecture is composed of a total of sixteen layers, thirteen of which are convolutional layers and the remaining three are fully connected layers. The function of the convolutional layers in a neural network is to extract characteristic from the input layers, while fully connected layers are used for classification </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sharma2023deep</span> </a></cite><span id="S4.I2.i1.I1.i2.p1.1.5" class="ltx_text" style="color:#000000;">. Various CNN models for detecting cancer have been developed using integration of VGG-16 models. Santos-Bustos et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">santos2022towards</span> </a></cite><span id="S4.I2.i1.I1.i2.p1.1.8" class="ltx_text" style="color:#000000;"> presented an approach of uveal melanoma, a type of eye cancer. The two CNN architectures that were employed by the authors are VGG-16 and ResNet-18. The performance of the models have been assessed through various configurations and data augmentation techniques. The results indicated that the suggested method outperformed previous models obtaining 99% accuracy and 98.4% precision in identifying uveal melanoma. Although, the models were trained on a relatively small dataset (150 healthy and 33 unhealthy). Sharma et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sharma2023deep</span> </a></cite><span id="S4.I2.i1.I1.i2.p1.1.11" class="ltx_text" style="color:#000000;"> proposed a new method for pneumonia classification and prediction, the approach was to combine VGG-16 with Neural Networks.
VGG-16 was used in this article as a pre-trained feature extractor in the proposed method for pneumonia prediction in CXR images. The model can transfer the learned features from the large dataset of images so that the accuracy of pneumonia prediction can be improved. VGG-16 has also been used in detection of lung cancer, in </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib113" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">pandian2022detection</span> </a></cite><span id="S4.I2.i1.I1.i2.p1.1.14" class="ltx_text" style="color:#000000;"> the authors proposed an algorithm using pre-trained neural networks, specifically GoogleNet and VGG16 network for the purpose of classifying various types of lung cancer. These types include Adenocarcinoma, Large Cell Carcinoma, and Squamous Cell Carcinoma from normal lung images with an overall accuracy of 98%. Fig. </span><a href="#S4.F8" title="Figure 8 ‣ item D1. ‣ 4.1.2 Deep Learning models (D) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">8</span></a><span id="S4.I2.i1.I1.i2.p1.1.15" class="ltx_text" style="color:#000000;"> shows the architecture and the role of each layers in VGG16.</span></p>
</div>
</li>
<li id="S4.I2.i1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.I1.i3.p1" class="ltx_para">
<p id="S4.I2.i1.I1.i3.p1.1" class="ltx_p"><span id="S4.I2.i1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">ResNet-50: </span><span id="S4.I2.i1.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;">is a CNN architecture specifically designed for image recognition, particularly medical image analysis. ResNet-50 consists of five sets of 49 convolutional layers and a single fully connected layer. The convolution kernels used in this architecture have dimensions of 1x1, 3x3 and 5x5 and are used to extract image features at different resolutions </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib114" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wu2023improved</span> </a></cite><span id="S4.I2.i1.I1.i3.p1.1.5" class="ltx_text" style="color:#000000;">. The dimensionality of the output of the fully connected layer is directly related to the number of categories. Several CNN architectures have been developed to detect cancer by integrating ResNet-50 models. Sarwinda et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib115" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sarwinda2021deep</span> </a></cite><span id="S4.I2.i1.I1.i3.p1.1.8" class="ltx_text" style="color:#000000;"> proposed a method for detecting colorectal cancer using ResNet-18 and ResNet-50 models. The authors used a dataset of 165 images of the intestinal glands, consisting of 74 benign tumors and 91 malignant tumors. Therefore, based on the results presented in the article, the ResNet-50 model was the best model that gave the best results. Ikechukwu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib116" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ikechukwu2021resnet</span> </a></cite><span id="S4.I2.i1.I1.i3.p1.1.11" class="ltx_text" style="color:#000000;"> conducted experiments using ResNet-50 as a pre-trained model to extract pneumonia images from normal chest X-ray images. The research showed that the validation accuracy of ResNet-50 was higher compared to other traditional techniques due to the effectiveness of using pre-trained models. Bütün et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib117" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">butun2023automatic</span> </a></cite><span id="S4.I2.i1.I1.i3.p1.1.14" class="ltx_text" style="color:#000000;"> stated that ResNet-50 achieved 98.54% classification accuracy in detecting cancer metastasis in lymph node images. However, the authors stated that the accuracy of ResNet architectures increases with increasing network depth, resulting in ResNet-101 providing the best results.</span></p>
</div>
</li>
<li id="S4.I2.i1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.I1.i4.p1" class="ltx_para">
<p id="S4.I2.i1.I1.i4.p1.1" class="ltx_p"><span id="S4.I2.i1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Xception: </span><span id="S4.I2.i1.I1.i4.p1.1.2" class="ltx_text" style="color:#000000;">is a deep CNN inspired by the Inception architecture. It represents a sequence of convolution layers characterized by depth separability and complemented by residual connections, which facilitates the definition and modification of the layers </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib118" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chollet2017xception</span> </a></cite><span id="S4.I2.i1.I1.i4.p1.1.5" class="ltx_text" style="color:#000000;">. Panthakkan et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib119" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">panthakkan2022concatenated</span> </a></cite><span id="S4.I2.i1.I1.i4.p1.1.8" class="ltx_text" style="color:#000000;"> proposed X-R50, a hybrid model that combined the Xception and ResNet-50 networks. The role of the Xception architecture in this study was to capture fine-grained features that helped improve the results of accurate skin CD. Upasana et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib120" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">upasana2023attention</span> </a></cite><span id="S4.I2.i1.I1.i4.p1.1.11" class="ltx_text" style="color:#000000;"> have developed a model for detecting pneumothorax in chest X-ray images by integrating the Xception neural network with an attention module. The proposed model was tested on 2,597 chest X-ray images and achieved a training accuracy of 99.18%, a validation accuracy of 87.53%, and an average AUC of 90.00%. Sharma et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib121" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sharma2022xception</span> </a></cite><span id="S4.I2.i1.I1.i4.p1.1.14" class="ltx_text" style="color:#000000;"> proposed an approach using TL with the Xception model for feature extraction and implemented a support vector machine (SVM) with Radial Basis Function kernel to perform classification of histopathological images in the context of breast CD. The study showed that the proposed Xception+SVM R,5 model outperformed other competing DL models in terms of classification accuracy. Xception used depth-separable convolutions, which showed superior computational efficiency compared to traditional convolutions. Therefore, the algorithm should be made more efficient when handling and processing large-scale medical images.</span></p>
</div>
</li>
<li id="S4.I2.i1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.I1.i5.p1" class="ltx_para">
<p id="S4.I2.i1.I1.i5.p1.1" class="ltx_p"><span id="S4.I2.i1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">MobileNet: </span><span id="S4.I2.i1.I1.i5.p1.1.2" class="ltx_text" style="color:#000000;">is an architecture specifically designed for computing efficiency and optimized for development on mobile and embedded devices with limited computing resources. This was achieved by using depth-separable convolutions </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib122" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2021mmf</span> </a></cite><span id="S4.I2.i1.I1.i5.p1.1.5" class="ltx_text" style="color:#000000;">. Mothkur et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib123" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mothkur2023classification</span> </a></cite><span id="S4.I2.i1.I1.i5.p1.1.8" class="ltx_text" style="color:#000000;"> proposed a lung nodule classification approach based on deep hybrid learning to improve the accuracy of lung CD. The proposed approach leveraged MobileNet, which has proven that lung lesions with low memory requirements can be accurately classified as malignant or benign. Zhao et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib124" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhao2022machine</span> </a></cite><span id="S4.I2.i1.I1.i5.p1.1.11" class="ltx_text" style="color:#000000;"> proposed a new method using the Mobilenet network structure for non-invasive classification of non-small cell lung cancer based on 18F-FDG PET/CT images. The MobileNet model showed good classification performance and can be used as a noninvasive technique to classify pathological subtypes of NSCLC. All of these studies have shown that MobileNet is efficient in classifying cancers without consuming important material resources.</span></p>
</div>
</li>
</ul>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2405.20126/assets/x10.png" id="S4.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.7.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.8.2" class="ltx_text" style="font-size:90%;">VGG16 CNN architecture and layers for CD with two classes (malicious and benign) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sharma2023deep</span> </a>; <a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">santos2022towards</span> </a></cite></span></figcaption>
</figure>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I2.i2.1.1.1" class="ltx_text ltx_font_bold">D2.</span></span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Transformers: </span><span id="S4.I2.i2.p1.1.2" class="ltx_text" style="color:#000000;"> are a type of neural network architecture based on self-attention mechanisms to model sequential data. They were first introduced for natural language processing tasks </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib125" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">khan2022transformers</span> </a></cite><span id="S4.I2.i2.p1.1.5" class="ltx_text" style="color:#000000;">. There has been some recent work in the field of medical imaging using transformers to detect cancer in histopathological or radiological images </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib126" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">he2023transformers</span> </a></cite><span id="S4.I2.i2.p1.1.8" class="ltx_text" style="color:#000000;">. This section lists the most commonly used transformer models for CD based on medical image analysis:</span></p>
<ul id="S4.I2.i2.I1" class="ltx_itemize">
<li id="S4.I2.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I2.i2.I1.i1.p1.1" class="ltx_p"><span id="S4.I2.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Vision Transformers (ViTs):</span><span id="S4.I2.i2.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;"> are transformative models that combine image analysis and self-attention-based architectures. They have revolutionized the field of computer vision and achieved excellent results in various tasks using the mechanism of self-attention. ViTs handle scalability well and are better than CNNs in terms of accuracy with more parameters, data and computing power. For example, ViT-G, a ViT model with 2B parameters, achieved an ImageNet accuracy of 90.45%, outperforming CNNs </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib127" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhai2022scaling</span> </a></cite><span id="S4.I2.i2.I1.i1.p1.1.5" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S4.I2.i2.I1.i1.p2" class="ltx_para">
<p id="S4.I2.i2.I1.i1.p2.1" class="ltx_p"><span id="S4.I2.i2.I1.i1.p2.1.1" class="ltx_text" style="color:#000000;">In medical image analysis, an input image is divided into smaller patches, which are processed to obtain patch embeddings. Position encodings are added to extract spatial information. Patch embeddings are then fed to a multi-layer transformer encoder, each containing a multi-head self-attention (MSA) block and a multi-layer perceptron (MLP) block. The MSA block detects global dependencies between the image patches, while the MLP block enables the ViT model to learn patterns and improve the encoding of image patches to outputs. After several transformer-encoder layers, a global feature representation of the complete image is obtained and passed to the global encoder. The global encoder aggregates the encoded patch embeddings and models relationships between all patches. A sequence is output that represents the full image with global context. This sequence is then passed to the classifier head, which predicts a label for the medical image </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">tyagi2023amalgamation</span> </a>; <a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2023vision</span> </a>; <a href="#bib.bib128" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2023vision</span> </a></cite><span id="S4.I2.i2.I1.i1.p2.1.4" class="ltx_text" style="color:#000000;">. This entire process is explained in Fig. </span><a href="#S4.F9" title="Figure 9 ‣ 2nd item ‣ item D2. ‣ 4.1.2 Deep Learning models (D) ‣ 4.1 Overview ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">9</span></a><span id="S4.I2.i2.I1.i1.p2.1.5" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S4.I2.i2.I1.i1.p3" class="ltx_para">
<p id="S4.I2.i2.I1.i1.p3.1" class="ltx_p"><span id="S4.I2.i2.I1.i1.p3.1.1" class="ltx_text" style="color:#000000;">ViTs have been used in several CD studies, Li et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2023vision</span> </a></cite><span id="S4.I2.i2.I1.i1.p3.1.4" class="ltx_text" style="color:#000000;"> proposed ViT-WSI, a ViT model for weakly supervised learning on whole slide images of brain tumor histopathology. For this study, the authors used multiple datasets such as internal brain tumor, TCGA and molecular biomarker datasets. ViT-WSI achieved a Macro AUC of 94.1% on over 5,000 primary brain tumor slides, outperforming other methods such as CNN classifiers. Although the authors pointed out that the limitation of the approach is the high memory usage during attribution analysis. Andrade-Miranda et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib129" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">andrade2022pure</span> </a></cite><span id="S4.I2.i2.I1.i1.p3.1.7" class="ltx_text" style="color:#000000;"> conducted a comparative study of various ViT models on BraTS2021 and found that the best performing model was the hybrid MCNN+ViTv-B/1 (combining CNN layers for feature extraction with Transformer-Layers for modeling long-range dependencies) with an average Dice of 91.1% across 1251 3D MRI scans. The pure transformer models performed significantly worse than hybrid models with an average Dice of 87.3%. Lu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib130" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">xu2022brain</span> </a></cite><span id="S4.I2.i2.I1.i1.p3.1.10" class="ltx_text" style="color:#000000;"> proposed a deep anchor attention learning (DAA) that uses a ViT and anchor-based attention to predict survival of brain tumor patients from MRI scans. DAA achieved C-index values of 0.69-0.70 when applied to 326 MRI scans from the BraTS 2020. Therefore, although the dataset size for DL methods is still relatively small, the C-index value is not high enough. In this study, ViTs performed better than using Resnet-18 for feature extraction.</span></p>
</div>
<div id="S4.I2.i2.I1.i1.p4" class="ltx_para">
<p id="S4.I2.i2.I1.i1.p4.1" class="ltx_p"><span id="S4.I2.i2.I1.i1.p4.1.1" class="ltx_text" style="color:#000000;">All of these studies conclude that ViTs are important for CD. The use of ViTs in CD has shown promising results and could be a valuable tool to improve cancer diagnosis.</span></p>
</div>
</li>
<li id="S4.I2.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I2.i2.I1.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Swin Transformer:</span><span id="S4.I2.i2.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;"> is a type of neural network architecture based on Transformers, suitable for processing images. It was created in 2021 by Microsoft Research Asia </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib131" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">xie2021melanoma</span> </a></cite><span id="S4.I2.i2.I1.i2.p1.1.5" class="ltx_text" style="color:#000000;">. Swin Transformers leverage a window-based self-attention mechanism to reduce computational complexity and model cross-window relationships </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib132" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2021swin</span> </a></cite><span id="S4.I2.i2.I1.i2.p1.1.8" class="ltx_text" style="color:#000000;">. They have been successfully used in pre-training models for 3D medical image analysis and medical image segmentation tasks </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib133" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jiang2022swinbts</span> </a>; <a href="#bib.bib134" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">karthik2023dual</span> </a></cite><span id="S4.I2.i2.I1.i2.p1.1.11" class="ltx_text" style="color:#000000;">. Swin Transformers bring the power of self-attention modeling to convolutional architectures like UNet to detect global context and long-range dependencies in images. To this end, many researchers have used Swin Transformers as feature extraction to improve medical image segmentation, such as Igbal et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib135" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">iqbal2023bts</span> </a></cite><span id="S4.I2.i2.I1.i2.p1.1.14" class="ltx_text" style="color:#000000;"> proposed BTS-ST from breast tumor segmentation, Swin Transformer was used as an encoder to extract global context features from the input images. BTS-ST achieved an F1 score of 90.8% in ultrasound and an F1 score of 85.6% for classification in an ultrasound dataset from Shantou University Hospital. However, the dataset size was relatively small (42 patients). Zidan et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib136" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zidan2023swincup</span> </a></cite><span id="S4.I2.i2.I1.i2.p1.1.17" class="ltx_text" style="color:#000000;"> proposed SwinCup, a segmentation model that uses Swin Transformers as encoders in an encoder-decoder architecture for two colon histology datasets. SwinCup achieved an average F1 score of 90% and an F1 score of 89% on the CRAG dataset, although more experience with larger datasets is an advantage would have been. Masood et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">masood2023multi</span> </a></cite><span id="S4.I2.i2.I1.i2.p1.1.20" class="ltx_text" style="color:#000000;"> proposed a modified Swin Transformer architecture ST-MSMLFFR for lung tumor segmentation in CT scans. The authors introduced a two-branch encoder with separate local and global feature extraction paths. ST-MSMLFFR achieved 96% Dice on the LIDC-IDRI dataset, outperforming UNet (82% Dice). Zou et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib137" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zou2023swine</span> </a></cite><span id="S4.I2.i2.I1.i2.p1.1.23" class="ltx_text" style="color:#000000;"> proposed a contribution to improve medical image segmentation by exploiting Swin Transformer’s self-attention on the ISIC 2017 skin lesion dataset with 3029 CT scan slices from 108 cases. 88.47% Dice and 92.29% precision were achieved </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib137" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zou2023swine</span> </a></cite><span id="S4.I2.i2.I1.i2.p1.1.26" class="ltx_text" style="color:#000000;">. But the authors only trained their proposed model on two datasets, the performance of the model on other datasets was not discussed.</span></p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2405.20126/assets/x11.png" id="S4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="185" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.7.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.8.2" class="ltx_text" style="font-size:90%;">ViT architecture and layers with TL for CD in two classes (malignant and benign)<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib125" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">khan2022transformers</span> </a>; <a href="#bib.bib129" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">andrade2022pure</span> </a></cite></span></figcaption>
</figure>
</li>
<li id="S4.I2.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.I1.i3.p1" class="ltx_para">
<p id="S4.I2.i2.I1.i3.p1.1" class="ltx_p"><span id="S4.I2.i2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">TransUNet:</span><span id="S4.I2.i2.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;">  is a DL architecture for medical image segmentation that combines a transformer encoder with a U-Net decoder. It was developed by Chen et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib138" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chen2021transunet</span> </a></cite><span id="S4.I2.i2.I1.i3.p1.1.5" class="ltx_text" style="color:#000000;"> in 2021. The approach combined the ability of Transformers to globally extract the high-level relationships between image components with the precise localization ability of U-Nets. The encoder used a ViT to encode image patches and the decoder used U-Net to recover spatial details </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib139" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">castro2022u</span> </a></cite><span id="S4.I2.i2.I1.i3.p1.1.8" class="ltx_text" style="color:#000000;">. Many researches have been conducted on CD. For example, Wang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib140" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2022accurate</span> </a></cite><span id="S4.I2.i2.I1.i3.p1.1.11" class="ltx_text" style="color:#000000;"> used TransUNet to classify lung nodules on CT scans into benign and malignant categories. The method was evaluated on the LIDC-IDRI dataset of 8,474 CT images and achieved a specificity of 93.17% and an AUC of 86.2% on the test set, outperforming other methods such as stacked autoencoders and CNNs. However, the authors did not discuss limitations of the proposed TransUnit approach. Chen et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib141" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chen2022brain</span> </a></cite><span id="S4.I2.i2.I1.i3.p1.1.14" class="ltx_text" style="color:#000000;"> proposed an approach that combined TransUNet with a convolutional block attention module (CBAM). This enabled both local features and global contextual information to be captured for accurate segmentation. Experiments were conducted on the BraTS 2021 dataset, which contained multimodal MRI scans of brain tumor patients. The proposed TransUNet achieved Dice scores of 93.08% for the whole tumor, 91.49% for the tumor core, and 87.76% for the tumor enhancement. This has outperformed other methods such as 3D UNet, Swin UNETR and VT-UNet. Wang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2022transunet</span> </a></cite><span id="S4.I2.i2.I1.i3.p1.1.17" class="ltx_text" style="color:#000000;"> used TransUNet with a refined loss to achieve better segmentation performance of brain tumors on MRI images. In this approach, CBAM was used for the upsampling part of the U-Net architecture. This helped the model focus on the most important features in the images when upsampling. In experiments, TransUNet achieved a higher Dice similarity coefficient (86.4%) compared to regular U-Net (81.1%) and TransUNet without CBAM (65.3%) in 3929 brain MRI cases. Kaggle datasets. Chen et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib142" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chen2021mt</span> </a></cite><span id="S4.I2.i2.I1.i3.p1.1.20" class="ltx_text" style="color:#000000;"> experimentally trained TransUNet on three public datasets - ISIC-2017, ISIC Additional and PH2 for skin lesion segmentation and classification, achieving a Dice score of 87%. However, the authors stated that performance degrades when the images are obscured (e.g. by hair…).</span></p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Frameworks of FL for ML Applications (F) </h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p"><span id="S4.SS1.SSS3.p1.1.1" class="ltx_text" style="color:#000000;">There are several open source frameworks available for implementing FL for ML applications that provide the necessary tools and infrastructure. Choosing the right framework depends on the specific use case and the requirements of the project. This comparison highlights the different characteristics and capabilities of each study’s approach to FL and TL for CD, as well as the differences between current solutions in this area. Here are some popular frameworks for FL:</span></p>
<ol id="S4.I3" class="ltx_enumerate">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I3.i1.1.1.1" class="ltx_text ltx_font_bold">F1.</span></span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p"><span id="S4.I3.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">OpenFL: </span><span id="S4.I3.i1.p1.1.2" class="ltx_text" style="color:#000000;">is an open source software library designed for FL applications. The library allows developers to build an ML model and train it on remote data owners or collaborating site nodes. The development of OpenFL is the result of a collaboration between Intel Labs and the prestigious University of Pennsylvania </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib143" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">foley2022openfl</span> </a></cite><span id="S4.I3.i1.p1.1.5" class="ltx_text" style="color:#000000;">. By using OpenFL, companies can jointly train AI models while ensuring the protection of sensitive information and simplifying data sharing challenges. Reina et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib144" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">reina2021openfl</span> </a></cite><span id="S4.I3.i1.p1.1.8" class="ltx_text" style="color:#000000;"> used OpenFL to train a consensus ML model used to identify and evaluate the boundaries of brain tumors. Foley et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib143" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">foley2022openfl</span> </a></cite><span id="S4.I3.i1.p1.1.11" class="ltx_text" style="color:#000000;"> used OpenFL to ensure the privacy and security of a DL model to predict the likelihood of acute respiratory distress syndrome and death in COVID-19. All of these researchers concluded that the use of OpenFL secured the collection of datasets from multiple sources. The diversity of the dataset improved the accuracy and reduced the bias of the DL model.</span></p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I3.i2.1.1.1" class="ltx_text ltx_font_bold">F2.</span></span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p"><span id="S4.I3.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Fed-BioMed:</span><span id="S4.I3.i2.p1.1.2" class="ltx_text" style="color:#000000;"> is an open source Python initiative specializing in applying FL to real-world medical applications </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib145" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">fedbio</span> </a></cite><span id="S4.I3.i2.p1.1.5" class="ltx_text" style="color:#000000;">. It facilitated ML model training on decentralized data without the need for data sharing. Fed-BioMed can be used in healthcare for various purposes, such as dimensionality reduction in multicenter structural brain imaging data from different geographical locations </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib146" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">silva2020fed</span> </a>; <a href="#bib.bib147" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">khan2023federated</span> </a></cite><span id="S4.I3.i2.p1.1.8" class="ltx_text" style="color:#000000;"> and analysis of MRI data for prostate cancer diagnosis </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib148" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jablecki2021federated</span> </a></cite><span id="S4.I3.i2.p1.1.11" class="ltx_text" style="color:#000000;">. Cremonesi et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib149" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">cremonesi2023fed</span> </a></cite><span id="S4.I3.i2.p1.1.14" class="ltx_text" style="color:#000000;"> presented results from implementing and training a prostate segmentation model with FL-Biomed, using data from three different medical institutions. Fed-BioMed also addressed many statistical and systems challenges in the field of biomedical research, demonstrating its capabilities for real-world applications of FL in hospital networks.</span></p>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Security and privacy (S)</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p id="S4.SS1.SSS4.p1.1" class="ltx_p"><span id="S4.SS1.SSS4.p1.1.1" class="ltx_text" style="color:#000000;">Security and privacy are crucial in FL because the integrity of the model and the privacy of participants can be attacked by various types of attacks. Below are some of the security and privacy challenges and suggested methods to mitigate these attacks in FL:</span></p>
<ol id="S4.I4" class="ltx_enumerate">
<li id="S4.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I4.i1.1.1.1" class="ltx_text ltx_font_bold">S1.</span></span> 
<div id="S4.I4.i1.p1" class="ltx_para">
<p id="S4.I4.i1.p1.1" class="ltx_p"><span id="S4.I4.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Privacy attacks: </span><span id="S4.I4.i1.p1.1.2" class="ltx_text" style="color:#000000;">While FL represents an advance toward collaborative learning with privacy, significant research challenges remain. One of the biggest privacy challenges is Gradient Inversion </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib150" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">hatamizadeh2023gradient</span> </a></cite><span id="S4.I4.i1.p1.1.5" class="ltx_text" style="color:#000000;">. While FL avoids sharing private raw data, the model updates exchanged during training could potentially reveal sensitive information about the training data. Another challenge is the reconstruction model attack. An example of this attack is the Generative Adversarial Network (GAN). GAN-based attacks are a type of reconstruction attack that aims to reproduce private training data using GAN. The latter are trained using model updates or gradients from victim patients as feedback to refine the artificially generated data </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib151" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">nair2023robust</span> </a></cite><span id="S4.I4.i1.p1.1.8" class="ltx_text" style="color:#000000;">. Techniques such as secure multiparty computation </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib152" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhao2019secure</span> </a></cite><span id="S4.I4.i1.p1.1.11" class="ltx_text" style="color:#000000;"> and homomorphic encryption </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib153" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">naehrig2011can</span> </a></cite><span id="S4.I4.i1.p1.1.14" class="ltx_text" style="color:#000000;"> can help mitigate these privacy attacks, but they have limitations. Protecting privacy while maintaining the usefulness and security of the model remains an ongoing challenge.</span></p>
</div>
</li>
<li id="S4.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I4.i2.1.1.1" class="ltx_text ltx_font_bold">S2.</span></span> 
<div id="S4.I4.i2.p1" class="ltx_para">
<p id="S4.I4.i2.p1.1" class="ltx_p"><span id="S4.I4.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Federated model attacks: </span><span id="S4.I4.i2.p1.1.2" class="ltx_text" style="color:#000000;">As FL is increasingly used in medical departments such as healthcare and IoT, securing these systems is critical. Federated models are subject to data poisoning attacks, where malicious participants send manipulated training data to degrade model performance. Ongoing research has been conducted to improve detection capabilities, for example by analyzing model performance changes or client update patterns to identify anomalies. However, new attack strategies are constantly emerging. This section provides an overview of these attacks:</span></p>
<ul id="S4.I4.i2.I1" class="ltx_itemize">
<li id="S4.I4.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I4.i2.I1.i1.p1.1" class="ltx_p"><span id="S4.I4.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Poisoning Sample:</span><span id="S4.I4.i2.I1.i1.p1.1.2" class="ltx_text" style="color:#000000;"> refers to an attack in which malicious participants inject manipulated or biased data into the training process to degrade the performance of the global model. There are three categories of data poisoning attacks: targeted label flipping, random label flipping, and random input data poisoning in an FL environment </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib154" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">gupta2023novel</span> </a></cite><span id="S4.I4.i2.I1.i1.p1.1.5" class="ltx_text" style="color:#000000;">. Many researches proposed data poisoning attacks, for example Kasyap et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib155" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kasyap2023beyond</span> </a></cite><span id="S4.I4.i2.I1.i1.p1.1.8" class="ltx_text" style="color:#000000;"> presented an inverted loss function and anti-training to maximally distort the training data. This technique resulted in the most effective data poisoning attack compared to other poisoning methods. Yang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib156" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2023model</span> </a></cite><span id="S4.I4.i2.I1.i1.p1.1.11" class="ltx_text" style="color:#000000;"> proposed a highly stealthy data poisoning attack called model shuffle attack (MSA), which is an attack that exploits the shuffling/scaling of model parameters to degrade the performance of FL systems. The attack remains undetected by bypassing common defenses. To resolve and detect intoxication attacks, many researches have been conducted, for example by Yang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib157" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2023demac</span> </a></cite><span id="S4.I4.i2.I1.i1.p1.1.14" class="ltx_text" style="color:#000000;"> introduced DeMAC, a novel defense method for FL against model poisoning attacks. By examining gradient norms of client updates, the authors found that DeMAC outperformed existing defense methods based on robust aggregation rules or anomaly detection in mitigating poisoning attacks. Tolpegin et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib158" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">tolpegin2020data</span> </a></cite><span id="S4.I4.i2.I1.i1.p1.1.17" class="ltx_text" style="color:#000000;"> proposed a defense method based on dimensionality reduction. The idea is to extract relevant parameters from high-dimensional updates and use PCA to reduce the dimensionality and visualize the clusters to identify potential poisoning attacks.</span></p>
</div>
</li>
<li id="S4.I4.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I4.i2.I1.i2.p1.1" class="ltx_p"><span id="S4.I4.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Label flipping:</span><span id="S4.I4.i2.I1.i2.p1.1.2" class="ltx_text" style="color:#000000;"> is an adversarial attack technique described in </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib159" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sanchez2023robust</span> </a></cite><span id="S4.I4.i2.I1.i2.p1.1.5" class="ltx_text" style="color:#000000;"> that intentionally changes the labels of some training data samples to incorrect values. This is an easy way for attackers to manipulate their local training data and corrupt the global model in FL. The authors simply swap the labels of some examples in their local data from a selected source class to a selected target class, while leaving the input functions unchanged </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib160" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jebreel2022defending</span> </a></cite><span id="S4.I4.i2.I1.i2.p1.1.8" class="ltx_text" style="color:#000000;">. A slight flip of the label can be very damaging. Flipping less than 10% of the labels can drastically affect model accuracy. Several studies are being conducted to detect label flipping using methods and techniques such as MCDFL </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib161" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jiang2023data</span> </a></cite><span id="S4.I4.i2.I1.i2.p1.1.11" class="ltx_text" style="color:#000000;"> and KPCA and K-Means Clustering </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib162" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2021detection</span> </a></cite><span id="S4.I4.i2.I1.i2.p1.1.14" class="ltx_text" style="color:#000000;">. These techniques have shown promising performance in detecting this type of attack. In summary, label flipping is used to simulate malicious training data poisoning in FL and analyze the robustness of various aggregation algorithms against this attack.</span></p>
</div>
</li>
<li id="S4.I4.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i2.I1.i3.p1" class="ltx_para">
<p id="S4.I4.i2.I1.i3.p1.1" class="ltx_p"><span id="S4.I4.i2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Backdoor attack: </span><span id="S4.I4.i2.I1.i3.p1.1.2" class="ltx_text" style="color:#000000;">This is an attack that allows attackers to hijack FL models for malicious purposes through targeted manipulation during training. They take advantage of the distributed nature of FL, where models from many participants are combined into a global model. An attacker uses some participant models to inject the backdoor </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib163" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">bagdasaryan2020backdoor</span> </a></cite><span id="S4.I4.i2.I1.i3.p1.1.5" class="ltx_text" style="color:#000000;">. Many techniques and methods have been developed to defend backdoor attacks. For example, Zhu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib164" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhu2023adfl</span> </a></cite><span id="S4.I4.i2.I1.i3.p1.1.8" class="ltx_text" style="color:#000000;"> proposed a defense method called adversarial distillation-based backdoor defense (ADFL) that uses GAN-based data augmentation. This method has effectively defended FL against backdoor attacks. Wang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib165" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2023scfl</span> </a></cite><span id="S4.I4.i2.I1.i3.p1.1.11" class="ltx_text" style="color:#000000;"> proposed a method called SCFL. This method works by intelligently selecting the harmless gradients and excluding backdoor gradients, by using singular value decomposition (SVD) and K-Means clustering. Also, Cosine similarity is used to select the cluster that is most likely to contain only harmless gradients for aggregation. Sun et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib166" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sun2019can</span> </a></cite><span id="S4.I4.i2.I1.i3.p1.1.14" class="ltx_text" style="color:#000000;"> conducted several experiments on EMNIST dataset, and found that the defensive norm clipping/thresholding had the best results against backdoor attacks.</span></p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection">4.1.5 </span>Types of cancer (T) </h4>

<div id="S4.SS1.SSS5.p1" class="ltx_para">
<p id="S4.SS1.SSS5.p1.1" class="ltx_p"><span id="S4.SS1.SSS5.p1.1.1" class="ltx_text" style="color:#000000;">Cancer is a group of more than 100 different diseases that can occur almost anywhere in the body. Doctors divide cancer into different types depending on where it starts. Here are some common cancers:</span></p>
<ol id="S4.I5" class="ltx_enumerate">
<li id="S4.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i1.1.1.1" class="ltx_text ltx_font_bold">T1.</span></span> 
<div id="S4.I5.i1.p1" class="ltx_para">
<p id="S4.I5.i1.p1.1" class="ltx_p"><span id="S4.I5.i1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Skin Cancer:</span><span id="S4.I5.i1.p1.1.2" class="ltx_text" style="color:#000000;"> is a pathological condition characterized by the uncontrollable proliferation of abnormal skin cells. This disorder is caused by unrepaired deoxyribonucleic acid (DNA) in skin cells </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib167" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">dildar2021skin</span> </a></cite><span id="S4.I5.i1.p1.1.5" class="ltx_text" style="color:#000000;"> caused by exposure to ultraviolet radiation (UVR). It was the most commonly identified cancer in the United States </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib168" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chang2023nanoparticles</span> </a></cite><span id="S4.I5.i1.p1.1.8" class="ltx_text" style="color:#000000;">. There are several variants of skin cancer, including melanoma, squamous cell carcinoma, basal cell carcinoma and actinic keratoses </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib169" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ferguson2023risk</span> </a></cite><span id="S4.I5.i1.p1.1.11" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S4.I5.i1.p2" class="ltx_para">
<p id="S4.I5.i1.p2.1" class="ltx_p"><span id="S4.I5.i1.p2.1.1" class="ltx_text" style="color:#000000;">Several studies have shown that FL has improved skin cancer detection in various ways. Hashmani el al </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">hashmani2021adaptive</span> </a></cite><span id="S4.I5.i1.p2.1.4" class="ltx_text" style="color:#000000;"> have developed a flexible federated ML-oriented model for skin disease detection, which can be of great help to dermatologists in the preliminary diagnosis of skin malignancies and assessing their severity. Cai et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">cai2021many</span> </a></cite><span id="S4.I5.i1.p2.1.7" class="ltx_text" style="color:#000000;"> proposed a federated deep skin CD model (FDSCDM) that used FL and DualGAN to tackle the challenges of data security and privacy concerns in medical IoT settings. The proposed framework demonstrated exceptional precision and area under the curve (AUC) in detecting skin cancer, while ensuring confidentiality and data privacy.</span></p>
</div>
<div id="S4.I5.i1.p3" class="ltx_para">
<p id="S4.I5.i1.p3.1" class="ltx_p"><span id="S4.I5.i1.p3.1.1" class="ltx_text" style="color:#000000;">Numerous research articles have used the practice of TL in skin CD. Hosny et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib171" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kassem2020skin</span> </a></cite><span id="S4.I5.i1.p3.1.4" class="ltx_text" style="color:#000000;"> used TL and image augmentation to propose a deep CNN to overcome the challenge of requiring a large number of labeled images for training. They showed that this technique outperforms other models in terms of accuracy, sensitivity, specificity and precision. Kondaveeti et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib172" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kondaveeti2020skin</span> </a></cite><span id="S4.I5.i1.p3.1.7" class="ltx_text" style="color:#000000;"> presented an analysis of the contribution of TL to improving the precision of skin lesion image classification in the HAM10000 dataset diagnosis using pre-trained CNNs as feature extractors. Ali et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib173" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ali2021enhanced</span> </a></cite><span id="S4.I5.i1.p3.1.10" class="ltx_text" style="color:#000000;"> used a pre-existing DenseNet model and fine-tuning it with their own skin cancer dataset. The authors also used a class-weighted and focal loss function to address the problem of class imbalance in the dataset.</span></p>
</div>
</li>
<li id="S4.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i2.1.1.1" class="ltx_text ltx_font_bold">T2.</span></span> 
<div id="S4.I5.i2.p1" class="ltx_para">
<p id="S4.I5.i2.p1.1" class="ltx_p"><span id="S4.I5.i2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Breast Cancer: </span><span id="S4.I5.i2.p1.1.2" class="ltx_text" style="color:#000000;">is the most commonly diagnosed cancer and the leading cause of cancer death in women. It is characterized by the uncontrollable growth of abnormal cells in the breast tissue </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib174" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">dumalaon2014causes</span> </a></cite><span id="S4.I5.i2.p1.1.5" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S4.I5.i2.p2" class="ltx_para">
<p id="S4.I5.i2.p2.1" class="ltx_p"><span id="S4.I5.i2.p2.1.1" class="ltx_text" style="color:#000000;">FL has been used in several researches, Roth et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib175" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">roth2020federated</span> </a></cite><span id="S4.I5.i2.p2.1.4" class="ltx_text" style="color:#000000;"> conducted a study to develop a FL model for breast classification of mammography data from seven clinical institutions across the world. This study showed that the results of this model improved by 6.3% compared to models trained on a central server, despite dissimilarities and differences between datasets across all sites. Jimènez-Sànchez et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jimenez2023memory</span> </a></cite><span id="S4.I5.i2.p2.1.7" class="ltx_text" style="color:#000000;"> proposed an approach: Fed-Align-CL, a decentralized model based on federated architecture. The proposed Fed-Align-CL model achieved the highest AUC and PR-AUC. Kumbhare et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kumbhare2023federated</span> </a></cite><span id="S4.I5.i2.p2.1.10" class="ltx_text" style="color:#000000;"> proposed a FL-based E-RNN model that collected mammography images of breast cancer and used the Densenet model for feature extraction. These extracted features are then classified using E-RNN for breast cancer detection.</span></p>
</div>
<div id="S4.I5.i2.p3" class="ltx_para">
<p id="S4.I5.i2.p3.1" class="ltx_p"><span id="S4.I5.i2.p3.1.1" class="ltx_text" style="color:#000000;">TL has been used in breast CD, Kumari et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kumari2023magnification</span> </a></cite><span id="S4.I5.i2.p3.1.4" class="ltx_text" style="color:#000000;"> used a pre-trained deep CNN architecture to perform breast cancer classification on two datasets. Densenet-201 architecture achieved the best results with classification accuracy of 99.50% and 99.12% from two datasets. Khan et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib176" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">khan2019novel</span> </a></cite><span id="S4.I5.i2.p3.1.7" class="ltx_text" style="color:#000000;"> proposed a DL framework for the detection and classification of breast cancer using TL and fine-tuning. Classification was based on two separate sets of microscopic breast images. The authors combined three CNN architectures (GoogLeNet, VGGNet, and ResNet). The images in both datasets were captured by a microscope at different magnifications (100X, 140X, 200X, and 500X). The authors compared this framework with other CNN models and the proposed framework achieved the best results.</span></p>
</div>
</li>
<li id="S4.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i3.1.1.1" class="ltx_text ltx_font_bold">T3.</span></span> 
<div id="S4.I5.i3.p1" class="ltx_para">
<p id="S4.I5.i3.p1.1" class="ltx_p"><span id="S4.I5.i3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Colon Cancer: </span><span id="S4.I5.i3.p1.1.2" class="ltx_text" style="color:#000000;">Commonly referred to as colon cancer, it is a malignant neoplasm that targets the colon or rectum. This disease is one of the most serious and deadly cancers in the world and is among the three most common worldwide </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib177" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">pacal2020comprehensive</span> </a></cite><span id="S4.I5.i3.p1.1.5" class="ltx_text" style="color:#000000;">. In the Netherlands, colorectal cancer accounts for almost 10% of all cases, with colorectal cancer being the second most common cause of cancer-related death </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib178" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ahmed2020colon</span> </a></cite><span id="S4.I5.i3.p1.1.8" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S4.I5.i3.p2" class="ltx_para">
<p id="S4.I5.i3.p2.1" class="ltx_p"><span id="S4.I5.i3.p2.1.1" class="ltx_text" style="color:#000000;">TL has been used in many researches to detect colon cancer. Murugesan et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib179" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">murugesan2023colon</span> </a></cite><span id="S4.I5.i3.p2.1.4" class="ltx_text" style="color:#000000;"> proposed the use of YOLOv3 MSF as a TL approach to identify and annotate different stages of colon cancer. The authors used the VColonDB dataset in their study. Mehmood et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mehmood2022malignancy</span> </a></cite><span id="S4.I5.i3.p2.1.7" class="ltx_text" style="color:#000000;"> used a dataset of 25,000 histopathological images of lung and colon tissue and trained an AlexNet model, using 80% of the images in each class for training purposes and 20% for testing purposes. The proposed class selective image processing (CSIP) approach improved the accuracy of the model from 89% to 98.8%. DenseNet and SENet architectures were used by Gessert et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib180" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">gessert2019deep</span> </a></cite><span id="S4.I5.i3.p2.1.10" class="ltx_text" style="color:#000000;"> to train pre-trained models to classify healthy colon and peritoneum tissue from confocal laser microscopy (CLM) images with AUC greater than 90%. This research showed that TL achieved great results and can be used with other approaches to improve the accuracy of the model.</span></p>
</div>
</li>
<li id="S4.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i4.1.1.1" class="ltx_text ltx_font_bold">T4.</span></span> 
<div id="S4.I5.i4.p1" class="ltx_para">
<p id="S4.I5.i4.p1.1" class="ltx_p"><span id="S4.I5.i4.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Brain Cancer: </span><span id="S4.I5.i4.p1.1.2" class="ltx_text" style="color:#000000;">refers to malignant and benign tumors that arise in the brain or surrounding tissues. The development of brain tumor can originate either from primary brain cells or from the metastasis of cancer cells from other regions of the body </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib181" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">hormuth2022opportunities</span> </a></cite><span id="S4.I5.i4.p1.1.5" class="ltx_text" style="color:#000000;">. Glioblastoma multiforme (GBM), classified as grade 4, is the most dangerous and widespread brain tumor. Despite advances in therapy, the survival rate of GBM patients has remained low in recent decades, with an average survival time of less than two years </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib182" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">al2022inflammation</span> </a></cite><span id="S4.I5.i4.p1.1.8" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S4.I5.i4.p2" class="ltx_para">
<p id="S4.I5.i4.p2.1" class="ltx_p"><span id="S4.I5.i4.p2.1.1" class="ltx_text" style="color:#000000;">In </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib183" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yi2020net</span> </a></cite><span id="S4.I5.i4.p2.1.4" class="ltx_text" style="color:#000000;">, Yi et al. proposed an efficient FL model SU-Net for brain tumor segmentation based on the encoder-decoder model. The approach was to collect brain MRI images from five medical institutions. The results of the study showed that the SU-Net model has the best performance, outperforming the baseline models (including FCN8s, standard U-Net and DeepLabv3+) in both AUC and DSC metrics across all datasets from five different institutions. Rehman et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rehman2020deep</span> </a></cite><span id="S4.I5.i4.p2.1.7" class="ltx_text" style="color:#000000;"> trained various CNN models such as AlexNet, GoogLeNet and VGGNet on the Figshare dataset (MRI images). The authors reported that the VGG16 approach achieved the best results with an accuracy of 98.69%. Talukder et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">talukder2023efficient</span> </a></cite><span id="S4.I5.i4.p2.1.10" class="ltx_text" style="color:#000000;"> have trained several TL algorithms, including Xception, ResNet-50V2, InceptionResNetV2, and DenseNet201 on Figshare brain tumor dataset and achieved accuracy scores of 99.40%, 99.68%, 99.36%, and 98.72% for each algorithm, respectively. Lyu </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">lyu2022transformer</span> </a></cite><span id="S4.I5.i4.p2.1.13" class="ltx_text" style="color:#000000;"> proposed the combination of convolutional layers and transformers for brain metastasis segmentation, the authors trained their model on MRI scans at Wake Forest School, the proposed approach also showed better performance than classical results in classification of brain metastases. Anaya-Isaza et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib184" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jacob2023brain</span> </a></cite><span id="S4.I5.i4.p2.1.16" class="ltx_text" style="color:#000000;"> applied multiple CNN models to BraTS 2018 and TCGA-LGG brain tumor detection datasets and concluded that DenseNet121, InceptionV3 and VGG19 produced the best results.</span></p>
</div>
</li>
<li id="S4.I5.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i5.1.1.1" class="ltx_text ltx_font_bold">T5.</span></span> 
<div id="S4.I5.i5.p1" class="ltx_para">
<p id="S4.I5.i5.p1.1" class="ltx_p"><span id="S4.I5.i5.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Lung Cancer:</span><span id="S4.I5.i5.p1.1.2" class="ltx_text" style="color:#000000;"> is characterized as a neoplasm of the respiratory organs and is the leading cause of global cancer incidence and mortality, with approximate of 2 million diagnoses and 1.8 million demises </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib185" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">thandra2021epidemiology</span> </a></cite><span id="S4.I5.i5.p1.1.5" class="ltx_text" style="color:#000000;">. In spite of other possible factors such as air pollution, genetic predisposition and exposure to radon. Smoking has been identified as a key risk factor for lung cancer </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib186" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rudin2021small</span> </a></cite><span id="S4.I5.i5.p1.1.8" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S4.I5.i5.p2" class="ltx_para">
<p id="S4.I5.i5.p2.1" class="ltx_p"><span id="S4.I5.i5.p2.1.1" class="ltx_text" style="color:#000000;">Heidari et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">heidari2023new</span> </a></cite><span id="S4.I5.i5.p2.1.4" class="ltx_text" style="color:#000000;"> proposed a framework named FBCLC-Rad that integrated edge and blockchain for lung CD, this model was based on horizontal FL approach. The authors trained their proposed model on four distinct datasets: cancer imaging archive (CIA), Kaggle data science bowl (KDSB), LUNA 16, and a local dataset. The model achieved an overall accuracy of 98.9%, a sensitivity of 98.7%, and a specificity of 99.1%. Liu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2023federated</span> </a></cite><span id="S4.I5.i5.p2.1.7" class="ltx_text" style="color:#000000;"> proposed a FL approach for lung nodule detection using a 3D ResNet18 dual path Faster R-CNN network. The authors trained their proposed model on Luna16 dataset and achieved F1 score of 83.401%. Ayekai et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib187" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ayekai2022federated</span> </a></cite><span id="S4.I5.i5.p2.1.10" class="ltx_text" style="color:#000000;"> trained a FL model with FedAvg strategy on Colon Cancer Histopathological Image Dataset, the authors used the CNN DenseNet121 model to train the data and achieved an accuracy of 94.5% on the test set. The centralized learning method had an accuracy of 95.2%. With these results, the authors concluded that the two approaches achieved very similar results despite protecting the privacy of the data.</span></p>
</div>
<div id="S4.I5.i5.p3" class="ltx_para">
<p id="S4.I5.i5.p3.1" class="ltx_p"><span id="S4.I5.i5.p3.1.1" class="ltx_text" style="color:#000000;">TL has been used in several research articles on lung cancer. Fang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">fang2018novel</span> </a></cite><span id="S4.I5.i5.p3.1.4" class="ltx_text" style="color:#000000;"> proposed an approach in which a GoogLeNet-based CNN was trained using median intensity projections (MIPs). The CNN performed the classification with an accuracy rate of 81% in identifying malignant and benign pulmonary nodules. Sajja et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sajja2019lung</span> </a></cite><span id="S4.I5.i5.p3.1.7" class="ltx_text" style="color:#000000;"> also proposed a pre-trained CNN based on GoogleNet to be used as a feature extractor. The model was trained on the lung image database consortium (LIDC) dataset with a test accuracy of 99.00%.</span></p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Public Datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Publicly available datasets provide researchers with the opportunity to develop FL and TL learning techniques for CD based on medical image analysis. Table </span><a href="#S4.T4" title="Table 4 ‣ 4.4 Advantages and disadvantages of each method ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS2.p1.1.2" class="ltx_text" style="color:#000000;"> contains a list of publicly available datasets covering various cancer types, including breast, brain, lung, skin and colon cancer. These datasets contain a range of medical images such as mammograms, MRI scans, CT scans and microscopic images with labels and annotations. Researchers can use these public datasets to develop and evaluate FL and TL approaches for CD tasks. Overall, these publicly available datasets are a critical resource for important advances in cancer research and improving CD. Fig. </span><a href="#S4.F10" title="Figure 10 ‣ 4.4 Advantages and disadvantages of each method ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">10</span></a><span id="S4.SS2.p1.1.3" class="ltx_text" style="color:#000000;"> represents a sample from the skin cancer dataset HAM10000.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Considerations for choosing between FL and TL</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="color:#000000;">There are several key factors to consider when deciding on the appropriate approach to build a strong FL and TL model:</span></p>
<ul id="S4.I6" class="ltx_itemize">
<li id="S4.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I6.i1.p1" class="ltx_para">
<p id="S4.I6.i1.p1.1" class="ltx_p"><span id="S4.I6.i1.p1.1.1" class="ltx_text" style="color:#000000;">FL enables model training on decentralized data residing on different devices without the need for direct data exchange. This improves data protection and reduces security risks. TL requires the aggregation of data on a central server, which raises greater privacy concerns.</span></p>
</div>
</li>
<li id="S4.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I6.i2.p1" class="ltx_para">
<p id="S4.I6.i2.p1.1" class="ltx_p"><span id="S4.I6.i2.p1.1.1" class="ltx_text" style="color:#000000;">FL is well suited for heterogeneous decentralized data. TL works better when the data is similar across all clients.</span></p>
</div>
</li>
<li id="S4.I6.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I6.i3.p1" class="ltx_para">
<p id="S4.I6.i3.p1.1" class="ltx_p"><span id="S4.I6.i3.p1.1.1" class="ltx_text" style="color:#000000;">FL performance depends on the quantity and quality of local data. TL allows reusing a pre-trained model on a very large dataset, enabling better performance even on small local datasets.</span></p>
</div>
</li>
<li id="S4.I6.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I6.i4.p1" class="ltx_para">
<p id="S4.I6.i4.p1.1" class="ltx_p"><span id="S4.I6.i4.p1.1.1" class="ltx_text" style="color:#000000;">With FL, higher communication costs are incurred between the central server and local devices during training. TL only requires a one-time transfer of the pre-trained model.</span></p>
</div>
</li>
<li id="S4.I6.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I6.i5.p1" class="ltx_para">
<p id="S4.I6.i5.p1.1" class="ltx_p"><span id="S4.I6.i5.p1.1.1" class="ltx_text" style="color:#000000;">FL allows models to be customized to any local dataset. TL creates a more general model.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Advantages and disadvantages of each method</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text" style="color:#000000;">This section provides an overview of the advantages and limitations of FL and TL for CD based on image analysis. An analysis for each technique is used for each learning type.</span></p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text" style="color:#000000;">FL had several advantages for CD, including improving the diversity of the model and allowing personalized learning on client devices when using the FedAvg algorithm. FL also used fragmented medical data across sites while protecting privacy. As shown in Table </span><a href="#S4.T5" title="Table 5 ‣ 4.4 Advantages and disadvantages of each method ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.SS4.p2.1.2" class="ltx_text" style="color:#000000;">, using a 3D ResNet18 model trained with FedAvg. Bayesian modeling within FL handles recommendations for unseen groups and prevents collapsed representations. However, FL faces some limitations - the FedAvg algorithm can struggle to handle heterogeneous data distributions across clients. Small dataset sizes, as in an LSTM and Transformer FL model, limit performance. And convergence analysis remains challenging with algorithms like FedAvg.</span></p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.5.2" class="ltx_text" style="font-size:90%;">Public Cancer Datasets</span></figcaption>
<table id="S4.T4.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.6.1" class="ltx_tr">
<td id="S4.T4.6.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.1.1.1" class="ltx_text" style="color:#000000;">URL</span></td>
<td id="S4.T4.6.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.1.2.1" class="ltx_text" style="color:#000000;">Name</span></td>
<td id="S4.T4.6.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.1.3.1" class="ltx_text" style="color:#000000;">Cancer Type</span></td>
<td id="S4.T4.6.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.1.4.1" class="ltx_text" style="color:#000000;">Description</span></td>
<td id="S4.T4.6.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.1.5.1" class="ltx_text" style="color:#000000;"># of Samples</span></td>
</tr>
<tr id="S4.T4.6.2" class="ltx_tr">
<td id="S4.T4.6.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000" title="" class="ltx_ref ltx_href" style="color:#000000;">D1</a></td>
<td id="S4.T4.6.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.2.2.1" class="ltx_text" style="color:#000000;">HAM10000</span></td>
<td id="S4.T4.6.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.2.3.1" class="ltx_text" style="color:#000000;">Skin</span></td>
<td id="S4.T4.6.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.2.4.1" class="ltx_text"></span><span id="S4.T4.6.2.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.2.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.2.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.2.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A collection of multi-source dermatoscopic images</span></span>
<span id="S4.T4.6.2.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.2.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">of common pigmented skin lesions</span></span>
</span></span><span id="S4.T4.6.2.4.3" class="ltx_text"></span><span id="S4.T4.6.2.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.2.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.2.5.1" class="ltx_text" style="color:#000000;">10,015 images</span></td>
</tr>
<tr id="S4.T4.6.3" class="ltx_tr">
<td id="S4.T4.6.3.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://www.ebi.ac.uk/ega/studies/EGAS00000000083" title="" class="ltx_ref ltx_href" style="color:#000000;">D2</a></td>
<td id="S4.T4.6.3.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.3.2.1" class="ltx_text" style="color:#000000;">METABRIC</span></td>
<td id="S4.T4.6.3.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.3.3.1" class="ltx_text" style="color:#000000;">Breast</span></td>
<td id="S4.T4.6.3.4" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.3.4.1" class="ltx_text"></span><span id="S4.T4.6.3.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.3.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.3.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.3.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of genomic and clinical data from</span></span>
<span id="S4.T4.6.3.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.3.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">breast cancer patients</span></span>
</span></span><span id="S4.T4.6.3.4.3" class="ltx_text"></span><span id="S4.T4.6.3.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.3.5" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.3.5.1" class="ltx_text" style="color:#000000;">Over 2,000 patients</span></td>
</tr>
<tr id="S4.T4.6.4" class="ltx_tr">
<td id="S4.T4.6.4.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://www.bcsc-research.org/data/data-access" title="" class="ltx_ref ltx_href" style="color:#000000;">D3</a></td>
<td id="S4.T4.6.4.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.4.2.1" class="ltx_text" style="color:#000000;">BCSC</span></td>
<td id="S4.T4.6.4.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.4.3.1" class="ltx_text" style="color:#000000;">Breast</span></td>
<td id="S4.T4.6.4.4" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.4.4.1" class="ltx_text"></span><span id="S4.T4.6.4.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.4.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.4.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.4.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of mammography and clinical data</span></span>
</span></span><span id="S4.T4.6.4.4.3" class="ltx_text"></span><span id="S4.T4.6.4.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.4.5" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.4.5.1" class="ltx_text" style="color:#000000;">Over 2 million women</span></td>
</tr>
<tr id="S4.T4.6.5" class="ltx_tr">
<td id="S4.T4.6.5.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="http://www.cgga.org.cn/download.jsp" title="" class="ltx_ref ltx_href" style="color:#000000;">D4</a></td>
<td id="S4.T4.6.5.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.5.2.1" class="ltx_text" style="color:#000000;">CGGA</span></td>
<td id="S4.T4.6.5.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.5.3.1" class="ltx_text" style="color:#000000;">Brain</span></td>
<td id="S4.T4.6.5.4" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.5.4.1" class="ltx_text"></span><span id="S4.T4.6.5.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.5.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.5.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of genomic and clinical data</span></span>
<span id="S4.T4.6.5.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">from glioma patients</span></span>
</span></span><span id="S4.T4.6.5.4.3" class="ltx_text"></span><span id="S4.T4.6.5.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.5.5" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.5.5.1" class="ltx_text" style="color:#000000;">Over 1,000 patients</span></td>
</tr>
<tr id="S4.T4.6.6" class="ltx_tr">
<td id="S4.T4.6.6.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://www.med.upenn.edu/cbica/brats2021/data.html" title="" class="ltx_ref ltx_href" style="color:#000000;">D5</a></td>
<td id="S4.T4.6.6.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.6.2.1" class="ltx_text" style="color:#000000;">BraTS</span></td>
<td id="S4.T4.6.6.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.6.3.1" class="ltx_text" style="color:#000000;">Brain</span></td>
<td id="S4.T4.6.6.4" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.6.4.1" class="ltx_text"></span><span id="S4.T4.6.6.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.6.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.6.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.6.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of MRI scans of brain tumors for the</span></span>
<span id="S4.T4.6.6.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.6.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">purpose of developing algorithms</span></span>
<span id="S4.T4.6.6.4.2.1.3" class="ltx_tr">
<span id="S4.T4.6.6.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">for automated segmentation of brain tumors</span></span>
</span></span><span id="S4.T4.6.6.4.3" class="ltx_text"></span><span id="S4.T4.6.6.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.6.5" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.6.5.1" class="ltx_text" style="color:#000000;">1,000+ patients</span></td>
</tr>
<tr id="S4.T4.6.7" class="ltx_tr">
<td id="S4.T4.6.7.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://portal.gdc.cancer.gov/projects/TCGA-COAD" title="" class="ltx_ref ltx_href" style="color:#000000;">D6</a></td>
<td id="S4.T4.6.7.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.7.2.1" class="ltx_text" style="color:#000000;">TCGA</span></td>
<td id="S4.T4.6.7.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.7.3.1" class="ltx_text" style="color:#000000;">Colon</span></td>
<td id="S4.T4.6.7.4" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.7.4.1" class="ltx_text"></span><span id="S4.T4.6.7.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.7.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.7.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.7.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of genomic, transcriptomic, and epigenomic</span></span>
<span id="S4.T4.6.7.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.7.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">data from colon cancer patients</span></span>
</span></span><span id="S4.T4.6.7.4.3" class="ltx_text"></span><span id="S4.T4.6.7.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.7.5" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.7.5.1" class="ltx_text" style="color:#000000;">Over 400 patients</span></td>
</tr>
<tr id="S4.T4.6.8" class="ltx_tr">
<td id="S4.T4.6.8.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE17538" title="" class="ltx_ref ltx_href" style="color:#000000;">D7</a></td>
<td id="S4.T4.6.8.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.8.2.1" class="ltx_text" style="color:#000000;">GEO</span></td>
<td id="S4.T4.6.8.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.8.3.1" class="ltx_text" style="color:#000000;">Colon</span></td>
<td id="S4.T4.6.8.4" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.8.4.1" class="ltx_text"></span><span id="S4.T4.6.8.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.8.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.8.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.8.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of gene expression data from</span></span>
<span id="S4.T4.6.8.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.8.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">colon cancer patients</span></span>
</span></span><span id="S4.T4.6.8.4.3" class="ltx_text"></span><span id="S4.T4.6.8.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.8.5" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.8.5.1" class="ltx_text" style="color:#000000;">Over 200 patients</span></td>
</tr>
<tr id="S4.T4.6.9" class="ltx_tr">
<td id="S4.T4.6.9.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://portal.gdc.cancer.gov/projects/TCGA-LUAD" title="" class="ltx_ref ltx_href" style="color:#000000;">D8</a></td>
<td id="S4.T4.6.9.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.9.2.1" class="ltx_text" style="color:#000000;">GDC</span></td>
<td id="S4.T4.6.9.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.9.3.1" class="ltx_text" style="color:#000000;">Lung</span></td>
<td id="S4.T4.6.9.4" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.9.4.1" class="ltx_text"></span><span id="S4.T4.6.9.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.9.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.9.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.9.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of genomic and clinical data</span></span>
<span id="S4.T4.6.9.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.9.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">from over 1,000 lung cancer patients</span></span>
</span></span><span id="S4.T4.6.9.4.3" class="ltx_text"></span><span id="S4.T4.6.9.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.9.5" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.9.5.1" class="ltx_text" style="color:#000000;">Over 1,000 patients</span></td>
</tr>
<tr id="S4.T4.6.10" class="ltx_tr">
<td id="S4.T4.6.10.1" class="ltx_td ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://dcc.icgc.org/releases/PCAWG" title="" class="ltx_ref ltx_href" style="color:#000000;">D9</a></td>
<td id="S4.T4.6.10.2" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.10.2.1" class="ltx_text" style="color:#000000;">ICGC</span></td>
<td id="S4.T4.6.10.3" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.10.3.1" class="ltx_text" style="color:#000000;">Lung</span></td>
<td id="S4.T4.6.10.4" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.10.4.1" class="ltx_text"></span><span id="S4.T4.6.10.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.10.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.10.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.10.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of genomic and transcriptomic data</span></span>
<span id="S4.T4.6.10.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.10.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">from lung cancer patients</span></span>
</span></span><span id="S4.T4.6.10.4.3" class="ltx_text"></span><span id="S4.T4.6.10.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.10.5" class="ltx_td ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.10.5.1" class="ltx_text" style="color:#000000;">Over 500 patients</span></td>
</tr>
<tr id="S4.T4.6.11" class="ltx_tr">
<td id="S4.T4.6.11.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><a target="_blank" href="https://www.kaggle.com/nodoubttome/skin-cancer-isic" title="" class="ltx_ref ltx_href" style="color:#000000;">D10</a></td>
<td id="S4.T4.6.11.2" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.11.2.1" class="ltx_text" style="color:#000000;">ISIC</span></td>
<td id="S4.T4.6.11.3" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.11.3.1" class="ltx_text" style="color:#000000;">Skin</span></td>
<td id="S4.T4.6.11.4" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T4.6.11.4.1" class="ltx_text"></span><span id="S4.T4.6.11.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T4.6.11.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.6.11.4.2.1.1" class="ltx_tr">
<span id="S4.T4.6.11.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">A dataset of images of malignant and benign</span></span>
<span id="S4.T4.6.11.4.2.1.2" class="ltx_tr">
<span id="S4.T4.6.11.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:2.5pt;padding-bottom:2.5pt;">oncological diseases</span></span>
</span></span><span id="S4.T4.6.11.4.3" class="ltx_text"></span><span id="S4.T4.6.11.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T4.6.11.5" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S4.T4.6.11.5.1" class="ltx_text" style="color:#000000;">2,357 images</span></td>
</tr>
</table>
</figure>
<figure id="S4.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F10.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2405.20126/assets/fig13_a.jpg" id="S4.F10.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F10.sf1.5.2" class="ltx_text" style="font-size:90%;">Melanoma skin tumor</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F10.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2405.20126/assets/fig13_b.jpg" id="S4.F10.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.sf2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F10.sf2.5.2" class="ltx_text" style="font-size:90%;">Melanocytic Nevi skin tumor</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.6.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S4.F10.7.2" class="ltx_text" style="font-size:90%;">Sample of Skin Cancer MNIST: HAM10000 dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib188" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">codella2019skin</span> </a>; <a href="#bib.bib189" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">tschandl2018ham10000</span> </a></cite></span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.4.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.5.2" class="ltx_text" style="font-size:90%;">Advantage and limitations of FL and TL for CD</span></figcaption>
<table id="S4.T5.6" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.6.1" class="ltx_tr">
<td id="S4.T5.6.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.1.1.1" class="ltx_text" style="color:#000000;">Ref</span></td>
<td id="S4.T5.6.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.1.2.1" class="ltx_text" style="color:#000000;">Type</span></td>
<td id="S4.T5.6.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.1.3.1" class="ltx_text" style="color:#000000;">Technique</span></td>
<td id="S4.T5.6.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.1.4.1" class="ltx_text" style="color:#000000;">Advantage</span></td>
<td id="S4.T5.6.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.1.5.1" class="ltx_text" style="color:#000000;">Limitation</span></td>
</tr>
<tr id="S4.T5.6.2" class="ltx_tr">
<td id="S4.T5.6.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2023federated</span> </a></cite></td>
<td id="S4.T5.6.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.2.2.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T5.6.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.2.3.1" class="ltx_text"></span><span id="S4.T5.6.2.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.2.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.2.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.2.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">3D ResNet18</span></span>
<span id="S4.T5.6.2.3.2.1.2" class="ltx_tr">
<span id="S4.T5.6.2.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">+ FedAvg</span></span>
</span></span><span id="S4.T5.6.2.3.3" class="ltx_text"></span><span id="S4.T5.6.2.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.2.4.1" class="ltx_text"></span><span id="S4.T5.6.2.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.2.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.2.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.2.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Utilizes fragmented medical data,</span></span>
<span id="S4.T5.6.2.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.2.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">protects privacy</span></span>
</span></span><span id="S4.T5.6.2.4.3" class="ltx_text"></span><span id="S4.T5.6.2.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.2.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.2.5.1" class="ltx_text" style="color:#000000;">Effect of client number</span></td>
</tr>
<tr id="S4.T5.6.3" class="ltx_tr">
<td id="S4.T5.6.3.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">fang2018novel</span> </a></cite></td>
<td id="S4.T5.6.3.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.3.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.3.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.3.3.1" class="ltx_text"></span><span id="S4.T5.6.3.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.3.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.3.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">GoogLeNet,</span></span>
<span id="S4.T5.6.3.3.2.1.2" class="ltx_tr">
<span id="S4.T5.6.3.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">MIPs</span></span>
</span></span><span id="S4.T5.6.3.3.3" class="ltx_text"></span><span id="S4.T5.6.3.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.3.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.3.4.1" class="ltx_text"></span><span id="S4.T5.6.3.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.3.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.3.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.3.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Improves accuracy and convergence</span></span>
<span id="S4.T5.6.3.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.3.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">rate compared to training a CNN</span></span>
<span id="S4.T5.6.3.4.2.1.3" class="ltx_tr">
<span id="S4.T5.6.3.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">from ground up</span></span>
</span></span><span id="S4.T5.6.3.4.3" class="ltx_text"></span><span id="S4.T5.6.3.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.3.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.3.5.1" class="ltx_text"></span><span id="S4.T5.6.3.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.3.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.3.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.3.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Develop a fast, accurate and</span></span>
<span id="S4.T5.6.3.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.3.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">stable DL system for</span></span>
<span id="S4.T5.6.3.5.2.1.3" class="ltx_tr">
<span id="S4.T5.6.3.5.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">lung cancer detection from</span></span>
<span id="S4.T5.6.3.5.2.1.4" class="ltx_tr">
<span id="S4.T5.6.3.5.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">CT scans</span></span>
</span></span><span id="S4.T5.6.3.5.3" class="ltx_text"></span><span id="S4.T5.6.3.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.4" class="ltx_tr">
<td id="S4.T5.6.4.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">cai2021many</span> </a></cite></td>
<td id="S4.T5.6.4.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.4.2.1" class="ltx_text" style="color:#000000;">FTL</span></td>
<td id="S4.T5.6.4.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.4.3.1" class="ltx_text" style="color:#000000;">DGAN</span></td>
<td id="S4.T5.6.4.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.4.4.1" class="ltx_text"></span><span id="S4.T5.6.4.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.4.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.4.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.4.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Augments limited data, protects privacy</span></span>
</span></span><span id="S4.T5.6.4.4.3" class="ltx_text"></span><span id="S4.T5.6.4.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.4.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.4.5.1" class="ltx_text"></span><span id="S4.T5.6.4.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.4.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.4.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.4.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Impact of client number,</span></span>
<span id="S4.T5.6.4.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.4.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">malicious clients</span></span>
</span></span><span id="S4.T5.6.4.5.3" class="ltx_text"></span><span id="S4.T5.6.4.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.5" class="ltx_tr">
<td id="S4.T5.6.5.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2023vision</span> </a></cite></td>
<td id="S4.T5.6.5.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.5.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.5.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.5.3.1" class="ltx_text" style="color:#000000;">ViT</span></td>
<td id="S4.T5.6.5.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.5.4.1" class="ltx_text"></span><span id="S4.T5.6.5.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.5.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.5.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Captures global context, outperforms</span></span>
<span id="S4.T5.6.5.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">CNN models</span></span>
</span></span><span id="S4.T5.6.5.4.3" class="ltx_text"></span><span id="S4.T5.6.5.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.5.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.5.5.1" class="ltx_text"></span><span id="S4.T5.6.5.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.5.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.5.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.5.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Requires large datasets for</span></span>
<span id="S4.T5.6.5.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.5.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">pretraining</span></span>
</span></span><span id="S4.T5.6.5.5.3" class="ltx_text"></span><span id="S4.T5.6.5.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.6" class="ltx_tr">
<td id="S4.T5.6.6.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">paya2022automatic</span> </a></cite></td>
<td id="S4.T5.6.6.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.6.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.6.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.6.3.1" class="ltx_text"></span><span id="S4.T5.6.6.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.6.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.6.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.6.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">SCL</span></span>
</span></span><span id="S4.T5.6.6.3.3" class="ltx_text"></span><span id="S4.T5.6.6.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.6.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.6.4.1" class="ltx_text"></span><span id="S4.T5.6.6.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.6.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.6.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.6.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Improves embryo viability and</span></span>
<span id="S4.T5.6.6.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.6.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">quality classification</span></span>
</span></span><span id="S4.T5.6.6.4.3" class="ltx_text"></span><span id="S4.T5.6.6.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.6.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.6.5.1" class="ltx_text" style="color:#000000;">Limited dataset from single clinic</span></td>
</tr>
<tr id="S4.T5.6.7" class="ltx_tr">
<td id="S4.T5.6.7.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2021survey</span> </a></cite></td>
<td id="S4.T5.6.7.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.7.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.7.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.7.3.1" class="ltx_text"></span><span id="S4.T5.6.7.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.7.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.7.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.7.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">K-means, spectral</span></span>
<span id="S4.T5.6.7.3.2.1.2" class="ltx_tr">
<span id="S4.T5.6.7.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">clustering</span></span>
</span></span><span id="S4.T5.6.7.3.3" class="ltx_text"></span><span id="S4.T5.6.7.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.7.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.7.4.1" class="ltx_text"></span><span id="S4.T5.6.7.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.7.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.7.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.7.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Can produce good clustering results</span></span>
<span id="S4.T5.6.7.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.7.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">when instance size is small by transferring</span></span>
<span id="S4.T5.6.7.4.2.1.3" class="ltx_tr">
<span id="S4.T5.6.7.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">knowledge from source domain</span></span>
</span></span><span id="S4.T5.6.7.4.3" class="ltx_text"></span><span id="S4.T5.6.7.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.7.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.7.5.1" class="ltx_text"></span><span id="S4.T5.6.7.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.7.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.7.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.7.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Most research focuses on</span></span>
<span id="S4.T5.6.7.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.7.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">classification and recognition,</span></span>
<span id="S4.T5.6.7.5.2.1.3" class="ltx_tr">
<span id="S4.T5.6.7.5.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">not clustering</span></span>
</span></span><span id="S4.T5.6.7.5.3" class="ltx_text"></span><span id="S4.T5.6.7.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.8" class="ltx_tr">
<td id="S4.T5.6.8.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">das2021multi</span> </a></cite></td>
<td id="S4.T5.6.8.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.8.2.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T5.6.8.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.8.3.1" class="ltx_text"></span><span id="S4.T5.6.8.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.8.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.8.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.8.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">TDCD</span></span>
</span></span><span id="S4.T5.6.8.3.3" class="ltx_text"></span><span id="S4.T5.6.8.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.8.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.8.4.1" class="ltx_text"></span><span id="S4.T5.6.8.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.8.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.8.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.8.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Communication efficient, exploits vertical</span></span>
<span id="S4.T5.6.8.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.8.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">and horizontal data partitions</span></span>
</span></span><span id="S4.T5.6.8.4.3" class="ltx_text"></span><span id="S4.T5.6.8.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.8.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.8.5.1" class="ltx_text" style="color:#000000;">Analysis on non-IID data needed</span></td>
</tr>
<tr id="S4.T5.6.9" class="ltx_tr">
<td id="S4.T5.6.9.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib103" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ek2020evaluation</span> </a></cite></td>
<td id="S4.T5.6.9.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.9.2.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T5.6.9.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.9.3.1" class="ltx_text" style="color:#000000;">FedAvg</span></td>
<td id="S4.T5.6.9.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.9.4.1" class="ltx_text"></span><span id="S4.T5.6.9.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.9.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.9.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.9.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Improves model diversity and provides</span></span>
<span id="S4.T5.6.9.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.9.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">personalized learning on client devices</span></span>
</span></span><span id="S4.T5.6.9.4.3" class="ltx_text"></span><span id="S4.T5.6.9.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.9.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.9.5.1" class="ltx_text"></span><span id="S4.T5.6.9.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.9.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.9.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.9.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Difficult to handle data</span></span>
<span id="S4.T5.6.9.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.9.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">distributions across clients</span></span>
</span></span><span id="S4.T5.6.9.5.3" class="ltx_text"></span><span id="S4.T5.6.9.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.10" class="ltx_tr">
<td id="S4.T5.6.10.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sharma2023deep</span> </a></cite></td>
<td id="S4.T5.6.10.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.10.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.10.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.10.3.1" class="ltx_text" style="color:#000000;">VGG-16</span></td>
<td id="S4.T5.6.10.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.10.4.1" class="ltx_text"></span><span id="S4.T5.6.10.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.10.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.10.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.10.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Achieves high accuracy for pneumonia</span></span>
<span id="S4.T5.6.10.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.10.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">detection from chest X-rays</span></span>
</span></span><span id="S4.T5.6.10.4.3" class="ltx_text"></span><span id="S4.T5.6.10.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.10.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.10.5.1" class="ltx_text"></span><span id="S4.T5.6.10.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.10.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.10.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.10.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Limited comparison to other</span></span>
<span id="S4.T5.6.10.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.10.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">best models</span></span>
</span></span><span id="S4.T5.6.10.5.3" class="ltx_text"></span><span id="S4.T5.6.10.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.11" class="ltx_tr">
<td id="S4.T5.6.11.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib116" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ikechukwu2021resnet</span> </a></cite></td>
<td id="S4.T5.6.11.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.11.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.11.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.11.3.1" class="ltx_text" style="color:#000000;">ResNet-50, VGG-19</span></td>
<td id="S4.T5.6.11.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.11.4.1" class="ltx_text"></span><span id="S4.T5.6.11.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.11.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.11.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.11.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Improved performance over training</span></span>
<span id="S4.T5.6.11.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.11.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">from ground up</span></span>
</span></span><span id="S4.T5.6.11.4.3" class="ltx_text"></span><span id="S4.T5.6.11.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.11.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.11.5.1" class="ltx_text" style="color:#000000;">Small dataset size</span></td>
</tr>
<tr id="S4.T5.6.12" class="ltx_tr">
<td id="S4.T5.6.12.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib119" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">panthakkan2022concatenated</span> </a></cite></td>
<td id="S4.T5.6.12.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.12.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.12.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.12.3.1" class="ltx_text"></span><span id="S4.T5.6.12.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.12.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.12.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.12.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Xception and</span></span>
<span id="S4.T5.6.12.3.2.1.2" class="ltx_tr">
<span id="S4.T5.6.12.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">ResNet50</span></span>
</span></span><span id="S4.T5.6.12.3.3" class="ltx_text"></span><span id="S4.T5.6.12.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.12.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.12.4.1" class="ltx_text"></span><span id="S4.T5.6.12.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.12.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.12.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.12.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Achieves high accuracy for skin</span></span>
<span id="S4.T5.6.12.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.12.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">cancer classification</span></span>
</span></span><span id="S4.T5.6.12.4.3" class="ltx_text"></span><span id="S4.T5.6.12.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.12.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.12.5.1" class="ltx_text" style="color:#000000;">Image dataset is imbalanced</span></td>
</tr>
<tr id="S4.T5.6.13" class="ltx_tr">
<td id="S4.T5.6.13.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib136" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zidan2023swincup</span> </a></cite></td>
<td id="S4.T5.6.13.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.13.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.13.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.13.3.1" class="ltx_text"></span><span id="S4.T5.6.13.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.13.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.13.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.13.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Swin Transformer</span></span>
<span id="S4.T5.6.13.3.2.1.2" class="ltx_tr">
<span id="S4.T5.6.13.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">and CNN</span></span>
</span></span><span id="S4.T5.6.13.3.3" class="ltx_text"></span><span id="S4.T5.6.13.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.13.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.13.4.1" class="ltx_text"></span><span id="S4.T5.6.13.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.13.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.13.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.13.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Accurate segmentation, captures</span></span>
<span id="S4.T5.6.13.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.13.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">global context</span></span>
</span></span><span id="S4.T5.6.13.4.3" class="ltx_text"></span><span id="S4.T5.6.13.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.13.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.13.5.1" class="ltx_text" style="color:#000000;">Dataset size</span></td>
</tr>
<tr id="S4.T5.6.14" class="ltx_tr">
<td id="S4.T5.6.14.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib190" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">raval2023comprehensive</span> </a></cite></td>
<td id="S4.T5.6.14.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.14.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.14.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.14.3.1" class="ltx_text"></span><span id="S4.T5.6.14.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.14.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.14.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.14.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">AlexNet, VGGNet,</span></span>
<span id="S4.T5.6.14.3.2.1.2" class="ltx_tr">
<span id="S4.T5.6.14.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Inception</span></span>
</span></span><span id="S4.T5.6.14.3.3" class="ltx_text"></span><span id="S4.T5.6.14.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.14.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.14.4.1" class="ltx_text"></span><span id="S4.T5.6.14.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.14.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.14.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.14.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">DenseNet achieved best performance</span></span>
<span id="S4.T5.6.14.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.14.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">for skin CD</span></span>
</span></span><span id="S4.T5.6.14.4.3" class="ltx_text"></span><span id="S4.T5.6.14.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.14.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.14.5.1" class="ltx_text"></span><span id="S4.T5.6.14.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.14.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.14.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.14.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Limited dataset available for</span></span>
<span id="S4.T5.6.14.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.14.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">oral cancer, making</span></span>
<span id="S4.T5.6.14.5.2.1.3" class="ltx_tr">
<span id="S4.T5.6.14.5.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">detection difficult</span></span>
</span></span><span id="S4.T5.6.14.5.3" class="ltx_text"></span><span id="S4.T5.6.14.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.15" class="ltx_tr">
<td id="S4.T5.6.15.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib191" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">dey2022screening</span> </a></cite></td>
<td id="S4.T5.6.15.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.15.2.1" class="ltx_text" style="color:#000000;">TL</span></td>
<td id="S4.T5.6.15.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.15.3.1" class="ltx_text" style="color:#000000;">DenseNet121</span></td>
<td id="S4.T5.6.15.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.15.4.1" class="ltx_text" style="color:#000000;">Accurate classification</span></td>
<td id="S4.T5.6.15.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.15.5.1" class="ltx_text" style="color:#000000;">Single dataset</span></td>
</tr>
<tr id="S4.T5.6.16" class="ltx_tr">
<td id="S4.T5.6.16.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib192" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">hanser2023federated</span> </a></cite></td>
<td id="S4.T5.6.16.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.16.2.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T5.6.16.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.16.3.1" class="ltx_text"></span><span id="S4.T5.6.16.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.16.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.16.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.16.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Model-driven and</span></span>
<span id="S4.T5.6.16.3.2.1.2" class="ltx_tr">
<span id="S4.T5.6.16.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">data-driven FL</span></span>
</span></span><span id="S4.T5.6.16.3.3" class="ltx_text"></span><span id="S4.T5.6.16.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.16.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.16.4.1" class="ltx_text"></span><span id="S4.T5.6.16.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.16.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.16.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.16.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Improves model performance and</span></span>
<span id="S4.T5.6.16.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.16.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">applicability domain</span></span>
</span></span><span id="S4.T5.6.16.4.3" class="ltx_text"></span><span id="S4.T5.6.16.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.16.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.16.5.1" class="ltx_text"></span><span id="S4.T5.6.16.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.16.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.16.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.16.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Communication efficiency</span></span>
<span id="S4.T5.6.16.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.16.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">challenges</span></span>
</span></span><span id="S4.T5.6.16.5.3" class="ltx_text"></span><span id="S4.T5.6.16.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.17" class="ltx_tr">
<td id="S4.T5.6.17.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib193" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">su2021hierarchical</span> </a></cite></td>
<td id="S4.T5.6.17.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.17.2.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T5.6.17.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.17.3.1" class="ltx_text" style="color:#000000;">SCGDA</span></td>
<td id="S4.T5.6.17.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.17.4.1" class="ltx_text" style="color:#000000;">Utilizes sample and feature diversity</span></td>
<td id="S4.T5.6.17.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.17.5.1" class="ltx_text" style="color:#000000;">Convergence analysis needed</span></td>
</tr>
<tr id="S4.T5.6.18" class="ltx_tr">
<td id="S4.T5.6.18.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib194" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">lee2022bayesian</span> </a></cite></td>
<td id="S4.T5.6.18.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.18.2.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T5.6.18.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.18.3.1" class="ltx_text" style="color:#000000;">Bayesian modeling</span></td>
<td id="S4.T5.6.18.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.18.4.1" class="ltx_text"></span><span id="S4.T5.6.18.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.18.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.18.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.18.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Handles recommendations for unseen groups.</span></span>
<span id="S4.T5.6.18.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.18.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Better prevents collapsed representations</span></span>
</span></span><span id="S4.T5.6.18.4.3" class="ltx_text"></span><span id="S4.T5.6.18.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.18.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.18.5.1" class="ltx_text"></span><span id="S4.T5.6.18.5.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.18.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.18.5.2.1.1" class="ltx_tr">
<span id="S4.T5.6.18.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Slower convergence compared</span></span>
<span id="S4.T5.6.18.5.2.1.2" class="ltx_tr">
<span id="S4.T5.6.18.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">to some existing models</span></span>
</span></span><span id="S4.T5.6.18.5.3" class="ltx_text"></span><span id="S4.T5.6.18.5.4" class="ltx_text" style="color:#000000;"></span>
</td>
</tr>
<tr id="S4.T5.6.19" class="ltx_tr">
<td id="S4.T5.6.19.1" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib195" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kamei2023comparison</span> </a></cite></td>
<td id="S4.T5.6.19.2" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.19.2.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T5.6.19.3" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.19.3.1" class="ltx_text"></span><span id="S4.T5.6.19.3.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.19.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.19.3.2.1.1" class="ltx_tr">
<span id="S4.T5.6.19.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">LSTM, Transformer</span></span>
<span id="S4.T5.6.19.3.2.1.2" class="ltx_tr">
<span id="S4.T5.6.19.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">and FedAvg</span></span>
</span></span><span id="S4.T5.6.19.3.3" class="ltx_text"></span><span id="S4.T5.6.19.3.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.19.4" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.19.4.1" class="ltx_text"></span><span id="S4.T5.6.19.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.19.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.19.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.19.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">Decentralized approaches address data privacy</span></span>
<span id="S4.T5.6.19.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.19.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">and security issues</span></span>
</span></span><span id="S4.T5.6.19.4.3" class="ltx_text"></span><span id="S4.T5.6.19.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.19.5" class="ltx_td ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.19.5.1" class="ltx_text" style="color:#000000;">Small dataset size</span></td>
</tr>
<tr id="S4.T5.6.20" class="ltx_tr">
<td id="S4.T5.6.20.1" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;"><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib196" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sun2022decentralized</span> </a></cite></td>
<td id="S4.T5.6.20.2" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.20.2.1" class="ltx_text" style="color:#000000;">FL</span></td>
<td id="S4.T5.6.20.3" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.20.3.1" class="ltx_text" style="color:#000000;">FedAvg</span></td>
<td id="S4.T5.6.20.4" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S4.T5.6.20.4.1" class="ltx_text"></span><span id="S4.T5.6.20.4.2" class="ltx_text" style="color:#000000;">
<span id="S4.T5.6.20.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.6.20.4.2.1.1" class="ltx_tr">
<span id="S4.T5.6.20.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">blackuces communication bottlenecks, more</span></span>
<span id="S4.T5.6.20.4.2.1.2" class="ltx_tr">
<span id="S4.T5.6.20.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:5pt;padding-bottom:5pt;">robust to node failures and privacy attacks</span></span>
</span></span><span id="S4.T5.6.20.4.3" class="ltx_text"></span><span id="S4.T5.6.20.4.4" class="ltx_text" style="color:#000000;"></span>
</td>
<td id="S4.T5.6.20.5" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;"><span id="S4.T5.6.20.5.1" class="ltx_text" style="color:#000000;">Convergence analysis is challenging</span></td>
</tr>
</table>
</figure>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text" style="color:#000000;">TL also provides advantages for CD. TL improves accuracy and convergence rate compared to training a model from ground up, as demonstrated through a GoogLeNet model with Median Intensity Projections (MIP) for lung cancer. TL used features learned on large datasets by models like VGG16 and ResNet50 to improve performance on new tasks. ViTs like Swin Transformer and ViT capture global context from medical images to aid analysis. However, TL has some limitations. ViTs require large datasets for pretraining to be effective. Models like DenseNet121 generalize poorly to new patients or classes. And individual institutions often have limited datasets, as seen in a supervised contrastive learning model for embryo images. Table </span><a href="#S4.T5" title="Table 5 ‣ 4.4 Advantages and disadvantages of each method ‣ 4 Federated Learning vs Transfer Learning ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.SS4.p3.1.2" class="ltx_text" style="color:#000000;"> highlights the main advantages and limitations of FL and TL.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">5 </span>Challenges and Open Issues</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="color:#000000;">Despite TL comes with a set of advantages in the medical department, using TL for CD can present several challenges and limitations.</span></p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Domain shift</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text" style="color:#000000;">Data distribution within the CD target domain may show notable inconstancy when compared to the pre-trained model’s source domain. This can result in a domain shift phenomenon, which has the potential to negatively impact the model’s overall performance </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib197" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">gu2019progressive</span> </a>; <a href="#bib.bib198" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">stacke2020measuring</span> </a></cite><span id="S5.SS1.p1.1.4" class="ltx_text" style="color:#000000;">. Not having enough data from the target domain can cause TL and FL models to perform poorly.</span></p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text" style="color:#000000;">Domain shift is caused because of variations in staining methods, differences in scanning devices between datasets, number of classes, variation in object orientation (differences in image content, view angle, brightness, noise, color etc…) and number of channels in datasets </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib199" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zoetmulder2022domain</span> </a></cite><span id="S5.SS1.p2.1.4" class="ltx_text" style="color:#000000;">. Even with the same source and target domains, Zoetmulder et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib199" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zoetmulder2022domain</span> </a></cite><span id="S5.SS1.p2.1.7" class="ltx_text" style="color:#000000;"> findings on the impact of domain shift on lesion detection accuracy were inconsistent. This implies that there is still more to learn about the effects of domain shift. Additionally, TL models often experience a drop in performance when tested on data from a different distribution than the training data. However, the authors in </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib200" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">fogelberg2023domain</span> </a></cite><span id="S5.SS1.p2.1.10" class="ltx_text" style="color:#000000;"> tried to solve domain shift by grouping images from public dermoscopic skin cancer datasets into different domains to create domain-shifted test sets. Another way to overcome domain shift is using extensive augmentations like PatchShuffling to improve the learned representation and fine-tuning pre-trained model on source dataset can handle domain shift </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib201" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">vuong2022impash</span> </a></cite><span id="S5.SS1.p2.1.13" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text" style="color:#000000;">In summary, TL and FL face serious challenges under domain shift, which must be approached with caution due to the variability in data distributions, feature spaces, tasks, and data access.</span></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Computational and Hardware Limitations</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Healthcare organizations may face limitations in their hardware capabilities and there is a need for constant monitoring and updating to maintain the performance of the FL and TL models. For example, hardware limitations can have a negative impact on TL. This could negatively impact the data collection process and the use of TL techniques. For example, in </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib202" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">marathe2017performance</span> </a>; <a href="#bib.bib203" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">whatmough2019fixynn</span> </a></cite><span id="S5.SS2.p1.1.4" class="ltx_text" style="color:#000000;">, the authors emphasized that hardware limitations such as limited memory or limited computing capacity can make training complex DL models difficult and it is a challenging process. This could limit the type and size of neural network models that can be effectively trained.</span></p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text" style="color:#000000;">In FL, devices with slower processing speeds can cause delays in the training process as they do not complete their assigned training on time. This delay can negatively impact the overall progress of model training. Additionally, devices with limited memory and bandwidth may struggle to run complex FL algorithms efficiently, potentially resulting in suboptimal model performance </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib204" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">abreha2022federated</span> </a>; <a href="#bib.bib205" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">imteaj2021survey</span> </a></cite><span id="S5.SS2.p2.1.4" class="ltx_text" style="color:#000000;">. In addition, the heterogeneity of computing, storage and communication capabilities of different devices can cause latency issues in on-device distributed training </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib205" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">imteaj2021survey</span> </a></cite><span id="S5.SS2.p2.1.7" class="ltx_text" style="color:#000000;">. Energy is then used by the devices for local model training and communication. In general, less energy efficient devices and radios have lower processing power and bandwidth. This causes batteries to drain quickly when using wireless devices </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib206" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">tran2019federated</span> </a></cite><span id="S5.SS2.p2.1.10" class="ltx_text" style="color:#000000;">. Therefore, implementing FL systems often requires overcoming complex system challenges such as unreliable device connectivity, interrupted executions, and slow convergence rates compared to learning on centralized data </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib207" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2019adaptive</span> </a>; <a href="#bib.bib208" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2020federated</span> </a></cite><span id="S5.SS2.p2.1.13" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text" style="color:#000000;">In conclusion, limited hardware resources may slow down the training process significantly, making it challenging to test diverse techniques efficiently and develop powerful and accurate FL or TL models.</span></p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Limited availability of medical annotated data</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text" style="color:#000000;">The acquisition of annotated medical data is an essential prerequisite for the effective training of ML models. Although, within the domain of CD, the attainment of annotated data can prove to be a difficult task due to ethical and privacy-related issues and the process of medical data annotation requires expertise </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">shamshiri2023compatible</span> </a></cite><span id="S5.SS3.p1.1.4" class="ltx_text" style="color:#000000;">. This can subsequently constrain the quantity of data that is accessible for TL. This lack of data can cause domain shift and different distribution between training and test data that are independent of dataset size, in which can affect drastically the performance of TL </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib209" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">alzubaidi2021novel</span> </a></cite><span id="S5.SS3.p1.1.7" class="ltx_text" style="color:#000000;">. Moreover, The limited number of annotated data and small dataset size can lead TL to problems such as miscalibration between classes and increased generalization error, especially for imbalanced classes </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib210" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">abbas2020detrac</span> </a></cite><span id="S5.SS3.p1.1.10" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text" style="color:#000000;">The field of healthcare data analysis faces several challenges related to data insufficiency, heterogeneity, and labeling. For example, studies on colon cancer outcomes suffered from insufficiency of labels in datasets and heterogeneity of studies </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib177" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">pacal2020comprehensive</span> </a></cite><span id="S5.SS3.p2.1.4" class="ltx_text" style="color:#000000;">. Also, the lack of large labeled cancer medical imaging datasets affected the training of FL and complicated the convergence during federated optimization </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib211" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rieke2020future</span> </a></cite><span id="S5.SS3.p2.1.7" class="ltx_text" style="color:#000000;">. Finally, handling non-IID data distributions and imbalanced datasets across clients is another challenge that needs to be addressed in healthcare data analysis </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib212" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rehman2023federated</span> </a></cite><span id="S5.SS3.p2.1.10" class="ltx_text" style="color:#000000;">. So for that, techniques such as data augmentation and collaborative efforts have been done to build annotated and standard datasets for CD </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib212" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rehman2023federated</span> </a></cite><span id="S5.SS3.p2.1.13" class="ltx_text" style="color:#000000;">. In general, limited data allows TL or FL models to overfitting to the specified features of the training set, non-IID data distributions and imbalanced datasets. Leading to poor general applicability and eventual model failure.</span></p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Heterogeneity of cancer</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p"><span id="S5.SS4.p1.1.1" class="ltx_text" style="color:#000000;">Cancer is a very complex and diverse ailment characterized by numerous subcategories and developmental phases (malign and benign tumors…), which presents a challenge to TL and FL capacity to generalize across dissimilar cancer subtypes and stages. In a study by Jaber et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib213" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">jaber2020deep</span> </a></cite><span id="S5.SS4.p1.1.4" class="ltx_text" style="color:#000000;"> An image-based DL intrinsic molecular subtype classifier was developed to classify breast tumors into different subtypes based on H&amp;E-stained biopsy tissue sections. The classifier was able to correctly subtype the majority of samples in a retained set of tumors, but in many cases significant heterogeneity in the assigned subtypes was observed across patches within a single overall slide image. This heterogeneity impacted the accuracy of the classifier and may require more detailed subtype analysis to improve classification results. Nyman et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib214" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">nyman2023spatially</span> </a></cite><span id="S5.SS4.p1.1.7" class="ltx_text" style="color:#000000;"> highlighted that heterogeneity within tumor samples (microheterogeneity) is common and can impact CD using DL. Quantifying and accounting for this heterogeneity is important for improving cancer classification, prediction, and treatment planning. Heterogeneity within a tumor can lead to misclassifications or lower accuracy for DL models </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib215" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">inglese2017deep</span> </a></cite><span id="S5.SS4.p1.1.10" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Data Privacy and Security</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><span id="S5.SS5.p1.1.1" class="ltx_text" style="color:#000000;">FL allows different organizations to work together and train DL models without sharing their sensitive data. Unfortunately, there are several risks associated with this approach, such as model poisoning, label flipping, and backdoor attacks </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib157" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yang2023demac</span> </a>; <a href="#bib.bib159" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sanchez2023robust</span> </a>; <a href="#bib.bib163" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">bagdasaryan2020backdoor</span> </a></cite><span id="S5.SS5.p1.1.4" class="ltx_text" style="color:#000000;">. These threats can compromise the integrity and accuracy of the models being developed. To avoid these risks, it is important to detect any malicious activity early on and take appropriate action. Moreover, FL also presents some unique challenges when it comes to data privacy, security, and dealing with limited datasets. These issues can be tackled by safeguarding confidential information and minimizing communication expenses </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib216" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">khan2021federated</span> </a></cite><span id="S5.SS5.p1.1.7" class="ltx_text" style="color:#000000;">. However, strict privacy policies and restricted data access pose limitations for collaborative model building across institutions. Implementing FL can be a complex process due to high communication overhead and intricate system requirements. Therefore, addressing these obstacles is critical to ensuring that FL succeeds in practical settings and in a healthy environment.</span></p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Interpretability and Algorithm Challenges</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p"><span id="S5.SS6.p1.1.1" class="ltx_text" style="color:#000000;">Adapting powerful TL models like transformers and CNN with FL to computer vision can improve the accuracy of image analysis tasks and the performance of decentralized learning algorithms, but big issue remains: Interpretability.</span></p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p id="S5.SS6.p2.1" class="ltx_p"><span id="S5.SS6.p2.1.1" class="ltx_text" style="color:#000000;">The decentralized design of FL creates difficulties for traditional model interpretation approaches since they usually need complete access to all features, training data, and internal model components </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib217" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2019interpret</span> </a></cite><span id="S5.SS6.p2.1.4" class="ltx_text" style="color:#000000;">. It is difficult to interpret why a single global model works well or poorly for different clients, without interpretability in the client selection and aggregation process, it is hard to understand and improve the FL system </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib218" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">qin2023reliable</span> </a></cite><span id="S5.SS6.p2.1.7" class="ltx_text" style="color:#000000;">. In TL and despite it is difficult to understand how the TL models make decisions. Interpretability helps understand the interface between source and target domains. For this reason, the authors in </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib219" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kim2019structure</span> </a></cite><span id="S5.SS6.p2.1.10" class="ltx_text" style="color:#000000;"> proposed a feature network (FN) approach that uses interpretable features defined by humans to improve interpretability in TL and improve the results. This address challenges in TL like negative transfer, insufficient target data, and lack of transparency </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib220" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">mao2022interpretable</span> </a></cite><span id="S5.SS6.p2.1.13" class="ltx_text" style="color:#000000;">. As a result, new strategies must be created to offer transparent and interpretable explanations about how FL and TL models work while still maintaining the privacy of the data and improve the accuracy.</span></p>
</div>
</section>
<section id="S5.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.7 </span>3D Imaging</h3>

<div id="S5.SS7.p1" class="ltx_para">
<p id="S5.SS7.p1.1" class="ltx_p"><span id="S5.SS7.p1.1.1" class="ltx_text" style="color:#000000;">Medical image analysis can be challenging, especially when it comes to 3D images. One of the most important challenges is data availability, this is due to the difficulty and expense of obtaining 3D images compared to 2D images, which prevents the availability of big and diverse datasets for training TL models </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib221" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">chen2019med3d</span> </a></cite><span id="S5.SS7.p1.1.4" class="ltx_text" style="color:#000000;">. Despite techniques such as data augmentation are used to add number of samples but the authors in </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib222" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">gupta2020performance</span> </a></cite><span id="S5.SS7.p1.1.7" class="ltx_text" style="color:#000000;"> stated that 2D data augmentation techniques like rotations, flips and crops don’t directly translate to 3D. Furthermore, in comparison to 2D images, 3D images are more complex and require more advanced techniques to analyze </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib223" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">cheplygina2019not</span> </a></cite><span id="S5.SS7.p1.1.10" class="ltx_text" style="color:#000000;">. Another issue is data heterogeneity, this is due to diverse modalities, dimensionalities, and features that can be found in 3D images, along with differences in acquisition and demographics </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib224" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2016new</span> </a></cite><span id="S5.SS7.p1.1.13" class="ltx_text" style="color:#000000;">, which can affect negatively TL. For example, the authors in </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib225" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhou2019models</span> </a></cite><span id="S5.SS7.p1.1.16" class="ltx_text" style="color:#000000;"> stated that 3D medical images can be highly complex, analyzing 3D structure adds complexity compared to 2D images.</span></p>
</div>
<div id="S5.SS7.p2" class="ltx_para">
<p id="S5.SS7.p2.1" class="ltx_p"><span id="S5.SS7.p2.1.1" class="ltx_text" style="color:#000000;">Even with these challenges, researches are actively investigating to improve 3D medical image analysis through new approaches and techniques that could have a significant positive impact on TL training.</span></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">6 </span>Case Studies</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="color:#000000;">FL and TL have shown promising applications in CD based on image analysis. The following subsections provide more details on current applications, future directions, and limitations of these methods:</span></p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Example of FL in CD</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text" style="color:#000000;">Kareem et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib226" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kareem2023federated</span> </a></cite><span id="S6.SS1.p1.1.4" class="ltx_text" style="color:#000000;"> proposed a framework for medical image detection using FL where training data was divided among virtual devices representing medical institutions. Pre-trained models like AlexNet, DenseNet, ResNet-50, Inception, and VGG19 were used in the FL framework. The FL ResNet-50 model achieved 93% accuracy, while VGG19 achieved 97.94% accuracy for pneumonia detection when combined with an alternative classifier. FL integrated with TL models showed promise for enhancing disease detection in medical images while ensuring data privacy. However, more details on the data sources used for training would help better assess the proposed model’s performance. Repetto et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib227" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">repetto2022breast</span> </a></cite><span id="S6.SS1.p1.1.7" class="ltx_text" style="color:#000000;"> proposed an FL model using goal programming to optimize model performance across nodes for breast CD. Experiments were conducted on the Wisconsin Breast Cancer Dataset. Compared to the initial model, the FL model showed significant 15-27% improvements on metrics like J-index and AUC when evaluated on test data. The federated model also outperformed models trained at individual sites. The results demonstrated the efficacy of the proposed FL approach for breast CD. Pati et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib228" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">pati2022federated</span> </a></cite><span id="S6.SS1.p1.1.10" class="ltx_text" style="color:#000000;"> applied FL to detect tumor boundaries in glioblastoma, a rare brain cancer, using 1963 MRI scans from 71 global sites. Model performance was evaluated on test data from participating sites and completely unseen data. The FL model showed 15-33% improvements in Dice scores over the initial model, demonstrating its efficacy. However, only a single 3D-ResUNet architecture was tried. Evaluating other neural network architectures could have found an even better model for this use case. Arthi et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib229" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">arthi2022decentralized</span> </a></cite><span id="S6.SS1.p1.1.13" class="ltx_text" style="color:#000000;"> proposed a FL-based model for detecting colorectal cancer in histopathological images while ensuring privacy. The authors used heterogeneous datasets from multiple healthcare facilities with 0.1 million sample histological images of human colon cancer and healthy tissue with 9 classes. The authors applied various CNN models and found that ResNeXt50 achieved the highest accuracy of 99.53%. They then implemented ResNeXt50 on FL, resulting in an accuracy of 96.045% and an F1 score of 0.96. Fig. </span><a href="#S7.F11" title="Figure 11 ‣ 7.4 Decentralized Training Paradigm for FL: ‣ 7 Future Directions ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">11</span></a><span id="S6.SS1.p1.1.14" class="ltx_text" style="color:#000000;"> representes the performance of the study across 20 communication rounds, with 10 epochs per round. The proposed workflow involved acquiring a dataset, image processing, and dividing the dataset into train, test, and validation data in which the dataset was distributed among local devices for training and testing to use the FL architecture. Twenty communication rounds between local and global models were performed to aggregate the trained models. We conducted experiments using Google Colaboratory </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib230" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">bisong2019colab</span> </a></cite><span id="S6.SS1.p1.1.17" class="ltx_text" style="color:#000000;"> notebooks with a Tesla K80 GPU backend accelerator and 12GB of RAM allocation. As shown, we were able to extract the accuracy and loss of ResNeXt50 on FL for detecting colorectal cancer in
histopathological images in Fig. </span><a href="#S7.F11" title="Figure 11 ‣ 7.4 Decentralized Training Paradigm for FL: ‣ 7 Future Directions ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">11</span></a><span id="S6.SS1.p1.1.18" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Example of TL in CD</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p"><span id="S6.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Kassani et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib231" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kassani2022deep</span> </a></cite><span id="S6.SS2.p1.1.4" class="ltx_text" style="color:#000000;"> conducted a comparison study of TL techniques for classifying colorectal cancer images. They used five pre-trained models (DenseNet121, DenseNet201, InceptionV3, MobileNetV2, ResNet50) with knowledge from ImageNet. Three TL techniques were compared: using the original network, fine-tuning, and adding convolutional layers. The TCGA CRC DX dataset was used. MobileNetV2 achieved the highest accuracy around 98%. TL showed potential to improve cancer classification, though more techniques like using Imagenet and evaluation metrics could give a more comprehensive analysis. Luo et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib232" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">luo2023systematic</span> </a></cite><span id="S6.SS2.p1.1.7" class="ltx_text" style="color:#000000;"> also studied different TL techniques like fine-tuning and adding convolutional layers for classifying colorectal cancer images. Cross-validation assessed performance. Adding convolutional layers outperformed other methods. TL was found efficient for small medical datasets and could reduce workload. More background on colorectal cancer’s significance would have helped readers. Freitas et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib233" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">freitas2022detection</span> </a></cite><span id="S6.SS2.p1.1.10" class="ltx_text" style="color:#000000;"> applied TL to pretrain models (AlexNet, VGG16, GoogLeNet, ResNet) for diagnosing bladder cancer in cystoscopy images. Capnets achieved 96.9% accuracy, outperforming other TL models. TL increased performance over traditional techniques without requiring extensive data. A larger dataset and comparison to other DL models could have provided more insights. Aziz et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib234" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">azizi2017transfer</span> </a></cite><span id="S6.SS2.p1.1.13" class="ltx_text" style="color:#000000;"> investigated using TL for prostate CD from ultrasound data. TL could compensate for differences between data types. Cancerous regions were accurately identified, showing promise. But evaluation on other cancers would have been beneficial. Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib235" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhang2022shuffle</span> </a></cite><span id="S6.SS2.p1.1.16" class="ltx_text" style="color:#000000;"> proposed a Shuffle Instances-based ViT (SI-ViT) approach that reduced image perturbations and improve cross-instance modeling. SI-ViT achieved significantly higher accuracy of 94% compared to CNNs and basic ViT 91% on a dataset of 1773 cancer and 3315 normal images. Fig. </span><a href="#S7.F12" title="Figure 12 ‣ 7.4 Decentralized Training Paradigm for FL: ‣ 7 Future Directions ‣ Federated and Transfer Learning for Cancer Detection Based on Image Analysis" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">12</span></a><span id="S6.SS2.p1.1.17" class="ltx_text" style="color:#000000;"> demonstrates the results of our implementation of various models on MICCAI datasets using the code provided by Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib236" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">si2022milsicode</span> </a></cite><span id="S6.SS2.p1.1.20" class="ltx_text" style="color:#000000;">. The experiments were conducted using Google Colaboratory </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib230" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">bisong2019colab</span> </a></cite><span id="S6.SS2.p1.1.23" class="ltx_text" style="color:#000000;"> notebooks with a Tesla K80 GPU backend accelerator and 12GB of RAM allocation. As shown, we were able to replicate the findings that SI-ViT effectively extracted pathological patterns for ROSE diagnosis </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib235" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhang2022shuffle</span> </a></cite><span id="S6.SS2.p1.1.26" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">7 </span>Future Directions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p"><span id="S7.p1.1.1" class="ltx_text" style="color:#000000;">Testing FL and TL approaches in a real clinical setting is critical before deployment. It is also important to evaluate the performance, bias, interpretability and accountability of the model for implementing these techniques into clinical practice. Overall, FL and TL are exciting directions for the further development of privacy-preserving and precise AI in healthcare. These are future directions of this work:</span></p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>FL for non-IID and Heterogeneous Data:</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p"><span id="S7.SS1.p1.1.1" class="ltx_text" style="color:#000000;">Dealing with non-IID and heterogeneous data is a critical challenge in FL, especially in the medical domain. Medical data is often distributed across federated clients, leading to non-uniform data distributions due to differences in data collection methods, user demographics, environmental conditions, and device configurations </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2023heterogeneity</span> </a></cite><span id="S7.SS1.p1.1.4" class="ltx_text" style="color:#000000;">. This imbalanced distribution of data can create generalized models that are flexible to different use cases, but their performance is drastically impacted </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib237" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">cheng2023gfl</span> </a></cite><span id="S7.SS1.p1.1.7" class="ltx_text" style="color:#000000;">. To address this issues, researchers have proposed various solutions. One promising approach is the Label-wise Clustering algorithm (FedLC) </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib238" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rehman2024fedcscd</span> </a></cite><span id="S7.SS1.p1.1.10" class="ltx_text" style="color:#000000;">, which guarantees convergence among local clients that hold unique data distributions, offering robust convergence on highly skewed and biased non-IID datasets </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib238" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rehman2024fedcscd</span> </a></cite><span id="S7.SS1.p1.1.13" class="ltx_text" style="color:#000000;">. Another study focuses on mitigating the negative impact of batch normalization (BN) on FL performance in the presence of non-IID data. The proposed FedTAN method aims to achieve robust FL performance under various data distributions </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib239" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2023batch</span> </a></cite><span id="S7.SS1.p1.1.16" class="ltx_text" style="color:#000000;">. Moreover, Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib240" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhang2023tackling</span> </a></cite><span id="S7.SS1.p1.1.19" class="ltx_text" style="color:#000000;"> proposed FedGH, an approach that addresses local drifts caused by non-IID data and device heterogeneity by harmonizing gradients during server aggregation. FedGH has demonstrated impressive results on FL baselines across various benchmarks and non-IID scenarios. In addition to addressing non-IID data, researchers have explored data synthesis techniques to augment and improve FL performance. Li et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib241" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2024feature</span> </a></cite><span id="S7.SS1.p1.1.22" class="ltx_text" style="color:#000000;"> proposed a novel data synthesis method called hard feature matching data synthesis (HFMDS), which optimizes synthetic data to be task-relevant and privacy-preserving by matching the class-relevant features of real data. When integrated with FL (HFMDS-FL), this method consistently outperformed baseline methods in terms of accuracy, privacy preservation, and computational costs.</span></p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Large language model for FL:</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p"><span id="S7.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Large language models (LLMs) have emerged as a transformative technology in natural language processing, demonstrating remarkable capabilities in understanding and generating human-like text </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib242" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">thirunavukarasu2023large</span> </a></cite><span id="S7.SS2.p1.1.4" class="ltx_text" style="color:#000000;">. The combination of LLMs and FL offers a promising future direction, sharing the powerful capabilities of LLMs while addressing the privacy concerns associated with their training </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib243" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">bose2023fully</span> </a></cite><span id="S7.SS2.p1.1.7" class="ltx_text" style="color:#000000;">. Several studies have combined both techniques. For example, Sadot et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib244" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">sadot2023novel</span> </a></cite><span id="S7.SS2.p1.1.10" class="ltx_text" style="color:#000000;"> have proposed a novel approach that combined FL with BERT, a large language model, and 1D-CNN for multi-label text classification on a large text dataset. Compared to centralized training on the entire dataset, the federated setup divided the data into groups and demonstrated lower computational power requirements while achieving higher accuracy, precision, F1 score, and lower Hamming loss for an equivalent global model. Kuang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib245" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">kuang2023federatedscope</span> </a></cite><span id="S7.SS2.p1.1.13" class="ltx_text" style="color:#000000;"> introduced FederatedScope-LLM (FS-LLM), a comprehensive package for fine-tuning LLMs in FL settings. FS-LLM consists of three main components, LLM-BENCHMARKS; LLM-ALGZOO; LLM-TRAINER. The authors discussed challenges such as optimizing communication and computational resources, handling various data preparation tasks, and addressing distinct information protection demands. Liu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib246" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">liu2023differentially</span> </a></cite><span id="S7.SS2.p1.1.16" class="ltx_text" style="color:#000000;"> introduced DP-LoRA, that integrates differential privacy (to guarantee data privacy) and low-rank adaptation (to reduce communication overhead during distributed training). The experimental results demonstrated the effectiveness of DP-LoRA in maintaining strict privacy constraints while minimizing communication overhead. Ye et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib247" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">ye2024openfedllm</span> </a></cite><span id="S7.SS2.p1.1.19" class="ltx_text" style="color:#000000;"> proposed OpenFedLLM combining instruction tuning, value alignment for LLMs, FL algorithms, and support for multiple datasets. Empirical studies using OpenFedLLM showed that models trained with FL consistently outperformed those trained individually on private data, achieving up to 12% improvement on benchmarks like MT-Bench for general datasets. All these studies provides insights and suggests new directions for future work in the area of FL.</span></p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Collaborative FL:</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p"><span id="S7.SS3.p1.1.1" class="ltx_text" style="color:#000000;">Collaborative FL is emerging as a promising approach to enable secure multi-party collaboration while preserving data privacy in a decentralized, trustworthy, and flexible manner. This paradigm allows participants to collaboratively train a model without frequently sending local models or gradients to a central server, thus ensuring the privacy of their data. Recent advancements in this field have demonstrated the potential of Collaborative FL in various domains, particularly in healthcare applications </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib248" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">abou2022collaborative</span> </a></cite><span id="S7.SS3.p1.1.4" class="ltx_text" style="color:#000000;">. Moving forward, Collaborative FL is poised to play a pivotal role in enabling collaborative training using synthetic data generated by GANs. Frameworks like FedCSCD-GAN combined optimized FL and GAN-based optimization to address privacy, data quality, and model performance challenges </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib249" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">li2023point</span> </a></cite><span id="S7.SS3.p1.1.7" class="ltx_text" style="color:#000000;">. By using synthetic data, such approaches can improve overall performance, stabilize classifiers, and prevent overfitting, while ensuring data privacy. In the case of cancer diagnosis and subtype prediction, Collaborative FL has shown promising results. Techniques like HistFL </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib238" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">rehman2024fedcscd</span> </a></cite><span id="S7.SS3.p1.1.10" class="ltx_text" style="color:#000000;">, which incorporate attention-based multiple instance learning (MIL) and differential privacy, have demonstrated improved performance in collaboratively training models on multi-site whole slide images (WSIs). Additionally, collaborative healthcare diagnostic systems have the potential to improve local diagnosis performance, particularly for resource-constrained institutions with lower-quality imaging data </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib250" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">gao2023federated</span> </a></cite><span id="S7.SS3.p1.1.13" class="ltx_text" style="color:#000000;">. Furthermore, Collaborative FL has been successfully applied to breast cancer detection, achieving impressive F1-scores of 97% </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib251" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">almufareh2023federated</span> </a></cite><span id="S7.SS3.p1.1.16" class="ltx_text" style="color:#000000;">, showcasing its advantage over traditional centralized approaches. As research in this area continues, it is anticipated that Collaborative FL will play a pivotal role in enabling secure and privacy-preserving collaboration across various domains, facilitating the development of robust and accurate models while protecting sensitive data.</span></p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Decentralized Training Paradigm for FL:</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p"><span id="S7.SS4.p1.1.1" class="ltx_text" style="color:#000000;">In decentralized FL, there is no central server that aggregates the model updates from the local devices. Instead, the devices communicate with each other and aggregate the updates in a peer-to-peer fashion. This approach can further enhance data privacy and security as there is no need to transmit any data, even in encrypted form, to a central server.</span></p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p"><span id="S7.SS4.p2.1.1" class="ltx_text" style="color:#000000;">Roy el al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">roy2019braintorrent</span> </a></cite><span id="S7.SS4.p2.1.4" class="ltx_text" style="color:#000000;"> proposed an approach called BrainTorrent, in which there is no central server that aggregates model updates from the different sites. Instead, sites share and aggregate model weights directly in a peer-to-peer fashion</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S7.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F11.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2405.20126/assets/x12.png" id="S7.F11.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="352" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S7.F11.sf1.5.2" class="ltx_text" style="font-size:90%;">Accuracy</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F11.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2405.20126/assets/x13.png" id="S7.F11.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.sf2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S7.F11.sf2.5.2" class="ltx_text" style="font-size:90%;">Loss</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.6.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S7.F11.7.2" class="ltx_text" style="font-size:90%;">Performance (Accuracy and Loss) of FL based model with ResNeXt50 for detecting colorectal cancer in histopathological images <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib229" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">arthi2022decentralized</span> </a></cite></span></figcaption>
</figure>
<figure id="S7.F12" class="ltx_figure"><img src="/html/2405.20126/assets/x14.png" id="S7.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="323" height="273" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="S7.F12.8.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S7.F12.9.2" class="ltx_text" style="font-size:90%;">SI-ViT and CNN models for Pancreatic Cancer MICCAI Image Classification based on accuracy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib235" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhang2022shuffle</span> </a></cite>.</span></figcaption>
</figure>
<div id="S7.SS4.p3" class="ltx_para">
<p id="S7.SS4.p3.1" class="ltx_p"><span id="S7.SS4.p3.1.1" class="ltx_text" style="color:#000000;">without a central coordinator. The authors found that BrainTorrent showed good results and outperformed traditional FL with a global server. The study in </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib252" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">tedeschini2022decentralized</span> </a></cite><span id="S7.SS4.p3.1.4" class="ltx_text" style="color:#000000;"> discussed the use of decentralized FL for healthcare networks, specifically in the context of brain tumor segmentation. The authors propose a real-time platform for integrating network and FL functions, and validate the proposed solution in a real-world deployment. In </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib253" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wicaksana2022customized</span> </a></cite><span id="S7.SS4.p3.1.7" class="ltx_text" style="color:#000000;"> the authors proposed CusFL as a Personalized FL framework to handle the inter-client data heterogeneity issue in traditional FL, the authors evaluated CusFL on two multi-source medical image classification tasks: prostate cancer identification and skin lesion classification, demonstrating its superiority over traditional FL in handling the inter-client heterogeneity present in decentralized medical data. Decentralized FL can be particularly useful in scenarios where there is no trusted central authority or where the communication with a central server is not feasible due to network constraints.</span></p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>Domain Adaptation for TL:</h3>

<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p"><span id="S7.SS5.p1.1.1" class="ltx_text" style="color:#000000;">The main advantage of using domain adaptation is that it helps improve the general performance of TL models when there is a domain shift between the training and testing datasets </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib254" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">han2023tl</span> </a></cite><span id="S7.SS5.p1.1.4" class="ltx_text" style="color:#000000;">. The domain adaptation aspect arises from the ability to effectively integrate and transfer knowledge from data-rich source domains to data-poor target domains, without negative transfer when the domains are not closely related </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib255" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">hajiramezanali2018bayesian</span> </a>; <a href="#bib.bib256" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">choudhary2020advancing</span> </a></cite><span id="S7.SS5.p1.1.7" class="ltx_text" style="color:#000000;">. For example, Gu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib197" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">gu2019progressive</span> </a></cite><span id="S7.SS5.p1.1.10" class="ltx_text" style="color:#000000;"> explored extending the cycle consistent adversarial networks (CycleGAN) approach to handle larger structural changes between source and target domains, as the authors note that the model struggles with large differences. The authors applied the domain adaptation approaches to skin lesion classification where training data is limited. This has led to higher classification accuracy on the target dataset compared to just training on the original target data. You et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib257" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">you2019towards</span> </a></cite><span id="S7.SS5.p1.1.13" class="ltx_text" style="color:#000000;"> applied deep embedded validation (DEV) for model selection in domain adaptation tasks specific to cancer detection, where the source and target domains differ in imaging modalities, scanner types and patient populations. DEA improved model selection and domain adaptation performance for TL in CD tasks involving different imaging domains. Wang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib258" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">wang2023iterative</span> </a></cite><span id="S7.SS5.p1.1.16" class="ltx_text" style="color:#000000;"> proposed using a gaussian kernel-based distance constraint (DDA) to reduce the distance between the source domain and target domain in the feature space. This is a type of discrepancy-based domain adaptation. It helped decouple the motion-related features from user-specific features, this has improved TL. So domain adaptation techniques aims to reduce the cross-domain distribution shift by adapting source knowledge to the target domain, enabling transfer of learning across domains.</span></p>
</div>
</section>
<section id="S7.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">7.6 </span>Multimodal TL:</h3>

<div id="S7.SS6.p1" class="ltx_para">
<p id="S7.SS6.p1.1" class="ltx_p"><span id="S7.SS6.p1.1.1" class="ltx_text" style="color:#000000;">Multimodal TL is an emerging paradigm that shows promise for various applications, particularly in the medical field. It refers to the process of transferring knowledge from one or more pre-trained models across different data modalities (such as text, images, audio, etc.) to a target model operating on multiple modalities simultaneously </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib259" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">azher2023assessment</span> </a></cite><span id="S7.SS6.p1.1.4" class="ltx_text" style="color:#000000;">. The main advantage of multimodal TL is that it allows the target model to learn from different types of data more effectively, thus overcoming the challenge of limited data availability in certain modalities. In the field of medical imaging, multimodal TL has demonstrated its potential in various tasks such as cross-modal retrieval (CMR), prostate cancer detection and brain image analysis. For example, the deep multimodal TL (DMTL) approach </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib260" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhen2020deep</span> </a></cite><span id="S7.SS6.p1.1.7" class="ltx_text" style="color:#000000;"> transferred knowledge from known categories to new categories by learning distinguishing features from both annotated labels and pseudo-labels, thereby enabling multiple baselines and existing methods with respect to the MAP exceeds scores for CMR. In addition, researchers have investigated the fusion of features from different input modalities, such as: B. different MRI modalities, to obtain a more comprehensive representation of the clinical condition of interest </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib261" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yuan2019prostate</span> </a></cite><span id="S7.SS6.p1.1.10" class="ltx_text" style="color:#000000;">. In the field of brain image analysis, multimodal deep TL techniques have been proposed to utilize different viewing planes (axial, sagittal and coronal) in MRI brain images </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib259" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">azher2023assessment</span> </a></cite><span id="S7.SS6.p1.1.13" class="ltx_text" style="color:#000000;">. These techniques include novel multimodal feature encoders and adaptation techniques to address the distribution shift between training and test sets to improve overall performance. Furthermore, multimodal TL has played a crucial role in improving the segmentation accuracy of hypopharyngeal cancer risk areas in medical images </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib262" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">zhang2023twist</span> </a></cite><span id="S7.SS6.p1.1.16" class="ltx_text" style="color:#000000;">. Approaches like Twist-Net combine low-level detail and high-level semantic information from different feature maps, resulting in improved segmentation performance. Looking forward, multimodal TL will play a crucial role in overcoming the challenges of limited data availability and improving the performance of various medical imaging tasks. By using complementary information from multiple data modalities such as X-ray, ultrasound and CT scans </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib263" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">horry2020covid</span> </a></cite><span id="S7.SS6.p1.1.19" class="ltx_text" style="color:#000000;">, multimodal TL techniques can potentially open new avenues for more accurate and robust analysis and diagnosis of medical imaging.</span></p>
</div>
</section>
<section id="S7.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">7.7 </span>Attention Mechanism for TL:</h3>

<div id="S7.SS7.p1" class="ltx_para">
<p id="S7.SS7.p1.1" class="ltx_p"><span id="S7.SS7.p1.1.1" class="ltx_text" style="color:#000000;">Future studies could investigate more sophisticated attention modules that can better capture the intricate patterns and relationships present in medical imaging data. For instance, self-attention mechanisms have been explored using transformer architectures or graph attention networks </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib264" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">khan2023transformers</span> </a></cite><span id="S7.SS7.p1.1.4" class="ltx_text" style="color:#000000;">.
Furthermore, instead of simply freezing shallow layers during TL attention can be used to guide the layers or weights to transfer and fine-tune them based on the target task and dataset. This could lead to more efficient and effective transfer, and while transferring between related tasks such as different cancer types showed benefits, it would be valuable to study attention-based transfer between radically different imaging domains (e.g., transferring from natural images to medical images). This could open up new sources of pretrained models for use.
Moreover, using TL on multiple complementary imaging modalities (for example, MRI, CT, and ultrasound) by fusing their representations could provide more comprehensive models </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">yu2022survey</span> </a></cite><span id="S7.SS7.p1.1.7" class="ltx_text" style="color:#000000;">.
More sophisticated attention schemes, such as transformer-based models, can potentially capture longer-range dependencies and achieve even better performance in transfer tasks. Multi-head and hierarchical attention architectures are also worth exploring.
Many medical datasets lack comprehensive annotation. Attention mechanisms can be used to transfer knowledge from strongly annotated datasets to tasks with weak labels or small amounts of labeled data through techniques such as multiple-instance learning </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib265" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">latif2023transformers</span> </a></cite><span id="S7.SS7.p1.1.10" class="ltx_text" style="color:#000000;">.
Electronic health records often contain multimodal data such as images, text reports, and genomic analyses. Attention provides a principled way to fuse heterogeneous data sources into unified predictive models in a transfer-learning setting </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib266" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">shaik2023survey</span> </a></cite><span id="S7.SS7.p1.1.13" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p"><span id="S8.p1.1.1" class="ltx_text" style="color:#000000;">This review has highlighted the significant potential and challenges of federated learning (FL) and transfer learning (TL) in advancing cancer detection through medical image analysis. FL’s privacy-preserving capabilities allow for collaborative model training across multiple institutions without compromising patient data, while TL leverages pre-existing models to improve performance even with limited labeled data. Together, they represent a promising direction for overcoming current limitations in medical image analysis, particularly in cancer detection.
However, challenges such as domain shift, computational and hardware limitations, data privacy, heterogeneity of cancer, and the limited availability of annotated medical data persist. These challenges highlight the need for ongoing research to refine these methodologies and ensure their applicability in real-world clinical settings.</span></p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p"><span id="S8.p2.1.1" class="ltx_text" style="color:#000000;">Future directions in this field should focus on enhancing models’ ability to handle non-IID and heterogeneous data more effectively; improving the accuracy and general applicability of FL and TL in diverse medical contexts; and developing strategies for collaborative FL to further enhance privacy preservation while leveraging the collective strength of distributed data; exploring decentralized training paradigms to mitigate central points of failure and further enhance privacy and security; advancing domain adaptation techniques within TL to better manage the domain shift and improve model performance across different medical imaging tasks; leveraging multimodal TL to integrate diverse data types and sources for a more holistic understanding and detection of cancer; and incorporating attention mechanisms in TL to improve model interpretability and efficacy, particularly in complex cancer detection tasks.</span></p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p"><span id="S8.p3.1.1" class="ltx_text" style="color:#000000;">As the fields of FL and TL continue to evolve, their integration into clinical workflows promises not only to enhance cancer detection accuracy but also to democratize access to advanced diagnostic tools. This will require a concerted effort from researchers, clinicians, and policymakers to navigate the ethical, technical, and operational challenges ahead. Ultimately, the goal is to harness the power of collaborative and intelligent computing to save lives and improve health outcomes globally.</span></p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">Data availability</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text" style="color:#000000;">Data will be made available on request.</span></p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">Conflict of Interest</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p"><span id="Sx2.p1.1.1" class="ltx_text" style="color:#000000;">The authors declare no conflicts of interest.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="color:#000000;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="color:#000000;">(1)</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="color:#000000;">
Anukriti, A. Dhasmana, S. Uniyal, P. Somvanshi, U. Bhardwaj, M. Gupta,
S. Haque, M. Lohani, D. Kumar, J. Ruokolainen, et al., Investigation of
precise molecular mechanistic action of tobacco-associated carcinogen
‘nnk’induced carcinogenesis: A system biology approach, Genes 10 (8)
(2019) 564.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="color:#000000;">(2)</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="color:#000000;">
D. Shrivastava, S. Sanyal, A. K. Maji, D. Kandar, Bone cancer detection using
machine learning techniques, in: Smart Healthcare for Disease Diagnosis and
Prevention, Elsevier, 2020, pp. 175–183.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="color:#000000;">(3)</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="color:#000000;">
A. Hamza, B. Lekouaghet, Y. Himeur, Hybrid whale-mud-ring optimization for
precise color skin cancer image segmentation, in: 2023 6th International
Conference on Signal Processing and Information Security (ICSPIS), IEEE,
2023, pp. 87–92.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="color:#000000;">(4)</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="color:#000000;">
M. Tahmooresi, A. Afshar, B. B. Rad, K. Nowshath, M. Bamiah, Early detection of
breast cancer using machine learning techniques, Journal of
Telecommunication, Electronic and Computer Engineering (JTEC) 10 (3-2) (2018)
21–27.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="color:#000000;">(5)</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="color:#000000;">
A. Bechar, Y. Elmir, R. Medjoudj, Y. Himeur, A. Amira, Harnessing transformers:
A leap forward in lung cancer image detection, in: 2023 6th International
Conference on Signal Processing and Information Security (ICSPIS), IEEE,
2023, pp. 218–223.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="color:#000000;">(6)</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="color:#000000;">
K. Pradhan, P. Chawla, Medical internet of things using machine learning
algorithms for lung cancer detection, Journal of Management Analytics 7 (4)
(2020) 591–623.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="color:#000000;">(7)</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="color:#000000;">
C. Farrelly, Y. Singh, Q. A. Hathaway, G. Carlsson, A. Choudhary, R. Paul,
G. Doretto, Y. Himeur, S. Atalls, W. Mansoor, Current topological and machine
learning applications for bias detection in text, in: 2023 6th International
Conference on Signal Processing and Information Security (ICSPIS), IEEE,
2023, pp. 190–195.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="color:#000000;">(8)</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="color:#000000;">
Q. Wu, W. Zhao, Small-cell lung cancer detection using a supervised machine
learning algorithm, in: 2017 international symposium on computer science and
intelligent controls (ISCSIC), IEEE, 2017, pp. 88–91.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="color:#000000;">(9)</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="color:#000000;">
M. Iman, H. R. Arabnia, K. Rasheed, A review of deep transfer learning and
recent advancements, Technologies 11 (2) (2023) 40.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="color:#000000;">(10)</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="color:#000000;">
T. Wittkopp, A. Acker, Decentralized federated learning preserves model and
data privacy, in: Service-Oriented Computing–ICSOC 2020 Workshops: AIOps,
CFTIC, STRAPS, AI-PA, AI-IOTS, and Satellite Events, Dubai, United Arab
Emirates, December 14–17, 2020, Proceedings, Springer, 2021, pp. 176–187.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="color:#000000;">(11)</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="color:#000000;">
Y. Himeur, S. Al-Maadeed, H. Kheddar, N. Al-Maadeed, K. Abualsaud, A. Mohamed,
T. Khattab, Video surveillance using deep transfer learning and deep domain
adaptation: Towards better generalization, Engineering Applications of
Artificial Intelligence 119 (2023) 105698.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="color:#000000;">(12)</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="color:#000000;">
H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, Deep transfer
learning for automatic speech recognition: Towards better generalization,
arXiv preprint arXiv:2304.14535 (2023).
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="color:#000000;">(13)</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="color:#000000;">
H. Bousbiat, R. Bousselidj, Y. Himeur, A. Amira, F. Bensaali, F. Fadli,
W. Mansoor, W. Elmenreich, Crossing roads of federated learning and smart
grids: Overview, challenges, and perspectives, arXiv preprint
arXiv:2304.08602 (2023).
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="color:#000000;">(14)</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="color:#000000;">
R. Razavi-Far, B. Wang, M. E. Taylor, Q. Yang, An introduction to federated and
transfer learning, in: Federated and Transfer Learning, Springer, 2022, pp.
1–6.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="color:#000000;">(15)</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="color:#000000;">
J. Ferlay, M. Colombet, I. Soerjomataram, D. M. Parkin, M. Piñeros,
A. Znaor, F. Bray, Cancer statistics for the year 2020: An overview,
International journal of cancer 149 (4) (2021) 778–789.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="color:#000000;">(16)</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="color:#000000;">
P. R. Jeyaraj, E. R. Samuel Nadar, Computer-assisted medical image
classification for early diagnosis of oral cancer employing deep learning
algorithm, Journal of cancer research and clinical oncology 145 (2019)
829–837.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="color:#000000;">(17)</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="color:#000000;">
X. Jiang, Z. Hu, S. Wang, Y. Zhang, Deep learning for medical image-based
cancer diagnosis, Cancers 15 (14) (2023) 3608.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="color:#000000;">(18)</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="color:#000000;">
C. Ebert, P. Louridas, Machine learning, IEEE Software 33 (5) (2016) 110–115.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="color:#000000;">(19)</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="color:#000000;">
T. Saba, Recent advancement in cancer detection using machine learning:
Systematic survey of decades, comparisons and challenges, Journal of
Infection and Public Health 13 (9) (2020) 1274–1289.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="color:#000000;">(20)</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="color:#000000;">
Y. Xie, W.-Y. Meng, R.-Z. Li, Y.-W. Wang, X. Qian, C. Chan, Z.-F. Yu, X.-X.
Fan, H.-D. Pan, C. Xie, et al., Early lung cancer diagnostic biomarker
discovery by machine learning methods, Translational oncology 14 (1) (2021)
100907.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="color:#000000;">(21)</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="color:#000000;">
A. R. Vaka, B. Soni, S. Reddy, Breast cancer detection by leveraging machine
learning, Ict Express 6 (4) (2020) 320–324.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="color:#000000;">(22)</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="color:#000000;">
S. A. Mohammed, S. Darrab, S. A. Noaman, G. Saake, Analysis of breast cancer
detection using different machine learning techniques, in: Data Mining and
Big Data: 5th International Conference, DMBD 2020, Belgrade, Serbia, July
14–20, 2020, Proceedings 5, Springer, 2020, pp. 108–117.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="color:#000000;">(23)</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="color:#000000;">
A. Mehmood, S. Yang, Z. Feng, M. Wang, A. S. Ahmad, R. Khan, M. Maqsood,
M. Yaqub, A transfer learning approach for early diagnosis of alzheimer’s
disease on mri images, Neuroscience 460 (2021) 43–52.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="color:#000000;">(24)</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="color:#000000;">
Y. Zheng, C. Li, X. Zhou, H. Chen, H. Xu, Y. Li, H. Zhang, X. Li, H. Sun,
X. Huang, et al., Application of transfer learning and ensemble learning in
image-level classification for breast histopathology, Intelligent Medicine
(2022).
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="color:#000000;">(25)</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="color:#000000;">
T. Alam, R. Gupta, Federated learning and its role in the privacy preservation
of iot devices, Future Internet 14 (9) (2022) 246.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="color:#000000;">(26)</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="color:#000000;">
K. Weiss, T. M. Khoshgoftaar, D. Wang, A survey of transfer learning, Journal
of Big data 3 (1) (2016) 1–40.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="color:#000000;">(27)</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="color:#000000;">
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh,
D. Bacon, Federated learning: Strategies for improving communication
efficiency, arXiv preprint arXiv:1610.05492 (2016).
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="color:#000000;">(28)</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="color:#000000;">
B. Yu, W. Mao, Y. Lv, C. Zhang, Y. Xie, A survey on federated learning in data
mining, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery
12 (1) (2022) e1443.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="color:#000000;">(29)</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="color:#000000;">
W. Liu, L. Chen, Y. Chen, W. Zhang, Accelerating federated learning via
momentum gradient descent, IEEE Transactions on Parallel and Distributed
Systems 31 (8) (2020) 1754–1766.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="color:#000000;">(30)</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="color:#000000;">
X. Li, S. Zhao, C. Chen, Z. Zheng, Heterogeneity-aware fair federated learning,
Information Sciences 619 (2023) 968–986.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="color:#000000;">(31)</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="color:#000000;">
W. Liu, L. Chen, W. Zhang, Decentralized federated learning: Balancing
communication and computing costs, IEEE Transactions on Signal and
Information Processing over Networks 8 (2022) 131–143.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="color:#000000;">(32)</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="color:#000000;">
W. Huang, T. Li, D. Wang, S. Du, J. Zhang, T. Huang, Fairness and accuracy in
horizontal federated learning, Information Sciences 589 (2022) 170–185.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="color:#000000;">(33)</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="color:#000000;">
H. Zhu, R. Wang, Y. Jin, K. Liang, Pivodl: Privacy-preserving vertical
federated learning over distributed labels, IEEE Transactions on Artificial
Intelligence 4 (5) (2021) 988–1001.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="color:#000000;">(34)</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="color:#000000;">
Y. Liu, Y. Kang, C. Xing, T. Chen, Q. Yang, A secure federated transfer
learning framework, IEEE Intelligent Systems 35 (4) (2020) 70–82.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="color:#000000;">(35)</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="color:#000000;">
E. T. M. Beltrán, M. Q. Pérez, P. M. S. Sánchez, S. L. Bernal,
G. Bovet, M. G. Pérez, G. M. Pérez, A. H. Celdrán, Decentralized
federated learning: Fundamentals, state of the art, frameworks, trends, and
challenges, IEEE Communications Surveys &amp; Tutorials (2023).
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="color:#000000;">(36)</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="color:#000000;">
K. Meghana, N. Nandal, R. Tanwar, L. Goel, G. Chhabra, Breast cancer detection
with machine learning-a review, in: 2023 International Conference on
Sustainable Computing and Data Communication Systems (ICSCDS), IEEE, 2023,
pp. 168–172.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="color:#000000;">(37)</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="color:#000000;">
R. Rani, J. Sahoo, S. Bellamkonda, Application of deep transfer learning in
detection of lung cancer: A systematic survey, in: 2022 OPJU International
Technology Conference on Emerging Technologies for Sustainable Development
(OTCON), IEEE, 2023, pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="color:#000000;">(38)</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="color:#000000;">
K. K. Coelho, M. Nogueira, A. B. Vieira, E. F. Silva, J. A. Nacif, A survey on
federated learning for security and privacy in healthcare applications,
Computer Communications (2023).
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="color:#000000;">(39)</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="color:#000000;">
A. Rahman, M. S. Hossain, G. Muhammad, D. Kundu, T. Debnath, M. Rahman,
M. S. I. Khan, P. Tiwari, S. S. Band, Federated learning-based ai approaches
in smart healthcare: concepts, taxonomies, challenges and open issues,
Cluster computing 26 (4) (2023) 2271–2311.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="color:#000000;">(40)</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="color:#000000;">
A. Chowdhury, H. Kassem, N. Padoy, R. Umeton, A. Karargyris, A review of
medical federated learning: Applications in oncology and cancer research, in:
International MICCAI Brainlesion Workshop, Springer, 2021, pp. 3–24.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="color:#000000;">(41)</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="color:#000000;">
G. Ayana, K. Dese, S.-w. Choe, Transfer learning in breast cancer diagnoses via
ultrasound imaging, Cancers 13 (4) (2021) 738.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="color:#000000;">(42)</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="color:#000000;">
A. Rauniyar, D. H. Hagos, D. Jha, J. E. Håkegård, U. Bagci, D. B.
Rawat, V. Vlassov, Federated learning for medical applications: A taxonomy,
current trends, challenges, and future research directions, IEEE Internet of
Things Journal (2023).
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="color:#000000;">(43)</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="color:#000000;">
M. K. Hasan, M. T. E. Elahi, M. A. Alam, M. T. Jawad, R. Martí,
Dermoexpert: Skin lesion classification using a hybrid convolutional neural
network through segmentation, transfer learning, and augmentation,
Informatics in Medicine Unlocked 28 (2022) 100819.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="color:#000000;">(44)</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="color:#000000;">
Y. Kumar, R. Singla, Federated learning systems for healthcare: perspective and
recent progress, Federated Learning Systems: Towards Next-Generation AI
(2021) 141–156.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="color:#000000;">(45)</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="color:#000000;">
M. Joshi, A. Pal, M. Sankarasubbu, Federated learning for healthcare
domain-pipeline, applications and challenges, ACM Transactions on Computing
for Healthcare 3 (4) (2022) 1–36.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="color:#000000;">(46)</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="color:#000000;">
A. Yang, Z. Ma, C. Zhang, Y. Han, Z. Hu, W. Zhang, X. Huang, Y. Wu, Review on
application progress of federated learning model and security hazard
protection, Digital Communications and Networks (2022).
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.2.2.1" class="ltx_text" style="color:#000000;">(47)</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="color:#000000;">
E. Darzidehkalani, M. Ghasemi-Rad, P. van Ooijen, Federated learning in medical
imaging: Part ii: methods, challenges, and considerations, Journal of the
American College of Radiology 19 (8) (2022) 975–982.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.2.2.1" class="ltx_text" style="color:#000000;">(48)</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="color:#000000;">
J. S.-P. Díaz, Á. L. García, Study of the performance and
scalability of federated learning for medical imaging with intermittent
clients, Neurocomputing 518 (2023) 142–154.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.2.2.1" class="ltx_text" style="color:#000000;">(49)</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.4.1" class="ltx_text" style="color:#000000;">
F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, Q. He, A
comprehensive survey on transfer learning, Proceedings of the IEEE 109 (1)
(2020) 43–76.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.2.2.1" class="ltx_text" style="color:#000000;">(50)</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.4.1" class="ltx_text" style="color:#000000;">
M. Wang, W. Deng, Deep visual domain adaptation: A survey, Neurocomputing 312
(2018) 135–153.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.2.2.1" class="ltx_text" style="color:#000000;">(51)</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.4.1" class="ltx_text" style="color:#000000;">
K. You, M. Long, Z. Cao, J. Wang, M. I. Jordan, Universal domain adaptation,
in: Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, 2019, pp. 2720–2729.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.2.2.1" class="ltx_text" style="color:#000000;">(52)</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.4.1" class="ltx_text" style="color:#000000;">
C.-Y. Lee, T. Batra, M. H. Baig, D. Ulbricht, Sliced wasserstein discrepancy
for unsupervised domain adaptation, in: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2019, pp.
10285–10295.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.2.2.1" class="ltx_text" style="color:#000000;">(53)</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.4.1" class="ltx_text" style="color:#000000;">
J.-C. Su, Y.-H. Tsai, K. Sohn, B. Liu, S. Maji, M. Chandraker, Active
adversarial domain adaptation, in: Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision, 2020, pp. 739–748.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.2.2.1" class="ltx_text" style="color:#000000;">(54)</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="color:#000000;">
A. Zhou, S. Levine, Bayesian adaptation for covariate shift, Advances in Neural
Information Processing Systems 34 (2021) 914–927.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.2.2.1" class="ltx_text" style="color:#000000;">(55)</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.4.1" class="ltx_text" style="color:#000000;">
Q. Yang, Y. Liu, T. Chen, Y. Tong, Federated machine learning: Concept and
applications, ACM Transactions on Intelligent Systems and Technology (TIST)
10 (2) (2019) 1–19.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.2.2.1" class="ltx_text" style="color:#000000;">(56)</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.4.1" class="ltx_text" style="color:#000000;">
S. Zhu, C. Zhou, Y. Wang, Super resolution reconstruction method for infrared
images based on pseudo transferred features, Displays (2022) 102187.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.2.2.1" class="ltx_text" style="color:#000000;">(57)</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.4.1" class="ltx_text" style="color:#000000;">
M. A. Morid, A. Borjali, G. Del Fiol, A scoping review of transfer learning
research on medical image analysis using imagenet, Computers in biology and
medicine 128 (2021) 104115.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.2.2.1" class="ltx_text" style="color:#000000;">(58)</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.4.1" class="ltx_text" style="color:#000000;">
K. T. Chui, V. Arya, S. S. Band, M. Alhalabi, R. W. Liu, H. R. Chi,
Facilitating innovation and knowledge transfer between homogeneous and
heterogeneous datasets: Generic incremental transfer learning approach and
multidisciplinary studies, Journal of Innovation &amp; Knowledge 8 (2) (2023)
100313.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.2.2.1" class="ltx_text" style="color:#000000;">(59)</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.4.1" class="ltx_text" style="color:#000000;">
M. A. Shamshiri, A. Krzyżak, M. Kowal, J. Korbicz, Compatible-domain
transfer learning for breast cancer classification with limited annotated
data, Computers in Biology and Medicine 154 (2023) 106575.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.2.2.1" class="ltx_text" style="color:#000000;">(60)</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.4.1" class="ltx_text" style="color:#000000;">
A. Rehman, S. Naz, M. I. Razzak, F. Akram, M. Imran, A deep learning-based
framework for automatic brain tumors classification using transfer learning,
Circuits, Systems, and Signal Processing 39 (2020) 757–775.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.2.2.1" class="ltx_text" style="color:#000000;">(61)</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.4.1" class="ltx_text" style="color:#000000;">
M. A. Talukder, M. M. Islam, M. A. Uddin, A. Akhter, M. A. J. Pramanik,
S. Aryal, M. A. A. Almoyad, K. F. Hasan, M. A. Moni, An efficient deep
learning model to categorize brain tumor using reconstruction and
fine-tuning, Expert Systems with Applications (2023) 120534.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.2.2.1" class="ltx_text" style="color:#000000;">(62)</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.4.1" class="ltx_text" style="color:#000000;">
Q. Lyu, S. V. Namjoshi, E. McTyre, U. Topaloglu, R. Barcus, M. D. Chan, C. K.
Cramer, W. Debinski, M. N. Gurcan, G. J. Lesser, et al., A transformer-based
deep-learning approach for classifying brain metastases into primary organ
sites using clinical whole-brain mri images, Patterns 3 (11) (2022) 100613.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.2.2.1" class="ltx_text" style="color:#000000;">(63)</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.4.1" class="ltx_text" style="color:#000000;">
A. Jiménez-Sánchez, M. Tardy, M. A. G. Ballester, D. Mateus, G. Piella,
Memory-aware curriculum federated learning for breast cancer classification,
Computer Methods and Programs in Biomedicine 229 (2023) 107318.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.2.2.1" class="ltx_text" style="color:#000000;">(64)</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.4.1" class="ltx_text" style="color:#000000;">
V. Kumari, R. Ghosh, A magnification-independent method for breast cancer
classification using transfer learning, Healthcare Analytics (2023) 100207.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.2.2.1" class="ltx_text" style="color:#000000;">(65)</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.4.1" class="ltx_text" style="color:#000000;">
S. Kumbhare, A. B. Kathole, S. Shinde, Federated learning aided breast cancer
detection with intelligent heuristic-based deep learning framework,
Biomedical Signal Processing and Control 86 (2023) 105080.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.2.2.1" class="ltx_text" style="color:#000000;">(66)</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.4.1" class="ltx_text" style="color:#000000;">
M. A. Talukder, M. M. Islam, M. A. Uddin, A. Akhter, K. F. Hasan, M. A. Moni,
Machine learning-based lung and colon cancer detection using deep feature
extraction and ensemble learning, Expert Systems with Applications 205 (2022)
117695.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.2.2.1" class="ltx_text" style="color:#000000;">(67)</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.4.1" class="ltx_text" style="color:#000000;">
S. Mehmood, T. M. Ghazal, M. A. Khan, M. Zubair, M. T. Naseem, T. Faiz,
M. Ahmad, Malignancy detection in lung and colon histopathology images using
transfer learning with class selective image processing, IEEE Access 10
(2022) 25657–25668.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.2.2.1" class="ltx_text" style="color:#000000;">(68)</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.4.1" class="ltx_text" style="color:#000000;">
L. Liu, K. Fan, M. Yang, Federated learning: a deep learning model based on
resnet18 dual path for lung nodule detection, Multimedia Tools and
Applications 82 (11) (2023) 17437–17450.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.2.2.1" class="ltx_text" style="color:#000000;">(69)</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.4.1" class="ltx_text" style="color:#000000;">
T. Fang, A novel computer-aided lung cancer detection method based on transfer
learning from googlenet and median intensity projections, in: 2018 IEEE
international conference on computer and communication engineering technology
(CCET), IEEE, 2018, pp. 286–290.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.2.2.1" class="ltx_text" style="color:#000000;">(70)</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.4.1" class="ltx_text" style="color:#000000;">
T. Sajja, R. Devarapalli, H. Kalluri, Lung cancer detection based on ct scan
images by using deep transfer learning., Traitement du Signal 36 (4) (2019)
339–344.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.2.2.1" class="ltx_text" style="color:#000000;">(71)</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.4.1" class="ltx_text" style="color:#000000;">
A. Heidari, D. Javaheri, S. Toumaj, N. J. Navimipour, M. Rezaei, M. Unal, A new
lung cancer detection method based on the chest ct images using federated
learning and blockchain systems, Artificial Intelligence in Medicine 141
(2023) 102572.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.2.2.1" class="ltx_text" style="color:#000000;">(72)</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.4.1" class="ltx_text" style="color:#000000;">
A. Rajagopal, E. Redekop, A. Kemisetti, R. Kulkarni, S. Raman, K. Sarma,
K. Magudia, C. W. Arnold, P. E. Larson, Federated learning with research
prototypes: application to multi-center mri-based detection of prostate
cancer with diverse histopathology, Academic Radiology 30 (4) (2023)
644–657.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.2.2.1" class="ltx_text" style="color:#000000;">(73)</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.4.1" class="ltx_text" style="color:#000000;">
H. Wang, M. Yurochkin, Y. Sun, D. Papailiopoulos, Y. Khazaeni,
</span><a target="_blank" href="https://openreview.net/forum?id=BkluqlSFDS" title="" class="ltx_ref ltx_href" style="color:#000000;">Federated learning with
matched averaging</a><span id="bib.bib73.5.2" class="ltx_text" style="color:#000000;"> (2020).
</span>
<br class="ltx_break"><span id="bib.bib73.6.3" class="ltx_text" style="color:#000000;">URL </span><a target="_blank" href="https://openreview.net/forum?id=BkluqlSFDS" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://openreview.net/forum?id=BkluqlSFDS</a><span id="bib.bib73.7.4" class="ltx_text" style="color:#000000;">
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.2.2.1" class="ltx_text" style="color:#000000;">(74)</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.4.1" class="ltx_text" style="color:#000000;">
E. Wang, Y. Hu, X. Yang, X. Tian, Transunet with attention mechanism for brain
tumor segmentation on mr images, in: 2022 IEEE International Conference on
Artificial Intelligence and Computer Applications (ICAICA), IEEE, 2022, pp.
573–577.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.2.2.1" class="ltx_text" style="color:#000000;">(75)</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.4.1" class="ltx_text" style="color:#000000;">
A. Masood, U. Naseem, I. Razzak, Multi-scale swin transformer enabled automatic
detection and segmentation of lung metastases using ct images, in: 2023 IEEE
20th International Symposium on Biomedical Imaging (ISBI), IEEE, 2023, pp.
1–5.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.2.2.1" class="ltx_text" style="color:#000000;">(76)</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.4.1" class="ltx_text" style="color:#000000;">
K. M. Hosny, M. A. Kassem, M. M. Foaud, Skin cancer classification using deep
learning and transfer learning, in: 2018 9th Cairo international biomedical
engineering conference (CIBEC), IEEE, 2018, pp. 90–93.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.2.2.1" class="ltx_text" style="color:#000000;">(77)</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.4.1" class="ltx_text" style="color:#000000;">
X. Cai, Y. Lan, Z. Zhang, J. Wen, Z. Cui, W. Zhang, A many-objective
optimization based federal deep generation model for enhancing data
processing capability in iot, IEEE Transactions on Industrial Informatics
19 (1) (2021) 561–569.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.2.2.1" class="ltx_text" style="color:#000000;">(78)</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.4.1" class="ltx_text" style="color:#000000;">
S. Tyagi, D. T. Kushnure, S. N. Talbar, An amalgamation of vision transformer
with convolutional neural network for automatic lung tumor segmentation,
Computerized Medical Imaging and Graphics (2023) 102258.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.2.2.1" class="ltx_text" style="color:#000000;">(79)</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.4.1" class="ltx_text" style="color:#000000;">
X. Li, Z. Yang, Q. Wang, Y. Sun, A. Liu, Vision transformer for cell tumor
image classification, in: 2023 3rd International Conference on Frontiers of
Electronics, Information and Computation Technologies (ICFEICT), IEEE, 2023,
pp. 176–180.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.2.2.1" class="ltx_text" style="color:#000000;">(80)</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.4.1" class="ltx_text" style="color:#000000;">
Y. Himeur, M. Elnour, F. Fadli, N. Meskin, I. Petri, Y. Rezgui, F. Bensaali,
A. Amira, Next-generation energy systems for sustainable smart cities: Roles
of transfer learning, Sustainable Cities and Society (2022) 104059.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.2.2.1" class="ltx_text" style="color:#000000;">(81)</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.4.1" class="ltx_text" style="color:#000000;">
A. Slim, A. Melouah, U. Faghihi, K. Sahib, Improving neural machine translation
for low resource algerian dialect by transductive transfer learning strategy,
Arabian Journal for Science and Engineering 47 (8) (2022) 10411–10418.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.2.2.1" class="ltx_text" style="color:#000000;">(82)</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.4.1" class="ltx_text" style="color:#000000;">
E. Payá, L. Bori, A. Colomer, M. Meseguer, V. Naranjo, Automatic
characterization of human embryos at day 4 post-insemination from time-lapse
imaging using supervised contrastive learning and inductive transfer learning
techniques, Computer Methods and Programs in Biomedicine 221 (2022) 106895.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.2.2.1" class="ltx_text" style="color:#000000;">(83)</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.4.1" class="ltx_text" style="color:#000000;">
N. Agarwal, A. Sondhi, K. Chopra, G. Singh, Transfer learning: Survey and
classification, Smart Innovations in Communication and Computational
Sciences: Proceedings of ICSICCS 2020 (2021) 145–155.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.2.2.1" class="ltx_text" style="color:#000000;">(84)</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.4.1" class="ltx_text" style="color:#000000;">
Z. Chen, M. Mousavi, V. R. de Sa, Multi-subject unsupervised transfer with
weighted subspace alignment for common spatial patterns, in: 2022 10th
International Winter Conference on Brain-Computer Interface (BCI), IEEE,
2022, pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.2.2.1" class="ltx_text" style="color:#000000;">(85)</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.4.1" class="ltx_text" style="color:#000000;">
L. Yang, B. Lu, Q. Zhou, P. Su, Unsupervised domain adaptation via re-weighted
transfer subspace learning with inter-class sparsity, Knowledge-Based Systems
(2023) 110277.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.2.2.1" class="ltx_text" style="color:#000000;">(86)</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.4.1" class="ltx_text" style="color:#000000;">
J. Zhou, T. Komuro, An asymmetrical-structure auto-encoder for unsupervised
representation learning of skeleton sequences, Computer Vision and Image
Understanding 222 (2022) 103491.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.2.2.1" class="ltx_text" style="color:#000000;">(87)</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.4.1" class="ltx_text" style="color:#000000;">
F. Wang, L. Jiao, Q. Pan, A survey on unsupervised transfer clustering, in:
2021 40th Chinese Control Conference (CCC), IEEE, 2021, pp. 7361–7365.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.2.2.1" class="ltx_text" style="color:#000000;">(88)</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.4.1" class="ltx_text" style="color:#000000;">
S. AbdulRahman, H. Tout, H. Ould-Slimane, A. Mourad, C. Talhi, M. Guizani, A
survey on federated learning: The journey from centralized to distributed
on-site learning and beyond, IEEE Internet of Things Journal 8 (7) (2020)
5476–5497.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.2.2.1" class="ltx_text" style="color:#000000;">(89)</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.4.1" class="ltx_text" style="color:#000000;">
A. G. Roy, S. Siddiqui, S. Pölsterl, N. Navab, C. Wachinger, Braintorrent:
A peer-to-peer environment for decentralized federated learning, arXiv
preprint arXiv:1905.06731 (2019).
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.2.2.1" class="ltx_text" style="color:#000000;">(90)</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.4.1" class="ltx_text" style="color:#000000;">
Y. Guo, Y. Sun, R. Hu, Y. Gong, Hybrid local sgd for federated learning with
heterogeneous communications, in: International Conference on Learning
Representations, 2022.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.2.2.1" class="ltx_text" style="color:#000000;">(91)</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.4.1" class="ltx_text" style="color:#000000;">
X. Zhang, W. Yin, M. Hong, T. Chen, Hybrid federated learning: Algorithms and
implementation, arXiv preprint arXiv:2012.12420 (2020).
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.2.2.1" class="ltx_text" style="color:#000000;">(92)</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.4.1" class="ltx_text" style="color:#000000;">
P. M. Mammen, Federated learning: Opportunities and challenges, arXiv preprint
arXiv:2101.05428 (2021).
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.2.2.1" class="ltx_text" style="color:#000000;">(93)</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.4.1" class="ltx_text" style="color:#000000;">
B. Gu, A. Xu, Z. Huo, C. Deng, H. Huang, Privacy-preserving asynchronous
vertical federated learning algorithms for multiparty collaborative learning,
IEEE transactions on neural networks and learning systems 33 (11) (2021)
6103–6115.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib94.2.2.1" class="ltx_text" style="color:#000000;">(94)</span></span>
<span class="ltx_bibblock"><span id="bib.bib94.4.1" class="ltx_text" style="color:#000000;">
A. Das, S. Patterson, Multi-tier federated learning for vertically partitioned
data, in: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), IEEE, 2021, pp. 3100–3104.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib95.2.2.1" class="ltx_text" style="color:#000000;">(95)</span></span>
<span class="ltx_bibblock"><span id="bib.bib95.4.1" class="ltx_text" style="color:#000000;">
S. Yang, B. Ren, X. Zhou, L. Liu, Parallel distributed logistic regression for
vertical federated learning without third-party coordinator, arXiv preprint
arXiv:1911.09824 (2019).
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib96.2.2.1" class="ltx_text" style="color:#000000;">(96)</span></span>
<span class="ltx_bibblock"><span id="bib.bib96.4.1" class="ltx_text" style="color:#000000;">
K. Wei, J. Li, C. Ma, M. Ding, S. Wei, F. Wu, G. Chen, T. Ranbaduge, Vertical
federated learning: Challenges, methodologies and experiments, arXiv preprint
arXiv:2202.04309 (2022).
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib97.2.2.1" class="ltx_text" style="color:#000000;">(97)</span></span>
<span class="ltx_bibblock"><span id="bib.bib97.4.1" class="ltx_text" style="color:#000000;">
S. Ambesange, B. Annappa, S. G. Koolagudi, Simulating federated transfer
learning for lung segmentation using modified unet model, Procedia Computer
Science 218 (2023) 1485–1496.
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib98.2.2.1" class="ltx_text" style="color:#000000;">(98)</span></span>
<span class="ltx_bibblock"><span id="bib.bib98.4.1" class="ltx_text" style="color:#000000;">
R. Wang, F. Yan, L. Yu, C. Shen, X. Hu, J. Chen, A federated transfer learning
method with low-quality knowledge filtering and dynamic model aggregation for
rolling bearing fault diagnosis, Mechanical Systems and Signal Processing 198
(2023) 110413.
</span>
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib99.2.2.1" class="ltx_text" style="color:#000000;">(99)</span></span>
<span class="ltx_bibblock"><span id="bib.bib99.4.1" class="ltx_text" style="color:#000000;">
X. Li, C. Zhang, X. Li, W. Zhang, Federated transfer learning in fault
diagnosis under data privacy with target self-adaptation, Journal of
Manufacturing Systems 68 (2023) 523–535.
</span>
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib100.2.2.1" class="ltx_text" style="color:#000000;">(100)</span></span>
<span class="ltx_bibblock"><span id="bib.bib100.4.1" class="ltx_text" style="color:#000000;">
X. Li, K. Huang, W. Yang, S. Wang, Z. Zhang,
</span><a target="_blank" href="https://openreview.net/forum?id=HJxNAnVtDS" title="" class="ltx_ref ltx_href" style="color:#000000;">On the convergence of
fedavg on non-iid data</a><span id="bib.bib100.5.2" class="ltx_text" style="color:#000000;">, in: International Conference on Learning
Representations, 2020.
</span>
<br class="ltx_break"><span id="bib.bib100.6.3" class="ltx_text" style="color:#000000;">URL </span><a target="_blank" href="https://openreview.net/forum?id=HJxNAnVtDS" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://openreview.net/forum?id=HJxNAnVtDS</a><span id="bib.bib100.7.4" class="ltx_text" style="color:#000000;">
</span>
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib101.2.2.1" class="ltx_text" style="color:#000000;">(101)</span></span>
<span class="ltx_bibblock"><span id="bib.bib101.4.1" class="ltx_text" style="color:#000000;">
B. McMahan, E. Moore, D. Ramage, S. Hampson, B. A. y Arcas,
Communication-efficient learning of deep networks from decentralized data,
in: Artificial intelligence and statistics, PMLR, 2017, pp. 1273–1282.
</span>
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib102.2.2.1" class="ltx_text" style="color:#000000;">(102)</span></span>
<span class="ltx_bibblock"><span id="bib.bib102.4.1" class="ltx_text" style="color:#000000;">
B. Casella, R. Esposito, C. Cavazzoni, M. Aldinucci, Benchmarking fedavg and
fedcurv for image classification tasks, arXiv preprint arXiv:2303.17942
(2023).
</span>
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib103.2.2.1" class="ltx_text" style="color:#000000;">(103)</span></span>
<span class="ltx_bibblock"><span id="bib.bib103.4.1" class="ltx_text" style="color:#000000;">
S. Ek, F. Portet, P. Lalanda, G. Vega, Evaluation of federated learning
aggregation algorithms: application to human activity recognition, in:
Adjunct proceedings of the 2020 ACM international joint conference on
pervasive and ubiquitous computing and proceedings of the 2020 ACM
international symposium on wearable computers, 2020, pp. 638–643.
</span>
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib104.2.2.1" class="ltx_text" style="color:#000000;">(104)</span></span>
<span class="ltx_bibblock"><span id="bib.bib104.4.1" class="ltx_text" style="color:#000000;">
X. Gu, K. Huang, J. Zhang, L. Huang, Fast federated learning in the presence of
arbitrary device unavailability, Advances in Neural Information Processing
Systems 34 (2021) 12052–12064.
</span>
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib105.2.2.1" class="ltx_text" style="color:#000000;">(105)</span></span>
<span class="ltx_bibblock"><span id="bib.bib105.4.1" class="ltx_text" style="color:#000000;">
W. Shin, J. Shin, Fedvar: Federated learning algorithm with weight variation in
clients, in: 2022 37th International Technical Conference on
Circuits/Systems, Computers and Communications (ITC-CSCC), IEEE, 2022, pp.
1–4.
</span>
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib106.2.2.1" class="ltx_text" style="color:#000000;">(106)</span></span>
<span class="ltx_bibblock"><span id="bib.bib106.4.1" class="ltx_text" style="color:#000000;">
K. Thonglek, K. Takahashi, K. Ichikawa, H. Iida, C. Nakasan, Federated learning
of neural network models with heterogeneous structures, in: 2020 19th IEEE
International Conference on Machine Learning and Applications (ICMLA), IEEE,
2020, pp. 735–740.
</span>
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib107.2.2.1" class="ltx_text" style="color:#000000;">(107)</span></span>
<span class="ltx_bibblock"><span id="bib.bib107.4.1" class="ltx_text" style="color:#000000;">
M. Ahmed, N. Afreen, M. Ahmed, M. Sameer, J. Ahamed, An inception v3 approach
for malware classification using machine learning and transfer learning,
International Journal of Intelligent Networks 4 (2023) 11–18.
</span>
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib108.2.2.1" class="ltx_text" style="color:#000000;">(108)</span></span>
<span class="ltx_bibblock"><span id="bib.bib108.4.1" class="ltx_text" style="color:#000000;">
Z. Liu, C. Yang, J. Huang, S. Liu, Y. Zhuo, X. Lu, Deep learning framework
based on integration of s-mask r-cnn and inception-v3 for ultrasound
image-aided diagnosis of prostate cancer, Future Generation Computer Systems
114 (2021) 358–367.
</span>
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib109.2.2.1" class="ltx_text" style="color:#000000;">(109)</span></span>
<span class="ltx_bibblock"><span id="bib.bib109.4.1" class="ltx_text" style="color:#000000;">
N. Dong, L. Zhao, C.-H. Wu, J.-F. Chang, Inception v3 based cervical cell
classification combined with artificially extracted features, Applied Soft
Computing 93 (2020) 106311.
</span>
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib110.2.2.1" class="ltx_text" style="color:#000000;">(110)</span></span>
<span class="ltx_bibblock"><span id="bib.bib110.4.1" class="ltx_text" style="color:#000000;">
A. Demir, F. Yilmaz, O. Kose, Early detection of skin cancer using deep
learning architectures: resnet-101 and inception-v3, in: 2019 medical
technologies congress (TIPTEKNO), IEEE, 2019, pp. 1–4.
</span>
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib111.2.2.1" class="ltx_text" style="color:#000000;">(111)</span></span>
<span class="ltx_bibblock"><span id="bib.bib111.4.1" class="ltx_text" style="color:#000000;">
S. Sharma, K. Guleria, A deep learning based model for the detection of
pneumonia from chest x-ray images using vgg-16 and neural networks, Procedia
Computer Science 218 (2023) 357–366.
</span>
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib112.2.2.1" class="ltx_text" style="color:#000000;">(112)</span></span>
<span class="ltx_bibblock"><span id="bib.bib112.4.1" class="ltx_text" style="color:#000000;">
D. F. Santos-Bustos, B. M. Nguyen, H. E. Espitia, Towards automated eye cancer
classification via vgg and resnet networks using transfer learning,
Engineering Science and Technology, an International Journal 35 (2022)
101214.
</span>
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib113.2.2.1" class="ltx_text" style="color:#000000;">(113)</span></span>
<span class="ltx_bibblock"><span id="bib.bib113.4.1" class="ltx_text" style="color:#000000;">
R. Pandian, V. Vedanarayanan, D. R. Kumar, R. Rajakumar, Detection and
classification of lung cancer using cnn and google net, Measurement: Sensors
24 (2022) 100588.
</span>
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib114.2.2.1" class="ltx_text" style="color:#000000;">(114)</span></span>
<span class="ltx_bibblock"><span id="bib.bib114.4.1" class="ltx_text" style="color:#000000;">
D. Wu, Y. Ying, M. Zhou, J. Pan, D. Cui, Improved resnet-50 deep learning
algorithm for identifying chicken gender, Computers and Electronics in
Agriculture 205 (2023) 107622.
</span>
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib115.2.2.1" class="ltx_text" style="color:#000000;">(115)</span></span>
<span class="ltx_bibblock"><span id="bib.bib115.4.1" class="ltx_text" style="color:#000000;">
D. Sarwinda, R. H. Paradisa, A. Bustamam, P. Anggia, Deep learning in image
classification using residual network (resnet) variants for detection of
colorectal cancer, Procedia Computer Science 179 (2021) 423–431.
</span>
</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib116.2.2.1" class="ltx_text" style="color:#000000;">(116)</span></span>
<span class="ltx_bibblock"><span id="bib.bib116.4.1" class="ltx_text" style="color:#000000;">
A. V. Ikechukwu, S. Murali, R. Deepu, R. Shivamurthy, Resnet-50 vs vgg-19 vs
training from scratch: A comparative analysis of the segmentation and
classification of pneumonia from chest x-ray images, Global Transitions
Proceedings 2 (2) (2021) 375–381.
</span>
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib117.2.2.1" class="ltx_text" style="color:#000000;">(117)</span></span>
<span class="ltx_bibblock"><span id="bib.bib117.4.1" class="ltx_text" style="color:#000000;">
E. Bütün, M. Uçan, M. Kaya, Automatic detection of cancer
metastasis in lymph node using deep learning, Biomedical Signal Processing
and Control 82 (2023) 104564.
</span>
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib118.2.2.1" class="ltx_text" style="color:#000000;">(118)</span></span>
<span class="ltx_bibblock"><span id="bib.bib118.4.1" class="ltx_text" style="color:#000000;">
F. Chollet, Xception: Deep learning with depthwise separable convolutions, in:
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 1251–1258.
</span>
</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib119.2.2.1" class="ltx_text" style="color:#000000;">(119)</span></span>
<span class="ltx_bibblock"><span id="bib.bib119.4.1" class="ltx_text" style="color:#000000;">
A. Panthakkan, S. Anzar, S. Jamal, W. Mansoor, Concatenated
xception-resnet50—a novel hybrid approach for accurate skin cancer
prediction, Computers in Biology and Medicine 150 (2022) 106170.
</span>
</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib120.2.2.1" class="ltx_text" style="color:#000000;">(120)</span></span>
<span class="ltx_bibblock"><span id="bib.bib120.4.1" class="ltx_text" style="color:#000000;">
C. Upasana, A. S. Tewari, J. P. Singh, An attention-based pneumothorax
classification using modified xception model, Procedia Computer Science 218
(2023) 74–82.
</span>
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib121.2.2.1" class="ltx_text" style="color:#000000;">(121)</span></span>
<span class="ltx_bibblock"><span id="bib.bib121.4.1" class="ltx_text" style="color:#000000;">
S. Sharma, S. Kumar, The xception model: A potential feature extractor in
breast cancer histology images classification, ICT Express 8 (1) (2022)
101–108.
</span>
</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib122.2.2.1" class="ltx_text" style="color:#000000;">(122)</span></span>
<span class="ltx_bibblock"><span id="bib.bib122.4.1" class="ltx_text" style="color:#000000;">
Y. Liu, C. Miao, J. Ji, X. Li, Mmf: A multi-scale mobilenet based fusion method
for infrared and visible image, Infrared Physics &amp; Technology 119 (2021)
103894.
</span>
</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib123.2.2.1" class="ltx_text" style="color:#000000;">(123)</span></span>
<span class="ltx_bibblock"><span id="bib.bib123.4.1" class="ltx_text" style="color:#000000;">
R. Mothkur, B. Veerappa, Classification of lung cancer using lightweight deep
neural networks, Procedia Computer Science 218 (2023) 1869–1877.
</span>
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib124.2.2.1" class="ltx_text" style="color:#000000;">(124)</span></span>
<span class="ltx_bibblock"><span id="bib.bib124.4.1" class="ltx_text" style="color:#000000;">
H. Zhao, Y. Su, M. Wang, Z. Lyu, P. Xu, Y. Jiao, L. Zhang, W. Han, L. Tian,
P. Fu, The machine learning model for distinguishing pathological subtypes of
non-small cell lung cancer, Frontiers in oncology 12 (2022) 875761.
</span>
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib125.2.2.1" class="ltx_text" style="color:#000000;">(125)</span></span>
<span class="ltx_bibblock"><span id="bib.bib125.4.1" class="ltx_text" style="color:#000000;">
S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, M. Shah, Transformers in
vision: A survey, ACM computing surveys (CSUR) 54 (10s) (2022) 1–41.
</span>
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib126.2.2.1" class="ltx_text" style="color:#000000;">(126)</span></span>
<span class="ltx_bibblock"><span id="bib.bib126.4.1" class="ltx_text" style="color:#000000;">
K. He, C. Gan, Z. Li, I. Rekik, Z. Yin, W. Ji, Y. Gao, Q. Wang, J. Zhang,
D. Shen, Transformers in medical image analysis, Intelligent Medicine 3 (1)
(2023) 59–78.
</span>
</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib127.2.2.1" class="ltx_text" style="color:#000000;">(127)</span></span>
<span class="ltx_bibblock"><span id="bib.bib127.4.1" class="ltx_text" style="color:#000000;">
X. Zhai, A. Kolesnikov, N. Houlsby, L. Beyer, Scaling vision transformers, in:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 12104–12113.
</span>
</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib128.2.2.1" class="ltx_text" style="color:#000000;">(128)</span></span>
<span class="ltx_bibblock"><span id="bib.bib128.4.1" class="ltx_text" style="color:#000000;">
P. Wang, Q. Yang, Z. He, Y. Yuan, Vision transformers in multi-modal brain
tumor mri segmentation: A review, Meta-Radiology (2023) 100004.
</span>
</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib129.2.2.1" class="ltx_text" style="color:#000000;">(129)</span></span>
<span class="ltx_bibblock"><span id="bib.bib129.4.1" class="ltx_text" style="color:#000000;">
G. Andrade-Miranda, V. Jaouen, V. Bourbonne, F. Lucia, D. Visvikis, P.-H.
Conze, Pure versus hybrid transformers for multi-modal brain tumor
segmentation: a comparative study, in: 2022 IEEE International Conference on
Image Processing (ICIP), IEEE, 2022, pp. 1336–1340.
</span>
</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib130.2.2.1" class="ltx_text" style="color:#000000;">(130)</span></span>
<span class="ltx_bibblock"><span id="bib.bib130.4.1" class="ltx_text" style="color:#000000;">
X. Xu, P. Prasanna, Brain cancer survival prediction on treatment-naïve
mri using deep anchor attention learning with vision transformer, in: 2022
IEEE 19th International Symposium on Biomedical Imaging (ISBI), IEEE, 2022,
pp. 1–5.
</span>
</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib131.2.2.1" class="ltx_text" style="color:#000000;">(131)</span></span>
<span class="ltx_bibblock"><span id="bib.bib131.4.1" class="ltx_text" style="color:#000000;">
J. Xie, Z. Wu, R. Zhu, H. Zhu, Melanoma detection based on swin transformer and
simam, in: 2021 IEEE 5th Information Technology, Networking, Electronic and
Automation Control Conference (ITNEC), Vol. 5, IEEE, 2021, pp. 1517–1521.
</span>
</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib132.2.2.1" class="ltx_text" style="color:#000000;">(132)</span></span>
<span class="ltx_bibblock"><span id="bib.bib132.4.1" class="ltx_text" style="color:#000000;">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin
transformer: Hierarchical vision transformer using shifted windows, in:
Proceedings of the IEEE/CVF international conference on computer vision,
2021, pp. 10012–10022.
</span>
</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib133.2.2.1" class="ltx_text" style="color:#000000;">(133)</span></span>
<span class="ltx_bibblock"><span id="bib.bib133.4.1" class="ltx_text" style="color:#000000;">
Y. Jiang, Y. Zhang, X. Lin, J. Dong, T. Cheng, J. Liang, Swinbts: A method for
3d multimodal brain tumor segmentation using swin transformer, Brain sciences
12 (6) (2022) 797.
</span>
</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib134.2.2.1" class="ltx_text" style="color:#000000;">(134)</span></span>
<span class="ltx_bibblock"><span id="bib.bib134.4.1" class="ltx_text" style="color:#000000;">
R. Karthik, S. Hussain, T. T. George, R. Mishra, A dual track deep fusion
network for citrus disease classification using group shuffle depthwise
feature pyramid and swin transformer, Ecological Informatics (2023) 102302.
</span>
</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib135.2.2.1" class="ltx_text" style="color:#000000;">(135)</span></span>
<span class="ltx_bibblock"><span id="bib.bib135.4.1" class="ltx_text" style="color:#000000;">
A. Iqbal, M. Sharif, Bts-st: Swin transformer network for segmentation and
classification of multimodality breast cancer images, Knowledge-Based Systems
267 (2023) 110393.
</span>
</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib136.2.2.1" class="ltx_text" style="color:#000000;">(136)</span></span>
<span class="ltx_bibblock"><span id="bib.bib136.4.1" class="ltx_text" style="color:#000000;">
U. Zidan, M. M. Gaber, M. M. Abdelsamea, Swincup: Cascaded swin transformer for
histopathological structures segmentation in colorectal cancer, Expert
Systems with Applications 216 (2023) 119452.
</span>
</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib137.2.2.1" class="ltx_text" style="color:#000000;">(137)</span></span>
<span class="ltx_bibblock"><span id="bib.bib137.4.1" class="ltx_text" style="color:#000000;">
P. Zou, J.-S. Wu, Swine-unet3+: swin transformer encoder network for medical
image segmentation, Progress in Artificial Intelligence 12 (1) (2023)
99–105.
</span>
</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib138.2.2.1" class="ltx_text" style="color:#000000;">(138)</span></span>
<span class="ltx_bibblock"><span id="bib.bib138.4.1" class="ltx_text" style="color:#000000;">
J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, Y. Zhou,
Transunet: Transformers make strong encoders for medical image segmentation,
arXiv preprint arXiv:2102.04306 (2021).
</span>
</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib139.2.2.1" class="ltx_text" style="color:#000000;">(139)</span></span>
<span class="ltx_bibblock"><span id="bib.bib139.4.1" class="ltx_text" style="color:#000000;">
R. Castro, L. Ramos, S. Román, M. Bermeo, A. Crespo, E. Cuenca, U-net vs.
transunet: Performance comparison in medical image segmentation, in:
International Conference on Applied Technologies, Springer, 2022, pp.
212–226.
</span>
</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib140.2.2.1" class="ltx_text" style="color:#000000;">(140)</span></span>
<span class="ltx_bibblock"><span id="bib.bib140.4.1" class="ltx_text" style="color:#000000;">
H. Wang, H. Zhu, L. Ding, Accurate classification of lung nodules on ct images
using the transunet, Frontiers in Public Health 10 (2022) 1060798.
</span>
</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib141.2.2.1" class="ltx_text" style="color:#000000;">(141)</span></span>
<span class="ltx_bibblock"><span id="bib.bib141.4.1" class="ltx_text" style="color:#000000;">
X. Chen, L. Yang, Brain tumor segmentation based on cbam-transunet, in:
Proceedings of the 1st ACM Workshop on Mobile and Wireless Sensing for Smart
Healthcare, 2022, pp. 33–38.
</span>
</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib142.2.2.1" class="ltx_text" style="color:#000000;">(142)</span></span>
<span class="ltx_bibblock"><span id="bib.bib142.4.1" class="ltx_text" style="color:#000000;">
J. Chen, J. Chen, Z. Zhou, B. Li, A. Yuille, Y. Lu, Mt-transunet: Mediating
multi-task tokens in transformers for skin lesion segmentation and
classification, arXiv preprint arXiv:2112.01767 (2021).
</span>
</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib143.2.2.1" class="ltx_text" style="color:#000000;">(143)</span></span>
<span class="ltx_bibblock"><span id="bib.bib143.4.1" class="ltx_text" style="color:#000000;">
P. Foley, M. J. Sheller, B. Edwards, S. Pati, W. Riviera, M. Sharma, P. N.
Moorthy, S.-h. Wang, J. Martin, P. Mirhaji, et al., Openfl: the open
federated learning library, Physics in Medicine &amp; Biology 67 (21) (2022)
214001.
</span>
</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib144.2.2.1" class="ltx_text" style="color:#000000;">(144)</span></span>
<span class="ltx_bibblock"><span id="bib.bib144.4.1" class="ltx_text" style="color:#000000;">
G. A. Reina, A. Gruzdev, P. Foley, O. Perepelkina, M. Sharma, I. Davidyuk,
I. Trushkin, M. Radionov, A. Mokrov, D. Agapov, et al., Openfl: An
open-source framework for federated learning, arXiv preprint arXiv:2105.06413
(2021).
</span>
</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib145.2.2.1" class="ltx_text" style="color:#000000;">(145)</span></span>
<span class="ltx_bibblock"><span id="bib.bib145.4.1" class="ltx_text" style="color:#000000;">
INRIA, An open-source federated learning framework, Fed-BioMed.
</span>
</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib146.2.2.1" class="ltx_text" style="color:#000000;">(146)</span></span>
<span class="ltx_bibblock"><span id="bib.bib146.4.1" class="ltx_text" style="color:#000000;">
S. Silva, A. Altmann, B. Gutman, M. Lorenzi, Fed-biomed: A general open-source
frontend framework for federated learning in healthcare, in: Domain
Adaptation and Representation Transfer, and Distributed and Collaborative
Learning: Second MICCAI Workshop, DART 2020, and First MICCAI Workshop, DCL
2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020,
Proceedings 2, Springer, 2020, pp. 201–210.
</span>
</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib147.2.2.1" class="ltx_text" style="color:#000000;">(147)</span></span>
<span class="ltx_bibblock"><span id="bib.bib147.4.1" class="ltx_text" style="color:#000000;">
M. Khan, F. G. Glavin, M. Nickles, Federated learning as a privacy solution-an
overview, Procedia Computer Science 217 (2023) 316–325.
</span>
</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib148.2.2.1" class="ltx_text" style="color:#000000;">(148)</span></span>
<span class="ltx_bibblock"><span id="bib.bib148.4.1" class="ltx_text" style="color:#000000;">
P. Jabłecki, F. Ślazyk, M. Malawski, Federated learning in the cloud for
analysis of medical images-experience with open source frameworks, in:
Clinical Image-Based Procedures, Distributed and Collaborative Learning,
Artificial Intelligence for Combating COVID-19 and Secure and
Privacy-Preserving Machine Learning: 10th Workshop, CLIP 2021, Second
Workshop, DCL 2021, First Workshop, LL-COVID19 2021, and First Workshop and
Tutorial, PPML 2021, Held in Conjunction with MICCAI 2021, Strasbourg,
France, September 27 and October 1, 2021, Proceedings 2, Springer, 2021, pp.
111–119.
</span>
</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib149.2.2.1" class="ltx_text" style="color:#000000;">(149)</span></span>
<span class="ltx_bibblock"><span id="bib.bib149.4.1" class="ltx_text" style="color:#000000;">
F. Cremonesi, M. Vesin, S. Cansiz, Y. Bouillard, I. Balelli, L. Innocenti,
S. Silva, S.-S. Ayed, R. Taiello, L. Kameni, et al., Fed-biomed: Open,
transparent and trusted federated learning for real-world healthcare
applications, arXiv preprint arXiv:2304.12012 (2023).
</span>
</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib150.2.2.1" class="ltx_text" style="color:#000000;">(150)</span></span>
<span class="ltx_bibblock"><span id="bib.bib150.4.1" class="ltx_text" style="color:#000000;">
A. Hatamizadeh, H. Yin, P. Molchanov, A. Myronenko, W. Li, P. Dogra, A. Feng,
M. G. Flores, J. Kautz, D. Xu, et al., Do gradient inversion attacks make
federated learning unsafe?, IEEE Transactions on Medical Imaging (2023).
</span>
</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib151.2.2.1" class="ltx_text" style="color:#000000;">(151)</span></span>
<span class="ltx_bibblock"><span id="bib.bib151.4.1" class="ltx_text" style="color:#000000;">
A. K. Nair, E. D. Raj, J. Sahoo, A robust analysis of adversarial attacks on
federated learning environments, Computer Standards &amp; Interfaces (2023)
103723.
</span>
</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib152.2.2.1" class="ltx_text" style="color:#000000;">(152)</span></span>
<span class="ltx_bibblock"><span id="bib.bib152.4.1" class="ltx_text" style="color:#000000;">
C. Zhao, S. Zhao, M. Zhao, Z. Chen, C.-Z. Gao, H. Li, Y.-a. Tan, Secure
multi-party computation: theory, practice and applications, Information
Sciences 476 (2019) 357–372.
</span>
</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib153.2.2.1" class="ltx_text" style="color:#000000;">(153)</span></span>
<span class="ltx_bibblock"><span id="bib.bib153.4.1" class="ltx_text" style="color:#000000;">
M. Naehrig, K. Lauter, V. Vaikuntanathan, Can homomorphic encryption be
practical?, in: Proceedings of the 3rd ACM workshop on Cloud computing
security workshop, 2011, pp. 113–124.
</span>
</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib154.2.2.1" class="ltx_text" style="color:#000000;">(154)</span></span>
<span class="ltx_bibblock"><span id="bib.bib154.4.1" class="ltx_text" style="color:#000000;">
P. Gupta, K. Yadav, B. B. Gupta, M. Alazab, T. R. Gadekallu, A novel data
poisoning attack in federated learning based on inverted loss function,
Computers &amp; Security 130 (2023) 103270.
</span>
</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib155.2.2.1" class="ltx_text" style="color:#000000;">(155)</span></span>
<span class="ltx_bibblock"><span id="bib.bib155.4.1" class="ltx_text" style="color:#000000;">
H. Kasyap, S. Tripathy, Beyond data poisoning in federated learning, Expert
Systems with Applications (2023) 121192.
</span>
</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib156.2.2.1" class="ltx_text" style="color:#000000;">(156)</span></span>
<span class="ltx_bibblock"><span id="bib.bib156.4.1" class="ltx_text" style="color:#000000;">
M. Yang, H. Cheng, F. Chen, X. Liu, M. Wang, X. Li, Model poisoning attack in
differential privacy-based federated learning, Information Sciences 630
(2023) 158–172.
</span>
</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib157.2.2.1" class="ltx_text" style="color:#000000;">(157)</span></span>
<span class="ltx_bibblock"><span id="bib.bib157.4.1" class="ltx_text" style="color:#000000;">
H. Yang, D. Gu, J. He, Demac: Towards detecting model poisoning attacks in
federated learning system, Internet of Things 23 (2023) 100875.
</span>
</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib158.2.2.1" class="ltx_text" style="color:#000000;">(158)</span></span>
<span class="ltx_bibblock"><span id="bib.bib158.4.1" class="ltx_text" style="color:#000000;">
V. Tolpegin, S. Truex, M. E. Gursoy, L. Liu, Data poisoning attacks against
federated learning systems, in: Computer Security–ESORICS 2020: 25th
European Symposium on Research in Computer Security, ESORICS 2020, Guildford,
UK, September 14–18, 2020, Proceedings, Part I 25, Springer, 2020, pp.
480–501.
</span>
</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib159.2.2.1" class="ltx_text" style="color:#000000;">(159)</span></span>
<span class="ltx_bibblock"><span id="bib.bib159.4.1" class="ltx_text" style="color:#000000;">
P. M. Sánchez Sánchez, A. Huertas Celdrán, J. R.
Buendía Rubio, G. Bovet, G. Martínez Pérez, Robust federated
learning for execution time-based device model identification under
label-flipping attack, Cluster Computing (2023) 1–12.
</span>
</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib160.2.2.1" class="ltx_text" style="color:#000000;">(160)</span></span>
<span class="ltx_bibblock"><span id="bib.bib160.4.1" class="ltx_text" style="color:#000000;">
N. M. Jebreel, J. Domingo-Ferrer, D. Sánchez, A. Blanco-Justicia, Defending
against the label-flipping attack in federated learning, arXiv preprint
arXiv:2207.01982 (2022).
</span>
</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib161.2.2.1" class="ltx_text" style="color:#000000;">(161)</span></span>
<span class="ltx_bibblock"><span id="bib.bib161.4.1" class="ltx_text" style="color:#000000;">
Y. Jiang, W. Zhang, Y. Chen, Data quality detection mechanism against label
flipping attacks in federated learning, IEEE Transactions on Information
Forensics and Security 18 (2023) 1625–1637.
</span>
</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib162.2.2.1" class="ltx_text" style="color:#000000;">(162)</span></span>
<span class="ltx_bibblock"><span id="bib.bib162.4.1" class="ltx_text" style="color:#000000;">
D. Li, W. E. Wong, W. Wang, Y. Yao, M. Chau, Detection and mitigation of
label-flipping attacks in federated learning systems with kpca and k-means,
in: 2021 8th International Conference on Dependable Systems and Their
Applications (DSA), IEEE, 2021, pp. 551–559.
</span>
</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib163.2.2.1" class="ltx_text" style="color:#000000;">(163)</span></span>
<span class="ltx_bibblock"><span id="bib.bib163.4.1" class="ltx_text" style="color:#000000;">
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, V. Shmatikov, How to backdoor
federated learning, in: International conference on artificial intelligence
and statistics, PMLR, 2020, pp. 2938–2948.
</span>
</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib164.2.2.1" class="ltx_text" style="color:#000000;">(164)</span></span>
<span class="ltx_bibblock"><span id="bib.bib164.4.1" class="ltx_text" style="color:#000000;">
C. Zhu, J. Zhang, X. Sun, B. Chen, W. Meng, Adfl: Defending backdoor attacks in
federated learning via adversarial distillation, Computers &amp; Security (2023)
103366.
</span>
</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib165.2.2.1" class="ltx_text" style="color:#000000;">(165)</span></span>
<span class="ltx_bibblock"><span id="bib.bib165.4.1" class="ltx_text" style="color:#000000;">
Y. Wang, D.-H. Zhai, Y. Xia, Scfl: Mitigating backdoor attacks in federated
learning based on svd and clustering, Computers &amp; Security (2023) 103414.
</span>
</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib166.2.2.1" class="ltx_text" style="color:#000000;">(166)</span></span>
<span class="ltx_bibblock"><span id="bib.bib166.4.1" class="ltx_text" style="color:#000000;">
Z. Sun, P. Kairouz, A. T. Suresh, H. B. McMahan, Can you really backdoor
federated learning?, arXiv preprint arXiv:1911.07963 (2019).
</span>
</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib167.2.2.1" class="ltx_text" style="color:#000000;">(167)</span></span>
<span class="ltx_bibblock"><span id="bib.bib167.4.1" class="ltx_text" style="color:#000000;">
M. Dildar, S. Akram, M. Irfan, H. U. Khan, M. Ramzan, A. R. Mahmood, S. A.
Alsaiari, A. H. M. Saeed, M. O. Alraddadi, M. H. Mahnashi, Skin cancer
detection: a review using deep learning techniques, International journal of
environmental research and public health 18 (10) (2021) 5479.
</span>
</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib168.2.2.1" class="ltx_text" style="color:#000000;">(168)</span></span>
<span class="ltx_bibblock"><span id="bib.bib168.4.1" class="ltx_text" style="color:#000000;">
J. Chang, B. Yu, W. M. Saltzman, M. Girardi, Nanoparticles as a therapeutic
delivery system for skin cancer prevention and treatment, JID Innovations
(2023) 100197.
</span>
</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib169.2.2.1" class="ltx_text" style="color:#000000;">(169)</span></span>
<span class="ltx_bibblock"><span id="bib.bib169.4.1" class="ltx_text" style="color:#000000;">
J. Ferguson, V. Eleftheriadou, J. Nesnas, Risk of melanoma and non-melanoma
skin cancer in people with vitiligo: Uk population-based cohort study,
Journal of Investigative Dermatology (2023).
</span>
</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib170.2.2.1" class="ltx_text" style="color:#000000;">(170)</span></span>
<span class="ltx_bibblock"><span id="bib.bib170.4.1" class="ltx_text" style="color:#000000;">
M. A. Hashmani, S. M. Jameel, S. S. H. Rizvi, S. Shukla, An adaptive federated
machine learning-based intelligent system for skin disease detection: A step
toward an intelligent dermoscopy device, Applied Sciences 11 (5) (2021) 2145.
</span>
</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib171.2.2.1" class="ltx_text" style="color:#000000;">(171)</span></span>
<span class="ltx_bibblock"><span id="bib.bib171.4.1" class="ltx_text" style="color:#000000;">
M. A. Kassem, K. M. Hosny, M. M. Fouad, Skin lesions classification into eight
classes for isic 2019 using deep convolutional neural network and transfer
learning, IEEE Access 8 (2020) 114822–114832.
</span>
</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib172.2.2.1" class="ltx_text" style="color:#000000;">(172)</span></span>
<span class="ltx_bibblock"><span id="bib.bib172.4.1" class="ltx_text" style="color:#000000;">
H. K. Kondaveeti, P. Edupuganti, Skin cancer classification using transfer
learning, in: 2020 IEEE International Conference on Advent Trends in
Multidisciplinary Research and Innovation (ICATMRI), IEEE, 2020, pp. 1–4.
</span>
</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib173.2.2.1" class="ltx_text" style="color:#000000;">(173)</span></span>
<span class="ltx_bibblock"><span id="bib.bib173.4.1" class="ltx_text" style="color:#000000;">
M. S. Ali, M. S. Miah, J. Haque, M. M. Rahman, M. K. Islam, An enhanced
technique of skin cancer classification using deep convolutional neural
network with transfer learning models, Machine Learning with Applications 5
(2021) 100036.
</span>
</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib174.2.2.1" class="ltx_text" style="color:#000000;">(174)</span></span>
<span class="ltx_bibblock"><span id="bib.bib174.4.1" class="ltx_text" style="color:#000000;">
J. A. Dumalaon-Canaria, A. D. Hutchinson, I. Prichard, C. Wilson, What causes
breast cancer? a systematic review of causal attributions among breast cancer
survivors and how these compare to expert-endorsed risk factors, Cancer
Causes &amp; Control 25 (2014) 771–785.
</span>
</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib175.2.2.1" class="ltx_text" style="color:#000000;">(175)</span></span>
<span class="ltx_bibblock"><span id="bib.bib175.4.1" class="ltx_text" style="color:#000000;">
H. R. Roth, K. Chang, P. Singh, N. Neumark, W. Li, V. Gupta, S. Gupta, L. Qu,
A. Ihsani, B. C. Bizzo, et al., Federated learning for breast density
classification: A real-world implementation, in: Domain Adaptation and
Representation Transfer, and Distributed and Collaborative Learning: Second
MICCAI Workshop, DART 2020, and First MICCAI Workshop, DCL 2020, Held in
Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings 2,
Springer, 2020, pp. 181–191.
</span>
</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib176.2.2.1" class="ltx_text" style="color:#000000;">(176)</span></span>
<span class="ltx_bibblock"><span id="bib.bib176.4.1" class="ltx_text" style="color:#000000;">
S. Khan, N. Islam, Z. Jan, I. U. Din, J. J. C. Rodrigues, A novel deep learning
based framework for the detection and classification of breast cancer using
transfer learning, Pattern Recognition Letters 125 (2019) 1–6.
</span>
</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib177.2.2.1" class="ltx_text" style="color:#000000;">(177)</span></span>
<span class="ltx_bibblock"><span id="bib.bib177.4.1" class="ltx_text" style="color:#000000;">
I. Pacal, D. Karaboga, A. Basturk, B. Akay, U. Nalbantoglu, A comprehensive
review of deep learning in colon cancer, Computers in Biology and Medicine
126 (2020) 104003.
</span>
</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib178.2.2.1" class="ltx_text" style="color:#000000;">(178)</span></span>
<span class="ltx_bibblock"><span id="bib.bib178.4.1" class="ltx_text" style="color:#000000;">
M. Ahmed, Colon cancer: a clinician’s perspective in 2019, Gastroenterology
research 13 (1) (2020) 1–10.
</span>
</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib179.2.2.1" class="ltx_text" style="color:#000000;">(179)</span></span>
<span class="ltx_bibblock"><span id="bib.bib179.4.1" class="ltx_text" style="color:#000000;">
M. Murugesan, R. M. Arieth, S. Balraj, R. Nirmala, Colon cancer stage detection
in colonoscopy images using yolov3 msf deep learning architecture, Biomedical
Signal Processing and Control 80 (2023) 104283.
</span>
</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib180.2.2.1" class="ltx_text" style="color:#000000;">(180)</span></span>
<span class="ltx_bibblock"><span id="bib.bib180.4.1" class="ltx_text" style="color:#000000;">
N. Gessert, M. Bengs, L. Wittig, D. Drömann, T. Keck, A. Schlaefer, D. B.
Ellebrecht, Deep transfer learning methods for colon cancer classification in
confocal laser microscopy images, International journal of computer assisted
radiology and surgery 14 (2019) 1837–1845.
</span>
</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib181.2.2.1" class="ltx_text" style="color:#000000;">(181)</span></span>
<span class="ltx_bibblock"><span id="bib.bib181.4.1" class="ltx_text" style="color:#000000;">
D. A. Hormuth II, M. Farhat, C. Christenson, B. Curl, C. C. Quarles, C. Chung,
T. E. Yankeelov, Opportunities for improving brain cancer treatment outcomes
through imaging-based mathematical modeling of the delivery of radiotherapy
and immunotherapy, Advanced Drug Delivery Reviews (2022) 114367.
</span>
</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib182.2.2.1" class="ltx_text" style="color:#000000;">(182)</span></span>
<span class="ltx_bibblock"><span id="bib.bib182.4.1" class="ltx_text" style="color:#000000;">
A. Al Mamun, M. S. Uddin, A. Perveen, N. K. Jha, B. S. Alghamdi, P. Jeandet,
H.-J. Zhang, G. M. Ashraf, Inflammation-targeted nanomedicine against brain
cancer: From design strategies to future developments, in: Seminars in Cancer
Biology, Elsevier, 2022.
</span>
</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib183.2.2.1" class="ltx_text" style="color:#000000;">(183)</span></span>
<span class="ltx_bibblock"><span id="bib.bib183.4.1" class="ltx_text" style="color:#000000;">
L. Yi, J. Zhang, R. Zhang, J. Shi, G. Wang, X. Liu, Su-net: an efficient
encoder-decoder model of federated learning for brain tumor segmentation, in:
Artificial Neural Networks and Machine Learning–ICANN 2020: 29th
International Conference on Artificial Neural Networks, Bratislava, Slovakia,
September 15–18, 2020, Proceedings, Part I, Springer, 2020, pp. 761–773.
</span>
</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib184.2.2.1" class="ltx_text" style="color:#000000;">(184)</span></span>
<span class="ltx_bibblock"><span id="bib.bib184.4.1" class="ltx_text" style="color:#000000;">
V. Jacob, G. Sagar, K. Goura, P. S. Pedalanka, Brain tumor classification based
on deep cnn and modified butterfly optimization algorithm, Computer Methods
in Biomechanics and Biomedical Engineering: Imaging &amp; Visualization (2023)
1–12.
</span>
</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib185.2.2.1" class="ltx_text" style="color:#000000;">(185)</span></span>
<span class="ltx_bibblock"><span id="bib.bib185.4.1" class="ltx_text" style="color:#000000;">
K. C. Thandra, A. Barsouk, K. Saginala, J. S. Aluru, A. Barsouk, Epidemiology
of lung cancer, Contemporary Oncology/Współczesna Onkologia 25 (1)
(2021) 45–52.
</span>
</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib186.2.2.1" class="ltx_text" style="color:#000000;">(186)</span></span>
<span class="ltx_bibblock"><span id="bib.bib186.4.1" class="ltx_text" style="color:#000000;">
C. M. Rudin, E. Brambilla, C. Faivre-Finn, J. Sage, Small-cell lung cancer,
Nature Reviews Disease Primers 7 (1) (2021) 3.
</span>
</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib187.2.2.1" class="ltx_text" style="color:#000000;">(187)</span></span>
<span class="ltx_bibblock"><span id="bib.bib187.4.1" class="ltx_text" style="color:#000000;">
B. J. Ayekai, C. Wenyu, M. T. Hailemichael, L. D. Fiasam, A. V. Kwaku,
F. Agbley, W. Ayivi, F. Sam, J. M. Danso, D. Kulevome, et al., Federated lung
cancer prediction using histopathological medical images, in: 2022 19th
International Computer Conference on Wavelet Active Media Technology and
Information Processing (ICCWAMTIP), IEEE, 2022, pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib188.2.2.1" class="ltx_text" style="color:#000000;">(188)</span></span>
<span class="ltx_bibblock"><span id="bib.bib188.4.1" class="ltx_text" style="color:#000000;">
N. Codella, V. Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza, D. Gutman,
B. Helba, A. Kalloo, K. Liopyris, M. Marchetti, et al., Skin lesion analysis
toward melanoma detection 2018: A challenge hosted by the international skin
imaging collaboration (isic), arXiv preprint arXiv:1902.03368 (2019).
</span>
</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib189.2.2.1" class="ltx_text" style="color:#000000;">(189)</span></span>
<span class="ltx_bibblock"><span id="bib.bib189.4.1" class="ltx_text" style="color:#000000;">
P. Tschandl, C. Rosendahl, H. Kittler, The ham10000 dataset, a large collection
of multi-source dermatoscopic images of common pigmented skin lesions,
Scientific data 5 (1) (2018) 1–9.
</span>
</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib190.2.2.1" class="ltx_text" style="color:#000000;">(190)</span></span>
<span class="ltx_bibblock"><span id="bib.bib190.4.1" class="ltx_text" style="color:#000000;">
D. Raval, J. N. Undavia, A comprehensive assessment of convolutional neural
networks for skin and oral cancer detection using medical images, Healthcare
Analytics 3 (2023) 100199.
</span>
</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib191.2.2.1" class="ltx_text" style="color:#000000;">(191)</span></span>
<span class="ltx_bibblock"><span id="bib.bib191.4.1" class="ltx_text" style="color:#000000;">
S. Dey, R. Roychoudhury, S. Malakar, R. Sarkar, Screening of breast cancer from
thermogram images by edge detection aided deep transfer learning model,
Multimedia Tools and Applications 81 (7) (2022) 9331–9349.
</span>
</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib192.2.2.1" class="ltx_text" style="color:#000000;">(192)</span></span>
<span class="ltx_bibblock"><span id="bib.bib192.4.1" class="ltx_text" style="color:#000000;">
T. Hanser, Federated learning for molecular discovery, Current Opinion in
Structural Biology 79 (2023) 102545.
</span>
</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib193.2.2.1" class="ltx_text" style="color:#000000;">(193)</span></span>
<span class="ltx_bibblock"><span id="bib.bib193.4.1" class="ltx_text" style="color:#000000;">
L. Su, V. K. Lau, Hierarchical federated learning for hybrid data partitioning
across multitype sensors, IEEE Internet of Things Journal 8 (13) (2021)
10922–10939.
</span>
</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib194.2.2.1" class="ltx_text" style="color:#000000;">(194)</span></span>
<span class="ltx_bibblock"><span id="bib.bib194.4.1" class="ltx_text" style="color:#000000;">
K. H. Lee, M. H. Kim, Bayesian inductive learning in group recommendations for
seen and unseen groups, Information Sciences 610 (2022) 725–745.
</span>
</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib195.2.2.1" class="ltx_text" style="color:#000000;">(195)</span></span>
<span class="ltx_bibblock"><span id="bib.bib195.4.1" class="ltx_text" style="color:#000000;">
S. Kamei, S. Taghipour, A comparison study of centralized and decentralized
federated learning approaches utilizing the transformer architecture for
estimating remaining useful life, Reliability Engineering &amp; System Safety
233 (2023) 109130.
</span>
</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib196.2.2.1" class="ltx_text" style="color:#000000;">(196)</span></span>
<span class="ltx_bibblock"><span id="bib.bib196.4.1" class="ltx_text" style="color:#000000;">
T. Sun, D. Li, B. Wang, Decentralized federated averaging, IEEE Transactions on
Pattern Analysis and Machine Intelligence 45 (4) (2022) 4289–4301.
</span>
</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib197.2.2.1" class="ltx_text" style="color:#000000;">(197)</span></span>
<span class="ltx_bibblock"><span id="bib.bib197.4.1" class="ltx_text" style="color:#000000;">
Y. Gu, Z. Ge, C. P. Bonnington, J. Zhou, Progressive transfer learning and
adversarial domain adaptation for cross-domain skin disease classification,
IEEE journal of biomedical and health informatics 24 (5) (2019) 1379–1393.
</span>
</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib198.2.2.1" class="ltx_text" style="color:#000000;">(198)</span></span>
<span class="ltx_bibblock"><span id="bib.bib198.4.1" class="ltx_text" style="color:#000000;">
K. Stacke, G. Eilertsen, J. Unger, C. Lundström, Measuring domain shift for
deep learning in histopathology, IEEE journal of biomedical and health
informatics 25 (2) (2020) 325–336.
</span>
</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib199.2.2.1" class="ltx_text" style="color:#000000;">(199)</span></span>
<span class="ltx_bibblock"><span id="bib.bib199.4.1" class="ltx_text" style="color:#000000;">
R. Zoetmulder, E. Gavves, M. Caan, H. Marquering, Domain-and task-specific
transfer learning for medical segmentation tasks, Computer Methods and
Programs in Biomedicine 214 (2022) 106539.
</span>
</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib200.2.2.1" class="ltx_text" style="color:#000000;">(200)</span></span>
<span class="ltx_bibblock"><span id="bib.bib200.4.1" class="ltx_text" style="color:#000000;">
K. Fogelberg, S. Chamarthi, R. C. Maron, J. Niebling, T. J. Brinker, Domain
shifts in dermoscopic skin cancer datasets: Evaluation of essential
limitations for clinical translation, New Biotechnology 76 (2023) 106–117.
</span>
</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib201.2.2.1" class="ltx_text" style="color:#000000;">(201)</span></span>
<span class="ltx_bibblock"><span id="bib.bib201.4.1" class="ltx_text" style="color:#000000;">
T. T. L. Vuong, Q. D. Vu, M. Jahanifar, S. Graham, J. T. Kwak, N. Rajpoot,
Impash: A novel domain-shift resistant representation for colorectal cancer
tissue classification, in: European Conference on Computer Vision, Springer,
2022, pp. 543–555.
</span>
</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib202.2.2.1" class="ltx_text" style="color:#000000;">(202)</span></span>
<span class="ltx_bibblock"><span id="bib.bib202.4.1" class="ltx_text" style="color:#000000;">
A. Marathe, R. Anirudh, N. Jain, A. Bhatele, J. Thiagarajan, B. Kailkhura,
J.-S. Yeom, B. Rountree, T. Gamblin, Performance modeling under resource
constraints using deep transfer learning, in: Proceedings of the
International Conference for High Performance Computing, Networking, Storage
and Analysis, 2017, pp. 1–12.
</span>
</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib203.2.2.1" class="ltx_text" style="color:#000000;">(203)</span></span>
<span class="ltx_bibblock"><span id="bib.bib203.4.1" class="ltx_text" style="color:#000000;">
P. N. Whatmough, C. Zhou, P. Hansen, S. K. Venkataramanaiah, J.-s. Seo,
M. Mattina, Fixynn: Efficient hardware for mobile computer vision via
transfer learning, arXiv preprint arXiv:1902.11128 (2019).
</span>
</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib204.2.2.1" class="ltx_text" style="color:#000000;">(204)</span></span>
<span class="ltx_bibblock"><span id="bib.bib204.4.1" class="ltx_text" style="color:#000000;">
H. G. Abreha, M. Hayajneh, M. A. Serhani, Federated learning in edge computing:
a systematic survey, Sensors 22 (2) (2022) 450.
</span>
</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib205.2.2.1" class="ltx_text" style="color:#000000;">(205)</span></span>
<span class="ltx_bibblock"><span id="bib.bib205.4.1" class="ltx_text" style="color:#000000;">
A. Imteaj, U. Thakker, S. Wang, J. Li, M. H. Amini, A survey on federated
learning for resource-constrained iot devices, IEEE Internet of Things
Journal 9 (1) (2021) 1–24.
</span>
</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib206.2.2.1" class="ltx_text" style="color:#000000;">(206)</span></span>
<span class="ltx_bibblock"><span id="bib.bib206.4.1" class="ltx_text" style="color:#000000;">
N. H. Tran, W. Bao, A. Zomaya, M. N. Nguyen, C. S. Hong, Federated learning
over wireless networks: Optimization model design and analysis, in: IEEE
INFOCOM 2019-IEEE conference on computer communications, IEEE, 2019, pp.
1387–1395.
</span>
</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib207.2.2.1" class="ltx_text" style="color:#000000;">(207)</span></span>
<span class="ltx_bibblock"><span id="bib.bib207.4.1" class="ltx_text" style="color:#000000;">
S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, K. Chan,
Adaptive federated learning in resource constrained edge computing systems,
IEEE journal on selected areas in communications 37 (6) (2019) 1205–1221.
</span>
</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib208.2.2.1" class="ltx_text" style="color:#000000;">(208)</span></span>
<span class="ltx_bibblock"><span id="bib.bib208.4.1" class="ltx_text" style="color:#000000;">
K. Yang, T. Jiang, Y. Shi, Z. Ding, Federated learning via over-the-air
computation, IEEE transactions on wireless communications 19 (3) (2020)
2022–2035.
</span>
</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib209.2.2.1" class="ltx_text" style="color:#000000;">(209)</span></span>
<span class="ltx_bibblock"><span id="bib.bib209.4.1" class="ltx_text" style="color:#000000;">
L. Alzubaidi, M. Al-Amidie, A. Al-Asadi, A. J. Humaidi, O. Al-Shamma, M. A.
Fadhel, J. Zhang, J. Santamaría, Y. Duan, Novel transfer learning
approach for medical imaging with limited labeled data, Cancers 13 (7) (2021)
1590.
</span>
</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib210.2.2.1" class="ltx_text" style="color:#000000;">(210)</span></span>
<span class="ltx_bibblock"><span id="bib.bib210.4.1" class="ltx_text" style="color:#000000;">
A. Abbas, M. M. Abdelsamea, M. M. Gaber, Detrac: Transfer learning of class
decomposed medical images in convolutional neural networks, IEEE Access 8
(2020) 74901–74913.
</span>
</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib211.2.2.1" class="ltx_text" style="color:#000000;">(211)</span></span>
<span class="ltx_bibblock"><span id="bib.bib211.4.1" class="ltx_text" style="color:#000000;">
N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni, S. Bakas,
M. N. Galtier, B. A. Landman, K. Maier-Hein, et al., The future of digital
health with federated learning, NPJ digital medicine 3 (1) (2020) 119.
</span>
</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib212.2.2.1" class="ltx_text" style="color:#000000;">(212)</span></span>
<span class="ltx_bibblock"><span id="bib.bib212.4.1" class="ltx_text" style="color:#000000;">
M. H. u. Rehman, W. Hugo Lopez Pinaya, P. Nachev, J. T. Teo, S. Ourselin, M. J.
Cardoso, Federated learning for medical imaging radiology, The British
Journal of Radiology 96 (1150) (2023) 20220890.
</span>
</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib213.2.2.1" class="ltx_text" style="color:#000000;">(213)</span></span>
<span class="ltx_bibblock"><span id="bib.bib213.4.1" class="ltx_text" style="color:#000000;">
M. I. Jaber, B. Song, C. Taylor, C. J. Vaske, S. C. Benz, S. Rabizadeh,
P. Soon-Shiong, C. W. Szeto, A deep learning image-based intrinsic molecular
subtype classifier of breast tumors reveals tumor heterogeneity that may
affect survival, Breast Cancer Research 22 (2020) 1–10.
</span>
</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib214.2.2.1" class="ltx_text" style="color:#000000;">(214)</span></span>
<span class="ltx_bibblock"><span id="bib.bib214.4.1" class="ltx_text" style="color:#000000;">
J. Nyman, T. Denize, Z. Bakouny, C. Labaki, B. M. Titchen, K. Bi, S. N. Hari,
J. Rosenthal, N. Mehta, B. Jiang, et al., Spatially aware deep learning
reveals tumor heterogeneity patterns that encode distinct kidney cancer
states, Cell Reports Medicine 4 (9) (2023).
</span>
</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib215.2.2.1" class="ltx_text" style="color:#000000;">(215)</span></span>
<span class="ltx_bibblock"><span id="bib.bib215.4.1" class="ltx_text" style="color:#000000;">
P. Inglese, J. S. McKenzie, A. Mroz, J. Kinross, K. Veselkov, E. Holmes,
Z. Takats, J. K. Nicholson, R. C. Glen, Deep learning and 3d-desi imaging
reveal the hidden metabolic heterogeneity of cancer, Chemical science 8 (5)
(2017) 3500–3511.
</span>
</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib216.2.2.1" class="ltx_text" style="color:#000000;">(216)</span></span>
<span class="ltx_bibblock"><span id="bib.bib216.4.1" class="ltx_text" style="color:#000000;">
L. U. Khan, W. Saad, Z. Han, E. Hossain, C. S. Hong, Federated learning for
internet of things: Recent advances, taxonomy, and open challenges, IEEE
Communications Surveys &amp; Tutorials 23 (3) (2021) 1759–1799.
</span>
</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib217.2.2.1" class="ltx_text" style="color:#000000;">(217)</span></span>
<span class="ltx_bibblock"><span id="bib.bib217.4.1" class="ltx_text" style="color:#000000;">
G. Wang, Interpret federated learning with shapley values, arXiv preprint
arXiv:1905.04519 (2019).
</span>
</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib218.2.2.1" class="ltx_text" style="color:#000000;">(218)</span></span>
<span class="ltx_bibblock"><span id="bib.bib218.4.1" class="ltx_text" style="color:#000000;">
Z. Qin, L. Yang, Q. Wang, Y. Han, Q. Hu, Reliable and interpretable
personalized federated learning, in: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2023, pp. 20422–20431.
</span>
</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib219.2.2.1" class="ltx_text" style="color:#000000;">(219)</span></span>
<span class="ltx_bibblock"><span id="bib.bib219.4.1" class="ltx_text" style="color:#000000;">
D. Kim, W. Lim, M. Hong, H. Kim, The structure of deep neural network for
interpretable transfer learning, in: 2019 IEEE International Conference on
Big Data and Smart Computing (BigComp), IEEE, 2019, pp. 1–4.
</span>
</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib220.2.2.1" class="ltx_text" style="color:#000000;">(220)</span></span>
<span class="ltx_bibblock"><span id="bib.bib220.4.1" class="ltx_text" style="color:#000000;">
W. Mao, J. Liu, J. Chen, X. Liang, An interpretable deep transfer
learning-based remaining useful life prediction approach for bearings with
selective degradation knowledge fusion, IEEE Transactions on Instrumentation
and Measurement 71 (2022) 1–16.
</span>
</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib221.2.2.1" class="ltx_text" style="color:#000000;">(221)</span></span>
<span class="ltx_bibblock"><span id="bib.bib221.4.1" class="ltx_text" style="color:#000000;">
S. Chen, K. Ma, Y. Zheng, Med3d: Transfer learning for 3d medical image
analysis, arXiv preprint arXiv:1904.00625 (2019).
</span>
</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib222.2.2.1" class="ltx_text" style="color:#000000;">(222)</span></span>
<span class="ltx_bibblock"><span id="bib.bib222.4.1" class="ltx_text" style="color:#000000;">
V. Gupta, M. Demirer, M. Bigelow, K. J. Little, S. Candemir, L. M. Prevedello,
R. D. White, T. P. O’Donnell, M. Wels, B. S. Erdal, Performance of a deep
neural network algorithm based on a small medical image dataset: incremental
impact of 3d-to-2d reformation combined with novel data augmentation,
photometric conversion, or transfer learning, Journal of digital imaging 33
(2020) 431–438.
</span>
</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib223.2.2.1" class="ltx_text" style="color:#000000;">(223)</span></span>
<span class="ltx_bibblock"><span id="bib.bib223.4.1" class="ltx_text" style="color:#000000;">
V. Cheplygina, M. de Bruijne, J. P. Pluim, Not-so-supervised: a survey of
semi-supervised, multi-instance, and transfer learning in medical image
analysis, Medical image analysis 54 (2019) 280–296.
</span>
</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib224.2.2.1" class="ltx_text" style="color:#000000;">(224)</span></span>
<span class="ltx_bibblock"><span id="bib.bib224.4.1" class="ltx_text" style="color:#000000;">
P. Wang, W. Xu, J. Sun, C. Yang, G. Wang, Y. Sa, X.-H. Hu, Y. Feng, A new
assessment model for tumor heterogeneity analysis with [18] f-fdg pet images,
EXCLI journal 15 (2016) 75.
</span>
</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib225.2.2.1" class="ltx_text" style="color:#000000;">(225)</span></span>
<span class="ltx_bibblock"><span id="bib.bib225.4.1" class="ltx_text" style="color:#000000;">
Z. Zhou, V. Sodha, M. M. Rahman Siddiquee, R. Feng, N. Tajbakhsh, M. B. Gotway,
J. Liang, Models genesis: Generic autodidactic models for 3d medical image
analysis, in: Medical Image Computing and Computer Assisted
Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China,
October 13–17, 2019, Proceedings, Part IV 22, Springer, 2019, pp. 384–393.
</span>
</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib226.2.2.1" class="ltx_text" style="color:#000000;">(226)</span></span>
<span class="ltx_bibblock"><span id="bib.bib226.4.1" class="ltx_text" style="color:#000000;">
A. Kareem, H. Liu, V. Velisavljevic, A federated learning framework for
pneumonia image detection using distributed data, Healthcare Analytics (2023)
100204.
</span>
</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib227.2.2.1" class="ltx_text" style="color:#000000;">(227)</span></span>
<span class="ltx_bibblock"><span id="bib.bib227.4.1" class="ltx_text" style="color:#000000;">
M. Repetto, D. La Torre, Breast cancer detection and prediction using federated
multicriteria machine learning, in: 2022 5th International Conference on
Signal Processing and Information Security (ICSPIS), IEEE, 2022, pp. 1–4.
</span>
</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib228.2.2.1" class="ltx_text" style="color:#000000;">(228)</span></span>
<span class="ltx_bibblock"><span id="bib.bib228.4.1" class="ltx_text" style="color:#000000;">
S. Pati, U. Baid, B. Edwards, M. Sheller, S.-H. Wang, G. A. Reina, P. Foley,
A. Gruzdev, D. Karkada, C. Davatzikos, et al., Federated learning enables big
data for rare cancer boundary detection, Nature communications 13 (1) (2022)
7346.
</span>
</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib229.2.2.1" class="ltx_text" style="color:#000000;">(229)</span></span>
<span class="ltx_bibblock"><span id="bib.bib229.4.1" class="ltx_text" style="color:#000000;">
N. T. Arthi, K. E. Mubin, J. Rahman, G. Rafi, T. T. Sheja, M. T. Reza, M. A.
Alam, Decentralized federated learning and deep learning leveraging xai-based
approach to classify colorectal cancer, in: 2022 IEEE Asia-Pacific Conference
on Computer Science and Data Engineering (CSDE), IEEE, 2022, pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib230.2.2.1" class="ltx_text" style="color:#000000;">(230)</span></span>
<span class="ltx_bibblock"><span id="bib.bib230.4.1" class="ltx_text" style="color:#000000;">
E. Bisong, Google colaboratory, in: Building Machine Learning and Deep Learning
Models on Google Cloud Platform, Apress, Berkeley, CA, 2019, pp. 59–64.
</span>
</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib231.2.2.1" class="ltx_text" style="color:#000000;">(231)</span></span>
<span class="ltx_bibblock"><span id="bib.bib231.4.1" class="ltx_text" style="color:#000000;">
S. H. Kassani, P. H. Kassani, M. J. Wesolowski, K. A. Schneider, R. Deters,
Deep transfer learning based model for colorectal cancer histopathology
segmentation: A comparative study of deep pre-trained models, International
Journal of Medical Informatics 159 (2022) 104669.
</span>
</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib232.2.2.1" class="ltx_text" style="color:#000000;">(232)</span></span>
<span class="ltx_bibblock"><span id="bib.bib232.4.1" class="ltx_text" style="color:#000000;">
R. Luo, T. Bocklitz, A systematic study of transfer learning for colorectal
cancer detection, Informatics in Medicine Unlocked (2023) 101292.
</span>
</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib233.2.2.1" class="ltx_text" style="color:#000000;">(233)</span></span>
<span class="ltx_bibblock"><span id="bib.bib233.4.1" class="ltx_text" style="color:#000000;">
N. R. Freitas, P. M. Vieira, A. Cordeiro, C. Tinoco, N. Morais, J. Torres,
S. Anacleto, M. P. Laguna, E. Lima, C. S. Lima, Detection of bladder cancer
with feature fusion, transfer learning and capsnets, Artificial Intelligence
in Medicine 126 (2022) 102275.
</span>
</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib234.2.2.1" class="ltx_text" style="color:#000000;">(234)</span></span>
<span class="ltx_bibblock"><span id="bib.bib234.4.1" class="ltx_text" style="color:#000000;">
S. Azizi, P. Mousavi, P. Yan, A. Tahmasebi, J. T. Kwak, S. Xu, B. Turkbey,
P. Choyke, P. Pinto, B. Wood, et al., Transfer learning from rf to b-mode
temporal enhanced ultrasound features for prostate cancer detection,
International journal of computer assisted radiology and surgery 12 (2017)
1111–1121.
</span>
</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib235.2.2.1" class="ltx_text" style="color:#000000;">(235)</span></span>
<span class="ltx_bibblock"><span id="bib.bib235.4.1" class="ltx_text" style="color:#000000;">
T. Zhang, Y. Feng, Y. Feng, Y. Zhao, Y. Lei, N. Ying, Z. Yan, Y. He, G. Zhang,
Shuffle instances-based vision transformer for pancreatic cancer rose image
classification, arXiv preprint arXiv:2208.06833 (2022).
</span>
</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib236.2.2.1" class="ltx_text" style="color:#000000;">(236)</span></span>
<span class="ltx_bibblock"><span id="bib.bib236.4.1" class="ltx_text" style="color:#000000;">
T. Zhang, Mil-si, </span><a target="_blank" href="https://github.com/sagizty/MIL-SI/tree/main" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://github.com/sagizty/MIL-SI/tree/main</a><span id="bib.bib236.5.2" class="ltx_text" style="color:#000000;"> (2022).
</span>
</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib237.2.2.1" class="ltx_text" style="color:#000000;">(237)</span></span>
<span class="ltx_bibblock"><span id="bib.bib237.4.1" class="ltx_text" style="color:#000000;">
Y. Cheng, L. Zhang, A. Li, Gfl: Federated learning on non-iid data via
privacy-preserving synthetic data, in: 2023 IEEE International Conference on
Pervasive Computing and Communications (PerCom), IEEE, 2023, pp. 61–70.
</span>
</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib238.2.2.1" class="ltx_text" style="color:#000000;">(238)</span></span>
<span class="ltx_bibblock"><span id="bib.bib238.4.1" class="ltx_text" style="color:#000000;">
A. Rehman, H. Xing, L. Feng, M. Hussain, N. Gulzar, M. A. Khan, A. Hussain,
D. Saeed, Fedcscd-gan: A secure and collaborative framework for clinical
cancer diagnosis via optimized federated learning and gan, Biomedical Signal
Processing and Control 89 (2024) 105893.
</span>
</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib239.2.2.1" class="ltx_text" style="color:#000000;">(239)</span></span>
<span class="ltx_bibblock"><span id="bib.bib239.4.1" class="ltx_text" style="color:#000000;">
Y. Wang, Q. Shi, T.-H. Chang, Why batch normalization damage federated learning
on non-iid data?, IEEE Transactions on Neural Networks and Learning Systems
(2023).
</span>
</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib240.2.2.1" class="ltx_text" style="color:#000000;">(240)</span></span>
<span class="ltx_bibblock"><span id="bib.bib240.4.1" class="ltx_text" style="color:#000000;">
X. Zhang, W. Sun, Y. Chen, Tackling the non-iid issue in heterogeneous
federated learning by gradient harmonization, arXiv preprint arXiv:2309.06692
(2023).
</span>
</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib241.2.2.1" class="ltx_text" style="color:#000000;">(241)</span></span>
<span class="ltx_bibblock"><span id="bib.bib241.4.1" class="ltx_text" style="color:#000000;">
Z. Li, Y. Sun, J. Shao, Y. Mao, J. H. Wang, J. Zhang, Feature matching data
synthesis for non-iid federated learning, IEEE Transactions on Mobile
Computing (2024).
</span>
</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib242.2.2.1" class="ltx_text" style="color:#000000;">(242)</span></span>
<span class="ltx_bibblock"><span id="bib.bib242.4.1" class="ltx_text" style="color:#000000;">
A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan,
D. S. W. Ting, Large language models in medicine, Nature medicine 29 (8)
(2023) 1930–1940.
</span>
</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib243.2.2.1" class="ltx_text" style="color:#000000;">(243)</span></span>
<span class="ltx_bibblock"><span id="bib.bib243.4.1" class="ltx_text" style="color:#000000;">
A. Bose, L. Bai, A fully decentralized homomorphic federated learning
framework, in: 2023 IEEE 20th International Conference on Mobile Ad Hoc and
Smart Systems (MASS), IEEE, 2023, pp. 178–185.
</span>
</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib244.2.2.1" class="ltx_text" style="color:#000000;">(244)</span></span>
<span class="ltx_bibblock"><span id="bib.bib244.4.1" class="ltx_text" style="color:#000000;">
A. A. I. M. Sadot, M. M. Mehjabin, A. Mahafuz, A novel approach to efficient
multilabel text classification: Bert-federated learning fusion, in: 2023 26th
International Conference on Computer and Information Technology (ICCIT),
IEEE, 2023, pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib245.2.2.1" class="ltx_text" style="color:#000000;">(245)</span></span>
<span class="ltx_bibblock"><span id="bib.bib245.4.1" class="ltx_text" style="color:#000000;">
W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan, Y. Xie, Y. Li, B. Ding,
J. Zhou, Federatedscope-llm: A comprehensive package for fine-tuning large
language models in federated learning, arXiv preprint arXiv:2309.00363
(2023).
</span>
</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib246.2.2.1" class="ltx_text" style="color:#000000;">(246)</span></span>
<span class="ltx_bibblock"><span id="bib.bib246.4.1" class="ltx_text" style="color:#000000;">
X.-Y. Liu, R. Zhu, D. Zha, J. Gao, S. Zhong, M. Qiu, Differentially private
low-rank adaptation of large language model using federated learning, arXiv
preprint arXiv:2312.17493 (2023).
</span>
</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib247.2.2.1" class="ltx_text" style="color:#000000;">(247)</span></span>
<span class="ltx_bibblock"><span id="bib.bib247.4.1" class="ltx_text" style="color:#000000;">
R. Ye, W. Wang, J. Chai, D. Li, Z. Li, Y. Xu, Y. Du, Y. Wang, S. Chen,
Openfedllm: Training large language models on decentralized private data via
federated learning, arXiv preprint arXiv:2402.06954 (2024).
</span>
</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib248.2.2.1" class="ltx_text" style="color:#000000;">(248)</span></span>
<span class="ltx_bibblock"><span id="bib.bib248.4.1" class="ltx_text" style="color:#000000;">
Z. Abou El Houda, A. S. Hafid, L. Khoukhi, B. Brik, When collaborative
federated learning meets blockchain to preserve privacy in healthcare, IEEE
Transactions on Network Science and Engineering (2022).
</span>
</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib249.2.2.1" class="ltx_text" style="color:#000000;">(249)</span></span>
<span class="ltx_bibblock"><span id="bib.bib249.4.1" class="ltx_text" style="color:#000000;">
B. Li, Z. Liu, L. Shao, B. Qiu, H. Bu, J. Tian, Point transformer with
federated learning for predicting breast cancer her2 status from hematoxylin
and eosin-stained whole slide images, arXiv preprint arXiv:2312.06454 (2023).
</span>
</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib250.2.2.1" class="ltx_text" style="color:#000000;">(250)</span></span>
<span class="ltx_bibblock"><span id="bib.bib250.4.1" class="ltx_text" style="color:#000000;">
W. Gao, D. Wang, Y. Huang, Federated learning-driven collaborative diagnostic
system for metastatic breast cancer, medRxiv (2023) 2023–10.
</span>
</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib251.2.2.1" class="ltx_text" style="color:#000000;">(251)</span></span>
<span class="ltx_bibblock"><span id="bib.bib251.4.1" class="ltx_text" style="color:#000000;">
M. F. Almufareh, N. Tariq, M. Humayun, B. Almas, A federated learning approach
to breast cancer prediction in a collaborative learning framework, in:
Healthcare, Vol. 11, MDPI, 2023, p. 3185.
</span>
</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib252.2.2.1" class="ltx_text" style="color:#000000;">(252)</span></span>
<span class="ltx_bibblock"><span id="bib.bib252.4.1" class="ltx_text" style="color:#000000;">
B. C. Tedeschini, S. Savazzi, R. Stoklasa, L. Barbieri, I. Stathopoulos,
M. Nicoli, L. Serio, Decentralized federated learning for healthcare
networks: A case study on tumor segmentation, IEEE access 10 (2022)
8693–8708.
</span>
</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib253.2.2.1" class="ltx_text" style="color:#000000;">(253)</span></span>
<span class="ltx_bibblock"><span id="bib.bib253.4.1" class="ltx_text" style="color:#000000;">
J. Wicaksana, Z. Yan, X. Yang, Y. Liu, L. Fan, K.-T. Cheng, Customized
federated learning for multi-source decentralized medical image
classification, IEEE Journal of Biomedical and Health Informatics 26 (11)
(2022) 5596–5607.
</span>
</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib254.2.2.1" class="ltx_text" style="color:#000000;">(254)</span></span>
<span class="ltx_bibblock"><span id="bib.bib254.4.1" class="ltx_text" style="color:#000000;">
K. Han, Y. Kim, D. Han, H. Lee, S. Hong, Tl-ada: Transferable loss-based active
domain adaptation, Neural Networks 161 (2023) 670–681.
</span>
</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib255.2.2.1" class="ltx_text" style="color:#000000;">(255)</span></span>
<span class="ltx_bibblock"><span id="bib.bib255.4.1" class="ltx_text" style="color:#000000;">
E. Hajiramezanali, S. Zamani Dadaneh, A. Karbalayghareh, M. Zhou, X. Qian,
Bayesian multi-domain learning for cancer subtype discovery from
next-generation sequencing count data, Advances in Neural Information
Processing Systems 31 (2018).
</span>
</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib256.2.2.1" class="ltx_text" style="color:#000000;">(256)</span></span>
<span class="ltx_bibblock"><span id="bib.bib256.4.1" class="ltx_text" style="color:#000000;">
A. Choudhary, L. Tong, Y. Zhu, M. D. Wang, Advancing medical imaging
informatics by deep learning-based domain adaptation, Yearbook of medical
informatics 29 (01) (2020) 129–138.
</span>
</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib257.2.2.1" class="ltx_text" style="color:#000000;">(257)</span></span>
<span class="ltx_bibblock"><span id="bib.bib257.4.1" class="ltx_text" style="color:#000000;">
K. You, X. Wang, M. Long, M. Jordan, Towards accurate model selection in deep
unsupervised domain adaptation, in: International Conference on Machine
Learning, PMLR, 2019, pp. 7124–7133.
</span>
</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib258.2.2.1" class="ltx_text" style="color:#000000;">(258)</span></span>
<span class="ltx_bibblock"><span id="bib.bib258.4.1" class="ltx_text" style="color:#000000;">
K. Wang, Y. Chen, Y. Zhang, X. Yang, C. Hu, Iterative self-training based
domain adaptation for cross-user semg gesture recognition, IEEE Transactions
on Neural Systems and Rehabilitation Engineering (2023).
</span>
</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib259.2.2.1" class="ltx_text" style="color:#000000;">(259)</span></span>
<span class="ltx_bibblock"><span id="bib.bib259.4.1" class="ltx_text" style="color:#000000;">
Z. L. Azher, A. Suvarna, J.-Q. Chen, Z. Zhang, B. C. Christensen, L. A. Salas,
L. J. Vaickus, J. J. Levy, Assessment of emerging pretraining strategies in
interpretable multimodal deep learning for cancer prognostication, BioData
Mining 16 (1) (2023) 23.
</span>
</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib260.2.2.1" class="ltx_text" style="color:#000000;">(260)</span></span>
<span class="ltx_bibblock"><span id="bib.bib260.4.1" class="ltx_text" style="color:#000000;">
L. Zhen, P. Hu, X. Peng, R. S. M. Goh, J. T. Zhou, Deep multimodal transfer
learning for cross-modal retrieval, IEEE Transactions on Neural Networks and
Learning Systems 33 (2) (2020) 798–810.
</span>
</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib261.2.2.1" class="ltx_text" style="color:#000000;">(261)</span></span>
<span class="ltx_bibblock"><span id="bib.bib261.4.1" class="ltx_text" style="color:#000000;">
Y. Yuan, W. Qin, M. Buyyounouski, B. Ibragimov, S. Hancock, B. Han, L. Xing,
Prostate cancer classification with multiparametric mri transfer learning
model, Medical physics 46 (2) (2019) 756–765.
</span>
</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib262.2.2.1" class="ltx_text" style="color:#000000;">(262)</span></span>
<span class="ltx_bibblock"><span id="bib.bib262.4.1" class="ltx_text" style="color:#000000;">
S. Zhang, Y. Miao, J. Chen, X. Zhang, L. Han, D. Ran, Z. Huang, N. Pei, H. Liu,
C. An, Twist-net: A multi-modality transfer learning network with the hybrid
bilateral encoder for hypopharyngeal cancer segmentation, Computers in
Biology and Medicine 154 (2023) 106555.
</span>
</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib263.2.2.1" class="ltx_text" style="color:#000000;">(263)</span></span>
<span class="ltx_bibblock"><span id="bib.bib263.4.1" class="ltx_text" style="color:#000000;">
M. J. Horry, S. Chakraborty, M. Paul, A. Ulhaq, B. Pradhan, M. Saha, N. Shukla,
Covid-19 detection through transfer learning using multimodal imaging data,
Ieee Access 8 (2020) 149808–149824.
</span>
</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib264.2.2.1" class="ltx_text" style="color:#000000;">(264)</span></span>
<span class="ltx_bibblock"><span id="bib.bib264.4.1" class="ltx_text" style="color:#000000;">
R. F. Khan, B.-D. Lee, M. S. Lee, Transformers in medical image segmentation: a
narrative review, Quantitative Imaging in Medicine and Surgery 13 (12) (2023)
8747.
</span>
</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib265.2.2.1" class="ltx_text" style="color:#000000;">(265)</span></span>
<span class="ltx_bibblock"><span id="bib.bib265.4.1" class="ltx_text" style="color:#000000;">
S. Latif, A. Zaidi, H. Cuayahuitl, F. Shamshad, M. Shoukat, J. Qadir,
Transformers in speech processing: A survey, arXiv preprint arXiv:2303.11607
(2023).
</span>
</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib266.2.2.1" class="ltx_text" style="color:#000000;">(266)</span></span>
<span class="ltx_bibblock"><span id="bib.bib266.4.1" class="ltx_text" style="color:#000000;">
T. Shaik, X. Tao, L. Li, H. Xie, J. D. Velásquez, A survey of multimodal
information fusion for smart healthcare: Mapping the journey from data to
wisdom, Information Fusion (2023) 102040.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.20125" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.20126" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.20126">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.20126" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.20127" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 17:46:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
