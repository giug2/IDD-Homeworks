<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2002.03501] SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA</title><meta property="og:description" content="Segmentation of unseen industrial parts is essential for autonomous industrial systems. However, industrial components are texture-less, reflective, and often found in cluttered and unstructured environments with heavy…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2002.03501">

<!--Generated on Sun Mar 17 19:51:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\TPMargin</span>
<p id="p1.2" class="ltx_p">5pt</p>
</div>
<h1 class="ltx_title ltx_title_document">SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Segmentation of unseen industrial parts is essential for autonomous industrial systems. However, industrial components are texture-less, reflective, and often found in cluttered and unstructured environments with heavy occlusion, which makes it more challenging to deal with unseen objects. To tackle this problem, we present a synthetic data generation pipeline that randomizes textures via domain randomization to focus on the shape information. In addition, we propose an RGB-D Fusion Mask R-CNN with a confidence map estimator, which exploits reliable depth information in multiple feature levels. We transferred the trained model to real-world scenarios and evaluated its performance by making comparisons with baselines and ablation studies. We demonstrate that our methods, which use only synthetic data, could be effective solutions for unseen industrial components segmentation.</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">{textblock}</span>
<p id="p2.2" class="ltx_p">0.84(0.08,0.93) <span id="p2.2.1" class="ltx_text" style="font-size:80%;">©2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
</span></p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p"><span id="p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p3.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
Unseen Object Instance Segmentation, Industrial Component, RGB-D Fusion, Synthetic Data</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the industrial domain, the detection of unseen objects is a key necessity for various industrial automation and robotics applications, such as warehouse bin-picking, sorting, and robotic assembly. Recent advances in deep learning have enabled pixelwise instance segmentation of objects in the broad domain, but generalization performance of deep learning models relies heavily on the amount and quality of training dataset. Thus, building a large-scale, high-quality dataset is crucial for robust and industry-applicable performance. However, repeating the data collecting and labeling procedure manually is costly and often infeasible. Utilizing a large amount of synthetic data for model training can be an inexpensive and effective solution. Through simulations, images and corresponding labels are auto-generated. Additionally, virtual environments can be easily altered, which can offer the flexibility required to deal with fast production cycles and improve the robustness of the model. Recently, the concept of category-agnostic instance segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> has been proposed to detect unseen segment objects efficiently. It has been shown that exploiting deep learning models such as Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> with a large amount of synthetic data for segmentation is useful to generalize unseen objects. Though the results are promising, unseen object instance segmentation for industrial applications has not been studied in detail.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/syn_rgb.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">RGB image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/syn_mask.png" id="S1.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">Segmentation mask</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/syn_depth.png" id="S1.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S1.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">Depth image (initial)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/syn_noisy_depth.png" id="S1.F1.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S1.F1.sf4.3.2" class="ltx_text" style="font-size:90%;">Depth image (augmented)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Examples of synthetic data generated with our proposed method.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Segmenting unseen industrial components is quite challenging due to its unique properties. Many industrial components are texture-less, metallic, and shiny so that their visual appearances vary based on light and object configuration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, while high reflectivity causes missing values and noise in depth maps. They are also often placed in the clutter with heavy occlusion in an unstructured environment while the shape and size of industrial components vary. Besides, only a few industry-relevant public datasets are available, and these datasets lack variation in the number of images and objects. For example, the T-less dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> only includes texture-less objects while ITODD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> only offers 800 scenes of industrial objects. Though <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> offers a vast amount of image pairs in a bin clutter environment, it includes only 10 industrial objects.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">For metallic component detection, Lee et al. proposed a method to generate photorealistic synthetic training data for the segmentation of wrenches in the real world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, they do not concern detecting novel objects but focus on the segmentation of only a single wrench, and fine-tuning using real data should be performed. On the other hand, domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> can be promising since it can be used for the sim-to-real transfer of the deep learning model by utilizing only synthetic data, while domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> requires real data for feature learning. Rendering highly photorealistic synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> can be an alternative to reduce the dependency on real-data, but this requires carefully designed simulations and high computation costs, which offers low flexibility and is not suitable for industrial purposes.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we present a synthetic data generation method that simulates an industrial bin environment with high clutter and occlusion by randomizing textures and adding depth noise. We propose an RGB-D Fusion Mask R-CNN with a confidence module that exploits RGB and depth more effectively. We trained our model using a synthetic dataset and evaluated its performance by conducting a varying input modality ablation study. Through experiments, we demonstrate that the proposed method could be applied to segment unseen industrial components, and fusing RGB and depth with a confidence map estimation can improve industrial component segmentation performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">For the detection of unseen industrial objects, we generated a large-scale dataset that consists of 35,000 RGB-Depth-Mask pairs in a bin clutter environment. Then, we trained the category-agnostic instance segmentation model on a synthetic dataset and transferred the model to the real world without fine-tuning using real-world data. To exploit RGB and depth inputs together, we adopted a confidence module to fuse RGB and depth in Mask R-CNN. We demonstrate that our trained model can segment unseen objects in the real world with non-realistic rendering.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Synthetic Data Generation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To segment industrial components within a clutter, the model should focus on an object’s visual shape while ignoring texture information due to an object’s texture-less, reflective properties. By randomly texturing the objects, a trait adopted from domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we can encourage the model to learn domain-relevant features, such as the shape and edges, while reducing computational costs and human efforts required to optimize simulation environments.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We consider a typical industrial environment where multiple industrial components are placed in a bin with high clutter and occlusion. We collected a total of 149 3D CAD models from public datasets, including industrial objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and household objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.1 Synthetic Data Generation ‣ 2 Method ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/rgb1.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="699" height="393" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/rgb4.png" id="S2.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="699" height="393" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/rgb3.png" id="S2.F2.3.g1" class="ltx_graphics ltx_img_landscape" width="699" height="393" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/rgb4.png" id="S2.F2.4.g1" class="ltx_graphics ltx_img_landscape" width="699" height="393" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 2</span>: </span><span id="S2.F2.7.2" class="ltx_text" style="font-size:90%;">Examples of domain randomized RGB images.</span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/objs.png" id="S2.F3.1.g1" class="ltx_graphics ltx_img_square" width="699" height="606" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/partslist.jpg" id="S2.F3.2.g1" class="ltx_graphics ltx_img_square" width="699" height="699" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 3</span>: </span><span id="S2.F3.5.2" class="ltx_text" style="font-size:90%;">Left: Exemplary objects used for synthetic data generation, Right: Industrial objects for real data.</span></figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2002.03501/assets/img/network.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Proposed RGB-D fusion Mask R-CNN with confidence map estimator.</span></figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Then, we configured a virtual bin-picking environment using V-REP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to render images using physical simulations. For each configuration, we randomized the object’s properties, arrangement, and camera configurations. Specifically, 1-20 random objects were picked from the set and placed in the bin until the total number of objects became 40. For each object in the bin, we captured three RGB-Depth-Segmentation Mask pairs by varying the position, pose, texture, and color of each object. Additionally, the light intensity, camera’s distance, and orientation from the center of the bin were adjusted for each scene. Objects above the bin were removed, and the camera parameters were randomized to fit within the range detectable by the commercial RGB-D sensor Realsense D415. We repeated this procedure and obtained 30,000 training and 5000 validation images. Objects were also split into training and validation sets in the proportion 4:1 to prevent overlap.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">While initial synthetic depth was complete and precise, depth maps in the real world are noisy and have missing values. To add realistic noises and sparsity to the synthetic depth map, we applied 3D Perlin noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> for random distortion. Then, we added random sparse edge noise using a segmentation mask and salt-and-pepper noise. Lastly, random erasing was performed to mimic the missing values in real depth maps. Examples of synthetic data are shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S2.F2" title="Figure 2 ‣ 2.1 Synthetic Data Generation ‣ 2 Method ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>RGB-D Fusion Mask R-CNN with Confidence Map Estimator</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">When texture-less objects are piled within a clutter, using only color information would not be sufficient since occlusion boundaries become vague and dim in RGB images. In this situation, exploiting depth maps with RGB can improve segmentation performance since depths are likely to change sharply on the edge of objects. However, due to the reflective properties of metallic objects, raw depth maps are quite noisy and have missing values. Though raw or inpainted depth maps with hole-filling methods can be used as inputs for the model, this can lead to inaccurate results since the model assigns the same confidence to the whole depth map.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.1 Synthetic Data Generation ‣ 2 Method ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the overall architecture of our proposed network, which utilizes Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> with an FPN backbone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. For RGB-D fusion, we adopted a fusion module (FM) and confidence map estimator (CME), which recalls the concept for a confidence map <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> that was originally proposed for surface normal estimation tasks. In addition, we split the convolutional backbone with RGB and depth branches to handle different modalities with separate networks. ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> was used as the backbone of each branch.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">With the RGB and raw depth maps as inputs, our network extracts RGB and depth features from maps. Then, four different scales of RGB and depth features, (outputs of conv2, conv3, conv4 and conv5 [C2, C3, C4, C5]) in each branch are fused with the resized estimated confidence map in the fusion module. The confidence map estimator assigns pixel-wise reliability using the raw depth map and validity mask with five convolutional layers. The estimated confidence map is resized and multiplied with depth feature maps to apply spatial attention on the depth map. Through this operation, features with high confidence become larger values while ones with lower confidence become small. Then, RGB and depth feature maps are concatenated and converted into a fused feature map using the 1x1 convolution, which halves the number of channels. Finally, the fused feature maps are fed into the feature pyramid network (FPN) and the Mask R-CNN head and predict the bounding box and mask for the object and background.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.14" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:244.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(57.3pt,-32.3pt) scale(1.35905978802897,1.35905978802897) ;">
<table id="S3.T1.14.14" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.14.14.15.1" class="ltx_tr">
<td id="S3.T1.14.14.15.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.14.14.15.1.1.1" class="ltx_text">Method</span></td>
<td id="S3.T1.14.14.15.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.14.14.15.1.2.1" class="ltx_text">RGB</span></td>
<td id="S3.T1.14.14.15.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Depth</td>
<td id="S3.T1.14.14.15.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.14.14.15.1.4.1" class="ltx_text">
<span id="S3.T1.14.14.15.1.4.1.1" class="ltx_inline-block">
<span id="S3.T1.14.14.15.1.4.1.1.1" class="ltx_p">RGB-D</span>
<span id="S3.T1.14.14.15.1.4.1.1.2" class="ltx_p">Fusion Strategy</span>
</span></span></td>
<td id="S3.T1.14.14.15.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Metrics</td>
</tr>
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">filled</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">raw</td>
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">AP<sub id="S3.T1.1.1.1.1.1" class="ltx_sub"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">AP</td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">AR</td>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="7"><span id="S3.T1.2.2.2.2.1" class="ltx_text">
<span id="S3.T1.2.2.2.2.1.1" class="ltx_inline-block">
<span id="S3.T1.2.2.2.2.1.1.1" class="ltx_p">Mask</span>
<span id="S3.T1.2.2.2.2.1.1.2" class="ltx_p">R-CNN</span>
</span></span></td>
<td id="S3.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.2.2.2.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.2.2.2.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.2.2.2.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">67.1</td>
<td id="S3.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t">54.9</td>
<td id="S3.T1.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t">65.0</td>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.3.3.3.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.3.3.3.5" class="ltx_td ltx_align_center">61.2</td>
<td id="S3.T1.3.3.3.6" class="ltx_td ltx_align_center">52.6</td>
<td id="S3.T1.3.3.3.7" class="ltx_td ltx_align_center">62.4</td>
</tr>
<tr id="S3.T1.4.4.4" class="ltx_tr">
<td id="S3.T1.4.4.4.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.4.4.4.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.4.4.4.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.4.4.4.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.4.4.4.5" class="ltx_td ltx_align_center">51.9</td>
<td id="S3.T1.4.4.4.6" class="ltx_td ltx_align_center">45.7</td>
<td id="S3.T1.4.4.4.7" class="ltx_td ltx_align_center">56.6</td>
</tr>
<tr id="S3.T1.6.6.6" class="ltx_tr">
<td id="S3.T1.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.5.5.5.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.6.6.6.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.6.6.6.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Early Fusion</td>
<td id="S3.T1.6.6.6.5" class="ltx_td ltx_align_center">63.3</td>
<td id="S3.T1.6.6.6.6" class="ltx_td ltx_align_center">54.5</td>
<td id="S3.T1.6.6.6.7" class="ltx_td ltx_align_center">64.5</td>
</tr>
<tr id="S3.T1.8.8.8" class="ltx_tr">
<td id="S3.T1.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.7.7.7.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.8.8.8.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.8.8.8.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Early Fusion</td>
<td id="S3.T1.8.8.8.5" class="ltx_td ltx_align_center">60.8</td>
<td id="S3.T1.8.8.8.6" class="ltx_td ltx_align_center">53.7</td>
<td id="S3.T1.8.8.8.7" class="ltx_td ltx_align_center">62.3</td>
</tr>
<tr id="S3.T1.10.10.10" class="ltx_tr">
<td id="S3.T1.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.9.9.9.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.10.10.10.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.10.10.10.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Late Fusion</td>
<td id="S3.T1.10.10.10.5" class="ltx_td ltx_align_center">67.9</td>
<td id="S3.T1.10.10.10.6" class="ltx_td ltx_align_center">55.8</td>
<td id="S3.T1.10.10.10.7" class="ltx_td ltx_align_center">63.2</td>
</tr>
<tr id="S3.T1.12.12.12" class="ltx_tr">
<td id="S3.T1.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.11.11.11.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.12.12.12.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.12.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><svg id="S3.T1.12.12.12.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Late Fusion</td>
<td id="S3.T1.12.12.12.5" class="ltx_td ltx_align_center">67.5</td>
<td id="S3.T1.12.12.12.6" class="ltx_td ltx_align_center">55.5</td>
<td id="S3.T1.12.12.12.7" class="ltx_td ltx_align_center">65.5</td>
</tr>
<tr id="S3.T1.14.14.14" class="ltx_tr">
<td id="S3.T1.14.14.14.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Ours</td>
<td id="S3.T1.13.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><svg id="S3.T1.13.13.13.1.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.14.14.14.4" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S3.T1.14.14.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><svg id="S3.T1.14.14.14.2.pic1" class="ltx_picture" height="11.02" overflow="visible" version="1.1" width="15.75"><g transform="translate(0,11.02) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none"></path></g></svg>
</td>
<td id="S3.T1.14.14.14.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Confidence Map</td>
<td id="S3.T1.14.14.14.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T1.14.14.14.6.1" class="ltx_text ltx_font_bold">69.0</span></td>
<td id="S3.T1.14.14.14.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T1.14.14.14.7.1" class="ltx_text ltx_font_bold">57.7</span></td>
<td id="S3.T1.14.14.14.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T1.14.14.14.8.1" class="ltx_text ltx_font_bold">66.1</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.16.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.17.2" class="ltx_text" style="font-size:90%;">Performance comparison of the Mask R-CNN baselines and our network on real data.</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To evaluate the effectiveness of our synthetic data generation method, the category-agnostic segmentation performance of our model was examined on unseen real-world objects. Additionally, ablation studies were conducted to investigate the effects of input modality and fusion strategy in our proposed network. As evaluation metrics, average precision with IoU thresholds of 0.5 (AP<sub id="S3.p1.1.1" class="ltx_sub"><span id="S3.p1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>), average precision (AP), and average recall (AR) using IoU thresholds from 0.50 to 0.95 with a 0.05 margin were used. The objects occluded by more than 80% were excluded when computing the metrics.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">For the performance evaluation of our proposed method in real-world scenarios, we collected 100 RGB-D image pairs using a commodity-level RGB-D sensor (Realsense D415) and labeled it manually. We captured RGB-D images in a bin environment by randomizing the objects and camera configurations within the randomization range performed in the simulations. As shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1 Synthetic Data Generation ‣ 2 Method ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, objects used in the real dataset vary in shapes and size and has no direct relationship with the simulated objects. Depending on the number of objects in the scene, we define images with less than 15 objects as having a low occlusion and more than 15 objects as high occlusion while ensuring that the number of objects did not exceed 40. The real dataset includes 60 low-occlusion and 40 high-occlusion images.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion2/89/input_raw_depth_89.png" id="S3.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Raw depth image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion2/89/input_filled_depth_89.png" id="S3.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Filled depth image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion2/89/rgb89.png" id="S3.F5.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">RGB only</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion2/89/filleddepth89.png" id="S3.F5.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F5.sf4.3.2" class="ltx_text" style="font-size:90%;">Filled depth + Early fusion</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion2/89/filledlatefusion89.png" id="S3.F5.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S3.F5.sf5.3.2" class="ltx_text" style="font-size:90%;">Filled depth + Late fusion</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion2/89/confidencefusion89.png" id="S3.F5.sf6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S3.F5.sf6.3.2" class="ltx_text" style="font-size:90%;">Ours</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Input depth images (<a href="#S3.F5.sf1" title="In Figure 5 ‣ 3 Experiments ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>, 5b) and inference results with different methods: Mask R-CNN with (5c) RGB input, (5d) RGB and filled depth using early fusion, (5e) RGB and filled depth using late fusion, and (5f) Ours.</span></figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">To train the model, only synthetic data were used. Weights of the network were updated with the Adam optimizer with a learning rate of 1e-4 for 30 epochs, and the best model on validation set was tested on real data. ResNet-50 in the RGB branch was pretrained on ImageNet. For RGB images, random color jitters and Gaussian noise were applied for data augmentation. The depth cut was performed from 0.35–0.8m, and images were resized to 640x360 before being fed into the network. When filled depth maps were used as the input, missing values in the depth maps were filled with a hole filling filter with a nearest neighboring pixel closest to the sensor after applying the spatial edge-preserving filter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">The experimental results are shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 Experiments ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We compared the performance of our model with Mask R-CNN baselines with different input modalities (RGB, filled depth, and raw depth) and RGB-D fusion strategies. When depth is applied as an input, the validity mask was connected and fed into the model with depth together. Early fusion indicates the Mask R-CNN baseline that uses the concatenation of RGB, depth maps and validity masks as inputs. The late fusion model refers to the model with RGB, a depth branch, and a fusion model but does not contain a confidence map estimator. Except for the confidence map estimator, all architecture in late fusion was the same as our proposed model.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Mask R-CNN baselines with RGB inputs show reasonable generality over unseen objects with 54.9 AP and 65.0 AR. This shows that our synthetic data generation methods can be applied to unseen industrial object segmentation without fine-tuning using real data. Among the baselines, our model with a confidence map estimator achieved the best performance compared to all others with 57.7 AP and 66.1 AR. Interestingly, the performance of the baseline that uses only RGB as an input was better than the early fusion baseline. Meanwhile, the performance of the RGB-D fusion with the raw depth model is lower than the model that uses filled depth in both early and late RGB-D fusion. This suggests that an inappropriate RGB-D fusion strategy can lower the segmentation performance, while our model can effectively exploit a raw depth map with a confidence map estimator, which leads to a significant improvement compared to other baselines. Figure <a href="#S3.F5" title="Figure 5 ‣ 3 Experiments ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S3.F6" title="Figure 6 ‣ 3 Experiments ‣ SEGMENTING UNSEEN INDUSTRIAL COMPONENTS IN A HEAVY CLUTTER USING RGB-D FUSION AND SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> show qualitative comparisons between baselines and our model, which suggest that our method can capture unseen objects more sharply than other baselines.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion3/rgb_91.png" id="S3.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="699" height="393" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion3/confidencefusion91.png" id="S3.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="699" height="393" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion3/rgb_93.png" id="S3.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">RGB only</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2002.03501/assets/img/conclusion3/confidencefusion93.png" id="S3.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">Ours</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 6</span>: </span><span id="S3.F6.5.2" class="ltx_text" style="font-size:90%;">Qualitative comparison between Mask R-CNN baselines with RGB inputs and our proposed model.</span></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this study, we aimed to segment unseen industrial parts in a bin-clutter environment using only a synthetic dataset for training. We synthesized RGB-Depth-Mask pairs in simulations by randomizing object textures and mimicking real-world depth noise. To accurately segment texture-less and metallic parts, we proposed a Mask R-CNN with a confidence map estimator for RGB-D fusion, which can exploit raw depth images effectively. We showed that confidence map estimation in comparison to the baseline could lead to better generalization performance of unseen real objects, both qualitatively and quantitatively. As future work, we are planning to extend our system to unseen generic object instance segmentation and compare it with the other algorithms on public datasets.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Michael Danielczuk, Matthew Matl, Saurabh Gupta, Andrew Li, Andrew Lee, Jeffrey
Mahler, and Ken Goldberg,

</span>
<span class="ltx_bibblock">“Segmenting unknown 3d objects from real depth images using mask
r-cnn trained on synthetic data,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">2019 International Conference on Robotics and Automation
(ICRA)</span>. IEEE, 2019, pp. 7283–7290.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Christopher Xie, Yu Xiang, Arsalan Mousavian, and Dieter Fox,

</span>
<span class="ltx_bibblock">“The best of both modes: Separately leveraging rgb and depth for
unseen object instance segmentation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.13236</span>, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick,

</span>
<span class="ltx_bibblock">“Mask r-cnn,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, 2017, pp. 2961–2969.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
José Jeronimo Rodrigues, Jun-Sik Kim, Makoto Furukawa, Joao Xavier, Pedro
Aguiar, and Takeo Kanade,

</span>
<span class="ltx_bibblock">“6d pose estimation of textureless shiny objects using random ferns
for bin-picking,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2012 IEEE/RSJ International Conference on Intelligent Robots
and Systems</span>. IEEE, 2012, pp. 3334–3341.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Tomáš Hodan, Pavel Haluza, Štepán Obdržálek, Jiri
Matas, Manolis Lourakis, and Xenophon Zabulis,

</span>
<span class="ltx_bibblock">“T-less: An rgb-d dataset for 6d pose estimation of texture-less
objects,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">2017 IEEE Winter Conference on Applications of Computer
Vision (WACV)</span>. IEEE, 2017, pp. 880–888.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bertram Drost, Markus Ulrich, Paul Bergmann, Philipp Hartinger, and Carsten
Steger,

</span>
<span class="ltx_bibblock">“Introducing mvtec itodd-a dataset for 3d object recognition in
industry,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span>, 2017, pp. 2200–2208.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Kilian Kleeberger, Christian Landgraf, and Marco F Huber,

</span>
<span class="ltx_bibblock">“Large-scale 6d object pose estimation dataset for industrial
bin-picking,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.12125</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yu-Hui Lee, Chu-Chun Chuang, Shang-Hong Lai, and Zih-Jian Jhang,

</span>
<span class="ltx_bibblock">“Automatic generation of photorealistic training data for detection
of industrial components,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">2019 IEEE International Conference on Image Processing
(ICIP)</span>. IEEE, 2019, pp. 2751–2755.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
Pieter Abbeel,

</span>
<span class="ltx_bibblock">“Domain randomization for transferring deep neural networks from
simulation to the real world,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">2017 IEEE/RSJ international conference on intelligent robots
and systems (IROS)</span>. IEEE, 2017, pp. 23–30.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and Rama
Chellappa,

</span>
<span class="ltx_bibblock">“Learning from synthetic data: Addressing domain shift for semantic
segmentation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, 2018, pp. 3752–3761.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Tomáš Hodaň, Vibhav Vineet, Ran Gal, Emanuel Shalev, Jon
Hanzelka, Treb Connell, Pedro Urbina, Sudipta N Sinha, and Brian Guenter,

</span>
<span class="ltx_bibblock">“Photorealistic image synthesis for object instance detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">2019 IEEE International Conference on Image Processing
(ICIP)</span>. IEEE, 2019, pp. 66–70.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jonathan Dekhtiar, Alexandre Durupt, Matthieu Bricogne, Benoit Eynard, Harvey
Rowson, and Dimitris Kiritsis,

</span>
<span class="ltx_bibblock">“Deep learning for big data applications in cad and plm – research
review, opportunities and case study,”

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Computers in Industry</span>, vol. 100, pp. 227 – 243, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary
Bradski, Kurt Konolige, and Nassir Navab,

</span>
<span class="ltx_bibblock">“Model based training, detection and pose estimation of texture-less
3d objects in heavily cluttered scenes,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Asian conference on computer vision</span>. Springer, 2012, pp.
548–562.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Paul Wohlhart and Vincent Lepetit,

</span>
<span class="ltx_bibblock">“Learning descriptors for object recognition and 3d pose
estimation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2015, pp. 3109–3118.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Eric Rohmer, Surya PN Singh, and Marc Freese,

</span>
<span class="ltx_bibblock">“V-rep: A versatile and scalable robot simulation framework,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">2013 IEEE/RSJ International Conference on Intelligent Robots
and Systems</span>. IEEE, 2013, pp. 1321–1326.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Sergey Zakharov, Benjamin Planche, Ziyan Wu, Andreas Hutter, Harald Kosch, and
Slobodan Ilic,

</span>
<span class="ltx_bibblock">“Keep it unreal: Bridging the realism gap for 2.5 d recognition with
geometry priors only,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">2018 International Conference on 3D Vision (3DV)</span>. IEEE,
2018, pp. 1–11.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan,
and Serge Belongie,

</span>
<span class="ltx_bibblock">“Feature pyramid networks for object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2017, pp. 2117–2125.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jin Zeng, Yanfeng Tong, Yunmu Huang, Qiong Yan, Wenxiu Sun, Jing Chen, and
Yongtian Wang,

</span>
<span class="ltx_bibblock">“Deep surface normal estimation with hierarchical rgb-d fusion,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, 2019, pp. 6153–6162.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,

</span>
<span class="ltx_bibblock">“Deep residual learning for image recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Eduardo SL Gastal and Manuel M Oliveira,

</span>
<span class="ltx_bibblock">“Domain transform for edge-aware image and video processing,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">ACM SIGGRAPH 2011 papers</span>, pp. 1–12. 2011.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2002.03500" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2002.03501" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2002.03501">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2002.03501" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2002.03502" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 19:51:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
