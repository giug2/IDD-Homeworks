<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.19934] Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition</title><meta property="og:description" content="Deep learning developments have improved medical imaging diagnoses dramatically, increasing accuracy in several domains. Nonetheless, obstacles continue to exist because of the requirement for huge datasets and legal l‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.19934">

<!--Generated on Sat Oct  5 19:29:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning Robustness Medical Imaging">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>CINVESTAV, Guadalajara, Mexico </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Universidad Nacional de Ingenieria, Lima, Peru </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Tecnologico de Monterrey, School of Engineering and Sciences, Mexico </span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>CRAN UMR 7039, Universit√© de Lorraine and CNRS, Nancy, France
</span></span></span>
<h1 class="ltx_title ltx_title_document">Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ivan Reyes-Amezcua 
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Rojas-Ruiz
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gilberto Ochoa-Ruiz
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Andres Mendez-Vazquez
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christian Daul
</span><span class="ltx_author_notes">44</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Deep learning developments have improved medical imaging diagnoses dramatically, increasing accuracy in several domains. Nonetheless, obstacles continue to exist because of the requirement for huge datasets and legal limitations on data exchange. A solution is provided by Federated Learning (FL), which permits decentralized model training while maintaining data privacy. However, FL models are susceptible to data corruption, which may result in performance degradation. Using pre-trained models, this research suggests a strong FL framework to improve kidney stone diagnosis. Two different kidney stone datasets, each with six different categories of images, are used in our experimental setting. Our method involves two stages: Learning Parameter Optimization (LPO) and Federated Robustness Validation (FRV). We achieved a peak accuracy of 84.1% with seven epochs and 10 rounds during LPO stage, and 77.2% during FRV stage, showing enhanced diagnostic accuracy and robustness against image corruption. This highlights the potential of merging pre-trained models with FL to address privacy and performance concerns in medical diagnostics, and guarantees improved patient care and enhanced trust in FL-based medical systems.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Federated Learning Robustness Medical Imaging
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advancements of artificial intelligence (AI) in medical imaging have transformed healthcare by enhancing diagnostic capabilities and guiding medical procedures across specialized fields including oncology¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, neurology¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, urology¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and radiology¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. These innovations have revolutionized the medical practice, facilitating precise diagnosis and customized treatments while also encouraging exploration and research initiatives. However, despite the progress made in establishing large-scale medical datasets, challenges persist, especially in clinical areas such as surgical data science and computer-assisted interventions¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The necessity for extensive datasets for training AI models demands collaborative efforts and data sharing among multiple healthcare institutions¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Despite these advantages, such collaboration is often constrained by regulatory requirements, including the EU General Data Protection Regulation and other national privacy laws, as well as concerns regarding data privacy and legal implications. To overcome the challenge of privacy concerns, there is a growing research interest in Federated Learning (FL) within the medical field.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">FL is a decentralized machine learning (ML) approach where model training occurs locally on data distributed across multiple devices or institutions and model updates occur through central aggregation. This enables collaborative learning while forgoing centralized data sharing, fostering privacy and security regarding the patients‚Äô data ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
However, FL can encounter unforeseen challenges due to the drifting and heterogeneity in the local datasets, such as image corruptions and perturbations. These perturbations, originating from various sources such as noise or anomalies or even worse, through intentional malicious alterations, can negatively impact the model performance, leading to inaccurate predictions and a loss of trust ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The vulnerability of FL models to this type of corruption underscores the critical importance of developing robust systems, especially in applications such as medical imaging diagnosis. To address the impact of image perturbations on model performance, researchers are actively exploring various strategies, including robust training techniques¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, adversarial training¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and the use of pre-trained models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. These approaches aim to enhance the resilience of FL models against image corruption by incorporating mechanisms for detecting and mitigating the effects of perturbations during the training process.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the challenges posed by corrupted samples from various distributed institutions and the imperative of maintaining privacy in medical imaging, we propose leveraging pre-trained models within a FL framework.
As highlighted by ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, the use of pre-trained models shows promise in enhancing model robustness and uncertainty quantification. Nonetheless, in the context of medical data, safeguarding data privacy remains essential. In this study, we leverage the use of <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Flower</span>, a specialized framework designed for FL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Flower is an open-source framework that fosters collaboration and innovation in the research community. Its open nature allows for easy sharing, extension, and replication of experiments, promoting transparency and reproducibility in FL research.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Considering these challenges and the potential solutions discussed above, this paper aims to delve deeper into implementing FL with the pre-trained model ResNet18, exploring its efficacy in addressing image corruptions and perturbations to safeguard data privacy in medical imaging within a Flower FL scheme. We are utilizing a medical imaging schema where each federated client is a hospital with different datasets. By examining real applications and experimental results, we aim to shed light on the potential of this approach to revolutionize diagnostic accuracy and patient care in healthcare, particularly focusing on common and critical urological conditions.
To this end, we are using a case study dataset of kidney stone identification across two different data sources.
This dataset allows us to evaluate the performance and robustness of the FL framework in handling data from diverse origins while ensuring the privacy and security of sensitive medical information. Through this case study, we aim to provide comprehensive insights into the practical applications and benefits of integrating FL with pre-trained models in the medical imaging domain.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The rest of the paper is organized as follows. First, in Section 2 we provide an overview of the state of the art of the FL framework and the challenges related to robustness against image corruption. Section 3 discusses the integration of a pre-trained model, specifically ResNet18, into the FL framework and introduces the datasets used in this research. Additionally, it examines the benefits of this approach for enhancing model robustness through robustness transfer. Section 4 details the experimental setup, including preprocessing methods and the two-stage process of Learning Parameter Optimization (LPO) and Federated Robustness Validation (FRV). In section 5, we analyze the results, focusing on the global model‚Äôs performance across different scenarios and parameter configurations. Finally, Section 6 concludes the paper by discussing the implications of our findings and suggesting directions for future research.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>State of the Art</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">FL is an innovative ML technique that enables multiple organizations or institutions to collaboratively solve an ML problem while keeping their data localized <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Coordinated by a central server, a given model is trained and deployed by distributing it to remote data centers, such as hospitals, allowing each site to maintain its own data privacy. Instead of sharing data, each institution trains the model locally and sends updates back to the central server, which aggregates these updates to improve the global model. This process ensures that data from each contributing center is never exchanged during training, enhancing privacy and security. The iterative process continues until the global model is fully trained.
In FL, an important aspect of the training process is the aggregation of model weights from various institutions. This process can impact the overall model performance, and strategies such as federated averaging (FedAvg), which combines the weights of all models, are commonly used.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">A relevant case study can be found in the field of urology, particularly in the analysis and classification of kidney stones using the so-called Morpho-constitutional analysis, particularly using endoscopic images ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. In this context, patient images are confined within individual hospitals, each of which has its restrictions on sharing private data. This scenario presents a significant challenge: developing a robust model despite the presence of corrupted or noisy training images at each hospital. The lack of research on mitigating the impact of noise in kidney stone analysis further underscores the importance of finding effective solutions to enhance model performance in FL frameworks.
To address these challenges, we utilize Flower, a flexible and user-friendly open-source framework designed specifically for FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Flower enables researchers to implement FL systems. Notably, Flower has been identified as a robust framework for FL in medical imaging tasks, as evidenced in the benchmarking study by Fonio <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. This research underscores Flower‚Äôs scalability and usability, making it a standout choice for healthcare applications.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Robustness on Image Corruptions</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Image corruption refers to unintended alterations in image data that can occur due to several factors. Typical examples of corruption in real-world scenarios include Gaussian noise, impulse noise, and defocus blur (Figure <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Robustness on Image Corruptions ‚Ä£ 2 State of the Art ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Weather conditions also affect image quality, due to the presence of fog, snow, and different levels of brightness can also impact image quality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Image corruption is a significant challenge in FL because these perturbations can significantly affect the performance and reliability of the models that are aggregated on the global model. In FL, multiple institutions, such as hospitals, collaborate to train a global model while keeping their data decentralized <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. However, the diversity in data collection methods, imaging devices, and environmental conditions across these institutions increases the likelihood of encountering corrupted images in FL settings. Recent developments have addressed this issue by employing various strategies to reduce the negative impact of image corruption on performance.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.19934/assets/Images/SEC_corruptions.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Example of image corruptions on Kidney stone dataset subjected to Section view (SEC) corruption at a third severity level. These image corruption simulates scenarios with structured environmental changes that can affect image quality.</span></figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Robustness for computer vision evaluates the susceptibility of deep neural networks (DDNs) to image corruptions. Dan Hendrycks and Thomas Dietterich‚Äôs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> research introduces ImageNet-C and ImageNet-P benchmarks to assess DNN performance against common real-world corruptions, such as noise and blur. In contrast, Rusak <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> study focuses on enhancing model resilience through data augmentation and adversarial training techniques. Additionally, Hendrycks <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, introduce the ImageNet-A and ImageNet-O datasets to test model performance against natural adversarial images, further exploring DNN robustness in challenging scenarios. Similarly, in the context of FL, Suyi Li <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> propose a detection framework to identify and exclude malicious clients, defending against adversarial attacks such as Byzantine faults and model poisoning.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Pre-training models on large, clean datasets and fine-tuning them on smaller, target datasets (under certain conditions) is another effective strategy to improve the model‚Äôs robustness against image corruption. The effectiveness of this approach has been supported by Kornblith <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, who demonstrated that models with higher performance on ImageNet tend to transfer better to other tasks. Additionally, Hendrycks <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> found that pre-training not only improves model robustness to adversarial attacks but also enhances uncertainty estimation, making models more reliable in out-of-distribution scenarios. In the context of medical imaging, Raghu <span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> explored the transferability of features from natural images to medical images, providing insights into how pre-training and fine-tuning strategies can be optimized for medical tasks.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Kidney stone formation is influenced by factors like diet, low fluid intake, sedentary lifestyle, age, genetics, and chronic diseases. Identifying the types of kidney stones is crucial for appropriate treatment and relapse prevention. The Morpho-Constitutional Analysis (MCA) is the standard method, involving visual inspection and biochemical analysis of kidney stone fragments, but it is time-consuming and requires specialized expertise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. As an alternative, Endoscopic Stone Recognition (ESR) allows for visual identification during ureteroscopy but is subjective and requires significant expertise. To address these limitations, deep-learning methods have been proposed to automate and accelerate kidney stone identification, aiding real-time decision-making during procedures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Kidney stone morphological and constitutional studies are essential for understanding their biochemical makeup, which is crucial for effective treatment and preventing future episodes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Traditionally, this involves physically extracting stone fragments during ureteroscopy and conducting labor-intensive, time-consuming laboratory analyses. The ability to identify the type of kidney stone directly from in-vivo endoscopic images could transform this process by speeding up diagnosis and potentially eliminating the need for physical extraction, thereby reducing patient exposure to invasive procedures and lowering healthcare costs.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Moreover, the integration of deep learning into kidney stone classification can significantly enhance the accuracy and efficiency of identifying various stone types, which is vital for personalized treatment plans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Deep learning models, trained on vast datasets of endoscopic images, can recognize subtle patterns and characteristics that may be missed by the human eye, thus providing a more reliable diagnosis.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">In the context of medical research and practice, the implementation of federated learning is particularly necessary. Federated learning allows for the development of robust deep learning models across multiple institutions without the need for centralized data sharing, thus addressing privacy concerns associated with patient data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. This decentralized approach ensures that sensitive medical information remains local while still contributing to the collective improvement of AI models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. By harnessing federated learning, medical researchers and practitioners can collaborate effectively, improving diagnostic tools and treatment strategies while maintaining strict patient confidentiality. This is especially critical in healthcare, where data security and privacy are paramount.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Federated Learning for Kidney Stone Identification</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this study, we employ Flower, a Federated Learning framework known for its simplicity and effectiveness in managing complex data distributions and privacy requirements among multiple decentralized nodes (clients). Flower‚Äôs client-server architecture, illustrated in Figure¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3.2 Federated Learning for Kidney Stone Identification ‚Ä£ 3 Method ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, allows the server to coordinate and manage the learning process while clients train models locally on their data, preserving privacy. After local training, clients send model updates (weights or gradients) back to the central server, which uses aggregation techniques like FedAvg to refine the global model. This iterative process of training, updating, and aggregating, repeated over several rounds, enhances the model‚Äôs accuracy, making it a promising approach for real-time kidney stone identification during ureteroscopy.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.19934/assets/Images/Illustration.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.6.3.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">The schematic representation illustrates two stages: Learning Parameters Optimization (LPO), where two clients optimize the <math id="S3.F2.3.1.m1.1" class="ltx_Math" alttext="n_{e}" display="inline"><semantics id="S3.F2.3.1.m1.1b"><msub id="S3.F2.3.1.m1.1.1" xref="S3.F2.3.1.m1.1.1.cmml"><mi id="S3.F2.3.1.m1.1.1.2" xref="S3.F2.3.1.m1.1.1.2.cmml">n</mi><mi id="S3.F2.3.1.m1.1.1.3" xref="S3.F2.3.1.m1.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.3.1.m1.1c"><apply id="S3.F2.3.1.m1.1.1.cmml" xref="S3.F2.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.3.1.m1.1.1.1.cmml" xref="S3.F2.3.1.m1.1.1">subscript</csymbol><ci id="S3.F2.3.1.m1.1.1.2.cmml" xref="S3.F2.3.1.m1.1.1.2">ùëõ</ci><ci id="S3.F2.3.1.m1.1.1.3.cmml" xref="S3.F2.3.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.1.m1.1d">n_{e}</annotation></semantics></math> and <math id="S3.F2.4.2.m2.1" class="ltx_Math" alttext="n_{r}" display="inline"><semantics id="S3.F2.4.2.m2.1b"><msub id="S3.F2.4.2.m2.1.1" xref="S3.F2.4.2.m2.1.1.cmml"><mi id="S3.F2.4.2.m2.1.1.2" xref="S3.F2.4.2.m2.1.1.2.cmml">n</mi><mi id="S3.F2.4.2.m2.1.1.3" xref="S3.F2.4.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.4.2.m2.1c"><apply id="S3.F2.4.2.m2.1.1.cmml" xref="S3.F2.4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.4.2.m2.1.1.1.cmml" xref="S3.F2.4.2.m2.1.1">subscript</csymbol><ci id="S3.F2.4.2.m2.1.1.2.cmml" xref="S3.F2.4.2.m2.1.1.2">ùëõ</ci><ci id="S3.F2.4.2.m2.1.1.3.cmml" xref="S3.F2.4.2.m2.1.1.3">ùëü</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.2.m2.1d">n_{r}</annotation></semantics></math> parameters through model training, and Federated Robustness Validation (FRV), which splits datasets into ‚Äúgood‚Äù and ‚Äúcorrupted‚Äù subsets. The ‚Äúcorrupted‚Äù images undergo diverse corruptions to evaluate the model‚Äôs robustness using the optimized parameters.</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p"><span id="S3.SS2.p2.2.1" class="ltx_text ltx_font_bold">Learning Parameters Optimization (LPO):</span> The initial stage of this research focuses on determining the optimal number of epochs <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="(n_{e})" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.1.m1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.2.cmml">n</mi><mi id="S3.SS2.p2.1.m1.1.1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.1.1.3.cmml">e</mi></msub><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.2">ùëõ</ci><ci id="S3.SS2.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">(n_{e})</annotation></semantics></math> during the local learning phase and the appropriate number of rounds <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="(n_{r})" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p2.2.m2.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.2.cmml">n</mi><mi id="S3.SS2.p2.2.m2.1.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml">r</mi></msub><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2">ùëõ</ci><ci id="S3.SS2.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3">ùëü</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">(n_{r})</annotation></semantics></math> for the collaborative learning phase, as depicted in the left part of Figure¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3.2 Federated Learning for Kidney Stone Identification ‚Ä£ 3 Method ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. These parameters are being identified to establish the conditions under which the FL model reaches its highest accuracy, thereby ensuring optimal performance across distributed settings. To validate our approach, experiments were conducted using two distinct datasets of kidney stones, each representing data from different hospitals. These datasets include six different classes, reflecting the diversity of medical imaging scenarios encountered in each hospital.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p"><span id="S3.SS2.p3.2.1" class="ltx_text ltx_font_bold">Federated Robustness Validation (FRV):</span> The next stage of the study, shown in the right part of Figure¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3.2 Federated Learning for Kidney Stone Identification ‚Ä£ 3 Method ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, involves assuming that the parameters <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="n_{e}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">n</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">ùëõ</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">n_{e}</annotation></semantics></math> and <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="n_{r}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">n</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ùëõ</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">ùëü</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">n_{r}</annotation></semantics></math>, determined in the initial stage, can be applied to the partitions of the datasets used previously. This phase is designed to test the robustness of the FL model in handling noisy clients, such as hospitals, where images may be affected by various types of corruption at different levels of severity.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Datasets</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The experiments were conducted using two kidney stone datasets, where the images were captured using either standard CCD cameras or a ureteroscope. Detailed descriptions of these datasets are provided below.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Dataset A</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>: This endoscopic dataset contains 409 images with 246 surface images and 163 section images. It shares the same classes as Dataset A, except Carbapatite fragments are replaced by Weddelite (sub-type IIa) and Uric Acid (IIIa).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Dataset B</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>: This ex-vivo dataset comprises 366 CCD camera images divided into 209 surface images and 157 section images. It includes six different stone types, categorized by sub-types: WW (Whewellite, subtype Ia), CAR (Carbapatite, IVa), CAR2 (Carbapatite, IVa2), STR (Struvite, IVc), BRU (Brushite, IVd), and CYS (Cystine, Va).</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.19934/assets/x1.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="109" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.19934/assets/x2.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="207" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Samples of ex-vivo kidney stone images captured using (a) a CCD camera and (b) an endoscope. "SEC" refers to section views, and "SUR" refers to surface views.</span></figcaption>
</figure>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Each dataset generated a total of 12,000 patches, categorized into six classes: Dataset A (WW, STR, CYS, BRU, CAR, CAR2) and Dataset B (WW, WD, UA, STR, BRU, CYS). For both datasets, 80% of the patches (9,600 patches) are allocated for training and validation, while the remaining 20% (2,400 patches) are reserved for testing (200 patches per class) (Fig. ¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Experimental Setting ‚Ä£ 4 Experimental Validation ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Robustness Transfer</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In this study, transfer robustness involves using hyperparameters such as the number of epochs and rounds, derived from a model trained under ideal conditions, to guide training in a more challenging environment with corrupted and noisy data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. This preserves foundational knowledge and enhances performance in adverse conditions.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The process starts with clean training on high-quality images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, where the model learns key features and patterns. This phase helps determine optimal hyperparameters for the number of epochs and rounds.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Next, these hyperparameters are applied to training on noisy and corrupted images, crucial for assessing and improving model robustness. This simulates real-world conditions where image quality varies. Using hyperparameters from the clean training phase helps reduce performance loss typically seen with noisy data. This approach is vital in Federated Learning (FL) contexts, where hospitals have varying data-sharing policies and imaging conditions, leading to potential data corruption. Despite these challenges, transfer learning helps maintain model performance and reliability across diverse datasets.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Overall, our FL framework, supported by the Flower platform, combines Federated Robustness Validation (FRV) and Learning Parameters Optimization (LPO) to develop robust models for handling corrupted images, improving diagnostic accuracy and patient care in kidney stone analysis and other medical imaging scenarios.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Validation</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setting</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To evaluate the proposed approach, we conducted a series of experiments in two stages. In the first stage, Learning Parameter Optimization (LPO), we utilized different datasets A and B (Fig. <a href="#S3.F3" title="Figure 3 ‚Ä£ 3.3 Datasets ‚Ä£ 3 Method ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), each containing data from different hospitals.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.19934/assets/Images/two_clients.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Sizes of the train and test subsets from the Jonathan El-Beze (Dataset A) and Michel Daudon (Dataset B) used in the Learning Parameter Optimization (LPO).</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">All images from each dataset underwent a series of transformations such as cropping, flipping, and normalization to help the model become more adaptable to different image conditions. In contrast, consistent resizing and central cropping during testing ensure fair and reliable performance evaluation. Additionally, before starting the training of the global model, 10% of the training data was set aside for validation purposes.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The process began with initializing the global model on the server. In the FL framework, the global model parameters were sent to the connected clients (hospitals), ensuring that each participant started their local training with identical model parameters. These parameters were fine-tuned using the weights of ResNet18, pre-trained on ImageNet, and adapted to classify the six different stone types mentioned earlier, enhancing performance accuracy through transfer learning. After local training, each client produced slightly different versions of the model parameters based on their data. These updates were then sent back to the server, where they were combined through a process called aggregation. In this study, Federated Averaging (FedAvg) was employed as the aggregation strategy. FedAvg computes the weighted average of the model updates, with weights based on the number of clients contributing to the training, resulting in a single unified model.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.19934/assets/Images/MIX_corruptions.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Kidney stone image affected by different types of corruption at the third severity level. This image depicts the MIX subset, showcasing the impact of various image corruptions on kidney stone images from our datasets.</span></figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">The entire process, from the global model parameters being sent to the participating clients to aggregating the updates, constitutes a single round of FL. During each round, the clients only train their local models for a short period. This means that after the aggregation, the model has been trained on the data from all participating clients, but only for a limited time. We then repeat this training process iteratively to eventually arrive at a fully trained model that performs well across the data of all clients.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.2" class="ltx_p">The LPO stage aims to identify the optimal parameters for the number of epochs (<math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="n_{e}" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><msub id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mi id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml">n</mi><mi id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2">ùëõ</ci><ci id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">n_{e}</annotation></semantics></math>) in each local training session and the number of rounds (<math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="n_{r}" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><msub id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml"><mi id="S4.SS1.p5.2.m2.1.1.2" xref="S4.SS1.p5.2.m2.1.1.2.cmml">n</mi><mi id="S4.SS1.p5.2.m2.1.1.3" xref="S4.SS1.p5.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><apply id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.2.m2.1.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p5.2.m2.1.1.2.cmml" xref="S4.SS1.p5.2.m2.1.1.2">ùëõ</ci><ci id="S4.SS1.p5.2.m2.1.1.3.cmml" xref="S4.SS1.p5.2.m2.1.1.3">ùëü</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">n_{r}</annotation></semantics></math>) required for the global model to achieve the highest accuracy. In this stage, we assumed that all clients trained their data for an equal number of epochs. We systematically varied both the number of epochs and rounds, ranging from 1 to 10, to determine the combination that yields the best performance for the global model. Then, these parameters are utilized in the Federated Robustness Validation (FRV) stage.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2409.19934/assets/Images/SUR_corruptions.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Kidney stone image exposed to various types of corruption at the third severity level. This image represents the SUR subset, demonstrating the effects of different image corruptions on kidney stone visuals from our datasets.</span></figcaption>
</figure>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">In FRV stage, each dataset is divided equally into two subsets: one half is labeled as ‚Äúgood‚Äù and the other half as ‚Äúcorrupted‚Äù (Fig.¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.1 Experimental Setting ‚Ä£ 4 Experimental Validation ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). This approach is designed to simulate real-world conditions, where data from different hospitals may vary in quality due to diverse factors such as equipment differences, imaging techniques, or environmental conditions. These corruptions are implemented using custom Python scripts that introduce common image corruptions such as changes in brightness, darkness, contrast variations, motion blur, etc. into clean images. Therefore, the ‚Äúcorrupted‚Äù subsets include images intentionally subjected to various types of corruption at different severity levels (Fig. <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Robustness on Image Corruptions ‚Ä£ 2 State of the Art ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <a href="#S4.F5" title="Figure 5 ‚Ä£ 4.1 Experimental Setting ‚Ä£ 4 Experimental Validation ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#S4.F6" title="Figure 6 ‚Ä£ 4.1 Experimental Setting ‚Ä£ 4 Experimental Validation ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). Corrupted images are generated by randomly sampling the corruption types and severity levels from a uniform distribution.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2409.19934/assets/Images/four_clients.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Sizes of the train and test subsets from the four datasets derived by splitting the Jonathan El-Beze and Michel Daudon datasets. Two subsets are labeled as "good" and the other two as "corrupted".</span></figcaption>
</figure>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">Before initializing the global model on the server, the images from the four clients underwent the same series of transformations as in the first stage. Assuming the optimal parameters identified above are applicable in a four-client setting, they are applied in this FL framework for both the local training of each client and the training of the global model. Similar to the LPO stage, the global model parameters were fine-tuned using ResNet18 weights, pre-trained on ImageNet. FedAvg was also employed as the aggregation strategy.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p">The objective of this stage is to evaluate the robustness of the global model by applying the optimal parameters for both local and global training in a real-world setting, which includes handling image corruption across different clients while preserving data privacy.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.2" class="ltx_p">Both the LPO and FRV stages operate ensuring that no data is exchanged between clients, which is a paramount feature of an FL framework. Additionally, the Adam optimizer was used in this study with a learning rate of <math id="S4.SS1.p9.1.m1.1" class="ltx_Math" alttext="\alpha=0.0001" display="inline"><semantics id="S4.SS1.p9.1.m1.1a"><mrow id="S4.SS1.p9.1.m1.1.1" xref="S4.SS1.p9.1.m1.1.1.cmml"><mi id="S4.SS1.p9.1.m1.1.1.2" xref="S4.SS1.p9.1.m1.1.1.2.cmml">Œ±</mi><mo id="S4.SS1.p9.1.m1.1.1.1" xref="S4.SS1.p9.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p9.1.m1.1.1.3" xref="S4.SS1.p9.1.m1.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p9.1.m1.1b"><apply id="S4.SS1.p9.1.m1.1.1.cmml" xref="S4.SS1.p9.1.m1.1.1"><eq id="S4.SS1.p9.1.m1.1.1.1.cmml" xref="S4.SS1.p9.1.m1.1.1.1"></eq><ci id="S4.SS1.p9.1.m1.1.1.2.cmml" xref="S4.SS1.p9.1.m1.1.1.2">ùõº</ci><cn type="float" id="S4.SS1.p9.1.m1.1.1.3.cmml" xref="S4.SS1.p9.1.m1.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p9.1.m1.1c">\alpha=0.0001</annotation></semantics></math> and a weight decay of <math id="S4.SS1.p9.2.m2.1" class="ltx_Math" alttext="1\times 10^{-5}" display="inline"><semantics id="S4.SS1.p9.2.m2.1a"><mrow id="S4.SS1.p9.2.m2.1.1" xref="S4.SS1.p9.2.m2.1.1.cmml"><mn id="S4.SS1.p9.2.m2.1.1.2" xref="S4.SS1.p9.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p9.2.m2.1.1.1" xref="S4.SS1.p9.2.m2.1.1.1.cmml">√ó</mo><msup id="S4.SS1.p9.2.m2.1.1.3" xref="S4.SS1.p9.2.m2.1.1.3.cmml"><mn id="S4.SS1.p9.2.m2.1.1.3.2" xref="S4.SS1.p9.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p9.2.m2.1.1.3.3" xref="S4.SS1.p9.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.p9.2.m2.1.1.3.3a" xref="S4.SS1.p9.2.m2.1.1.3.3.cmml">‚àí</mo><mn id="S4.SS1.p9.2.m2.1.1.3.3.2" xref="S4.SS1.p9.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p9.2.m2.1b"><apply id="S4.SS1.p9.2.m2.1.1.cmml" xref="S4.SS1.p9.2.m2.1.1"><times id="S4.SS1.p9.2.m2.1.1.1.cmml" xref="S4.SS1.p9.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p9.2.m2.1.1.2.cmml" xref="S4.SS1.p9.2.m2.1.1.2">1</cn><apply id="S4.SS1.p9.2.m2.1.1.3.cmml" xref="S4.SS1.p9.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p9.2.m2.1.1.3.1.cmml" xref="S4.SS1.p9.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p9.2.m2.1.1.3.2.cmml" xref="S4.SS1.p9.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.p9.2.m2.1.1.3.3.cmml" xref="S4.SS1.p9.2.m2.1.1.3.3"><minus id="S4.SS1.p9.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p9.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p9.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p9.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p9.2.m2.1c">1\times 10^{-5}</annotation></semantics></math>. The training was conducted with a batch size of 4, and the Cross-Entropy Loss function was employed during both the training and testing phases. All experiments were implemented in Python 3.10 using PyTorch 2.3.0+cu121 and the Flower 1.9.0 framework. The model was trained on a Tesla T4 GPU (15 GB) provided by Google Colaboratory.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Table¬†<a href="#S5.T1" title="Table 1 ‚Ä£ 5 Results ‚Ä£ Leveraging Pre-trained Models for Robust Federated Learning for Kidney Stone Type Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the performance of the FL model across 10 epochs and 10 rounds during the <span id="S5.p1.1.1" class="ltx_text ltx_font_bold">LPO</span> stage. The heatmap reveals that the highest accuracy, <span id="S5.p1.1.2" class="ltx_text ltx_font_bold">84.1%</span>, is achieved using 7 rounds and 10 epochs.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1" class="ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><svg version="1.1" height="76.94" width="136.5" overflow="visible"><g transform="translate(0,76.94) scale(1,-1)"><path d="M 0,76.94 136.5,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,38.47) scale(1, -1)"><foreignObject width="67.72" height="38.47" overflow="visible">
<span id="S5.T1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S5.T1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S5.T1.1.1.1.pic1.1.1.1.2" class="ltx_p"><span id="S5.T1.1.1.1.pic1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Number</span></span>
<span id="S5.T1.1.1.1.pic1.1.1.1.3" class="ltx_p">of</span>
<span id="S5.T1.1.1.1.pic1.1.1.1.1" class="ltx_p">epochs (<math id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1" class="ltx_Math" alttext="n_{e}" display="inline"><semantics id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1a"><msub id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.2" xref="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.2.cmml">n</mi><mi id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.3" xref="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.2">ùëõ</ci><ci id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.1.1.1.pic1.1.1.1.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.pic1.1.1.1.1.m1.1c">n_{e}</annotation></semantics></math>)</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(68.25,38.47)"><g transform="translate(0,38.47) scale(1, -1)"><foreignObject width="68.25" height="38.47" overflow="visible">
<span id="S5.T1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S5.T1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S5.T1.1.1.1.pic1.2.1.1.2" class="ltx_p"><span id="S5.T1.1.1.1.pic1.2.1.1.2.1" class="ltx_text" style="font-size:70%;">Number</span></span>
<span id="S5.T1.1.1.1.pic1.2.1.1.3" class="ltx_p">of</span>
<span id="S5.T1.1.1.1.pic1.2.1.1.1" class="ltx_p">rounds (<math id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1" class="ltx_Math" alttext="n_{r}" display="inline"><semantics id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1a"><msub id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1" xref="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.cmml"><mi id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.2" xref="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.2.cmml">n</mi><mi id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.3" xref="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.2.cmml" xref="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.2">ùëõ</ci><ci id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.3.cmml" xref="S5.T1.1.1.1.pic1.2.1.1.1.m1.1.1.3">ùëü</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.pic1.2.1.1.1.m1.1c">n_{r}</annotation></semantics></math>)</span>
</span>
</span></foreignObject></g></g></g></svg></th>
<th id="S5.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.2.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">1</span></th>
<th id="S5.T1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.3.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">2</span></th>
<th id="S5.T1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.4.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">3</span></th>
<th id="S5.T1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.5.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">4</span></th>
<th id="S5.T1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.6.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">5</span></th>
<th id="S5.T1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.7.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">6</span></th>
<th id="S5.T1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.8.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">7</span></th>
<th id="S5.T1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.9.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">8</span></th>
<th id="S5.T1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.10.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">9</span></th>
<th id="S5.T1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.11.1" class="ltx_text ltx_inline-block" style="font-size:70%;width:14.0pt;">10</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<th id="S5.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt"><span id="S5.T1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">1</span></th>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.2.1" class="ltx_text" style="font-size:70%;">0.671</span></td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.3.1" class="ltx_text" style="font-size:70%;">0.679</span></td>
<td id="S5.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.4.1" class="ltx_text" style="font-size:70%;">0.681</span></td>
<td id="S5.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.5.1" class="ltx_text" style="font-size:70%;">0.704</span></td>
<td id="S5.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.6.1" class="ltx_text" style="font-size:70%;">0.770</span></td>
<td id="S5.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.7.1" class="ltx_text" style="font-size:70%;">0.680</span></td>
<td id="S5.T1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.8.1" class="ltx_text" style="font-size:70%;">0.710</span></td>
<td id="S5.T1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.9.1" class="ltx_text" style="font-size:70%;">0.737</span></td>
<td id="S5.T1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.10.1" class="ltx_text" style="font-size:70%;">0.785</span></td>
<td id="S5.T1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T1.1.2.1.11.1" class="ltx_text" style="font-size:70%;">0.777</span></td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<th id="S5.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.3.2.1.1" class="ltx_text" style="font-size:70%;">2</span></th>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.2.1" class="ltx_text" style="font-size:70%;">0.695</span></td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.3.1" class="ltx_text" style="font-size:70%;">0.713</span></td>
<td id="S5.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.4.1" class="ltx_text" style="font-size:70%;">0.754</span></td>
<td id="S5.T1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.5.1" class="ltx_text" style="font-size:70%;">0.743</span></td>
<td id="S5.T1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.6.1" class="ltx_text" style="font-size:70%;">0.794</span></td>
<td id="S5.T1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.7.1" class="ltx_text" style="font-size:70%;">0.770</span></td>
<td id="S5.T1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.8.1" class="ltx_text" style="font-size:70%;">0.774</span></td>
<td id="S5.T1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.9.1" class="ltx_text" style="font-size:70%;">0.734</span></td>
<td id="S5.T1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.10.1" class="ltx_text" style="font-size:70%;">0.774</span></td>
<td id="S5.T1.1.3.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.3.2.11.1" class="ltx_text" style="font-size:70%;">0.745</span></td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<th id="S5.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.4.3.1.1" class="ltx_text" style="font-size:70%;">3</span></th>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.2.1" class="ltx_text" style="font-size:70%;">0.661</span></td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.3.1" class="ltx_text" style="font-size:70%;">0.703</span></td>
<td id="S5.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.4.1" class="ltx_text" style="font-size:70%;">0.727</span></td>
<td id="S5.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.5.1" class="ltx_text" style="font-size:70%;">0.760</span></td>
<td id="S5.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.6.1" class="ltx_text" style="font-size:70%;">0.782</span></td>
<td id="S5.T1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.7.1" class="ltx_text" style="font-size:70%;">0.780</span></td>
<td id="S5.T1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.8.1" class="ltx_text" style="font-size:70%;">0.769</span></td>
<td id="S5.T1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.9.1" class="ltx_text" style="font-size:70%;">0.771</span></td>
<td id="S5.T1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.10.1" class="ltx_text" style="font-size:70%;">0.800</span></td>
<td id="S5.T1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.4.3.11.1" class="ltx_text" style="font-size:70%;">0.815</span></td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<th id="S5.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.5.4.1.1" class="ltx_text" style="font-size:70%;">4</span></th>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.2.1" class="ltx_text" style="font-size:70%;">0.683</span></td>
<td id="S5.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.3.1" class="ltx_text" style="font-size:70%;">0.705</span></td>
<td id="S5.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.4.1" class="ltx_text" style="font-size:70%;">0.739</span></td>
<td id="S5.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.5.1" class="ltx_text" style="font-size:70%;">0.768</span></td>
<td id="S5.T1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.6.1" class="ltx_text" style="font-size:70%;">0.738</span></td>
<td id="S5.T1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.7.1" class="ltx_text" style="font-size:70%;">0.735</span></td>
<td id="S5.T1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.8.1" class="ltx_text" style="font-size:70%;">0.772</span></td>
<td id="S5.T1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.9.1" class="ltx_text" style="font-size:70%;">0.797</span></td>
<td id="S5.T1.1.5.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.10.1" class="ltx_text" style="font-size:70%;">0.782</span></td>
<td id="S5.T1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.5.4.11.1" class="ltx_text" style="font-size:70%;">0.807</span></td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<th id="S5.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.6.5.1.1" class="ltx_text" style="font-size:70%;">5</span></th>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.2.1" class="ltx_text" style="font-size:70%;">0.634</span></td>
<td id="S5.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.3.1" class="ltx_text" style="font-size:70%;">0.683</span></td>
<td id="S5.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.4.1" class="ltx_text" style="font-size:70%;">0.701</span></td>
<td id="S5.T1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.5.1" class="ltx_text" style="font-size:70%;">0.702</span></td>
<td id="S5.T1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.6.1" class="ltx_text" style="font-size:70%;">0.739</span></td>
<td id="S5.T1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.7.1" class="ltx_text" style="font-size:70%;">0.783</span></td>
<td id="S5.T1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.8.1" class="ltx_text" style="font-size:70%;">0.818</span></td>
<td id="S5.T1.1.6.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.9.1" class="ltx_text" style="font-size:70%;">0.791</span></td>
<td id="S5.T1.1.6.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.10.1" class="ltx_text" style="font-size:70%;">0.810</span></td>
<td id="S5.T1.1.6.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.6.5.11.1" class="ltx_text" style="font-size:70%;">0.816</span></td>
</tr>
<tr id="S5.T1.1.7.6" class="ltx_tr">
<th id="S5.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.7.6.1.1" class="ltx_text" style="font-size:70%;">6</span></th>
<td id="S5.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.2.1" class="ltx_text" style="font-size:70%;">0.618</span></td>
<td id="S5.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.3.1" class="ltx_text" style="font-size:70%;">0.618</span></td>
<td id="S5.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.4.1" class="ltx_text" style="font-size:70%;">0.670</span></td>
<td id="S5.T1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.5.1" class="ltx_text" style="font-size:70%;">0.655</span></td>
<td id="S5.T1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.6.1" class="ltx_text" style="font-size:70%;">0.702</span></td>
<td id="S5.T1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.7.1" class="ltx_text" style="font-size:70%;">0.725</span></td>
<td id="S5.T1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.8.1" class="ltx_text" style="font-size:70%;">0.748</span></td>
<td id="S5.T1.1.7.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.9.1" class="ltx_text" style="font-size:70%;">0.771</span></td>
<td id="S5.T1.1.7.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.10.1" class="ltx_text" style="font-size:70%;">0.794</span></td>
<td id="S5.T1.1.7.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.7.6.11.1" class="ltx_text" style="font-size:70%;">0.818</span></td>
</tr>
<tr id="S5.T1.1.8.7" class="ltx_tr">
<th id="S5.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.8.7.1.1" class="ltx_text" style="font-size:70%;">7</span></th>
<td id="S5.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.2.1" class="ltx_text" style="font-size:70%;">0.656</span></td>
<td id="S5.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.3.1" class="ltx_text" style="font-size:70%;">0.683</span></td>
<td id="S5.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.4.1" class="ltx_text" style="font-size:70%;">0.709</span></td>
<td id="S5.T1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.5.1" class="ltx_text" style="font-size:70%;">0.698</span></td>
<td id="S5.T1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.6.1" class="ltx_text" style="font-size:70%;">0.734</span></td>
<td id="S5.T1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.7.1" class="ltx_text" style="font-size:70%;">0.770</span></td>
<td id="S5.T1.1.8.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.8.1" class="ltx_text" style="font-size:70%;">0.737</span></td>
<td id="S5.T1.1.8.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.9.1" class="ltx_text" style="font-size:70%;">0.773</span></td>
<td id="S5.T1.1.8.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.10.1" class="ltx_text" style="font-size:70%;">0.809</span></td>
<td id="S5.T1.1.8.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.8.7.11.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.841</span></td>
</tr>
<tr id="S5.T1.1.9.8" class="ltx_tr">
<th id="S5.T1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.9.8.1.1" class="ltx_text" style="font-size:70%;">8</span></th>
<td id="S5.T1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.2.1" class="ltx_text" style="font-size:70%;">0.540</span></td>
<td id="S5.T1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.3.1" class="ltx_text" style="font-size:70%;">0.681</span></td>
<td id="S5.T1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.4.1" class="ltx_text" style="font-size:70%;">0.763</span></td>
<td id="S5.T1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.5.1" class="ltx_text" style="font-size:70%;">0.772</span></td>
<td id="S5.T1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.6.1" class="ltx_text" style="font-size:70%;">0.781</span></td>
<td id="S5.T1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.7.1" class="ltx_text" style="font-size:70%;">0.790</span></td>
<td id="S5.T1.1.9.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.8.1" class="ltx_text" style="font-size:70%;">0.800</span></td>
<td id="S5.T1.1.9.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.9.1" class="ltx_text" style="font-size:70%;">0.809</span></td>
<td id="S5.T1.1.9.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.10.1" class="ltx_text" style="font-size:70%;">0.818</span></td>
<td id="S5.T1.1.9.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.9.8.11.1" class="ltx_text" style="font-size:70%;">0.828</span></td>
</tr>
<tr id="S5.T1.1.10.9" class="ltx_tr">
<th id="S5.T1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.10.9.1.1" class="ltx_text" style="font-size:70%;">9</span></th>
<td id="S5.T1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.2.1" class="ltx_text" style="font-size:70%;">0.602</span></td>
<td id="S5.T1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.3.1" class="ltx_text" style="font-size:70%;">0.702</span></td>
<td id="S5.T1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.4.1" class="ltx_text" style="font-size:70%;">0.736</span></td>
<td id="S5.T1.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.5.1" class="ltx_text" style="font-size:70%;">0.748</span></td>
<td id="S5.T1.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.6.1" class="ltx_text" style="font-size:70%;">0.759</span></td>
<td id="S5.T1.1.10.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.7.1" class="ltx_text" style="font-size:70%;">0.770</span></td>
<td id="S5.T1.1.10.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.8.1" class="ltx_text" style="font-size:70%;">0.781</span></td>
<td id="S5.T1.1.10.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.9.1" class="ltx_text" style="font-size:70%;">0.793</span></td>
<td id="S5.T1.1.10.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.10.1" class="ltx_text" style="font-size:70%;">0.804</span></td>
<td id="S5.T1.1.10.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.10.9.11.1" class="ltx_text" style="font-size:70%;">0.815</span></td>
</tr>
<tr id="S5.T1.1.11.10" class="ltx_tr">
<th id="S5.T1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T1.1.11.10.1.1" class="ltx_text" style="font-size:70%;">10</span></th>
<td id="S5.T1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.2.1" class="ltx_text" style="font-size:70%;">0.655</span></td>
<td id="S5.T1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.3.1" class="ltx_text" style="font-size:70%;">0.614</span></td>
<td id="S5.T1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.4.1" class="ltx_text" style="font-size:70%;">0.618</span></td>
<td id="S5.T1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.5.1" class="ltx_text" style="font-size:70%;">0.623</span></td>
<td id="S5.T1.1.11.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.6.1" class="ltx_text" style="font-size:70%;">0.627</span></td>
<td id="S5.T1.1.11.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.7.1" class="ltx_text" style="font-size:70%;">0.632</span></td>
<td id="S5.T1.1.11.10.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.8.1" class="ltx_text" style="font-size:70%;">0.636</span></td>
<td id="S5.T1.1.11.10.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.9.1" class="ltx_text" style="font-size:70%;">0.641</span></td>
<td id="S5.T1.1.11.10.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.10.1" class="ltx_text" style="font-size:70%;">0.646</span></td>
<td id="S5.T1.1.11.10.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.11.10.11.1" class="ltx_text" style="font-size:70%;">0.649</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.4.2" class="ltx_text" style="font-size:90%;">Accuracy achieved by the global model over 10 epochs and 10 rounds during the Learning Parameter Optimization (LPO) stage.</span></figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">This configuration is used in the next stage, <span id="S5.p2.1.1" class="ltx_text ltx_font_bold">FRV</span>, where corrupted clients are included to assess the model‚Äôs resilience under more challenging conditions. Despite the introduction of data corruptions that simulate real-world disruptions encountered in clinical environments, the model maintains a commendable accuracy of <span id="S5.p2.1.2" class="ltx_text ltx_font_bold">77.2%</span>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we applied FL with the Flower framework to address the problems caused by noisy and distorted images in medical imaging, particularly for urology‚Äôs kidney stone analysis. We used two stages in our methodology: FRV using noisy and corrupted images to imitate real-world settings, and LPO utilizing clean datasets to build a solid model foundation. Through the application of transfer learning, we improved the robustness and resilience of the model by using pre-trained weights from the clean model to reduce performance deterioration when training with noisy data.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The findings show that when clean initial training is combined with later robust validation, FL models‚Äô accuracy and dependability when handling a variety of decentralized datasets are much increased. In the field of medical imaging, where data privacy and quality vary throughout institutions, this technique is very beneficial. The Flower framework highlighted the potential of FL to improve patient care and diagnostic accuracy in healthcare by enabling collaborative learning across several hospitals while maintaining data privacy. Subsequent investigations will concentrate on enhancing the robustness of the model and investigating supplementary medical imaging uses to promote decentralized and privacy-preserving machine learning in healthcare.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors wish to acknowledge the Mexican Council for Science and Technology (CONAHCYT) for their support in terms of postgraduate scholarships in this project, and the Data Science Hub at Tecnologico de Monterrey for their support on this project.
This work has been supported by Azure Sponsorship credits granted by Microsoft‚Äôs AI for Good Research Lab through the AI for Health program. The project was also supported by the French-Mexican ANUIES CONAHCYT Ecos Nord grant 322537. Finally, Michael Rojas work was supported by the Google ExploreCSR project ‚ÄúLATAM Undergraduate Research Program (2024)."</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ali, S., Jha, D., Ghatwary, N., Realdon, S., Cannizzaro, R., Salem, O.E., Lamarque, D., Daul, C., Riegler, M.A., Anonsen, K.V., et¬†al.: A multi-centre polyp detection and segmentation dataset for generalisability assessment. Scientific Data <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">10</span>, ¬†75 (2023). https://doi.org/10.1038/s41597-023-01981-y

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Beutel, D.J., Topal, T., Mathur, A., Qiu, X., Fernandez-Marques, J., Gao, Y., Sani, L., Li, K.H., Parcollet, T., de¬†Gusm√£o, P.P.B., et¬†al.: Flower: A friendly federated learning research framework (2020), preprint at <a target="_blank" href="https://arxiv.org/abs/2007.14390" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/2007.14390</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bi, W.L., Hosny, A., Schabath, M.B., Giger, M.L., Birkbak, N.J., Mehrtash, A., Allison, T., Arnaout, O., Abbosh, C., Dunn, I.F., et¬†al.: Artificial intelligence in cancer imaging: clinical challenges and applications. CA: a cancer journal for clinicians <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">69</span>, 127‚Äì157 (2019). https://doi.org/10.3322/caac.21552

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chowdhury, D., Banerjee, S., Sannigrahi, M., Chakraborty, A., Das, A., Dey, A., Dwivedi, A.D.: Federated learning based covid-19 detection. Expert Systems <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">40</span>, e13173 (2023). https://doi.org/10.1111/exsy.13173

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Corrales, M., Doizi, S., Barghouthy, Y., Traxer, O., Daudon, M.: Classification of stones according to michel daudon: a narrative review. European Urology Focus <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">7</span>(1), 13‚Äì21 (2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Daudon, M., Jungers, P.: Clinical value of crystalluria and quantitative morphoconstitutional analysis of urinary calculi. Nephron Physiology <span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">98</span>(2), p31‚Äìp36 (2004)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Deng, Y., Gazagnadou, N., Hong, J., Mahdavi, M., Lyu, L.: On the hardness of robustness transfer: A perspective from rademacher complexity over symmetric difference hypothesis space (2023), preprint at <a target="_blank" href="https://arxiv.org/abs/2302.12351" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/2302.12351</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
El¬†Beze, J., Mazeaud, C., Daul, C., Ochoa-Ruiz, G., Daudon, M., Eschw√®ge, P., Hubert, J.: Evaluation and understanding of automated urinary stone recognition methods. BJU international (2022)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Estrade, V., Daudon, M., Traxer, O., M√©ria, P., et¬†al.: Pourquoi l‚Äôurologue doit savoir reconna√Ætre un calcul et comment faire? les bases de la reconnaissance endoscopique. Progr√®s en Urologie-FMC <span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">27</span>(2), F26‚ÄìF35 (2017)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Fonio, S.: Benchmarking federated learning frameworks for medical imaging tasks (2024), paper presented at Image Analysis and Processing - ICIAP 2023 Workshops, 21 January 2024

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hamdi, M., Bourouis, S., Rastislav, K., Mohmed, F.: Evaluation of neuro images for the diagnosis of alzheimer‚Äôs disease using deep learning neural network. Frontiers in Public Health <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">10</span>, 834032 (2022). https://doi.org/10.3389/fpubh.2022.834032

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hendrycks, D., Dietterich, T.: Benchmarking neural network robustness to common corruptions and perturbations (2019), preprint at <a target="_blank" href="https://arxiv.org/abs/1903.12261" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/1903.12261</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hendrycks, D., Lee, K., Mazeika, M.: Using pre-training can improve model robustness and uncertainty (2019), preprint at <a target="_blank" href="https://arxiv.org/abs/1901.09960" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/1901.09960</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Song, D.: Natural adversarial examples (2021), preprint at <a target="_blank" href="https://arxiv.org/abs/1907.07174" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/1907.07174</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hong, J., Wang, H., Wang, Z., Zhou, J.: Federated robustness propagation: Sharing adversarial robustness in federated learning (2022), preprint at <a target="_blank" href="https://arxiv.org/abs/2106.10196" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/2106.10196</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kornblith, S., Shlens, J., Le, Q.V.: Do better imagenet models transfer better? (2019), preprint at <a target="_blank" href="https://arxiv.org/abs/1805.08974" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/1805.08974</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Li, S., Cheng, Y., Wang, W., Liu, Y., Chen, T.: Learning to detect malicious clients for robust federated learning (2020), preprint at <a target="_blank" href="https://arxiv.org/abs/2002.00211" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/2002.00211</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Li, T., Sahu, A.K., Talwalkar, A., Smith, V.: Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">37</span>, 50‚Äì60 (2020). https://doi.org/10.1109/MSP.2020.2975749

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Lopez, F., Varelo, A., Hinojosa, O., Mendez, M., Trinh, D.H., ElBeze, Y., Hubert, J., Estrade, V., Gonzalez, M., Ochoa, G., Daul, C.: Assessing deep learning methods for the identification of kidney stones in endoscopic images (2021), paper presented at the 43rd Annual International Conference of the IEEE Engineering in Medicine and Biology Conference (EMBC), November 2021

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Lopez-Tiro, F., Flores-Araiza, D., Betancur-Rengifo, J.P., Reyes-Amezcua, I., Hubert, J., Ochoa-Ruiz, G., Daul, C.: Boosting kidney stone identification in endoscopic images using two-step transfer learning. In: Mexican International Conference on Artificial Intelligence. pp. 131‚Äì141. Springer (2023)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Lyu, L., Yu, H., Ma, X., Chen, C., Sun, L., Zhao, J., Yang, Q., Philip, S.Y.: Privacy and robustness in federated learning: Attacks and defenses. IEEE transactions on neural networks and learning systems pp. 1‚Äì21 (2022). https://doi.org/10.1109/TNNLS.2022.3216981

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Maier-Hein, L., Eisenmann, M., Sarikaya, D., M√§rz, K., Collins, T., Malpani, A., Fallert, J., Feussner, H., Giannarou, S., Mascagni, P., et¬†al.: Surgical data science‚Äìfrom concepts toward clinical translation. Medical image analysis <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">76</span>, 102306 (2022). https://doi.org/10.1016/j.media.2021.102306

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
McMahan, B., Moore, E., Ramage, D., Hampson, S., y¬†Arcas, B.A.: Communication-efficient learning of deep networks from decentralized data (2017), preprint at <a target="_blank" href="https://arxiv.org/abs/1602.05629" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/1602.05629</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ng, D., Lan, X., Yao, M.M.S., Chan, W.P., Feng, M.: Federated learning: a collaborative effort to achieve better medical imaging models for individual sites that have small labelled datasets. Quantitative Imaging in Medicine and Surgery <span id="bib.bib24.1.1" class="ltx_text ltx_font_bold">11</span>, ¬†852 (2021). https://doi.org/10.21037/qims-20-595

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Raghu, M., Zhang, C., Kleinberg, J., Bengio, S.: Transfusion: Understanding transfer learning for medical imaging (2019), preprint at <a target="_blank" href="https://arxiv.org/abs/1902.07208" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/1902.07208</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Reyes-Amezcua, I., Ochoa-Ruiz, G., Mendez-Vazquez, A.: Enhancing image classification robustness through adversarial sampling with delta data augmentation (dda) (2024), paper presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Rusak, E., Schott, L., Zimmermann, R.S., Bitterwolf, J., Bringmann, O., Bethge, M., Brendel, W.: A simple way to make neural networks robust against diverse image corruptions. In: Proceedings of the Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Part III 16. pp. 53‚Äì69 (2020)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Sheller, M.J., Edwards, B., Reina, G.A., Martin, J., Pati, S., Kotrotsou, A., Milchenko, M., Xu, W., Marcus, D., Colen, R.R., et¬†al.: Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data. Scientific reports <span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">10</span>, 12598 (2020). https://doi.org/10.1038/s41598-020-69250-1

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Vaishnavi, P., Eykholt, K., Rahmati, A.: Transferring adversarial robustness through robust representation matching (2022), paper presented at the 31st USENIX Security Symposium (USENIX Security 22)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wang, S., Veldhuis, R., Brune, C., Strisciuglio, N.: A survey on the robustness of computer vision models against common corruptions (2023), preprint at <a target="_blank" href="https://arxiv.org/abs/2305.06024" title="" class="ltx_ref ltx_url">https://arxiv.org/abs/2305.06024</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.19933" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.19934" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.19934">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.19934" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.19935" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 19:29:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
