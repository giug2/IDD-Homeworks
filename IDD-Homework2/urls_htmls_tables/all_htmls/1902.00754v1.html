<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1902.00754] From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home</title><meta property="og:description" content="On the one hand, speech is a key aspect to people’s communication.
On the other, it is widely acknowledged that language proficiency is related to intelligence.
Therefore, intelligent robots should be able to understan…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1902.00754">

<!--Generated on Sat Mar  2 20:41:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Active Vision Group (AGAS), University of Koblenz-Landau.
<br class="ltx_break">Universitätsstr. 1, 56070 Koblenz, Germany</span></span></span>
<h1 class="ltx_title ltx_title_document">From Commands to Goal-based Dialogs:
<br class="ltx_break">A Roadmap to Achieve Natural Language Interaction in RoboCup@Home</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mauricio Matamoros
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karin Harbusch
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dietrich Paulus
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" lang="en-US">On the one hand, speech is a key aspect to people’s communication.
On the other, it is widely acknowledged that language proficiency is related to intelligence.
Therefore, intelligent robots should be able to understand, at least, people’s orders within their application domain.
These insights are not new in RoboCup@Home, but we lack of a long-term plan to evaluate this approach.
</span></p>
<p id="id2.id2" class="ltx_p"><span id="id2.id2.1" class="ltx_text" lang="en-US">In this paper we conduct a brief review of the achievements on automated speech recognition and natural language understanding in RoboCup@Home.
Furthermore, we discuss main challenges to tackle in spoken human-robot interaction within the scope of this competition.
Finally, we contribute by presenting a pipelined road map to engender research in the area of natural language understanding applied to domestic service robotics.
</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
<span id="id3.id1" class="ltx_text" lang="en-US">Robotic competitions Natural Language Understanding Artificial intelligence and robotics.</span>
</div>
<section id="S1" class="ltx_section" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">From its foundation, RoboCup@Home has stressed the importance of natural interaction between humans and robots.
With the target perspective that intelligent robots can understand people’s orders that fall within their application domain.
Thus, <span id="S1.p1.1.1" class="ltx_ERROR undefined">\IfSubStr</span>nHuman-Robot Interaction (HRI)Human-Robot Interaction (HRI) has always been pursued in the competition.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, a detailed evaluation of natural language interactions is not easy.
In favor of many other functions in early stages of the robot development, simple straightforward commands often served the purpose of interaction.
This, along with

<span id="S1.I1" class="ltx_inline-enumerate">
<span id="S1.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">a)</span> <span id="S1.I1.i1.1" class="ltx_text">the noisy competition environments,
</span></span>
<span id="S1.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">b)</span> <span id="S1.I1.i2.1" class="ltx_text">the biased, non-native speaker operators;
and
</span></span>
<span id="S1.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">c)</span> <span id="S1.I1.i3.1" class="ltx_text">the use of command generators to instruct robots
</span></span>
</span>
have impeded the definition of fine-grained evaluation measures RoboCup@Home for natural language understanding in the domain of RoboCup@Home.
</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In order to achieve this goal, we first analyzed the progression of the league in <span id="S1.p3.1.1" class="ltx_ERROR undefined">\IfSubStr</span>nAutomatic Speech Recognition (ASR)Automatic Speech Recognition (ASR) and <span id="S1.p3.1.2" class="ltx_ERROR undefined">\IfSubStr</span>nNatural-Language Understanding (NLU)Natural-Language Understanding (NLU) in the last nine years. Our proposal is based on claims made by the participating teams in their <span id="S1.p3.1.3" class="ltx_ERROR undefined">\IfSubStr</span>nTeam Description Papers (TDPs)Team Description Papers (TDPs), relevant publications, rulebooks, multimedia material available on-line, and our cumulative experience as participants and referees in RoboCup@Home since 2009.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper we present a road map to pave the way towards a completely natural interaction between humans and robots.
This ultimate goal is achieved by defining milestones that promote the use of natural-language interaction.
We underpin our strategy by getting the general public involved in the creation of a large annotated dataset of untrained and unbiased operators, inexistent to the extent of our knowledge.
Here, the competition plays a fundamental role, since RoboCup@Home sets the perfect scenario to involve the audience in data production for scientific use.
We believe this data will foster research in both, <span id="S1.p4.1.1" class="ltx_ERROR undefined">\IfSubStr</span>nNatural-Language Processing (NLP)Natural-Language Processing (NLP) and <span id="S1.p4.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI].</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The paper is organized as follows.
In <a href="#S2" title="2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we briefly introduce the RoboCup@Home league. In turn, we present a summary of the last nine years of the competition, and give an overview of the strategies used by the teams to solve the tests.
In <a href="#S3" title="3 Challenges ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a> we summarize the main unresolved challenges in huma-robot interaction.
In <a href="#S4" title="4 Solution strategy and roadmap ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a> we discuss a roadmap and its milestones to deal with the key problems.
Finally, in <a href="#S5" title="5 Conclusions ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5</span></a> we sum up and draw conclusions.</p>
</div>
</section>
<section id="S2" class="ltx_section" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Speech Recognition and Natural Language Understanding in RoboCup@Home</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">RoboCup@Home aims at developing service and assistive robot technology by evaluating a robot’s performance with a series of <span id="S2.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">tests</span> in an unstandardized and realistic scenario.
Here, new approaches are tested in a competitive setup where robots have to solve a set of common household <span id="S2.p1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">tasks</span> upon request.
Consequently, the competition influences research, even sometimes directing it.
Such is the case of <span id="S2.p1.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] and <span id="S2.p1.1.4" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU], since they are fundamental to <span id="S2.p1.1.5" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI].</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In a <span id="S2.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">test</span>, a task is divided in a sequential set of subgoals or phases, being necessary to fulfill one goal to advance to the next phase <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
This approach keeps difficulty reasonably low for newcomers, while still striving for high top level performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Usually, in the beginning, the <span id="S2.p2.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">operator</span> gives a spoken command to the robot.
A <span id="S2.p2.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">command</span> is typically a short imperative sentence containing all required information to execute a predefined task.
Normally, the operator and the <span id="S2.p2.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;">referee</span> take account of the robot’s processing, whereas the scoring is executed by the <span id="S2.p2.1.5" class="ltx_ERROR undefined">\IfSubStr</span>fnnTechnical Committee (TC)Technical Committee (TC), which is also in charge of design tests and review rules.
Detailed descriptions can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In RoboCup@Home, <span id="S2.p3.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] and <span id="S2.p3.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU] are closely related.
Spoken commands have always been the preferred way to operate a robot and, since robots need to confirm they have understood the command, both abilities are commonly scored together.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The evolution process of <span id="S2.p4.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>xnASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] and <span id="S2.p4.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>xnNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU] in RoboCup@Home is addressed as follows.
In <a href="#S2.SS1" title="2.1 Historical Overview of \IfStrEqCasenHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] testing ‣ 2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>, we summarize the last nine years of competition, focusing on the main challenges the teams had to face.
In <a href="#S2.SS2" title="2.2 Adopted Strategies and Software Solutions for \IfStrEqCasenHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] ‣ 2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>, we present an overview on strategies used to solve the tests.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Historical Overview of <span id="S2.SS1.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] testing</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Our review starts in 2009. We have chosen this year because the rulebook of 2009 is the oldest available on the RoboCup@Home website<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://www.robocupathome.org" title="" class="ltx_ref ltx_url"><span id="footnote1.1.1" class="ltx_text" style="color:#0000FF;">http://www.robocupathome.org</span></a></span></span></span>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">For the competition in <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Graz 2009</span> the guidelines for operating a robot were loose.
Most interactions were hardwired, based on each team’s preconceptions of what a natural <span id="S2.SS1.p2.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] could be.
In addition, the use of headsets and wireless microphones was common,
and some tests rewarded the use of gestures over speech.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Istanbul 2010</span> came with important changes:

<span id="S2.I1" class="ltx_inline-enumerate">
<span id="S2.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">a)</span> <span id="S2.I1.i1.1" class="ltx_text">scoring was made explicit,
</span></span>
<span id="S2.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">b)</span> <span id="S2.I1.i2.1" class="ltx_text">score sheets were included in the rulebook,
and
</span></span>
<span id="S2.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">c)</span> <span id="S2.I1.i3.1" class="ltx_text">interaction guidelines were included as part of the tasks in each test.
</span></span>
</span>
For instance, a robot could score for catching the name of a person or room (typically, only the name was given).</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">But the cornerstone of 2010 was the inclusion of the <span id="S2.SS1.p4.1.1" class="ltx_ERROR undefined">\IfSubStr</span>fnn<em id="S2.SS1.p4.1.2" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)<em id="S2.SS1.p4.1.3" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR) test.
In this test the operator could command the robot to perform any task from any of other test (including those of former years).
Moreover, robots had to deal with long sentences and incomplete information for the first time; a big step in <span id="S2.SS1.p4.1.4" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU] and <span id="S2.SS1.p4.1.5" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI].
Notwithstanding, all interactions in <span id="S2.SS1.p4.1.6" class="ltx_ERROR undefined">\IfStrEqCase</span>nGPSRf<em id="S2.SS1.p4.1.7" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S2.SS1.p4.1.8" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S2.SS1.p4.1.9" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S2.SS1.p4.1.10" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR] followed the patterns used in the other tests.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">For <span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Singapore 2011</span> most interaction guidelines had been removed and a <span id="S2.SS1.p5.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">command generator</span> <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Available on-line: <a target="_blank" href="http://komeisugiura.jp/software/2010_GeneralPurposeTest.tgz" title="" class="ltx_ref ltx_url"><span id="footnote2.1.1" class="ltx_text" style="color:#0000FF;">http://komeisugiura.jp/software/2010˙GeneralPurposeTest.tgz</span></a></span></span></span> was developed by the <span id="S2.SS1.p5.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nTCfTechnical Committee (TC)fnTechnical Committee (TC)xTechnical CommitteexnTechnical Committee[TC] for <span id="S2.SS1.p5.1.4" class="ltx_ERROR undefined">\IfStrEqCase</span>nGPSRf<em id="S2.SS1.p5.1.5" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S2.SS1.p5.1.6" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S2.SS1.p5.1.7" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S2.SS1.p5.1.8" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR] (teams only had access to a limited version).
</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p">No significant changes in <span id="S2.SS1.p6.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP] or spoken <span id="S2.SS1.p6.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] were introduced in the next three years.
Nevertheless, the <span id="S2.SS1.p6.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nTCfTechnical Committee (TC)fnTechnical Committee (TC)xTechnical CommitteexnTechnical Committee[TC] noticed a sustained performance decrease in command retrieval.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p">To help teams, in <span id="S2.SS1.p7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">João Pessoa 2014</span> the <span id="S2.SS1.p7.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nTCfTechnical Committee (TC)fnTechnical Committee (TC)xTechnical CommitteexnTechnical Committee[TC] introduced a way to bypass speech recognition in order to solve the task: the <span id="S2.SS1.p7.1.3" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Continue<span id="S2.SS1.p7.1.3.1" class="ltx_text ltx_font_upright"> rule</span></span>.
However, only few teams took advantage of it.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<p id="S2.SS1.p8.1" class="ltx_p">For <span id="S2.SS1.p8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Hefei 2015</span> the use of QR codes to bypass <span id="S2.SS1.p8.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] was made compulsory.
Moreover, the data recording feature allowed teams to get a scoring bonus and contributing with the league by providing all data acquired by the robot during a test.
In addition, a new command generator was open sourced.
Notwithstanding, even having access to the verbatim output of the <span id="S2.SS1.p8.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nGPSRf<em id="S2.SS1.p8.1.4" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S2.SS1.p8.1.5" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S2.SS1.p8.1.6" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S2.SS1.p8.1.7" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR] command generator via the QR Code, many robots remained unresponsive.</p>
</div>
<div id="S2.SS1.p9" class="ltx_para">
<p id="S2.SS1.p9.1" class="ltx_p">In consequence, <span id="S2.SS1.p9.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Leipzig 2016</span>, the <span id="S2.SS1.p9.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nTCfTechnical Committee (TC)fnTechnical Committee (TC)xTechnical CommitteexnTechnical Committee[TC] decided to provide open access to the command generator and the generation grammars about one month before the competition.
This was a crucial decision due to its direct impact on natural <span id="S2.SS1.p9.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI], as explained in <a href="#S3" title="3 Challenges ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
Cloud Computing was another minor but important change.
Prior to this year, the availability of an Internet connection through the arena’s wireless network wasn’t granted.
However, in combination with the low reliability of the network, discouraged teams from exploring solutions based on cloud services.</p>
</div>
<div id="S2.SS1.p10" class="ltx_para">
<p id="S2.SS1.p10.1" class="ltx_p">By <span id="S2.SS1.p10.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Nagoya 2017</span>, the previous two years of tests focused on benchmarking had paid off (See <a href="#S2.T2" title="In 2.2 Adopted Strategies and Software Solutions for \IfStrEqCasenHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] ‣ 2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>).
In addition to an increase in performance in <span id="S2.SS1.p10.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR], the relevance of command interpretation and <span id="S2.SS1.p10.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU] grew with the inclusion of the <span id="S2.SS1.p10.1.4" class="ltx_ERROR undefined">\IfSubStr</span>nStandard Platform Leagues (SPL)Standard Platform Leagues (SPL).
The out-of-the-box-ready robots allowed teams to focus on high-level problems such as command interpretation, <span id="S2.SS1.p10.1.5" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU], and task planning and reasoning.
This year was the first time in which robots had to answer questions about their environment and previously executed tasks.
Moreover, in the Tour-Guide test they also had to attract people outside the competition area, introduce themselves, and answer people’s questions without the help of any grammar.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Adopted Strategies and Software Solutions for <span id="S2.SS2.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI]</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Either in face-to-face communication or remotely like by phone, radio, or TV, the hearer decodes the produced sounds of the speaker, trying to match them with the best interpretation given the current context.
Similarly, spoken <span id="S2.SS2.p1.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>xnHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] requires the extraction and analysis of the language elements of the uttered sentence.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">One of the most common processing chains for spoken commands</span></figcaption><img src="/html/1902.00754/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="49" alt="Refer to caption">
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Most used 
<br class="ltx_break">ASR software in 2017</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<th id="S2.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Usage</th>
<th id="S2.T1.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">ASR Engine Name</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.4.2.1" class="ltx_tr">
<th id="S2.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">25.81%</th>
<td id="S2.T1.4.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">CMU [Pocket] Sphinx</td>
</tr>
<tr id="S2.T1.4.3.2" class="ltx_tr">
<th id="S2.T1.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">16.13%</th>
<td id="S2.T1.4.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Google Speech Api</td>
</tr>
<tr id="S2.T1.4.4.3" class="ltx_tr">
<th id="S2.T1.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">16.13%</th>
<td id="S2.T1.4.4.3.2" class="ltx_td ltx_align_left ltx_border_r">Microsoft Speech Api</td>
</tr>
<tr id="S2.T1.4.5.4" class="ltx_tr">
<th id="S2.T1.4.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">12.90%</th>
<td id="S2.T1.4.5.4.2" class="ltx_td ltx_align_left ltx_border_r">Julius</td>
</tr>
<tr id="S2.T1.4.6.5" class="ltx_tr">
<th id="S2.T1.4.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">9.68%</th>
<td id="S2.T1.4.6.5.2" class="ltx_td ltx_align_left ltx_border_r">Nuance VoCon/DNS</td>
</tr>
<tr id="S2.T1.4.7.6" class="ltx_tr">
<th id="S2.T1.4.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">6.45%</th>
<td id="S2.T1.4.7.6.2" class="ltx_td ltx_align_left ltx_border_r">Rospeex</td>
</tr>
<tr id="S2.T1.4.8.7" class="ltx_tr">
<th id="S2.T1.4.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">6.45%</th>
<td id="S2.T1.4.8.7.2" class="ltx_td ltx_align_left ltx_border_r">Intel RealSense SDK</td>
</tr>
<tr id="S2.T1.4.9.8" class="ltx_tr">
<th id="S2.T1.4.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">3.23%</th>
<td id="S2.T1.4.9.8.2" class="ltx_td ltx_align_left ltx_border_r">Amazon AWS / Alexa</td>
</tr>
<tr id="S2.T1.4.10.9" class="ltx_tr">
<th id="S2.T1.4.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">3.23%</th>
<td id="S2.T1.4.10.9.2" class="ltx_td ltx_align_left ltx_border_r">Kaldi</td>
</tr>
<tr id="S2.T1.4.11.10" class="ltx_tr">
<th id="S2.T1.4.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">3.23%</th>
<td id="S2.T1.4.11.10.2" class="ltx_td ltx_align_left ltx_border_r">iFlyTek</td>
</tr>
<tr id="S2.T1.4.12.11" class="ltx_tr">
<th id="S2.T1.4.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">9.68%</th>
<td id="S2.T1.4.12.11.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Unreported</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Although possible, dealing with audio signals
can be extremely difficult in high levels of abstraction
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Therefore, the most broadly adopted solution consists in using an <span id="S2.SS2.p2.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>fnnASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] engine to get a text-transcript for further processing as depicted in <a href="#S2.F1" title="In 2.2 Adopted Strategies and Software Solutions for \IfStrEqCasenHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] ‣ 2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
Typically, <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">cardioid microphones</span> are used as main audio input for the <span id="S2.SS2.p2.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] engine to cope with noise.
Afterwards, the <span id="S2.SS2.p2.1.4" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] engine output is preprocessed by some sort of [natural] language processor so the task planner can select the most adequate behavior.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Other than the microphone itself, filters are normally absent and the filtering task is delegated to the <span id="S2.SS2.p3.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] engine itself <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Although several noise-reduction filters have been tested in the competitions, we couldn’t find any reported successful solution other than <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">HARK</span> <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>HARK (<span id="footnote3.1" class="ltx_text ltx_font_bold">H</span>onda Research Institute Japan <span id="footnote3.2" class="ltx_text ltx_font_bold">A</span>udition for <span id="footnote3.3" class="ltx_text ltx_font_bold">R</span>obots with <span id="footnote3.4" class="ltx_text ltx_font_bold">K</span>yoto University) is an open-source robot audition software that includes modules for <span id="footnote3.5" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] and sound-source localization and sound separation. Source: <a target="_blank" href="https://www.hark.jp/" title="" class="ltx_ref ltx_url"><span id="footnote3.6.1" class="ltx_text" style="color:#0000FF;">https://www.hark.jp/</span></a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
However, HARK is used mostly for sound-source localization and separation.
</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Continuing with the pipeline of <a href="#S2.F1" title="In 2.2 Adopted Strategies and Software Solutions for \IfStrEqCasenHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] ‣ 2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>,
the most commonly adopted software solutions include Loquendo (now Nuance) ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, and the Microsoft Speech API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, being most popular <span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CMU Sphinx</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as <a href="#S2.T1" title="In 2.2 Adopted Strategies and Software Solutions for \IfStrEqCasenHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] ‣ 2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows.
However, in 2017, due to the limited computing power of the robots in the <span id="S2.SS2.p4.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nSPLfStandard Platform Leagues (SPL)fnStandard Platform Leagues (SPL)xStandard Platform LeaguesxnStandard Platform Leagues[SPL], most teams used cloud services (mainly the Google speech API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>) or relied on the built-in <span id="S2.SS2.p4.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> which, as the rightmost two columns of <a href="#S2.T2" title="In 2.2 Adopted Strategies and Software Solutions for \IfStrEqCasenHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] ‣ 2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> show, were not as good as other solutions.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.5.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.6.2" class="ltx_text" style="font-size:80%;">Top-10 scores in ASR-related tests
<br class="ltx_break">(final rank in parentheses)</span></figcaption>
<table id="S2.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.2.3.1" class="ltx_tr">
<td id="S2.T2.2.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t" rowspan="2"><span id="S2.T2.2.3.1.1.1" class="ltx_text"> Rank</span></td>
<td id="S2.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.2.3.1.2.1" class="ltx_text">2015</span></td>
<td id="S2.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T2.2.3.1.3.1" class="ltx_text">2016</span></td>
<td id="S2.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">2017</td>
</tr>
<tr id="S2.T2.2.4.2" class="ltx_tr">
<td id="S2.T2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_rr">OPL</td>
<td id="S2.T2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">DSPL</td>
<td id="S2.T2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">SSPL</td>
</tr>
<tr id="S2.T2.2.5.3" class="ltx_tr">
<td id="S2.T2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_tt">1</td>
<td id="S2.T2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">86.7% ( 1)</td>
<td id="S2.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">86.7% ( 3)</td>
<td id="S2.T2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">96.7% ( 1)</td>
<td id="S2.T2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">67.3% ( 1)</td>
<td id="S2.T2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">71.5% ( 1)</td>
</tr>
<tr id="S2.T2.2.6.4" class="ltx_tr">
<td id="S2.T2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr">2</td>
<td id="S2.T2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r">83.3% ( 2)</td>
<td id="S2.T2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r">83.3% ( 6)</td>
<td id="S2.T2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_rr">83.3% ( 3)</td>
<td id="S2.T2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r">45.3% ( 5)</td>
<td id="S2.T2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_r">52.1% ( 2)</td>
</tr>
<tr id="S2.T2.2.7.5" class="ltx_tr">
<td id="S2.T2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr">3</td>
<td id="S2.T2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r">70.0% ( 8)</td>
<td id="S2.T2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r">72.7% (13)</td>
<td id="S2.T2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_rr">73.3% ( 5)</td>
<td id="S2.T2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r">37.3% ( 3)</td>
<td id="S2.T2.2.7.5.6" class="ltx_td ltx_align_center ltx_border_r">36.4% ( 4)</td>
</tr>
<tr id="S2.T2.2.8.6" class="ltx_tr">
<td id="S2.T2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr">4</td>
<td id="S2.T2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r">70.0% (10)</td>
<td id="S2.T2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r">60.7% ( 2)</td>
<td id="S2.T2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_rr">70.0% ( 4)</td>
<td id="S2.T2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_r">12.0% ( 4)</td>
<td id="S2.T2.2.8.6.6" class="ltx_td ltx_align_center ltx_border_r">30.3% ( 5)</td>
</tr>
<tr id="S2.T2.2.9.7" class="ltx_tr">
<td id="S2.T2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr">5</td>
<td id="S2.T2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r">66.7% ( 6)</td>
<td id="S2.T2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r">60.7% ( 5)</td>
<td id="S2.T2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_rr">66.7% ( 2)</td>
<td id="S2.T2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r">3.3% ( 7)</td>
<td id="S2.T2.2.9.7.6" class="ltx_td ltx_align_center ltx_border_r">29.1% ( 6)</td>
</tr>
<tr id="S2.T2.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_tt">
<span id="S2.T2.1.1.1.1" class="ltx_text" style="font-size:50%;">Top5</span> <math id="S2.T2.1.1.1.m1.1" class="ltx_Math" alttext="\overline{x}" display="inline"><semantics id="S2.T2.1.1.1.m1.1a"><mover accent="true" id="S2.T2.1.1.1.m1.1.1" xref="S2.T2.1.1.1.m1.1.1.cmml"><mi id="S2.T2.1.1.1.m1.1.1.2" xref="S2.T2.1.1.1.m1.1.1.2.cmml">x</mi><mo id="S2.T2.1.1.1.m1.1.1.1" xref="S2.T2.1.1.1.m1.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.1b"><apply id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1"><ci id="S2.T2.1.1.1.m1.1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1.1">¯</ci><ci id="S2.T2.1.1.1.m1.1.1.2.cmml" xref="S2.T2.1.1.1.m1.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.1c">\overline{x}</annotation></semantics></math>
</td>
<td id="S2.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">75.3%</td>
<td id="S2.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">72.8%</td>
<td id="S2.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">78.0%</td>
<td id="S2.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">33.0%</td>
<td id="S2.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">43.9%</td>
</tr>
<tr id="S2.T2.2.2" class="ltx_tr">
<td id="S2.T2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr">
<span id="S2.T2.2.2.1.1" class="ltx_text" style="font-size:50%;">Top5</span> <math id="S2.T2.2.2.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S2.T2.2.2.1.m1.1a"><mi id="S2.T2.2.2.1.m1.1.1" xref="S2.T2.2.2.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.1.m1.1b"><ci id="S2.T2.2.2.1.m1.1.1.cmml" xref="S2.T2.2.2.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.1.m1.1c">\sigma</annotation></semantics></math>
</td>
<td id="S2.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">9.0%</td>
<td id="S2.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">12.2%</td>
<td id="S2.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">12.2%</td>
<td id="S2.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">25.8%</td>
<td id="S2.T2.2.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">18.0%</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Despite the remarkable advances achieved in <span id="S2.SS2.p5.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>xnNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP] and task planning in recent years, RoboCup@Home has taken little advantage of it.
Some reasons include

<span id="S2.I2" class="ltx_inline-enumerate">
<span id="S2.I2.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">a)</span> <span id="S2.I2.i1.1" class="ltx_text">the sequential nature of the tests,
</span></span>
<span id="S2.I2.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">b)</span> <span id="S2.I2.i2.1" class="ltx_text">the simplicity of the tasks (<span id="S2.I2.i2.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI]-wise),
</span></span>
<span id="S2.I2.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">c)</span> <span id="S2.I2.i3.1" class="ltx_text">the computational power available in the robot,
</span></span>
<span id="S2.I2.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">d)</span> <span id="S2.I2.i4.1" class="ltx_text">the lack of awareness due to sensors’ limitations,
and
</span></span>
<span id="S2.I2.i5" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">e)</span> <span id="S2.I2.i5.1" class="ltx_text">the need of recognizing only a few words.
</span></span>
</span>
In consequence, most approaches for <span id="S2.SS2.p5.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP] relied in <span id="S2.SS2.p5.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">keyword spotting</span> or pattern matching to trigger the execution of a <span id="S2.SS2.p5.1.4" class="ltx_text ltx_font_bold" style="font-size:90%;">state machine</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
At least in the beginning, this strategy seemed to be faster and more robust than its more advanced counterparts, albeit much simpler.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">Despite this, robust A.I. solutions have always been in play.
As of 2013, it was common to find the task planner and the natural language processor fused in the same module, which doesn’t seem to be the case anymore.
Common strategies included

<span id="S2.I3" class="ltx_inline-enumerate">
<span id="S2.I3.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">a)</span> <span id="S2.I3.i1.1" class="ltx_text">the use of rules of inference for both, sentence parsing and task planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>;
</span></span>
<span id="S2.I3.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">b)</span> <span id="S2.I3.i2.1" class="ltx_text">probabilistic parsers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,
</span></span>
<span id="S2.I3.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">c)</span> <span id="S2.I3.i3.1" class="ltx_text">semantic networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>,
and
</span></span>
<span id="S2.I3.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">d)</span> <span id="S2.I3.i4.1" class="ltx_text">regular expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</span></span>
</span>
to name some.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">Although keyword spotting, <span id="S2.SS2.p7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">pattern matching</span>, and state machines are currently used solutions (specially in simple tests), we spotted more powerful and avant-grade approaches.
Whilst in 2017 only 39% of the teams didn’t mentioned neither <span id="S2.SS2.p7.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP] nor <span id="S2.SS2.p7.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU] approaches in their <span id="S2.SS2.p7.1.4" class="ltx_ERROR undefined">\IfSubStr</span>nTeam Description Paper (TDP)Team Description Paper (TDP), in all other reports the task planner and the <span id="S2.SS2.p7.1.5" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP] are separated.
Among the approaches for processing language, we found

<span id="S2.I4" class="ltx_inline-enumerate">
<span id="S2.I4.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">a)</span> <span id="S2.I4.i1.1" class="ltx_text">Probabilistic Semantic Parsers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>;
</span></span>
<span id="S2.I4.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">b)</span> <span id="S2.I4.i2.1" class="ltx_text">Multimodal Residual Deep Neural Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>,
</span></span>
<span id="S2.I4.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">c)</span> <span id="S2.I4.i3.1" class="ltx_text">ontology-based parsers over inference engines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>,
and
</span></span>
<span id="S2.I4.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">d)</span> <span id="S2.I4.i4.1" class="ltx_text">probabilistic parsers for syntax-tree extraction along with lambda calculus for semantic parsing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
</span></span>
</span>
The <span id="S2.SS2.p7.1.6" class="ltx_text ltx_font_bold" style="font-size:90%;">Stanford Parser</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is the most broadly adopted solution for POS-tagging and syntactic tree extraction, and <span id="S2.SS2.p7.1.7" class="ltx_text ltx_font_bold" style="font-size:90%;">LU4R</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, a Spoken Language Understanding Chain for HRI developed in La Sapienza <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> by participants of RoboCup@Home which is used by several teams.
</p>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p">These newer solutions have increased the robots’ performance in the latter versions of <span id="S2.SS2.p8.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nGPSRf<em id="S2.SS2.p8.1.2" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S2.SS2.p8.1.3" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S2.SS2.p8.1.4" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S2.SS2.p8.1.5" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR] when some ambiguity was added.
Nevertheless, the league does not acquire these newer approaches.
Most robots are still using grammar-based <span id="S2.SS2.p8.1.6" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] engines, which limit the input of the <span id="S2.SS2.p8.1.7" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP] unit.
Moreover, so far no test requires the resolution of ellipsis or anaphora.
Finally, another major inconvenience, is the use of command generators instead of natural interactions.
How to develop <span id="S2.SS2.p8.1.8" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] tests which focus on these problems is discussed in <a href="#S3" title="3 Challenges ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Challenges</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As stated in previous sections, several setbacks have been holding back the league’s advances in natural language dialogs with the robot.
Here we exemplify challenges and come up with guidelines for future research when addressing tougher scenarios.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Noise</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">One of the most problematic aspects (and to which most attention has been paid) is the noise in the competition environment.
Having a separated hall as in 2012 didn’t help much.
The <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ambient noise</span> produced by over two hundred people greatly exceeds the noise levels of an average apartment.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In the past, several solutions have been proposed to this problem.
Having a separate hall, arenas with tall walls, and transform the arena in a sound-proof closed area with glass walls are recurrent examples.
However, service robots will also operate in noisy environments such as airports and shopping malls.
Thus, we think it is best to deal with this issue in an early stage.
For this reason, noise is addressed in the roadmap presented in <a href="#S4" title="4 Solution strategy and roadmap ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Operators</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">An aspect that has characterized RoboCup@Home is that the league has always been robot-friendly.
People volunteering as operators in RoboCup@Home are often patient and prone to follow the robot’s instructions, unconsciously trying to help the robot to succeed (e.g. repeat a given command louder when the robot seems unresponsive).
In addition, almost all the operators are specialists, or at least familiar with service robotics.
In other words, all testing has been performed by (unconsciously) <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">biased operators</span>, making it easier for the robots to accomplish their tasks.
However, trying to give a positive impression to the audience, has the disadvantage of voiding the purpose of RoboCup@Home of providing real-case scenarios for testing.
</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Another important closely related aspect addresses the demographics of the league’s participants.
An international community will offer <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">diverse accents</span> to the robots and styles (which may not be correct).
But this diversity comes with a price.
With few exceptions, operators having English as second language lack the richness and verbal fluency of a native speaker.
Same hold for gender and age.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Therefore bias, variety of speech and diversity of accents, lexicon, and styles, are also addressed in the roadmap of <a href="#S4" title="4 Solution strategy and roadmap ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Generators</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Although the rulebooks never provided interaction templates, suggested guidelines and the sentences produced by the command generators codify biased <span id="S3.SS3.p1.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] in a similar way.
Besides, officializing the release of the command generator and its <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">grammars</span> had the pernicious effect of replacing natural language with a simplistic one, i.e. artificializing interactions.
Moreover, the person in charge of the generator (one of the authors of this paper), although proficient, is not a native speaker of the official competition language (US-English).
In summary, despite stimulating immediate positive results and supplying the teams with a powerful tool, the careless use of generators might delve into a ballast in the long term.
For this reason, the use of command generators is revised in the roadmap of <a href="#S4" title="4 Solution strategy and roadmap ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In order to overcome the problems discussed in this section, the proposed solution should force robots to deal with a vast diversity of unbiased operators speaking freely and under reasonable conditions of noise,
while keeping tests fair and scientifically meaningful.
This ultimate goal is analyzed and split into small steps which are presented in <a href="#S4" title="4 Solution strategy and roadmap ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Solution strategy and roadmap</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Ideally, the robots will lead a natural-language dialogue in real-environment conditions.
This is the ultimate goal in service robotics regarding <span id="S4.p1.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU].
Considering the challenges presented in <a href="#S3" title="3 Challenges ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a> we propose a series of specific tests and changes to the existing ones.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In the previous section we identified the biased operators in combination with restricted GPSR as a main obstacle for the stagnation in elaborate HRI.
In order to set up a rich corpus of human-robot dialogs, we opt for recruiting untrained English native speakers as <span id="S4.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">untrained operators</span>, e.g. selected from the audience.
The <span id="S4.p2.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">unbiased operators</span> must have little to no previous experience in RoboCup@Home (or preferably in robotics), and be allowed to interact freely with the robot.
With these changes, we are solving biasing, variety and diversity of speech, and tackling the artificiality introduced with the command generators.
</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Needless to say, the proposed changes towards free natural-language dialogs with the robot have to be applied gradually.
Therefore, in this section we present a roadmap of three phases that takes as axis the <span id="S4.p3.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>xnnGPSRf<em id="S4.p3.1.2" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S4.p3.1.3" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S4.p3.1.4" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S4.p3.1.5" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR] test before changes are propagated and adapted for other tests.
Each phase sets a constraint that will rule over the upcoming featured milestones.
Milestones present <span id="S4.p3.1.6" class="ltx_text ltx_font_bold" style="font-size:90%;">small increments</span> in the difficulty of the <span id="S4.p3.1.7" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU] task, presumably achievable with the data collected in former years.
Once solved in <span id="S4.p3.1.8" class="ltx_ERROR undefined">\IfStrEqCase</span>nGPSRf<em id="S4.p3.1.9" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S4.p3.1.10" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S4.p3.1.11" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S4.p3.1.12" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR], the constraints of a milestone would become standard practices in other tests.
In addition, milestones are planned to respect the two-years-limit established by the founders of the league <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Furthermore, the proposed roadmap has the advantage of being adaptable.
If sufficient performance hasn’t been achieved, the milestones can be shifted forward in time.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In order not to overtax the teams, we propose the following 3-step pipeline to implement the changes by small increments towards free natural-language dialogs with the robots.
First, teams test and benchmark with recordings addressing the features of the milestones newly defined; then, those features are tested in <span id="S4.p4.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nGPSRf<em id="S4.p4.1.2" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S4.p4.1.3" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S4.p4.1.4" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S4.p4.1.5" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR]; and finally, propagated to all other pertinent tests.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Hence, the strategy considers also the inclusion of a <span id="S4.p5.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span><span id="S4.p5.1.2" class="ltx_text ltx_font_italic">xnNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU] and Action Planning</span> benchmark in Stage I with a small contribution to the overall score.
In this benchmark, a team receives a set of <span id="S4.p5.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">sound files</span>, each one containing a task-execution request from an unbiased operator of the same kind that would be given in <span id="S4.p5.1.4" class="ltx_ERROR undefined">\IfStrEqCase</span>nGPSRf<em id="S4.p5.1.5" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S4.p5.1.6" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S4.p5.1.7" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S4.p5.1.8" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR].
The A.I. of the robot needs to transcribe and analyze these recordings, extracting either a goal, a plan with a set of actions to carry out, or a set of questions or statements to continue the interaction.
The score should consider not only the analytical quality of producing transcripts (where applicable penalizing hard-wired constructions in favor of generalized <span id="S4.p5.1.9" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP]-rules), but also how far the robot went in its planning, and if it was following the right direction.
In this way, milestones could be tested one or two years before being implemented in <span id="S4.p5.1.10" class="ltx_ERROR undefined">\IfStrEqCase</span>nGPSRf<em id="S4.p5.1.11" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)fn<em id="S4.p5.1.12" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em> (GPSR)x<em id="S4.p5.1.13" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>xn<em id="S4.p5.1.14" class="ltx_emph ltx_font_italic">General Purpose Service Robot</em>[GPSR] and propagated to other tests, giving time to teams to prepare.
Furthermore, by giving the same recordings to all teams, <span id="S4.p5.1.15" class="ltx_text ltx_font_bold" style="font-size:90%;">fairness</span> and replicability are addressed,
while noise can be tackled by superposing ambient <span id="S4.p5.1.16" class="ltx_text ltx_font_bold" style="font-size:90%;">noise</span> to the recordings of the operators.
It will be up to the <span id="S4.p5.1.17" class="ltx_ERROR undefined">\IfStrEqCase</span>nTCfTechnical Committee (TC)fnTechnical Committee (TC)xTechnical CommitteexnTechnical Committee[TC] to decide on the nature and intensity of the noise regarding the league’s advances in the subject.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">In addition, to support the league and foster research,
the proposed strategy exploits the <span id="S4.p6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Data Recording</span>
feature, incorporated in 2015 to the competition. This is deemed as fundamental for the success of our proposal.
We think all interactions between the operator and the robot should be recorded and distributed under <span id="S4.p6.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Open Access</span> license as soon as a transcription is available.
This includes all benchmarking sets from previous years.
In this way, the league supports both, experienced and new competitors.
Beyond this obvious assistance the entire scientific community gains relevance.
In early stages, teams providing recordings of all speech-based interactions during a test might receive a bonus proportional to the score obtained, as well as an additional bonus or a certificate (as decided by the <span id="S4.p6.1.3" class="ltx_ERROR undefined">\IfStrEqCase</span>nTCfTechnical Committee (TC)fnTechnical Committee (TC)xTechnical CommitteexnTechnical Committee[TC]) for annotating the provided data.
Later on, such policy should be made compulsory for the roadmap to work with full efficiency, automating the collection process if possible.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">Before summing up by means of presenting the roadmap, we have to state some minor clarifications:

<span id="S4.I1" class="ltx_inline-enumerate">
<span id="S4.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">1)</span> <span id="S4.I1.i1.1" class="ltx_text">In all phases, operators with no background in robotics are selected from the audience by the <span id="S4.I1.i1.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nTCfTechnical Committee (TC)fnTechnical Committee (TC)xTechnical CommitteexnTechnical Committee[TC].
</span></span>
<span id="S4.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">2)</span> <span id="S4.I1.i2.1" class="ltx_text">Unless the test specifies otherwise, chosen operators shall be fluent English speakers.
</span></span>
<span id="S4.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">3)</span> <span id="S4.I1.i3.1" class="ltx_text">Commands, goals, and tasks are generated before the test. A referee must explain all pertinent information to the operators before spoken interactions are recorded and noise overlapped.
</span></span>
<span id="S4.I1.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">4)</span> <span id="S4.I1.i4.1" class="ltx_text">The <span id="S4.I1.i4.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nTCfTechnical Committee (TC)fnTechnical Committee (TC)xTechnical CommitteexnTechnical Committee[TC] randomly assigns to each robot a set of generated tasks from the pool. The robot will listen to the recording, but the operator must be present for further (unexpected) interactions.
</span></span>
</span></p>
</div>
<div id="S4.p8" class="ltx_para ltx_noindent">
<p id="S4.p8.1" class="ltx_p"><span id="S4.p8.1.1" class="ltx_text ltx_font_bold">Roadmap phase 1: From commands to goal formulation</span></p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">The referee gives a specific goal to the operator along with all pertinent information regarding the desired task.
For this purpose, a random command generator can be used.
When required, the operator can practice with the referee. Referees assist operators in unexpected situations.</p>
</div>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">The intensity of overlapped noise should increase gradually, starting from relatively quiet environments (country house, city apartment, office, etc) towards moderately noisy ones (busy office, or a restaurant with background music).</p>
</div>
<div id="S4.p11" class="ltx_para ltx_noindent">
<p id="S4.p11.1" class="ltx_p"><span id="S4.p11.1.1" class="ltx_text ltx_font_bold">Yearly progress</span></p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2019</span> 
<div id="S4.I2.ix1.p1" class="ltx_para">
<p id="S4.I2.ix1.p1.1" class="ltx_p"><span id="S4.I2.ix1.p1.1.1" class="ltx_text" style="font-size:80%;">The operator reads the generated command.</span>
<br class="ltx_break"><span id="S4.I2.ix1.p1.1.2" class="ltx_text ltx_font_bold" style="font-size:80%;">Note:</span><span id="S4.I2.ix1.p1.1.3" class="ltx_text" style="font-size:80%;"> This vanilla milestone introduces no change to allow the benchmarking of the next milestone in the pipeline.</span></p>
</div>
</li>
<li id="S4.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2020</span> 
<div id="S4.I2.ix2.p1" class="ltx_para">
<p id="S4.I2.ix2.p1.1" class="ltx_p"><span id="S4.I2.ix2.p1.1.1" class="ltx_text" style="font-size:80%;">The operator tries to memorize the command, repeating it to the robot afterwards. Slight variations are expected.</span></p>
</div>
</li>
<li id="S4.I2.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2022</span> 
<div id="S4.I2.ix3.p1" class="ltx_para">
<p id="S4.I2.ix3.p1.1" class="ltx_p"><span id="S4.I2.ix3.p1.1.1" class="ltx_text" style="font-size:80%;">The referee explains the task to the operator, who has to command the robot using their own words (e.g. rephrasing).</span></p>
</div>
</li>
<li id="S4.I2.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2024</span> 
<div id="S4.I2.ix4.p1" class="ltx_para">
<p id="S4.I2.ix4.p1.1" class="ltx_p"><span id="S4.I2.ix4.p1.1.1" class="ltx_text" style="font-size:80%;">The operator requires the robot to accomplish a task, although not necessarily in an imperative way, as if suggesting.</span></p>
</div>
</li>
<li id="S4.I2.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2028</span> 
<div id="S4.I2.ix5.p1" class="ltx_para">
<p id="S4.I2.ix5.p1.1" class="ltx_p"><span id="S4.I2.ix5.p1.1.1" class="ltx_text" style="font-size:80%;">The operator tries to explain the goal to the robot, not specifying what to do, but the expected final result.</span></p>
</div>
</li>
</ol>
</div>
<div id="S4.p12" class="ltx_para">
<p id="S4.p12.1" class="ltx_p">While no big changes are expected during the first four years in <span id="S4.p12.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP], we foresee the inclusion of more robust filters and the use of less constrained grammars.
However, by the third milestone (2022), operators might unintentionally neglect information, so robots will need to reconcile information as it arrives, making <span id="S4.p12.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">questions</span> as needed.
Furthermore, people normally make a very efficient use of language, so references are very common.
Therefore, we expect the exploration of <span id="S4.p12.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">reference resolution</span> such as ellipsis and anaphora by this year.
Finally, the latter milestones will keep the pace, slightly moving the focus to task planning while addressing new types of sentences.</p>
</div>
<div id="S4.p13" class="ltx_para ltx_noindent">
<p id="S4.p13.1" class="ltx_p"><span id="S4.p13.1.1" class="ltx_text ltx_font_bold">Roadmap phase 2: Towards dialog-based interaction</span></p>
</div>
<div id="S4.p14" class="ltx_para">
<p id="S4.p14.1" class="ltx_p">The referee gives to the operator a set of examples of what robots can do in a given domain. The operator must propose a similar task for the robot which has to be approved by the referee. The robot may not know how to accomplish the task, in which case the procedure is explained by the operator.</p>
</div>
<div id="S4.p15" class="ltx_para">
<p id="S4.p15.1" class="ltx_p">The intensity of overlapped noise ranges from medium to high, with sudden bursts of loud recorded human voices like in shopping malls, grocery stores, and airports are good examples.</p>
</div>
<div id="S4.p16" class="ltx_para ltx_noindent">
<p id="S4.p16.1" class="ltx_p"><span id="S4.p16.1.1" class="ltx_text ltx_font_bold">Yearly progress</span></p>
<ol id="S4.I3" class="ltx_enumerate">
<li id="S4.I3.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2032</span> 
<div id="S4.I3.ix1.p1" class="ltx_para">
<p id="S4.I3.ix1.p1.1" class="ltx_p"><span id="S4.I3.ix1.p1.1.1" class="ltx_text" style="font-size:80%;">After receiving a set of examples, the operator elicits a similar behavior to the robot.
The procedure can be explained in detail step by step.</span></p>
</div>
</li>
<li id="S4.I3.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2036</span> 
<div id="S4.I3.ix2.p1" class="ltx_para">
<p id="S4.I3.ix2.p1.1" class="ltx_p"><span id="S4.I3.ix2.p1.1.1" class="ltx_text" style="font-size:80%;">After receiving a topic and a set of examples, the operator requests something of the same difficulty.
Sub-goals are detailed to the robot, but not the individual commands.
The operator corrects the robot’s plan.</span></p>
</div>
</li>
</ol>
</div>
<div id="S4.p17" class="ltx_para">
<p id="S4.p17.1" class="ltx_p">In these phases, we continuously add new elements to <span id="S4.p17.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU], integrating it deeper with <span id="S4.p17.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">planning</span>, while, at the same time, we are collecting new applications from users (the potential market).
Plan-learning using natural language requires the integration of <span id="S4.p17.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">cardinality</span>, as well as spatial and temporal relationships
(e.g. the operator may request <span id="S4.p17.1.4" class="ltx_text ltx_font_italic">make me a sandwich</span>, explaining later on the steps like <span id="S4.p17.1.5" class="ltx_text ltx_font_italic">grab a slice of bread, spread mayo…</span>).</p>
</div>
<div id="S4.p18" class="ltx_para">
<p id="S4.p18.1" class="ltx_p">In contrast to only presenting an initiated plan to the operator, the tasks stimulate that the operator and the robot have to enter in a <span id="S4.p18.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">dialog</span> towards an unconstrained natural interaction.</p>
</div>
<div id="S4.p19" class="ltx_para ltx_noindent">
<p id="S4.p19.1" class="ltx_p"><span id="S4.p19.1.1" class="ltx_text ltx_font_bold">Roadmap Goal: Reaching unconstrained Interaction</span></p>
</div>
<div id="S4.p20" class="ltx_para">
<p id="S4.p20.1" class="ltx_p">In the last phase, the operators only get explained the State-of-the-Art, i.e. the limits of what the robots can achieve.
They can request anything crossing their minds within those limits the way they want. The robot may not know how to accomplish the task, in which case the procedure is freely explained by the operator in a dialog.</p>
</div>
<div id="S4.p21" class="ltx_para ltx_noindent">
<p id="S4.p21.1" class="ltx_p"><span id="S4.p21.1.1" class="ltx_text ltx_font_bold">Yearly progress</span></p>
<ol id="S4.I4" class="ltx_enumerate">
<li id="S4.I4.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2040</span> 
<div id="S4.I4.ix1.p1" class="ltx_para">
<p id="S4.I4.ix1.p1.1" class="ltx_p"><span id="S4.I4.ix1.p1.1.1" class="ltx_text" style="font-size:80%;">The operator explains to the robot how to perform an entirely new task and what the results should be.</span></p>
</div>
</li>
<li id="S4.I4.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2044</span> 
<div id="S4.I4.ix2.p1" class="ltx_para">
<p id="S4.I4.ix2.p1.1" class="ltx_p"><span id="S4.I4.ix2.p1.1.1" class="ltx_text" style="font-size:80%;">The operator requires anything from the robot within the state-of-the-art. All planning is left to the robot.</span></p>
</div>
</li>
</ol>
<br class="ltx_break">
<p id="S4.p21.2" class="ltx_p">We believe the presented roadmap helps the RoboCup@Home community to push the boundaries of research in <span id="S4.p21.2.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI].</p>
</div>
<div id="S4.p22" class="ltx_para">
<p id="S4.p22.1" class="ltx_p">The first steps will force teams to look for alternatives to grammar-based <span id="S4.p22.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] engines, or at least a less restricted ones.
Besides, the continuous analysis of audio signals to separate the operators’ voice from noise is also addressed, although dealing with noise can be left as an option for daring teams.</p>
</div>
<div id="S4.p23" class="ltx_para">
<p id="S4.p23.1" class="ltx_p">Regarding <span id="S4.p23.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLUfNatural-Language Understanding (NLU)fnNatural-Language Understanding (NLU)xNatural-Language UnderstandingxnNatural-Language Understanding[NLU], the first steps will take current approaches to the limit.
Unbiased operators will gradually expose robots to the richness of <span id="S4.p23.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">free speech</span>, while the pipeline will grant time to prepare.</p>
</div>
<div id="S4.p24" class="ltx_para">
<p id="S4.p24.1" class="ltx_p">Later on, moving from imperative sentences (e.g. <span id="S4.p24.1.1" class="ltx_text ltx_font_italic">clean the bedroom</span>) to goal-driven sentences of any kind (e.g. <span id="S4.p24.1.2" class="ltx_text ltx_font_italic">I need the room clean for tonight</span>, <span id="S4.p24.1.3" class="ltx_text ltx_font_italic">is the breakfast ready?</span>) will not only have the league working with semantics and pragmatics, but also might foster research in action planning.</p>
</div>
<div id="S4.p25" class="ltx_para">
<p id="S4.p25.1" class="ltx_p">Especially, in the second phase, more complex elements of language analysis are incorporated, paving the road towards a dialog-based interaction.
On that basis, we stipulate conversational robots in RoboCup@Home.
This claim is substantiated in the last phase by widening the domain and removing all constrains from the operators, in order to deal with real-world situations.
</p>
</div>
<div id="S4.p26" class="ltx_para">
<p id="S4.p26.1" class="ltx_p">Nevertheless the roadmap provides the necessary flexibility for the worst-case scenario where direct instructions can be given to the robots.</p>
</div>
</section>
<section id="S5" class="ltx_section" lang="en-US">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper we provided a historical overview of testing natural-language interactions with robots over the last nine years of RoboCup@Home.
We outlined the state-of-the-art <span id="S5.p1.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>xnASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] and <span id="S5.p1.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>xnNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP].
We quantified recent strategies and software solutions adopted by teams to overcome the trials set in the competition.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In these observations we identified a set of challenges that haven’t been tackled.
Critical components that hamper free dialogs with the robot are posed by the command generator along with the subconscious unintentional bias of the operators.
We inspected how individual test features prevent the league from dealing with free natural language.
Based on this study, we propose a strategy and a roadmap that formulate key features to implement the in the individual <span id="S5.p2.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP] components of the robot (see <a href="#S2.F1" title="In 2.2 Adopted Strategies and Software Solutions for \IfStrEqCasenHRIfHuman-Robot Interaction (HRI)fnHuman-Robot Interaction (HRI)xHuman-Robot InteractionxnHuman-Robot Interaction[HRI] ‣ 2 Speech Recognition and Natural Language Understanding in RoboCup@Home ‣ From Commands to Goal-based Dialogs: A Roadmap to Achieve Natural Language Interaction in RoboCup@Home" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>) to conquer those challenges.
Many test details strongly depend on the advancement of <span id="S5.p2.1.2" class="ltx_ERROR undefined">\IfStrEqCase</span>nASRfAutomatic Speech Recognition (ASR)fnAutomatic Speech Recognition (ASR)xAutomatic Speech RecognitionxnAutomatic Speech Recognition[ASR] engines, neglects non-verbal communication, so there should be an entangled roadmap for these features.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">We hope this roadmap will serve as initiative to promote long-term planning in RoboCup@Home.
In particular our contribution elaborates on stepstones to elicit thorough <span id="S5.p3.1.1" class="ltx_ERROR undefined">\IfStrEqCase</span>nNLPfNatural-Language Processing (NLP)fnNatural-Language Processing (NLP)xNatural-Language ProcessingxnNatural-Language Processing[NLP] to empower robots with strong natural-language skills.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en-US">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:70%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:70%;">
Bastianelli, E., Croce, D., Vanzo, A., Basili, R., Nardi, D.: A discriminative
approach to grounded spoken language understanding in interactive robotics.
In: IJCAI. pp. 2747–2753 (2016)
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:70%;">
Demura, K., Demura, K., Nagashima, K., Enomoto, K., Yamakawa, T., Iwasaki, R.,
Mashimo, S.: Happy mini 2017 team description paper. RoboCup @Home 2017 Team
Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:70%;">
Dominey, P.F.: Learning grammatical constructions from narrated video events
for human–robot interaction. In: Proceedings IEEE humanoid robotics
conference, Karlsruhe, Germany (2003)
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:70%;">
Doostdar, M., Schiffer, S., Lakemeyer, G.: A robust speech recognition system
for service-robotics applications. In: Iocchi, L., Matsubara, H.,
Weitzenfeld, A., Zhou, C. (eds.) RoboCup 2008: Robot Soccer World Cup XII.
pp. 1–12. Springer Berlin Heidelberg, Berlin, Heidelberg (2009)
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:70%;">
Gaisser, F., Aswin Chandarr, A., Rudinac, M., Bruinink, M., Pons, S., Rueda,
M.B., Lung, G.L., Wisse, M., Jonker, P.: Delft robotics robocup@home 2013
team description paper. Proceedings RoboCup competition (2013)
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:70%;">
Hart, J.W., Stone, P., Thomaz, A., Niekum, S.: Ut austin villa robocup@home
domestic standard platform league team description paper. RoboCup @Home 2017
Team Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:70%;">
Hori, S., Ishida, Y., Kiyama, Y., Tanaka, Y., Kuroda, Y., Hisano, M., Imamura,
Y., Himaki, T., Yoshimoto, Y., Aratani, Y., Hashimoto, K., Iwamoto, G.,
Morie, T., Tamukoh, H.: Hibikino-musashi@home 2017 team description paper.
RoboCup @Home 2017 Team Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:70%;">
Iocchi, L., Holz, D., Ruiz-del Solar, J., Sugiura, K., Van Der Zant, T.:
Robocup@home: Analysis and results of evolving competitions for domestic and
service robots. Artificial Intelligence </span><span id="bib.bib8.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">229</span><span id="bib.bib8.3.3" class="ltx_text" style="font-size:70%;">, 258–281 (2015)
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:70%;">
Lee, B.J., Choi, J.Y., Lee, C.Y., Park, K.W., Choi, S., Baek, C., Zhang, B.T.:
2017 aupair team description paper. RoboCup @Home 2017 Team Description
Papers (2017)
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:70%;">
Liu, J., Zhang, Z., Tang, B., Chen, X.: Wrighteagle@home 2017 team description
paper. RoboCup @Home 2017 Team Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:70%;">
Llarena, A., Boldt, J.F., Steinke, N.S., Engelmeyer, H., Rojas, R.:
Berlinunited@ home 2013 team description paper. Proceedings RoboCup
competition (2013)
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:70%;">
Martínez, L., Muñoz, R., Olave, G., Pais, G., Hernan, G., Gomez, D.,
Garrido, L., Campanini, D., Orellana, P., Loncomilla, P., Ruiz-del Solar, J.:
Uchile homebreakers 2017 team description paper. RoboCup @Home 2017 Team
Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:70%;">
M.T. Lázaro, Iocchi, L., Nardi, D., Hanheide, M., Fentanes, J.P.: Spqrel 2017
team description paper. RoboCup @Home 2017 Team Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:70%;">
Oishi, S., Miura, J., Koide, K., Demura, M., Kohari, Y., Une, S.,
Villamar-Gomez, L., Kato, T., Kojima, M., Morohashi, K.: Aisl-tut @home
league 2017 team description paper. RoboCup @Home 2017 Team Description
Papers (2017)
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:70%;">
Pineda, L., Meza, I., Fuentes, G., Rascón, C., Peña, M., Ortega, H.,
Reyes-Castillo, M., Salinas, L., Ortega, J., Rodríguez-García, A.,
et al.: The golem team, robocup@home 2013 (2013)
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:70%;">
Roy, D.K., Pentland, A.P.: Learning words from sights and sounds: A
computational model. Cognitive science </span><span id="bib.bib16.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">26</span><span id="bib.bib16.3.3" class="ltx_text" style="font-size:70%;">(1), 113–146 (2002)
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:70%;">
Savage, J., Negrete, M., Matamoros, M., Cruz, J., Contreras, L., Pacheco, A.,
Figueroa, I., Márquez, J.: Pumas @home 2013 team description paper (2013)
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:70%;">
Savage, J., Negrete, M., Cruz, J., Marquez, J., Martell, R., Cruz, J., Vazquez,
E., Pano, M., Cruz, J., Silva, E., Estrada, H., Arce, H., Matamoros, M.,
Garzon, A., Fuentes, O.: Pumas@home 2017 team description paper. RoboCup
@Home 2017 Team Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:70%;">
Schiffer, S., Niemüller, T., Doostdar, M., Lakemeyer, G.: Allemaniacs @home
2009 team description. Proceedings CD RoboCup (2009)
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:70%;">
Seib, V., Manthe, S., Memmesheimer, R., Polster, F., Paulus, D.: Team
homer@unikoblenz—approaches and contributions to the robocup@home
competition. In: Robot Soccer World Cup. pp. 83–94. Springer (2015)
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:70%;">
Stanford: Corenlp (2011), </span><a target="_blank" href="http://nlp.stanford.edu:8080/corenlp/" title="" class="ltx_ref ltx_url" style="font-size:70%;"><span id="bib.bib21.2.2.1" class="ltx_text" style="color:#0000FF;">http://nlp.stanford.edu:8080/corenlp/</span></a><span id="bib.bib21.3.3" class="ltx_text" style="font-size:70%;">
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:70%;">
Stückler, J., Dröschel, D., Gräve, K., Holz, D., Schreiber, M.,
Behnke, S.: Nimbro @home 2010 team description (2010)
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:70%;">
Wachsmuth, S., Lier, F., Meyer zu Borgsen, S., Kummert, J., Lach, L., Sixt, D.:
Tobi - team of bielefeld a human-robot interaction system for robocup@home
2017. RoboCup @Home 2017 Team Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:70%;">
Williams, M.A., Pfeiffer, S., Vitale, J., Tonkin, M., Ojha, S., Billingsley,
R., Alam, S., Kang, L., Gudi, S.L.K.C., Clark, J., Wang, X., Johnston, B.:
Uts unleashed! for robocup 2017 @home spl. RoboCup @Home 2017 Team
Description Papers (2017)
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:70%;">
Wisspeintner, T., Van Der Zant, T., Iocchi, L., Schiffer, S.: Robocup@home:
Scientific competition and benchmarking for domestic service robots.
Interaction Studies </span><span id="bib.bib25.2.2" class="ltx_text ltx_font_bold" style="font-size:70%;">10</span><span id="bib.bib25.3.3" class="ltx_text" style="font-size:70%;">(3), 392–426 (2009)
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:70%;">
Yaguchi, H., Tran, B., Takeda, M., Kochigami, K., Li, Z., Sasabuchi, K.,
Furuta, Y., Nagahama, K., Okada, K., Inaba, M.: Jsk@home: Team description
paper for robocup@home 2017. RoboCup @Home 2017 Team Description Papers
(2017)
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:70%;">
van der Zant, T., Wisspeintner, T.: Robocup@home: Creating and benchmarking
tomorrows service robot applications. In: Robotic Soccer. InTech (2007)
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1902.00753" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1902.00754" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1902.00754">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1902.00754" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1902.00755" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 20:41:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
