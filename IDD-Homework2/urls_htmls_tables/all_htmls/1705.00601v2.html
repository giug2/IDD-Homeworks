<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1705.00601] The Promise of Premise: Harnessing Question Premises in Visual Question Answering</title><meta property="og:description" content="In this paper, we make a simple observation that questions about images often contain premises – objects and relationships implied by the question – and that reasoning about premises can help Visual Question Answering …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Promise of Premise: Harnessing Question Premises in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Promise of Premise: Harnessing Question Premises in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1705.00601">

<!--Generated on Sun Mar  3 13:47:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\title</span>
<p id="p1.2" class="ltx_p">Premise arXiv (v2)











































<span id="p1.2.1" class="ltx_text" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">The Promise of Premise: Harnessing Question Premises
<br class="ltx_break">in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aroma Mahendru<math id="id1.1.m1.1" class="ltx_math_unparsed" alttext="\,\,{}^{,1}" display="inline"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1"><mi id="id1.1.m1.1.1a"></mi><mrow id="id1.1.m1.1.1.1"><mo id="id1.1.m1.1.1.1.1">,</mo><mn id="id1.1.m1.1.1.1.2">1</mn></mrow></msup><annotation encoding="application/x-tex" id="id1.1.m1.1b">\,\,{}^{,1}</annotation></semantics></math>  Viraj Prabhu<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">∗,1</span></sup>  Akrit Mohapatra<sup id="id9.9.id2" class="ltx_sup"><span id="id9.9.id2.1" class="ltx_text ltx_font_italic">∗,1</span></sup>  Dhruv Batra<sup id="id10.10.id3" class="ltx_sup"><span id="id10.10.id3.1" class="ltx_text ltx_font_italic">2</span></sup>  Stefan Lee<sup id="id11.11.id4" class="ltx_sup"><span id="id11.11.id4.1" class="ltx_text ltx_font_italic">1</span></sup>  
<br class="ltx_break"><sup id="id12.12.id5" class="ltx_sup">1</sup>Virginia Tech   <sup id="id13.13.id6" class="ltx_sup">2</sup>Georgia Institute of Technology
<br class="ltx_break"><span id="id14.14.id7" class="ltx_text ltx_font_typewriter">{maroma, virajp, akrit}@vt.edu</span>, <span id="id15.15.id8" class="ltx_text ltx_font_typewriter">dbatra@gatech.edu</span>, <span id="id16.16.id9" class="ltx_text ltx_font_typewriter">steflee@vt.edu</span>
</span><span class="ltx_author_notes">   Denotes equal contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p"><span id="id17.id1.1" class="ltx_text" lang="en">In this paper, we make a simple observation that questions about images often contain <em id="id17.id1.1.1" class="ltx_emph ltx_font_italic">premises</em> – objects and relationships implied by the question – and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions.</span></p>
<p id="id18.id2" class="ltx_p"><span id="id18.id2.1" class="ltx_text" lang="en">When presented with a question that is irrelevant to an image, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in nonsensical or even misleading answers. We note that a visual question is irrelevant to an image if at least one of its premises is false (<em id="id18.id2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="id18.id2.1.2" class="ltx_text"></span> not depicted in the image). We leverage this observation to construct a dataset for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not.</span></p>
<p id="id19.id3" class="ltx_p"><span id="id19.id3.1" class="ltx_text" lang="en">We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.</span></p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The task of providing natural language answers to free-form questions about an image – <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p1.1.2" class="ltx_text"></span> Visual Question Answering (VQA) –
has received substantial attention in the past few years <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib16" title="" class="ltx_ref">2014</a>); Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>); Malinowski et al. (<a href="#bib.bib17" title="" class="ltx_ref">2015</a>); Zitnick et al. (<a href="#bib.bib27" title="" class="ltx_ref">2016</a>); Kim et al. (<a href="#bib.bib10" title="" class="ltx_ref">2016</a>); Wu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2016</a>); Lu et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>); Andreas et al. (<a href="#bib.bib3" title="" class="ltx_ref">2016</a>); Lu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> and has quickly become a popular problem area. Despite significant progress on VQA benchmarks <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>,
current models still present a number of unintelligent and problematic tendencies.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1705.00601/assets/figures/teaser.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="360" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Questions asked about images often contain <em id="S1.F1.2.1" class="ltx_emph ltx_font_italic">‘premises’</em> that imply visual semantics. From the above question, we can infer that a relevant image must contain a man, a racket, and that the man must be holding the racket. We extract these premises from visually grounded questions and use them to construct a new dataset and models for question relevance prediction. We also find that augmenting standard VQA training with simple premise-based questions results in improvements on tasks requiring compositional reasoning.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">When faced with questions that are irrelevant or not applicable for an image, current ‘forced choice’ models will still produce an answer. For example, given an
image of a dog and a query “<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">What color is the bird?</span>”, standard VQA models might answer “<span id="S1.p2.1.2" class="ltx_text ltx_font_italic">Red</span>” confidently, based
solely on language biases in the training set (<em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.4" class="ltx_text"></span> an overabundance of the word “red”). In these cases, the predicted answers are senseless at best and misleading at worst,
with either case posing serious problems for real-world applications. Like <cite class="ltx_cite ltx_citemacro_citet">Ray et al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>, we argue that practical VQA systems must be able to identify and explain irrelevant questions. For instance, a more intelligent VQA model with this capability
might answer “<span id="S1.p2.1.5" class="ltx_text ltx_font_italic">There is no bird in the image</span>” for this example.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Premises.</h4>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">In this paper, we show that question <em id="S1.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">premises</em> - <em id="S1.SS0.SSS0.Px1.p1.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.SS0.SSS0.Px1.p1.1.3" class="ltx_text"></span> objects and relationships implied by a question - can enable VQA models to respond more intelligently to irrelevant or previously unseen questions. We develop a premise extraction pipeline based on SPICE <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite> and demonstrate how these premises can be used to improve modern VQA models in the face of irrelevant or previously unseen questions.</p>
</div>
<div id="S1.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p2.1" class="ltx_p">Concretely, we define premises as facts implied by the language of questions, for example the question
<span id="S1.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">“What brand of racket is the man holding?”</span> shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> implies the existence of a man, a racket, and that the man is holding the racket. For visually grounded questions (<em id="S1.SS0.SSS0.Px1.p2.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.SS0.SSS0.Px1.p2.1.3" class="ltx_text"></span> those asked about a particular image) these premises
imply visual qualities, including the presence of objects as well as their attributes and relationships.</p>
</div>
<div id="S1.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p3.1" class="ltx_p">Broadly speaking, we explore the usefulness of premises in two settings – when visual questions are known to be relevant to the images they are asked on (<em id="S1.SS0.SSS0.Px1.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.SS0.SSS0.Px1.p3.1.2" class="ltx_text"></span> in the VQA dataset) and in real-life situations where such an assumption cannot be made (<em id="S1.SS0.SSS0.Px1.p3.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.SS0.SSS0.Px1.p3.1.4" class="ltx_text"></span> when generated by visually impaired users). In the former case, we show that knowing that a question is relevant allows us to perform data augmentation by creating additional simple question-answer pairs using the premises of source questions.
In the latter case, we show that explicitly reasoning about premises provides an effective and interpretable way of determining whether a question is relevant to an image.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Irrelevant Question Detection.</h4>

<div id="S1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p1.1" class="ltx_p">We consider a question to be relevant to an image if all of the question’s premises apply to the corresponding image, that is to say all objects,
attributes, and interactions implied by the question are depicted in the image. We refer to premises that apply for a given image as true premises and those that do not apply as false premises. In order to train and evaluate models for this task, we curate a new irrelevant question detection dataset which we call the Question Relevance Prediction and Explanation (QRPE) dataset. QRPE is automatically curated from annotations already present in existing datasets, requiring no additional labeling.</p>
</div>
<div id="S1.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p2.1" class="ltx_p">We collect the QRPE dataset by taking each image-question pair in the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> and finding the most
visually similar other image for which exactly one of the question premises is false. In this way, we collect tuples consisting of two
images, a question, and a premise where the question is relevant for one image and not for the other due to the premise being false.</p>
</div>
<div id="S1.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p3.1" class="ltx_p">For context, the only other
existing irrelevant question detection dataset
<cite class="ltx_cite ltx_citemacro_cite">Ray et al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> collected irrelevant question-image pairs by human verification of random pairs. In comparison, QRPE is substantially larger, balanced between irrelevant and relevant examples, and presents a
considerably more difficult task due to the closeness of the image pairs both visually and with respect to question premises.</p>
</div>
<div id="S1.SS0.SSS0.Px2.p4" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p4.1" class="ltx_p">We train novel models for irrelevant question detection on the QRPE dataset and compare to existing methods. In these experiments,
we show that models that explicitly reason about question premises consistently outperform baseline models that do not.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA Data Augmentation.</h4>

<div id="S1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px3.p1.1" class="ltx_p">Finally, we also introduce an approach to generate simple, templated question-answer pairs about elementary concepts from premises of complex training questions. In initial experiments, we show that adding these simple question-answer pairs to VQA training data can improve performance
on tasks requiring compositional reasoning. These simple questions improve training by bringing implicit training concepts “to the surface”,
<em id="S1.SS0.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.SS0.SSS0.Px3.p1.1.2" class="ltx_text"></span> introducing direct supervision of important implicit concepts by transforming them to simple training pairs.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual Question Answering:</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Starting from simple bag-of-word and CNN+LSTM models
<cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>, VQA architectures have seen considerable innovation. Many top-performing models integrate
attention mechanisms (over the image, the question, or both) to focus on important structures
<cite class="ltx_cite ltx_citemacro_cite">Fukui et al. (<a href="#bib.bib6" title="" class="ltx_ref">2016</a>); Lu et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite>, and some have been designed with compositionality in mind <cite class="ltx_cite ltx_citemacro_cite">Andreas et al. (<a href="#bib.bib3" title="" class="ltx_ref">2016</a>); Hendricks et al. (<a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>. However, improving compositionality or performance through data augmentation remains a largely unstudied area.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">Some other recent work has developed models which produce natural language explanations for their outputs <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib19" title="" class="ltx_ref">2016</a>); Wang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2016</a>)</cite>, but there has not been work on generating explanations for irrelevant questions or false premises.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Question Relevance:</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Most related to our work is that of <cite class="ltx_cite ltx_citemacro_citet">Ray et al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>, which introduced the task of irrelevant question detection for VQA. To evaluate on this task, they created the Visual True and False Question (VTFQ) dataset by pairing VQA questions with random VQA images and having human annotators verify whether or not the question was relevant. As a result, many of the irrelevant image-question pairs exhibit a complete mismatch of image and question content. Our Question Relevance Prediction and Explanation (QRPE) dataset on the other hand is collected such that irrelevant images for each question closely resemble the source image both visually and semantically.
We also provide premise-level annotations which can be used to develop models that not only decide whether a question is relevant, but also provide explanations for <em id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">why</em> that is the case.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Semantic Tuple Extraction:</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Extracting structured facts in the form of semantic tuples from text is a well studied problem <cite class="ltx_cite ltx_citemacro_cite">Schuster et al. (<a href="#bib.bib22" title="" class="ltx_ref">2015</a>); Anderson et al. (<a href="#bib.bib2" title="" class="ltx_ref">2016</a>); Elhoseiny et al. (<a href="#bib.bib5" title="" class="ltx_ref">2016</a>)</cite>; however, recent work has begun extending these techniques to visual domains <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Johnson et al. (<a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite>. Additionally, the Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite> dataset contains dense image annotations for objects and their attributes and relationships. However, we are the first to consider these facts to reason about question relevancy and compositional reasoning in VQA.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Extracting Premises of a Question</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In Section <a href="#S1" title="1 Introduction ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we introduced the concept of premises and how they can be used. We now formalize this
concept and explain how premises can be extracted from questions.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We define question premises as
facts implied about an image from a question asked about it, which we represent as tuples. Returning to our
running example question <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">“What brand of racket is the man holding?”</span>, we can express these premises
as the tuples <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">‘&lt;man&gt;’</span>, <span id="S3.p2.1.3" class="ltx_text ltx_font_italic">‘&lt;racket&gt;’</span>, and <span id="S3.p2.1.4" class="ltx_text ltx_font_italic">‘&lt;man, holding, racket&gt;’</span> respectively. We categorize these tuples
into three groups based on their complexity. First-order premises representing the presence of objects
(<span id="S3.p2.1.5" class="ltx_text ltx_font_italic">‘&lt;man&gt;’, ‘&lt;cat&gt;’, ‘&lt;sky&gt;’</span>), second-order premises capturing the attributes of objects (<span id="S3.p2.1.6" class="ltx_text ltx_font_italic">‘&lt;man, tall&gt;’, ‘&lt;car, moving&gt;’</span>), and third-order premises containing interactions between objects (<em id="S3.p2.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p2.1.8" class="ltx_text"></span> <span id="S3.p2.1.9" class="ltx_text ltx_font_italic">‘&lt;man, kicking, ball&gt;’, ‘&lt;cat, above, car&gt;’</span>).</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Premise Extraction:</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">To extract premises from questions, we use the semantic tuple extraction pipeline used
in the SPICE metric <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite>. Originally defined as a metric for image captioning, SPICE transforms
a sentence into a scene graph using the Stanford Scene Graph Parser <cite class="ltx_cite ltx_citemacro_cite">Schuster et al. (<a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite> and then extracts
semantic tuples from this representation. Fig. <a href="#S3.F2" title="Figure 2 ‣ Premise Extraction: ‣ 3 Extracting Premises of a Question ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows this process for a sample question. The question
is represented as a graph of objects, attributes, and relationships from which first, second, and third order premises
are extracted respectively. As this pipeline was originally designed for descriptive captions rather than questions, we
found a number of minor modifications helpful in extracting quality question premises, including disabling pronoun
resolution, verb lemmatization and METEOR-based Synset matching. We will release our premise extraction code publicly to encourage reproducibility.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1705.00601/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.2.1" class="ltx_text ltx_font_bold">Premise Extraction Pipeline.</span> Objects (gray), attributes (green), and relations (blue) scene graph nodes are converted into 1st, 2nd, and 3rd order premises respectively.</figcaption>
</figure>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p">While this extraction process typically produces high quality premise tuples, there are some sources of noise which
must be filtered out. The SPICE process occasionally produces duplicate nodes or object nodes not linked to nouns in
the question, which we filter out. We also remove premises containing words like photo, image, <em id="S3.SS0.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">etc</em>.<span id="S3.SS0.SSS0.Px1.p2.1.2" class="ltx_text"></span> that refer to the
image rather than its content.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p3.1" class="ltx_p">A more nuanced source of erroneous premises comes from the ambiguity in existential questions, <em id="S3.SS0.SSS0.Px1.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS0.SSS0.Px1.p3.1.2" class="ltx_text"></span> those about the existence of certain image content. For example, while the question
<span id="S3.SS0.SSS0.Px1.p3.1.3" class="ltx_text ltx_font_italic">“Is the little girl moving?”</span> contains the premise ‘<span id="S3.SS0.SSS0.Px1.p3.1.4" class="ltx_text ltx_font_italic">&lt;girl, little&gt;</span>’, it is unclear without the answer whether ‘<span id="S3.SS0.SSS0.Px1.p3.1.5" class="ltx_text ltx_font_italic">&lt;girl, moving&gt;</span>’ is also
a premise. Similarly, for the question <span id="S3.SS0.SSS0.Px1.p3.1.6" class="ltx_text ltx_font_italic">“How many giraffes are in the image?”</span>, ‘<span id="S3.SS0.SSS0.Px1.p3.1.7" class="ltx_text ltx_font_italic">&lt;giraffe, many&gt;</span>’ cannot be considered a premise as there may be 0 giraffes in the image.
To avoid introducing false premises, we filter out existential and counting questions.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Question Relevance Prediction and Explanation (QRPE) Dataset</h2>

<figure id="S4.F3" class="ltx_figure"><img src="/html/1705.00601/assets/figures/fig3_new.png" id="S4.F3.g1" class="ltx_graphics ltx_img_landscape" width="628" height="232" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.12.1" class="ltx_text ltx_font_bold">Some Examples from QRPE Dataset.</span> For a given question <math id="S4.F3.6.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.F3.6.m1.1b"><mi id="S4.F3.6.m1.1.1" xref="S4.F3.6.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.F3.6.m1.1c"><ci id="S4.F3.6.m1.1.1.cmml" xref="S4.F3.6.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m1.1d">Q</annotation></semantics></math> and a relevant image <math id="S4.F3.7.m2.1" class="ltx_Math" alttext="I^{+}" display="inline"><semantics id="S4.F3.7.m2.1b"><msup id="S4.F3.7.m2.1.1" xref="S4.F3.7.m2.1.1.cmml"><mi id="S4.F3.7.m2.1.1.2" xref="S4.F3.7.m2.1.1.2.cmml">I</mi><mo id="S4.F3.7.m2.1.1.3" xref="S4.F3.7.m2.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S4.F3.7.m2.1c"><apply id="S4.F3.7.m2.1.1.cmml" xref="S4.F3.7.m2.1.1"><csymbol cd="ambiguous" id="S4.F3.7.m2.1.1.1.cmml" xref="S4.F3.7.m2.1.1">superscript</csymbol><ci id="S4.F3.7.m2.1.1.2.cmml" xref="S4.F3.7.m2.1.1.2">𝐼</ci><plus id="S4.F3.7.m2.1.1.3.cmml" xref="S4.F3.7.m2.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.7.m2.1d">I^{+}</annotation></semantics></math>, we find an irrelevant image <math id="S4.F3.8.m3.1" class="ltx_Math" alttext="I^{-}" display="inline"><semantics id="S4.F3.8.m3.1b"><msup id="S4.F3.8.m3.1.1" xref="S4.F3.8.m3.1.1.cmml"><mi id="S4.F3.8.m3.1.1.2" xref="S4.F3.8.m3.1.1.2.cmml">I</mi><mo id="S4.F3.8.m3.1.1.3" xref="S4.F3.8.m3.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S4.F3.8.m3.1c"><apply id="S4.F3.8.m3.1.1.cmml" xref="S4.F3.8.m3.1.1"><csymbol cd="ambiguous" id="S4.F3.8.m3.1.1.1.cmml" xref="S4.F3.8.m3.1.1">superscript</csymbol><ci id="S4.F3.8.m3.1.1.2.cmml" xref="S4.F3.8.m3.1.1.2">𝐼</ci><minus id="S4.F3.8.m3.1.1.3.cmml" xref="S4.F3.8.m3.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.8.m3.1d">I^{-}</annotation></semantics></math> for which exactly one premise <math id="S4.F3.9.m4.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.F3.9.m4.1b"><mi id="S4.F3.9.m4.1.1" xref="S4.F3.9.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.F3.9.m4.1c"><ci id="S4.F3.9.m4.1.1.cmml" xref="S4.F3.9.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.9.m4.1d">P</annotation></semantics></math> of the question is false. If there are multiple such candidates, we select the candidate most visually most similar to <math id="S4.F3.10.m5.1" class="ltx_Math" alttext="I^{+}" display="inline"><semantics id="S4.F3.10.m5.1b"><msup id="S4.F3.10.m5.1.1" xref="S4.F3.10.m5.1.1.cmml"><mi id="S4.F3.10.m5.1.1.2" xref="S4.F3.10.m5.1.1.2.cmml">I</mi><mo id="S4.F3.10.m5.1.1.3" xref="S4.F3.10.m5.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S4.F3.10.m5.1c"><apply id="S4.F3.10.m5.1.1.cmml" xref="S4.F3.10.m5.1.1"><csymbol cd="ambiguous" id="S4.F3.10.m5.1.1.1.cmml" xref="S4.F3.10.m5.1.1">superscript</csymbol><ci id="S4.F3.10.m5.1.1.2.cmml" xref="S4.F3.10.m5.1.1.2">𝐼</ci><plus id="S4.F3.10.m5.1.1.3.cmml" xref="S4.F3.10.m5.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.10.m5.1d">I^{+}</annotation></semantics></math>. As can be seen from these examples, the QRPE dataset is very challenging, with only minor visual and semantic differences separating the relevant and irrelevant images.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">As discussed in Section <a href="#S1" title="1 Introduction ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, modern VQA models fail to differentiate between relevant and irrelevant questions,
answering either with confidence. This behavior is detrimental to the real world application of VQA systems.
In this section, we curate a new dataset for question relevance in VQA which we call the Question Relevance
Prediction and Explanation (QRPE) dataset. We plan to release QRPE publicly to help future efforts.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.7" class="ltx_p">In order to train and evaluate models for irrelevant question detection, we would like to create a dataset of tuples
<math id="S4.p2.1.m1.4" class="ltx_Math" alttext="(I^{+},Q,P,I^{-})" display="inline"><semantics id="S4.p2.1.m1.4a"><mrow id="S4.p2.1.m1.4.4.2" xref="S4.p2.1.m1.4.4.3.cmml"><mo stretchy="false" id="S4.p2.1.m1.4.4.2.3" xref="S4.p2.1.m1.4.4.3.cmml">(</mo><msup id="S4.p2.1.m1.3.3.1.1" xref="S4.p2.1.m1.3.3.1.1.cmml"><mi id="S4.p2.1.m1.3.3.1.1.2" xref="S4.p2.1.m1.3.3.1.1.2.cmml">I</mi><mo id="S4.p2.1.m1.3.3.1.1.3" xref="S4.p2.1.m1.3.3.1.1.3.cmml">+</mo></msup><mo id="S4.p2.1.m1.4.4.2.4" xref="S4.p2.1.m1.4.4.3.cmml">,</mo><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">Q</mi><mo id="S4.p2.1.m1.4.4.2.5" xref="S4.p2.1.m1.4.4.3.cmml">,</mo><mi id="S4.p2.1.m1.2.2" xref="S4.p2.1.m1.2.2.cmml">P</mi><mo id="S4.p2.1.m1.4.4.2.6" xref="S4.p2.1.m1.4.4.3.cmml">,</mo><msup id="S4.p2.1.m1.4.4.2.2" xref="S4.p2.1.m1.4.4.2.2.cmml"><mi id="S4.p2.1.m1.4.4.2.2.2" xref="S4.p2.1.m1.4.4.2.2.2.cmml">I</mi><mo id="S4.p2.1.m1.4.4.2.2.3" xref="S4.p2.1.m1.4.4.2.2.3.cmml">−</mo></msup><mo stretchy="false" id="S4.p2.1.m1.4.4.2.7" xref="S4.p2.1.m1.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.4b"><vector id="S4.p2.1.m1.4.4.3.cmml" xref="S4.p2.1.m1.4.4.2"><apply id="S4.p2.1.m1.3.3.1.1.cmml" xref="S4.p2.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.3.3.1.1.1.cmml" xref="S4.p2.1.m1.3.3.1.1">superscript</csymbol><ci id="S4.p2.1.m1.3.3.1.1.2.cmml" xref="S4.p2.1.m1.3.3.1.1.2">𝐼</ci><plus id="S4.p2.1.m1.3.3.1.1.3.cmml" xref="S4.p2.1.m1.3.3.1.1.3"></plus></apply><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑄</ci><ci id="S4.p2.1.m1.2.2.cmml" xref="S4.p2.1.m1.2.2">𝑃</ci><apply id="S4.p2.1.m1.4.4.2.2.cmml" xref="S4.p2.1.m1.4.4.2.2"><csymbol cd="ambiguous" id="S4.p2.1.m1.4.4.2.2.1.cmml" xref="S4.p2.1.m1.4.4.2.2">superscript</csymbol><ci id="S4.p2.1.m1.4.4.2.2.2.cmml" xref="S4.p2.1.m1.4.4.2.2.2">𝐼</ci><minus id="S4.p2.1.m1.4.4.2.2.3.cmml" xref="S4.p2.1.m1.4.4.2.2.3"></minus></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.4c">(I^{+},Q,P,I^{-})</annotation></semantics></math> comprised of a natural language question <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">Q</annotation></semantics></math>, an image <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="I^{+}" display="inline"><semantics id="S4.p2.3.m3.1a"><msup id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">I</mi><mo id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">superscript</csymbol><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">𝐼</ci><plus id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">I^{+}</annotation></semantics></math> for which <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p2.4.m4.1a"><mi id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><ci id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">Q</annotation></semantics></math> is relevant, and an image
<math id="S4.p2.5.m5.1" class="ltx_Math" alttext="I^{-}" display="inline"><semantics id="S4.p2.5.m5.1a"><msup id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><mi id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml">I</mi><mo id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1">superscript</csymbol><ci id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2">𝐼</ci><minus id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">I^{-}</annotation></semantics></math> for which <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p2.6.m6.1a"><mi id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><ci id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">Q</annotation></semantics></math> is irrelevant because premise <math id="S4.p2.7.m7.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.p2.7.m7.1a"><mi id="S4.p2.7.m7.1.1" xref="S4.p2.7.m7.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.1b"><ci id="S4.p2.7.m7.1.1.cmml" xref="S4.p2.7.m7.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.1c">P</annotation></semantics></math> is false. While it is not required to collect both a relevant and irrelevant image for each
question, we argue that doing so is a simple way to balance the dataset and it ensures that biases against rarer questions
(which would be irrelevant for most images) cannot be exploited to inflate performance.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.6" class="ltx_p">We base our dataset on the existing VQA corpus <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>, taking the human-generated (and therefore relevant)
image-question pairs from VQA as <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="I^{+}" display="inline"><semantics id="S4.p3.1.m1.1a"><msup id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">I</mi><mo id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1">superscript</csymbol><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">𝐼</ci><plus id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">I^{+}</annotation></semantics></math> and <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">Q</annotation></semantics></math>. As previously discussed, we can define the relevancy of a question in terms
of the validity of its premises for an image, so we extract premises from each question <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p3.3.m3.1a"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">Q</annotation></semantics></math> and must find a suitable irrelevant image <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="I^{-}" display="inline"><semantics id="S4.p3.4.m4.1a"><msup id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml"><mi id="S4.p3.4.m4.1.1.2" xref="S4.p3.4.m4.1.1.2.cmml">I</mi><mo id="S4.p3.4.m4.1.1.3" xref="S4.p3.4.m4.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><apply id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p3.4.m4.1.1.1.cmml" xref="S4.p3.4.m4.1.1">superscript</csymbol><ci id="S4.p3.4.m4.1.1.2.cmml" xref="S4.p3.4.m4.1.1.2">𝐼</ci><minus id="S4.p3.4.m4.1.1.3.cmml" xref="S4.p3.4.m4.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">I^{-}</annotation></semantics></math>.
However, there are certainly many images for which one or more of <math id="S4.p3.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p3.5.m5.1a"><mi id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><ci id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">Q</annotation></semantics></math>’s premises are false and an important design decision is
then how to select <math id="S4.p3.6.m6.1" class="ltx_Math" alttext="I^{-}" display="inline"><semantics id="S4.p3.6.m6.1a"><msup id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml"><mi id="S4.p3.6.m6.1.1.2" xref="S4.p3.6.m6.1.1.2.cmml">I</mi><mo id="S4.p3.6.m6.1.1.3" xref="S4.p3.6.m6.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.1b"><apply id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p3.6.m6.1.1.1.cmml" xref="S4.p3.6.m6.1.1">superscript</csymbol><ci id="S4.p3.6.m6.1.1.2.cmml" xref="S4.p3.6.m6.1.1.2">𝐼</ci><minus id="S4.p3.6.m6.1.1.3.cmml" xref="S4.p3.6.m6.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.1c">I^{-}</annotation></semantics></math> from this set.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.2" class="ltx_p">To ensure our dataset is as realistic and challenging as possible, we consider
irrelevant images which only have a single false question premise under <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p4.1.m1.1a"><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">Q</annotation></semantics></math> which we denote <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.p4.2.m2.1a"><mi id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><ci id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">P</annotation></semantics></math>. For example, the question
<span id="S4.p4.2.1" class="ltx_text ltx_font_italic">“Is the big red dog old?”</span> could be matched with an image containing a big, white dog or a small red dog, but not
a small white dog. In this way, we ensure that image content is semantically appropriate for the question topic but
not quite relevant. Additionally, this provides each irrelevant image with an explanation for why the question does not apply.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Furthermore, we sort this subset of irrelevant image by their visual distance to the source image <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="I^{+}" display="inline"><semantics id="S4.p5.1.m1.1a"><msup id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mi id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">I</mi><mo id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1">superscript</csymbol><ci id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">𝐼</ci><plus id="S4.p5.1.m1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">I^{+}</annotation></semantics></math> based on image encodings from a VGGNet <cite class="ltx_cite ltx_citemacro_cite">Simonyan and Zisserman (<a href="#bib.bib23" title="" class="ltx_ref">2014</a>)</cite> pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">Russakovsky et al. (<a href="#bib.bib21" title="" class="ltx_ref">2012</a>)</cite>. This ensures that
the relevant and irrelevant images are visually similar and act as difficult examples.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">A major difficulty with our proposed data collection process is how to verify whether a premise if true or false for any
given image in order to identify irrelevant images. We detail dataset construction and our approach for this problem in the following section.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset Construction</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/1705.00601/assets/figures/comparison2.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="249" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
<span id="S4.F4.2.1" class="ltx_text ltx_font_bold">A comparison of the QRPE and VTFQ Datasets.</span> On the left, we plot the Euclidean distance between VGGNet-fc7 features extracted from each relevant-irrelevant image pair for each dataset. Note that VTFQ has significantly higher visual distances. On the right, we show some qualitative examples of irrelevant images for questions that occur in both datasets. VTFQ images are significantly less related to the source image and question than in our dataset.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We curate our QRPE dataset automatically from existing annotations in COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib12" title="" class="ltx_ref">2014</a>)</cite> and Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>.
COCO is a set of over 300,000 images annotated with object segmentations and presence information for 80 classes as well as text descriptions of image content. Visual Genome builds on this dataset, providing more detailed object, attribute, and relationship annotations for over 100,000 COCO images. We make use of these data sources to extract first and second order premises from VQA questions which are also based on COCO images.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For first order premises (<em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p2.1.2" class="ltx_text"></span> existential premises), we consider only the 80 classes present in COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib12" title="" class="ltx_ref">2014</a>)</cite>. As VQA
and COCO share the same images, we can easily determine if a first order premise is true or false for a candidate irrelevant
image simply by checking for the absence of the appropriate class annotation.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">For second order premises (<em id="S4.SS1.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p3.1.2" class="ltx_text"></span> attributed objects), we rely on Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite> annotations for object
and attribute labels. Unlike in COCO, the lack of a particular object label in an image for Visual Genome does not necessarily
indicate that the object is not present, both due to annotation noise and the use of multiple synonyms for objects by human labelers.
As a consequence, we restrict the set of candidate irrelevant images to those which contain a matching object to the question premise
but a different attribute. Without further restriction, the selected irrelevant attributes do not tend to be mutually exclusive
with the source attribute (<em id="S4.SS1.p3.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p3.1.4" class="ltx_text"></span> matching <span id="S4.SS1.p3.1.5" class="ltx_text ltx_font_italic">‘&lt;dog, old&gt;’</span> and <span id="S4.SS1.p3.1.6" class="ltx_text ltx_font_italic">‘&lt;dog, red&gt;’</span>). To correct this and ensure a false premise, we further restrict
the set to attributes which are antonyms (<em id="S4.SS1.p3.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS1.p3.1.8" class="ltx_text"></span> <span id="S4.SS1.p3.1.9" class="ltx_text ltx_font_italic">‘&lt;young&gt;’</span> for source attribute <span id="S4.SS1.p3.1.10" class="ltx_text ltx_font_italic">‘&lt;old&gt;’)</span> or taxonomic sister terms (<em id="S4.SS1.p3.1.11" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS1.p3.1.12" class="ltx_text"></span> <span id="S4.SS1.p3.1.13" class="ltx_text ltx_font_italic">‘&lt;green&gt;’</span> for source attribute <span id="S4.SS1.p3.1.14" class="ltx_text ltx_font_italic">‘&lt;red&gt;’</span>) of the original premise attribute. We also experimented with third order
premises; however, the lack of a corresponding sense of mutual exclusion for verbs and the sparsity of &lt;object, relationship, object&gt; premises made finding non-trivial irrelevant images difficult.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">To recap, our data collection approach is to take each image-question pair in the VQA dataset and extract its first and second order
question premises. For each premise, we find all images which lack only this premise and rank them by their visual distance. The
closest of these is kept as the irrelevant image for each image-question pair.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Exploring the Dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Fig. <a href="#S4.F3" title="Figure 3 ‣ 4 Question Relevance Prediction and Explanation (QRPE) Dataset ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows sample <math id="S4.SS2.p1.1.m1.4" class="ltx_Math" alttext="(I^{+},Q,P,I^{-})" display="inline"><semantics id="S4.SS2.p1.1.m1.4a"><mrow id="S4.SS2.p1.1.m1.4.4.2" xref="S4.SS2.p1.1.m1.4.4.3.cmml"><mo stretchy="false" id="S4.SS2.p1.1.m1.4.4.2.3" xref="S4.SS2.p1.1.m1.4.4.3.cmml">(</mo><msup id="S4.SS2.p1.1.m1.3.3.1.1" xref="S4.SS2.p1.1.m1.3.3.1.1.cmml"><mi id="S4.SS2.p1.1.m1.3.3.1.1.2" xref="S4.SS2.p1.1.m1.3.3.1.1.2.cmml">I</mi><mo id="S4.SS2.p1.1.m1.3.3.1.1.3" xref="S4.SS2.p1.1.m1.3.3.1.1.3.cmml">+</mo></msup><mo id="S4.SS2.p1.1.m1.4.4.2.4" xref="S4.SS2.p1.1.m1.4.4.3.cmml">,</mo><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">Q</mi><mo id="S4.SS2.p1.1.m1.4.4.2.5" xref="S4.SS2.p1.1.m1.4.4.3.cmml">,</mo><mi id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">P</mi><mo id="S4.SS2.p1.1.m1.4.4.2.6" xref="S4.SS2.p1.1.m1.4.4.3.cmml">,</mo><msup id="S4.SS2.p1.1.m1.4.4.2.2" xref="S4.SS2.p1.1.m1.4.4.2.2.cmml"><mi id="S4.SS2.p1.1.m1.4.4.2.2.2" xref="S4.SS2.p1.1.m1.4.4.2.2.2.cmml">I</mi><mo id="S4.SS2.p1.1.m1.4.4.2.2.3" xref="S4.SS2.p1.1.m1.4.4.2.2.3.cmml">−</mo></msup><mo stretchy="false" id="S4.SS2.p1.1.m1.4.4.2.7" xref="S4.SS2.p1.1.m1.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.4b"><vector id="S4.SS2.p1.1.m1.4.4.3.cmml" xref="S4.SS2.p1.1.m1.4.4.2"><apply id="S4.SS2.p1.1.m1.3.3.1.1.cmml" xref="S4.SS2.p1.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.3.3.1.1.1.cmml" xref="S4.SS2.p1.1.m1.3.3.1.1">superscript</csymbol><ci id="S4.SS2.p1.1.m1.3.3.1.1.2.cmml" xref="S4.SS2.p1.1.m1.3.3.1.1.2">𝐼</ci><plus id="S4.SS2.p1.1.m1.3.3.1.1.3.cmml" xref="S4.SS2.p1.1.m1.3.3.1.1.3"></plus></apply><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑄</ci><ci id="S4.SS2.p1.1.m1.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2">𝑃</ci><apply id="S4.SS2.p1.1.m1.4.4.2.2.cmml" xref="S4.SS2.p1.1.m1.4.4.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.4.4.2.2.1.cmml" xref="S4.SS2.p1.1.m1.4.4.2.2">superscript</csymbol><ci id="S4.SS2.p1.1.m1.4.4.2.2.2.cmml" xref="S4.SS2.p1.1.m1.4.4.2.2.2">𝐼</ci><minus id="S4.SS2.p1.1.m1.4.4.2.2.3.cmml" xref="S4.SS2.p1.1.m1.4.4.2.2.3"></minus></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.4c">(I^{+},Q,P,I^{-})</annotation></semantics></math> tuples from our dataset. These examples illustrate the difficulty of our dataset. For instance, the images in the second column
differ only by the presence of the water bottle and images in the fourth column are differentiated by the color of the devices. Both of these are fine details of the image content.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The QRPE dataset contains 53,911 <math id="S4.SS2.p2.1.m1.4" class="ltx_Math" alttext="(I^{+},Q,P,I^{-})" display="inline"><semantics id="S4.SS2.p2.1.m1.4a"><mrow id="S4.SS2.p2.1.m1.4.4.2" xref="S4.SS2.p2.1.m1.4.4.3.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.4.4.2.3" xref="S4.SS2.p2.1.m1.4.4.3.cmml">(</mo><msup id="S4.SS2.p2.1.m1.3.3.1.1" xref="S4.SS2.p2.1.m1.3.3.1.1.cmml"><mi id="S4.SS2.p2.1.m1.3.3.1.1.2" xref="S4.SS2.p2.1.m1.3.3.1.1.2.cmml">I</mi><mo id="S4.SS2.p2.1.m1.3.3.1.1.3" xref="S4.SS2.p2.1.m1.3.3.1.1.3.cmml">+</mo></msup><mo id="S4.SS2.p2.1.m1.4.4.2.4" xref="S4.SS2.p2.1.m1.4.4.3.cmml">,</mo><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">Q</mi><mo id="S4.SS2.p2.1.m1.4.4.2.5" xref="S4.SS2.p2.1.m1.4.4.3.cmml">,</mo><mi id="S4.SS2.p2.1.m1.2.2" xref="S4.SS2.p2.1.m1.2.2.cmml">P</mi><mo id="S4.SS2.p2.1.m1.4.4.2.6" xref="S4.SS2.p2.1.m1.4.4.3.cmml">,</mo><msup id="S4.SS2.p2.1.m1.4.4.2.2" xref="S4.SS2.p2.1.m1.4.4.2.2.cmml"><mi id="S4.SS2.p2.1.m1.4.4.2.2.2" xref="S4.SS2.p2.1.m1.4.4.2.2.2.cmml">I</mi><mo id="S4.SS2.p2.1.m1.4.4.2.2.3" xref="S4.SS2.p2.1.m1.4.4.2.2.3.cmml">−</mo></msup><mo stretchy="false" id="S4.SS2.p2.1.m1.4.4.2.7" xref="S4.SS2.p2.1.m1.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.4b"><vector id="S4.SS2.p2.1.m1.4.4.3.cmml" xref="S4.SS2.p2.1.m1.4.4.2"><apply id="S4.SS2.p2.1.m1.3.3.1.1.cmml" xref="S4.SS2.p2.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.3.3.1.1.1.cmml" xref="S4.SS2.p2.1.m1.3.3.1.1">superscript</csymbol><ci id="S4.SS2.p2.1.m1.3.3.1.1.2.cmml" xref="S4.SS2.p2.1.m1.3.3.1.1.2">𝐼</ci><plus id="S4.SS2.p2.1.m1.3.3.1.1.3.cmml" xref="S4.SS2.p2.1.m1.3.3.1.1.3"></plus></apply><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑄</ci><ci id="S4.SS2.p2.1.m1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2">𝑃</ci><apply id="S4.SS2.p2.1.m1.4.4.2.2.cmml" xref="S4.SS2.p2.1.m1.4.4.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.4.4.2.2.1.cmml" xref="S4.SS2.p2.1.m1.4.4.2.2">superscript</csymbol><ci id="S4.SS2.p2.1.m1.4.4.2.2.2.cmml" xref="S4.SS2.p2.1.m1.4.4.2.2.2">𝐼</ci><minus id="S4.SS2.p2.1.m1.4.4.2.2.3.cmml" xref="S4.SS2.p2.1.m1.4.4.2.2.3"></minus></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.4c">(I^{+},Q,P,I^{-})</annotation></semantics></math> tuples generated from as many premises. In total, it contains 1530 unique premises and 28,853 unique questions. Among the 53,911 premises, 3876 are second-order, attributed object premises while the remaining 50,035 are first-order object/scene premises. We divide our dataset into two parts – a training set with 35,486 tuples that are generated from the VQA training set and a validation set with 18,425 tuples generated from the VQA validation set.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Manual Validation.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.4" class="ltx_p">We also manually validated 1000 randomly selected <math id="S4.SS2.SSS0.Px1.p1.1.m1.4" class="ltx_Math" alttext="(I^{+},Q,P,I^{-})" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.4a"><mrow id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml"><mo stretchy="false" id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">(</mo><msup id="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.2.cmml">I</mi><mo id="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.3.cmml">+</mo></msup><mo id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.4" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">Q</mi><mo id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.5" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px1.p1.1.m1.2.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">P</mi><mo id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.6" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">,</mo><msup id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.cmml"><mi id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.2.cmml">I</mi><mo id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.3.cmml">−</mo></msup><mo stretchy="false" id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.7" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.4b"><vector id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2"><apply id="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.2">𝐼</ci><plus id="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.3.3.1.1.3"></plus></apply><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1">𝑄</ci><ci id="S4.SS2.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.2.2">𝑃</ci><apply id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.2">𝐼</ci><minus id="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.4.4.2.2.3"></minus></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.4c">(I^{+},Q,P,I^{-})</annotation></semantics></math> tuples from our dataset. We noted that 99.10% of the premises <math id="S4.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.2.m2.1a"><mi id="S4.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.2.m2.1b"><ci id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.2.m2.1c">P</annotation></semantics></math> were valid (<em id="S4.SS2.SSS0.Px1.p1.4.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS2.SSS0.Px1.p1.4.2" class="ltx_text"></span> implied by the question) in <math id="S4.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="I^{+}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.3.m3.1a"><msup id="S4.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">I</mi><mo id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝐼</ci><plus id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.3.m3.1c">I^{+}</annotation></semantics></math> and 97.3% were false for the negative image <math id="S4.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="I^{-}" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.4.m4.1a"><msup id="S4.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml">I</mi><mo id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1">superscript</csymbol><ci id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.2">𝐼</ci><minus id="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.4.m4.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.4.m4.1c">I^{-}</annotation></semantics></math>. This demonstrates the high reliability of our automated annotation pipeline.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison to VTFQ</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We contrast our approach to the VTFQ dataset of <cite class="ltx_cite ltx_citemacro_citet">Ray et al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>. As discussed prior, VTFQ was collected by selecting a random question
and image from the VQA set and asking human annotators to report if the question was relevant, producing a pair. This approach results in irrelevant
image-question pairs that are unambiguously unrelated, with the visual content of the image having nothing at all to do with
the question or its source image from VQA.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.6" class="ltx_p"><span id="S4.SS3.p2.6.6" class="ltx_text" style="color:#000000;">To quantify this effect and compare to QRPE, we pair each irrelevant image-question pair <math id="S4.SS3.p2.1.1.m1.2" class="ltx_Math" alttext="(I^{-},Q)" display="inline"><semantics id="S4.SS3.p2.1.1.m1.2a"><mrow id="S4.SS3.p2.1.1.m1.2.2.1" xref="S4.SS3.p2.1.1.m1.2.2.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S4.SS3.p2.1.1.m1.2.2.1.2" xref="S4.SS3.p2.1.1.m1.2.2.2.cmml">(</mo><msup id="S4.SS3.p2.1.1.m1.2.2.1.1" xref="S4.SS3.p2.1.1.m1.2.2.1.1.cmml"><mi mathcolor="#000000" id="S4.SS3.p2.1.1.m1.2.2.1.1.2" xref="S4.SS3.p2.1.1.m1.2.2.1.1.2.cmml">I</mi><mo mathcolor="#000000" id="S4.SS3.p2.1.1.m1.2.2.1.1.3" xref="S4.SS3.p2.1.1.m1.2.2.1.1.3.cmml">−</mo></msup><mo mathcolor="#000000" id="S4.SS3.p2.1.1.m1.2.2.1.3" xref="S4.SS3.p2.1.1.m1.2.2.2.cmml">,</mo><mi mathcolor="#000000" id="S4.SS3.p2.1.1.m1.1.1" xref="S4.SS3.p2.1.1.m1.1.1.cmml">Q</mi><mo mathcolor="#000000" stretchy="false" id="S4.SS3.p2.1.1.m1.2.2.1.4" xref="S4.SS3.p2.1.1.m1.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.1.m1.2b"><interval closure="open" id="S4.SS3.p2.1.1.m1.2.2.2.cmml" xref="S4.SS3.p2.1.1.m1.2.2.1"><apply id="S4.SS3.p2.1.1.m1.2.2.1.1.cmml" xref="S4.SS3.p2.1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.1.m1.2.2.1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.2.2.1.1">superscript</csymbol><ci id="S4.SS3.p2.1.1.m1.2.2.1.1.2.cmml" xref="S4.SS3.p2.1.1.m1.2.2.1.1.2">𝐼</ci><minus id="S4.SS3.p2.1.1.m1.2.2.1.1.3.cmml" xref="S4.SS3.p2.1.1.m1.2.2.1.1.3"></minus></apply><ci id="S4.SS3.p2.1.1.m1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1">𝑄</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.1.m1.2c">(I^{-},Q)</annotation></semantics></math> from VTFQ with a relevant image from the VQA dataset. Specifically, we find the nearest neighbor question <math id="S4.SS3.p2.2.2.m2.1" class="ltx_Math" alttext="Q^{nn}" display="inline"><semantics id="S4.SS3.p2.2.2.m2.1a"><msup id="S4.SS3.p2.2.2.m2.1.1" xref="S4.SS3.p2.2.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S4.SS3.p2.2.2.m2.1.1.2" xref="S4.SS3.p2.2.2.m2.1.1.2.cmml">Q</mi><mrow id="S4.SS3.p2.2.2.m2.1.1.3" xref="S4.SS3.p2.2.2.m2.1.1.3.cmml"><mi mathcolor="#000000" id="S4.SS3.p2.2.2.m2.1.1.3.2" xref="S4.SS3.p2.2.2.m2.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.2.2.m2.1.1.3.1" xref="S4.SS3.p2.2.2.m2.1.1.3.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS3.p2.2.2.m2.1.1.3.3" xref="S4.SS3.p2.2.2.m2.1.1.3.3.cmml">n</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.2.m2.1b"><apply id="S4.SS3.p2.2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.2.m2.1.1">superscript</csymbol><ci id="S4.SS3.p2.2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.2.m2.1.1.2">𝑄</ci><apply id="S4.SS3.p2.2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.2.m2.1.1.3"><times id="S4.SS3.p2.2.2.m2.1.1.3.1.cmml" xref="S4.SS3.p2.2.2.m2.1.1.3.1"></times><ci id="S4.SS3.p2.2.2.m2.1.1.3.2.cmml" xref="S4.SS3.p2.2.2.m2.1.1.3.2">𝑛</ci><ci id="S4.SS3.p2.2.2.m2.1.1.3.3.cmml" xref="S4.SS3.p2.2.2.m2.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.2.m2.1c">Q^{nn}</annotation></semantics></math> in the VQA dataset to <math id="S4.SS3.p2.3.3.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.SS3.p2.3.3.m3.1a"><mi mathcolor="#000000" id="S4.SS3.p2.3.3.m3.1.1" xref="S4.SS3.p2.3.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.3.m3.1b"><ci id="S4.SS3.p2.3.3.m3.1.1.cmml" xref="S4.SS3.p2.3.3.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.3.m3.1c">Q</annotation></semantics></math> based on an average of the word2vec <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a href="#bib.bib18" title="" class="ltx_ref">2013</a>)</cite> embedding of each word, and select the image on which <math id="S4.SS3.p2.4.4.m4.1" class="ltx_Math" alttext="Q^{nn}" display="inline"><semantics id="S4.SS3.p2.4.4.m4.1a"><msup id="S4.SS3.p2.4.4.m4.1.1" xref="S4.SS3.p2.4.4.m4.1.1.cmml"><mi mathcolor="#000000" id="S4.SS3.p2.4.4.m4.1.1.2" xref="S4.SS3.p2.4.4.m4.1.1.2.cmml">Q</mi><mrow id="S4.SS3.p2.4.4.m4.1.1.3" xref="S4.SS3.p2.4.4.m4.1.1.3.cmml"><mi mathcolor="#000000" id="S4.SS3.p2.4.4.m4.1.1.3.2" xref="S4.SS3.p2.4.4.m4.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.4.4.m4.1.1.3.1" xref="S4.SS3.p2.4.4.m4.1.1.3.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS3.p2.4.4.m4.1.1.3.3" xref="S4.SS3.p2.4.4.m4.1.1.3.3.cmml">n</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.4.m4.1b"><apply id="S4.SS3.p2.4.4.m4.1.1.cmml" xref="S4.SS3.p2.4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.4.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.4.m4.1.1">superscript</csymbol><ci id="S4.SS3.p2.4.4.m4.1.1.2.cmml" xref="S4.SS3.p2.4.4.m4.1.1.2">𝑄</ci><apply id="S4.SS3.p2.4.4.m4.1.1.3.cmml" xref="S4.SS3.p2.4.4.m4.1.1.3"><times id="S4.SS3.p2.4.4.m4.1.1.3.1.cmml" xref="S4.SS3.p2.4.4.m4.1.1.3.1"></times><ci id="S4.SS3.p2.4.4.m4.1.1.3.2.cmml" xref="S4.SS3.p2.4.4.m4.1.1.3.2">𝑛</ci><ci id="S4.SS3.p2.4.4.m4.1.1.3.3.cmml" xref="S4.SS3.p2.4.4.m4.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.4.m4.1c">Q^{nn}</annotation></semantics></math> was asked as <math id="S4.SS3.p2.5.5.m5.1" class="ltx_Math" alttext="I^{+}" display="inline"><semantics id="S4.SS3.p2.5.5.m5.1a"><msup id="S4.SS3.p2.5.5.m5.1.1" xref="S4.SS3.p2.5.5.m5.1.1.cmml"><mi mathcolor="#000000" id="S4.SS3.p2.5.5.m5.1.1.2" xref="S4.SS3.p2.5.5.m5.1.1.2.cmml">I</mi><mo mathcolor="#000000" id="S4.SS3.p2.5.5.m5.1.1.3" xref="S4.SS3.p2.5.5.m5.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.5.m5.1b"><apply id="S4.SS3.p2.5.5.m5.1.1.cmml" xref="S4.SS3.p2.5.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.5.5.m5.1.1.1.cmml" xref="S4.SS3.p2.5.5.m5.1.1">superscript</csymbol><ci id="S4.SS3.p2.5.5.m5.1.1.2.cmml" xref="S4.SS3.p2.5.5.m5.1.1.2">𝐼</ci><plus id="S4.SS3.p2.5.5.m5.1.1.3.cmml" xref="S4.SS3.p2.5.5.m5.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.5.m5.1c">I^{+}</annotation></semantics></math> to form <math id="S4.SS3.p2.6.6.m6.4" class="ltx_Math" alttext="(I^{+},Q,P,I^{-})" display="inline"><semantics id="S4.SS3.p2.6.6.m6.4a"><mrow id="S4.SS3.p2.6.6.m6.4.4.2" xref="S4.SS3.p2.6.6.m6.4.4.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S4.SS3.p2.6.6.m6.4.4.2.3" xref="S4.SS3.p2.6.6.m6.4.4.3.cmml">(</mo><msup id="S4.SS3.p2.6.6.m6.3.3.1.1" xref="S4.SS3.p2.6.6.m6.3.3.1.1.cmml"><mi mathcolor="#000000" id="S4.SS3.p2.6.6.m6.3.3.1.1.2" xref="S4.SS3.p2.6.6.m6.3.3.1.1.2.cmml">I</mi><mo mathcolor="#000000" id="S4.SS3.p2.6.6.m6.3.3.1.1.3" xref="S4.SS3.p2.6.6.m6.3.3.1.1.3.cmml">+</mo></msup><mo mathcolor="#000000" id="S4.SS3.p2.6.6.m6.4.4.2.4" xref="S4.SS3.p2.6.6.m6.4.4.3.cmml">,</mo><mi mathcolor="#000000" id="S4.SS3.p2.6.6.m6.1.1" xref="S4.SS3.p2.6.6.m6.1.1.cmml">Q</mi><mo mathcolor="#000000" id="S4.SS3.p2.6.6.m6.4.4.2.5" xref="S4.SS3.p2.6.6.m6.4.4.3.cmml">,</mo><mi mathcolor="#000000" id="S4.SS3.p2.6.6.m6.2.2" xref="S4.SS3.p2.6.6.m6.2.2.cmml">P</mi><mo mathcolor="#000000" id="S4.SS3.p2.6.6.m6.4.4.2.6" xref="S4.SS3.p2.6.6.m6.4.4.3.cmml">,</mo><msup id="S4.SS3.p2.6.6.m6.4.4.2.2" xref="S4.SS3.p2.6.6.m6.4.4.2.2.cmml"><mi mathcolor="#000000" id="S4.SS3.p2.6.6.m6.4.4.2.2.2" xref="S4.SS3.p2.6.6.m6.4.4.2.2.2.cmml">I</mi><mo mathcolor="#000000" id="S4.SS3.p2.6.6.m6.4.4.2.2.3" xref="S4.SS3.p2.6.6.m6.4.4.2.2.3.cmml">−</mo></msup><mo mathcolor="#000000" stretchy="false" id="S4.SS3.p2.6.6.m6.4.4.2.7" xref="S4.SS3.p2.6.6.m6.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.6.m6.4b"><vector id="S4.SS3.p2.6.6.m6.4.4.3.cmml" xref="S4.SS3.p2.6.6.m6.4.4.2"><apply id="S4.SS3.p2.6.6.m6.3.3.1.1.cmml" xref="S4.SS3.p2.6.6.m6.3.3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.6.6.m6.3.3.1.1.1.cmml" xref="S4.SS3.p2.6.6.m6.3.3.1.1">superscript</csymbol><ci id="S4.SS3.p2.6.6.m6.3.3.1.1.2.cmml" xref="S4.SS3.p2.6.6.m6.3.3.1.1.2">𝐼</ci><plus id="S4.SS3.p2.6.6.m6.3.3.1.1.3.cmml" xref="S4.SS3.p2.6.6.m6.3.3.1.1.3"></plus></apply><ci id="S4.SS3.p2.6.6.m6.1.1.cmml" xref="S4.SS3.p2.6.6.m6.1.1">𝑄</ci><ci id="S4.SS3.p2.6.6.m6.2.2.cmml" xref="S4.SS3.p2.6.6.m6.2.2">𝑃</ci><apply id="S4.SS3.p2.6.6.m6.4.4.2.2.cmml" xref="S4.SS3.p2.6.6.m6.4.4.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.6.6.m6.4.4.2.2.1.cmml" xref="S4.SS3.p2.6.6.m6.4.4.2.2">superscript</csymbol><ci id="S4.SS3.p2.6.6.m6.4.4.2.2.2.cmml" xref="S4.SS3.p2.6.6.m6.4.4.2.2.2">𝐼</ci><minus id="S4.SS3.p2.6.6.m6.4.4.2.2.3.cmml" xref="S4.SS3.p2.6.6.m6.4.4.2.2.3"></minus></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.6.m6.4c">(I^{+},Q,P,I^{-})</annotation></semantics></math> tuples like in our proposed dataset.</span></p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text" style="color:#000000;">In Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Dataset Construction ‣ 4 Question Relevance Prediction and Explanation (QRPE) Dataset ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we present a quantitative and qualitative comparison of the two datasets based on these tuples. On the left side of the figure, we plot the distributions of Euclidean distance between the fc7 features of each <math id="S4.SS3.p3.1.1.m1.2" class="ltx_Math" alttext="(I^{+},I^{-})" display="inline"><semantics id="S4.SS3.p3.1.1.m1.2a"><mrow id="S4.SS3.p3.1.1.m1.2.2.2" xref="S4.SS3.p3.1.1.m1.2.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S4.SS3.p3.1.1.m1.2.2.2.3" xref="S4.SS3.p3.1.1.m1.2.2.3.cmml">(</mo><msup id="S4.SS3.p3.1.1.m1.1.1.1.1" xref="S4.SS3.p3.1.1.m1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S4.SS3.p3.1.1.m1.1.1.1.1.2" xref="S4.SS3.p3.1.1.m1.1.1.1.1.2.cmml">I</mi><mo mathcolor="#000000" id="S4.SS3.p3.1.1.m1.1.1.1.1.3" xref="S4.SS3.p3.1.1.m1.1.1.1.1.3.cmml">+</mo></msup><mo mathcolor="#000000" id="S4.SS3.p3.1.1.m1.2.2.2.4" xref="S4.SS3.p3.1.1.m1.2.2.3.cmml">,</mo><msup id="S4.SS3.p3.1.1.m1.2.2.2.2" xref="S4.SS3.p3.1.1.m1.2.2.2.2.cmml"><mi mathcolor="#000000" id="S4.SS3.p3.1.1.m1.2.2.2.2.2" xref="S4.SS3.p3.1.1.m1.2.2.2.2.2.cmml">I</mi><mo mathcolor="#000000" id="S4.SS3.p3.1.1.m1.2.2.2.2.3" xref="S4.SS3.p3.1.1.m1.2.2.2.2.3.cmml">−</mo></msup><mo mathcolor="#000000" stretchy="false" id="S4.SS3.p3.1.1.m1.2.2.2.5" xref="S4.SS3.p3.1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.1.m1.2b"><interval closure="open" id="S4.SS3.p3.1.1.m1.2.2.3.cmml" xref="S4.SS3.p3.1.1.m1.2.2.2"><apply id="S4.SS3.p3.1.1.m1.1.1.1.1.cmml" xref="S4.SS3.p3.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS3.p3.1.1.m1.1.1.1.1">superscript</csymbol><ci id="S4.SS3.p3.1.1.m1.1.1.1.1.2.cmml" xref="S4.SS3.p3.1.1.m1.1.1.1.1.2">𝐼</ci><plus id="S4.SS3.p3.1.1.m1.1.1.1.1.3.cmml" xref="S4.SS3.p3.1.1.m1.1.1.1.1.3"></plus></apply><apply id="S4.SS3.p3.1.1.m1.2.2.2.2.cmml" xref="S4.SS3.p3.1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p3.1.1.m1.2.2.2.2.1.cmml" xref="S4.SS3.p3.1.1.m1.2.2.2.2">superscript</csymbol><ci id="S4.SS3.p3.1.1.m1.2.2.2.2.2.cmml" xref="S4.SS3.p3.1.1.m1.2.2.2.2.2">𝐼</ci><minus id="S4.SS3.p3.1.1.m1.2.2.2.2.3.cmml" xref="S4.SS3.p3.1.1.m1.2.2.2.2.3"></minus></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.1.m1.2c">(I^{+},I^{-})</annotation></semantics></math> pair in both datasets. We find that the mean distance in the VTFQ dataset is nearly twice that of our QRPE dataset, indicating that irrelevant images in VTFQ are less visually related to source images though we do note the distribution of distances in both datasets is long tailed.</span></p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">On the right side of Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Dataset Construction ‣ 4 Question Relevance Prediction and Explanation (QRPE) Dataset ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we also provide qualitative examples of questions that occur in both datasets. The example on the last row is perhaps most striking. The source question is asking the color of a fork and the relevant image shows an overhead view of a meal with an orange fork set nearby. The irrelevant image in QRPE is a similar image of food, but with chopsticks! Conversely, the image from VTFQ is a man playing baseball.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Question Relevance Detection</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.3" class="ltx_p">In this section, we introduce a simple baseline for irrelevant question detection on the QRPE dataset and demonstrate that explicitly reasoning about premises improves performance for both our new model and existing methods. More formally, we consider the binary classification task of predicting if a question <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="Q_{i}" display="inline"><semantics id="S5.p1.1.m1.1a"><msub id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">Q</mi><mi id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1">subscript</csymbol><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝑄</ci><ci id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">Q_{i}</annotation></semantics></math> from an image-question pair <math id="S5.p1.2.m2.2" class="ltx_Math" alttext="(I_{i},Q_{i})" display="inline"><semantics id="S5.p1.2.m2.2a"><mrow id="S5.p1.2.m2.2.2.2" xref="S5.p1.2.m2.2.2.3.cmml"><mo stretchy="false" id="S5.p1.2.m2.2.2.2.3" xref="S5.p1.2.m2.2.2.3.cmml">(</mo><msub id="S5.p1.2.m2.1.1.1.1" xref="S5.p1.2.m2.1.1.1.1.cmml"><mi id="S5.p1.2.m2.1.1.1.1.2" xref="S5.p1.2.m2.1.1.1.1.2.cmml">I</mi><mi id="S5.p1.2.m2.1.1.1.1.3" xref="S5.p1.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.p1.2.m2.2.2.2.4" xref="S5.p1.2.m2.2.2.3.cmml">,</mo><msub id="S5.p1.2.m2.2.2.2.2" xref="S5.p1.2.m2.2.2.2.2.cmml"><mi id="S5.p1.2.m2.2.2.2.2.2" xref="S5.p1.2.m2.2.2.2.2.2.cmml">Q</mi><mi id="S5.p1.2.m2.2.2.2.2.3" xref="S5.p1.2.m2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S5.p1.2.m2.2.2.2.5" xref="S5.p1.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.2b"><interval closure="open" id="S5.p1.2.m2.2.2.3.cmml" xref="S5.p1.2.m2.2.2.2"><apply id="S5.p1.2.m2.1.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.p1.2.m2.1.1.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S5.p1.2.m2.1.1.1.1.2.cmml" xref="S5.p1.2.m2.1.1.1.1.2">𝐼</ci><ci id="S5.p1.2.m2.1.1.1.1.3.cmml" xref="S5.p1.2.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S5.p1.2.m2.2.2.2.2.cmml" xref="S5.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S5.p1.2.m2.2.2.2.2.1.cmml" xref="S5.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S5.p1.2.m2.2.2.2.2.2.cmml" xref="S5.p1.2.m2.2.2.2.2.2">𝑄</ci><ci id="S5.p1.2.m2.2.2.2.2.3.cmml" xref="S5.p1.2.m2.2.2.2.2.3">𝑖</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.2c">(I_{i},Q_{i})</annotation></semantics></math> is relevant to image <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S5.p1.3.m3.1a"><msub id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mi id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml">I</mi><mi id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1">subscript</csymbol><ci id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">𝐼</ci><ci id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">I_{i}</annotation></semantics></math>.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">A Simple Premise-Aware Model.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.2" class="ltx_p">Like the standard VQA task, question relevance detection also requires making a
prediction based on an encoded image and question. With this in mind, we begin with a straight-forward approach based
on the Deeper LSTM VQA model architecture of <cite class="ltx_cite ltx_citemacro_citet">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>. This model encodes the image <math id="S5.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">I</annotation></semantics></math> via a VGGNet and the
question <math id="S5.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="S5.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.2.m2.1c">Q</annotation></semantics></math> with an LSTM over one-hot word encodings. The concatenation of these embeddings are input to a multi-layer perceptron. We
fine-tune this model for the binary question relevance detection task starting from a model pre-trained on the VQA task. We denote
this model as <span id="S5.SS0.SSS0.Px1.p1.2.1" class="ltx_text ltx_font_typewriter">VQA-Bin</span>.</p>
</div>
<div id="S5.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p2.1" class="ltx_p">We extend the <span id="S5.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_typewriter">VQA-Bin</span> model to explicitly reason about premises. We extract first and second order premises from the question <math id="S5.SS0.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S5.SS0.SSS0.Px1.p2.1.m1.1a"><mi id="S5.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p2.1.m1.1b"><ci id="S5.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p2.1.m1.1c">Q</annotation></semantics></math> and encode them as two concatenated one-hot vectors. We add an additional LSTM to encode the premises and concatenate this added feature to the
image and question feature. We refer to this premise-aware model as <span id="S5.SS0.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_typewriter">VQA-Bin-Premise</span>.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Attention Models.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">We also extend the attention based Hierarchical Co-Attention VQA model of <cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite> for the task of question relevance in a way similar to Deeper LSTM model. We call this model <span id="S5.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">HieCoAtt-Bin</span>. The corresponding premise-aware model is referred to as <span id="S5.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">HieCoAtt-Bin-Prem</span>.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Existing Methods.</h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">We compare our approaches with the best performing model of <cite class="ltx_cite ltx_citemacro_citet">Ray et al. (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>.
This model (which we denote <span id="S5.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">QC-Sim</span>) uses a pretrained captioning model to automatically provide natural language image descriptions and reasons about relevance based on a learned similarity between the question and image caption.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/1705.00601/assets/figures/explanation.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
<span id="S5.F5.4.1" class="ltx_text ltx_font_bold">Question relevance explanation:</span> We provide selected examples of predictions from the False Premise Detection model (<span id="S5.F5.5.2" class="ltx_text ltx_font_typewriter">FPD</span>) on the QRPE test set. Reasoning about premises presents the opportunity to produce natural language statements indicating <span id="S5.F5.6.3" class="ltx_text ltx_font_italic">why</span> a question is irrelevant to an image, by pointing to the premise that is invalid.
</figcaption>
</figure>
<div id="S5.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p2.1" class="ltx_p"><span id="S5.SS0.SSS0.Px3.p2.1.1" class="ltx_text" style="color:#000000;">Specifically, the approach uses
NeuralTalk2 <cite class="ltx_cite ltx_citemacro_cite">Karpathy and Li (<a href="#bib.bib9" title="" class="ltx_ref">2015</a>)</cite> trained on the MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib12" title="" class="ltx_ref">2014</a>)</cite> to generate a caption for each image.
Both the caption and question are embedded as a fixed length vector through an encoding LSTM (with words being represented as word2vec <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a href="#bib.bib18" title="" class="ltx_ref">2013</a>)</cite> vectors).
These question and caption embeddings are concatenated and fed to a multilayer perceptron to predict relevance. We consider two additional versions of this approach that consider only premise-caption similarity (<span id="S5.SS0.SSS0.Px3.p2.1.1.1" class="ltx_text ltx_font_typewriter">PC-Sim</span>) and question-premise-caption similarities (<span id="S5.SS0.SSS0.Px3.p2.1.1.2" class="ltx_text ltx_font_typewriter">QPC-Sim</span>).</span></p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results.</h4>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">We train each model on the QRPE train split and report results on the test set in Table <a href="#S5.T1" title="Table 1 ‣ Results. ‣ 5 Question Relevance Detection ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As the dataset is balanced in the label space, random accuracy stands at 50%. We find that the simple <span id="S5.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_typewriter">VQA-Bin</span> model achieves 66.5% accuracy while the attention based model <span id="S5.SS0.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_typewriter">HieCoAtt-Bin</span> attains 70.74% accuracy. Surprisingly, the caption-similarity based <span id="S5.SS0.SSS0.Px4.p1.1.3" class="ltx_text ltx_font_typewriter">QC-Sim</span> model significantly outperforms these baseline, obtaining an accuracy of 74.35% <span id="S5.SS0.SSS0.Px4.p1.1.4" class="ltx_text" style="color:#000000;">while only reasoning about relevancy from textual descriptions of images. We note that the caption similarity based approaches use a large amount of outside data during pretraining of the captioning model and the word2vec embeddings, which may have contributed to the effectiveness of these methods.</span></p>
</div>
<figure id="S5.T1" class="ltx_table">
<div id="S5.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:205.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(65.1pt,-30.9pt) scale(1.42925270239455,1.42925270239455) ;">
<table id="S5.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Models</td>
<td id="S5.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Overall</td>
<td id="S5.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">First Order</td>
<td id="S5.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:8.5pt;padding-right:8.5pt;">Second Order</td>
</tr>
<tr id="S5.T1.1.1.2.2" class="ltx_tr">
<td id="S5.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">VQA-Bin</td>
<td id="S5.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">66.50</td>
<td id="S5.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">67.36</td>
<td id="S5.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">53.00</td>
</tr>
<tr id="S5.T1.1.1.3.3" class="ltx_tr">
<td id="S5.T1.1.1.3.3.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">VQA-Bin-Prem</td>
<td id="S5.T1.1.1.3.3.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">66.77</td>
<td id="S5.T1.1.1.3.3.3" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">67.04</td>
<td id="S5.T1.1.1.3.3.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">54.38</td>
</tr>
<tr id="S5.T1.1.1.4.4" class="ltx_tr">
<td id="S5.T1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">HieCoAtt-Bin</td>
<td id="S5.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">70.74</td>
<td id="S5.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">71.35</td>
<td id="S5.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T1.1.1.4.4.4.1" class="ltx_text ltx_font_bold">61.54</span></td>
</tr>
<tr id="S5.T1.1.1.5.5" class="ltx_tr">
<td id="S5.T1.1.1.5.5.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">HieCoAtt-Bin-Prem</td>
<td id="S5.T1.1.1.5.5.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">73.34</td>
<td id="S5.T1.1.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">73.97</td>
<td id="S5.T1.1.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">60.35</td>
</tr>
<tr id="S5.T1.1.1.6.6" class="ltx_tr">
<td id="S5.T1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">QC-Sim</td>
<td id="S5.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">74.35</td>
<td id="S5.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">75.82</td>
<td id="S5.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.5pt;padding-right:8.5pt;">55.12</td>
</tr>
<tr id="S5.T1.1.1.7.7" class="ltx_tr">
<td id="S5.T1.1.1.7.7.1" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">PC-Sim</td>
<td id="S5.T1.1.1.7.7.2" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">75.05</td>
<td id="S5.T1.1.1.7.7.3" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">76.47</td>
<td id="S5.T1.1.1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:8.5pt;padding-right:8.5pt;">56.04</td>
</tr>
<tr id="S5.T1.1.1.8.8" class="ltx_tr">
<td id="S5.T1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">QPC-Sim</td>
<td id="S5.T1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T1.1.1.8.8.2.1" class="ltx_text ltx_font_bold">75.31</span></td>
<td id="S5.T1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;"><span id="S5.T1.1.1.8.8.3.1" class="ltx_text ltx_font_bold">76.67</span></td>
<td id="S5.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.5pt;padding-right:8.5pt;">55.95</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy of Question Relevance models on the QRPE test set. We find that premise-aware models consistently outperform alternative models. </figcaption>
</figure>
<div id="S5.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p2.1" class="ltx_p">Most interestingly, we find that the addition of extracted premise representations consistently improves performance of base models. <span id="S5.SS0.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_typewriter">VQA-Bin-Prem</span>, <span id="S5.SS0.SSS0.Px4.p2.1.2" class="ltx_text ltx_font_typewriter">HieCoAtt-Bin-Prem</span>, <span id="S5.SS0.SSS0.Px4.p2.1.3" class="ltx_text ltx_font_typewriter">PC-Sim</span>, and <span id="S5.SS0.SSS0.Px4.p2.1.4" class="ltx_text ltx_font_typewriter">QPC-Sim</span> outperform their no-premise information counterparts, with <span id="S5.SS0.SSS0.Px4.p2.1.5" class="ltx_text ltx_font_typewriter">QPC-Sim</span> being the overall best performing approach at 75.31% accuracy. This is especially interesting given that the models <em id="S5.SS0.SSS0.Px4.p2.1.6" class="ltx_emph ltx_font_italic">already</em> have access to the question from which the premises were extracted. This result seems to imply there is value in explicitly isolating premises from sentence grammar.</p>
</div>
<div id="S5.SS0.SSS0.Px4.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p3.1" class="ltx_p">We further divide our test set into two splits consisting of <math id="S5.SS0.SSS0.Px4.p3.1.m1.2" class="ltx_Math" alttext="(Q,I)" display="inline"><semantics id="S5.SS0.SSS0.Px4.p3.1.m1.2a"><mrow id="S5.SS0.SSS0.Px4.p3.1.m1.2.3.2" xref="S5.SS0.SSS0.Px4.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S5.SS0.SSS0.Px4.p3.1.m1.2.3.2.1" xref="S5.SS0.SSS0.Px4.p3.1.m1.2.3.1.cmml">(</mo><mi id="S5.SS0.SSS0.Px4.p3.1.m1.1.1" xref="S5.SS0.SSS0.Px4.p3.1.m1.1.1.cmml">Q</mi><mo id="S5.SS0.SSS0.Px4.p3.1.m1.2.3.2.2" xref="S5.SS0.SSS0.Px4.p3.1.m1.2.3.1.cmml">,</mo><mi id="S5.SS0.SSS0.Px4.p3.1.m1.2.2" xref="S5.SS0.SSS0.Px4.p3.1.m1.2.2.cmml">I</mi><mo stretchy="false" id="S5.SS0.SSS0.Px4.p3.1.m1.2.3.2.3" xref="S5.SS0.SSS0.Px4.p3.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p3.1.m1.2b"><interval closure="open" id="S5.SS0.SSS0.Px4.p3.1.m1.2.3.1.cmml" xref="S5.SS0.SSS0.Px4.p3.1.m1.2.3.2"><ci id="S5.SS0.SSS0.Px4.p3.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p3.1.m1.1.1">𝑄</ci><ci id="S5.SS0.SSS0.Px4.p3.1.m1.2.2.cmml" xref="S5.SS0.SSS0.Px4.p3.1.m1.2.2">𝐼</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p3.1.m1.2c">(Q,I)</annotation></semantics></math> pairs created by either falsifying first-order and second-order premises. We find that all our models perform significantly better on the first-order split. We hypothesize that the significant diversity in visual representations of attributed objects and comparatively fewer examples for each type makes it more difficult to learn subtle differences for second-order premises.</p>
</div>
</section>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Question Relevance Explanation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In addition to identifying whether a question is irrelevant to an image, being able to indicate <em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">why</em> carries significant real-world utility. From an interpretability perspective, reporting which premise is false is more informative than simply answering the question in the negative,
as it can help to correct the questioner’s misconception regarding the scene. We propose to generate such explanations by identifying the particular question premise(s) that do not apply to an image.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">By construction, irrelevant images in the QRPE dataset are picked on the basis of negating a single premise – we now use our dataset to train models to detect false premises, and use the premises classified as irrelevant to generate templated natural language explanations.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Fig. <a href="#S5.F5" title="Figure 5 ‣ Existing Methods. ‣ 5 Question Relevance Detection ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the task setup for false premise detection. Given a question-image pair, say <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">“What color is the cat’s tie?”</span>, the objective is to identify which (if any) question premises are not grounded in the image, in this case both <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">&lt;cat&gt;</span> and <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_italic">&lt;tie&gt;</span>. Alternatively, for the question <span id="S5.SS1.p3.1.4" class="ltx_text ltx_font_italic">“What kind of building is the large white building?”</span>, both premises <span id="S5.SS1.p3.1.5" class="ltx_text ltx_font_italic">&lt;building, large&gt;</span> and <span id="S5.SS1.p3.1.6" class="ltx_text ltx_font_italic">&lt;building, white&gt;</span> are true premises grounded in the image.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div id="S5.F6.1" class="ltx_inline-block ltx_transformed_outer" style="width:432.5pt;height:29518.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(181.2pt,-12364.5pt) scale(6.16361154102679,6.16361154102679) ;">
<table id="S5.F6.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.F6.1.1.1.1" class="ltx_tr">
<td id="S5.F6.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.1.1.1.1.1" class="ltx_p"><span id="S5.F6.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">What player number is about to swing at the ball?</span></span>
</span>
</td>
<td id="S5.F6.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.1.1.2.1.1" class="ltx_p"><span id="S5.F6.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Why is the man looking at the lady?</span></span>
</span>
</td>
<td id="S5.F6.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.1.1.3.1.1" class="ltx_p"><span id="S5.F6.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">How many people are wearing safety jackets?</span></span>
</span>
</td>
</tr>
<tr id="S5.F6.1.1.2.2" class="ltx_tr">
<td id="S5.F6.1.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.2.2.1.1.1" class="ltx_p">Is there a player number? Yes</span>
</span>
</td>
<td id="S5.F6.1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.2.2.2.1.1" class="ltx_p">Who is looking at the lady? Man</span>
</span>
</td>
<td id="S5.F6.1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.2.2.3.1.1" class="ltx_p">Can you see people in the image? Yes</span>
</span>
</td>
</tr>
<tr id="S5.F6.1.1.3.3" class="ltx_tr">
<td id="S5.F6.1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.3.3.1.1.1" class="ltx_p">Is there a ball in the image? Yes</span>
</span>
</td>
<td id="S5.F6.1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.3.3.2.1.1" class="ltx_p">Is there a lady in the image? Yes</span>
</span>
</td>
<td id="S5.F6.1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.3.3.3.1.1" class="ltx_p">What are the people wearing? Jacket</span>
</span>
</td>
</tr>
<tr id="S5.F6.1.1.4.4" class="ltx_tr">
<td id="S5.F6.1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.4.4.1.1.1" class="ltx_p">Is there a number in the image? Yes</span>
</span>
</td>
<td id="S5.F6.1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.4.4.2.1.1" class="ltx_p ltx_align_center">Is there a man in the image? Yes</span>
</span>
</td>
<td id="S5.F6.1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.4.4.3.1.1" class="ltx_p">Who is wearing the jacket? People</span>
</span>
</td>
</tr>
<tr id="S5.F6.1.1.5.5" class="ltx_tr">
<td id="S5.F6.1.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.5.5.1.1.1" class="ltx_p"><span id="S5.F6.1.1.5.5.1.1.1.1" class="ltx_text ltx_font_bold">What is the child sitting on?</span></span>
</span>
</td>
<td id="S5.F6.1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.5.5.2.1.1" class="ltx_p"><span id="S5.F6.1.1.5.5.2.1.1.1" class="ltx_text ltx_font_bold">Where is the pink hat?</span></span>
</span>
</td>
<td id="S5.F6.1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.5.5.3.1.1" class="ltx_p"><span id="S5.F6.1.1.5.5.3.1.1.1" class="ltx_text ltx_font_bold">What is the item called that the cat is looking at?</span></span>
</span>
</td>
</tr>
<tr id="S5.F6.1.1.6.6" class="ltx_tr">
<td id="S5.F6.1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.6.6.1.1.1" class="ltx_p">What is the child doing? Sitting</span>
</span>
</td>
<td id="S5.F6.1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.6.6.2.1.1" class="ltx_p">What is the color of hat? Pink</span>
</span>
</td>
<td id="S5.F6.1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.6.6.3.1.1" class="ltx_p">Is there a cat in the image? Yes</span>
</span>
</td>
</tr>
<tr id="S5.F6.1.1.7.7" class="ltx_tr">
<td id="S5.F6.1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.7.7.1.1.1" class="ltx_p">Is there a child in the image? Yes</span>
</span>
</td>
<td id="S5.F6.1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.7.7.2.1.1" class="ltx_p">Is there a hat in the image? Yes</span>
</span>
</td>
<td id="S5.F6.1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S5.F6.1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.F6.1.1.7.7.3.1.1" class="ltx_p">Is there an item in the image? Yes</span>
</span>
</td>
</tr>
<tr id="S5.F6.1.1.8.8" class="ltx_tr">
<td id="S5.F6.1.1.8.8.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S5.F6.1.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="S5.F6.1.1.8.8.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Sample generated premise questions from source questions. Source questions are in bold. Ground-truth answers are extracted using the premise tuples. </figcaption>
</figure>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">We train a simple false premise detection model for this task. Our model is a multilayer perceptron that takes one-hot encodings of premises and VGGNet <cite class="ltx_cite ltx_citemacro_cite">Simonyan and Zisserman (<a href="#bib.bib23" title="" class="ltx_ref">2014</a>)</cite> image features as input to predict whether the premise is grounded in the image or not.
We trained our false premise detection model (<span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_typewriter">FPD</span>) model on all premises in the QRPE dataset.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">Our <span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_typewriter">FPD</span> model achieves an accuracy of 61.12% on the QRPE dataset. In Fig. <a href="#S5.F5" title="Figure 5 ‣ Existing Methods. ‣ 5 Question Relevance Detection ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we present qualitative results of our premise classification and explanation pipeline. For the question <span id="S5.SS1.p5.1.2" class="ltx_text ltx_font_italic">“What color is the cat’s tie?”</span>, the model correctly recognizes ‘cat’ and ‘tie’ as false premises, and we generate statements in natural language indicating the same. Thus, determining question relevance by reasoning about each premise presents the opportunity to generate simple explanations that can provide valuable feedback to the questioner, and help improve model trust.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.2.1" class="ltx_text" style="color:#000000;">Premise-Based Visual Question Answering Data Augmentation</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we develop a premise-based data augmentation scheme for VQA that generates simple, templated questions
based on premises present in complex visually-grounded questions from the VQA (training) dataset.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Using the pipeline presented in Section <a href="#S3" title="3 Extracting Premises of a Question ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we extract premises from questions in the VQA dataset and apply
a simple templated question generation strategy to transform premises into question and answer pairs. Note that because the source questions come from sighted humans about an image, we do not need to filter out binary or counting questions in order to avoid false premises as in Section <a href="#S3" title="3 Extracting Premises of a Question ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We do however filter based on SPICE similarity between the generated and source questions to avoid generating duplicates.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">We design templates
for each type of premise – first-order (<em id="S6.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.p3.1.2" class="ltx_text"></span> <span id="S6.p3.1.3" class="ltx_text ltx_font_italic">‘&lt;man&gt;’</span> – <span id="S6.p3.1.4" class="ltx_text ltx_font_italic">“Is there a man?” Yes</span>), second-order
(<em id="S6.p3.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.p3.1.6" class="ltx_text"></span> <span id="S6.p3.1.7" class="ltx_text ltx_font_italic">‘&lt;man, walking&gt;’</span> – <span id="S6.p3.1.8" class="ltx_text ltx_font_italic">“What is the man doing?” Walking</span>, and <span id="S6.p3.1.9" class="ltx_text ltx_font_italic">‘&lt;car, red&gt;’</span> – <span id="S6.p3.1.10" class="ltx_text ltx_font_italic">“What is
the color of the car?” Red</span>), and third-order (<span id="S6.p3.1.11" class="ltx_text ltx_font_italic">‘&lt;man, holding, racket&gt;’</span> – <span id="S6.p3.1.12" class="ltx_text ltx_font_italic">“What is the man holding?” Racket,
“Who is holding the racket?” Man</span>). This process transforms implicit premise concepts which previously had to be learned as part of understanding more complex questions into simple, explicit training examples that can be directly supervised.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Fig. <a href="#A2.F7" title="Figure 7 ‣ Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows sample premise questions produced from source VQA questions using our pipeline. We note that the distribution of premise questions varies drastically from the source VQA distribution (see Table <a href="#A2.T5" title="Table 5 ‣ Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<figure id="S6.1" class="ltx_table">
<div id="S6.1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:65.5pt;vertical-align:-29.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.1pt) scale(0.992371431755214,0.992371431755214) ;">
<div id="S6.1.1.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="S6.1.1.1.p1" class="ltx_para">
<table id="S6.1.1.1.p1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.1.1.1.p1.1.1.1" class="ltx_tr">
<th id="S6.1.1.1.p1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Training Data</th>
<th id="S6.1.1.1.p1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Other</th>
<th id="S6.1.1.1.p1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Number</th>
<th id="S6.1.1.1.p1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Yes</th>
<th id="S6.1.1.1.p1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">No</th>
<th id="S6.1.1.1.p1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.1.1.1.p1.1.2.1" class="ltx_tr">
<td id="S6.1.1.1.p1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Source</td>
<td id="S6.1.1.1.p1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">123,817</td>
<td id="S6.1.1.1.p1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">29,698</td>
<td id="S6.1.1.1.p1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">57217</td>
<td id="S6.1.1.1.p1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">35842</td>
<td id="S6.1.1.1.p1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">246,574</td>
</tr>
<tr id="S6.1.1.1.p1.1.3.2" class="ltx_tr">
<td id="S6.1.1.1.p1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">Premise</td>
<td id="S6.1.1.1.p1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">137,483</td>
<td id="S6.1.1.1.p1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">1,850</td>
<td id="S6.1.1.1.p1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">387,941</td>
<td id="S6.1.1.1.p1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">0</td>
<td id="S6.1.1.1.p1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">527,274</td>
</tr>
</tbody>
</table>
</div>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> Answer type distribution of source and premise questions on the Compositional VQA train set.</figcaption>
</figure>
</div>
</span></div>
</figure>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">We evaluate multiple models with and without premise augmentation on two splits of the VQA dataset - the standard split and the compositional split of <cite class="ltx_cite ltx_citemacro_citet">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>. The compositional split is specifically designed to test a model’s ability to generalize to unseen/rarely seen combinations of concepts at test time.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Augmentation Strategies.</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">We evaluate the Deeper LSTM model of <cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite>
on the standard and compositional splits with two augmentation strategies - <span id="S6.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">All</span> which includes the entire set of premise questions and <span id="S6.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_typewriter">Top-1k-A</span> which includes only questions with answers in the top 1000 most common VQA answers. The results are listed in Table <a href="#A2.T6" title="Table 6 ‣ B.3 Analysis and Results ‣ Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We find minor improvement of 0.34% on the standard split under <span id="S6.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_typewriter">Top-1k-A</span> premise question augmentation. On the compositional split, we observe a 1.16% gain with <span id="S6.SS0.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_typewriter">Top-1k-A</span> augmentation over no augmentation. In this setting, explicitly reasoning about objects and attributes seen in the questions seems to help the model disentangle objects from their common characteristics.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Other Models.</h4>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">To check the general effectiveness of our approach, we further evaluate <span id="S6.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">Top-1k-A</span> augmentation for three additional VQA models on the compositional split. We find inconsistent improvements for these more advanced models with some improving while others see reductions in accuracy when adding premises.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<div id="S6.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:232pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(99.1pt,-53.0pt) scale(1.84148803652789,1.84148803652789) ;">
<table id="S6.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.1.1.1.1" class="ltx_tr">
<th id="S6.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding:1.25pt 3.0pt;"></th>
<th id="S6.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:1.25pt 3.0pt;">Augmentation</th>
<th id="S6.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.25pt 3.0pt;">Overall</th>
<th id="S6.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.25pt 3.0pt;">Other</th>
<th id="S6.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.25pt 3.0pt;">Number</th>
<th id="S6.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.25pt 3.0pt;">Yes/No</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.1.1.2.1" class="ltx_tr">
<th id="S6.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1.25pt 3.0pt;" rowspan="3"><span id="S6.T3.1.1.2.1.1.1" class="ltx_text">
<span id="S6.T3.1.1.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:40pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:40.0pt;transform:translate(-16.54pt,-16.54pt) rotate(-90deg) ;">
<span id="S6.T3.1.1.2.1.1.1.1.1" class="ltx_p">Standard</span>
</span></span></span></th>
<th id="S6.T3.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1.25pt 3.0pt;">None</th>
<td id="S6.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 3.0pt;">54.23</td>
<td id="S6.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 3.0pt;">40.34</td>
<td id="S6.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 3.0pt;">33.27</td>
<td id="S6.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 3.0pt;">79.82</td>
</tr>
<tr id="S6.T3.1.1.3.2" class="ltx_tr">
<th id="S6.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.25pt 3.0pt;">All</th>
<td id="S6.T3.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;">53.74</td>
<td id="S6.T3.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;">39.28</td>
<td id="S6.T3.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;"><span id="S6.T3.1.1.3.2.4.1" class="ltx_text ltx_font_bold">33.38</span></td>
<td id="S6.T3.1.1.3.2.5" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;">79.89</td>
</tr>
<tr id="S6.T3.1.1.4.3" class="ltx_tr">
<th id="S6.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.25pt 3.0pt;">Top-1k-A</th>
<td id="S6.T3.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;"><span id="S6.T3.1.1.4.3.2.1" class="ltx_text ltx_font_bold">54.47</span></td>
<td id="S6.T3.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;"><span id="S6.T3.1.1.4.3.3.1" class="ltx_text ltx_font_bold">40.56</span></td>
<td id="S6.T3.1.1.4.3.4" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;">33.24</td>
<td id="S6.T3.1.1.4.3.5" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;"><span id="S6.T3.1.1.4.3.5.1" class="ltx_text ltx_font_bold">80.19</span></td>
</tr>
<tr id="S6.T3.1.1.5.4" class="ltx_tr">
<th id="S6.T3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding:1.25pt 3.0pt;" rowspan="3"><span id="S6.T3.1.1.5.4.1.1" class="ltx_text">
<span id="S6.T3.1.1.5.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:28.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:28.9pt;transform:translate(-10.06pt,-9.08pt) rotate(-90deg) ;">
<span id="S6.T3.1.1.5.4.1.1.1.1" class="ltx_p">Comp.</span>
</span></span></span></th>
<th id="S6.T3.1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1.25pt 3.0pt;">None</th>
<td id="S6.T3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 3.0pt;">46.69</td>
<td id="S6.T3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 3.0pt;">31.92</td>
<td id="S6.T3.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 3.0pt;">29.73</td>
<td id="S6.T3.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 3.0pt;">70.49</td>
</tr>
<tr id="S6.T3.1.1.6.5" class="ltx_tr">
<th id="S6.T3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.25pt 3.0pt;">All</th>
<td id="S6.T3.1.1.6.5.2" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;">47.63</td>
<td id="S6.T3.1.1.6.5.3" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;">31.97</td>
<td id="S6.T3.1.1.6.5.4" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;"><span id="S6.T3.1.1.6.5.4.1" class="ltx_text ltx_font_bold">30.77</span></td>
<td id="S6.T3.1.1.6.5.5" class="ltx_td ltx_align_center" style="padding:1.25pt 3.0pt;"><span id="S6.T3.1.1.6.5.5.1" class="ltx_text ltx_font_bold">72.52</span></td>
</tr>
<tr id="S6.T3.1.1.7.6" class="ltx_tr">
<th id="S6.T3.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:1.25pt 3.0pt;">Top-1k-A</th>
<td id="S6.T3.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.25pt 3.0pt;"><span id="S6.T3.1.1.7.6.2.1" class="ltx_text ltx_font_bold">47.85</span></td>
<td id="S6.T3.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.25pt 3.0pt;"><span id="S6.T3.1.1.7.6.3.1" class="ltx_text ltx_font_bold">32.58</span></td>
<td id="S6.T3.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.25pt 3.0pt;">30.59</td>
<td id="S6.T3.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.25pt 3.0pt;">72.38</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy on the standard and compositional VQA validation sets for different augmentation strategies for DeeperLSTM<cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>.</figcaption>
</figure>
<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:103.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(28.3pt,-6.7pt) scale(1.14992949338831,1.14992949338831) ;">
<table id="S6.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.1.1.1.1" class="ltx_tr">
<th id="S6.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:1.25pt 2.0pt;">VQA Model</th>
<th id="S6.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.25pt 2.0pt;">Baseline</th>
<th id="S6.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.25pt 2.0pt;">+Premises</th>
<td id="S6.T4.1.1.1.1.4" class="ltx_td ltx_border_tt" style="padding:1.25pt 2.0pt;"></td>
</tr>
<tr id="S6.T4.1.1.2.2" class="ltx_tr">
<th id="S6.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1.25pt 2.0pt;">DeeperLSTM<cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite>
</th>
<td id="S6.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 2.0pt;">46.69</td>
<td id="S6.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.25pt 2.0pt;"><span id="S6.T4.1.1.2.2.3.1" class="ltx_text ltx_font_bold">47.85</span></td>
<td id="S6.T4.1.1.2.2.4" class="ltx_td ltx_border_t" style="padding:1.25pt 2.0pt;"></td>
</tr>
<tr id="S6.T4.1.1.3.3" class="ltx_tr">
<th id="S6.T4.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.25pt 2.0pt;">HieCoAtt<cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="S6.T4.1.1.3.3.2" class="ltx_td ltx_align_center" style="padding:1.25pt 2.0pt;"><span id="S6.T4.1.1.3.3.2.1" class="ltx_text ltx_font_bold">50.17</span></td>
<td id="S6.T4.1.1.3.3.3" class="ltx_td ltx_align_center" style="padding:1.25pt 2.0pt;">49.98</td>
<td id="S6.T4.1.1.3.3.4" class="ltx_td" style="padding:1.25pt 2.0pt;"></td>
</tr>
<tr id="S6.T4.1.1.4.4" class="ltx_tr">
<th id="S6.T4.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.25pt 2.0pt;">NMN<cite class="ltx_cite ltx_citemacro_cite">Andreas et al. (<a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="S6.T4.1.1.4.4.2" class="ltx_td ltx_align_center" style="padding:1.25pt 2.0pt;"><span id="S6.T4.1.1.4.4.2.1" class="ltx_text ltx_font_bold">49.05</span></td>
<td id="S6.T4.1.1.4.4.3" class="ltx_td ltx_align_center" style="padding:1.25pt 2.0pt;">48.43</td>
<td id="S6.T4.1.1.4.4.4" class="ltx_td" style="padding:1.25pt 2.0pt;"></td>
</tr>
<tr id="S6.T4.1.1.5.5" class="ltx_tr">
<th id="S6.T4.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:1.25pt 2.0pt;">MCB<cite class="ltx_cite ltx_citemacro_cite">Fukui et al. (<a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="S6.T4.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.25pt 2.0pt;">50.13</td>
<td id="S6.T4.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.25pt 2.0pt;"><span id="S6.T4.1.1.5.5.3.1" class="ltx_text ltx_font_bold">50.57</span></td>
<td id="S6.T4.1.1.5.5.4" class="ltx_td ltx_border_bb" style="padding:1.25pt 2.0pt;"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Overall accuracy of different VQA models on the Compositional VQA test split using Top-1k-A augmentation.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions and Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we made the simple observation that questions about images often contain premises implied by the question and that reasoning about premises can help VQA models respond more intelligently to irrelevant or novel questions.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We develop a system for automatically extracting these question premises. Using these premises, we automatically created a novel dataset for Question Relevance Prediction and Explanation (QRPE) which consists of 53,911 question, relevant image, and irrelevant image triplets. We also train novel question relevance prediction models and show that models that take advantage of premise information outperform models that do not. Furthermore, we demonstrated that questions generated from premises may be an effective data augmentation technique for VQA tasks that require compositional reasoning.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Integrating Question Relevance Prediction and Explanation (QRPE) models with existing VQA systems would form a natural extension to our approach. In this setting, the relevance prediction model would determine the applicability of a question to an image, and select an appropriate path of action. If the question is classified as relevant, the VQA model would generate a prediction; otherwise, a question relevance explanation model would provide a natural language sentence indicating which premise(s) are not valid for the image. Such systems would be a step in the direction of making VQA systems move beyond academic settings to real-world environments.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2017)</span>
<span class="ltx_bibblock">
A. Agrawal, A. Kembhavi, D. Batra, and D. Parikh. 2017.

</span>
<span class="ltx_bibblock">C-VQA: A Compositional Split of the Visual Question Answering (VQA)
v1.0 Dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.08243</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2016)</span>
<span class="ltx_bibblock">
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016.

</span>
<span class="ltx_bibblock">Spice: Semantic propositional image caption evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 382–398.
Springer.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas et al. (2016)</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016.

</span>
<span class="ltx_bibblock">Neural module networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 39–48.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pages 2425–2433.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elhoseiny et al. (2016)</span>
<span class="ltx_bibblock">
Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, and Ahmed Elgammal.
2016.

</span>
<span class="ltx_bibblock">Automatic annotation of structured facts in images.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1604.00466</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fukui et al. (2016)</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach. 2016.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.01847</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendricks et al. (2016)</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney,
Kate Saenko, and Trevor Darrell. 2016.

</span>
<span class="ltx_bibblock">Deep compositional captioning: Describing novel object categories
without paired training data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2015)</span>
<span class="ltx_bibblock">
Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael
Bernstein, and Li Fei-Fei. 2015.

</span>
<span class="ltx_bibblock">Image retrieval using scene graphs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition</em>,
pages 3668–3678.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy and Li (2015)</span>
<span class="ltx_bibblock">
Andrej Karpathy and Fei-Fei Li. 2015.

</span>
<span class="ltx_bibblock">Deep visual-semantic alignments for generating image descriptions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2016)</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Sang-Woo Lee, Donghyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo
Ha, and Byoung-Tak Zhang. 2016.

</span>
<span class="ltx_bibblock">Multimodal residual learning for visual qa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
361–369.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2016)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma,
Michael Bernstein, and Li Fei-Fei. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1602.07332" title="" class="ltx_ref ltx_href">Visual genome: Connecting
language and vision using crowdsourced dense image annotations</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2015)</span>
<span class="ltx_bibblock">
Jiasen Lu, Xiao Lin, Dhruv Batra, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Deeper lstm and normalized cnn visual question answering model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/VT-vision-lab/VQA_LSTM_CNN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/VT-vision-lab/VQA_LSTM_CNN</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2017)</span>
<span class="ltx_bibblock">
Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. 2017.

</span>
<span class="ltx_bibblock">Knowing When to Look: Adaptive Attention via A Visual
Sentinel for Image Captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances In Neural Information Processing Systems</em>, pages
289–297.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
1682–1690.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski et al. (2015)</span>
<span class="ltx_bibblock">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015.

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions
about images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pages 1–9.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
3111–3119.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2016)</span>
<span class="ltx_bibblock">
Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Bernt Schiele, Trevor
Darrell, and Marcus Rohrbach. 2016.

</span>
<span class="ltx_bibblock">Attentive explanations: Justifying decisions and pointing to the
evidence.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.04757</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ray et al. (2016)</span>
<span class="ltx_bibblock">
Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Question relevance in vqa: Identifying non-visual and false-premise
questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky et al. (2012)</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Jonathan Krause, Alex Berg, and Li Fei-Fei. 2012.

</span>
<span class="ltx_bibblock">The ImageNet Large Scale Visual Recognition Challenge 2012
(ILSVRC2012).

</span>
<span class="ltx_bibblock">http://www.image-net.org/challenges/LSVRC/2012/.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster et al. (2015)</span>
<span class="ltx_bibblock">
Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-Fei, and Christopher D
Manning. 2015.

</span>
<span class="ltx_bibblock">Generating semantically precise scene graphs from textual
descriptions for improved image retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Workshop on Vision and Language</em>,
pages 70–80.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2014)</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman. 2014.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1409.1556.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2016)</span>
<span class="ltx_bibblock">
Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. 2016.

</span>
<span class="ltx_bibblock">Fvqa: Fact-based visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.05433</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2016)</span>
<span class="ltx_bibblock">
Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, and Anton van den Hengel.
2016.

</span>
<span class="ltx_bibblock">What value do explicit high level concepts have in vision to language
problems?

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 203–212.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2017)</span>
<span class="ltx_bibblock">
Danfei Xu, Yuke Zhu, Christopher Choy, and Li Fei-Fei. 2017.

</span>
<span class="ltx_bibblock">Scene graph generation by iterative message passing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zitnick et al. (2016)</span>
<span class="ltx_bibblock">
C Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell,
Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Measuring machine intelligence through visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1608.08716</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

</section>
<section id="A1" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Compositional VQA Split</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section, we provide details regarding the Compositional VQA split introduced by <cite class="ltx_cite ltx_citemacro_citet">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>, on which we perform our data augmentation experiments (Section <a href="#S6" title="6 Premise-Based Visual Question Answering Data Augmentation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).
The compositional splits were created by re-arranging the training and validation splits of the VQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>. These splits were created such that the question-answer (QA) pairs in the compositional test split (<em id="A1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="A1.p1.1.2" class="ltx_text"></span>, Question: “What color is the plate?”, Answer: “green”) are not seen in the compositional train split, but the concepts that compose the test QA pairs (<em id="A1.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="A1.p1.1.4" class="ltx_text"></span>, “plate”, “green”) have been seen in the compositional train split (<em id="A1.p1.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="A1.p1.1.6" class="ltx_text"></span>, Question: “What color is the apple?”, Answer: “Green”, Question: “How many plates are on the table?”, Answer: “4”) to the extent possible. Evaluating a VQA model under such a setting helps in testing – 1) whether the model is capable of learning disentangled representations for different concepts (<em id="A1.p1.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="A1.p1.1.8" class="ltx_text"></span>, “plate”, “green”, “apple”, “4”, “table”), and 2) whether the model can compose these learned concepts to correctly answer questions about novel questions at test time.</p>
</div>
</section>
<section id="A2" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Question Generation</h2>

<figure id="A2.F7" class="ltx_figure">
<div id="A2.F7.1" class="ltx_inline-block ltx_transformed_outer" style="width:432.5pt;height:29518.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(181.2pt,-12364.5pt) scale(6.16361154102679,6.16361154102679) ;">
<table id="A2.F7.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.F7.1.1.1.1" class="ltx_tr">
<td id="A2.F7.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.1.1.1.1.1" class="ltx_p"><span id="A2.F7.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">What player number is about to swing at the ball?</span></span>
</span>
</td>
<td id="A2.F7.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.1.1.2.1.1" class="ltx_p"><span id="A2.F7.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Why is the man looking at the lady?</span></span>
</span>
</td>
<td id="A2.F7.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.1.1.3.1.1" class="ltx_p"><span id="A2.F7.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">How many people are wearing safety jackets?</span></span>
</span>
</td>
</tr>
<tr id="A2.F7.1.1.2.2" class="ltx_tr">
<td id="A2.F7.1.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.2.2.1.1.1" class="ltx_p">Is there a player number? Yes</span>
</span>
</td>
<td id="A2.F7.1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.2.2.2.1.1" class="ltx_p">Who is looking at the lady? Man</span>
</span>
</td>
<td id="A2.F7.1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.2.2.3.1.1" class="ltx_p">Can you see people in the image? Yes</span>
</span>
</td>
</tr>
<tr id="A2.F7.1.1.3.3" class="ltx_tr">
<td id="A2.F7.1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.3.3.1.1.1" class="ltx_p">Is there a ball in the image? Yes</span>
</span>
</td>
<td id="A2.F7.1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.3.3.2.1.1" class="ltx_p">Is there a lady in the image? Yes</span>
</span>
</td>
<td id="A2.F7.1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.3.3.3.1.1" class="ltx_p">What are the people wearing? Jacket</span>
</span>
</td>
</tr>
<tr id="A2.F7.1.1.4.4" class="ltx_tr">
<td id="A2.F7.1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.4.4.1.1.1" class="ltx_p">Is there a number in the image? Yes</span>
</span>
</td>
<td id="A2.F7.1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.4.4.2.1.1" class="ltx_p">Is there a man in the image? Yes</span>
</span>
</td>
<td id="A2.F7.1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.4.4.3.1.1" class="ltx_p">Who is wearing the jacket? People</span>
</span>
</td>
</tr>
<tr id="A2.F7.1.1.5.5" class="ltx_tr">
<td id="A2.F7.1.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.5.5.1.1.1" class="ltx_p"><span id="A2.F7.1.1.5.5.1.1.1.1" class="ltx_text ltx_font_bold">What is the child sitting on?</span></span>
</span>
</td>
<td id="A2.F7.1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.5.5.2.1.1" class="ltx_p ltx_align_center"><span id="A2.F7.1.1.5.5.2.1.1.1" class="ltx_text ltx_font_bold">Where is the pink hat?</span></span>
</span>
</td>
<td id="A2.F7.1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.5.5.3.1.1" class="ltx_p"><span id="A2.F7.1.1.5.5.3.1.1.1" class="ltx_text ltx_font_bold">What is the item called that the cat is looking at?</span></span>
</span>
</td>
</tr>
<tr id="A2.F7.1.1.6.6" class="ltx_tr">
<td id="A2.F7.1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.6.6.1.1.1" class="ltx_p">What is the child doing? Sitting</span>
</span>
</td>
<td id="A2.F7.1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.6.6.2.1.1" class="ltx_p">What is the color of hat? Pink</span>
</span>
</td>
<td id="A2.F7.1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.6.6.3.1.1" class="ltx_p">Is there a cat in the image? Yes</span>
</span>
</td>
</tr>
<tr id="A2.F7.1.1.7.7" class="ltx_tr">
<td id="A2.F7.1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.7.7.1.1.1" class="ltx_p">Is there a child in the image? Yes</span>
</span>
</td>
<td id="A2.F7.1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.7.7.2.1.1" class="ltx_p">Is there a hat in the image? Yes</span>
</span>
</td>
<td id="A2.F7.1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="A2.F7.1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.F7.1.1.7.7.3.1.1" class="ltx_p">Is there an item in the image? Yes</span>
</span>
</td>
</tr>
<tr id="A2.F7.1.1.8.8" class="ltx_tr">
<td id="A2.F7.1.1.8.8.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="A2.F7.1.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;"></td>
<td id="A2.F7.1.1.8.8.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:173.0pt;padding-top:0.25pt;padding-bottom:0.25pt;"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Sample generated premise questions from source questions. Source questions are in bold. Ground-truth answers are extracted using the premise tuples. </figcaption>
</figure>
<figure id="A2.F8" class="ltx_figure"><img src="/html/1705.00601/assets/figures/compositionality.png" id="A2.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Some interesting examples of how augmentation helps the DeeperLSTM model <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> on the compositional VQA split.</figcaption>
</figure>
<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">For the data augmentation experiments in Section <a href="#S6" title="6 Premise-Based Visual Question Answering Data Augmentation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we generate premise questions using a rule-based pipeline. Different templates of questions are assigned for different kinds of facts.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">First order premises like &lt;man&gt;, &lt;bus&gt; are transformed into existential questions like “Is there a man?”, “Is there a bus?” and so on. Second order premises can generate two kinds of questions depending on whether the second element is an action or an attribute. For example, &lt;man, walking&gt; would become “What is the man doing?” while &lt;car, red&gt; would become "What is the color of the car?”. In general, questions generated from third order premises look like “Is the man holding the racket?", and “What is the cat on top of?” for &lt;man, holding, racket&gt; and &lt;cat, on top of, box&gt;, respectively. However, third order premises are more complicated and many different questions can be generated from them depending on the types of components in the premise.</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">Question generation also involves minor pre-processing and post-processing, i.e. filtering out erroneous premises and linguistically ambiguous questions. We also run SPICE on the generated and source questions and threshold the result to eliminate generated questions that are near duplicates of the source questions. Code for our question generation pipeline will be made available.</p>
</div>
<div id="A2.p4" class="ltx_para">
<p id="A2.p4.1" class="ltx_p">A random selection of premise questions generated from the VQA dataset can be seen in Fig. <a href="#A2.F7" title="Figure 7 ‣ Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The answer type distribution of generated premise questions can be seen in Table <a href="#A2.T5" title="Table 5 ‣ Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We find that generated premise questions are twice in number as compared to source questions. We generate relatively few ‘Number’ questions – very few second-order tuples of this type occur in the premises we extract, as questions about multiple number of objects at a time are rare in the VQA dataset. By design, we generate only ‘Yes’ questions and zero ‘No’ questions. The reason for that is twofold – first, we only generate premise questions from true premises, and second, first order premises are the most frequent premises in source questions (first order premises generate ‘Yes’ questions).</p>
</div>
<figure id="A2.1" class="ltx_table">
<div id="A2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:65.5pt;vertical-align:-29.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.1pt) scale(0.992371431755214,0.992371431755214) ;">
<div id="A2.1.1.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A2.1.1.1.p1" class="ltx_para">
<table id="A2.1.1.1.p1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.1.1.1.p1.1.1.1" class="ltx_tr">
<th id="A2.1.1.1.p1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Training Data</th>
<th id="A2.1.1.1.p1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Other</th>
<th id="A2.1.1.1.p1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Number</th>
<th id="A2.1.1.1.p1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Yes</th>
<th id="A2.1.1.1.p1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">No</th>
<th id="A2.1.1.1.p1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.1.1.1.p1.1.2.1" class="ltx_tr">
<td id="A2.1.1.1.p1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Source</td>
<td id="A2.1.1.1.p1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">123,817</td>
<td id="A2.1.1.1.p1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">29,698</td>
<td id="A2.1.1.1.p1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">57217</td>
<td id="A2.1.1.1.p1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">35842</td>
<td id="A2.1.1.1.p1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">246,574</td>
</tr>
<tr id="A2.1.1.1.p1.1.3.2" class="ltx_tr">
<td id="A2.1.1.1.p1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">Premise</td>
<td id="A2.1.1.1.p1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">137,483</td>
<td id="A2.1.1.1.p1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">1,850</td>
<td id="A2.1.1.1.p1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">387,941</td>
<td id="A2.1.1.1.p1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">0</td>
<td id="A2.1.1.1.p1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">527,274</td>
</tr>
</tbody>
</table>
</div>
<figure id="A2.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> Answer type distribution of source and premise questions on the Compositional VQA train set.</figcaption>
</figure>
</div>
</span></div>
</figure>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Data Augmentation</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">We perform a series of data augmentation experiments using the questions generated in <a href="#A2" title="Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> and evaluate performance of models on both the standard VQA split and the Compostitional VQA split described in <a href="#A1" title="Appendix A Compositional VQA Split ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Experimental Setup</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">For the augmentation experiments, we start by generating premise questions from the Compositional VQA train split <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>. The generated premise questions are added to the original source questions for training models. The number of generated premise questions is almost twice the number of source questions, therefore we try a series of augmentation strategies based on different subsets of premise questions to be added. The model used for these experiments is the DeeperLSTM model by <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>. The various augmentation ablations are:</p>
<ol id="A2.I1" class="ltx_enumerate">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Baseline:</span> No premise questions added to the training set.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">All:</span> Adding all the generated premise questions along with source questions to the training set.</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Only-Binary:</span> Only binary (Questions with answers ‘Yes’ or ‘No’) premise questions added along with the source questions.</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">No-Other:</span> All questions except premise questions of type ‘Other’ (answers outside of Binary and Number answers) added to the training set.</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">No-Binary:</span> All questions except binary premise questions are added to the training set.</p>
</div>
</li>
<li id="A2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="A2.I1.i6.p1" class="ltx_para">
<p id="A2.I1.i6.p1.1" class="ltx_p"><span id="A2.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Comm-Other:</span> All binary premise questions added. ‘Other’ and ‘Number’ premise question types whose answers lie in the pool of source question answers are added to the training set.</p>
</div>
</li>
<li id="A2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="A2.I1.i7.p1" class="ltx_para">
<p id="A2.I1.i7.p1.1" class="ltx_p"><span id="A2.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Top1k-A:</span> All binary premise questions added. Also, premise questions of type ‘Other’ with answers amongst the top 1000 VQA source answers are added.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Analysis and Results</h3>

<figure id="A2.T6" class="ltx_table">
<div id="A2.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:253.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.4pt,-54.9pt) scale(1.76275612691708,1.76275612691708) ;">
<table id="A2.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T6.1.1.1.1" class="ltx_tr">
<th id="A2.T6.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Data Ablation</th>
<th id="A2.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Overall</th>
<th id="A2.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Other</th>
<th id="A2.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Number</th>
<th id="A2.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Yes/No</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T6.1.1.2.1" class="ltx_tr">
<th id="A2.T6.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Baseline</th>
<td id="A2.T6.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">46.69</td>
<td id="A2.T6.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">31.92</td>
<td id="A2.T6.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">29.73</td>
<td id="A2.T6.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">70.49</td>
</tr>
<tr id="A2.T6.1.1.3.2" class="ltx_tr">
<th id="A2.T6.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">All</th>
<td id="A2.T6.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.63</td>
<td id="A2.T6.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">31.97</td>
<td id="A2.T6.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.1.1.3.2.4.1" class="ltx_text ltx_font_bold">30.77</span></td>
<td id="A2.T6.1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.1.1.3.2.5.1" class="ltx_text ltx_font_bold">72.52</span></td>
</tr>
<tr id="A2.T6.1.1.4.3" class="ltx_tr">
<th id="A2.T6.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Only-Binary</th>
<td id="A2.T6.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.25</td>
<td id="A2.T6.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">32.45</td>
<td id="A2.T6.1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">29.65</td>
<td id="A2.T6.1.1.4.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.30</td>
</tr>
<tr id="A2.T6.1.1.5.4" class="ltx_tr">
<th id="A2.T6.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">No-Other</th>
<td id="A2.T6.1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.33</td>
<td id="A2.T6.1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">32.47</td>
<td id="A2.T6.1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">29.85</td>
<td id="A2.T6.1.1.5.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.42</td>
</tr>
<tr id="A2.T6.1.1.6.5" class="ltx_tr">
<th id="A2.T6.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">No-Binary</th>
<td id="A2.T6.1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.76</td>
<td id="A2.T6.1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">31.69</td>
<td id="A2.T6.1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">29.39</td>
<td id="A2.T6.1.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.09</td>
</tr>
<tr id="A2.T6.1.1.7.6" class="ltx_tr">
<th id="A2.T6.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Comm-Other</th>
<td id="A2.T6.1.1.7.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.53</td>
<td id="A2.T6.1.1.7.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">32.41</td>
<td id="A2.T6.1.1.7.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">28.88</td>
<td id="A2.T6.1.1.7.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.33</td>
</tr>
<tr id="A2.T6.1.1.8.7" class="ltx_tr">
<th id="A2.T6.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">Top1k-A</th>
<td id="A2.T6.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.1.1.8.7.2.1" class="ltx_text ltx_font_bold">47.85</span></td>
<td id="A2.T6.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A2.T6.1.1.8.7.3.1" class="ltx_text ltx_font_bold">32.58</span></td>
<td id="A2.T6.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">30.59</td>
<td id="A2.T6.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">72.38</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance of DeeperLSTM <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> on Compositional VQA test split with different augmentations.</figcaption>
</figure>
<figure id="A2.T7" class="ltx_table">
<div id="A2.T7.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:89.3pt;vertical-align:-41.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.2pt) scale(0.992371431755214,0.992371431755214) ;">
<figure id="A2.T7.1.fig1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:433.6pt;">
<table id="A2.T7.1.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T7.1.fig1.1.1.1" class="ltx_tr">
<th id="A2.T7.1.fig1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">VQA Model</th>
<th id="A2.T7.1.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">Baseline</th>
<th id="A2.T7.1.fig1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">With Premises</th>
<td id="A2.T7.1.fig1.1.1.1.4" class="ltx_td ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr id="A2.T7.1.fig1.1.2.2" class="ltx_tr">
<th id="A2.T7.1.fig1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">DeeperLSTM<cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>
</th>
<td id="A2.T7.1.fig1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">46.69</td>
<td id="A2.T7.1.fig1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">47.85</td>
<td id="A2.T7.1.fig1.1.2.2.4" class="ltx_td ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr id="A2.T7.1.fig1.1.3.3" class="ltx_tr">
<th id="A2.T7.1.fig1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">HieCoAtt<cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="A2.T7.1.fig1.1.3.3.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.17</td>
<td id="A2.T7.1.fig1.1.3.3.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">49.98</td>
<td id="A2.T7.1.fig1.1.3.3.4" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr id="A2.T7.1.fig1.1.4.4" class="ltx_tr">
<th id="A2.T7.1.fig1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">NMN<cite class="ltx_cite ltx_citemacro_cite">Andreas et al. (<a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="A2.T7.1.fig1.1.4.4.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">49.05</td>
<td id="A2.T7.1.fig1.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">48.43</td>
<td id="A2.T7.1.fig1.1.4.4.4" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr id="A2.T7.1.fig1.1.5.5" class="ltx_tr">
<th id="A2.T7.1.fig1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">MCB<cite class="ltx_cite ltx_citemacro_cite">Fukui et al. (<a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="A2.T7.1.fig1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">50.13</td>
<td id="A2.T7.1.fig1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">50.57</td>
<td id="A2.T7.1.fig1.1.5.5.4" class="ltx_td ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 7: </span>Accuracy of different VQA models on the Compositional VQA test split using Top1k-A augmentation.</figcaption>
</figure>
</span></div>
</figure>
<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p">Table <a href="#A2.T6" title="Table 6 ‣ B.3 Analysis and Results ‣ Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the VQA accuracy of the DeeperLSTM model for these different dataset augmentation strategies. While all settings show some improvements over the standard training set, we find the largest increase with the <span id="A2.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">Top1k-A</span> setting. By restricting the additional question to those having answers in the top-1000 most commonly occurring answers from the standard VQA set, the added data does not significantly shift the types of answers the model learns are likely. Some examples where the augmented DeeperLSTM model performs better than a non-augmented model are shown in Fig. <a href="#A2.F8" title="Figure 8 ‣ Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div id="A2.SS3.p2" class="ltx_para">
<p id="A2.SS3.p2.1" class="ltx_p">Keeping the <span id="A2.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">Top1k-A</span> data augmentation setting, we extend our experiments to additional VQA models. Table <a href="#A2.T7" title="Table 7 ‣ B.3 Analysis and Results ‣ Appendix B Question Generation ‣ The Promise of Premise: Harnessing Question Premises in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the results of these experiments. We find that while this data augmentation technique results in improvements for some models, it fails to consistently deliver significantly better performance overall.</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1705.00600" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1705.00601" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1705.00601">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1705.00601" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1705.00602" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar  3 13:47:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
