<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.06783] Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging</title><meta property="og:description" content="Federated learning is a recent development in the machine learning area that allows a system of devices to train on one or more tasks without sharing their data to a single location or device. However, this framework s…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.06783">

<!--Generated on Thu Feb 29 20:16:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated learning,  Lifelong learning,  Deep reinforcement learning,  Landmark localization">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_affiliation_postcode">21201</span></p>
</div>
<h1 class="ltx_title ltx_title_document">Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guangyao Zheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Rice University</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_streetaddress">6100 Main St</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_city">Houston</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_state">Texas</span><span id="id5.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id6.6.id6" class="ltx_text ltx_affiliation_postcode">77005</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tz30@rice.edu">tz30@rice.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vladimir Braverman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Rice University</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_streetaddress">6100 Main St</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_city">Houston</span><span id="id10.4.id4" class="ltx_text ltx_affiliation_state">Texas</span><span id="id11.5.id5" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael A. Jacobs
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id12.1.id1" class="ltx_text ltx_affiliation_institution">Department Of Diagnostic And Interventional Imaging, McGovern Medical School, UTHealth</span><span id="id13.2.id2" class="ltx_text ltx_affiliation_streetaddress">6431 Fannin St</span><span id="id14.3.id3" class="ltx_text ltx_affiliation_city">Houston</span><span id="id15.4.id4" class="ltx_text ltx_affiliation_state">Texas</span><span id="id16.5.id5" class="ltx_text ltx_affiliation_country">USA</span><span id="id17.6.id6" class="ltx_text ltx_affiliation_postcode">77030</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vishwa S. Parekh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id18.1.id1" class="ltx_text ltx_affiliation_institution">The University of Maryland Medical Intelligent Imaging (UM2ii) Center</span><span id="id19.2.id2" class="ltx_text ltx_affiliation_streetaddress">520 W Lombard St</span><span id="id20.3.id3" class="ltx_text ltx_affiliation_city">Baltimore</span><span id="id21.4.id4" class="ltx_text ltx_affiliation_state">Maryland</span><span id="id22.5.id5" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
</div>
<div class="ltx_dates">(2023; 20 February 2007; 12 March 2009; 5 June 2009)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id23.id1" class="ltx_p">Federated learning is a recent development in the machine learning area that allows a system of devices to train on one or more tasks without sharing their data to a single location or device. However, this framework still requires a centralized server to consolidate individual models into one synchronously or have inefficient or frail peer-to-peer communication, which are potential bottlenecks for the use of federated learning. In this paper, we propose a novel method of asynchronous decentralized federated lifelong learning (ADFLL) method that inherits the merits of federated learning and can train on multiple tasks simultaneously without the need for a central node or synchronous training, or less-than-desirable peer-to-peer communication. Thus, overcoming the potential drawbacks of conventional federated learning. We demonstrate excellent performance on the brain tumor segmentation (BRATS) dataset for localizing the left ventricle on multiple image sequences and image orientation. Our framework allows agents to achieve the best performance with a mean distance error of 7.81, better than the conventional central aggregation agent’s mean distance error of 11.78, and significantly (p=0.01) better than a conventional lifelong reinforcement learning (LL) agent with a distance error of 15.17 after eight rounds of training. In addition, all ADFLL agents have better performance than a conventional reinforcement learning (RL) agent with no LL implementation. In conclusion, we developed an ADFLL framework with excellent performance and speed-up compared to conventional LL agents.</p>
</div>
<div class="ltx_keywords">Federated learning, Lifelong learning, Deep reinforcement learning, Landmark localization
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>KDD FL4Data-Mining ’23: International Workshop on Federated Learning for Distributed Data Mining; August 7, 2023; Long Beach, CA, USA</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>KDD FL4Data-Mining ’23, August 7, 2023, Long Beach, CA, USA.</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Multi-agent reinforcement learning</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Medical imaging, MRI (Magnetic Resonance Imaging), PET (Positron Emission Tomography), CT (Computerized Tomography), X-ray, and Ultrasound, play a critical role in the diagnosis, prognosis, and preventative care of patients. The use of machine learning methods in medical imaging, such as classification, segmentation, noise reduction, and landmark localization, has been used in different completing complicated environments and settings <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>; Pan et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2022</a>; Tripathi et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Khairandish et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2022</a>; Noothout et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>. However, these methods are usually done on single tasks, without the ability to generalize to other tasks. They often require a full dataset on a device for training, which may cause privacy concerns about patient data and computational constraints for the device specifications <cite class="ltx_cite ltx_citemacro_citep">(Yoo et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address these challenges, Federated Learning (FL) has emerged as a promising approach that enables multiple agents to collaboratively train a model without sharing their data <cite class="ltx_cite ltx_citemacro_citep">(Adnan et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>. A federated learning system aims to protect data privacy and reduce computational costs at the local agent level by distributing the computation to multiple agents to train the model on their local data and sharing only the model updates with a central server. Federated learning implementations have shown promising results in various medical applications <cite class="ltx_cite ltx_citemacro_citep">(Roth et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2022</a>; Yan et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>. However, federated learning frameworks often rely on synchronized learning schedules, meaning all participating agents start training at the same time. They also require agents to have the same architecture in order for the central server to aggregate the model weights. Data and agent heterogeneity influence the training speed which greatly reduces the efficiency and challenges the robustness of these approaches <cite class="ltx_cite ltx_citemacro_citep">(Rieke et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>. Additionally, Federated learning approaches cannot perform Lifelong Learning (LL), which is an important aspect of machine learning applied to medical imaging. Works have shown the ability to improve accuracy, have excellent performance on multiple tasks, and generalize <cite class="ltx_cite ltx_citemacro_citep">(Karani et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>. With medical imaging tasks involving constant and rapid altering in imaging environments, such as new imaging sequences or abnormal patient conditions, a federated learning framework that is trained in an older environment may not perform well when evaluated in an unseen environment. One way to address this issue is to train the model again on the new dataset. However, new environment data can potentially be scarce, and this approach may lead to catastrophic forgetting, where the model loses its ability to operate effectively in the older environment. Federated lifelong learning implementations such as Huang et. al. exist <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> in the medical field, but it is limited by their ring-type structure, which means the communication delay can potentially be very high due to irresponsive or failed agents that bottleneck the entire system.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address all the limitations mentioned above, we propose an asynchronous decentralized federated lifelong learning (ADFLL) approach to landmark localization in medical imaging. This framework leverages Federated learning’s ability to protect data privacy and reduce computational constraints, while also permitting data and agent heterogeneity to be in the system. This framework does not require a central node, and can together lifelong learn multiple tasks without catastrophic forgetting. We provide a flexible, efficient, and robust framework that can be deployed in real-world applications. This paper presents experimental results demonstrating the efficacy of our framework on the 2017 brain tumor segmentation (BraTS) dataset consisting of 8 different image environments and imaging sequences, highlighting its potential to revolutionize landmark localization in the medical imaging field while also maintaining data privacy and reducing computational costs.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.06783/assets/Resources/DFL_figure3.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Illustration of the 8 task-environment pairs. The red boxes indicate the true landmark location of the top left ventricle. The yellow box is a predicted location from ADFLL agents during their training progressions</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Experiment and Result</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Dataset and Experimental Setup</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1. </span>Clinical Data</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">For evaluation of our ADFLL framework, we utilized the 2017 brain tumor segmentation (BraTS) dataset <cite class="ltx_cite ltx_citemacro_citep">(Menze et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2014</a>)</cite>. This dataset consisted of 285 patients and included pre-contrast T1-weight, post-contrast T1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR) sequences in the axial orientation. From this dataset, we randomly sampled a subset of 100 patients to use as our experiment dataset. 60 patients have high-grade glioma (HGG) and 40 patients have low-grade glioma (LGG). We split the 100 patients into two parts 80:20, 80 were used for training and 20 for evaluation, with the training set consisting of 48 HGG and 32 LGG tumors, and the test set consisting of 12 HGG and 8 LGG tumors. We reconstructed the dataset to include all three imaging orientations (coronal, sagittal, and axial). As a result, we obtained a total of twenty-four imaging environments with combinations of two pathologies, 4 imaging sequences, and 3 image orientations. The top left ventricle was chosen as the task for this experiment, and 8 task-environment pairs were sampled as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2. </span>Deployment Experimental Setup</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">Every agent implements a multi-task lifelong reinforcement learning algorithm for localizing landmarks across the human anatomy. The federated lifelong learning component is implemented by integrating experience replay buffers from previous experiences shared by agents across the network for training. There are four agents in this experimental system: we implemented two on an NVIDIA DGX-1 each with an NVIDIA V100 and two on Google Cloud each with an NVIDIA T4. The topology of the system is shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.1.2. Deployment Experimental Setup ‣ 2.1. Dataset and Experimental Setup ‣ 2. Experiment and Result ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The two agents A1, and A2 on Google Cloud have their individual hubs H1 and H2. The two agents A3, and A4 on the DGX-1 are connected to the third hub H3 with a total of three hubs for 4 agents. Since the GPUs on DGX-1 are much more powerful than the GPUs on Google Cloud, A3 and A4 will run significantly faster than A1 and A2. We also implemented asynchronous learning, meaning when the agent finishes training on a task, as long as there are new ERBs that they have not learned from, they will start a new round and learn from those ERBs. Each agent will also get a different image training dataset each round. This process is continued until all four agents complete three rounds of training, guaranteeing all 8 sampled tasks for this experiment will be learned by the system.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2303.06783/assets/Resources/DFL_figure4.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Illustration of the 4-agent decentralized federated lifelong learning framework of our experiment.</figcaption>
</figure>
<figure id="S2.T1" class="ltx_table"><img src="/html/2303.06783/assets/Resources/ADFL_Table2.png" id="S2.T1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="260" alt="[Uncaptioned image]">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Comparison of distance error between our agents (Agent 1-4) after round 3, all-knowing deep reinforcement learning agent (Agent X) after round 1, partially-knowing deep reinforcement learning agent (Agent Y) after round 1, and traditional lifelong deep reinforcement learning agent (Agent M) after round 8.</figcaption>
</figure>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p"><span id="S2.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">All-knowing agent and partially-knowing agent:</span> To better compare our framework with non-lifelong learning ones, we ran two different deep reinforcement learning agents. Agent X is the all-knowing agent, with all 8 datasets available to it at the start of training for the deployment experiment and 24 datasets for the ablation study. It trained on the available data for one round. Agent Y is the partially-knowing agent, which only has access to one dataset and can therefore only train for one round.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p"><span id="S2.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Traditional lifelong deep reinforcement learning agent:</span>
To better compare our framework with the traditional lifelong learning framework, we ran an Agent M that has access to the dataset sequentially and is therefore trained for eight rounds to account for learning eight different environments for the deployment experiment and two rounds for the ablation study.</p>
</div>
<div id="S2.SS1.SSS2.p4" class="ltx_para">
<p id="S2.SS1.SSS2.p4.1" class="ltx_p"><span id="S2.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Experimental Metric:</span>
The performance metric was set as the terminal Euclidean distance between the agent’s prediction and the target landmark. We performed paired t-tests to compare the performance of the decentralized federated lifelong learning framework with the traditional lifelong learning framework and all-knowing deep reinforcement learning agent and partial-knowing deep reinforcement learning agent. The p-value for statistical significance was set to <math id="S2.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="p\leq 0.05" display="inline"><semantics id="S2.SS1.SSS2.p4.1.m1.1a"><mrow id="S2.SS1.SSS2.p4.1.m1.1.1" xref="S2.SS1.SSS2.p4.1.m1.1.1.cmml"><mi id="S2.SS1.SSS2.p4.1.m1.1.1.2" xref="S2.SS1.SSS2.p4.1.m1.1.1.2.cmml">p</mi><mo id="S2.SS1.SSS2.p4.1.m1.1.1.1" xref="S2.SS1.SSS2.p4.1.m1.1.1.1.cmml">≤</mo><mn id="S2.SS1.SSS2.p4.1.m1.1.1.3" xref="S2.SS1.SSS2.p4.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p4.1.m1.1b"><apply id="S2.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1"><leq id="S2.SS1.SSS2.p4.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1.1"></leq><ci id="S2.SS1.SSS2.p4.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1.2">𝑝</ci><cn type="float" id="S2.SS1.SSS2.p4.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p4.1.m1.1c">p\leq 0.05</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3. </span>Ablation Study</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">We conducted two simulation experiments to evaluate the scalability, flexibility, and robustness of our framework. We initialized the same type of agents in both experiments as in the functionality experiment. For both experiments, we evaluated the average performance of all the agents for the task of localizing the top left ventricle across all 24 imaging environments. Additionally, since it is prohibitively expensive to experiment on 24 different machines, these systems were simulated on the NVIDIA DGX-1, with a synchronous training protocol.</p>
</div>
<div id="S2.SS1.SSS3.p2" class="ltx_para">
<p id="S2.SS1.SSS3.p2.1" class="ltx_p"><span id="S2.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Addition of agents experiment:</span>
We initialized a system with four agents, as previously described in the functionality experiment. We subsequently increased the number of agents in the system from 4 to 16 agents over the progression of 4 rounds (4,8,12,16). We further simulated a communication dropout of <math id="S2.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S2.SS1.SSS3.p2.1.m1.1a"><mrow id="S2.SS1.SSS3.p2.1.m1.1.1" xref="S2.SS1.SSS3.p2.1.m1.1.1.cmml"><mn id="S2.SS1.SSS3.p2.1.m1.1.1.2" xref="S2.SS1.SSS3.p2.1.m1.1.1.2.cmml">75</mn><mo id="S2.SS1.SSS3.p2.1.m1.1.1.1" xref="S2.SS1.SSS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.1.m1.1b"><apply id="S2.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S2.SS1.SSS3.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.SS1.SSS3.p2.1.m1.1.1.2.cmml" xref="S2.SS1.SSS3.p2.1.m1.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.1.m1.1c">75\%</annotation></semantics></math> to account for network communication issues in the real world leading information loss while transmitting ERBs across agents. The goal of this experiment was to demonstrate how newer agents joining the system at different points in time can take advantage of the available within the system to learn the collective knowledge available in the system within just one round.</p>
</div>
<div id="S2.SS1.SSS3.p3" class="ltx_para">
<p id="S2.SS1.SSS3.p3.1" class="ltx_p"><span id="S2.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Deletion of agents experiment:</span>
In the deletion experiment, we gradually decreased The number of agents in the system from 24 to 1 agent over the progression of 5 rounds (24,12,6,3,1). The communication for this experiment was also simulated with a <math id="S2.SS1.SSS3.p3.1.m1.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S2.SS1.SSS3.p3.1.m1.1a"><mrow id="S2.SS1.SSS3.p3.1.m1.1.1" xref="S2.SS1.SSS3.p3.1.m1.1.1.cmml"><mn id="S2.SS1.SSS3.p3.1.m1.1.1.2" xref="S2.SS1.SSS3.p3.1.m1.1.1.2.cmml">75</mn><mo id="S2.SS1.SSS3.p3.1.m1.1.1.1" xref="S2.SS1.SSS3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p3.1.m1.1b"><apply id="S2.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS3.p3.1.m1.1.1.1.cmml" xref="S2.SS1.SSS3.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.SS1.SSS3.p3.1.m1.1.1.2.cmml" xref="S2.SS1.SSS3.p3.1.m1.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p3.1.m1.1c">75\%</annotation></semantics></math> dropout. The goal of this experiment was to demonstrate how the proposed ADFLL system preserves the collective knowledge in a lifelong learning manner across all the tasks even as the agents contributing the knowledge leave the system.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Results</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We conducted a deployment experiment based on 8 sub-task-environment pairs: Axial HGG t1ce, Sagittal HGG t1ce, Coronal HGG t1ce, Axial HGG flair, Sagittal LGG flair, Coronal LGG flair, Coronal LGG t2, Sagittal LGG t1. We sampled one image from each task to test the performance of our model and baseline models. Each round the four federated lifelong learning will receive a new task. They will begin the next round when there is also ERB to train from. Since the agents’ training speeds are very different, A1 and A2 will finish their tasks slower, allowing them to learn from more ERBs at once. As shown in Table <a href="#S2.T1" title="Table 1 ‣ 2.1.2. Deployment Experimental Setup ‣ 2.1. Dataset and Experimental Setup ‣ 2. Experiment and Result ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, after three rounds of training, A2 was able to achieve a mean distance error of 7.81 on all 8 tasks, compared to the all-knowing agent’s 11.78 (p=0.22), but significantly lower compared to partially-knowing agent’s 54.58 (p¡0.001), and the traditional lifelong learning agent’s 15.17 (p=0.01) after eight rounds of training. Since this is a real-world experiment, other agents trained faster than A2, meaning that they did not have all the ERBs available to them when they started their last round of training, resulting in their performances being worse than A2. The possible reasons for Agent 1 to have lower performance than Agent X and Agent M can be because it did not have all ERBs or the training was stuck at a local minimum. This can be easily solved by sharing the model parameters of the latest agent. Note that the all-knowing agent and the partially-knowing agent only train for 1 round for this experiment because they have no lifelong learning capability.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Compared to the All-knowing agent shown in <a href="#S2.F3" title="Figure 3 ‣ 2.2. Results ‣ 2. Experiment and Result ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our framework is able to achieve an excellent performance boost. Compared to the traditional lifelong learning agent, our framework is able to achieve a significant performance boost and an outstanding speedup.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2303.06783/assets/Resources/result_comparison.png" id="S2.F3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="275" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Comparison of distance error across 8 different tasks between Agent X (Central Aggregation) and Agent 2 (ADFLL).</figcaption>
</figure>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In our two ablation studies, our framework showed scalability of up to 24 agents, robustness against network dropout, and flexibility in system topology. As shown in Figure <a href="#S2.F4" title="Figure 4 ‣ 2.2. Results ‣ 2. Experiment and Result ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we see that the average Euclidean distance error across all agents decreases as more agents are added to the system, with an average Euclidean distance error of 16.89 at the end of 4 rounds. As shown in Figure <a href="#S2.F5" title="Figure 5 ‣ 2.2. Results ‣ 2. Experiment and Result ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we also see that the average Euclidean distance error across all agents decreases, while half of the agents are deleted every round, resulting in an average Euclidean distance error of 8.55 after 5 rounds. This shows that the knowledge agents learned, captured in ERBs are not lost when agents are removed from the system. And when agents are being added, the new agents can catch up to existing agents in one round. Moreover, the <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mn id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">75</mn><mo id="S2.SS2.p3.1.m1.1.1.1" xref="S2.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">75\%</annotation></semantics></math> dropout rate that is applied to every round of both experiments shows the robustness of our framework against network failures, a major bottleneck for federated learning frameworks.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2303.06783/assets/Resources/addition.png" id="S2.F4.g1" class="ltx_graphics ltx_img_landscape" width="299" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Comparison of distance error of all agents in the system across 4 rounds of training</figcaption>
</figure>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2303.06783/assets/Resources/deletion.png" id="S2.F5.g1" class="ltx_graphics ltx_img_landscape" width="299" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Comparison of distance error of all agents in the system across 4 rounds of training</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Conclusion</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Previous works have explored the application of federated learning to the medical field <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2022</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022</a>; Roy et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. They have shown decentralized federated learning system setups, each demonstrating good performance in their experiment tasks. But because of their system topology implementation, one node failure can potentially collapse the entire system. Moreover, the learning tasks examined were limited, binary classifications or MNIST dataset classification, resulting in limited potential applications. Additionally, their implementation offers a synchronous training procedure, which means in a real application scenario, users of their framework will have to coordinate the training process.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Asynchronous federated learning has also been explored in other areas <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>. They offer the ability to deal with nodes with different computational power but lack the decentralization that allows the system to be more flexible.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Asynchronous decentralized federated learning has also been explored <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. However, they are still limited in their system implementation. The cost of removing a central node is a quadratic complexity communication scheme in that every node communicates with every node.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">We have demonstrated a privacy-aware, asynchronous decentralized federated learning system with robust and efficient system topology. We have demonstrated excellent performance on landmark localization tasks on the BraTS image dataset. Our framework performs better than all-knowing deep reinforcement learning agents and traditional lifelong learning agents. Moreover, in our ablation study, our framework demonstrated excellent scalability, flexibility, and robustness. In the future, we will optimize our approach, expand the system further, and increase computational efficiency.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<p id="S3.F6.1" class="ltx_p ltx_align_center">[htb]
<img src="/html/2303.06783/assets/Resources/Picture_ADFLL_pic1.png" id="S3.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="479" height="200" alt="Refer to caption"></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Illustration of decentralized federated learning setup. Blue circles represent individual agents, and orange circles represent hubs.</figcaption>
</figure>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported by the DARPA grant: DARPA-PA-20-02-11-HR00112190130 and 5P30CA006973 (Imaging Response Assessment Team-IRAT), U01CA140204

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adnan et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Mohammed Adnan, Shivam
Kalra, Jesse C. Cresswell, Graham W.
Taylor, and Hamid R. Tizhoosh.
2022.

</span>
<span class="ltx_bibblock">Federated learning and differential privacy for
medical image analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Scientific Reports</em> 12,
1 (Dec. 2022).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1038/s41598-022-05539-7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1038/s41598-022-05539-7</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alansary et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Amir Alansary, Loic
Le Folgoc, Ghislain Vaillant, Ozan
Oktay, Yuanwei Li, Wenjia Bai,
Jonathan Passerat-Palmbach, Ricardo
Guerrero, Konstantinos Kamnitsas,
Benjamin Hou, et al<span id="bib.bib3.3.1" class="ltx_text">.</span>
2018.

</span>
<span class="ltx_bibblock">Automatic view planning with multi-scale deep
reinforcement learning agents. In <em id="bib.bib3.4.1" class="ltx_emph ltx_font_italic">International
Conference on Medical Image Computing and Computer-Assisted Intervention</em>.
Springer, 277–285.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yujing Chen, Yue Ning,
and Huzefa Rangwala. 2019.

</span>
<span class="ltx_bibblock">Asynchronous Online Federated Learning for Edge
Devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/1911.02134
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Junlong Cheng, Shengwei
Tian, Long Yu, Chengrui Gao,
Xiaojing Kang, Xiang Ma,
Weidong Wu, Shijia Liu, and
Hongchun Lu. 2022.

</span>
<span class="ltx_bibblock">ResGANet: Residual group attention network for
medical image classification and segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>
76 (2022), 102313.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.media.2021.102313" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.media.2021.102313</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yixing Huang, Christoph
Bert, Stefan Fischer, Manuel Schmidt,
Arnd Dörfler, Andreas Maier,
Rainer Fietkau, and Florian Putz.
2022.

</span>
<span class="ltx_bibblock">Continual Learning for Peer-to-Peer Federated
Learning: A Study on Automated Brain Metastasis Identification.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:arXiv:2204.13591

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Meirui Jiang, Zirui Wang,
and Qi Dou. 2022.

</span>
<span class="ltx_bibblock">HarmoFL: Harmonizing Local and Global Drifts in
Federated Learning on Heterogeneous Medical Images.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on
Artificial Intelligence</em> 36, 1
(Jun. 2022), 1087–1095.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1609/aaai.v36i1.19993" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1609/aaai.v36i1.19993</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karani et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Neerav Karani, Krishna
Chaitanya, Christian Baumgartner, and
Ender Konukoglu. 2018.

</span>
<span class="ltx_bibblock">A Lifelong Learning Approach to Brain MR
Segmentation Across Scanners and Protocols. In
<em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted
Intervention – MICCAI 2018</em>,
Alejandro F. Frangi,
Julia A. Schnabel, Christos Davatzikos,
Carlos Alberola-López, and Gabor
Fichtinger (Eds.). Springer International Publishing,
Cham, 476–484.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khairandish et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
M.O. Khairandish, M.
Sharma, V. Jain, J.M. Chatterjee, and
N.Z. Jhanjhi. 2022.

</span>
<span class="ltx_bibblock">A Hybrid CNN-SVM Threshold Segmentation Approach
for Tumor Detection and Classification of MRI Brain Images.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">IRBM</em> 43,
4 (2022), 290–299.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.irbm.2021.06.003" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.irbm.2021.06.003</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Qi Liu, Bo-Jun Yang,
Zhaojian Wang, Dafeng Zhu,
Xinyi Wang, Kai Ma, and
Xinping Guan. 2022.

</span>
<span class="ltx_bibblock">Asynchronous Decentralized Federated Learning for
Collaborative Fault Diagnosis of PV Stations.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and
Engineering</em> 9 (2022),
1680–1696.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Menze et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Bjoern H Menze, Andras
Jakab, Stefan Bauer, Jayashree
Kalpathy-Cramer, Keyvan Farahani, Justin
Kirby, Yuliya Burren, Nicole Porz,
Johannes Slotboom, Roland Wiest,
et al<span id="bib.bib11.3.1" class="ltx_text">.</span> 2014.

</span>
<span class="ltx_bibblock">The multimodal brain tumor image segmentation
benchmark (BRATS).

</span>
<span class="ltx_bibblock"><em id="bib.bib11.4.1" class="ltx_emph ltx_font_italic">IEEE transactions on medical imaging</em>
34, 10 (2014),
1993–2024.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mnih et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Volodymyr Mnih, Koray
Kavukcuoglu, David Silver, Alex Graves,
Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. 2013.

</span>
<span class="ltx_bibblock">Playing atari with deep reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.5602</em>
(2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
TV Nguyen, MA Dakka,
SM Diakiw, MD VerMilyea,
M Perugini, JMM Hall, and
D Perugini. 2022.

</span>
<span class="ltx_bibblock">A Novel Decentralized Federated Learning Approach to
Train on Globally Distributed, Poor Quality, and Protected Private Medical
Data.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.21203/rs.3.rs-1371143/v1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.21203/rs.3.rs-1371143/v1</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noothout et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Julia M. H. Noothout,
Bob D. De Vos, Jelmer M. Wolterink,
Elbrich M. Postma, Paul A. M. Smeets,
Richard A. P. Takx, Tim Leiner,
Max A. Viergever, and Ivana Išgum.
2020.

</span>
<span class="ltx_bibblock">Deep Learning-Based Regression and Classification
for Automatic Landmark Localization in Medical Images.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>
39, 12 (2020),
4011–4022.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/TMI.2020.3009002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TMI.2020.3009002</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Liangrui Pan, Hetian
Wang, Lian Wang, Boya Ji,
Mingting Liu, Mitchai Chongcheawchamnan,
Jin Yuan, and Shaoliang Peng.
2022.

</span>
<span class="ltx_bibblock">Noise-reducing attention cross fusion learning
transformer for histological image classification of osteosarcoma.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Biomedical Signal Processing and Control</em>
77 (2022), 103824.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.bspc.2022.103824" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.bspc.2022.103824</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parekh et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Vishwa S Parekh, Vladimir
Braverman, Michael A Jacobs, et al<span id="bib.bib16.3.1" class="ltx_text">.</span>
2020.

</span>
<span class="ltx_bibblock">Multitask radiological modality invariant landmark
localization using deep reinforcement learning. In
<em id="bib.bib16.4.1" class="ltx_emph ltx_font_italic">Medical Imaging with Deep Learning</em>. PMLR,
588–600.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rieke et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Nicola Rieke, Jonny
Hancox, Wenqi Li, Fausto Milletarì,
Holger R. Roth, Shadi Albarqouni,
Spyridon Bakas, Mathieu N. Galtier,
Bennett A. Landman, Klaus Maier-Hein,
Sébastien Ourselin, Micah Sheller,
Ronald M. Summers, Andrew Trask,
Daguang Xu, Maximilian Baust, and
M. Jorge Cardoso. 2020.

</span>
<span class="ltx_bibblock">The future of digital health with federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">npj Digital Medicine</em> 3,
1 (1 Dec. 2020).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1038/s41746-020-00323-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1038/s41746-020-00323-1</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rolnick et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
David Rolnick, Arun
Ahuja, Jonathan Schwarz, Timothy
Lillicrap, and Gregory Wayne.
2019.

</span>
<span class="ltx_bibblock">Experience replay for continual learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Holger R. Roth, Ken
Chang, Praveer Singh, Nir Neumark,
Wenqi Li, Vikash Gupta,
Sharut Gupta, Liangqiong Qu,
Alvin Ihsani, Bernardo C. Bizzo,
Yuhong Wen, Varun Buch,
Meesam Shah, Felipe Kitamura,
Matheus Mendonça, Vitor Lavor,
Ahmed Harouni, Colin Compas,
Jesse Tetreault, Prerna Dogra,
Yan Cheng, Selnur Erdal,
Richard White, Behrooz Hashemian,
Thomas Schultz, Miao Zhang,
Adam McCarthy, B. Min Yun,
Elshaimaa Sharaf, Katharina V. Hoebel,
Jay B. Patel, Bryan Chen,
Sean Ko, Evan Leibovitz,
Etta D. Pisano, Laura Coombs,
Daguang Xu, Keith J. Dreyer,
Ittai Dayan, Ram C. Naidu,
Mona Flores, Daniel Rubin, and
Jayashree Kalpathy-Cramer.
2020.

</span>
<span class="ltx_bibblock">Federated Learning for Breast Density
Classification: A Real-World Implementation. In
<em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Domain Adaptation and Representation Transfer, and
Distributed and Collaborative Learning: Second MICCAI Workshop, DART 2020,
and First MICCAI Workshop, DCL 2020, Held in Conjunction with MICCAI 2020,
Lima, Peru, October 4–8, 2020, Proceedings</em>.
Springer-Verlag, Berlin, Heidelberg,
181–191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roy et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Abhijit Guha Roy, Shayan
Siddiqui, Sebastian Pölsterl, Nassir
Navab, and Christian Wachinger.
2019.

</span>
<span class="ltx_bibblock">BrainTorrent: A Peer-to-Peer Environment for
Decentralized Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/1905.06731
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tripathi et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Sumit Tripathi,
Taresh Sarvesh Sharan, Shiru Sharma,
and Neeraj Sharma. 2022.

</span>
<span class="ltx_bibblock">An Augmented Deep Learning Network with Noise
Suppression Feature for Efficient Segmentation of Magnetic Resonance Images.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">IETE Technical Review</em> 39,
4 (2022), 960–973.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1080/02564602.2021.1937349" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/02564602.2021.1937349</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vlontzos et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Athanasios Vlontzos, Amir
Alansary, Konstantinos Kamnitsas, Daniel
Rueckert, and Bernhard Kainz.
2019.

</span>
<span class="ltx_bibblock">Multiple Landmark Detection using Multi-Agent
Reinforcement Learning. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">International
Conference on Medical Image Computing and Computer-Assisted Intervention</em>.
Springer, 262–270.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhao Wang, Yifan Hu,
Shiyang Yan, Zhihao Wang,
Ruijie Hou, and Chao Wu.
2022.

</span>
<span class="ltx_bibblock">Efficient Ring-Topology Decentralized Federated
Learning with Deep Generative Models for Medical Data in eHealthcare
Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Electronics</em> 11,
10 (2022).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/electronics11101548" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/electronics11101548</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zengqiang Yan, Jeffry
Wicaksana, Zhiwei Wang, Xin Yang, and
Kwang-Ting Cheng. 2021.

</span>
<span class="ltx_bibblock">Variation-Aware Federated Learning With
Multi-Source Decentralized Medical Image Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">IEEE Journal of Biomedical and Health
Informatics</em> 25, 7
(2021), 2615–2628.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/JBHI.2020.3040015" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/JBHI.2020.3040015</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoo et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Joo Hun Yoo, Hyejun
Jeong, Jaehyeok Lee, and Tai-Myoung
Chung. 2021.

</span>
<span class="ltx_bibblock">Federated Learning: Issues in Medical Application.
In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Future Data and Security Engineering: 8th
International Conference, FDSE 2021, Virtual Event, November 24–26, 2021,
Proceedings</em>. Springer, Berlin,
Heidelberg, 3–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Research Method</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Deep Reinforcement Learning</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.2" class="ltx_p">We created a deep reinforcement learning framework that utilizes the deep Q-network (DQN) algorithm, which is depicted in Figure <a href="#S3.F6" title="Figure 6 ‣ 3. Conclusion ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The 3D DQN model we used in this paper was adapted from existing works <cite class="ltx_cite ltx_citemacro_citep">(Mnih et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2013</a>; Alansary et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Vlontzos et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>; Parekh et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. The environment was represented by a 3D imaging volume with x, y, and z dimensions. The agent was represented by a 3-dimensional bounding box with six possible actions: moving in the positive or negative in the x, y, or z axis. The state was defined by the current location (or a chain of locations) of the agent, each represented by a 3-dimensional bounding box. The reward was calculated by the change in distance to the target landmark location before and after the agent takes an action. The agent’s exploration within the environment generated state-action-reward-resulting state <math id="A1.SS1.p1.1.m1.4" class="ltx_Math" alttext="[s,a,r,s^{\prime}]" display="inline"><semantics id="A1.SS1.p1.1.m1.4a"><mrow id="A1.SS1.p1.1.m1.4.4.1" xref="A1.SS1.p1.1.m1.4.4.2.cmml"><mo stretchy="false" id="A1.SS1.p1.1.m1.4.4.1.2" xref="A1.SS1.p1.1.m1.4.4.2.cmml">[</mo><mi id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml">s</mi><mo id="A1.SS1.p1.1.m1.4.4.1.3" xref="A1.SS1.p1.1.m1.4.4.2.cmml">,</mo><mi id="A1.SS1.p1.1.m1.2.2" xref="A1.SS1.p1.1.m1.2.2.cmml">a</mi><mo id="A1.SS1.p1.1.m1.4.4.1.4" xref="A1.SS1.p1.1.m1.4.4.2.cmml">,</mo><mi id="A1.SS1.p1.1.m1.3.3" xref="A1.SS1.p1.1.m1.3.3.cmml">r</mi><mo id="A1.SS1.p1.1.m1.4.4.1.5" xref="A1.SS1.p1.1.m1.4.4.2.cmml">,</mo><msup id="A1.SS1.p1.1.m1.4.4.1.1" xref="A1.SS1.p1.1.m1.4.4.1.1.cmml"><mi id="A1.SS1.p1.1.m1.4.4.1.1.2" xref="A1.SS1.p1.1.m1.4.4.1.1.2.cmml">s</mi><mo id="A1.SS1.p1.1.m1.4.4.1.1.3" xref="A1.SS1.p1.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="A1.SS1.p1.1.m1.4.4.1.6" xref="A1.SS1.p1.1.m1.4.4.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.4b"><list id="A1.SS1.p1.1.m1.4.4.2.cmml" xref="A1.SS1.p1.1.m1.4.4.1"><ci id="A1.SS1.p1.1.m1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1">𝑠</ci><ci id="A1.SS1.p1.1.m1.2.2.cmml" xref="A1.SS1.p1.1.m1.2.2">𝑎</ci><ci id="A1.SS1.p1.1.m1.3.3.cmml" xref="A1.SS1.p1.1.m1.3.3">𝑟</ci><apply id="A1.SS1.p1.1.m1.4.4.1.1.cmml" xref="A1.SS1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.1.m1.4.4.1.1.1.cmml" xref="A1.SS1.p1.1.m1.4.4.1.1">superscript</csymbol><ci id="A1.SS1.p1.1.m1.4.4.1.1.2.cmml" xref="A1.SS1.p1.1.m1.4.4.1.1.2">𝑠</ci><ci id="A1.SS1.p1.1.m1.4.4.1.1.3.cmml" xref="A1.SS1.p1.1.m1.4.4.1.1.3">′</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.4c">[s,a,r,s^{\prime}]</annotation></semantics></math> tuples, which were recorded in the experience replay buffer (ERB) over multiple episodes. The information contained in the ERBs are non-sensitive information, as the action and reward are numbers regarding the DRL model, and the state and resulting states are small fractions of the total 3D image, roughly <math id="A1.SS1.p1.2.m2.1" class="ltx_Math" alttext="0.3\%" display="inline"><semantics id="A1.SS1.p1.2.m2.1a"><mrow id="A1.SS1.p1.2.m2.1.1" xref="A1.SS1.p1.2.m2.1.1.cmml"><mn id="A1.SS1.p1.2.m2.1.1.2" xref="A1.SS1.p1.2.m2.1.1.2.cmml">0.3</mn><mo id="A1.SS1.p1.2.m2.1.1.1" xref="A1.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m2.1b"><apply id="A1.SS1.p1.2.m2.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="A1.SS1.p1.2.m2.1.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="A1.SS1.p1.2.m2.1.1.2.cmml" xref="A1.SS1.p1.2.m2.1.1.2">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.2.m2.1c">0.3\%</annotation></semantics></math>.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Lifelong Learning</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">We implemented lifelong learning using selective experience replay <cite class="ltx_cite ltx_citemacro_citep">(Rolnick et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>. The goal of selective experience replay is to avoid catastrophic forgetting by focusing on selected experiences from previous tasks. Additionally, this technique is agnostic to the model being used and enables the sharing of experiences across different models. To achieve lifelong learning, we utilized a selective experience replay buffer that collects a sequence of experience samples throughout the model’s training process. In order to learn a generalized representation of both current and past tasks, the model selects a batch of experiences from both the ERB of its current task and from the replay buffers of previous tasks during training.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>Asynchronous Decentralized Federated Lifelong Learning</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">We developed the Asynchronous Decentralized Federated lifelong Learning (ADFLL) by constructing a network of lifelong deep reinforcement learning agents. Each agent shares their database of personal experiences with each other to facilitate learning from each other experiences. More specifically, once an agent finishes training with a dataset and an ERB, the resulting experience from the training is shared with the network. Furthermore, we modified the training setup for each agent to sample experiences from the current dataset ERB, the agent’s personal experiences and the incoming experiences from other agents, as shown in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3. Conclusion ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. As a result, every agent in the network can learn from each other’s experiences, thereby integrating federated lifelong learning capability.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2303.06783/assets/Resources/HubExample.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="85" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Snapshot of the shared database maintained by the hub nodes</figcaption>
</figure>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.1" class="ltx_p">In a naive setup, every agent would communicate their experiences with every other agent in the network. However, such an all-to-all communication setup is highly inefficient and not scalable as it would require a large amount of communication bandwidth. To address this issue, we implemented a homogeneous distributed database system as illustrated in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3. Conclusion ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. As shown in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3. Conclusion ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, our network consists of a predefined set of hub nodes that act as communication hubs for spatially adjacent nodes in the network. Subsequently, every agent in the network exclusively communicates with their nearest hub node at the end of each personal training round. The experience sharing between an agent and a hub node is bidirectional. Finally, every hub node maintains a shared experience database on the network as shown in Fig. <a href="#A1.F7" title="Figure 7 ‣ A.3. Asynchronous Decentralized Federated Lifelong Learning ‣ Appendix A Research Method ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The hub nodes periodically communicate with each other to synchronize their databases. The agents in the system are not required to have standardized training speed or start training at the same time. The hub will regulate and preserve the experiences in the system and agents in the system can train on different tasks. An example of this system is demonstrated in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3. Conclusion ‣ Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="A1.SS3.p3" class="ltx_para">
<p id="A1.SS3.p3.1" class="ltx_p">The advantage of our system setup is that it is robust against node or hub failures. When a node fails, the only loss is the training information from that node, and when a hub fails, the loss is the ERBs it contains but other hubs do not. Moreover, the communication complexity is linear with respect to the number of nodes, each node only needs to communicate with its respective hub, and hubs sync periodically. Compared to other federated learning systems, centralized or not, they either are prone to system-wide failure caused by a node failure, or sacrifice communication complexity to prevent system-wide failures.</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.06782" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.06783" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.06783">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.06783" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.06784" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 20:16:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
