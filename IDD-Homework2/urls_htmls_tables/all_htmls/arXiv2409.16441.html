<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation</title>
<!--Generated on Tue Sep 24 20:15:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.16441v1/"/></head>
<body>
<nav class="ltx_page_navbar">
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<h1 class="ltx_title ltx_title_document">A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Avisha Kumar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kunal Kotkar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kelly Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span>
<span class="ltx_contact ltx_role_affiliation">these authors contributed equally to this work
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Meghana Bhimreddy
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span>
<span class="ltx_contact ltx_role_affiliation">these authors contributed equally to this work
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Davidar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span>
<span class="ltx_contact ltx_role_affiliation">these authors contributed equally to this work
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Carly Weber-Levine
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span>
<span class="ltx_contact ltx_role_affiliation">these authors contributed equally to this work
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Siddharth Krishnan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Max J. Kerensky
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruixing Liang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kelley K. Leadingham
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Denis Routkevitch
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew M. Hersh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kimberly Ashayeri
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Betty Tyler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ian Suk
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jennifer Son
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Cleveland Clinic, Cleveland, OH, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nicholas Theodore
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nitish Thakor
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University, Baltimore, MD, USA
</span>
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University School of Medicine, Baltimore, MD, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amir Manbachi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Johns Hopkins University, Baltimore, MD, USA
</span>
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">While deep learning has catalyzed breakthroughs across numerous domains, its broader adoption in clinical settings is inhibited by the costly and time-intensive nature of data acquisition and annotation. To further facilitate medical machine learning, we present an ultrasound dataset of 10,223 Brightness-mode (B-mode) images consisting of sagittal slices of porcine spinal cords (N=25) before and after a contusion injury. We additionally benchmark the performance metrics of several state-of-the-art object detection algorithms to localize the site of injury and semantic segmentation models to label the anatomy for comparison and creation of task-specific architectures. Finally, we evaluate the zero-shot generalization capabilities of the segmentation models on human ultrasound spinal cord images to determine whether training on our porcine dataset is sufficient for accurately interpreting human data. Our results show that the YOLOv8 detection model outperforms all evaluated models for injury localization, achieving a mean Average Precision (mAP50-95) score of 0.606. Segmentation metrics indicate that the DeepLabv3 segmentation model achieves the highest accuracy on unseen porcine anatomy, with a Mean Dice score of 0.587, while SAMed achieves the highest Mean Dice score generalizing to human anatomy (0.445). To the best of our knowledge, this is the largest annotated dataset of spinal cord ultrasound images made publicly available to researchers and medical professionals, as well as the first public report of object detection and segmentation architectures to assess anatomical markers in the spinal cord for methodology development and clinical applications.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Object detection, semantic segmentation, medical dataset, ultrasound, spinal cord injury
</div>
<section class="ltx_section" id="Sx1" lang="en">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Spinal cord injury (SCI) is a devastating condition affecting an estimated 900,000 people worldwide in 2019 alone, predominantly stemming from motor vehicle accidents and violence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib2" title="">2</a>]</cite>. The aftermath of SCI extends beyond the acute trauma, as patients experience mobility impairments, severe pain, autonomic dysregulation, and increased risk of mortality if not treated effectively and efficiently <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib3" title="">3</a>]</cite>. SCI can be divided into 2 phases: the primary injury that results immediately from the traumatic event, and the secondary injury, in which the clinical manifestations of SCI are exacerbated by inflammation and edema, compromising vascular perfusion within the cord <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib5" title="">5</a>]</cite>. The current treatment approach involves a surgical decompression procedure, during which the vertebral structures surrounding the injury site are removed to promote blood flow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Interventions such as cerebrospinal fluid (CSF) drainage, augmentation of mean arterial pressure goals, and electrical stimulation show promise to mitigate the adverse physiological sequelae following SCI and may improve functional outcome <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib10" title="">10</a>]</cite>. However, optimally titrating these therapies remains a challenge due to the lack of real-time and automatic monitoring of spinal cord parameters such as hematoma development and tissue inflammation. Ultrasound is a promising diagnostic tool for SCI management, providing clinicians with real-time, remote, and portable imaging capabilities for visualizing anatomical boundaries and identifying pathological abnormalities within the tissue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib11" title="">11</a>]</cite>. Brightness-mode (B-mode) ultrasound emits acoustic waves at frequencies <math alttext="\geq" class="ltx_Math" display="inline" id="Sx1.p2.1.m1.1"><semantics id="Sx1.p2.1.m1.1a"><mo id="Sx1.p2.1.m1.1.1" xref="Sx1.p2.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="Sx1.p2.1.m1.1b"><geq id="Sx1.p2.1.m1.1.1.cmml" xref="Sx1.p2.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p2.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="Sx1.p2.1.m1.1d">≥</annotation></semantics></math> 20 kHz to construct a two-dimensional grayscale image representing the anatomy within the imaging plane <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib12" title="">12</a>]</cite>. Without the acoustic barrier of the vertebral bone after surgical decompression (e.g., laminectomy), clinicians can perform intraoperative imaging of the spinal cord with ultrasound. This information allows surgeons to verify sufficient decompression of the cord and evaluate the need for additional interventions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Compared to other widely accepted imaging modalities, like magnetic resonance imaging (MRI), computed tomography (CT), and conventional radiography (X-ray), ultrasound has benefits for rapid detection of tissue irregularities and injury management due to its safety, cost-effectiveness, and real-time imaging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib14" title="">14</a>]</cite>. The severity and location of SCI can be understood by studying the inflammation in the tissue and the development of a hematoma (i.e., pooling of blood in a tissue due to broken blood vessels from the injury). Detailed knowledge of the injury and its evolution can provide valuable insights on treatment efficacy and patient prognosis, as spinal cord compression, swelling, and hemorrhage are associated with poor prognosis of neurologic recovery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib16" title="">16</a>]</cite>. Furthermore, monitoring the swelling of the key anatomy surrounding the injury provides new avenues for researching the underlying physiological mechanisms of secondary injury. While ultrasound’s real-time capabilities make it a valuable tool for continuous tissue monitoring, its utility is often compromised by poor image quality, signal reverberation, artifacts, and speckle. These imaging challenges require the expertise of skilled sonographers or radiologists to ensure accurate interpretation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib18" title="">18</a>]</cite>. Unfortunately, these image quality issues are a significant contributor to inter-observer variability, which further complicates medical image evaluations. As a result, these factors are key drivers in diagnostic errors underscoring the critical need for medical image algorithms for improving diagnostic results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">To take full advantage of continuous imaging with ultrasound, there are several research efforts on wearable and implantable ultrasound based devices for clinical applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib23" title="">23</a>]</cite>. However, continuous evaluation of clinical metrics necessitates integrated computer vision algorithms to detect relevant features from ultrasound images. In this study, we explore the efficacy of deep learning models for automatic hematoma tracking and anatomical segmentation on our B-mode ultrasound dataset of 10,223 images of porcine spinal cords (N=25). To our knowledge, this is the first open source dataset showcasing the anatomy (dura, CSF, pia, spinal cord, vertebral structures) of both healthy and injured spinal cords. We benchmark the performance of several state-of-the-art object detection models to localize the site of injury (i.e., hematoma) within the ultrasound images, along with semantic segmentation algorithms for segmenting the spinal cord into its corresponding anatomy. In our analysis, we propose a new metric to evaluate how effectively each model can be deployed on wearable and implantable ultrasound devices to enable continuous monitoring of SCI. Finally, we explore the zero-shot generalization capabilities (i.e., model’s ability to adapt to previously unseen data) of these semantic segmentation models by evaluating the trained porcine models on human spinal cord images and discuss methods to improve these models for human clinical translation. Our work serves as a pioneering effort to automate continuous diagnostics in SCI, providing avenues for more personalized and proactive treatment to improve clinical outcomes. With this released dataset, we hope to promote both methodology development for improved computer vision on medical datasets, along with clinical investigations and applications in the domain of SCI.</p>
</div>
</section>
<section class="ltx_section" id="Sx2" lang="en">
<h2 class="ltx_title ltx_title_section">Previous Works</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Achieving high accuracy and generalizability in supervised computer vision in spinal cord images typically requires training data that are sufficiently dense and representative of the domain – a costly and time-consuming challenge in the medical field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib24" title="">24</a>]</cite>. In contrast to the vast databases of natural images, medical datasets are often scarce and noisy, presenting a substantial challenge for supervised deep learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib27" title="">27</a>]</cite>. In this section, we highlight some recent efforts for developing medical datasets of spinal cord images.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">Prados et al. provided axial MRI images of the cervical spine from 80 healthy subjects across 4 acquisition centers, with annotated white and gray matter segmentation masks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib28" title="">28</a>]</cite>. Another published MRI dataset includes spinal cord images from 260 participants to study quantitative metrics (e.g., cross-sectional area of the cord and white/gray matter) of the regions of interest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib29" title="">29</a>]</cite>. CTSpine1k provides 1,005 CT volumes with labeled vertebrae for bone segmentation across different spinal cord conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib30" title="">30</a>]</cite>. However, to our knowledge there are no existing open-source datasets of spinal cord ultrasound images. Unlike CT and MRI, ultrasound has the capability to provide real-time continuous imaging for a unique perspective on spinal cord pulsatility and soft-tissue distinction, allowing clinical and surgical decision-making.</p>
</div>
<div class="ltx_para" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1">There have been several studies which aim to use machine learning (ML) methods to automatically detect injury and segment the spinal cord. Research groups have proposed segment-based classification models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib32" title="">32</a>]</cite> and hybrid thresholding and convolutional neural network (CNN)-based approaches to identify the severity of SCI and predict disease patterns using the segmented features in an SCI MR image database <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib33" title="">33</a>]</cite>. Other groups have used deep-learning based techniques for lesion (e.g., cervical disc herniation, traumatic SCI) and compression detection within MR images of the cervical spine <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib35" title="">35</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Sx2.p4">
<p class="ltx_p" id="Sx2.p4.1">Spinal cord segmentation efforts include Paugum et al.’s neural network framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib36" title="">36</a>]</cite> and a spinal cord segmentation challenge to segment white and gray matter in spinal cord structures from axial MRI data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib28" title="">28</a>]</cite>. Another CNN-based study proposed a spinal cord segmentation model on 2D axial-view MRI slices from 20 patients (359 images) with cervical spondylotic myelopathy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib37" title="">37</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Sx2.p5">
<p class="ltx_p" id="Sx2.p5.1">While there are many computer vision techniques for spinal cord segmentation and disease identification, the published methods at present are limited to MRIs and CTs, which do not provide continuous imaging or detailed soft-tissue delineation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib38" title="">38</a>]</cite>. As the deep learning field increasingly focuses on video-based understanding, leveraging consecutive frames to enhance network constraints, ultrasound stands out as the optimal modality for AI methods due to its high frame rate for data acquisition. The only other effort for segmentation in ultrasound spinal cord is done by Benjdira et al., where they evaluate models on images (N=10) post-laminectomy. In their analysis, they tested 3 ML networks on axial slices of the spinal cord to benchmark their performance on outputting a mask of the spinal cord <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib39" title="">39</a>]</cite>. In our study, we aim to identify all relevant spinal cord anatomy (dura, CSF, pia, spinal cord) along with injury site within high resolution sagittal slices of the cord providing a longitudinal perspective of the spine for detecting conditions that extend over multiple vertebral levels. This dataset is publicly released to further the efforts for medical computer vision and automated injury monitoring.</p>
</div>
</section>
<section class="ltx_section" id="Sx3" lang="en">
<h2 class="ltx_title ltx_title_section">Methods</h2>
<section class="ltx_subsection" id="Sx3.SSx1">
<h3 class="ltx_title ltx_title_subsection">Dataset formation</h3>
<section class="ltx_subsubsection" id="Sx3.SSx1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Data acquisition</h4>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p1.1">Over the course of two years (2021 - 2022), ultrasound data were collected from 25 female Yorkshire pigs weighing approximately 50 pounds. All animal experiments were conducted in compliance with the National Institutes of Health guide for the care and use of laboratory animals (NIH Publications No. 8023, revised 1978). All animal methods were approved by the Johns Hopkins University Animal Care and Use Committee (SW20M221) and were performed following the ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments). To collect the <span class="ltx_text ltx_font_italic" id="Sx3.SSx1.SSSx1.p1.1.1">in vivo</span> ultrasound images in the dataset, a laminectomy was performed (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F1" title="Figure 1 ‣ Data acquisition ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>A) from the 4th to 6th thoracic vertebrae (T4 - T6) to provide an acoustic window (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F1" title="Figure 1 ‣ Data acquisition ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>B). Sagittal images were collected at different angles and locations of the spinal cord using a Canon Aplio i800 ultrasound system (Canon Medical Systems, Tustin, CA) connected to either an i22LH8 transducer (operating frequency: 20 MHz) or an i18LX5 transducer (operating frequency: 12 MHz) placed above the spinal cord (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F1" title="Figure 1 ‣ Data acquisition ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>C). A controlled injury was subsequently induced with a weight drop (either 20, 40, or 60 grams) from a height of 17 cm, and then images were recollected. Image acquisition parameters, such as spinal cord depth (distance of the spinal cord from the transducer), imaging angle, gain, brightness, and ultrasound signal coupling medium (saline solution), were varied to obtain an expressive and diverse dataset. These B-mode ultrasound images were collected by several different people during the data acquisition period, and were stored as Digital Imaging and Communications in Medicine (DICOM) files (the international standard for viewing and storing medical imaging information) for further preprocessing to make the final dataset (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F1" title="Figure 1 ‣ Data acquisition ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>D).</p>
</div>
<figure class="ltx_figure" id="Sx3.F1">
<p class="ltx_p ltx_align_center" id="Sx3.F1.1"><span class="ltx_text" id="Sx3.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="832" id="Sx3.F1.1.1.g1" src="extracted/5877334/fig1.png" width="1361"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Data collection of porcine spinal cord. (a) An aerial view of the female Yorkshire pig with a laminectomy to expose the spinal cord. (b) The spinous processes and lamina of the 4th to 6th thoracic vertebrae (T4-T6) are removed to provide an acoustic window and an injury is induced with a weight drop. (c) An i22LH8 transducer connected to Canon Aplio i800 ultrasound system is placed above the spinal cord to capture Brightness-mode (B-mode) images of the region of interest. (d) The resulting Digital Imaging and Communications in Medicine (DICOM) image is displayed on a personal computer, showcasing the dura, cerebrospinal fluid (CSF), pia, spinal cord, and the injury location (hematoma). The collected images are included in the final dataset for real-time injury localization and semantic segmentation.</figcaption>
</figure>
<div class="ltx_para" id="Sx3.SSx1.SSSx1.p2">
<p class="ltx_p" id="Sx3.SSx1.SSSx1.p2.1">Additionally, B-mode images of the human spinal cord were captured from 8 patients after a laminectomy was performed from the 11th thoracic level to the 1st lumbar level (T11 - L1). Each patient had varying spinal cord curvatures, with 6 undergoing a posterior vertebral column subtraction osteotomy (PVCSO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib40" title="">40</a>]</cite>. Similar to the animal data collection, an i18LX5 transducer connected to a Canon Aplio i800 ultrasound system was placed above the spinal cord and images were collected throughout different stages of the shortening procedure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib41" title="">41</a>]</cite>. These images were used to evaluate the semantic segmentation models that were trained on only the porcine spinal cord images. All human data collection methods were approved by the Johns Hopkins Institutional Review Board (IRB00273900). All human data collection methods were performed in accordance with the Declaration of Helsinki and were acquired with informed consent.</p>
</div>
<figure class="ltx_figure" id="Sx3.F2">
<p class="ltx_p ltx_align_center" id="Sx3.F2.1"><span class="ltx_text" id="Sx3.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="834" id="Sx3.F2.1.1.g1" src="extracted/5877334/fig2.png" width="2120"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Curation of the porcine and human ultrasound spinal cord dataset. (a) Sample pre-injury images from the porcine spinal cord dataset. These images included the primary anatomy of interest and did not suffer from severe noise or artifacts that would render the image difficult to interpret. (b) Sample post-injury images of the dataset that fulfilled the same inclusion criteria as the pre-injury images. The red bounding boxes indicate the hematoma. (c) Sample images that were excluded from the final dataset due to shadowing, noise, or artifacts that affect image quality and occlude the anatomy of interest. (d) Sample human spinal cord images that were used to test the generalizability of the segmentation models.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="Sx3.SSx1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Data description</h4>
<div class="ltx_para" id="Sx3.SSx1.SSSx2.p1">
<p class="ltx_p" id="Sx3.SSx1.SSSx2.p1.1">There were a total of 3 datasets acquired and created by our group as part of this study for: (1) injury localization in porcine images, (2) semantic segmentation in porcine images, (3) semantic segmentation in human images. After rejecting low quality images in which the spinal cord was occluded or corrupted with noise, the injury localization dataset was developed by randomly selecting 2,245 healthy and injured porcine spinal cord images from the collected data (N=23). The semantic segmentation dataset consists of 10,223 images divided into 4,467 healthy spinal cord images (N=20) and 5,756 images of injured spinal cord images (N=25). To reduce file size, the images (1280 × 960 pixels) were converted to a Portable Network Graphics (PNG) format, scaled to the depth of scan field of the ultrasound image, and cropped to include only the anatomy of interest (690 × 275 pixels) (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F2" title="Figure 2 ‣ Data acquisition ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>A-C). After preprocessing, each image shows 25 mm × 8 mm of the sagittal slice of the spinal cord. Finally, a third dataset consisting of 86 human spinal cord ultrasound images (N=8) was created and preprocessed to develop a unique test set for the semantic segmentation models. While these images did not include a hematoma within the cord, they showed the other anatomy of interest (i.e., dura, CSF, pia, spinal cord), as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F2" title="Figure 2 ‣ Data acquisition ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>D.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx3.SSx1.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Data annotation</h4>
<figure class="ltx_figure" id="Sx3.F3">
<p class="ltx_p ltx_align_center" id="Sx3.F3.1"><span class="ltx_text" id="Sx3.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="1087" id="Sx3.F3.1.1.g1" src="extracted/5877334/fig3.png" width="1379"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example images and their corresponding ground truth masks. (a) A typical spinal cord image displaying clear delineation between the anatomical boundaries. The bottom of the image corresponds to the ventral spinal cord. (b) A spinal cord image in which the induced injury caused swelling in the tissue, effectively blurring the delineation between the dura, cerebrospinal fluid (CSF), and the pia. To avoid mistakes in interpretation and inconsistencies in labelling, the region is annotated as the dura/pia complex. (c) In some images, noise and other intraoperative artifacts resulted in ambiguous delineation between the ventral dura and the ventral region. For these types of images, we label that anatomy as the dura/ventral complex.</figcaption>
</figure>
<div class="ltx_para" id="Sx3.SSx1.SSSx3.p1">
<p class="ltx_p" id="Sx3.SSx1.SSSx3.p1.1">The injury localization and anatomical segmentation tasks had different annotation processes. For injury localization, images with a hematoma present were labeled with a bounding box surrounding the injury. The ground truth image masks used for semantic segmentation were created in Computer Vision Annotation Tool (CVAT) (CVAT.ai Corporation, Palo Alto, CA). The anatomical structures of interest were the dura, CSF, pia, spinal cord, and hematoma (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F3" title="Figure 3 ‣ Data annotation ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>A). Additionally, the space above the dorsal dura, where the vertebral bone was removed, was labeled as the dorsal space, and the space below the ventral dura consisting of bony structures and cartilage was labeled as the ventral space. For some injuries, depending on the angle in which the ultrasound images were captured and the impact of the weight drop, the anatomical boundaries between the dura, CSF, and pia were unclear and could not be labeled with sufficient confidence. In order to maintain the integrity of the labels and minimize speculations during the labeling process, those regions were simplified to the dura/pia complex whenever necessary (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F3" title="Figure 3 ‣ Data annotation ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>B). Similarly, the dura/ventral complex label was used when there was ambiguity in the anatomical boundary between the ventral dura and the vertebral structures beneath it (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.F3" title="Figure 3 ‣ Data annotation ‣ Dataset formation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>C). The third dataset of human images was labeled using the same annotation process described above for the porcine spinal cord.</p>
</div>
<div class="ltx_para" id="Sx3.SSx1.SSSx3.p2">
<p class="ltx_p" id="Sx3.SSx1.SSSx3.p2.1">Images were labeled by a team of medical and graduate students trained by a board-certified radiologist. Each image in which there was a lack of clear delineation in anatomical boundaries was verified by the radiologist to ensure high quality and accurate labels. Once all the images were labeled, the masks were then validated by a neurosurgery spine fellow at the Johns Hopkins Hospital to provide a second round of verification and further ensure the robustness of the labels. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.T1" title="Table 1 ‣ Injury Localization ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of the image and pixel instances for each of these labels. These images and their corresponding annotations are available electronically on our <a class="ltx_ref ltx_href" href="https://github.com/HEPIUSLAB/ultrasound_spinal_cord_dataset.git" title=""><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx3.SSx1.SSSx3.p2.1.1.1">GitHub Respository</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Sx3.SSx2">
<h3 class="ltx_title ltx_title_subsection">Injury Localization</h3>
<div class="ltx_para" id="Sx3.SSx2.p1">
<p class="ltx_p" id="Sx3.SSx2.p1.1">Using the injury localization dataset of 2,245 images, we evaluated the performance of seven state-of-the-art object detection models: Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib42" title="">42</a>]</cite>, SSD300, SSD512 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib43" title="">43</a>]</cite>, RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib44" title="">44</a>]</cite>, Detection Transformer (DETR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib45" title="">45</a>]</cite>, YOLOv7 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib46" title="">46</a>]</cite>, and YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib47" title="">47</a>]</cite>. Faster RCNN is a two-stage detector where the first stage proposes candidate object bounding boxes with a Region Proposal Network, followed by a second stage that classifies the object and refines its bounding box <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib42" title="">42</a>]</cite>. Conversely, the SSD model divides the image into a grid and predicts bounding boxes and class probabilities for each grid cell, with SSD300 optimizing for speed with 300 × 300 pixel images, and SSD512 catering to higher-resolution 512 × 512 pixel images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib43" title="">43</a>]</cite>. RetinaNet uses a Feature Pyramid Network for building high-level semantic feature maps at multiple scales to efficiently detect objects of various sizes and introduces the Focal Loss function to address the challenge of foreground-background class imbalance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib44" title="">44</a>]</cite>. In contrast, DETR uniquely combines a CNN backbone with a transformer encoder-decoder architecture, employing learned positional embeddings and a shared feed-forward network for precise object class and bounding box predictions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib45" title="">45</a>]</cite>. YOLOv7, utilizes the Extended Efficient Layer Aggregation Network (E-ELAN) to optimize convergence and network efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib46" title="">46</a>]</cite>. Lastly, YOLOv8 utilizes an anchor-free architecture, enhancing detection accuracy across various object sizes and shapes, complemented by a versatile multi-scale prediction method for improved adaptability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib47" title="">47</a>]</cite>. More specific details on the models’ architectures can be found in their respective references. These models have exhibited high performance when applied to natural image datasets, like COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib26" title="">26</a>]</cite>, PASCAL VOC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib27" title="">27</a>]</cite>, and ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib25" title="">25</a>]</cite>. The learnable parameters of the model were initialized with pretrained weights and biases using the ImageNet dataset to reduce the training cost.</p>
</div>
<div class="ltx_para" id="Sx3.SSx2.p2">
<p class="ltx_p" id="Sx3.SSx2.p2.2">The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of <math alttext="\pm" class="ltx_Math" display="inline" id="Sx3.SSx2.p2.1.m1.1"><semantics id="Sx3.SSx2.p2.1.m1.1a"><mo id="Sx3.SSx2.p2.1.m1.1.1" xref="Sx3.SSx2.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p2.1.m1.1b"><csymbol cd="latexml" id="Sx3.SSx2.p2.1.m1.1.1.cmml" xref="Sx3.SSx2.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx2.p2.1.m1.1d">±</annotation></semantics></math> 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of <math alttext="\pm" class="ltx_Math" display="inline" id="Sx3.SSx2.p2.2.m2.1"><semantics id="Sx3.SSx2.p2.2.m2.1a"><mo id="Sx3.SSx2.p2.2.m2.1.1" xref="Sx3.SSx2.p2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p2.2.m2.1b"><csymbol cd="latexml" id="Sx3.SSx2.p2.2.m2.1.1.cmml" xref="Sx3.SSx2.p2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p2.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx2.p2.2.m2.1d">±</annotation></semantics></math> 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib48" title="">48</a>]</cite>. The main characteristics of each model are described in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.T2" title="Table 2 ‣ Injury Localization ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="Sx3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Pixel- and image-wise instances of each anatomical structure.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.1.1">Anatomy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.2.1">Pixel Instances</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx3.T1.1.1.1.3.1">Instances across Images</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx3.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.2.1.1">Dorsal Space</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.2.1.2">397,350,611</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.2.1.3">10,223</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.3.2.1">Dura</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.3.2.2">142,857,955</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.3.2.3">9,814</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.4.3.1">Pia</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.4.3.2">142,721,894</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.4.3.3">9,839</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.5.4.1">CSF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.5.4.2">118,161,757</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.5.4.3">9,613</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.6.5.1">Dura/Pia complex</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.6.5.2">57,124,376</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.6.5.3">2,732</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.7.6.1">Spinal cord</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.7.6.2">812,894,511</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.7.6.3">10,223</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.8.7.1">Hematoma</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.8.7.2">69,727,644</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.8.7.3">5,756</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.9.8.1">Dura/Ventral Complex</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.9.8.2">58,152,053</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.9.8.3">2,671</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.10.9.1">Ventral Space</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.10.9.2">89,571,059</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T1.1.10.9.3">6,099</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T1.1.11.10.1">Background</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T1.1.11.10.2">18,992,200</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T1.1.11.10.3">6,385</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Sx3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx3.T2.1" style="width:672.0pt;height:214.8pt;vertical-align:-2.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.3pt,11.8pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx3.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.2.1">Encoder</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.1.1.3.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.3.1.1.1.1"># of</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.3.1.2.1.1">Parameters</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.1.1.4.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.4.1.1.1.1">Learning</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.4.1.2.1.1">Rate</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.1.1.5.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.5.1.1.1.1">Batch</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.5.1.2.1.1">Size</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.1.1.6.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.1.1.6.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.6.1.1.1.1">Training</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.1.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.1.1.6.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.6.1.2.1.1">Epochs</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.7.1">Optimizer</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T2.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="Sx3.T2.1.1.1.1.8.1">Loss Function</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx3.T2.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.2.1.1">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.2.1.1.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.2.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.2.1.1.1.1.1">Faster</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.2.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.2.1.1.1.2.1">RCNN</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.2.1.2">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.2.1.3">41,299,161</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.2.1.4">0.004935</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.2.1.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.2.1.6">60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.2.1.7">SGD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.2.1.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.2.1.8.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.2.1.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.2.1.8.1.1.1">Cross Entropy + Smooth L1</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.3.2.1">SSD300</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.3.2.2">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.3.2.3">24,641,780</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.3.2.4">0.002571</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.3.2.5">32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.3.2.6">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.3.2.7">SGD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.3.2.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.3.2.8.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.3.2.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.3.2.8.1.1.1">Cross Entropy + Smooth L1</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.4.3.1">SSD512</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.4.3.2">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.4.3.3">24,641,780</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.4.3.4">0.000251</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.4.3.5">8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.4.3.6">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.4.3.7">SGD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.4.3.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.4.3.8.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.4.3.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.4.3.8.1.1.1">Cross Entropy + Smooth L1</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.5.4.1">RetinaNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.5.4.2">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.5.4.3">36,352,630</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.5.4.4">0.000099</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.5.4.5">8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.5.4.6">60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.5.4.7">SGD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.5.4.8">Focal Loss</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.6.5.1">DETR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.6.5.2">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.6.5.3">41,524,954</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.6.5.4">0.000011</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.6.5.5">8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.6.5.6">200</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.6.5.7">AdamW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.6.5.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.6.5.8.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.6.5.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.6.5.8.1.1.1">Cross Entropy + Smooth L1</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.7.6.1">YOLOv7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.7.6.2">E-ELAN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.7.6.3">37,196,556</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.7.6.4">0.000123</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.7.6.5">32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.7.6.6">75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.7.6.7">Adam</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T2.1.1.7.6.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.7.6.8.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.7.6.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.7.6.8.1.1.1">Binary Cross Entropy + Mean Square Error</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T2.1.1.8.7.1">YOLOv8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.1.8.7.2">CSPNet</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.1.8.7.3">25,856,899</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.1.8.7.4">0.000422</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.1.8.7.5">48</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.1.8.7.6">80</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.1.8.7.7">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.8.7.7.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.8.7.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.8.7.7.1.1.1">AdamW</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.8.7.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.8.7.7.1.2.1">+ SGD</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T2.1.1.8.7.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T2.1.1.8.7.8.1">
<tr class="ltx_tr" id="Sx3.T2.1.1.8.7.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.8.7.8.1.1.1">Binary Cross Entropy + Distribution Focal</td>
</tr>
<tr class="ltx_tr" id="Sx3.T2.1.1.8.7.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T2.1.1.8.7.8.1.2.1">Loss + Complete Intersection over Union</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="Sx3.SSx3">
<h3 class="ltx_title ltx_title_subsection">Spinal Cord Segmentation</h3>
<div class="ltx_para" id="Sx3.SSx3.p1">
<p class="ltx_p" id="Sx3.SSx3.p1.1">We evaluated six state-of-the-art semantic segmentation models on our dataset consisting of 10,223 porcine images annotated with masks delineating the anatomical structures. The specific models used were SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib49" title="">49</a>]</cite>, U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib50" title="">50</a>]</cite>, DeepLabv3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib51" title="">51</a>]</cite>, TransUNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib52" title="">52</a>]</cite>, Swin-UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib53" title="">53</a>]</cite>, and Segment Anything for medical images (SAMed) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib54" title="">54</a>]</cite>. SegFormer combines a transformer-based encoder to process global information and generate multiscale features, with a straightforward multilayer perceptron (MLP) decoder that integrates these features at various scales <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib49" title="">49</a>]</cite>. U-Net, renowned for its effectiveness in biomedical imaging, employs a contracting-expanding architecture of the convolutional layers (resulting in a u-shape) that balances spatial and feature information to ensure accurate localization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib50" title="">50</a>]</cite>. DeepLabv3 enhances multi-scale context capture through an Atrous Spatial Pyramid Pooling (ASPP) module, which uses dilated convolutions at various scales <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib51" title="">51</a>]</cite>. TransUNet leverages the global self-attention mechanisms of Transformers to augment the traditional U-Net’s convolutional approach, offering improved global information processing and localization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib52" title="">52</a>]</cite>. Swin-UNet adopts a Swin Transformer in both its encoder and decoder, utilizing shifted windows to enhance context feature extraction and spatial resolution restoration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib53" title="">53</a>]</cite>. Lastly, SAMed builds on the Segment Anything framework, specifically adapting its encoder and introducing a low-rank-based fine-tuning approach for medical image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib54" title="">54</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Sx3.SSx3.p2">
<p class="ltx_p" id="Sx3.SSx3.p2.1">The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [<math alttext="\pm" class="ltx_Math" display="inline" id="Sx3.SSx3.p2.1.m1.1"><semantics id="Sx3.SSx3.p2.1.m1.1a"><mo id="Sx3.SSx3.p2.1.m1.1.1" xref="Sx3.SSx3.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p2.1.m1.1b"><csymbol cd="latexml" id="Sx3.SSx3.p2.1.m1.1.1.cmml" xref="Sx3.SSx3.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx3.p2.1.m1.1d">±</annotation></semantics></math> 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib48" title="">48</a>]</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx3.T3" title="Table 3 ‣ Spinal Cord Segmentation ‣ Methods ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> describes the main characteristics of each semantic segmentation model.</p>
</div>
<figure class="ltx_table" id="Sx3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx3.T3.1" style="width:667.6pt;height:160.1pt;vertical-align:-2.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.6pt,4.1pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx3.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.2.1">Encoder</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T3.1.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T3.1.1.1.1.3.1">
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.3.1.1.1.1"># of</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.3.1.2.1.1">Parameters</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T3.1.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T3.1.1.1.1.4.1">
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.4.1.1.1.1">Learning</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.4.1.2.1.1">Rate</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T3.1.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T3.1.1.1.1.5.1">
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.5.1.1.1.1">Batch</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.5.1.2.1.1">Size</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T3.1.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T3.1.1.1.1.6.1">
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.1.1.6.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.6.1.1.1.1">Training</span></td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.1.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.1.1.6.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.6.1.2.1.1">Epochs</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T3.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.7.1">Optimizer</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Sx3.T3.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="Sx3.T3.1.1.1.1.8.1">Loss Function</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx3.T3.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T3.1.1.2.1.1">SegFormer</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.2.1.2">MiT-B5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.2.1.3">84,601,034</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.2.1.4">0.000972</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.2.1.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.2.1.6">75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.2.1.7">AdamW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.2.1.8">Cross Entropy</td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T3.1.1.3.2.1">U-Net</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.3.2.2">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.3.2.3">31,044,106</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.3.2.4">0.003011</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.3.2.5">8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.3.2.6">50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.3.2.7">SGD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.3.2.8">Cross Entropy</td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T3.1.1.4.3.1">DeepLabv3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.4.3.2">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.4.3.3">41,998,420</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.4.3.4">0.000334</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.4.3.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.4.3.6">100</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.4.3.7">AdamW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.4.3.8">Cross Entropy</td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T3.1.1.5.4.1">TransUNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.5.4.2">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T3.1.1.5.4.2.1">
<tr class="ltx_tr" id="Sx3.T3.1.1.5.4.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.5.4.2.1.1.1">ResNet50 + ViT_B16</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.5.4.3">105,323,306</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.5.4.4">0.004468</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.5.4.5">24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.5.4.6">200</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.5.4.7">SGD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.5.4.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T3.1.1.5.4.8.1">
<tr class="ltx_tr" id="Sx3.T3.1.1.5.4.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.5.4.8.1.1.1">Cross Entropy + Dice</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T3.1.1.6.5.1">Swin-UNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.6.5.2">Swin-T</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.6.5.3">27,153,156</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.6.5.4">0.061411</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.6.5.5">16</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.6.5.6">100</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.6.5.7">SGD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx3.T3.1.1.6.5.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T3.1.1.6.5.8.1">
<tr class="ltx_tr" id="Sx3.T3.1.1.6.5.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.6.5.8.1.1.1">Cross Entropy + Dice</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx3.T3.1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx3.T3.1.1.7.6.1">SAMed</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T3.1.1.7.6.2">SAM ViT-B</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T3.1.1.7.6.3">91,866,903</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T3.1.1.7.6.4">0.002840</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T3.1.1.7.6.5">24</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T3.1.1.7.6.6">200</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T3.1.1.7.6.7">AdamW</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx3.T3.1.1.7.6.8">
<table class="ltx_tabular ltx_align_middle" id="Sx3.T3.1.1.7.6.8.1">
<tr class="ltx_tr" id="Sx3.T3.1.1.7.6.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx3.T3.1.1.7.6.8.1.1.1">Cross Entropy + Dice</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="Sx3.SSx3.p3">
<p class="ltx_p" id="Sx3.SSx3.p3.1">Because the porcine and human spinal cord have similar anatomical structures and immune response after injury, we evaluated the semantic segmentation models on human spinal cord ultrasound images to understand whether models trained exclusively on porcine images can generalize with sufficient accuracy to human images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib55" title="">55</a>]</cite>. The annotated dataset containing the 86 human spinal cord images (N=8 patients) were used as the test set for the trained models to highlight the translatability of our trained models to clinical application.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx4" lang="en">
<h2 class="ltx_title ltx_title_section">Results</h2>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Injury Localization</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">After hyperparameter tuning, the performance of each model during inference is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx4.T4" title="Table 4 ‣ Injury Localization ‣ Results ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>, including the mean Average Precision (mAP) value, the Average Recall (AR) value, and speed using frames per second (FPS). mAP is a popular metric ranging from 0 to 1 that measures the accuracy of object detectors as it provides a comprehensive assessment of their performance, considering both precision (how accurate the detected objected are) and recall (how many relevant objects are detected) at multiple levels of confidence (i.e., 50%, and 50-95% at intervals of 5%). The levels of confidence, known as the Intersection over Union (IoU) thresholds, consider a detection accurate if the IoU of the ground truth bounding box and the predicted bounding box is greater than the set threshold (e.g., 0.5 for mAP50). The AR value is the recall metric averaged over a range of IoU threshold (0.5 - 1.0). From these results, it is evident that Faster RCNN and YOLOv8 show the strongest performance, achieving a mAP50 score of 0.985 and 0.979, respectively. These models also achieve the highest mAP50-95 score, which is a much more stringent metric for assessing model performance compared to mAP50, at 0.524 for Faster RCNN and 0.606 for YOLOv8. YOLOv8 also attains the highest AR score at 0.644.</p>
</div>
<figure class="ltx_table" id="Sx4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance of object detection models on unseen porcine spinal cord ultrasound images to detect the site of injury.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx4.T4.1" style="width:545.9pt;height:155pt;vertical-align:-2.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.2pt,19.1pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T4.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.1.2.1">mAP50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.1.3.1">mAP50-95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.1.4.1">AR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="Sx4.T4.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.1.5.1">CPU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="Sx4.T4.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.1.1.6.1">GPU</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.2.2.1.1">FPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.2.2.2.1">Load (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.2.2.3">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T4.1.1.2.2.3.1">
<tr class="ltx_tr" id="Sx4.T4.1.1.2.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T4.1.1.2.2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.2.2.3.1.1.1.1">Implantability</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.2.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T4.1.1.2.2.3.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.2.2.3.1.2.1.1">Score</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.2.2.4.1">FPS</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.2.2.5.1">Load (%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.2.2.6">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T4.1.1.2.2.6.1">
<tr class="ltx_tr" id="Sx4.T4.1.1.2.2.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T4.1.1.2.2.6.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.2.2.6.1.1.1.1">Implantability</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.2.2.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T4.1.1.2.2.6.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.2.2.6.1.2.1.1">Score</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.1">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T4.1.1.3.3.1.1">
<tr class="ltx_tr" id="Sx4.T4.1.1.3.3.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T4.1.1.3.3.1.1.1.1">Faster RCNN</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.2"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.3.3.2.1">0.985</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.3">0.524</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.4">0.594</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.5">1.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.6">33</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.7">0.676</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.8">14.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.9">62</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.3.3.10">0.613</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.1">SSD300</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.2">0.669</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.3">0.207</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.4">0.249</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.5">23.12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.6">36</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.7">0.735</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.8"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.4.4.8.1">147.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.9">27</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.4.4.10">0.766</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.1">SSD512</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.2">0.874</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.3">0.274</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.4">0.324</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.5"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.5.5.5.1">23.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.6">26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.7">0.866</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.8">125.03</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.9">18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.5.5.10">0.853</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.1">RetinaNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.2">0.912</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.3">0.264</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.4">0.426</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.5">1.89</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.6">32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.7">0.646</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.8">17.66</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.9">48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.6.6.10">0.616</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.1">DETR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.2">0.787</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.3">0.251</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.4">0.453</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.5">19.78</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.6">15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.7">0.812</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.8">114.35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.9">26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.7.7.10">0.772</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.1">YOLOv7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.2">0.923</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.3">0.439</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.4">0.499</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.5">19.54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.6">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.7">0.865</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.8">80.13</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.9">28</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T4.1.1.8.8.10">0.777</td>
</tr>
<tr class="ltx_tr" id="Sx4.T4.1.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.1">YOLOv8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.2">0.979</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.3"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.9.9.3.1">0.606</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.4"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.9.9.4.1">0.644</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.5">17.85</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.6">22</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.7"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.9.9.7.1">0.870</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.8">115.31</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.9">27</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T4.1.1.9.9.10"><span class="ltx_text ltx_font_bold" id="Sx4.T4.1.1.9.9.10.1">0.867</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="Sx4.SSx1.p2">
<p class="ltx_p" id="Sx4.SSx1.p2.1">In an effort to better assess the potential of incorporating these models into implantable or wearable devices, we propose a new metric, implantability score, that takes into account the model accuracy, speed, and computational load during inference. We evaluate this implantability score on both the CPU and the GPU with a weighted average of the mAP50 score, normalized average FPS, and average processing load, as shown in Equation (<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx4.E1" title="In Injury Localization ‣ Results ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
<table class="ltx_equation ltx_eqn_table" id="Sx4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math alttext="\textit{Implantability Score}=\frac{\textit{mAP50}}{2}+\frac{\textit{FPS}_{%
\textit{norm}}}{4}+\frac{1-\textit{Load}}{4}" class="ltx_Math" display="block" id="Sx4.E1.m1.1"><semantics id="Sx4.E1.m1.1a"><mrow id="Sx4.E1.m1.1.1" xref="Sx4.E1.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.2" xref="Sx4.E1.m1.1.1.2a.cmml">Implantability Score</mtext><mo id="Sx4.E1.m1.1.1.1" xref="Sx4.E1.m1.1.1.1.cmml">=</mo><mrow id="Sx4.E1.m1.1.1.3" xref="Sx4.E1.m1.1.1.3.cmml"><mfrac id="Sx4.E1.m1.1.1.3.2" xref="Sx4.E1.m1.1.1.3.2.cmml"><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.3.2.2" xref="Sx4.E1.m1.1.1.3.2.2a.cmml">mAP50</mtext><mn id="Sx4.E1.m1.1.1.3.2.3" xref="Sx4.E1.m1.1.1.3.2.3.cmml">2</mn></mfrac><mo id="Sx4.E1.m1.1.1.3.1" xref="Sx4.E1.m1.1.1.3.1.cmml">+</mo><mfrac id="Sx4.E1.m1.1.1.3.3" xref="Sx4.E1.m1.1.1.3.3.cmml"><msub id="Sx4.E1.m1.1.1.3.3.2" xref="Sx4.E1.m1.1.1.3.3.2.cmml"><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.3.3.2.2" xref="Sx4.E1.m1.1.1.3.3.2.2a.cmml">FPS</mtext><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.3.3.2.3" xref="Sx4.E1.m1.1.1.3.3.2.3a.cmml">norm</mtext></msub><mn id="Sx4.E1.m1.1.1.3.3.3" xref="Sx4.E1.m1.1.1.3.3.3.cmml">4</mn></mfrac><mo id="Sx4.E1.m1.1.1.3.1a" xref="Sx4.E1.m1.1.1.3.1.cmml">+</mo><mfrac id="Sx4.E1.m1.1.1.3.4" xref="Sx4.E1.m1.1.1.3.4.cmml"><mrow id="Sx4.E1.m1.1.1.3.4.2" xref="Sx4.E1.m1.1.1.3.4.2.cmml"><mn id="Sx4.E1.m1.1.1.3.4.2.2" xref="Sx4.E1.m1.1.1.3.4.2.2.cmml">1</mn><mo id="Sx4.E1.m1.1.1.3.4.2.1" xref="Sx4.E1.m1.1.1.3.4.2.1.cmml">−</mo><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.3.4.2.3" xref="Sx4.E1.m1.1.1.3.4.2.3a.cmml">Load</mtext></mrow><mn id="Sx4.E1.m1.1.1.3.4.3" xref="Sx4.E1.m1.1.1.3.4.3.cmml">4</mn></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx4.E1.m1.1b"><apply id="Sx4.E1.m1.1.1.cmml" xref="Sx4.E1.m1.1.1"><eq id="Sx4.E1.m1.1.1.1.cmml" xref="Sx4.E1.m1.1.1.1"></eq><ci id="Sx4.E1.m1.1.1.2a.cmml" xref="Sx4.E1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.2.cmml" xref="Sx4.E1.m1.1.1.2">Implantability Score</mtext></ci><apply id="Sx4.E1.m1.1.1.3.cmml" xref="Sx4.E1.m1.1.1.3"><plus id="Sx4.E1.m1.1.1.3.1.cmml" xref="Sx4.E1.m1.1.1.3.1"></plus><apply id="Sx4.E1.m1.1.1.3.2.cmml" xref="Sx4.E1.m1.1.1.3.2"><divide id="Sx4.E1.m1.1.1.3.2.1.cmml" xref="Sx4.E1.m1.1.1.3.2"></divide><ci id="Sx4.E1.m1.1.1.3.2.2a.cmml" xref="Sx4.E1.m1.1.1.3.2.2"><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.3.2.2.cmml" xref="Sx4.E1.m1.1.1.3.2.2">mAP50</mtext></ci><cn id="Sx4.E1.m1.1.1.3.2.3.cmml" type="integer" xref="Sx4.E1.m1.1.1.3.2.3">2</cn></apply><apply id="Sx4.E1.m1.1.1.3.3.cmml" xref="Sx4.E1.m1.1.1.3.3"><divide id="Sx4.E1.m1.1.1.3.3.1.cmml" xref="Sx4.E1.m1.1.1.3.3"></divide><apply id="Sx4.E1.m1.1.1.3.3.2.cmml" xref="Sx4.E1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="Sx4.E1.m1.1.1.3.3.2.1.cmml" xref="Sx4.E1.m1.1.1.3.3.2">subscript</csymbol><ci id="Sx4.E1.m1.1.1.3.3.2.2a.cmml" xref="Sx4.E1.m1.1.1.3.3.2.2"><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.3.3.2.2.cmml" xref="Sx4.E1.m1.1.1.3.3.2.2">FPS</mtext></ci><ci id="Sx4.E1.m1.1.1.3.3.2.3a.cmml" xref="Sx4.E1.m1.1.1.3.3.2.3"><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.3.3.2.3.cmml" mathsize="70%" xref="Sx4.E1.m1.1.1.3.3.2.3">norm</mtext></ci></apply><cn id="Sx4.E1.m1.1.1.3.3.3.cmml" type="integer" xref="Sx4.E1.m1.1.1.3.3.3">4</cn></apply><apply id="Sx4.E1.m1.1.1.3.4.cmml" xref="Sx4.E1.m1.1.1.3.4"><divide id="Sx4.E1.m1.1.1.3.4.1.cmml" xref="Sx4.E1.m1.1.1.3.4"></divide><apply id="Sx4.E1.m1.1.1.3.4.2.cmml" xref="Sx4.E1.m1.1.1.3.4.2"><minus id="Sx4.E1.m1.1.1.3.4.2.1.cmml" xref="Sx4.E1.m1.1.1.3.4.2.1"></minus><cn id="Sx4.E1.m1.1.1.3.4.2.2.cmml" type="integer" xref="Sx4.E1.m1.1.1.3.4.2.2">1</cn><ci id="Sx4.E1.m1.1.1.3.4.2.3a.cmml" xref="Sx4.E1.m1.1.1.3.4.2.3"><mtext class="ltx_mathvariant_italic" id="Sx4.E1.m1.1.1.3.4.2.3.cmml" xref="Sx4.E1.m1.1.1.3.4.2.3">Load</mtext></ci></apply><cn id="Sx4.E1.m1.1.1.3.4.3.cmml" type="integer" xref="Sx4.E1.m1.1.1.3.4.3">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.E1.m1.1c">\textit{Implantability Score}=\frac{\textit{mAP50}}{2}+\frac{\textit{FPS}_{%
\textit{norm}}}{4}+\frac{1-\textit{Load}}{4}</annotation><annotation encoding="application/x-llamapun" id="Sx4.E1.m1.1d">Implantability Score = divide start_ARG mAP50 end_ARG start_ARG 2 end_ARG + divide start_ARG FPS start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT end_ARG start_ARG 4 end_ARG + divide start_ARG 1 - Load end_ARG start_ARG 4 end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Sx4.SSx1.p2.2">The mAP50 score is an accuracy metric between 0 and 1. This is weighted twice as much as the other metrics to determine the implantability score as this has a significant impact on the diagnostic capability of the implant. The FPS is normalized to a metric between 0 and 1 by dividing all results with an FPS<sub class="ltx_sub" id="Sx4.SSx1.p2.2.1">max</sub> value. The FPS<sub class="ltx_sub" id="Sx4.SSx1.p2.2.2">max</sub> is set to the highest recorded FPS by any model on our CPU (24 FPS). To adapt the FPS normalization process for GPU applications, which generally have much higher speed, the FPS<sub class="ltx_sub" id="Sx4.SSx1.p2.2.3">max</sub> is set to the highest speed across all the evaluated models (148 FPS). Finally, because lower computational load is more desirable in this context, we use the the inverted average CPU or GPU load (1-Load), which is the percentage increase of computational power during inference, for calculating the score. Our results indicate that YOLOv8 has optimal characteristics for injury localization for continuous monitoring with ultrasound-based implants for both CPU and GPU based applications, with a CPU implantability score of 0.870 and GPU implantability score of 0.867.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Spinal Cord Segmentation</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx4.T5" title="Table 5 ‣ Spinal Cord Segmentation ‣ Results ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.</p>
</div>
<figure class="ltx_figure" id="Sx4.F4">
<p class="ltx_p ltx_align_center" id="Sx4.F4.1"><span class="ltx_text" id="Sx4.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="1969" id="Sx4.F4.1.1.g1" src="extracted/5877334/fig4.png" width="1373"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Visualization of the semantic segmentation models’ performance overlaid on example porcine images.</figcaption>
</figure>
<div class="ltx_para" id="Sx4.SSx2.p2">
<p class="ltx_p" id="Sx4.SSx2.p2.1">The class-wise IoU scores and Dice scores are included in the supplementary material (Supplementary Table 14 - 15). The metrics are evaluated on both unseen porcine images and human images, which the model had not been exposed to during the training process. To assess the potential for clinical translation, the zero-shot generalizability of these models on human spinal cord images is measured, including the MIoU and Mean Dice scores for the entire anatomy, along with class-wise scores (Supplementary Table 16 - 18). The implantable score is again computed using Equation (<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx4.E1" title="In Injury Localization ‣ Results ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>), replacing mAP50 with Mean Dice.</p>
</div>
<div class="ltx_para" id="Sx4.SSx2.p3">
<p class="ltx_p" id="Sx4.SSx2.p3.1">Our results indicate that DeepLabv3 outperforms all other segmentation models in terms of accuracy on porcine anatomy, with a Mean Dice score of 0.587, and SAMed generalizes best to human anatomy, achieving a Mean Dice score of 0.445. TransUNet generalizes the best to human spinal cord, with a Dice coefficient of 0.853 for the spinal cord class. Taking accuracy, speed, and computational load into account to determine the model’s implantable score (i.e., potential for deployment on an implantable or wearable device), SwinUNet outperforms all other models for CPU-based chips with an implantability score of 0.699 and DeepLabv3 achieves the highest score for GPU-based devices at 0.702. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx4.F4" title="Figure 4 ‣ Spinal Cord Segmentation ‣ Results ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> depicts the predicted segmentation masks for each model on an unseen porcine spinal cord image, along with the original ground truth image and mask.</p>
</div>
<figure class="ltx_table" id="Sx4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx4.T5.1" style="width:613.6pt;height:155pt;vertical-align:-2.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-76.7pt,19.1pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx4.T5.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T5.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="Sx4.T5.1.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T5.1.1.1.1.2.1">
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.2.1.1.1.1">Porcine</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.1.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.2.1.2.1.1">anatomy</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="Sx4.T5.1.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T5.1.1.1.1.3.1">
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.3.1.1.1.1">Porcine</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.3.1.2.1.1">spinal cord</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="Sx4.T5.1.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T5.1.1.1.1.4.1">
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.4.1.1.1.1">Human</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.4.1.2.1.1">anatomy</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="Sx4.T5.1.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T5.1.1.1.1.5.1">
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.5.1.1.1.1">Human</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.5.1.2.1.1">spinal cord</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="Sx4.T5.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.6.1">CPU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="Sx4.T5.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.1.1.7.1">GPU</span></th>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.1.1">MIoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.2.1">Dice</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.3.1">IoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.4.1">Dice</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.5.1">MIoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.6.1">Dice</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.7.1">IoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.8"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.8.1">Dice</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.9"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.9.1">FPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.10">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T5.1.1.2.2.10.1">
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.2.2.10.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.10.1.1.1.1">Load</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.2.2.10.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.10.1.2.1.1">(%)</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.11">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T5.1.1.2.2.11.1">
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2.11.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.2.2.11.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.11.1.1.1.1">Implantability</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2.11.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.2.2.11.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.11.1.2.1.1">Score</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.12"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.12.1">FPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.13">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T5.1.1.2.2.13.1">
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2.13.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.2.2.13.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.13.1.1.1.1">Load</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2.13.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.2.2.13.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.13.1.2.1.1">(%)</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.2.2.14">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T5.1.1.2.2.14.1">
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2.14.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.2.2.14.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.14.1.1.1.1">Implantability</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.2.2.14.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="Sx4.T5.1.1.2.2.14.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.2.2.14.1.2.1.1">Score</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.1">SegFormer</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.2">0.493</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.3">0.570</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.4">0.906</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.5">0.950</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.6">0.232</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.7">0.308</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.8">0.666</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.9">0.773</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.10">3.66</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.11">23</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.12">0.548</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.13">23.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.14">45</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.3.3.15">0.513</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.1">U-Net</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.2">0.476</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.3">0.553</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.4">0.867</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.5">0.928</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.6">0.253</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.7">0.349</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.8">0.609</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.9">0.722</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.10">6.06</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.11">32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.12">0.563</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.13">62.37</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.14">41</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.4.4.15">0.668</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.1">DeepLabv3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.2"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.5.5.2.1">0.515</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.5.5.3.1">0.587</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.4">0.910</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.5">0.952</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.6">0.200</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.7">0.289</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.8">0.506</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.9">0.656</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.10">4.76</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.11">27</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.12">0.568</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.13">64.11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.14">35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.5.5.15"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.5.5.15.1">0.702</span></td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.1">TransUNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.2">0.500</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.3">0.573</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.4"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.6.6.4.1">0.921</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.5"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.6.6.5.1">0.958</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.6">0.298</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.7">0.388</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.8"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.6.6.8.1">0.758</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.9"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.6.6.9.1">0.853</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.10">4.33</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.11">35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.12">0.532</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.13">40.85</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.14">34</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.6.6.15">0.609</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.1">SwinUNet</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.2">0.490</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.3">0.562</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.4">0.913</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.5">0.954</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.6">0.309</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.7">0.401</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.8">0.692</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.9">0.783</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.10">12.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.11">31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.12"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.7.7.12.1">0.699</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.13">63.36</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.14">34</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx4.T5.1.1.7.7.15">0.690</td>
</tr>
<tr class="ltx_tr" id="Sx4.T5.1.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.1">SAMed</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.2">0.497</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.3">0.574</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.4">0.908</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.5">0.951</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.6"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.8.8.6.1">0.347</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.7"><span class="ltx_text ltx_font_bold" id="Sx4.T5.1.1.8.8.7.1">0.445</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.8">0.616</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.9">0.740</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.10">5.40</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.11">37</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.12">0.535</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.13">29.43</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.14">35</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="Sx4.T5.1.1.8.8.15">0.563</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="Sx5" lang="en">
<h2 class="ltx_title ltx_title_section">Discussion</h2>
<div class="ltx_para" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">Deep learning is a promising tool for medical image processing for computer-aided diagnostics in SCI. However, due to the lack of medical image data and their corresponding semantic labels, large-scale computer vision models have been underutilized in this context. With this study, we establish an avenue for automatic and continuous monitoring in SCI with our large-scale ultrasound dataset. After benchmarking several object detection and semantic segmentation models, our results show that YOLOv8 and DeepLabv3 are best suited for hematoma tracking and anatomical segmentation, respectively. While it can be challenging to understand the exact reason why some models outperform others, it can be expected that YOLOv8 exhibits the highest performance due to a few distinct characteristics. YOLOv8 incorporates advanced feature aggregation techniques that help in combining features from different scales. This multi-scale approach is particularly useful in medical imaging where the size and shape of the injury can vary significantly. Moreover, YOLOv8 uses mosaic data augmentation, which mixes 4 images together, to provide the model with improved contextual information. Additionally, it is anticipated that YOLOv8’s anchor-free detection scheme, in which the model directly predicts an object’s mid-point, improves generalization and inference speed for our custom dataset. For semantic segmentation, DeepLabv3 may exhibit the highest accuracy on our dataset because of the atrous convolution approach along with atrous spatial pyramid pooling for capturing multi-scale contextual information without losing resolution. This architecture enables DeepLabv3 to effectively handle boundary information and delineate object boundaries more accurately, which is crucial for our soft-tissue segmentation task.</p>
</div>
<div class="ltx_para" id="Sx5.p2">
<p class="ltx_p" id="Sx5.p2.1">The algorithms were evaluated specifically for deployment on implantable or wearable ultrasound devices, which are ideal for long-term continuous diagnostics during secondary injury. Unlike the current clinical standard, where patient health monitoring through anatomical imaging is limited, our new dataset and benchmarking results enable a new paradigm for computer-aided diagnostics in SCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib6" title="">6</a>]</cite>. With injury localization and anatomical segmentation, we can track injury progression and inflammation in tissue to understand patient health trajectory and optimally titrate interventional therapies (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx5.F5" title="Figure 5 ‣ Discussion ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>). There are already several academic efforts on wearable and implantable ultrasound for clinical settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib23" title="">23</a>]</cite>, and growing research interest in exploring innovative applications and methodologies at the intersection of wearable technologies and machine learning algorithms with over 40,000 papers published in 2023 with the keywords "wearable" and "ML" <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib56" title="">56</a>]</cite>. Moreover, ML approaches can analyze long-term patterns from continuous data streams to uncover patterns that humans cannot due to the complexity and sheer volume of the data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib58" title="">58</a>]</cite>. These advantages can facilitate easier integration of ultrasound into robotic and tele-operative systems for remote diagnosis, presenting novel avenues for augmenting clinical insights and potentially improving the accessibility of medical care <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib59" title="">59</a>]</cite>. With this quantitative approach for image analysis, ML can also mitigate the issue of inter-observer variability. Furthermore, medical image segmentation of the spinal cord can aid in patient-specific computer simulations in the spinal cord to facilitate investigations in novel treatment paradigms using therapeutic focused ultrasound <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib61" title="">61</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="Sx5.F5">
<p class="ltx_p ltx_align_center" id="Sx5.F5.1"><span class="ltx_text" id="Sx5.F5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="1598" id="Sx5.F5.1.1.g1" src="extracted/5877334/fig5.png" width="1040"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>A comparison of the current standard of care for spinal cord injury with our approach using ultrasound imaging and deep learning for automatic diagnostics. Current treatment approaches do not provide a comprehensive and continuous avenue for monitoring patient spinal cord health after surgical decompression. With automatic injury localization and spinal cord segmentation, clinicians can capture changing biomarkers such as hematoma progression and tissue inflammation to personalize and better understand treatment approach.</figcaption>
</figure>
<div class="ltx_para" id="Sx5.p3">
<p class="ltx_p" id="Sx5.p3.1">It is important to note that there are some limitations to our study which we plan to improve in future works. Firstly, each segmentation model was pretrained on the ImageNet dataset, which contains millions of RGB images, before fine-tuning with our ultrasound dataset. We expect that fine-tuning with these grayscale medical images may cause some performance degradation in the resulting accuracy, which pretraining on a large medical dataset can improve. With this approach, the underlying learned features can have some foundational similarities with the dataset of interest. One such example of this is the Synapse multi-organ dataset, which contains CT images across several organs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib62" title="">62</a>]</cite>. Checkpoints for specific models trained on medical images may also be applicable here <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib63" title="">63</a>]</cite>. Deep learning algorithms pretrained on ImageNet may learn to make decisions of boundaries between different segmentation regions based on the variance of intensity, which is reasonable in natural images, but less suitable for medical images. This is because medical images are primarily grayscale, which can result in lower intensity variance compared to the rich color information in natural images. The reliability and generalizability of deep learning models is also curtailed by the issue that images were generated by only two different types of transducers at a single collection site (T5), which can be improved with more diverse data collection processes. The accuracy of these models on the test may also increase with augmentation techniques specific for B-mode ultrasound images that use physics-inspired transformations, including deformation, reverb, and signal-to-noise ratio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib65" title="">65</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="Sx5.F6">
<p class="ltx_p ltx_align_center" id="Sx5.F6.1"><span class="ltx_text" id="Sx5.F6.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="960" id="Sx5.F6.1.1.g1" src="extracted/5877334/fig6.png" width="1230"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visualization of SwinUNET’s performance on an example porcine image and example human image.</figcaption>
</figure>
<div class="ltx_para" id="Sx5.p4">
<p class="ltx_p" id="Sx5.p4.1">Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice <math alttext="&gt;" class="ltx_Math" display="inline" id="Sx5.p4.1.m1.1"><semantics id="Sx5.p4.1.m1.1a"><mo id="Sx5.p4.1.m1.1.1" xref="Sx5.p4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Sx5.p4.1.m1.1b"><gt id="Sx5.p4.1.m1.1.1.cmml" xref="Sx5.p4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Sx5.p4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="Sx5.p4.1.m1.1d">&gt;</annotation></semantics></math> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx5.F6" title="Figure 6 ‣ Discussion ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>). This may have a significant impact on the MIoU and Dice scores in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#Sx4.T5" title="Table 5 ‣ Spinal Cord Segmentation ‣ Results ‣ A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into "synthetic" human data, enhancing the model’s ability to generalize across species <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib67" title="">67</a>]</cite>. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib55" title="">55</a>]</cite>. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16441v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="Sx6" lang="en">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">The deployment of artificial intelligence and computer vision in ultrasound image analysis holds remarkable potential for streamlining diagnostics, with significant enhancements in image quality, diagnostic precision, and accessibility. However, effectively training deep learning models for this purpose presents various challenges, including the need for large, high-quality annotated datasets. We hope that publicly releasing this unique dataset will further facilitate computer vision efforts in medical imaging. By automating diagnostics within ultrasound, clinical workflows for personalized treatment paradigms can be augmented without overburdening clinicians. The significance of this dataset and research effort is highlighted by ultrasound’s unique capability for real-time and noninvasive imaging, providing continuous insights on patient health. With the rapidly evolving field of ultrasound imaging, enabling high-resolution and dense datasets, the benefits of deep learning can be realized across diverse healthcare settings.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Liu, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Spinal cord injury: global burden from 1990 to 2019 and projections up to 2030 using bayesian age-period-cohort analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.2.1"><span class="ltx_ERROR undefined" id="bib.bib1.2.1.1">\JournalTitle</span>Frontiers in Neurology</em> <span class="ltx_text ltx_font_bold" id="bib.bib1.3.2">14</span>, 1304153 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ding, W. <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Spinal cord injury: the global incidence, prevalence, and disability from the global burden of disease study 2019.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.2.1"><span class="ltx_ERROR undefined" id="bib.bib2.2.1.1">\JournalTitle</span>Spine</em> <span class="ltx_text ltx_font_bold" id="bib.bib2.3.2">47</span>, 1532–1540 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ahuja, C. S. <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Traumatic spinal cord injury.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.2.1"><span class="ltx_ERROR undefined" id="bib.bib3.2.1.1">\JournalTitle</span>Nature reviews Disease primers</em> <span class="ltx_text ltx_font_bold" id="bib.bib3.3.2">3</span>, 1–21 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Hwang, B. Y. <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Ultrasound in traumatic spinal cord injury: a wide-open field.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.2.1"><span class="ltx_ERROR undefined" id="bib.bib4.2.1.1">\JournalTitle</span>Neurosurgery</em> <span class="ltx_text ltx_font_bold" id="bib.bib4.3.2">89</span>, 372–382 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Tsehay, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Advances in monitoring for acute spinal cord injury: a narrative review of current literature.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.2.1"><span class="ltx_ERROR undefined" id="bib.bib5.2.1.1">\JournalTitle</span>The Spine Journal</em> <span class="ltx_text ltx_font_bold" id="bib.bib5.3.2">22</span>, 1372–1387 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Manbachi, A. <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Intraoperative ultrasound to monitor spinal cord blood flow after spinal cord injury.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.1">Medical Imaging 2020: Biomedical Applications in Molecular, Structural, and Functional Imaging</em>, vol. 11317, 58–65 (SPIE, 2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Martirosyan, N. L. <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Cerebrospinal fluid drainage and induced hypertension improve spinal cord perfusion after acute spinal cord injury in pigs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.2.1"><span class="ltx_ERROR undefined" id="bib.bib7.2.1.1">\JournalTitle</span>Neurosurgery</em> <span class="ltx_text ltx_font_bold" id="bib.bib7.3.2">76</span>, 461–469 (2015).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Horn, E. M. <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>
</span>
<span class="ltx_bibblock">The effects of intrathecal hypotension on tissue perfusion and pathophysiological outcome after acute spinal cord injury.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.2.1"><span class="ltx_ERROR undefined" id="bib.bib8.2.1.1">\JournalTitle</span>Neurosurgical focus</em> <span class="ltx_text ltx_font_bold" id="bib.bib8.3.2">25</span>, E12 (2008).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Schuhmann, M. K., Stoll, G., Bohr, A., Volkmann, J. &amp; Fluri, F.

</span>
<span class="ltx_bibblock">Electrical stimulation of the mesencephalic locomotor region attenuates neuronal loss and cytokine expression in the perifocal region of photothrombotic stroke in rats.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1"><span class="ltx_ERROR undefined" id="bib.bib9.1.1.1">\JournalTitle</span>International Journal of Molecular Sciences</em> <span class="ltx_text ltx_font_bold" id="bib.bib9.2.2">20</span>, 2341 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Whiting, A. C. <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Posterior reversible encephalopathic syndrome in the setting of induced elevated mean arterial pressure in patients with spinal cord injury.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.2.1"><span class="ltx_ERROR undefined" id="bib.bib10.2.1.1">\JournalTitle</span>Neurosurgery</em> <span class="ltx_text ltx_font_bold" id="bib.bib10.3.2">83</span>, 16–21 (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Lee, W. &amp; Roh, Y.

</span>
<span class="ltx_bibblock">Ultrasonic transducers for medical diagnostic imaging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1"><span class="ltx_ERROR undefined" id="bib.bib11.1.1.1">\JournalTitle</span>Biomedical engineering letters</em> <span class="ltx_text ltx_font_bold" id="bib.bib11.2.2">7</span>, 91–97 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Conlon, T. W. <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Moving beyond the stethoscope: diagnostic point-of-care ultrasound in pediatric practice.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.2.1"><span class="ltx_ERROR undefined" id="bib.bib12.2.1.1">\JournalTitle</span>Pediatrics</em> <span class="ltx_text ltx_font_bold" id="bib.bib12.3.2">144</span> (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Aarabi, B. <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Proposal of a management algorithm to predict the need for expansion duraplasty in american spinal injury association impairment scale grades a–c traumatic cervical spinal cord injury patients.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.2.1"><span class="ltx_ERROR undefined" id="bib.bib13.2.1.1">\JournalTitle</span>Journal of neurotrauma</em> <span class="ltx_text ltx_font_bold" id="bib.bib13.3.2">39</span>, 1716–1726 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kumar, A. <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Visualizing tactile feedback: an overview of current technologies with a focus on ultrasound elastography.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.2.1"><span class="ltx_ERROR undefined" id="bib.bib14.2.1.1">\JournalTitle</span>Frontiers in Medical Technology</em> <span class="ltx_text ltx_font_bold" id="bib.bib14.3.2">5</span> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Miyanji, F., Furlan, J. C., Aarabi, B., Arnold, P. M. &amp; Fehlings, M. G.

</span>
<span class="ltx_bibblock">Acute cervical traumatic spinal cord injury: Mr imaging findings correlated with neurologic outcome—prospective study with 100 consecutive patients.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1"><span class="ltx_ERROR undefined" id="bib.bib15.1.1.1">\JournalTitle</span>Radiology</em> <span class="ltx_text ltx_font_bold" id="bib.bib15.2.2">243</span>, 820–827 (2007).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Al-Habib, A. F. <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Clinical predictors of recovery after blunt spinal cord trauma: systematic review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.2.1"><span class="ltx_ERROR undefined" id="bib.bib16.2.1.1">\JournalTitle</span>Journal of neurotrauma</em> <span class="ltx_text ltx_font_bold" id="bib.bib16.3.2">28</span>, 1431–1443 (2011).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Ostras, O., Soulioti, D. E. &amp; Pinton, G.

</span>
<span class="ltx_bibblock">Diagnostic ultrasound imaging of the lung: A simulation approach based on propagation and reverberation in the human body.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1"><span class="ltx_ERROR undefined" id="bib.bib17.1.1.1">\JournalTitle</span>The Journal of the Acoustical Society of America</em> <span class="ltx_text ltx_font_bold" id="bib.bib17.2.2">150</span>, 3904–3913 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Long, J., Long, W., Bottenus, N. &amp; Trahey, G.

</span>
<span class="ltx_bibblock">Coherence-based quantification of acoustic clutter sources in medical ultrasound.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1"><span class="ltx_ERROR undefined" id="bib.bib18.1.1.1">\JournalTitle</span>The Journal of the Acoustical Society of America</em> <span class="ltx_text ltx_font_bold" id="bib.bib18.2.2">148</span>, 1051–1062 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Itri, J. N., Tappouni, R. R., McEachern, R. O., Pesch, A. J. &amp; Patel, S. H.

</span>
<span class="ltx_bibblock">Fundamentals of diagnostic error in imaging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1"><span class="ltx_ERROR undefined" id="bib.bib19.1.1.1">\JournalTitle</span>Radiographics</em> <span class="ltx_text ltx_font_bold" id="bib.bib19.2.2">38</span>, 1845–1865 (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Pinto, A. &amp; Brunese, L.

</span>
<span class="ltx_bibblock">Spectrum of diagnostic errors in radiology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1"><span class="ltx_ERROR undefined" id="bib.bib20.1.1.1">\JournalTitle</span>World journal of radiology</em> <span class="ltx_text ltx_font_bold" id="bib.bib20.2.2">2</span>, 377 (2010).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
La, T.-G. &amp; Le, L. H.

</span>
<span class="ltx_bibblock">Flexible and wearable ultrasound device for medical applications: A review on materials, structural designs, and current challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1"><span class="ltx_ERROR undefined" id="bib.bib21.1.1.1">\JournalTitle</span>Advanced Materials Technologies</em> <span class="ltx_text ltx_font_bold" id="bib.bib21.2.2">7</span>, 2100798 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Hu, H. <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Stretchable ultrasonic arrays for the three-dimensional mapping of the modulus of deep tissue.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.2.1"><span class="ltx_ERROR undefined" id="bib.bib22.2.1.1">\JournalTitle</span>Nature Biomedical Engineering</em> 1–14 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Liu, H.-C. <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Wearable bioadhesive ultrasound shear wave elastography.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.2.1"><span class="ltx_ERROR undefined" id="bib.bib23.2.1.1">\JournalTitle</span>Science Advances</em> <span class="ltx_text ltx_font_bold" id="bib.bib23.3.2">10</span>, eadk8426 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Tajbakhsh, N. <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.2.1"><span class="ltx_ERROR undefined" id="bib.bib24.2.1.1">\JournalTitle</span>Medical Image Analysis</em> <span class="ltx_text ltx_font_bold" id="bib.bib24.3.2">63</span>, 101693 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Deng, J. <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.1">2009 IEEE conference on computer vision and pattern recognition</em>, 248–255 (Ieee, 2009).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lin, T.-Y. <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, 740–755 (Springer, 2014).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Everingham, M., Van Gool, L., Williams, C. K., Winn, J. &amp; Zisserman, A.

</span>
<span class="ltx_bibblock">The pascal visual object classes (voc) challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1"><span class="ltx_ERROR undefined" id="bib.bib27.1.1.1">\JournalTitle</span>International journal of computer vision</em> <span class="ltx_text ltx_font_bold" id="bib.bib27.2.2">88</span>, 303–338 (2010).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Prados, F. <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Spinal cord grey matter segmentation challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.2.1"><span class="ltx_ERROR undefined" id="bib.bib28.2.1.1">\JournalTitle</span>Neuroimage</em> <span class="ltx_text ltx_font_bold" id="bib.bib28.3.2">152</span>, 312–329 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Cohen-Adad, J. <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Open-access quantitative mri data of the spinal cord and reproducibility across participants, sites and manufacturers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.2.1"><span class="ltx_ERROR undefined" id="bib.bib29.2.1.1">\JournalTitle</span>Scientific Data</em> <span class="ltx_text ltx_font_bold" id="bib.bib29.3.2">8</span>, 219 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Deng, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Ctspine1k: A large-scale dataset for spinal vertebrae segmentation in computed tomography.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.2.1"><span class="ltx_ERROR undefined" id="bib.bib30.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2105.14711</em> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Ahammad, S. H., Rajesh, V. &amp; Rahman, M. Z. U.

</span>
<span class="ltx_bibblock">Fast and accurate feature extraction-based segmentation framework for spinal cord injury severity classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1"><span class="ltx_ERROR undefined" id="bib.bib31.1.1.1">\JournalTitle</span>IEEE Access</em> <span class="ltx_text ltx_font_bold" id="bib.bib31.2.2">7</span>, 46092–46103 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Ahammad, S. H., Rahman, M. Z. U., Lay-Ekuakille, A. &amp; Giannoccaro, N. I.

</span>
<span class="ltx_bibblock">An efficient optimal threshold-based segmentation and classification model for multi-level spinal cord injury detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">2020 IEEE international symposium on medical measurements and applications (MeMeA)</em>, 1–6 (IEEE, 2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ahammad, S. H., Rajesh, V., Rahman, M. Z. U. &amp; Lay-Ekuakille, A.

</span>
<span class="ltx_bibblock">A hybrid cnn-based segmentation and boosting classifier for real time sensor spinal cord injury data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1"><span class="ltx_ERROR undefined" id="bib.bib33.1.1.1">\JournalTitle</span>IEEE Sensors Journal</em> <span class="ltx_text ltx_font_bold" id="bib.bib33.2.2">20</span>, 10092–10101 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ma, S., Huang, Y., Che, X. &amp; Gu, R.

</span>
<span class="ltx_bibblock">Faster rcnn-based detection of cervical spinal cord injury and disc degeneration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1"><span class="ltx_ERROR undefined" id="bib.bib34.1.1.1">\JournalTitle</span>Journal of applied clinical medical physics</em> <span class="ltx_text ltx_font_bold" id="bib.bib34.2.2">21</span>, 235–243 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Merali, Z. <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">et al.</em>
</span>
<span class="ltx_bibblock">A deep learning model for detection of cervical spinal cord compression in mri scans.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.2.1"><span class="ltx_ERROR undefined" id="bib.bib35.2.1.1">\JournalTitle</span>Scientific reports</em> <span class="ltx_text ltx_font_bold" id="bib.bib35.3.2">11</span>, 10473 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Paugam, F. <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Open-source pipeline for multi-class segmentation of the spinal cord with deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.2.1"><span class="ltx_ERROR undefined" id="bib.bib36.2.1.1">\JournalTitle</span>Magnetic resonance imaging</em> <span class="ltx_text ltx_font_bold" id="bib.bib36.3.2">64</span>, 21–27 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Zhang, X. <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Automatic spinal cord segmentation from axial-view mri slices using cnn with grayscale regularized active contour propagation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.2.1"><span class="ltx_ERROR undefined" id="bib.bib37.2.1.1">\JournalTitle</span>Computers in Biology and Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bib37.3.2">132</span>, 104345 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Garg, S. &amp; Bhagyashree, S.

</span>
<span class="ltx_bibblock">Spinal cord mri segmentation techniques and algorithms: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1"><span class="ltx_ERROR undefined" id="bib.bib38.1.1.1">\JournalTitle</span>SN Computer Science</em> <span class="ltx_text ltx_font_bold" id="bib.bib38.2.2">2</span>, 229 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Benjdira, B. <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Spinal cord segmentation in ultrasound medical imagery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.2.1"><span class="ltx_ERROR undefined" id="bib.bib39.2.1.1">\JournalTitle</span>Applied Sciences</em> <span class="ltx_text ltx_font_bold" id="bib.bib39.3.2">10</span>, 1370 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Theodore, N. <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Posterior vertebral column subtraction osteotomy for recurrent tethered cord syndrome: a multicenter, retrospective analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.2.1"><span class="ltx_ERROR undefined" id="bib.bib40.2.1.1">\JournalTitle</span>Neurosurgery</em> <span class="ltx_text ltx_font_bold" id="bib.bib40.3.2">88</span>, 637–647 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Kerensky, M. J. <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Tethered spinal cord tension assessed via ultrasound elastography in computational and intraoperative human studies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.2.1"><span class="ltx_ERROR undefined" id="bib.bib41.2.1.1">\JournalTitle</span>Communications Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bib41.3.2">4</span>, 4 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Girshick, R.

</span>
<span class="ltx_bibblock">Fast r-cnn.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE international conference on computer vision</em>, 1440–1448 (2015).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Liu, W. <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Ssd: Single shot multibox detector.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.2.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14</em>, 21–37 (Springer, 2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Lin, T.-Y., Goyal, P., Girshick, R., He, K. &amp; Dollár, P.

</span>
<span class="ltx_bibblock">Focal loss for dense object detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the IEEE international conference on computer vision</em>, 2980–2988 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Carion, N. <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">et al.</em>
</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.2.1">European conference on computer vision</em>, 213–229 (Springer, 2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Wang, C.-Y., Bochkovskiy, A. &amp; Liao, H.-Y. M.

</span>
<span class="ltx_bibblock">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 7464–7475 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Jocher, G., Chaurasia, A. &amp; Qiu, J.

</span>
<span class="ltx_bibblock">Ultralytics YOLO (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Microsoft.

</span>
<span class="ltx_bibblock">Neural Network Intelligence (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Xie, E. <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Segformer: Simple and efficient design for semantic segmentation with transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.2.1"><span class="ltx_ERROR undefined" id="bib.bib49.2.1.1">\JournalTitle</span>Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib49.3.2">34</span>, 12077–12090 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P. &amp; Brox, T.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em>, 234–241 (Springer, 2015).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Chen, L.-C., Papandreou, G., Schroff, F. &amp; Adam, H.

</span>
<span class="ltx_bibblock">Rethinking atrous convolution for semantic image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1"><span class="ltx_ERROR undefined" id="bib.bib51.1.1.1">\JournalTitle</span>arXiv:1706.05587</em> (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Chen, J. <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Transunet: Transformers make strong encoders for medical image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.2.1"><span class="ltx_ERROR undefined" id="bib.bib52.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2102.04306</em> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Cao, H. <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Swin-unet: Unet-like pure transformer for medical image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.2.1">European conference on computer vision</em>, 205–218 (Springer, 2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Zhang, K. &amp; Liu, D.

</span>
<span class="ltx_bibblock">Customized segment anything model for medical image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1"><span class="ltx_ERROR undefined" id="bib.bib54.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2304.13785</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Toossi, A. <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Comparative neuroanatomy of the lumbosacral spinal cord of the rat, cat, pig, monkey, and human.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.2.1"><span class="ltx_ERROR undefined" id="bib.bib55.2.1.1">\JournalTitle</span>Scientific Reports</em> <span class="ltx_text ltx_font_bold" id="bib.bib55.3.2">11</span>, 1955 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Olyanasab, A. &amp; Annabestani, M.

</span>
<span class="ltx_bibblock">Leveraging machine learning for personalized wearable biomedical devices: A review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1"><span class="ltx_ERROR undefined" id="bib.bib56.1.1.1">\JournalTitle</span>Journal of Personalized Medicine</em> <span class="ltx_text ltx_font_bold" id="bib.bib56.2.2">14</span>, 203 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Benke, K. &amp; Benke, G.

</span>
<span class="ltx_bibblock">Artificial intelligence and big data in public health.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1"><span class="ltx_ERROR undefined" id="bib.bib57.1.1.1">\JournalTitle</span>International journal of environmental research and public health</em> <span class="ltx_text ltx_font_bold" id="bib.bib57.2.2">15</span>, 2796 (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Huang, H., Wu, R. S., Lin, M. &amp; Xu, S.

</span>
<span class="ltx_bibblock">Emerging wearable ultrasound technology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1"><span class="ltx_ERROR undefined" id="bib.bib58.1.1.1">\JournalTitle</span>IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Duan, B., Xiong, L., Guan, X., Fu, Y. &amp; Zhang, Y.

</span>
<span class="ltx_bibblock">Tele-operated robotic ultrasound system for medical diagnosis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1"><span class="ltx_ERROR undefined" id="bib.bib59.1.1.1">\JournalTitle</span>Biomedical Signal Processing and Control</em> <span class="ltx_text ltx_font_bold" id="bib.bib59.2.2">70</span>, 102900 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Kumar, A. <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">et al.</em>
</span>
<span class="ltx_bibblock">A patient-specific preplanning treatment algorithm for focused ultrasound therapy of spinal cord injury.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.2.1">2023 11th International IEEE/EMBS Conference on Neural Engineering (NER)</em>, 1–4 (IEEE, 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Kumar, A. <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Computational modeling towards focused ultrasound therapy for spinal cord injury: visualization of beam propagation through patient-specific anatomy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.2.1">Medical Imaging 2023: Image-Guided Procedures, Robotic Interventions, and Modeling</em>, vol. 12466, 276–282 (SPIE, 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Xu, Z.

</span>
<span class="ltx_bibblock">Multi-atlas labeling beyond the cranial vault-workshop and challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1"><span class="ltx_ERROR undefined" id="bib.bib62.1.1.1">\JournalTitle</span>Synapse website</em> (2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Kang, Q., Gao, J., Li, K. &amp; Lao, Q.

</span>
<span class="ltx_bibblock">Deblurring masked autoencoder is better recipe for ultrasound image recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1"><span class="ltx_ERROR undefined" id="bib.bib63.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2306.08249</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Tirindelli, M. <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">et al.</em>
</span>
<span class="ltx_bibblock">Rethinking ultrasound augmentation: A physics-inspired approach.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.2.1">Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VIII 24</em>, 690–700 (Springer, 2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Al-Dhabyani, W., Gomaa, M., Khaled, H. &amp; Aly, F.

</span>
<span class="ltx_bibblock">Deep learning approaches for data augmentation and classification of breast masses using ultrasound images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1"><span class="ltx_ERROR undefined" id="bib.bib65.1.1.1">\JournalTitle</span>Int. J. Adv. Comput. Sci. Appl</em> <span class="ltx_text ltx_font_bold" id="bib.bib65.2.2">10</span>, 1–11 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Clum, C., Mixon, D. G. &amp; Scarnati, T.

</span>
<span class="ltx_bibblock">Matching component analysis for transfer learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1"><span class="ltx_ERROR undefined" id="bib.bib66.1.1.1">\JournalTitle</span>SIAM Journal on Mathematics of Data Science</em> <span class="ltx_text ltx_font_bold" id="bib.bib66.2.2">2</span>, 309–334 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Andrew, G., Arora, R., Bilmes, J. &amp; Livescu, K.

</span>
<span class="ltx_bibblock">Deep canonical correlation analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">International conference on machine learning</em>, 1247–1255 (PMLR, 2013).

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="Sx7" lang="en">
<h2 class="ltx_title ltx_title_section">Acronyms</h2>
<figure class="ltx_table" id="Sx7.T6">
<table class="ltx_tabular" id="Sx7.T6.1">
<tr class="ltx_tr" id="Sx7.T6.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.1.1.1">
<span class="ltx_p" id="Sx7.T6.1.1.1.1.1" style="width:142.3pt;">AR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.1.2.1">
<span class="ltx_p" id="Sx7.T6.1.1.2.1.1" style="width:341.4pt;">Average Recall</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.2.1.1">
<span class="ltx_p" id="Sx7.T6.1.2.1.1.1" style="width:142.3pt;">ASPP</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.2.2.1">
<span class="ltx_p" id="Sx7.T6.1.2.2.1.1" style="width:341.4pt;">Atrous Spatial Pyramid Pooling</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.3.1.1">
<span class="ltx_p" id="Sx7.T6.1.3.1.1.1" style="width:142.3pt;">B-mode</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.3.2.1">
<span class="ltx_p" id="Sx7.T6.1.3.2.1.1" style="width:341.4pt;">brightness-mode</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.4.1.1">
<span class="ltx_p" id="Sx7.T6.1.4.1.1.1" style="width:142.3pt;">CNN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.4.2.1">
<span class="ltx_p" id="Sx7.T6.1.4.2.1.1" style="width:341.4pt;">convolutional neural network</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.5.1.1">
<span class="ltx_p" id="Sx7.T6.1.5.1.1.1" style="width:142.3pt;">CPU</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.5.2.1">
<span class="ltx_p" id="Sx7.T6.1.5.2.1.1" style="width:341.4pt;">central processing unit</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.6.1.1">
<span class="ltx_p" id="Sx7.T6.1.6.1.1.1" style="width:142.3pt;">CSF</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.6.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.6.2.1">
<span class="ltx_p" id="Sx7.T6.1.6.2.1.1" style="width:341.4pt;">cerebrospinal fluid</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.7.1.1">
<span class="ltx_p" id="Sx7.T6.1.7.1.1.1" style="width:142.3pt;">CT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.7.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.7.2.1">
<span class="ltx_p" id="Sx7.T6.1.7.2.1.1" style="width:341.4pt;">computed tomography</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.8.1.1">
<span class="ltx_p" id="Sx7.T6.1.8.1.1.1" style="width:142.3pt;">CVAT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.8.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.8.2.1">
<span class="ltx_p" id="Sx7.T6.1.8.2.1.1" style="width:341.4pt;">Computer Vision Annotation Tool</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.9.1.1">
<span class="ltx_p" id="Sx7.T6.1.9.1.1.1" style="width:142.3pt;">DETR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.9.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.9.2.1">
<span class="ltx_p" id="Sx7.T6.1.9.2.1.1" style="width:341.4pt;">Detection Transformer</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.10.1.1">
<span class="ltx_p" id="Sx7.T6.1.10.1.1.1" style="width:142.3pt;">DICOM</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.10.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.10.2.1">
<span class="ltx_p" id="Sx7.T6.1.10.2.1.1" style="width:341.4pt;">Digital Imaging and Communications in Medicine</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.11.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.11.1.1">
<span class="ltx_p" id="Sx7.T6.1.11.1.1.1" style="width:142.3pt;">E-ELAN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.11.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.11.2.1">
<span class="ltx_p" id="Sx7.T6.1.11.2.1.1" style="width:341.4pt;">Extended Efficient Layer Aggregation Network</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.12.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.12.1.1">
<span class="ltx_p" id="Sx7.T6.1.12.1.1.1" style="width:142.3pt;">FPS</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.12.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.12.2.1">
<span class="ltx_p" id="Sx7.T6.1.12.2.1.1" style="width:341.4pt;">frames per second</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.13.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.13.1.1">
<span class="ltx_p" id="Sx7.T6.1.13.1.1.1" style="width:142.3pt;">GPU</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.13.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.13.2.1">
<span class="ltx_p" id="Sx7.T6.1.13.2.1.1" style="width:341.4pt;">graphics processing unit</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.14.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.14.1.1">
<span class="ltx_p" id="Sx7.T6.1.14.1.1.1" style="width:142.3pt;">IoU</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.14.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.14.2.1">
<span class="ltx_p" id="Sx7.T6.1.14.2.1.1" style="width:341.4pt;">Intersection over Union</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.15.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.15.1.1">
<span class="ltx_p" id="Sx7.T6.1.15.1.1.1" style="width:142.3pt;">mAP</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.15.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.15.2.1">
<span class="ltx_p" id="Sx7.T6.1.15.2.1.1" style="width:341.4pt;">mean Average Precision</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.16.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.16.1.1">
<span class="ltx_p" id="Sx7.T6.1.16.1.1.1" style="width:142.3pt;">MIoU</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.16.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.16.2.1">
<span class="ltx_p" id="Sx7.T6.1.16.2.1.1" style="width:341.4pt;">Mean Intersection over Union</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.17.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.17.1.1">
<span class="ltx_p" id="Sx7.T6.1.17.1.1.1" style="width:142.3pt;">ML</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.17.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.17.2.1">
<span class="ltx_p" id="Sx7.T6.1.17.2.1.1" style="width:341.4pt;">machine learning</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.18.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.18.1.1">
<span class="ltx_p" id="Sx7.T6.1.18.1.1.1" style="width:142.3pt;">MRI</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.18.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.18.2.1">
<span class="ltx_p" id="Sx7.T6.1.18.2.1.1" style="width:341.4pt;">magnetic resonance imaging</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.19">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.19.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.19.1.1">
<span class="ltx_p" id="Sx7.T6.1.19.1.1.1" style="width:142.3pt;">PNG</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.19.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.19.2.1">
<span class="ltx_p" id="Sx7.T6.1.19.2.1.1" style="width:341.4pt;">Portable Networks Graphics</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.20">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.20.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.20.1.1">
<span class="ltx_p" id="Sx7.T6.1.20.1.1.1" style="width:142.3pt;">PVCSO</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.20.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.20.2.1">
<span class="ltx_p" id="Sx7.T6.1.20.2.1.1" style="width:341.4pt;">posterior vertebral column subtraction osteotomy</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.21">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.21.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.21.1.1">
<span class="ltx_p" id="Sx7.T6.1.21.1.1.1" style="width:142.3pt;">RCNN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.21.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.21.2.1">
<span class="ltx_p" id="Sx7.T6.1.21.2.1.1" style="width:341.4pt;">Region-based Convolutional Neural Network</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.22">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.22.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.22.1.1">
<span class="ltx_p" id="Sx7.T6.1.22.1.1.1" style="width:142.3pt;">SAMed</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.22.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.22.2.1">
<span class="ltx_p" id="Sx7.T6.1.22.2.1.1" style="width:341.4pt;">Segment Anything for medical images</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.23">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.23.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.23.1.1">
<span class="ltx_p" id="Sx7.T6.1.23.1.1.1" style="width:142.3pt;">SCI</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.23.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.23.2.1">
<span class="ltx_p" id="Sx7.T6.1.23.2.1.1" style="width:341.4pt;">spinal cord injury</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.24">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.24.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.24.1.1">
<span class="ltx_p" id="Sx7.T6.1.24.1.1.1" style="width:142.3pt;">SSD</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.24.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.24.2.1">
<span class="ltx_p" id="Sx7.T6.1.24.2.1.1" style="width:341.4pt;">Single Shot Detector</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx7.T6.1.25">
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.25.1">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.25.1.1">
<span class="ltx_p" id="Sx7.T6.1.25.1.1.1" style="width:142.3pt;">YOLO</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx7.T6.1.25.2">
<span class="ltx_inline-block ltx_align_top" id="Sx7.T6.1.25.2.1">
<span class="ltx_p" id="Sx7.T6.1.25.2.1.1" style="width:341.4pt;">You Only Look Once</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_section" id="Sx8" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx8.p1">
<p class="ltx_p" id="Sx8.p1.1">This work received funding support from the National Science Foundation (NSF) STTR Phase 1 Award (#:1938939), Defense Advanced Research Projects Agency (DARPA) Award (#:N660012024075), National Institutes of Health (NIH) awards T32 GM136577 and F30 HL168823, and Johns Hopkins Institute for Clinical and Translational Research (ICTR)’s Clinical Research Scholars Program (KL2), administered by the NIH National Center for Advancing Translational Sciences (NCATS).</p>
</div>
</section>
<section class="ltx_section" id="Sx9" lang="en">
<h2 class="ltx_title ltx_title_section">Author contributions statement</h2>
<div class="ltx_para" id="Sx9.p1">
<p class="ltx_p" id="Sx9.p1.1">AK - project conception, data processing and annotation, deep learning, manuscript preparation, figures. KK - data processing and annotation, deep learning. KJ, MB, DD, CWL, SK, JS, KA - data annotation. MK - data collection, rendered Fig. 6. RL, KKL - manuscript review. AH, DR - data collection. IS - rendered Fig. 1. BT, NT, NT, AM - funding acquisition, supervision.</p>
</div>
</section>
<section class="ltx_section" id="Sx10" lang="en">
<h2 class="ltx_title ltx_title_section">Additional Information</h2>
<section class="ltx_subsection" id="Sx10.SSx1">
<h3 class="ltx_title ltx_title_subsection">Competing interests</h3>
<div class="ltx_para" id="Sx10.SSx1.p1">
<p class="ltx_p" id="Sx10.SSx1.p1.1">The authors declare no competing interests.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx10.SSx2">
<h3 class="ltx_title ltx_title_subsection">Data Availability</h3>
<div class="ltx_para" id="Sx10.SSx2.p1">
<p class="ltx_p" id="Sx10.SSx2.p1.1">The authors will maintain this dataset with changes and updates to be described on the following GitHub page: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/HEPIUSLAB/ultrasound_spinal_cord_dataset" title="">https://github.com/HEPIUSLAB/ultrasound_spinal_cord_dataset</a>.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx10.SSx3">
<h3 class="ltx_title ltx_title_subsection">Supplementary Materials</h3>
<div class="ltx_para" id="Sx10.SSx3.p1">
<p class="ltx_p" id="Sx10.SSx3.p1.1">The supplementary materials for this manuscript are available on the following page: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://drive.google.com/file/d/1cY400awTul8FEVQAm9LMCUouuuzCGT3d/view?usp=sharing" title="">https://drive.google.com/file/d/1cY400awTul8FEVQAm9LMCUouuuzCGT3d/view?usp=sharing</a></p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 20:15:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
