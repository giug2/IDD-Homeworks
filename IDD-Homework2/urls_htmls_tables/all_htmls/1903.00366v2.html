<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1903.00366] Answer Them All! Toward Universal Visual Question Answering Models</title><meta property="og:description" content="Visual Question Answering (VQA) research is split into two camps: the first focuses on VQA datasets that require natural image understanding and the second focuses on synthetic datasets that test reasoning. A good VQA …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Answer Them All! Toward Universal Visual Question Answering Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Answer Them All! Toward Universal Visual Question Answering Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1903.00366">

<!--Generated on Fri Mar  8 07:30:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Answer Them All! Toward Universal Visual Question Answering Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Robik Shrestha<sup id="id7.7.id1" class="ltx_sup">1</sup>  Kushal Kafle<sup id="id8.8.id2" class="ltx_sup">1</sup>  Christopher Kanan<sup id="id9.9.id3" class="ltx_sup"><span id="id9.9.id3.1" class="ltx_text ltx_font_italic">1,2,3</span></sup>
<br class="ltx_break"><sup id="id10.10.id4" class="ltx_sup">1</sup>Rochester Institute of Technology  <sup id="id11.11.id5" class="ltx_sup">2</sup>PAIGE   <sup id="id12.12.id6" class="ltx_sup">3</sup>Cornell Tech
<br class="ltx_break"><span id="id13.13.id7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{rss9369, kk6055, kanan}@rit.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">Visual Question Answering (VQA) research is split into two camps: the first focuses on VQA datasets that require natural image understanding and the second focuses on synthetic datasets that test reasoning. A good VQA algorithm should be capable of both, but only a few VQA algorithms are tested in this manner. We compare five state-of-the-art VQA algorithms across eight VQA datasets covering both domains. To make the comparison fair, all of the models are standardized as much as possible, <em id="id14.id1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="id14.id1.2" class="ltx_text"></span>, they use the same visual features, answer vocabularies, etc. We find that methods do not generalize across the two domains. To address this problem, we propose a new VQA algorithm that rivals or exceeds the state-of-the-art for both domains.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/1903.00366/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="243" height="390" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Many VQA algorithms do not transfer well across natural and synthetic datasets. We argue it is necessary to do well on both domains and present an algorithm that achieves this goal.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) requires a model to understand and reason about visuo-linguistic concepts to answer open-ended questions about images. Correctly answering these questions demands numerous capabilities, including object localization, attribute detection, activity classification, scene understanding, reasoning, counting, and more. The first VQA datasets contained real-world images with crowdsourced questions and answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. It was assumed that this would be an extremely difficult problem and was proposed as a form of Visual Turing Test to benchmark performance in computer vision. However, it became clear that many high performing algorithms were simply exploiting biases and superficial correlations, without really understanding the visual content <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. For example, answering ‘yes’ to all yes/no questions in <span title="" class="ltx_glossaryref">VQAv1</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> results in an accuracy of 71% on these questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Later natural image VQA datasets endeavored to address this issue. By associating each question with complementary images and different answers, <span title="" class="ltx_glossaryref">VQAv2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> reduces some forms of language bias. <span title="" class="ltx_glossaryref">TDIUC</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> analyzes generalization to multiple kinds of questions and rarer answers. <span title="" class="ltx_glossaryref">CVQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> tests concept compositionality and <span title="" class="ltx_glossaryref">VQACPv2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> tests performance when train and test distributions differ.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While later natural image datasets have reduced bias, the vast majority of questions in these datasets do not rigorously test reasoning skills. Several synthetic datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> were created as a remedy. They contain simple visual scenes with challenging questions that test multi-step reasoning, counting, and logical inference. To properly evaluate an algorithm’s robustness, the creators of these datasets have argued algorithms should be tested on both domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, almost all recent papers report their performance on only one of these two domains. The best algorithms for CLEVR are not tested on natural image VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, and vice versa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Here, we test five state-of-the-art VQA systems across eight datasets. We found that most methods do not perform well on both domains (Fig. <a href="#S0.F1" title="Figure 1 ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), with some suffering drastic losses in performance. We propose a new model that rivals state-of-the-art methods on all of the evaluated datasets.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Our major contributions are:</h4>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We perform a rigorous comparison of five state-of-the-art algorithms across eight VQA datasets, and we find that many do not generalize across domains.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Often VQA algorithms use different visual features and answer vocabularies, making it difficult to assess performance gains. We endeavor to standardize the components used across models, <em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.I1.i2.p1.1.2" class="ltx_text"></span>, all of the algorithms we compare use <em id="S1.I1.i2.p1.1.3" class="ltx_emph ltx_font_italic">identical</em> visual features, which required elevating the methods for synthetic scenes to use region proposals.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We find that most VQA algorithms are not capable of understanding real-word images <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">and</em> performing compositional reasoning. All of them fare poorly on generalization tests, indicating that these methods are still exploiting dataset biases.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We describe a new VQA algorithm that rivals state-of-the-art methods on all datasets and performs best overall.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>VQA Datasets</h3>

<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of datasets used in this paper. </figcaption>
<table id="S2.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.3.1.1" class="ltx_tr">
<th id="S2.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></th>
<th id="S2.T1.3.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.2.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S2.T1.3.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Num. of</span></td>
</tr>
<tr id="S2.T1.3.1.1.2.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S2.T1.3.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Images</span></td>
</tr>
</table>
</th>
<th id="S2.T1.3.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.3.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S2.T1.3.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Num. of</span></td>
</tr>
<tr id="S2.T1.3.1.1.3.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S2.T1.3.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">QA Pairs</span></td>
</tr>
</table>
</th>
<th id="S2.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.3.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.4.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Question</span></td>
</tr>
<tr id="S2.T1.3.1.1.4.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Source</span></td>
</tr>
</table>
</th>
<th id="S2.T1.3.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.3.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.5.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Image</span></td>
</tr>
<tr id="S2.T1.3.1.1.5.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Source</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.3.2.1" class="ltx_tr">
<td id="S2.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_border_t"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">VQAv1</span></td>
<td id="S2.T1.3.2.1.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S2.T1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">204K</span></td>
<td id="S2.T1.3.2.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S2.T1.3.2.1.3.1" class="ltx_text" style="font-size:90%;">614K</span></td>
<td id="S2.T1.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.3.2.1.4.1" class="ltx_text" style="font-size:90%;">Human</span></td>
<td id="S2.T1.3.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S2.T1.3.2.1.5.1" class="ltx_text" style="font-size:90%;">Natural</span></td>
</tr>
<tr id="S2.T1.3.3.2" class="ltx_tr">
<td id="S2.T1.3.3.2.1" class="ltx_td ltx_align_left"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">VQAv2</span></td>
<td id="S2.T1.3.3.2.2" class="ltx_td ltx_align_right"><span id="S2.T1.3.3.2.2.1" class="ltx_text" style="font-size:90%;">204K</span></td>
<td id="S2.T1.3.3.2.3" class="ltx_td ltx_align_right"><span id="S2.T1.3.3.2.3.1" class="ltx_text" style="font-size:90%;">1.1M</span></td>
<td id="S2.T1.3.3.2.4" class="ltx_td ltx_align_center"><span id="S2.T1.3.3.2.4.1" class="ltx_text" style="font-size:90%;">Human</span></td>
<td id="S2.T1.3.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.3.2.5.1" class="ltx_text" style="font-size:90%;">Natural</span></td>
</tr>
<tr id="S2.T1.3.4.3" class="ltx_tr">
<td id="S2.T1.3.4.3.1" class="ltx_td ltx_align_left"><span id="S2.T1.3.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">TDIUC</span></td>
<td id="S2.T1.3.4.3.2" class="ltx_td ltx_align_right"><span id="S2.T1.3.4.3.2.1" class="ltx_text" style="font-size:90%;">167K</span></td>
<td id="S2.T1.3.4.3.3" class="ltx_td ltx_align_right"><span id="S2.T1.3.4.3.3.1" class="ltx_text" style="font-size:90%;">1,6M</span></td>
<td id="S2.T1.3.4.3.4" class="ltx_td ltx_align_center"><span id="S2.T1.3.4.3.4.1" class="ltx_text" style="font-size:90%;">Both</span></td>
<td id="S2.T1.3.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.4.3.5.1" class="ltx_text" style="font-size:90%;">Natural</span></td>
</tr>
<tr id="S2.T1.3.5.4" class="ltx_tr">
<td id="S2.T1.3.5.4.1" class="ltx_td ltx_align_left"><span id="S2.T1.3.5.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">C-VQA</span></td>
<td id="S2.T1.3.5.4.2" class="ltx_td ltx_align_right"><span id="S2.T1.3.5.4.2.1" class="ltx_text" style="font-size:90%;">123K</span></td>
<td id="S2.T1.3.5.4.3" class="ltx_td ltx_align_right"><span id="S2.T1.3.5.4.3.1" class="ltx_text" style="font-size:90%;">369K</span></td>
<td id="S2.T1.3.5.4.4" class="ltx_td ltx_align_center"><span id="S2.T1.3.5.4.4.1" class="ltx_text" style="font-size:90%;">Human</span></td>
<td id="S2.T1.3.5.4.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.5.4.5.1" class="ltx_text" style="font-size:90%;">Natural</span></td>
</tr>
<tr id="S2.T1.3.6.5" class="ltx_tr">
<td id="S2.T1.3.6.5.1" class="ltx_td ltx_align_left"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">VQACPv2</span></td>
<td id="S2.T1.3.6.5.2" class="ltx_td ltx_align_right"><span id="S2.T1.3.6.5.2.1" class="ltx_text" style="font-size:90%;">219K</span></td>
<td id="S2.T1.3.6.5.3" class="ltx_td ltx_align_right"><span id="S2.T1.3.6.5.3.1" class="ltx_text" style="font-size:90%;">603K</span></td>
<td id="S2.T1.3.6.5.4" class="ltx_td ltx_align_center"><span id="S2.T1.3.6.5.4.1" class="ltx_text" style="font-size:90%;">Human</span></td>
<td id="S2.T1.3.6.5.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.6.5.5.1" class="ltx_text" style="font-size:90%;">Natural</span></td>
</tr>
<tr id="S2.T1.3.7.6" class="ltx_tr">
<td id="S2.T1.3.7.6.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.3.7.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CLEVR</span></td>
<td id="S2.T1.3.7.6.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S2.T1.3.7.6.2.1" class="ltx_text" style="font-size:90%;">100K</span></td>
<td id="S2.T1.3.7.6.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S2.T1.3.7.6.3.1" class="ltx_text" style="font-size:90%;">999K</span></td>
<td id="S2.T1.3.7.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.3.7.6.4.1" class="ltx_text" style="font-size:90%;">Synthetic</span></td>
<td id="S2.T1.3.7.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S2.T1.3.7.6.5.1" class="ltx_text" style="font-size:90%;">Synthetic</span></td>
</tr>
<tr id="S2.T1.3.8.7" class="ltx_tr">
<td id="S2.T1.3.8.7.1" class="ltx_td ltx_align_left"><span id="S2.T1.3.8.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CLEVR-H</span></td>
<td id="S2.T1.3.8.7.2" class="ltx_td ltx_align_right"><span id="S2.T1.3.8.7.2.1" class="ltx_text" style="font-size:90%;">32K</span></td>
<td id="S2.T1.3.8.7.3" class="ltx_td ltx_align_right"><span id="S2.T1.3.8.7.3.1" class="ltx_text" style="font-size:90%;">32K</span></td>
<td id="S2.T1.3.8.7.4" class="ltx_td ltx_align_center"><span id="S2.T1.3.8.7.4.1" class="ltx_text" style="font-size:90%;">Human</span></td>
<td id="S2.T1.3.8.7.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.8.7.5.1" class="ltx_text" style="font-size:90%;">Synthetic</span></td>
</tr>
<tr id="S2.T1.3.9.8" class="ltx_tr">
<td id="S2.T1.3.9.8.1" class="ltx_td ltx_align_left"><span id="S2.T1.3.9.8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CoGenT-A</span></td>
<td id="S2.T1.3.9.8.2" class="ltx_td ltx_align_right"><span id="S2.T1.3.9.8.2.1" class="ltx_text" style="font-size:90%;">100K</span></td>
<td id="S2.T1.3.9.8.3" class="ltx_td ltx_align_right"><span id="S2.T1.3.9.8.3.1" class="ltx_text" style="font-size:90%;">999K</span></td>
<td id="S2.T1.3.9.8.4" class="ltx_td ltx_align_center"><span id="S2.T1.3.9.8.4.1" class="ltx_text" style="font-size:90%;">Synthetic</span></td>
<td id="S2.T1.3.9.8.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.3.9.8.5.1" class="ltx_text" style="font-size:90%;">Synthetic</span></td>
</tr>
<tr id="S2.T1.3.10.9" class="ltx_tr">
<td id="S2.T1.3.10.9.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T1.3.10.9.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CoGenT-B</span></td>
<td id="S2.T1.3.10.9.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S2.T1.3.10.9.2.1" class="ltx_text" style="font-size:90%;">30K</span></td>
<td id="S2.T1.3.10.9.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S2.T1.3.10.9.3.1" class="ltx_text" style="font-size:90%;">299K</span></td>
<td id="S2.T1.3.10.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.3.10.9.4.1" class="ltx_text" style="font-size:90%;">Synthetic</span></td>
<td id="S2.T1.3.10.9.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S2.T1.3.10.9.5.1" class="ltx_text" style="font-size:90%;">Synthetic</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Many VQA datasets have been proposed over the past four years. Here, we briefly review the datasets used in our experiments. Statistics for these datasets are given in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 VQA Datasets ‣ 2 Related Work ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. See <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> for reviews.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span title="" class="ltx_glossaryref ltx_font_bold">VQAv1</span><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">/<span title="" class="ltx_glossaryref">VQAv2</span>.</span> <span title="" class="ltx_glossaryref">VQAv1</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is one of the earliest, open-ended VQA datasets collected from human annotators. <span title="" class="ltx_glossaryref">VQAv1</span> has multiple kinds of language bias, including some questions being heavily correlated with specific answers. <span title="" class="ltx_glossaryref">VQAv2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> endeavors to mitigate this kind of language bias by collecting complementary images per question that result in different answers, but other kinds of language bias are still present, <em id="S2.SS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p2.1.3" class="ltx_text"></span>, reasoning questions are rare compared to detection questions. Both datasets have been widely used and <span title="" class="ltx_glossaryref">VQAv2</span> is the de facto benchmark for natural image VQA.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span title="" class="ltx_glossaryref ltx_font_bold">TDIUC</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> attempts to address the bias in the <em id="S2.SS1.p3.1.1" class="ltx_emph ltx_font_italic">kinds</em> of questions posed by annotators by categorizing questions into 12 distinct types, enabling nuanced task-driven evaluation. It has metrics to evaluate generalization across question types.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><span title="" class="ltx_glossaryref ltx_font_bold">CVQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is a re-split of <span title="" class="ltx_glossaryref">VQAv1</span> to test generalization to concept compositions not seen during training, <em id="S2.SS1.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p4.1.2" class="ltx_text"></span>, if the train set asks about ‘green plate’ and ‘red light,’ the test set will ask about ‘red plate’ and ‘green light.’ <span title="" class="ltx_glossaryref">CVQA</span> tests the ability to combine previously seen concepts in unseen ways.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span title="" class="ltx_glossaryref ltx_font_bold">VQACPv2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> re-organizes <span title="" class="ltx_glossaryref">VQAv2</span> such that answers for each question type are distributed differently in the train and test sets, <em id="S2.SS1.p5.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p5.1.2" class="ltx_text"></span>, ‘blue’ and ‘white’ might be the most frequent answers to ‘What color…’ questions in the train set, but these answers will rarely occur in the test set. Since it has different biases in the train and test sets, doing well on <span title="" class="ltx_glossaryref">VQACPv2</span> suggests that the system is generalizing by overcoming the biases in the training set.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p"><span title="" class="ltx_glossaryref ltx_font_bold">CLEVR</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is a synthetically generated dataset, consisting of visual scenes with simple geometric shapes, designed to test ‘compositional language and elementary visual reasoning.’ CLEVR’s questions often require long chains of complex reasoning. To enable fine-grained evaluation of reasoning abilities, CLEVR’s questions are categorized into five tasks: ‘querying attribute,’ ‘comparing attributes,’ ‘existence,’ ‘counting,’ and ‘integer comparison.’
Because all of the questions are programmatically generated, the <span title="" class="ltx_glossaryref ltx_font_bold">CLEVR-Humans</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> dataset was created to provide human-generated questions for CLEVR scenes to test generalization to free-form questions.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p"><span title="" class="ltx_glossaryref ltx_font_bold">CLEVR-CoGenT</span> tests the ability to handle unseen concept composition and remember old concept combinations. It has two splits: CoGenT-A and CoGenT-B, with mutually exclusive shape+color combinations. If models trained on CoGenT-A perform well on CoGenT-B without fine-tuning, it indicates generalization to novel compositions. If models fine-tuned on CoGenT-B still perform well on CoGenT-A, it indicates the ability to remember old concept combinations. The questions in these datasets are more complex than most in CVQA.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<p id="S2.SS1.p8.1" class="ltx_p">Using VQAv1 and VQAv2 alone makes it difficult to gauge whether an algorithm is capable of performing robust compositional reasoning or whether it is using superficial correlations to predict an answer. In part, this is due to the limitations of seeking crowdsourced questions and answers, with humans biased towards asking certain kinds of questions more often for certain images, <em id="S2.SS1.p8.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p8.1.2" class="ltx_text"></span>, counting questions are most often asked if there are two things of the same type in a scene and almost never have an answer of zero. While CVQA and VQACPv2 try to overcome these issues, synthetic datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> minimize such biases to a greater extent, and serve as an important litmus-test to measure <span id="S2.SS1.p8.1.3" class="ltx_text ltx_font_italic">specific</span> reasoning skills, but the synthetic visual scenes lack complexity and variation.</p>
</div>
<div id="S2.SS1.p9" class="ltx_para">
<p id="S2.SS1.p9.1" class="ltx_p">Natural and synthetic datasets serve complementary purposes, and the creators of synthetic datasets have argued that both should be used, <em id="S2.SS1.p9.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p9.1.2" class="ltx_text"></span>, the creators of SHAPES, an early VQA dataset similar to CLEVR, wrote ‘While success on this dataset is by no means a sufficient condition for robust visual QA, we believe it is a necessary one’ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. While this advice has largely been ignored by the community, we <span id="S2.SS1.p9.1.3" class="ltx_text ltx_font_bold">strongly believe</span> it is necessary to show that VQA algorithms are capable of tackling VQA in both natural and synthetic domains with little modification. Otherwise, an algorithm’s ability to generalize will not be fully assessed.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>VQA Algorithms</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Many algorithms for natural image VQA have been proposed, including Bayesian approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, methods using spatial attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, compositional approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, bilinear pooling schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Spatial attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> are one of the most widely used methods for natural language VQA. Attention computes relevance scores over visual and textual features allowing models to process only relevant information. Among these, we evaluate UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, QCG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. We describe these algorithms in more detail in Sec. <a href="#S4" title="4 VQA Models Evaluated ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Similarly, many methods have been created for synthetic VQA datasets. Often, these algorithms place a much greater emphasis on learning compositionality, relational reasoning, and interpretability compared to algorithms for natural images. Common approaches include modular networks, with some using ground-truth programs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, and others learning compositional rules implicitly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Other approaches have included using relational networks (RNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, early fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and conditional feature transformations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. In our experiments, we evaluate RN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and MAC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which are explained in more detail in Sec. <a href="#S4" title="4 VQA Models Evaluated ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Although rare exceptions exist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, most of these algorithms are evaluated only on natural or synthetic VQA datasets and not both. Furthermore, several algorithms that claim specific abilities are not tested on datasets designed to test these abilities, <em id="S2.SS2.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS2.p3.1.2" class="ltx_text"></span>, QCG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> claims better compositional performance, but it is not evaluated on CVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Here, we evaluate multiple state-of-the-art algorithms on both natural and synthetic VQA datasets, and we propose a new algorithm that works well for both.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The <span title="" class="ltx_glossaryref">RAMEN</span> VQA Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose the Recurrent Aggregation of Multimodal Embeddings Network (<span title="" class="ltx_glossaryref">RAMEN</span>) model for VQA. It is designed as a conceptually simple architecture that can adapt to the complexity of natural scenes, while also being capable of answering questions requiring complex chains of compositional reasoning, which occur in synthetic datasets like CLEVR. As illustrated in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1 Formal Model Definition ‣ 3 The RAMEN VQA Model ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <span title="" class="ltx_glossaryref">RAMEN</span> processes visual and question features in three phases:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Early fusion of vision and language features.</span> Early fusion between visual and language features and/or early modulation of visual features using language has been shown to help with compositional reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Inspired by these approaches, we propose early fusion through concatenation of spatially localized visual features with question features.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Learning bimodal embeddings via shared projections.</span> The concatenated visual+question features are passed through a shared network, producing spatially localized bimodal embeddings. This phase helps the network learn the inter-relationships between the visual and textual features.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Recurrent aggregation of the learned bimodal embeddings.</span> We aggregate the bimodal embeddings across the scene using a bi-directional gated recurrent unit (bi-GRU) to capture interactions among the bimodal embeddings. The final forward and backward states essentially need to retain all of the information required to answer the question.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">While most recent state-of-the-art VQA models for natural images use attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> or bilinear pooling mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, RAMEN is able to perform comparably without these mechanisms. Likewise, in contrast to the state-of-the-art models for CLEVR, RAMEN does not use pre-defined modules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> or reasoning cells <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, yet our experiments demonstrate it is capable of compositional reasoning.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Formal Model Definition</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">The input to RAMEN is a question embedding <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{q}\in\mathbb{R}^{d}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">𝒒</mi><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><in id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></in><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝒒</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">ℝ</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\bm{q}\in\mathbb{R}^{d}</annotation></semantics></math> and a set of <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N</annotation></semantics></math> region proposals <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\bm{r}_{i}\in\mathbb{R}^{m}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><msub id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">𝒓</mi><mi id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml">m</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><in id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></in><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">𝒓</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">ℝ</ci><ci id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\bm{r}_{i}\in\mathbb{R}^{m}</annotation></semantics></math>, where each <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\bm{r}_{i}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">𝒓</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝒓</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\bm{r}_{i}</annotation></semantics></math> has both visual appearance features and a spatial position. RAMEN first concatenates each proposal with question vector, which is followed by batch normalization, <em id="S3.SS1.p1.4.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p1.4.2" class="ltx_text"></span>,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\bm{c}_{i}=\textit{BatchNorm}\left(\bm{r}_{i}\oplus\bm{q}\right)," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">𝒄</mi><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3a.cmml">BatchNorm</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝒓</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">⊕</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">𝒒</mi></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">𝒄</ci><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.3a.cmml" xref="S3.E1.m1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">BatchNorm</mtext></ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1">direct-sum</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2">𝒓</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">𝒒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\bm{c}_{i}=\textit{BatchNorm}\left(\bm{r}_{i}\oplus\bm{q}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.5" class="ltx_p">where <math id="S3.SS1.p1.5.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S3.SS1.p1.5.m1.1a"><mo id="S3.SS1.p1.5.m1.1.1" xref="S3.SS1.p1.5.m1.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m1.1b"><csymbol cd="latexml" id="S3.SS1.p1.5.m1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m1.1c">\oplus</annotation></semantics></math> represents concatenation.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.5" class="ltx_p">All <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">N</annotation></semantics></math> of the <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\bm{c}_{i}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">𝒄</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝒄</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\bm{c}_{i}</annotation></semantics></math> vectors are then passed through a function <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="F\left(\bm{c}_{i}\right)" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">​</mo><mrow id="S3.SS1.p2.3.m3.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.3.m3.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.3.m3.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.2.cmml">𝒄</mi><mi id="S3.SS1.p2.3.m3.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.3.m3.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><times id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2"></times><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝐹</ci><apply id="S3.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.2">𝒄</ci><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">F\left(\bm{c}_{i}\right)</annotation></semantics></math>, which mixes the features to produce a bimodal embedding <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\bm{b}_{i}=F\left(\bm{c}_{i}\right)" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><msub id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">𝒃</mi><mi id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">=</mo><mrow id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.1.3" xref="S3.SS1.p2.4.m4.1.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.1.2" xref="S3.SS1.p2.4.m4.1.1.1.2.cmml">​</mo><mrow id="S3.SS1.p2.4.m4.1.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.4.m4.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.4.m4.1.1.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.2.cmml">𝒄</mi><mi id="S3.SS1.p2.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><eq id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"></eq><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">𝒃</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3">𝑖</ci></apply><apply id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.1.2"></times><ci id="S3.SS1.p2.4.m4.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.1.3">𝐹</ci><apply id="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.2">𝒄</ci><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\bm{b}_{i}=F\left(\bm{c}_{i}\right)</annotation></semantics></math>, where <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="F\left(\bm{c}_{i}\right)" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mrow id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">​</mo><mrow id="S3.SS1.p2.5.m5.1.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.5.m5.1.1.1.1.2" xref="S3.SS1.p2.5.m5.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.5.m5.1.1.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.1.1.1.2" xref="S3.SS1.p2.5.m5.1.1.1.1.1.2.cmml">𝒄</mi><mi id="S3.SS1.p2.5.m5.1.1.1.1.1.3" xref="S3.SS1.p2.5.m5.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.5.m5.1.1.1.1.3" xref="S3.SS1.p2.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><times id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2"></times><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">𝐹</ci><apply id="S3.SS1.p2.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1.2">𝒄</ci><ci id="S3.SS1.p2.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">F\left(\bm{c}_{i}\right)</annotation></semantics></math> was modeled using a multi-layer perceptron (MLP) with residual connections.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.4" class="ltx_p">Next, we perform late-fusion by concatenating each bimodal embedding with the original question embedding and aggregate the collection using</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\bm{a}=A\left(\bm{b}_{1}\oplus\bm{q},\bm{b}_{2}\oplus\bm{q},\ldots,\bm{b}_{N}\oplus\bm{q}\right)," display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.5" xref="S3.E2.m1.2.2.1.1.5.cmml">𝒂</mi><mo id="S3.E2.m1.2.2.1.1.4" xref="S3.E2.m1.2.2.1.1.4.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.5" xref="S3.E2.m1.2.2.1.1.3.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.4" xref="S3.E2.m1.2.2.1.1.3.4.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.4.cmml"><mo id="S3.E2.m1.2.2.1.1.3.3.3.4" xref="S3.E2.m1.2.2.1.1.3.3.4.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml">𝒃</mi><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">⊕</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml">𝒒</mi></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.3.5" xref="S3.E2.m1.2.2.1.1.3.3.4.cmml">,</mo><mrow id="S3.E2.m1.2.2.1.1.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.cmml"><msub id="S3.E2.m1.2.2.1.1.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.cmml">𝒃</mi><mn id="S3.E2.m1.2.2.1.1.2.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.E2.m1.2.2.1.1.2.2.2.2.1" xref="S3.E2.m1.2.2.1.1.2.2.2.2.1.cmml">⊕</mo><mi id="S3.E2.m1.2.2.1.1.2.2.2.2.3" xref="S3.E2.m1.2.2.1.1.2.2.2.2.3.cmml">𝒒</mi></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.3.6" xref="S3.E2.m1.2.2.1.1.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">…</mi><mo id="S3.E2.m1.2.2.1.1.3.3.3.7" xref="S3.E2.m1.2.2.1.1.3.3.4.cmml">,</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.cmml"><msub id="S3.E2.m1.2.2.1.1.3.3.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.3.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.2.2" xref="S3.E2.m1.2.2.1.1.3.3.3.3.2.2.cmml">𝒃</mi><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.2.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.2.3.cmml">N</mi></msub><mo id="S3.E2.m1.2.2.1.1.3.3.3.3.1" xref="S3.E2.m1.2.2.1.1.3.3.3.3.1.cmml">⊕</mo><mi id="S3.E2.m1.2.2.1.1.3.3.3.3.3" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3.cmml">𝒒</mi></mrow><mo id="S3.E2.m1.2.2.1.1.3.3.3.8" xref="S3.E2.m1.2.2.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.4.cmml" xref="S3.E2.m1.2.2.1.1.4"></eq><ci id="S3.E2.m1.2.2.1.1.5.cmml" xref="S3.E2.m1.2.2.1.1.5">𝒂</ci><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><times id="S3.E2.m1.2.2.1.1.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.4"></times><ci id="S3.E2.m1.2.2.1.1.3.5.cmml" xref="S3.E2.m1.2.2.1.1.3.5">𝐴</ci><vector id="S3.E2.m1.2.2.1.1.3.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1">direct-sum</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.2">𝒃</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3">𝒒</ci></apply><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.1">direct-sum</csymbol><apply id="S3.E2.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.2">𝒃</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.2.3">2</cn></apply><ci id="S3.E2.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2.2.3">𝒒</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">…</ci><apply id="S3.E2.m1.2.2.1.1.3.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.3.3.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.1">direct-sum</csymbol><apply id="S3.E2.m1.2.2.1.1.3.3.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.3.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.2.2">𝒃</ci><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.2.3">𝑁</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.3.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.3.3.3">𝒒</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\bm{a}=A\left(\bm{b}_{1}\oplus\bm{q},\bm{b}_{2}\oplus\bm{q},\ldots,\bm{b}_{N}\oplus\bm{q}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.3" class="ltx_p">where the function <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">A</annotation></semantics></math> is modeled using a bi-GRU, with the output of <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">A</annotation></semantics></math> consisting of the concatenation of the final states of both the forward and backward GRUs. We refer to <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\bm{a}</annotation></semantics></math> as the RAMEN embedding, which is then sent to a classification layer that predicts the answer. While RAMEN is simpler than most recent VQA models, we show it is competitive across datasets, unlike more complex models.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1903.00366/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="253" height="199" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Our recurrent aggregation of multimodal embeddings network (RAMEN).</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implementation Details</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Input Representation.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.4" class="ltx_p">We represent question words as 300 dimensional embeddings initialized with pre-trained GloVe vectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, and process them with a GRU to obtain a 1024 dimensional question embedding, <em id="S3.SS2.SSS0.Px1.p1.4.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.SSS0.Px1.p1.4.2" class="ltx_text"></span>, <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\bm{q}\in\mathbb{R}^{1024}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">𝒒</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">1024</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><in id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1"></in><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝒒</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.3">1024</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">\bm{q}\in\mathbb{R}^{1024}</annotation></semantics></math>. Each region proposal <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\bm{r}_{i}\in\mathbb{R}^{2560}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">𝒓</mi><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">2560</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><in id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1"></in><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2">𝒓</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3">2560</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">\bm{r}_{i}\in\mathbb{R}^{2560}</annotation></semantics></math> is made of visual features concatenated with spatial information. The visual features are 2048 dimensional CNN features produced by the bottom-up architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> based on Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Spatial information is encoded by dividing each proposal into a <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><times id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">16</cn><cn type="integer" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">16\times 16</annotation></semantics></math> grid of <math id="S3.SS2.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.2a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">x</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.2b"><interval closure="open" id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2"><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">𝑥</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.2c">(x,y)</annotation></semantics></math>-coordinates, which is then flattened to form a 512-dimensional vector.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model Configuration.</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.2" class="ltx_p">The projector <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">F</annotation></semantics></math> is modeled as a 4-layer MLP with 1024 units with swish non-linear activation functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. It has residual connections in layers 2, 3 and 4. The aggregator <math id="S3.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">A</annotation></semantics></math> is a single-layer bi-GRU that has a 1024 dimensional hidden state, so the concatenation of forward and backward states produces a 2048 dimensional embedding. This embedding is projected through a 2048 dimensional fully connected swish layer, followed by an output classification layer that has one unit per possible answer in the dataset.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training Details.</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.5" class="ltx_p">RAMEN is trained with Adamax <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, we use a gradual learning rate warm up (<math id="S3.SS2.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="2.5*epoch*10^{-4}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml"><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.cmml"><mn id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.2.cmml">2.5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.1.cmml">∗</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.3.cmml">e</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1a" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.4" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1b" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.5" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1c" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.6" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.6.cmml">h</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml">∗</mo><msup id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mn id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.cmml"><mo id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3a" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1"></times><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2"><times id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.1"></times><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2"><times id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.1"></times><cn type="float" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.2">2.5</cn><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.2.3">𝑒</ci></apply><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.3">𝑝</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.4.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.4">𝑜</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.5.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.5">𝑐</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.6.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.6">ℎ</ci></apply><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2">10</cn><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3"><minus id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.1.m1.1c">2.5*epoch*10^{-4}</annotation></semantics></math> ) for the first <math id="S3.SS2.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.2.m2.1a"><mn id="S3.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.2.m2.1b"><cn type="integer" id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.2.m2.1c">4</annotation></semantics></math> epochs, <math id="S3.SS2.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="5*10^{-4}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml"><mn id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml">∗</mo><msup id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mn id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">10</mn><mrow id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.cmml"><mo id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3a" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.cmml">−</mo><mn id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1"><times id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2">5</cn><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.2">10</cn><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3"><minus id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3"></minus><cn type="integer" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.3.m3.1c">5*10^{-4}</annotation></semantics></math> for epochs 5 to 10, and then decay it at the rate of <math id="S3.SS2.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="0.25" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.4.m4.1a"><mn id="S3.SS2.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml">0.25</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.4.m4.1b"><cn type="float" id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1">0.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.4.m4.1c">0.25</annotation></semantics></math> for every <math id="S3.SS2.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.5.m5.1a"><mn id="S3.SS2.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.5.m5.1b"><cn type="integer" id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.5.m5.1c">2</annotation></semantics></math> epochs, with early stopping used. The mini-batch size is 64.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1903.00366/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="502" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Some example predictions from our model RAMEN compared to other existing methods.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>VQA Models Evaluated</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we will briefly describe the models evaluated in our experiments.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Bottom-Up-Attention and Top-Down (<span title="" class="ltx_glossaryref">UpDn</span>)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> combines bottom-up and top-down attention mechanisms to perform VQA, with the bottom-up mechanism generating object proposals from Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, and the top-down mechanism predicting an attention distribution over those proposals. The top-down attention is task-driven, using questions to predict attention weights over the image regions. This model obtained first place in the 2017 VQA Workshop Challenge. For fair comparison, we use its bottom-up region features for all other VQA models.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Question-Conditioned Graph (QCG)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> represents images as graphs where object-level features from bottom-up region proposals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> act as graph nodes and edges that encode interactions between regions that are conditioned on the question. For each node, QC-Graph chooses a neighborhood of nodes with the strongest edge connections, resulting in a question specific graph structure. This structure is processed by a patch operator to perform spatial graph convolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The main motivation behind choosing this model was to examine the efficacy of the proposed graph representations and operations for compositional reasoning.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Bilinear Attention Network (BAN)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> fuses visual and textual modalities by considering interactions between all region proposals (visual channels) with all question words (textual channels). Unlike dual-attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, BAN handles interactions between all channels. It can be considered a generalization of low-rank bilinear pooling methods that jointly represent each channel pair <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. BAN supports multiple glimpses of attention via connected residual connections. It achieves 70.35% on the test-std split of <span title="" class="ltx_glossaryref">VQAv2</span>, which is one of the best published results.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Relation Network (<span title="" class="ltx_glossaryref">RN</span>)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> takes in every pair of region proposals, embeds them, and sums up all <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="N^{2}" display="inline"><semantics id="S4.p5.1.m1.1a"><msup id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mi id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">N</mi><mn id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1">superscript</csymbol><ci id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S4.p5.1.m1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">N^{2}</annotation></semantics></math> pair embeddings to produce a vector that encodes relationships between objects. This pairwise feature aggregation mechanism enables compositional reasoning, as demonstrated by its performance on CLEVR dataset. However, RN’s computational complexity increases quadratically with the number of objects, making it expensive to run when the number of objects is large. There have been recent attempts at reducing the number of pairwise comparisons by reducing the number of input objects fed to RN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">The <span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Memory, Attention and Composition (<span title="" class="ltx_glossaryref">MAC</span>)</span> network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> uses computational cells that automatically learn to perform attention-based reasoning. Unlike, modular networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that require pre-defined modules to perform pre-specified reasoning functions, MAC learns reasoning mechanisms directly from the data. Each MAC cell maintains a control state representing the reasoning operation and a memory state that is the result of the reasoning operation. It has a computer-like architecture with read, write and control units. MAC was evaluated on the CLEVR datasets and reports significant improvements on the challenging counting and numerical comparison tasks.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Standardizing Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Often VQA models achieve state-of-the-art performance using visual features that differ from past models, making it difficult to tell if good performance came from model improvements or improvements to the visual feature representation. To make the comparison across models more meaningful, we use the same visual features for all algorithms across all datasets. Specifically, we use the 2048-dimensional ‘bottom-up’ CNN features produced by the region proposal generator of a trained Faster R-CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> with a ResNet-101 backend. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, we keep the number of proposals fixed at 36 for natural images, although performance can increase when additional proposals are used, e.g., others have reported that using 100 proposals with BAN can slightly increase its performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. This Faster R-CNN model is trained for object localization, attribute recognition, and bounding box regression on Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. While CNN feature maps have been common for CLEVR, state-of-the-art methods for CLEVR have also been shifting toward region proposals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. For datasets that use CLEVR’s images, we train a separate Faster R-CNN for multi-class classification and bounding box regression, because the Faster R-CNN trained on Visual Genome did not transfer well to CLEVR. To do this, we estimate the bounding boxes using 3D coordinates/rotations specified in the scene annotations. We keep the number of CLEVR regions fixed at 15. We also augment these features with a 512 dimensional vector representing positional information about the boxes as described in Sec. <a href="#S3.SS2" title="3.2 Implementation Details ‣ 3 The RAMEN VQA Model ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> for <span title="" class="ltx_glossaryref">TDIUC</span>, <span title="" class="ltx_glossaryref">CLEVR</span>, <span title="" class="ltx_glossaryref">CLEVR-Humans</span> and <span title="" class="ltx_glossaryref">CLEVR-CoGenT</span>. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we limit the set of candidate answers to those occurring at least 9 times in the training+validation set, resulting in vocabularies of 2185 answers for <span title="" class="ltx_glossaryref">VQAv1</span> and 3129 answers for <span title="" class="ltx_glossaryref">VQAv2</span>. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, we limit the answer vocabulary to the 1000 most frequent training set answers for <span title="" class="ltx_glossaryref">CVQA</span> and <span title="" class="ltx_glossaryref">VQACPv2</span>. For <span title="" class="ltx_glossaryref">VQAv2</span>, we train the models on training and validation splits and report results on test-dev split. For the remaining datasets, we train the models on their training splits and report performance on validation splits.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Overall results from six VQA models evaluated using same visual features across all datasets. We highlight the top-3 models for each dataset, using darker colors for better performers. To study the generalization gap, we present the results before fine-tuning for CLEVR-CoGenT and CLEVR-Humans. For <span title="" class="ltx_glossaryref">VQAv2</span>, we train models on the train and validation splits and report results on test-dev questions. For CLEVR-CoGenT-B, we report results on a sub-split of validation split. For the other datasets, we train models on the train split and report results on validation splits.</figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.4.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T2.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset/Algorithm</span></td>
<td id="S4.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.4.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">UpDn</span></td>
<td id="S4.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.4.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">QCG</span></td>
<td id="S4.T2.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.4.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">BAN</span></td>
<td id="S4.T2.4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.4.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MAC</span></td>
<td id="S4.T2.4.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.4.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RN</span></td>
<td id="S4.T2.4.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S4.T2.4.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ours</span></td>
</tr>
<tr id="S4.T2.4.2.2" class="ltx_tr">
<td id="S4.T2.4.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">VQAv1</span></td>
<td id="S4.T2.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.4.2.2.2.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.2.2.2.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF 60.62</span>
</td>
<td id="S4.T2.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.2.2.3.1" class="ltx_text" style="font-size:90%;">59.90</span></td>
<td id="S4.T2.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.4.2.2.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.2.2.4.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.2.2.4.3" class="ltx_text ltx_font_bold" style="font-size:90%;">62.98</span>
</td>
<td id="S4.T2.4.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.2.2.5.1" class="ltx_text" style="font-size:90%;">54.08</span></td>
<td id="S4.T2.4.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.2.2.6.1" class="ltx_text" style="font-size:90%;">51.84</span></td>
<td id="S4.T2.4.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">
<span id="S4.T2.4.2.2.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.2.2.7.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD 61.98</span>
</td>
</tr>
<tr id="S4.T2.4.3.3" class="ltx_tr">
<td id="S4.T2.4.3.3.1" class="ltx_td ltx_align_left"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">VQAv2</span></td>
<td id="S4.T2.4.3.3.2" class="ltx_td ltx_align_center">
<span id="S4.T2.4.3.3.2.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.3.3.2.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF64.55</span>
</td>
<td id="S4.T2.4.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.3.3.3.1" class="ltx_text" style="font-size:90%;">57.08</span></td>
<td id="S4.T2.4.3.3.4" class="ltx_td ltx_align_center">
<span id="S4.T2.4.3.3.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.3.3.4.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.3.3.4.3" class="ltx_text ltx_font_bold" style="font-size:90%;">67.39</span>
</td>
<td id="S4.T2.4.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.3.3.5.1" class="ltx_text" style="font-size:90%;">54.35</span></td>
<td id="S4.T2.4.3.3.6" class="ltx_td ltx_align_center"><span id="S4.T2.4.3.3.6.1" class="ltx_text" style="font-size:90%;">60.96</span></td>
<td id="S4.T2.4.3.3.7" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T2.4.3.3.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.3.3.7.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD 65.96</span>
</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_left"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">TDIUC</span></td>
<td id="S4.T2.4.4.4.2" class="ltx_td ltx_align_center">
<span id="S4.T2.4.4.4.2.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.4.4.2.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF68.82</span>
</td>
<td id="S4.T2.4.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.4.4.3.1" class="ltx_text" style="font-size:90%;">65.57</span></td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center">
<span id="S4.T2.4.4.4.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.4.4.4.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD71.10</span>
</td>
<td id="S4.T2.4.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.4.4.5.1" class="ltx_text" style="font-size:90%;">66.43</span></td>
<td id="S4.T2.4.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T2.4.4.4.6.1" class="ltx_text" style="font-size:90%;">65.06</span></td>
<td id="S4.T2.4.4.4.7" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T2.4.4.4.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.4.4.7.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF </span><span id="S4.T2.4.4.4.7.3" class="ltx_text ltx_font_bold" style="font-size:90%;">72.52</span>
</td>
</tr>
<tr id="S4.T2.4.5.5" class="ltx_tr">
<td id="S4.T2.4.5.5.1" class="ltx_td ltx_align_left"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">CVQA</span></td>
<td id="S4.T2.4.5.5.2" class="ltx_td ltx_align_center">
<span id="S4.T2.4.5.5.2.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.5.5.2.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF57.01</span>
</td>
<td id="S4.T2.4.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.5.5.3.1" class="ltx_text" style="font-size:90%;">56.45</span></td>
<td id="S4.T2.4.5.5.4" class="ltx_td ltx_align_center">
<span id="S4.T2.4.5.5.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.5.5.4.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD57.36</span>
</td>
<td id="S4.T2.4.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.5.5.5.1" class="ltx_text" style="font-size:90%;">50.99</span></td>
<td id="S4.T2.4.5.5.6" class="ltx_td ltx_align_center"><span id="S4.T2.4.5.5.6.1" class="ltx_text" style="font-size:90%;">48.11</span></td>
<td id="S4.T2.4.5.5.7" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T2.4.5.5.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.5.5.7.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.5.5.7.3" class="ltx_text ltx_font_bold" style="font-size:90%;">58.92</span>
</td>
</tr>
<tr id="S4.T2.4.6.6" class="ltx_tr">
<td id="S4.T2.4.6.6.1" class="ltx_td ltx_align_left"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">VQACPv2</span></td>
<td id="S4.T2.4.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.4.6.6.2.1" class="ltx_text" style="font-size:90%;">38.01</span></td>
<td id="S4.T2.4.6.6.3" class="ltx_td ltx_align_center">
<span id="S4.T2.4.6.6.3.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.6.6.3.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF38.32</span>
</td>
<td id="S4.T2.4.6.6.4" class="ltx_td ltx_align_center">
<span id="S4.T2.4.6.6.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.6.6.4.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.6.6.4.3" class="ltx_text ltx_font_bold" style="font-size:90%;">39.31</span>
</td>
<td id="S4.T2.4.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.6.6.5.1" class="ltx_text" style="font-size:90%;">31.96</span></td>
<td id="S4.T2.4.6.6.6" class="ltx_td ltx_align_center"><span id="S4.T2.4.6.6.6.1" class="ltx_text" style="font-size:90%;">26.70</span></td>
<td id="S4.T2.4.6.6.7" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T2.4.6.6.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.6.6.7.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD39.21</span>
</td>
</tr>
<tr id="S4.T2.4.7.7" class="ltx_tr">
<td id="S4.T2.4.7.7.1" class="ltx_td ltx_align_left ltx_border_t"><span title="" class="ltx_glossaryref ltx_font_bold" style="font-size:90%;">CLEVR</span></td>
<td id="S4.T2.4.7.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.7.7.2.1" class="ltx_text" style="font-size:90%;">80.04</span></td>
<td id="S4.T2.4.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.7.7.3.1" class="ltx_text" style="font-size:90%;">46.73</span></td>
<td id="S4.T2.4.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.4.7.7.4.1" class="ltx_text" style="font-size:90%;">90.79</span></td>
<td id="S4.T2.4.7.7.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.4.7.7.5.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.7.7.5.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.7.7.5.3" class="ltx_text ltx_font_bold" style="font-size:90%;">98.00</span>
</td>
<td id="S4.T2.4.7.7.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.4.7.7.6.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.7.7.6.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF95.97</span>
</td>
<td id="S4.T2.4.7.7.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">
<span id="S4.T2.4.7.7.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.7.7.7.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD 96.92</span>
</td>
</tr>
<tr id="S4.T2.4.8.8" class="ltx_tr">
<td id="S4.T2.4.8.8.1" class="ltx_td ltx_align_left"><span id="S4.T2.4.8.8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CLEVR-Humans</span></td>
<td id="S4.T2.4.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T2.4.8.8.2.1" class="ltx_text" style="font-size:90%;">54.51</span></td>
<td id="S4.T2.4.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.8.8.3.1" class="ltx_text" style="font-size:90%;">28.12</span></td>
<td id="S4.T2.4.8.8.4" class="ltx_td ltx_align_center">
<span id="S4.T2.4.8.8.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.8.8.4.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.8.8.4.3" class="ltx_text ltx_font_bold" style="font-size:90%;">60.23</span>
</td>
<td id="S4.T2.4.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.4.8.8.5.1" class="ltx_text" style="font-size:90%;">50.20</span></td>
<td id="S4.T2.4.8.8.6" class="ltx_td ltx_align_center">
<span id="S4.T2.4.8.8.6.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.8.8.6.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF57.65</span>
</td>
<td id="S4.T2.4.8.8.7" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T2.4.8.8.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.8.8.7.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD57.87</span>
</td>
</tr>
<tr id="S4.T2.4.9.9" class="ltx_tr">
<td id="S4.T2.4.9.9.1" class="ltx_td ltx_align_left"><span id="S4.T2.4.9.9.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CLEVR-CoGenT-A</span></td>
<td id="S4.T2.4.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T2.4.9.9.2.1" class="ltx_text" style="font-size:90%;">82.47</span></td>
<td id="S4.T2.4.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.9.9.3.1" class="ltx_text" style="font-size:90%;">59.63</span></td>
<td id="S4.T2.4.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.9.9.4.1" class="ltx_text" style="font-size:90%;">92.50</span></td>
<td id="S4.T2.4.9.9.5" class="ltx_td ltx_align_center">
<span id="S4.T2.4.9.9.5.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.9.9.5.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.9.9.5.3" class="ltx_text ltx_font_bold" style="font-size:90%;">98.04</span>
</td>
<td id="S4.T2.4.9.9.6" class="ltx_td ltx_align_center">
<span id="S4.T2.4.9.9.6.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.9.9.6.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF96.45</span>
</td>
<td id="S4.T2.4.9.9.7" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T2.4.9.9.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.9.9.7.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD96.74</span>
</td>
</tr>
<tr id="S4.T2.4.10.10" class="ltx_tr">
<td id="S4.T2.4.10.10.1" class="ltx_td ltx_align_left"><span id="S4.T2.4.10.10.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CLEVR-CoGenT-B</span></td>
<td id="S4.T2.4.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T2.4.10.10.2.1" class="ltx_text" style="font-size:90%;">72.22</span></td>
<td id="S4.T2.4.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T2.4.10.10.3.1" class="ltx_text" style="font-size:90%;">53.45</span></td>
<td id="S4.T2.4.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T2.4.10.10.4.1" class="ltx_text" style="font-size:90%;">79.48</span></td>
<td id="S4.T2.4.10.10.5" class="ltx_td ltx_align_center">
<span id="S4.T2.4.10.10.5.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.10.10.5.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.10.10.5.3" class="ltx_text ltx_font_bold" style="font-size:90%;">90.41</span>
</td>
<td id="S4.T2.4.10.10.6" class="ltx_td ltx_align_center">
<span id="S4.T2.4.10.10.6.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.10.10.6.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF84.68</span>
</td>
<td id="S4.T2.4.10.10.7" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S4.T2.4.10.10.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.10.10.7.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD 89.07</span>
</td>
</tr>
<tr id="S4.T2.4.11.11" class="ltx_tr">
<td id="S4.T2.4.11.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt"><span id="S4.T2.4.11.11.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Mean</span></td>
<td id="S4.T2.4.11.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt"><span id="S4.T2.4.11.11.2.1" class="ltx_text" style="font-size:90%;">64.18</span></td>
<td id="S4.T2.4.11.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt"><span id="S4.T2.4.11.11.3.1" class="ltx_text" style="font-size:90%;">51.69</span></td>
<td id="S4.T2.4.11.11.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">
<span id="S4.T2.4.11.11.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.11.11.4.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD69.00</span>
</td>
<td id="S4.T2.4.11.11.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">
<span id="S4.T2.4.11.11.5.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.11.11.5.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF</span><span id="S4.T2.4.11.11.5.3" class="ltx_text" style="font-size:90%;">66.05</span>
</td>
<td id="S4.T2.4.11.11.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt"><span id="S4.T2.4.11.11.6.1" class="ltx_text" style="font-size:90%;">65.26</span></td>
<td id="S4.T2.4.11.11.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_tt">
<span id="S4.T2.4.11.11.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S4.T2.4.11.11.7.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S4.T2.4.11.11.7.3" class="ltx_text ltx_font_bold" style="font-size:90%;">71.02</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Maintaining Compatibility.</span> UpDn, QCG and BAN are all designed to operate on region proposals. For both MAC and RN, we needed to modify the input layers to accept bottom-up features, instead of convolutional feature maps. This was done so that the same features could be used across all datasets and also to upgrade RN and MAC so that they would be competitive on natural image datasets where these features are typically used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. For MAC, we replace the initial 2D convolution operation with a linear projection of the bottom-up features. These are fed through MAC’s read unit, which is left unmodified. For RN, we remove the initial convolutional network and directly concatenate bottom-up features with question embeddings as the input. The performance of both models after these changes are comparable to the versions using learned convolutional feature maps as input, with MAC achieving 98% and RN achieving 95.97% on the CLEVR validation set.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments and Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Main Results</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In this section, we demonstrate the inability of current VQA algorithms to generalize across natural and synthetic datasets, and show that <span title="" class="ltx_glossaryref">RAMEN</span> rivals the best performing models on all datasets. We also present a comparative analysis of bias-resistance, compositionality, and generalization abilities for all six algorithms. Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Standardizing Models ‣ 4 VQA Models Evaluated ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides our main results for all six algorithms on all eight datasets. We use the standard metrics for all datasets, <em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p1.1.2" class="ltx_text"></span>, we use simple accuracy for the CLEVR family of datasets, mean-per-type for TDIUC, and ‘10-choose-3’ for VQAv1, VQAv2, CVQA, and VQACPv2. Some example outputs for RAMEN compared to other models are given in Fig. <a href="#S3.F3" title="Figure 3 ‣ Training Details. ‣ 3.2 Implementation Details ‣ 3 The RAMEN VQA Model ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Generalization Across VQA Datasets.</h4>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.2" class="ltx_p">RAMEN achieves the highest results on <span title="" class="ltx_glossaryref">TDIUC</span> and <span title="" class="ltx_glossaryref">CVQA</span> and is the second best model for VQAv1, VQAv2, VQACPv2 and all of the CLEVR datasets. On average, it has the highest score across datasets, showcasing that it can generalize across natural datasets and synthetic datasets that test reasoning. BAN achieves the next highest mean score. <span title="" class="ltx_glossaryref">BAN</span> works well for natural image datasets, outperforming other models on <span title="" class="ltx_glossaryref">VQAv1</span>, <span title="" class="ltx_glossaryref">VQAv2</span> and <span title="" class="ltx_glossaryref">VQACPv2</span>. However, <span title="" class="ltx_glossaryref">BAN</span> shows limited compositional reasoning ability. Despite being conceptually much simpler than <span title="" class="ltx_glossaryref">BAN</span>, RAMEN outperforms <span title="" class="ltx_glossaryref">BAN</span> by <math id="S5.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="6\%" display="inline"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">6</mn><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">6\%</annotation></semantics></math> (absolute) on <span title="" class="ltx_glossaryref">CLEVR</span> and <math id="S5.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S5.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S5.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">10</mn><mo id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.2.m2.1c">10\%</annotation></semantics></math> on CLEVR-CoGenT-B. <span title="" class="ltx_glossaryref">RAMEN</span> is within 1.4% of MAC on all compositional reasoning tests. <span title="" class="ltx_glossaryref">UpDn</span> and <span title="" class="ltx_glossaryref">QCG</span> perform poorly on <span title="" class="ltx_glossaryref">CLEVR</span>, with <span title="" class="ltx_glossaryref">QCG</span> obtaining a score below 50%.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance comparison on TDIUC using three different metrics. MPT measures task generalization and N-MPT measures generalization to rare answers. We highlight the top-3 models, emboldening the winner.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Metric / Algorithm</span></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">UpDn</span></th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">QCG</span></th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">BAN</span></th>
<th id="S5.T3.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MAC</span></th>
<th id="S5.T3.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RN</span></th>
<th id="S5.T3.1.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ours</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T3.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MPT</span></td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T3.1.2.1.2.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.2.1.2.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF68.82</span>
</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T3.1.2.1.3.1" class="ltx_text" style="font-size:90%;">65.67</span></td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T3.1.2.1.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.2.1.4.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD71.10</span>
</td>
<td id="S5.T3.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T3.1.2.1.5.1" class="ltx_text" style="font-size:90%;">66.43</span></td>
<td id="S5.T3.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T3.1.2.1.6.1" class="ltx_text" style="font-size:90%;">65.06</span></td>
<td id="S5.T3.1.2.1.7" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T3.1.2.1.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.2.1.7.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S5.T3.1.2.1.7.3" class="ltx_text ltx_font_bold" style="font-size:90%;">72.52</span>
</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left"><span id="S5.T3.1.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">N-MPT</span></td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_right"><span id="S5.T3.1.3.2.2.1" class="ltx_text" style="font-size:90%;">38.93</span></td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_right"><span id="S5.T3.1.3.2.3.1" class="ltx_text" style="font-size:90%;">37.43</span></td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_right">
<span id="S5.T3.1.3.2.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.3.2.4.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD40.65</span>
</td>
<td id="S5.T3.1.3.2.5" class="ltx_td ltx_align_right">
<span id="S5.T3.1.3.2.5.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.3.2.5.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF39.02</span>
</td>
<td id="S5.T3.1.3.2.6" class="ltx_td ltx_align_right"><span id="S5.T3.1.3.2.6.1" class="ltx_text" style="font-size:90%;">35.75</span></td>
<td id="S5.T3.1.3.2.7" class="ltx_td ltx_align_right">
<span id="S5.T3.1.3.2.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.3.2.7.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S5.T3.1.3.2.7.3" class="ltx_text ltx_font_bold" style="font-size:90%;">46.52</span>
</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<td id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T3.1.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Simple Accuracy</span></td>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_right ltx_border_bb">
<span id="S5.T3.1.4.3.2.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.4.3.2.2" class="ltx_text" style="font-size:90%;">[HTML]EBECFF82.91</span>
</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S5.T3.1.4.3.3.1" class="ltx_text" style="font-size:90%;">82.05</span></td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_right ltx_border_bb">
<span id="S5.T3.1.4.3.4.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.4.3.4.2" class="ltx_text" style="font-size:90%;">[HTML]C4C7FD84.81</span>
</td>
<td id="S5.T3.1.4.3.5" class="ltx_td ltx_align_right ltx_border_bb"><span id="S5.T3.1.4.3.5.1" class="ltx_text" style="font-size:90%;">82.53</span></td>
<td id="S5.T3.1.4.3.6" class="ltx_td ltx_align_right ltx_border_bb"><span id="S5.T3.1.4.3.6.1" class="ltx_text" style="font-size:90%;">84.61</span></td>
<td id="S5.T3.1.4.3.7" class="ltx_td ltx_align_right ltx_border_bb">
<span id="S5.T3.1.4.3.7.1" class="ltx_ERROR undefined">\cellcolor</span><span id="S5.T3.1.4.3.7.2" class="ltx_text" style="font-size:90%;">[HTML]B1B2FF</span><span id="S5.T3.1.4.3.7.3" class="ltx_text ltx_font_bold" style="font-size:90%;">86.86</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance on CLEVR’s query types.</figcaption>
<table id="S5.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.3.1.1" class="ltx_tr">
<td id="S5.T4.3.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S5.T4.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Exist</span></th>
<th id="S5.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T4.3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.3.1.1.3.1.1" class="ltx_tr">
<td id="S5.T4.3.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Query</span></td>
</tr>
<tr id="S5.T4.3.1.1.3.1.2" class="ltx_tr">
<td id="S5.T4.3.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Attribute</span></td>
</tr>
</table>
</th>
<th id="S5.T4.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T4.3.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.3.1.1.4.1.1" class="ltx_tr">
<td id="S5.T4.3.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Compare</span></td>
</tr>
<tr id="S5.T4.3.1.1.4.1.2" class="ltx_tr">
<td id="S5.T4.3.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Attribute</span></td>
</tr>
</table>
</th>
<th id="S5.T4.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T4.3.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.3.1.1.5.1.1" class="ltx_tr">
<td id="S5.T4.3.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Equal</span></td>
</tr>
<tr id="S5.T4.3.1.1.5.1.2" class="ltx_tr">
<td id="S5.T4.3.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Integer</span></td>
</tr>
</table>
</th>
<th id="S5.T4.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T4.3.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.3.1.1.6.1.1" class="ltx_tr">
<td id="S5.T4.3.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Greater</span></td>
</tr>
<tr id="S5.T4.3.1.1.6.1.2" class="ltx_tr">
<td id="S5.T4.3.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Than</span></td>
</tr>
</table>
</th>
<th id="S5.T4.3.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T4.3.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.3.1.1.7.1.1" class="ltx_tr">
<td id="S5.T4.3.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Less</span></td>
</tr>
<tr id="S5.T4.3.1.1.7.1.2" class="ltx_tr">
<td id="S5.T4.3.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.3.1.1.7.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Than</span></td>
</tr>
</table>
</th>
<th id="S5.T4.3.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.3.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Count</span></th>
</tr>
<tr id="S5.T4.3.2.2" class="ltx_tr">
<td id="S5.T4.3.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T4.3.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">UpDn</span></td>
<td id="S5.T4.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.3.2.2.2.1" class="ltx_text" style="font-size:90%;">83.07</span></td>
<td id="S5.T4.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.3.2.2.3.1" class="ltx_text" style="font-size:90%;">90.08</span></td>
<td id="S5.T4.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.3.2.2.4.1" class="ltx_text" style="font-size:90%;">79.87</span></td>
<td id="S5.T4.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.3.2.2.5.1" class="ltx_text" style="font-size:90%;">65.65</span></td>
<td id="S5.T4.3.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.3.2.2.6.1" class="ltx_text" style="font-size:90%;">80.43</span></td>
<td id="S5.T4.3.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.3.2.2.7.1" class="ltx_text" style="font-size:90%;">85.76</span></td>
<td id="S5.T4.3.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.3.2.2.8.1" class="ltx_text" style="font-size:90%;">64.03</span></td>
</tr>
<tr id="S5.T4.3.3.3" class="ltx_tr">
<td id="S5.T4.3.3.3.1" class="ltx_td ltx_align_left"><span id="S5.T4.3.3.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">QCG</span></td>
<td id="S5.T4.3.3.3.2" class="ltx_td ltx_align_center"><span id="S5.T4.3.3.3.2.1" class="ltx_text" style="font-size:90%;">66.11</span></td>
<td id="S5.T4.3.3.3.3" class="ltx_td ltx_align_center"><span id="S5.T4.3.3.3.3.1" class="ltx_text" style="font-size:90%;">31.11</span></td>
<td id="S5.T4.3.3.3.4" class="ltx_td ltx_align_center"><span id="S5.T4.3.3.3.4.1" class="ltx_text" style="font-size:90%;">51.47</span></td>
<td id="S5.T4.3.3.3.5" class="ltx_td ltx_align_center"><span id="S5.T4.3.3.3.5.1" class="ltx_text" style="font-size:90%;">59.76</span></td>
<td id="S5.T4.3.3.3.6" class="ltx_td ltx_align_center"><span id="S5.T4.3.3.3.6.1" class="ltx_text" style="font-size:90%;">69.35</span></td>
<td id="S5.T4.3.3.3.7" class="ltx_td ltx_align_center"><span id="S5.T4.3.3.3.7.1" class="ltx_text" style="font-size:90%;">70.57</span></td>
<td id="S5.T4.3.3.3.8" class="ltx_td ltx_align_center"><span id="S5.T4.3.3.3.8.1" class="ltx_text" style="font-size:90%;">44.19</span></td>
</tr>
<tr id="S5.T4.3.4.4" class="ltx_tr">
<td id="S5.T4.3.4.4.1" class="ltx_td ltx_align_left"><span id="S5.T4.3.4.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">BAN</span></td>
<td id="S5.T4.3.4.4.2" class="ltx_td ltx_align_center"><span id="S5.T4.3.4.4.2.1" class="ltx_text" style="font-size:90%;">94.72</span></td>
<td id="S5.T4.3.4.4.3" class="ltx_td ltx_align_center"><span id="S5.T4.3.4.4.3.1" class="ltx_text" style="font-size:90%;">90.56</span></td>
<td id="S5.T4.3.4.4.4" class="ltx_td ltx_align_center"><span id="S5.T4.3.4.4.4.1" class="ltx_text" style="font-size:90%;">98.44</span></td>
<td id="S5.T4.3.4.4.5" class="ltx_td ltx_align_center"><span id="S5.T4.3.4.4.5.1" class="ltx_text" style="font-size:90%;">72.35</span></td>
<td id="S5.T4.3.4.4.6" class="ltx_td ltx_align_center"><span id="S5.T4.3.4.4.6.1" class="ltx_text" style="font-size:90%;">81.35</span></td>
<td id="S5.T4.3.4.4.7" class="ltx_td ltx_align_center"><span id="S5.T4.3.4.4.7.1" class="ltx_text" style="font-size:90%;">86.39</span></td>
<td id="S5.T4.3.4.4.8" class="ltx_td ltx_align_center"><span id="S5.T4.3.4.4.8.1" class="ltx_text" style="font-size:90%;">86.47</span></td>
</tr>
<tr id="S5.T4.3.5.5" class="ltx_tr">
<td id="S5.T4.3.5.5.1" class="ltx_td ltx_align_left"><span id="S5.T4.3.5.5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MAC</span></td>
<td id="S5.T4.3.5.5.2" class="ltx_td ltx_align_center"><span id="S5.T4.3.5.5.2.1" class="ltx_text" style="font-size:90%;">99.18</span></td>
<td id="S5.T4.3.5.5.3" class="ltx_td ltx_align_center"><span id="S5.T4.3.5.5.3.1" class="ltx_text" style="font-size:90%;">99.59</span></td>
<td id="S5.T4.3.5.5.4" class="ltx_td ltx_align_center"><span id="S5.T4.3.5.5.4.1" class="ltx_text" style="font-size:90%;">99.33</span></td>
<td id="S5.T4.3.5.5.5" class="ltx_td ltx_align_center"><span id="S5.T4.3.5.5.5.1" class="ltx_text" style="font-size:90%;">85.44</span></td>
<td id="S5.T4.3.5.5.6" class="ltx_td ltx_align_center"><span id="S5.T4.3.5.5.6.1" class="ltx_text" style="font-size:90%;">96.82</span></td>
<td id="S5.T4.3.5.5.7" class="ltx_td ltx_align_center"><span id="S5.T4.3.5.5.7.1" class="ltx_text" style="font-size:90%;">97.55</span></td>
<td id="S5.T4.3.5.5.8" class="ltx_td ltx_align_center"><span id="S5.T4.3.5.5.8.1" class="ltx_text" style="font-size:90%;">95.46</span></td>
</tr>
<tr id="S5.T4.3.6.6" class="ltx_tr">
<td id="S5.T4.3.6.6.1" class="ltx_td ltx_align_left"><span id="S5.T4.3.6.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RN</span></td>
<td id="S5.T4.3.6.6.2" class="ltx_td ltx_align_center"><span id="S5.T4.3.6.6.2.1" class="ltx_text" style="font-size:90%;">98.40</span></td>
<td id="S5.T4.3.6.6.3" class="ltx_td ltx_align_center"><span id="S5.T4.3.6.6.3.1" class="ltx_text" style="font-size:90%;">98.19</span></td>
<td id="S5.T4.3.6.6.4" class="ltx_td ltx_align_center"><span id="S5.T4.3.6.6.4.1" class="ltx_text" style="font-size:90%;">97.81</span></td>
<td id="S5.T4.3.6.6.5" class="ltx_td ltx_align_center"><span id="S5.T4.3.6.6.5.1" class="ltx_text" style="font-size:90%;">77.30</span></td>
<td id="S5.T4.3.6.6.6" class="ltx_td ltx_align_center"><span id="S5.T4.3.6.6.6.1" class="ltx_text" style="font-size:90%;">93.40</span></td>
<td id="S5.T4.3.6.6.7" class="ltx_td ltx_align_center"><span id="S5.T4.3.6.6.7.1" class="ltx_text" style="font-size:90%;">84.27</span></td>
<td id="S5.T4.3.6.6.8" class="ltx_td ltx_align_center"><span id="S5.T4.3.6.6.8.1" class="ltx_text" style="font-size:90%;">90.90</span></td>
</tr>
<tr id="S5.T4.3.7.7" class="ltx_tr">
<td id="S5.T4.3.7.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T4.3.7.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">RAMEN</span></td>
<td id="S5.T4.3.7.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.7.7.2.1" class="ltx_text" style="font-size:90%;">98.90</span></td>
<td id="S5.T4.3.7.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.7.7.3.1" class="ltx_text" style="font-size:90%;">98.93</span></td>
<td id="S5.T4.3.7.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.7.7.4.1" class="ltx_text" style="font-size:90%;">99.30</span></td>
<td id="S5.T4.3.7.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.7.7.5.1" class="ltx_text" style="font-size:90%;">79.40</span></td>
<td id="S5.T4.3.7.7.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.7.7.6.1" class="ltx_text" style="font-size:90%;">93.41</span></td>
<td id="S5.T4.3.7.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.7.7.7.1" class="ltx_text" style="font-size:90%;">88.53</span></td>
<td id="S5.T4.3.7.7.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.7.7.8.1" class="ltx_text" style="font-size:90%;">94.10</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Generalization Across Question Types.</h4>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.1" class="ltx_p">We use TDIUC to study generalization across question types. TDIUC has multiple accuracy metrics, with mean-per-type (MPT) and normalized mean-per-type (N-MPT) compensating for biases. As shown in Table <a href="#S5.T3" title="Table 3 ‣ Generalization Across VQA Datasets. ‣ 5.1 Main Results ‣ 5 Experiments and Results ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, all methods achieve simple accuracy scores of over 82%; however, both MPT and N-MPT scores are 13-20% lower. Lower MPT scores indicate that all algorithms are struggling to generalize to multiple tasks. <span title="" class="ltx_glossaryref">RAMEN</span> obtains the highest MPT score of 72.52% followed by <span title="" class="ltx_glossaryref">BAN</span> at 71.10%. For all algorithms, ‘object presence,’ ‘object recognition,’ and ‘scene recognition’ are among the easiest tasks, with all of the methods achieving over 84% accuracy on them; however, these tasks all have relatively large amounts of training data (60K - 657K QA pairs each). All of the methods performed well on ‘sports recognition’ (31K QA pairs), achieving over 93%, but all performed poorly on a conceptually similar task of ‘activity recognition’ (8.5K QA pairs), achieving under 62% accuracy. This showcases the inability to generalize to question types with fewer examples. To emphasize this, TDIUC provides the Normalized MPT (N-MPT) metric that measures generalization to rare answers by taking answer frequency into account. The differences between normalized and un-normalized scores are large for all models. <span title="" class="ltx_glossaryref">RAMEN</span> has the smallest gap, indicating a better resistance to answer distribution biases, while <span title="" class="ltx_glossaryref">BAN</span> has the largest gap.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Generalization to Novel Concept Compositions.</h4>

<div id="S5.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px3.p1.1" class="ltx_p">We evaluate concept compositionality using CVQA and CLEVR-CoGenT-B. As shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Standardizing Models ‣ 4 VQA Models Evaluated ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, scores on CVQA are lower than VQAv1, suggesting all of the algorithms struggle when combining concepts in new ways. <span title="" class="ltx_glossaryref">MAC</span> has the largest performance drop, which suggests its reasoning cells were not able to compose real-world visuo-linguistic concepts effectively.</p>
</div>
<div id="S5.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S5.SS1.SSS0.Px3.p2.1" class="ltx_p">To evaluate the ability to generalize to new concept compositions on the synthetic datasets, we train the models on CLEVR-CoGenT-A’s train split and evaluate on the validation set without fine-tuning. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, we obtain a test split from the validation set of ‘B,’ and report performance without fine-tuning on ‘B.’ All algorithms show a large drop in performance. Unlike the CVQA results, MAC’s drop in performance is smaller. Again, RAMEN has a comparatively small decrease in performance.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Performance on <span title="" class="ltx_glossaryref">VQACPv2</span>’s Changing Priors.</h4>

<div id="S5.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px4.p1.1" class="ltx_p">All algorithms have a large drop in performance under changing priors. This suggests there is significantly more work to be done to make VQA algorithms overcome linguistic and visual priors so that they can more effectively learn to use generalizable concepts.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Counting and Numerical Comparisons.</h4>

<div id="S5.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px5.p1.1" class="ltx_p">For <span title="" class="ltx_glossaryref">CLEVR</span>, counting and number comparison (‘equal integer,’ ‘greater than,’ and ‘less than’) are the most challenging tasks across algorithms as shown in Table <a href="#S5.T4" title="Table 4 ‣ Generalization Across VQA Datasets. ‣ 5.1 Main Results ‣ 5 Experiments and Results ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. <span title="" class="ltx_glossaryref">MAC</span> performs best on these tasks, followed by <span title="" class="ltx_glossaryref">RAMEN</span>. Algorithms apart from <span title="" class="ltx_glossaryref">MAC</span> and <span title="" class="ltx_glossaryref">QCG</span> demonstrate a large (<math id="S5.SS1.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="&gt;4.8\%" display="inline"><semantics id="S5.SS1.SSS0.Px5.p1.1.m1.1a"><mrow id="S5.SS1.SSS0.Px5.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.2.cmml"></mi><mo id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.1" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.1.cmml">&gt;</mo><mrow id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.cmml"><mn id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.2" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.2.cmml">4.8</mn><mo id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.1" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px5.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1"><gt id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.2">absent</csymbol><apply id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.2.cmml" xref="S5.SS1.SSS0.Px5.p1.1.m1.1.1.3.2">4.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px5.p1.1.m1.1c">&gt;4.8\%</annotation></semantics></math>) discrepancy between ‘less than’ and ‘greater than’ question types, which require similar kinds of reasoning. This discrepancy is most pronounced for <span title="" class="ltx_glossaryref">RN</span> (9.13%), indicating a difficulty in linguistic understanding. <span title="" class="ltx_glossaryref">BAN</span> uses a counting module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>; however, its performance on CLEVR’s counting task is still 9% below <span title="" class="ltx_glossaryref">MAC</span>. All of the algorithms struggle with counting in natural images too. Despite TDIUC having over 164K counting questions, all methods achieve a score of under 62% on these questions.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Other CLEVR Tasks.</h4>

<div id="S5.SS1.SSS0.Px6.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px6.p1.1" class="ltx_p">As shown in Table <a href="#S5.T4" title="Table 4 ‣ Generalization Across VQA Datasets. ‣ 5.1 Main Results ‣ 5 Experiments and Results ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, <span title="" class="ltx_glossaryref">RAMEN</span> is within 0.03-1.5% of <span title="" class="ltx_glossaryref">MAC</span>’s performance on all tasks except number comparison. <span title="" class="ltx_glossaryref">UpDn</span> and <span title="" class="ltx_glossaryref">QCG</span> are the worst performing models on all query types. Except for <span title="" class="ltx_glossaryref">QCG</span>, all of the models find it easy to answer queries about object attributes and existence. Models apart from <span title="" class="ltx_glossaryref">UpDn</span> and <span title="" class="ltx_glossaryref">QCG</span> perform well on attribute comparison questions that require comparing these properties. Surprisingly, <span title="" class="ltx_glossaryref">BAN</span> finds attribute comparison, which requires more reasoning, easier than the simpler attribute query task.
We present results on CLEVR-Humans without fine-tuning to examine how well algorithms handle free-form language if they were only trained on CLEVR’s vocabulary. <span title="" class="ltx_glossaryref">BAN</span> shows the best generalization, followed by <span title="" class="ltx_glossaryref">RAMEN</span> and <span title="" class="ltx_glossaryref">RN</span>.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation studies comparing early versus late fusion between visual and question features, and comparing alternate aggregation strategies. </figcaption>
<table id="S5.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.3.1.1" class="ltx_tr">
<th id="S5.T5.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T5.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">VQAv2</span></th>
<th id="S5.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T5.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CLEVR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.3.2.1" class="ltx_tr">
<th id="S5.T5.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T5.3.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Without Early Fusion</span></th>
<td id="S5.T5.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.2.1.2.1" class="ltx_text" style="font-size:90%;">61.81</span></td>
<td id="S5.T5.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.2.1.3.1" class="ltx_text" style="font-size:90%;">77.48</span></td>
</tr>
<tr id="S5.T5.3.3.2" class="ltx_tr">
<th id="S5.T5.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T5.3.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Without Late Fusion</span></th>
<td id="S5.T5.3.3.2.2" class="ltx_td ltx_align_center"><span id="S5.T5.3.3.2.2.1" class="ltx_text" style="font-size:90%;">65.64</span></td>
<td id="S5.T5.3.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T5.3.3.2.3.1" class="ltx_text" style="font-size:90%;">96.63</span></td>
</tr>
<tr id="S5.T5.3.4.3" class="ltx_tr">
<th id="S5.T5.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T5.3.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Aggregation via Mean Pooling</span></th>
<td id="S5.T5.3.4.3.2" class="ltx_td ltx_align_center"><span id="S5.T5.3.4.3.2.1" class="ltx_text" style="font-size:90%;">63.01</span></td>
<td id="S5.T5.3.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T5.3.4.3.3.1" class="ltx_text" style="font-size:90%;">92.45</span></td>
</tr>
<tr id="S5.T5.3.5.4" class="ltx_tr">
<th id="S5.T5.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S5.T5.3.5.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Without Ablation</span></th>
<td id="S5.T5.3.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T5.3.5.4.2.1" class="ltx_text" style="font-size:90%;">65.96</span></td>
<td id="S5.T5.3.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T5.3.5.4.3.1" class="ltx_text" style="font-size:90%;">96.92</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablation Studies</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Results from several ablation studies to test the contributions of RAMEN’s components are given in Table <a href="#S5.T5" title="Table 5 ‣ Other CLEVR Tasks. ‣ 5.1 Main Results ‣ 5 Experiments and Results ‣ Answer Them All! Toward Universal Visual Question Answering Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We found that early fusion is critical to RAMEN’s performance, and removing it causes an almost 20% absolute drop in accuracy for CLEVR and a 4% drop for <span title="" class="ltx_glossaryref">VQAv2</span>. Removing late fusion has little impact on CLEVR and <span title="" class="ltx_glossaryref">VQAv2</span>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We also explored the utility of using a bi-GRU for aggregation compared to using mean pooling, and found that this caused a drop in performance for both datasets. We believe that the recurrent aggregation aids in capturing interactions between the bimodal embeddings, which is critical for reasoning tasks, and that it also helps remove duplicate proposals by performing a form of non-maximal suppression.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Newer Models</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Additional VQA algorithms have been released since we began this project, and some have achieved higher scores than the models we evaluated on <em id="S5.SS3.p1.1.1" class="ltx_emph ltx_font_italic">some</em> datasets. The Transparency By Design (TBD) network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> obtains 99.10% accuracy on <span title="" class="ltx_glossaryref">CLEVR</span> by using ground truth functional programs to train the network, which are not available for natural VQA datasets. Neural-Symbolic VQA (NS-VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> reports a score of 99.80% on CLEVR, but uses a question parser to allocate functional modules along with highly specialized segmentation-based CNN features. They did not perform ablation studies to determine the impact of using these visual features. None of the models we compare have access to these additional resources.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Results on VQAv2 can be significantly improved by using additional data from other VQA datasets and ensembling, <em id="S5.SS3.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS3.p2.1.2" class="ltx_text"></span>, the winner of the 2018 challenge used dialogues from Visual Dialog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> as additional question answer pairs and an ensemble of 30 models. These augmentations could be applied to any of the models we evaluated to improve performance. <span title="" class="ltx_glossaryref">VQACPv2</span> results can also be improved using specialized architectures, <em id="S5.SS3.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS3.p2.1.4" class="ltx_text"></span> GVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and <span title="" class="ltx_glossaryref">UpDn</span> with adversarial regularization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. However, their performance on <span title="" class="ltx_glossaryref">VQACPv2</span> is still poor, with <span title="" class="ltx_glossaryref">UpDn</span> with adversarial regularization obtaining 42.04% accuracy, showing only 2.98% improvement over the non-regularized model.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion: One Model to Rule them All?</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We conducted the first systematic study to examine if the VQA systems that work on synthetic datasets generalized to real-world datasets, and vice versa. This was the original scope of our project, but we were alarmed when we discovered none of the methods worked well across datasets. This motivated us to create a new algorithm. Despite being simpler than many algorithms, <span title="" class="ltx_glossaryref">RAMEN</span> rivals or even surpasses other methods. We believe some state-of-the-art architectures are likely over-engineered to exploit the biases in the domain they were initially tested on, resulting in a deterioration of performance when tested on other datasets. This leads us to question whether the use of highly specialized mechanisms that achieve state-of-the-art results on one specific dataset will lead to significant advances in the field, since our conceptually simpler algorithm performs competitively across both natural and synthetic datasets without such mechanisms.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We advocate for the development of a single VQA model that does well across a wide range of challenges. Training this model in a continual learning paradigm would assess forward and backward transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Another interesting avenue is to combine VQA with related tasks like visual query detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Regardless, existing algorithms, including ours, still have a long way to go toward showcasing both visuo-linguistic concept understanding <em id="S6.p2.1.1" class="ltx_emph ltx_font_italic">and</em> reasoning. As evidenced by the large performance drops on <span title="" class="ltx_glossaryref">CVQA</span> and <span title="" class="ltx_glossaryref">VQACPv2</span>, current algorithms perform poorly at learning compositional concepts and are affected by biases in these datasets, suggesting reliance on superficial correlations. We observed that methods developed solely for synthetic closed-world scenes are often unable to cope with unconstrained natural images and questions. Although performance on VQAv2 and CLEVR are approaching human levels on these benchmarks, our results show VQA is far from solved. We argue that future work should focus on creating one model that works well across domains. It would be interesting to train a dataset on a universal training set and then evaluate it on multiple test sets, with each test set demanding a different skill set. Doing so would help in seeking one VQA model that can rule them all.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Our work endeavors to set a new standard for what should be expected from a VQA algorithm: good performance across both natural scenes and challenging synthetic benchmarks. We hope that our work will lead to future advancements in VQA.</p>
</div>
<section id="S7.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgements.</h4>

<div id="S7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p1.1" class="ltx_p">We thank NVIDIA for the GPU donation. This work was supported in part by a gift from Adobe Research.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
M. Acharya, K. Jariwala, and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">VQD: Visual query detection in natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
M. Acharya, K. Kafle, and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">TallyQA: Answering complex counting questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
A. Agrawal, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Analyzing the behavior of visual question answering models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Don’t just assume; look and answer: Overcoming priors for visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
A. Agrawal, A. Kembhavi, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">C-vqa: A compositional split of the visual question answering (vqa)
v1.0 dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, abs/1704.08243, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Deep compositional question answering with neural module networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Learning to compose neural networks for question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">VQA: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
H. Ben-Younes, R. Cadene, M. Cord, and N. Thome.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Mutan: Multimodal tucker fusion for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and
D. Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Visual dialog.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
H. De Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin, and A. C.
Courville.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Modulating early visual processing by language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
M. R. Farazi and S. Khan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Reciprocal attention fusion for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">BMVC</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and
visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
R. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Fast R-CNN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
T. L. Hayes, N. D. Cahill, and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Memory efficient experience replay for streaming learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICRA</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Learning to reason: End-to-end module networks for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
D. A. Hudson and C. D. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Compositional attention networks for machine reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and
R. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman, L. Fei-Fei, C. L.
Zitnick, and R. B. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Inferring and executing programs for visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle, S. Cohen, B. Price, and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">DVQA: Understanding data visualizations via question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Answer-type prediction for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">An analysis of visual question answering algorithms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Visual question answering: Datasets, algorithms, and future
challenges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle, M. Yousefhussien, and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Data augmentation for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INLG</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Measuring catastrophic forgetting in neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
J.-H. Kim, J. Jun, and B.-T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Bilinear attention networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
J.-H. Kim, K.-W. On, J. Kim, J.-W. Ha, and B.-T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Hadamard product for low-rank bilinear pooling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
D. P. Kingma and J. Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, abs/1412.6980, 2014.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
T. N. Kipf and M. Welling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Semi-supervised classification with graph convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 123(1):32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
J. Lu, J. Yang, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Hierarchical question-image co-attention for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
M. Malinowski and C. Doersch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">The visual QA devil in the details: The impact of early fusion and
batch norm on clevr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1809.04482</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
M. Malinowski, C. Doersch, A. Santoro, and P. Battaglia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Learning visual question answering by bootstrapping hard attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
M. Malinowski and M. Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">A multi-world approach to question answering about real-world scenes
based on uncertain input.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
D. Mascharka, P. Tran, R. Soklaski, and A. Majumdar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Transparency by design: Closing the gap between performance and
interpretability in visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
H. Nam, J.-W. Ha, and J. Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Dual attention networks for multimodal reasoning and matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
D.-K. Nguyen and T. Okatani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Improved fusion of visual and language representations by dense
symmetric co-attention for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, June 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
H. Noh and B. Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Training recurrent answering units with joint loss minimization for
VQA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.03647</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
W. Norcliffe-Brown, E. Vafeais, and S. Parisot.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Learning conditioned graph structures for interpretable visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Continual lifelong learning with neural networks: A review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Networks</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
J. Pennington, R. Socher, and C. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Glove: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 1532–1543, 2014.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">FiLM: Visual Reasoning with a General Conditioning Layer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
P. Ramachandran, B. Zoph, and Q. V. Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Searching for activation functions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, abs/1710.05941, 2017.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
S. Ramakrishnan, A. Agrawal, and S. Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Overcoming language priors in visual question answering with
adversarial regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 1548–1558, 2018.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
S. Ren, K. He, R. Girshick, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Faster R-CNN: Towards real-time object detection with region
proposal networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia,
and T. Lillicrap.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">A simple neural network module for relational reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
D. Teney, P. Anderson, X. He, and A. van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Tips and tricks for visual question answering: Learnings from the
2017 challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
D. Teney and A. v. d. Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Visual question answering as a meta learning task.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, and A. v. d. Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Visual question answering: A survey of methods and datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Z. Yang, X. He, J. Gao, L. Deng, and A. J. Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Stacked attention networks for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, and J. B. Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Neural-Symbolic VQA: Disentangling Reasoning from Vision and
Language Understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhang, J. Hare, and A. Prügel-Bennett.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Learning to count objects in natural images for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1903.00365" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1903.00366" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1903.00366">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1903.00366" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1903.00367" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 07:30:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
