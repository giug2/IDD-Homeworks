<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference</title>
<!--Generated on Thu Oct 10 11:55:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.08289v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S1" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S2" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.SS1" title="In 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Supervised Fine-Tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.SS1.SSS0.Px1" title="In Figure 3 ‣ 3.1 Supervised Fine-Tuning ‣ 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title">Instruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.SS1.SSS0.Px2" title="In Figure 3 ‣ 3.1 Supervised Fine-Tuning ‣ 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title">Input</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.SS1.SSS0.Px3" title="In Figure 3 ‣ 3.1 Supervised Fine-Tuning ‣ 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title">Response</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.SS2" title="In 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Reward Modelling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.SS2.SSS1" title="In 3.2 Reward Modelling ‣ 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Format Critics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.SS3" title="In 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Reinforcement Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4.SS1" title="In 4 Experimental Setup ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4.SS2" title="In 4 Experimental Setup ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Generation Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4.SS3" title="In 4 Experimental Setup ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Data Splits</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4.SS4" title="In 4 Experimental Setup ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S5" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S5.SS1" title="In 5 Results and Discussion ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Error Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S6" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A1" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Reward Model Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A2" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Obtaining Zero-Shot Model Generations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A3" title="In Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>API-Based LLM Answerability Annotation</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
William Thorne<math alttext="\dagger" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">†</annotation></semantics></math> &amp;Ambrose Robinson<math alttext="\dagger" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">†</annotation></semantics></math> &amp;Bohua Peng<math alttext="\dagger" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><ci id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">†</annotation></semantics></math> &amp;Chenghua Lin<math alttext="\ddagger" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><mo id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><ci id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">‡</annotation></semantics></math> <span class="ltx_ERROR undefined" id="id8.8.id1">\AND</span>Diana Maynard<math alttext="\dagger" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><mo id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><ci id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">†</annotation></semantics></math>
<br class="ltx_break"/><math alttext="\dagger" class="ltx_Math" display="inline" id="id6.6.m6.1"><semantics id="id6.6.m6.1a"><mo id="id6.6.m6.1.1" xref="id6.6.m6.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id6.6.m6.1b"><ci id="id6.6.m6.1.1.cmml" xref="id6.6.m6.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m6.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id6.6.m6.1d">†</annotation></semantics></math> Department of Computer Science, University of Sheffield 
<br class="ltx_break"/><math alttext="\ddagger" class="ltx_Math" display="inline" id="id7.7.m7.1"><semantics id="id7.7.m7.1a"><mo id="id7.7.m7.1.1" xref="id7.7.m7.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id7.7.m7.1b"><ci id="id7.7.m7.1.1.cmml" xref="id7.7.m7.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id7.7.m7.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="id7.7.m7.1d">‡</annotation></semantics></math> Department of Computer Science, University of Manchester 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id2">{wthorne1, bpeng10, d.maynard}@sheffield.ac.uk</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.10.id3">ambrose@parablestudio.co.uk</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id11.11.id4">chenghua.lin@manchester.ac.uk</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">As the cultural heritage sector increasingly adopts technologies like Retrieval-Augmented Generation (RAG) to provide more personalised search experiences and enable conversations with collections data, the demand for specialised evaluation datasets has grown. While end-to-end system testing is essential, it’s equally important to assess individual components. We target the final, answering task, which is well-suited to Machine Reading Comprehension (MRC). Although existing MRC datasets address general domains, they lack the specificity needed for cultural heritage information. Unfortunately, the manual creation of such datasets is prohibitively expensive for most heritage institutions. This paper presents a cost-effective approach for generating domain-specific MRC datasets with increased difficulty using Reinforcement Learning from Human Feedback (RLHF) from synthetic preference data. Our method leverages the performance of existing question-answering models on a subset of SQuAD to create a difficulty metric, assuming that more challenging questions are answered correctly less frequently. This research contributes: (1) A methodology for increasing question difficulty using PPO and synthetic data; (2) Empirical evidence of the method’s effectiveness, including human evaluation; (3) An in-depth error analysis and study of emergent phenomena; and (4) An open-source codebase and set of three llama-2-chat adapters for reproducibility and adaptation.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.7">
<p class="ltx_p" id="p1.7.8"><span class="ltx_text ltx_font_bold" id="p1.7.8.1">Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.4.4" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.1.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1.1.1.1.1">
William Thorne<math alttext="\dagger" class="ltx_Math" display="inline" id="p1.1.1.1.1.1.1.1.m1.1"><semantics id="p1.1.1.1.1.1.1.1.m1.1a"><mo id="p1.1.1.1.1.1.1.1.m1.1.1" xref="p1.1.1.1.1.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="p1.1.1.1.1.1.1.1.m1.1b"><ci id="p1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="p1.1.1.1.1.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.1.1.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.1.1.1.1.1.1.1.m1.1d">†</annotation></semantics></math></span></span></span>
</span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.2.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.2.2.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.2.2.2.1.1">
<span class="ltx_td ltx_align_center" id="p1.2.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.2.2.2.1.1.1.1">Ambrose Robinson<math alttext="\dagger" class="ltx_Math" display="inline" id="p1.2.2.2.1.1.1.1.m1.1"><semantics id="p1.2.2.2.1.1.1.1.m1.1a"><mo id="p1.2.2.2.1.1.1.1.m1.1.1" xref="p1.2.2.2.1.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="p1.2.2.2.1.1.1.1.m1.1b"><ci id="p1.2.2.2.1.1.1.1.m1.1.1.cmml" xref="p1.2.2.2.1.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.2.2.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.2.2.2.1.1.1.1.m1.1d">†</annotation></semantics></math></span></span></span>
</span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.3.3.3" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.3.3.3.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.3.3.3.1.1">
<span class="ltx_td ltx_align_center" id="p1.3.3.3.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.3.3.3.1.1.1.1">Bohua Peng<math alttext="\dagger" class="ltx_Math" display="inline" id="p1.3.3.3.1.1.1.1.m1.1"><semantics id="p1.3.3.3.1.1.1.1.m1.1a"><mo id="p1.3.3.3.1.1.1.1.m1.1.1" xref="p1.3.3.3.1.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="p1.3.3.3.1.1.1.1.m1.1b"><ci id="p1.3.3.3.1.1.1.1.m1.1.1.cmml" xref="p1.3.3.3.1.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.3.3.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.3.3.3.1.1.1.1.m1.1d">†</annotation></semantics></math></span></span></span>
</span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.4.4.4" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.4.4.4.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.4.4.4.1.1">
<span class="ltx_td ltx_align_center" id="p1.4.4.4.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.4.4.4.1.1.1.1">Chenghua Lin<math alttext="\ddagger" class="ltx_Math" display="inline" id="p1.4.4.4.1.1.1.1.m1.1"><semantics id="p1.4.4.4.1.1.1.1.m1.1a"><mo id="p1.4.4.4.1.1.1.1.m1.1.1" xref="p1.4.4.4.1.1.1.1.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="p1.4.4.4.1.1.1.1.m1.1b"><ci id="p1.4.4.4.1.1.1.1.m1.1.1.cmml" xref="p1.4.4.4.1.1.1.1.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.4.4.1.1.1.1.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="p1.4.4.4.1.1.1.1.m1.1d">‡</annotation></semantics></math></span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.7.7" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.7.7.3" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.7.7.3.3">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.5.5.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.5.5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.5.5.1.1.1.1.1">Diana Maynard<math alttext="\dagger" class="ltx_Math" display="inline" id="p1.5.5.1.1.1.1.1.m1.1"><semantics id="p1.5.5.1.1.1.1.1.m1.1a"><mo id="p1.5.5.1.1.1.1.1.m1.1.1" xref="p1.5.5.1.1.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="p1.5.5.1.1.1.1.1.m1.1b"><ci id="p1.5.5.1.1.1.1.1.m1.1.1.cmml" xref="p1.5.5.1.1.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.5.1.1.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.5.5.1.1.1.1.1.m1.1d">†</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p1.6.6.2.2.2">
<span class="ltx_td ltx_align_center" id="p1.6.6.2.2.2.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="p1.6.6.2.2.2.1.m1.1"><semantics id="p1.6.6.2.2.2.1.m1.1a"><mo id="p1.6.6.2.2.2.1.m1.1.1" xref="p1.6.6.2.2.2.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="p1.6.6.2.2.2.1.m1.1b"><ci id="p1.6.6.2.2.2.1.m1.1.1.cmml" xref="p1.6.6.2.2.2.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.6.6.2.2.2.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.6.6.2.2.2.1.m1.1d">†</annotation></semantics></math> Department of Computer Science, University of Sheffield</span></span>
<span class="ltx_tr" id="p1.7.7.3.3.3">
<span class="ltx_td ltx_align_center" id="p1.7.7.3.3.3.1"><math alttext="\ddagger" class="ltx_Math" display="inline" id="p1.7.7.3.3.3.1.m1.1"><semantics id="p1.7.7.3.3.3.1.m1.1a"><mo id="p1.7.7.3.3.3.1.m1.1.1" xref="p1.7.7.3.3.3.1.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="p1.7.7.3.3.3.1.m1.1b"><ci id="p1.7.7.3.3.3.1.m1.1.1.cmml" xref="p1.7.7.3.3.3.1.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.7.7.3.3.3.1.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="p1.7.7.3.3.3.1.m1.1d">‡</annotation></semantics></math> Department of Computer Science, University of Manchester</span></span>
<span class="ltx_tr" id="p1.7.7.3.3.4.1">
<span class="ltx_td ltx_align_center" id="p1.7.7.3.3.4.1.1"><span class="ltx_text ltx_font_typewriter" id="p1.7.7.3.3.4.1.1.1">{wthorne1, bpeng10, d.maynard}@sheffield.ac.uk</span></span></span>
<span class="ltx_tr" id="p1.7.7.3.3.5.2">
<span class="ltx_td ltx_align_center" id="p1.7.7.3.3.5.2.1"><span class="ltx_text ltx_font_typewriter" id="p1.7.7.3.3.5.2.1.1">ambrose@parablestudio.co.uk</span></span></span>
<span class="ltx_tr" id="p1.7.7.3.3.6.3">
<span class="ltx_td ltx_align_center" id="p1.7.7.3.3.6.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.7.7.3.3.6.3.1.1">chenghua.lin@manchester.ac.uk</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The cultural heritage sector is increasingly leveraging advanced technologies like large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib30" title="">2024</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib40" title="">2023a</a>)</cite> and AI assistants <cite class="ltx_cite ltx_citemacro_citep">(Team Gemini, <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib39" title="">2023</a>; Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib2" title="">2024</a>)</cite> to increase and improve access to collections and their associated data. These technologies provide new opportunities for more dynamic and intuitive interactions with heritage content. One particularly promising technology is Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib19" title="">2021</a>)</cite>, which retrieves relevant information from a database of vectorized content to generate accurate, fact-based responses to user queries. We believe that RAG, and iterations on the approach, will play a significant role in improving the search capabilities of heritage institutions in the coming years.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="A comparison of two questions generated for an input prompt from SQuAD. The PPO model is shown to be an increase in difficulty over the supervised-fine-tuned model with respect to range of dependencies, entity disambiguation and not extracting segments from the source text." class="ltx_graphics ltx_centering ltx_img_square" height="538" id="S1.F1.g1" src="extracted/5916449/images/sample4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example generated questions from supervised-fine-tuned question generation model and one fine-tuned with PPO from synthetic difficulty samples.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Heritage search systems are used by the public and academics alike; however, the latter tend to submit more complex and specific queries <cite class="ltx_cite ltx_citemacro_citep">(Koolen and Kamps, <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib17" title="">2009</a>)</cite>. RAG has the capability to fulfil these needs but still requires robust evaluation. This includes not only end-to-end system testing but also the evaluation of individual components. As the response is generally required to be written based <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">only</span> on the retrieved documents to mitigate language model hallucinations, we argue that the task is one of Machine Reading Comprehension (MRC). While MRC datasets are well-established in the general domain, they are notably lacking in cultural heritage and the cost of their construction is prohibitive for most institutions. We estimate that the popular SQuAD dataset cost about $12,000 to just write the questions, based on their recommended time per question and stated hourly rate of $9 <cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib34" title="">2016</a>)</cite>; the actual cost is likely much higher.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these challenges, we propose using Automatic Question Generation (AQG) systems to generate MRC datasets. However, we argue that many automatically generated questions, particularly those from zero- or few-shot approaches, do not provide an adequate challenge for modern language models. Manipulating difficulty is challenging through traditional training approaches given its abstract and subjective nature, and prompt based solutions are intractable when considering the infinite permutations and interactions between different aspects of difficulty <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib21" title="">2015a</a>; Rajpurkar et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib34" title="">2016</a>; Beinborn et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib5" title="">2015</a>; Hsu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib14" title="">2018</a>; Cheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib8" title="">2021</a>; AlKhuzaey et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib1" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To control difficulty, we adapt the Reinforcement Learning from Human Feedback protocol used in AI assistant steering <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib31" title="">2022</a>; Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib4" title="">2022</a>)</cite>. In this regime, samples are ranked based on specific criteria and paired into <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">chosen</span> and <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">rejected</span> samples for training a reward model. This reward model learns to distinguish good samples from bad and outputs a signal which steers a policy model. Rather than relying on costly human annotations, we generate synthetic preference data by evaluating question-answering model performance on a subset of SQuAD, assuming that questions answered correctly less frequently are inherently more difficult. This approach leverages the language model’s innate feature extraction capabilities, eliminating the need to explicitly define difficulty components. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates this feature extraction ability by comparing questions generated with and without reinforcement fine-tuning.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We selected SQuAD over an in-domain QA dataset for two main reasons. First, it is a well-studied, large, and diverse dataset. Second, comparable QA datasets at SQuAD’s scale are either visual question-answering focused <cite class="ltx_cite ltx_citemacro_citep">(Sheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib38" title="">2016</a>; Asprino et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib3" title="">2022</a>)</cite> or have data reliability concerns such as OCR text <cite class="ltx_cite ltx_citemacro_citep">(Piryani et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib33" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">This approach enables cultural heritage practitioners to generate challenging evaluation datasets more efficiently and cost-effectively than manual curation. The primary expense is compute resources, which can be accessed in the cloud for only a few dollars per hour.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/pricing" title="">https://huggingface.co/pricing</a></span></span></span></p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We summarise this paper’s contributions as follows:</p>
</div>
<div class="ltx_para" id="S1.p8">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A methodology for increasing the difficulty of automatically generated questions using PPO and synthetic data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Empirical evidence of the methodology’s efficacy including human evaluation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">An in-depth error analysis and study of interesting phenomena that emerge as part of this approach.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">An open-source code base and set of models to recreate and adapt our work<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We release all code and a set of three LLaMa-2 adapters on <a class="ltx_ref ltx_href" href="https://github.com/wrmthorne/increasing-AQG-difficulty-via-RLHF" title="">GitHub</a>.</span></span></span>.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">A similar question generation approach to ours is employed by <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib45" title="">2022</a>)</cite> who adopt a pipeline structure. However, their primary objective is to generate suitable questions rather than specifically focusing on difficulty. An important distinction lies in their extensive pre-processing applied to identify candidate answers before feeding them to the question generation model. We argue that pre-identifying answers may limit diversity and prevent the inclusion of potentially complex answer types.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Analyzing and Controlling Question Difficulty.</span>  Understanding and managing question difficulty holds significant importance, especially in tasks involving the creation of exams and assessments <cite class="ltx_cite ltx_citemacro_citep">(Liu and Lin, <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib24" title="">2014</a>; AlKhuzaey et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib1" title="">2023</a>)</cite>. One approach, as presented by <cite class="ltx_cite ltx_citemacro_citet">Loginova et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib27" title="">2021</a>)</cite>, involves modelling the difficulty of multiple-choice questions through the use of softmax scores obtained from a pre-trained QA model. The variance in these scores is then calculated, with higher variance indicating greater difficulty.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib22" title="">2015b</a>)</cite> controls the difficulty of quiz questions through the selection of distractor answers based on semantic similarity between linked data items. This involves collecting both structured RDF data and unstructured text, computing similarity scores through K-means clustering, and generating questions and answers via template-based methods. Importantly, the semantic similarity plays a role in determining the difficulty level, with more challenging questions featuring distractors exhibiting higher semantic similarity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Reinforcement Learning with Human Feedback.</span>  RLHF is a machine learning paradigm that combines reinforcement learning with human-provided guidance to steer language models to meet the needs of users, finding frequent use in chatbot and AI assistant settings <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib31" title="">2022</a>)</cite>. The basis for most modern methods is the Proximal Policy Optimisation (PPO) algorithm <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib37" title="">2017</a>)</cite>, which iteratively enhances the language model’s policy to maximize cumulative rewards through interactions with a dataset or language simulation. It collects experiences, evaluates advantages, and updates the policy with a clipped surrogate objective to ensure stability, gradually improving the model’s performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">Automatic Question Generation.</span>  <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib7" title="">2019</a>)</cite> introduce a cross-entropy loss with a reinforcement learning-based loss function when training a gated bi-directional neural network for question generation. In this context, the reward model is optimising the semantic and syntactic quality of the question. BLEU-4, as a reward function, optimises the model for the evaluation metrics and the negative Word Movers Distance component is used to ensure semantic quality by maximising the similarity between a generated sequence and a ground truth sequence. Although question quality is maintained, other factors such as question difficulty are not considered.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Self-critic sequence training (SCST) <cite class="ltx_cite ltx_citemacro_citep">(Rennie et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib36" title="">2017</a>)</cite> uses a classical policy gradient method, REINFORCE, which is a Monte Carlo method. SCST computes rewards with n-gram token overlap as sub-sentence level rewards. Since training sets often have limited questions, these training rewards are arguably sparse, hindering the question generation model from extrapolating beyond the training distribution.
<cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib26" title="">2019</a>)</cite> adopt a two-component reward for refining ill-formed questions. Question wording is used as a measure of short-term reward, and alignment between the question and answer represents a long-term component.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To challenge the high cost of manual annotation while maintaining quality and increasing difficulty, we design and implement a robust system capable of generating contextually relevant, coherent, and challenging question-answer pairs from textual input. The process follows the core methodology of RLHF, deviating only in the use of synthetic preference data to train a reward model. Rather than explicitly defining the characteristics of difficulty and risking failure to capture certain aspects, we exploit the ability of leading question-answer models to derive which questions are challenging, and allow a reward model to extract the component features of the task.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We task three QA models with answering all questions in our validation split of SQuAD. These questions are assigned a score based on the number of times they were answered incorrectly, which are in turn used to generate pairwise preference data. These pairwise samples enable the training of a reward model (RM) for use in fine-tuning a supervised model (SFT) on the task of question generation using Proximal Policy Optimisation (PPO)<cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib37" title="">2017</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We embed this synthetic RLHF process into a greater pipeline for generating samples, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.F2" title="Figure 2 ‣ 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">2</span></a>. This ensures the quality of the final dataset. The pipeline also contains a set of rule-based critics which are used to exclude samples that are malformed and those with non-unique answers in the source text. Samples are then deduplicated using exact string matching.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="A graphical depiction of the project pipeline" class="ltx_graphics ltx_centering ltx_img_landscape" height="121" id="S3.F2.g1" src="extracted/5916449/images/project_pipeline_hd.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Depiction of our dataset generation pipeline. Question-Answering models are first used to create pairwise comparison data to train a reward model. An SFT model is trained on the train split of SQuAD and then fine-tuned using the reward model, producing the RL model. When generating question-answer pairs for the final dataset, generations are passed through the format critics to ensure data quality.</figcaption>
</figure>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The remainder of this section discusses each of the relevant components of the pipeline and the RLHF process.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Supervised Fine-Tuning</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In our training process for question generation and response formatting, we begin by employing a reformatted version of the SQuAD v1 training split (see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4.T1" title="Table 1 ‣ 4.3 Data Splits ‣ 4 Experimental Setup ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">1</span></a>). The reformatting converts SQuAD to the task of question-answer pair generation, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.F3" title="Figure 3 ‣ 3.1 Supervised Fine-Tuning ‣ 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">3</span></a>. We select the "correct" answer as the one that appears most frequently in the list of answers for each question in the dataset, selecting randomly among the most common if there is no victor. To ensure model robustness without overfitting, the model undergoes a single epoch of training, enabling it to effectively capture the nuances of the task.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<p class="ltx_p" id="S3.F3.1"><span class="ltx_inline-sectional-block ltx_parbox ltx_framed ltx_framed_rectangle" id="S3.F3.1.1">
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Instruction</h5>
<span class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<span class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">Write 1 answerable span extraction question and provide the correct answer based on the text.</span>
</span>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Input</h5>
<span class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<span class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">… Upon its arrival in Canberra, the Olympic flame was presented by Chinese officials to local Aboriginal elder <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.SS1.SSS0.Px2.p1.1.1">Agnes Shea</span>, of the Ngunnawal people. She, in turn, offered them a message stick …</span>
</span>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Response</h5>
<span class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<span class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">Who received the flame from Chinese officials in Canberra? (answer: <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.SS1.SSS0.Px3.p1.1.1">Agnes Shea</span>)</span>
</span>
</section></span>
</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example training sample from the reformatted SQuAD dataset for use in supervised fine-tuning.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Reward Modelling</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To control the difficulty of our generated questions, we leverage the intrinsic properties present in challenging questions from SQuAD. To extract these attributes, we employ three question answering models that almost match or exceed human performance on SQuAD v2 to evaluate our development split: a RoBERTa-large model<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href" href="https://huggingface.co/deepset/roberta-large-squad2" title="">deepset/roberta-large-squad2</a></span></span></span>, a DeBERTa-large model<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_href" href="https://huggingface.co/deepset/deberta-v3-large-squad2" title="">deepset/deberta-v3-large-squad2</a></span></span></span> and RetroReader <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib46" title="">2020</a>)</cite>. Each question is assigned a score based on the number of models that failed to correctly answer the question. These scores are used to place questions into a pairwise ranking setup against other questions for the same input context. Where a question’s scores are equal, they are considered ties, and no pairwise sample is created. We also record the margin, defined as the difference in score between the chosen and rejected samples, to experiment with the marginal ranking loss, as defined in <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib41" title="">2023b</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Format Critics</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">To ensure the quality of the final dataset, we utilise a collection of rule-based critics which we call <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.1.1">Format Critics</span>. These critics have two main functions: they remove questions that don’t adhere to the desired format of <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.1.2">Q? (answer: A)</span>; they ensure the provided answer is unique in the text, minimising the number of ambiguous or impossible questions. Samples that pass these critics are then deduplicated using exact matching.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Reinforcement Training</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We use Proximal Policy Optimisation <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib37" title="">2017</a>)</cite> with multiple sets of adapters to reduce the memory overhead during training, implemented using the Transformers Reinforcement Learning library <cite class="ltx_cite ltx_citemacro_citep">(von Werra et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib42" title="">2020</a>)</cite>. A single base model is used with separate LoRA adapters for the policy, reference, and reward model components; each is switched to perform the relevant aspect of the reinforcement training process.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">During early experiments, we found that training was often very unstable or resulted in low pass rates at the format critic. To combat this, we added a rule-based reward component to penalise generations that did not pass the format critic. This simple function converts the reward to be the negative absolute reward in the case that samples are malformed. Using a rule-based reward that manipulates the original reward prevents the instability caused by hard coding a fixed penalty and saves the computational complexity and imperfection of a second adapter-based reward model:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="R_{i}=\begin{cases}-|R_{i}|&amp;\text{if malformed}\\
R_{i}&amp;\text{otherwise}\end{cases}" class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.5" xref="S3.E1.m1.4.5.cmml"><msub id="S3.E1.m1.4.5.2" xref="S3.E1.m1.4.5.2.cmml"><mi id="S3.E1.m1.4.5.2.2" xref="S3.E1.m1.4.5.2.2.cmml">R</mi><mi id="S3.E1.m1.4.5.2.3" xref="S3.E1.m1.4.5.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.4.5.1" xref="S3.E1.m1.4.5.1.cmml">=</mo><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.5.3.1.cmml"><mo id="S3.E1.m1.4.4.5" xref="S3.E1.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S3.E1.m1.4.4.4" rowspacing="0pt" xref="S3.E1.m1.4.5.3.1.cmml"><mtr id="S3.E1.m1.4.4.4a" xref="S3.E1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.4.4.4b" xref="S3.E1.m1.4.5.3.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.4.4.4c" xref="S3.E1.m1.4.5.3.1.cmml"><mtext id="S3.E1.m1.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.1a.cmml">if malformed</mtext></mtd></mtr><mtr id="S3.E1.m1.4.4.4d" xref="S3.E1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.4.4.4e" xref="S3.E1.m1.4.5.3.1.cmml"><msub id="S3.E1.m1.3.3.3.3.1.1" xref="S3.E1.m1.3.3.3.3.1.1.cmml"><mi id="S3.E1.m1.3.3.3.3.1.1.2" xref="S3.E1.m1.3.3.3.3.1.1.2.cmml">R</mi><mi id="S3.E1.m1.3.3.3.3.1.1.3" xref="S3.E1.m1.3.3.3.3.1.1.3.cmml">i</mi></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.4.4.4f" xref="S3.E1.m1.4.5.3.1.cmml"><mtext id="S3.E1.m1.4.4.4.4.2.1" xref="S3.E1.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.5.cmml" xref="S3.E1.m1.4.5"><eq id="S3.E1.m1.4.5.1.cmml" xref="S3.E1.m1.4.5.1"></eq><apply id="S3.E1.m1.4.5.2.cmml" xref="S3.E1.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.5.2.1.cmml" xref="S3.E1.m1.4.5.2">subscript</csymbol><ci id="S3.E1.m1.4.5.2.2.cmml" xref="S3.E1.m1.4.5.2.2">𝑅</ci><ci id="S3.E1.m1.4.5.2.3.cmml" xref="S3.E1.m1.4.5.2.3">𝑖</ci></apply><apply id="S3.E1.m1.4.5.3.1.cmml" xref="S3.E1.m1.4.4"><csymbol cd="latexml" id="S3.E1.m1.4.5.3.1.1.cmml" xref="S3.E1.m1.4.4.5">cases</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><abs id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"></abs><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply><ci id="S3.E1.m1.2.2.2.2.2.1a.cmml" xref="S3.E1.m1.2.2.2.2.2.1"><mtext id="S3.E1.m1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1">if malformed</mtext></ci><apply id="S3.E1.m1.3.3.3.3.1.1.cmml" xref="S3.E1.m1.3.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.3.3.1.1.2">𝑅</ci><ci id="S3.E1.m1.3.3.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.3.3.1.1.3">𝑖</ci></apply><ci id="S3.E1.m1.4.4.4.4.2.1a.cmml" xref="S3.E1.m1.4.4.4.4.2.1"><mtext id="S3.E1.m1.4.4.4.4.2.1.cmml" xref="S3.E1.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">R_{i}=\begin{cases}-|R_{i}|&amp;\text{if malformed}\\
R_{i}&amp;\text{otherwise}\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { start_ROW start_CELL - | italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_CELL start_CELL if malformed end_CELL end_ROW start_ROW start_CELL italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_CELL start_CELL otherwise end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Models</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conduct our experiments with LLaMa2-7B-chat and apply LoRA adapters to all linear layers for all models. This drastically lowers the number of tunable parameters over full-finetuning, enabling training on a single A100 80GB GPU. We also make use of Flash Attention 2 <cite class="ltx_cite ltx_citemacro_citep">(Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib9" title="">2023</a>)</cite> to improve computational and memory efficiency. All LoRA adapters share the same hyperparameters: a LoRA rank of 16, as <cite class="ltx_cite ltx_citemacro_citet">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib10" title="">2023</a>)</cite> found rank to have minimal impact on task performance while enabling larger batches through reduced memory usage. This memory efficiency further allowed us to implement sample packing, particularly beneficial with Flash Attention 2’s preference for minimal padding. We set alpha to twice the rank <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lightning.ai/pages/community/lora-insights/" title="">https://lightning.ai/pages/community/lora-insights/</a></span></span></span>, use a dropout of 0.05 - shown optimal for 7B models by <cite class="ltx_cite ltx_citemacro_citet">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib10" title="">2023</a>)</cite>, and maintain LLaMa-2’s BF16. As a baseline, we compare to LLaMa-2-7B-chat in a zero-shot setting (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A2" title="Appendix B Obtaining Zero-Shot Model Generations ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">B</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We experiment with marginal ranking loss to help distinguish between slight and significant differences in question difficulty while training the reward model. Under the hypothesis that the difficulty of a question is not independent of the associated passage of text, we also experiment with training a reward model with and without the input text attached. Results of these experiments can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A1" title="Appendix A Reward Model Performance ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Generation Settings</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">During generation, the model is tasked with producing a single output for each question in the training set using nucleus sampling <cite class="ltx_cite ltx_citemacro_citep">(Holtzman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib13" title="">2020</a>)</cite>. We maintain the original configuration for LLaMa-2 with a repetition penalty of 1.1, top P of 0.7, and top K of 0 but increase the temperature from 0.6 to 0.9 to increase the diversity of generations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Data Splits</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We base our splits off the original SQuAD to minimise the risk of data leakage. We maintain the full train split unchanged as any model previously trained on SQuAD will have seen the full train split. We extract a test split of 500 contexts from the dev split, ensuring no contexts appear in both the dev and test splits. We extract 50 unique contexts from the test split for a human evaluation of question quality and answerability. In all cases, context-question pairs were only kept if they fit into the context length of LLaMa-2 when formatted in the correct prompt format. All samples were formatted into the three instruction components: <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">instruction</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">input</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.3">response</span> as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S3.F3" title="Figure 3 ‣ 3.1 Supervised Fine-Tuning ‣ 3 Method ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Only the dev set of our SQuAD dataset was used to derive difficulty comparison data, to ensure the reward model never sees the samples used for evaluation. To evaluate the reward model, we extract 10% of the comparison contexts. Full dataset statistics can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4.T1" title="Table 1 ‣ 4.3 Data Splits ‣ 4 Experimental Setup ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Split</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1"># Contexts</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1"># Questions</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.2.1">Train</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.2">18,891</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.3">87,599</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.3.3.1">Dev</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.2">1,567</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.3">8,038</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.4.4.1">Test</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.2">500</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.3">2,532</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.5.5.1">Human Test</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.2">50</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.3">50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.6.6.1">Train comp.</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.6.2">1,107</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.6.6.3">8,394</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T1.1.7.7.1">Dev comp.</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.7.7.2">123</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.7.7.3">950</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Split of contexts and questions from SQuAD. The <span class="ltx_text ltx_font_italic" id="S4.T1.3.1">comp.</span> splits are derived from the dev split, used to evaluate the performance of the reward model during training.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">As our goal is to evaluate the difficulty of answerable questions, we provide the input passage, question and answer to GPT-4o<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>gpt-4o as of 1st June 2024</span></span></span> and Gemini-1.5-pro<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>gemini-1.5-pro as of 1st June 2024</span></span></span> and ask whether the sample meets our specification of validity. We take samples to be answerable if they were unanimously labelled as such, and reject all other samples. GPT-based evaluations have demonstrated a robust alignment with human preferences across various complex tasks in reference-free settings <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib12" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib25" title="">2023</a>)</cite>. The results of this analysis can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A3" title="Appendix C API-Based LLM Answerability Annotation ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.20">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.5.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1">Total Valid (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.1">DeBERTa (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.m1.1"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><ci id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.m1.1d">↓</annotation></semantics></math>)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.1">RoBERTa (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.1.m1.1"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><ci id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.1.m1.1d">↓</annotation></semantics></math>)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.1">RetroReader (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.1.m1.1"><semantics id="S4.T2.4.4.4.1.m1.1a"><mo id="S4.T2.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T2.4.4.4.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><ci id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.1.m1.1d">↓</annotation></semantics></math>)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.20.21.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.20.21.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.20.21.1.1.1">SQuAD</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.20.21.1.2">2,532 (-)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.20.21.1.3">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.20.21.1.4">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.20.21.1.5">0.65</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.8.8.5"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.5.1">ZeroShot</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.5.5.1">357 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.5.5.1.m1.1"><semantics id="S4.T2.5.5.1.m1.1a"><mo id="S4.T2.5.5.1.m1.1.1" xref="S4.T2.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T2.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.1.m1.1d">±</annotation></semantics></math> 14 (0.14)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.6.2"><math alttext="0.644\pm 0.007" class="ltx_Math" display="inline" id="S4.T2.6.6.2.m1.1"><semantics id="S4.T2.6.6.2.m1.1a"><mrow id="S4.T2.6.6.2.m1.1.1" xref="S4.T2.6.6.2.m1.1.1.cmml"><mn id="S4.T2.6.6.2.m1.1.1.2" xref="S4.T2.6.6.2.m1.1.1.2.cmml">0.644</mn><mo id="S4.T2.6.6.2.m1.1.1.1" xref="S4.T2.6.6.2.m1.1.1.1.cmml">±</mo><mn id="S4.T2.6.6.2.m1.1.1.3" xref="S4.T2.6.6.2.m1.1.1.3.cmml">0.007</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.2.m1.1b"><apply id="S4.T2.6.6.2.m1.1.1.cmml" xref="S4.T2.6.6.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.6.6.2.m1.1.1.1.cmml" xref="S4.T2.6.6.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.6.6.2.m1.1.1.2.cmml" type="float" xref="S4.T2.6.6.2.m1.1.1.2">0.644</cn><cn id="S4.T2.6.6.2.m1.1.1.3.cmml" type="float" xref="S4.T2.6.6.2.m1.1.1.3">0.007</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.2.m1.1c">0.644\pm 0.007</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.2.m1.1d">0.644 ± 0.007</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.7.7.3"><math alttext="0.650\pm 0.007" class="ltx_Math" display="inline" id="S4.T2.7.7.3.m1.1"><semantics id="S4.T2.7.7.3.m1.1a"><mrow id="S4.T2.7.7.3.m1.1.1" xref="S4.T2.7.7.3.m1.1.1.cmml"><mn id="S4.T2.7.7.3.m1.1.1.2" xref="S4.T2.7.7.3.m1.1.1.2.cmml">0.650</mn><mo id="S4.T2.7.7.3.m1.1.1.1" xref="S4.T2.7.7.3.m1.1.1.1.cmml">±</mo><mn id="S4.T2.7.7.3.m1.1.1.3" xref="S4.T2.7.7.3.m1.1.1.3.cmml">0.007</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.3.m1.1b"><apply id="S4.T2.7.7.3.m1.1.1.cmml" xref="S4.T2.7.7.3.m1.1.1"><csymbol cd="latexml" id="S4.T2.7.7.3.m1.1.1.1.cmml" xref="S4.T2.7.7.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.7.7.3.m1.1.1.2.cmml" type="float" xref="S4.T2.7.7.3.m1.1.1.2">0.650</cn><cn id="S4.T2.7.7.3.m1.1.1.3.cmml" type="float" xref="S4.T2.7.7.3.m1.1.1.3">0.007</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.3.m1.1c">0.650\pm 0.007</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.3.m1.1d">0.650 ± 0.007</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.8.8.4"><math alttext="0.629\pm 0.009" class="ltx_Math" display="inline" id="S4.T2.8.8.4.m1.1"><semantics id="S4.T2.8.8.4.m1.1a"><mrow id="S4.T2.8.8.4.m1.1.1" xref="S4.T2.8.8.4.m1.1.1.cmml"><mn id="S4.T2.8.8.4.m1.1.1.2" xref="S4.T2.8.8.4.m1.1.1.2.cmml">0.629</mn><mo id="S4.T2.8.8.4.m1.1.1.1" xref="S4.T2.8.8.4.m1.1.1.1.cmml">±</mo><mn id="S4.T2.8.8.4.m1.1.1.3" xref="S4.T2.8.8.4.m1.1.1.3.cmml">0.009</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.4.m1.1b"><apply id="S4.T2.8.8.4.m1.1.1.cmml" xref="S4.T2.8.8.4.m1.1.1"><csymbol cd="latexml" id="S4.T2.8.8.4.m1.1.1.1.cmml" xref="S4.T2.8.8.4.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.8.8.4.m1.1.1.2.cmml" type="float" xref="S4.T2.8.8.4.m1.1.1.2">0.629</cn><cn id="S4.T2.8.8.4.m1.1.1.3.cmml" type="float" xref="S4.T2.8.8.4.m1.1.1.3">0.009</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.4.m1.1c">0.629\pm 0.009</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.4.m1.1d">0.629 ± 0.009</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.12.12.5"><span class="ltx_text ltx_font_bold" id="S4.T2.12.12.5.1">SFT</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.9.9.1">1252 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.9.9.1.m1.1"><semantics id="S4.T2.9.9.1.m1.1a"><mo id="S4.T2.9.9.1.m1.1.1" xref="S4.T2.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T2.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.1.m1.1d">±</annotation></semantics></math> 2 (0.49)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.10.10.2"><math alttext="0.654\pm 0.012" class="ltx_Math" display="inline" id="S4.T2.10.10.2.m1.1"><semantics id="S4.T2.10.10.2.m1.1a"><mrow id="S4.T2.10.10.2.m1.1.1" xref="S4.T2.10.10.2.m1.1.1.cmml"><mn id="S4.T2.10.10.2.m1.1.1.2" xref="S4.T2.10.10.2.m1.1.1.2.cmml">0.654</mn><mo id="S4.T2.10.10.2.m1.1.1.1" xref="S4.T2.10.10.2.m1.1.1.1.cmml">±</mo><mn id="S4.T2.10.10.2.m1.1.1.3" xref="S4.T2.10.10.2.m1.1.1.3.cmml">0.012</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.2.m1.1b"><apply id="S4.T2.10.10.2.m1.1.1.cmml" xref="S4.T2.10.10.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.10.10.2.m1.1.1.1.cmml" xref="S4.T2.10.10.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.10.10.2.m1.1.1.2.cmml" type="float" xref="S4.T2.10.10.2.m1.1.1.2">0.654</cn><cn id="S4.T2.10.10.2.m1.1.1.3.cmml" type="float" xref="S4.T2.10.10.2.m1.1.1.3">0.012</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.2.m1.1c">0.654\pm 0.012</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.2.m1.1d">0.654 ± 0.012</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.11.3"><math alttext="0.653\pm 0.005" class="ltx_Math" display="inline" id="S4.T2.11.11.3.m1.1"><semantics id="S4.T2.11.11.3.m1.1a"><mrow id="S4.T2.11.11.3.m1.1.1" xref="S4.T2.11.11.3.m1.1.1.cmml"><mn id="S4.T2.11.11.3.m1.1.1.2" xref="S4.T2.11.11.3.m1.1.1.2.cmml">0.653</mn><mo id="S4.T2.11.11.3.m1.1.1.1" xref="S4.T2.11.11.3.m1.1.1.1.cmml">±</mo><mn id="S4.T2.11.11.3.m1.1.1.3" xref="S4.T2.11.11.3.m1.1.1.3.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.3.m1.1b"><apply id="S4.T2.11.11.3.m1.1.1.cmml" xref="S4.T2.11.11.3.m1.1.1"><csymbol cd="latexml" id="S4.T2.11.11.3.m1.1.1.1.cmml" xref="S4.T2.11.11.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.11.11.3.m1.1.1.2.cmml" type="float" xref="S4.T2.11.11.3.m1.1.1.2">0.653</cn><cn id="S4.T2.11.11.3.m1.1.1.3.cmml" type="float" xref="S4.T2.11.11.3.m1.1.1.3">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.3.m1.1c">0.653\pm 0.005</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.11.3.m1.1d">0.653 ± 0.005</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.4"><math alttext="0.616\pm 0.015" class="ltx_Math" display="inline" id="S4.T2.12.12.4.m1.1"><semantics id="S4.T2.12.12.4.m1.1a"><mrow id="S4.T2.12.12.4.m1.1.1" xref="S4.T2.12.12.4.m1.1.1.cmml"><mn id="S4.T2.12.12.4.m1.1.1.2" xref="S4.T2.12.12.4.m1.1.1.2.cmml">0.616</mn><mo id="S4.T2.12.12.4.m1.1.1.1" xref="S4.T2.12.12.4.m1.1.1.1.cmml">±</mo><mn id="S4.T2.12.12.4.m1.1.1.3" xref="S4.T2.12.12.4.m1.1.1.3.cmml">0.015</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.4.m1.1b"><apply id="S4.T2.12.12.4.m1.1.1.cmml" xref="S4.T2.12.12.4.m1.1.1"><csymbol cd="latexml" id="S4.T2.12.12.4.m1.1.1.1.cmml" xref="S4.T2.12.12.4.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.12.12.4.m1.1.1.2.cmml" type="float" xref="S4.T2.12.12.4.m1.1.1.2">0.616</cn><cn id="S4.T2.12.12.4.m1.1.1.3.cmml" type="float" xref="S4.T2.12.12.4.m1.1.1.3">0.015</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.4.m1.1c">0.616\pm 0.015</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.12.4.m1.1d">0.616 ± 0.015</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.16.16.5"><span class="ltx_text ltx_font_bold" id="S4.T2.16.16.5.1">PPO-input</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.13.13.1"><span class="ltx_text ltx_font_bold" id="S4.T2.13.13.1.1">1375 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.13.13.1.1.m1.1"><semantics id="S4.T2.13.13.1.1.m1.1a"><mo id="S4.T2.13.13.1.1.m1.1.1" xref="S4.T2.13.13.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.13.13.1.1.m1.1.1.cmml" xref="S4.T2.13.13.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.13.1.1.m1.1d">±</annotation></semantics></math> 18 (0.54)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.14.14.2"><span class="ltx_text ltx_font_bold" id="S4.T2.14.14.2.1">0.601 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.14.14.2.1.m1.1"><semantics id="S4.T2.14.14.2.1.m1.1a"><mo id="S4.T2.14.14.2.1.m1.1.1" xref="S4.T2.14.14.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.2.1.m1.1b"><csymbol cd="latexml" id="S4.T2.14.14.2.1.m1.1.1.cmml" xref="S4.T2.14.14.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.14.2.1.m1.1d">±</annotation></semantics></math> 0.004</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.15.15.3"><span class="ltx_text ltx_font_bold" id="S4.T2.15.15.3.1">0.606 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.15.15.3.1.m1.1"><semantics id="S4.T2.15.15.3.1.m1.1a"><mo id="S4.T2.15.15.3.1.m1.1.1" xref="S4.T2.15.15.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.3.1.m1.1b"><csymbol cd="latexml" id="S4.T2.15.15.3.1.m1.1.1.cmml" xref="S4.T2.15.15.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.15.15.3.1.m1.1d">±</annotation></semantics></math> 0.003</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.16.16.4"><span class="ltx_text ltx_font_bold" id="S4.T2.16.16.4.1">0.582 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.16.16.4.1.m1.1"><semantics id="S4.T2.16.16.4.1.m1.1a"><mo id="S4.T2.16.16.4.1.m1.1.1" xref="S4.T2.16.16.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.4.1.m1.1b"><csymbol cd="latexml" id="S4.T2.16.16.4.1.m1.1.1.cmml" xref="S4.T2.16.16.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.16.16.4.1.m1.1d">±</annotation></semantics></math> 0.007</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.20.20.5"><span class="ltx_text ltx_font_bold" id="S4.T2.20.20.5.1">PPO-input-margin</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.17.17.1">1373 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.17.17.1.m1.1"><semantics id="S4.T2.17.17.1.m1.1a"><mo id="S4.T2.17.17.1.m1.1.1" xref="S4.T2.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.1.m1.1b"><csymbol cd="latexml" id="S4.T2.17.17.1.m1.1.1.cmml" xref="S4.T2.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.17.17.1.m1.1d">±</annotation></semantics></math> 4 (0.54)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.18.18.2"><math alttext="0.612\pm 0.001" class="ltx_Math" display="inline" id="S4.T2.18.18.2.m1.1"><semantics id="S4.T2.18.18.2.m1.1a"><mrow id="S4.T2.18.18.2.m1.1.1" xref="S4.T2.18.18.2.m1.1.1.cmml"><mn id="S4.T2.18.18.2.m1.1.1.2" xref="S4.T2.18.18.2.m1.1.1.2.cmml">0.612</mn><mo id="S4.T2.18.18.2.m1.1.1.1" xref="S4.T2.18.18.2.m1.1.1.1.cmml">±</mo><mn id="S4.T2.18.18.2.m1.1.1.3" xref="S4.T2.18.18.2.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.2.m1.1b"><apply id="S4.T2.18.18.2.m1.1.1.cmml" xref="S4.T2.18.18.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.18.18.2.m1.1.1.1.cmml" xref="S4.T2.18.18.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.18.18.2.m1.1.1.2.cmml" type="float" xref="S4.T2.18.18.2.m1.1.1.2">0.612</cn><cn id="S4.T2.18.18.2.m1.1.1.3.cmml" type="float" xref="S4.T2.18.18.2.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.2.m1.1c">0.612\pm 0.001</annotation><annotation encoding="application/x-llamapun" id="S4.T2.18.18.2.m1.1d">0.612 ± 0.001</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.19.19.3"><math alttext="0.608\pm 0.005" class="ltx_Math" display="inline" id="S4.T2.19.19.3.m1.1"><semantics id="S4.T2.19.19.3.m1.1a"><mrow id="S4.T2.19.19.3.m1.1.1" xref="S4.T2.19.19.3.m1.1.1.cmml"><mn id="S4.T2.19.19.3.m1.1.1.2" xref="S4.T2.19.19.3.m1.1.1.2.cmml">0.608</mn><mo id="S4.T2.19.19.3.m1.1.1.1" xref="S4.T2.19.19.3.m1.1.1.1.cmml">±</mo><mn id="S4.T2.19.19.3.m1.1.1.3" xref="S4.T2.19.19.3.m1.1.1.3.cmml">0.005</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.19.19.3.m1.1b"><apply id="S4.T2.19.19.3.m1.1.1.cmml" xref="S4.T2.19.19.3.m1.1.1"><csymbol cd="latexml" id="S4.T2.19.19.3.m1.1.1.1.cmml" xref="S4.T2.19.19.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.19.19.3.m1.1.1.2.cmml" type="float" xref="S4.T2.19.19.3.m1.1.1.2">0.608</cn><cn id="S4.T2.19.19.3.m1.1.1.3.cmml" type="float" xref="S4.T2.19.19.3.m1.1.1.3">0.005</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.19.3.m1.1c">0.608\pm 0.005</annotation><annotation encoding="application/x-llamapun" id="S4.T2.19.19.3.m1.1d">0.608 ± 0.005</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.20.20.4"><math alttext="0.587\pm 0.002" class="ltx_Math" display="inline" id="S4.T2.20.20.4.m1.1"><semantics id="S4.T2.20.20.4.m1.1a"><mrow id="S4.T2.20.20.4.m1.1.1" xref="S4.T2.20.20.4.m1.1.1.cmml"><mn id="S4.T2.20.20.4.m1.1.1.2" xref="S4.T2.20.20.4.m1.1.1.2.cmml">0.587</mn><mo id="S4.T2.20.20.4.m1.1.1.1" xref="S4.T2.20.20.4.m1.1.1.1.cmml">±</mo><mn id="S4.T2.20.20.4.m1.1.1.3" xref="S4.T2.20.20.4.m1.1.1.3.cmml">0.002</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.20.20.4.m1.1b"><apply id="S4.T2.20.20.4.m1.1.1.cmml" xref="S4.T2.20.20.4.m1.1.1"><csymbol cd="latexml" id="S4.T2.20.20.4.m1.1.1.1.cmml" xref="S4.T2.20.20.4.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.20.20.4.m1.1.1.2.cmml" type="float" xref="S4.T2.20.20.4.m1.1.1.2">0.587</cn><cn id="S4.T2.20.20.4.m1.1.1.3.cmml" type="float" xref="S4.T2.20.20.4.m1.1.1.3">0.002</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.20.4.m1.1c">0.587\pm 0.002</annotation><annotation encoding="application/x-llamapun" id="S4.T2.20.20.4.m1.1d">0.587 ± 0.002</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Question-Answering model performance on each set of samples. Models were only supplied samples which passed the format critics and were unanimously deemed answerable by GPT-4o and Gemini-1.5-pro. The <span class="ltx_text ltx_font_italic" id="S4.T2.22.1">Total Valid</span> column indicates this number of valid samples used during question answering. Accuracy is based on exact match and results are mean and standard deviation across three sets of generated samples. Lower accuracy indicates harder questions.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">To assess the quality of generated questions relative to our SQuAD test split, we <span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.1">intentionally avoid</span> <math alttext="n" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">italic_n</annotation></semantics></math>-gram based metrics such as BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib32" title="">2002</a>)</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib23" title="">2004</a>)</cite>, and more modern alternatives such as Q-Metrics <cite class="ltx_cite ltx_citemacro_citep">(Nema and Khapra, <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib29" title="">2018</a>)</cite>, as we believe they restrict diversity of generation, constraining the model to reference questions and answers. We instead adopt the following reference-free metrics:</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.1.1">Syntactic Divergence</span> provides a distance measure between two dependency paths which acts as a measure of difficulty. Word-lemma anchors, common to both the question and answer sentence, are first detected. A dependency path from the anchor to the interrogative word (who, what, etc.) in the question is compared to the dependency path between the anchor and the answer span in the answer sentence using Levenshtein distance <cite class="ltx_cite ltx_citemacro_citep">(Levenshtein et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib18" title="">1966</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">RQUGE</span> calculates an <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.2">acceptability-score</span> by generating an answer for the candidate question and predicting the semantic similarity between the predicted answer and the gold answer provided by the user. In our setup, this metric acts as an assessment of both the question and answer quality <cite class="ltx_cite ltx_citemacro_citep">(Mohammadshahi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib28" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p5.1.1">QAScore</span> attempts to align AQG evaluation to human judgements. Question-answer pairs are evaluated by summing log-probabilities of RoBERTa correct token predictions for all words in the answer when masked individually. QAScore claims to show strong correlation with human judgement (Spearman <math alttext="r=0.864" class="ltx_Math" display="inline" id="S4.SS4.p5.1.m1.1"><semantics id="S4.SS4.p5.1.m1.1a"><mrow id="S4.SS4.p5.1.m1.1.1" xref="S4.SS4.p5.1.m1.1.1.cmml"><mi id="S4.SS4.p5.1.m1.1.1.2" xref="S4.SS4.p5.1.m1.1.1.2.cmml">r</mi><mo id="S4.SS4.p5.1.m1.1.1.1" xref="S4.SS4.p5.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS4.p5.1.m1.1.1.3" xref="S4.SS4.p5.1.m1.1.1.3.cmml">0.864</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.1.m1.1b"><apply id="S4.SS4.p5.1.m1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1"><eq id="S4.SS4.p5.1.m1.1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1.1"></eq><ci id="S4.SS4.p5.1.m1.1.1.2.cmml" xref="S4.SS4.p5.1.m1.1.1.2">𝑟</ci><cn id="S4.SS4.p5.1.m1.1.1.3.cmml" type="float" xref="S4.SS4.p5.1.m1.1.1.3">0.864</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.1.m1.1c">r=0.864</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.1.m1.1d">italic_r = 0.864</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib15" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p6.1.1">Self-BLEU</span> assesses how similar questions are to other questions generated for a given context. Each question is taken as a hypothesis and the others as a reference for the BLEU calculation. The self-BLEU is taken as the average BLEU for the question collection <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib47" title="">2018</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Model Accuracy.</span>  To measure performance, we observe the difference in prediction accuracy for QA models on each dataset. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4.T2" title="Table 2 ‣ 4.4 Evaluation Metrics ‣ 4 Experimental Setup ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">2</span></a> shows that in all cases of PPO training, we observe a decrease in average model prediction accuracy and an increase in the total number of valid generations. The consistent decrease in absolute prediction accuracy for all models when using the PPO trained models over both zero-shot and SFT signifies an increase in average question difficulty. The SFT process vastly improves the model’s ability to generate valid questions. The PPO process further bolsters this capability which illustrates that the model is learning the intrinsic properties of high-quality questions. The performance of the reward models, shown in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A1" title="Appendix A Reward Model Performance ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">A</span></a>, is reflected here, showing lesser degrees of improvement for those models fine-tuned without access to the input passage.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">External Metrics.</span>   Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S5.F4" title="Figure 4 ‣ 5 Results and Discussion ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">4</span></a> shows results for the reference-free metrics. RQUGE is clearly effective at discriminating between human-written SQuAD samples, those generated by the fine-tuned models and the zero-shot examples, but it is unable to separate out the SFT and PPO results. The particularly high score for SQuAD could in part be due to data leakage as the answer generation model for the metric was trained on SQuAD <cite class="ltx_cite ltx_citemacro_citep">(Khashabi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib16" title="">2022</a>)</cite>. This would indicate why our newly generated questions might score lower as it cannot have memorised the answer. Syntactic divergence results for the SQuAD test split and all trained model generations follow a consistent distribution but the zero-shot results appear much better, despite having a higher average prediction accuracy than the SFT and PPO models. Zero-shot obtaining higher syntactic divergence could stem from the general purpose language generation objective of LLaMa-2-chat. This could cause the model to generate boilerplate text which distances the structure of the question from that of the answer sentence but doesn’t necessarily result in a more difficult question. QAScore proves uninformative, only being able to subtly identify SQuAD samples from model generated samples. Self-BLEU indicates that SQuAD samples are the most diverse, which is to be expected, but that zero-shot samples exhibit a distinct lack of diversity when compared with fine-tuned models. This result is, in part, misleading as Self-BLEU was only calculable for input passages with at least two valid questions. As the number of valid generations was so low for the zero-shot model, the cases where multiple valid questions were generated for a context was disproportionately in favour of identical generations.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="KDE Plots showing the distribution of scores for each metric, for each model. ZeroShot is shown to have anomalous syntactic divergence, SQuAD performs significantly better on RQUGE, Most models are matched on QA-Score and ZeroShot once again shows an anomalously high diversity on self-bleu." class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S5.F4.g1" src="extracted/5916449/images/valid_metrics.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Distribution of reference free metrics results for each model’s generations based on our SQuAD test set.</figcaption>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">In general we find the reference-free metrics to show limited correlation with model prediction accuracy and an ability to differentiate human written samples from model generations. We believe this is evidence for the continued need for more reliable, reference-free evaluation tools for question generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">Human Evaluation.</span>  To evaluate question quality, we conduct a human evaluation on a subset of 50 passages from the test split. Each input passage and question is filtered through the format critic then provided to two annotators who select either the correct answer span or indicate that the question cannot be answered. In the case of annotator disagreement or the annotated answers differing from the model generated answer, the annotator responses and the model answer are provided to two new annotators who both select which responses are appropriate. We allow annotators to select multiple responses as correct but only include those that were selected unanimously by both annotators as valid. We observe an agreement of <math alttext="\kappa=0.7975" class="ltx_Math" display="inline" id="S5.p4.1.m1.1"><semantics id="S5.p4.1.m1.1a"><mrow id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml"><mi id="S5.p4.1.m1.1.1.2" xref="S5.p4.1.m1.1.1.2.cmml">κ</mi><mo id="S5.p4.1.m1.1.1.1" xref="S5.p4.1.m1.1.1.1.cmml">=</mo><mn id="S5.p4.1.m1.1.1.3" xref="S5.p4.1.m1.1.1.3.cmml">0.7975</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><apply id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1"><eq id="S5.p4.1.m1.1.1.1.cmml" xref="S5.p4.1.m1.1.1.1"></eq><ci id="S5.p4.1.m1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.2">𝜅</ci><cn id="S5.p4.1.m1.1.1.3.cmml" type="float" xref="S5.p4.1.m1.1.1.3">0.7975</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">\kappa=0.7975</annotation><annotation encoding="application/x-llamapun" id="S5.p4.1.m1.1d">italic_κ = 0.7975</annotation></semantics></math> between annotators. The results of this evaluation, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S5.T3" title="Table 3 ‣ 5 Results and Discussion ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">3</span></a>, displays an equivalent or improved rate of answerability when fine-tuning with PPO; the answerability proportions for each dataset are roughly equivalent to those presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S4.T2" title="Table 2 ‣ 4.4 Evaluation Metrics ‣ 4 Experimental Setup ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">2</span></a>. This further corroborates the efficacy of our approach.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Full</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Partial</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.1.1.1">ZeroShot</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.2.1.2">0.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.3">0.14</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.2.1.1">SFT</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.3.2.2">0.52</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.3">0.60</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.3.1.1">PPO-input</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.4.3.2">0.52</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.3">0.64</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.4.1.1">PPO-input-margin</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.4.2.1">0.56</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.4.3.1">0.64</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of human evaluation for question quality. <span class="ltx_text ltx_font_italic" id="S5.T3.4.1">Full</span> indicates that the model generated answer was a valid answer according to the format critics and identified by human annotators and <span class="ltx_text ltx_font_italic" id="S5.T3.5.2">Partial</span> indicates that the sample passed format critics and a valid answer was identified for the question but the model generated answer did not match.</figcaption>
</figure>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">The results demonstrate that reinforcement learning can effectively manipulate question difficulty, while highlighting important avenues for future work. While SQuAD’s synchronic nature served our experimental needs, cultural heritage datasets typically present diachronic challenges that add complexity to question generation. Although specialised diachronic models exist <cite class="ltx_cite ltx_citemacro_citet">Drinkall et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib11" title="">2024</a>)</cite>, they lack the extensive training of general-domain LLMs. However, these larger models’ exposure to historical corpora, combined with their advanced instruction-following capabilities, suggests potential for manipulating temporal complexity as an additional dimension of question difficulty.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Error Analysis</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Failure Modes.</span>  At a high level, we can observe the reasons for sample rejection for each model. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S5.F5" title="Figure 5 ‣ 5.1 Error Analysis ‣ 5 Results and Discussion ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">5</span></a>, the zero-shot model is generally unable to generate samples that have a single answer span in the text, despite exactly specifying this in the prompt. The high number of incorrectly formatted samples was a result of only a question being generated or neither a question nor answer being generated. For all the trained model variants, the dominant failure mode was unanswerable questions. As shown in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A3" title="Appendix C API-Based LLM Answerability Annotation ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">C</span></a>, each of the fine-tuned models show a similar proportion of otherwise valid samples being unanswerable. The answerability rate could potentially be improved by generating candidate answers, as in <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib45" title="">2022</a>)</cite>, and passing an input passage and answer to the question generation model.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Four pie charts, ordered left to right as PPO-input-margin, SFT, ZeroShot, and PPO-in" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S5.F5.g1" src="extracted/5916449/images/invalid_metrics.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Error distribution of questions for SFT, ZeroShot, and the two best performing PPO variants. </figcaption>
</figure>
<figure class="ltx_figure" id="S5.F6"><img alt="A plot showing the distribution of model generated answers about which it also generated the question. All models show a very strong bias to the first 20% with the exception of ZeroShot whos peak is at around 22-25%" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S5.F6.g1" src="extracted/5916449/images/answer_position.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Position of answer span, merged to be a single word, as a proportion of the way through the input passage when split into words. SQuAD positions are selected from our test split and answers are chosen to be the most common from the list of suitable answers. Neither invalid nor exact duplicate questions are considered.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Positional Bias.</span>  One interesting phenomenon is the positional bias in where the model chooses to generate answers. To calculate positional bias, we treat the full answer span as a single "word" and calculate the proportion through the input paragraph in which the answer word appears. As seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S5.F6" title="Figure 6 ‣ 5.1 Error Analysis ‣ 5 Results and Discussion ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">6</span></a>, the zero-shot positional bias is less severe than in the other datasets. The positional bias of SQuAD is clearly seen as, after training on the dataset, all models exhibit this same preference for the beginning of input passages. The clear bias observed in the zero-shot model, despite not being fine-tuned, is documented in other tasks such as LLM ranking <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib43" title="">2023</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib20" title="">2023</a>)</cite> and in summarisation where introductory content is favoured <cite class="ltx_cite ltx_citemacro_citep">(Ravaut et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib35" title="">2023</a>)</cite>. A potential remedy is to supply the model with a sliding window of sentences across the context paragraph to force the model to generate questions throughout the text. While this would improve the diversity of a final dataset, it may have the adverse effect of limiting the range of dependencies, restricting potentially challenging questions across the whole text.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Hallucinated External Knowledge.</span>  Where ambiguous references to specific entities exist in the input passage such as <em class="ltx_emph ltx_font_italic" id="S5.SS1.p3.1.2">the museum collection</em>, the models frequently attempt to fill in which entity is being referred to. From a context containing ambiguous references to an unnamed museum, the questions <em class="ltx_emph ltx_font_italic" id="S5.SS1.p3.1.3">What year did the Tate acquire the statue of St John the Baptist?</em>, <em class="ltx_emph ltx_font_italic" id="S5.SS1.p3.1.4">How many works does Rodin have in the British Museum’s collection?</em> were generated across both the SFT and PPO models; the examples consistently passed LLM evaluations of answerability. This suggests the solution to this problem is more holistic and requires improvements at a foundational model level to resolve. We could resolve this at a critic level through more careful prompting, however, this returns to our original and intractable task of textually describing a complex task. A more holistic solution could be to adapt PPO with functional grounding <cite class="ltx_cite ltx_citemacro_citep">(Carta et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib6" title="">2023</a>)</cite> to be a pure text task. However, this may lower the quality of questions as it could discourage the use of implicit or complementary knowledge.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Unidirectional Relationships.</span>  A strategy to increase the difficulty of questions is to invert relationships found in the text. The models sometimes misappropriate this tool, resulting in invalid questions such as the question <em class="ltx_emph ltx_font_italic" id="S5.SS1.p4.1.2">What did the Ming dynasty represent?</em> from a passage containing <em class="ltx_emph ltx_font_italic" id="S5.SS1.p4.1.3">…explorer Zheng He representing the Ming Dynasty…</em>. Knowledge graph assisted generation could help to resolve these logical inconsistencies <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib21" title="">2015a</a>)</cite>. However, expecting our target demographics, emerging domains, to possess high-quality knowledge graphs is an unreasonable assumption.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduce a low-cost methodology for generating challenging MRC datasets to meet the growing need for evaluation datasets in the cultural heritage sector. By using high-performing question-answering models to identify the most difficult questions, we were able to create synthetic pairwise data for training a reward model. Rather than manually defining question difficulty, our approach allows the model to learn and extract these features autonomously, leading to a significant improvement in the difficulty of questions generated for evaluation.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">With this said, we trained on a general domain dataset in order to single out the training behaviour, in doing so losing many of the characteristic features of heritage datasets. In future work we will examine how the training paradigm fares under the unique challenges presented by such a varied industry.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Although this work was produced to meet the evaluation demands of our ongoing work in RAG at our institution, we also highlight that the approach can work in any domain and that with some modification, it could be used to augment other dataset formats.
We believe this approach can be extended further, allowing for the manipulation of multiple abstract properties simultaneously through multi-reward model setups <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#bib.bib44" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This project only shows the suitability of the method on a single model. In future work, we seek to address this by performing a more comprehensive review of the approach across a range of model sizes and architectures. We also acknowledge that this method currently only addresses answerable questions while most contemporary QA datasets utilise both answerable and unanswerable questions. Finally, despite using LoRA and multi-adapter training, we still required approximately 15 GPU hours on an A100 80GB which restricts the potential audience for this approach. Evaluating smaller models or quantisation will enable greater access to this project’s benefits.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This work was supported by the Arts and Humanities Research Council [grant number AH/X004201/1].</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">This project has been approved by the relevant institution’s ethics committee. We use LLaMa2 in accordance with Meta’s license<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/llama/license/" title="">https://ai.meta.com/llama/license/</a></span></span></span>. All annotators were located through word of mouth and paid £12 per hour - above the UK National Living Wage of £11.44.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AlKhuzaey et al. (2023)</span>
<span class="ltx_bibblock">
Samah AlKhuzaey, Floriana Grasso, Terry R. Payne, and Valentina Tamma. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s40593-023-00362-1" title="">Text-based Question Difficulty Prediction: A Systematic Review of Automatic Approaches</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">International Journal of Artificial Intelligence in Education</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
Anthropic. 2024.

</span>
<span class="ltx_bibblock">The Claude 3 Model Family: Opus, Sonnet, Haiku.

</span>
<span class="ltx_bibblock">Technical report, Anthropic.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asprino et al. (2022)</span>
<span class="ltx_bibblock">
Luigi Asprino, Luana Bulla, Ludovica Marinucci, Misael Mongiovì, and Valentina Presutti. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-95470-3_14" title="">A Large Visual Question Answering Dataset for Cultural Heritage</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Machine Learning, Optimization, and Data Science</em>, pages 193–197, Cham. Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2022)</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2204.05862" title="">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beinborn et al. (2015)</span>
<span class="ltx_bibblock">
Lisa Beinborn, Torsten Zesch, and Iryna Gurevych. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/W15-0601" title="">Candidate evaluation strategies for improved difficulty prediction of language tests</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 1–11, Denver, Colorado. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carta et al. (2023)</span>
<span class="ltx_bibblock">
Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.02662" title="">Grounding large language models in interactive environments with online reinforcement learning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Yu Chen, Lingfei Wu, and Mohammed J. Zaki. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1908.04942" title="">Reinforcement learning based graph-to-sequence model for natural question generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/1908.04942.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2021)</span>
<span class="ltx_bibblock">
Yi Cheng, Siyao Li, Bang Liu, Ruihui Zhao, Sujian Li, Chenghua Lin, and Yefeng Zheng. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.465" title="">Guiding the growth: Difficulty-controllable question generation through step-by-step rewriting</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 5968–5978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao (2023)</span>
<span class="ltx_bibblock">
Tri Dao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2307.08691" title="">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a>.

</span>
<span class="ltx_bibblock">ArXiv:2307.08691 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.14314v1" title="">QLoRA: Efficient Finetuning of Quantized LLMs</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drinkall et al. (2024)</span>
<span class="ltx_bibblock">
Felix Drinkall, Eghbal Rahimikia, Janet B. Pierrehumbert, and Stefan Zohren. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2404.18543v1" title="">Time Machine GPT</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2023)</span>
<span class="ltx_bibblock">
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.04166" title="">GPTScore: Evaluate as You Desire</a>.

</span>
<span class="ltx_bibblock">ArXiv:2302.04166 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et al. (2020)</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1904.09751" title="">The Curious Case of Neural Text Degeneration</a>.

</span>
<span class="ltx_bibblock">ArXiv:1904.09751 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2018)</span>
<span class="ltx_bibblock">
Fu-Yuan Hsu, Hahn-Ming Lee, Tao-Hsing Chang, and Yao-Ting Sung. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.ipm.2018.06.007" title="">Automated estimation of item difficulty for multiple-choice tests: An application of word embedding techniques</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Information Processing &amp; Management</em>, 54(6):969–984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2022)</span>
<span class="ltx_bibblock">
Tianbo Ji, Chenyang Lyu, Gareth Jones, Liting Zhou, and Yvette Graham. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3390/e24111514" title="">QAScore—An Unsupervised Unreferenced Metric for the Question Generation Evaluation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Entropy</em>, 24(11):1514.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khashabi et al. (2022)</span>
<span class="ltx_bibblock">
Daniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2202.12359" title="">UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training</a>.

</span>
<span class="ltx_bibblock">ArXiv:2202.12359 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koolen and Kamps (2009)</span>
<span class="ltx_bibblock">
Marijn Koolen and Jaap Kamps. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1179/174327909X441153" title="">Information Retrieval in Cultural Heritage</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">INTERDISCIPLINARY SCIENCE REVIEWS</em>, 343:268–284.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein et al. (1966)</span>
<span class="ltx_bibblock">
Vladimir I Levenshtein et al. 1966.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and reversals.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Soviet physics doklady</em>, volume 10, pages 707–710. Soviet Union.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2005.11401" title="">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2310.01432" title="">Split and Merge: Aligning Position Biases in Large Language Model based Evaluators</a>.

</span>
<span class="ltx_bibblock">ArXiv:2310.01432 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2015a)</span>
<span class="ltx_bibblock">
Chenghua Lin, Dong Liu, Wei Pang, and Edward Apeh. 2015a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/2815833.2815842" title="">Automatically Predicting Quiz Difficulty Level Using Similarity Measures</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 8th International Conference on Knowledge Capture</em>, K-CAP 2015, pages 1–8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2015b)</span>
<span class="ltx_bibblock">
Chenghua Lin, Dong Liu, Wei Pang, and Zhe Wang. 2015b.

</span>
<span class="ltx_bibblock">Sherlock: A semi-automatic framework for quiz generation using a hybrid semantic similarity measure.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Cognitive computation</em>, 7:667–679.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W04-1013" title="">ROUGE: A Package for Automatic Evaluation of Summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Text Summarization Branches Out</em>, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Lin (2014)</span>
<span class="ltx_bibblock">
Dong Liu and Chenghua Lin. 2014.

</span>
<span class="ltx_bibblock">Sherlock: a semi-automatic quiz generation system using linked data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ISWC (Posters &amp; Demos)</em>, pages 9–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.16634" title="">G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment</a>.

</span>
<span class="ltx_bibblock">ArXiv:2303.16634 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Ye Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and Philip S. Yu. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1908.05604" title="">Generative question refinement with deep reinforcement learning in retrieval-based QA system</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">CoRR</em>, abs/1908.05604.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loginova et al. (2021)</span>
<span class="ltx_bibblock">
Ekaterina Loginova, Luca Benedetto, Dries Benoit, and Paolo Cremonesi. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.ranlp-1.97" title="">Towards the Application of Calibrated Transformers to the Unsupervised Estimation of Question Difficulty from Text</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</em>, pages 846–855, Held Online. INCOMA Ltd.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohammadshahi et al. (2023)</span>
<span class="ltx_bibblock">
Alireza Mohammadshahi, Thomas Scialom, Majid Yazdani, Pouya Yanki, Angela Fan, James Henderson, and Marzieh Saeidi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.428" title="">RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 6845–6867, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nema and Khapra (2018)</span>
<span class="ltx_bibblock">
Preksha Nema and Mitesh M. Khapra. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1808.10192v2" title="">Towards a Better Metric for Evaluating Question Generation Systems</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.08774" title="">GPT-4 Technical Report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>, 35:27730–27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a Method for Automatic Evaluation of Machine Translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piryani et al. (2024)</span>
<span class="ltx_bibblock">
Bhawna Piryani, Jamshid Mozafari, and Adam Jatowt. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.17859v1" title="">ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. (2016)</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1606.05250" title="">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>.

</span>
<span class="ltx_bibblock">ArXiv:1606.05250 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravaut et al. (2023)</span>
<span class="ltx_bibblock">
Mathieu Ravaut, Shafiq Joty, Aixin Sun, and Nancy F. Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2310.10570" title="">On Context Utilization in Summarization with Large Language Models</a>.

</span>
<span class="ltx_bibblock">ArXiv:2310.10570 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rennie et al. (2017)</span>
<span class="ltx_bibblock">
Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017.

</span>
<span class="ltx_bibblock">Self-critical sequence training for image captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 7008–7024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et al. (2017)</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1707.06347" title="">Proximal Policy Optimization Algorithms</a>.

</span>
<span class="ltx_bibblock">ArXiv:1707.06347 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al. (2016)</span>
<span class="ltx_bibblock">
Shurong Sheng, Luc Van Gool, and Marie-Francine Moens. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W16-4003" title="">A Dataset for Multimodal Question Answering in the Cultural Heritage Domain</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH)</em>, pages 10–17, Osaka, Japan. The COLING 2016 Organizing Committee.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team Gemini (2023)</span>
<span class="ltx_bibblock">
Team Gemini. 2023.

</span>
<span class="ltx_bibblock">Gemini: A Family of Highly Capable Multimodal Models.

</span>
<span class="ltx_bibblock">https://arxiv.org/abs/2312.11805v4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2307.09288" title="">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2307.09288" title="">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>.

</span>
<span class="ltx_bibblock">ArXiv:2307.09288 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Werra et al. (2020)</span>
<span class="ltx_bibblock">
Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. 2020.

</span>
<span class="ltx_bibblock">Trl: Transformer reinforcement learning.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/trl" title="">https://github.com/huggingface/trl</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.17926" title="">Large Language Models are not Fair Evaluators</a>.

</span>
<span class="ltx_bibblock">ArXiv:2305.17926 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2306.01693" title="">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training</a>.

</span>
<span class="ltx_bibblock">ArXiv:2306.01693 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Cheng Zhang, Hao Zhang, Yicheng Sun, and Jie Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3558100.3563846" title="">Downstream transformer generation of question-answer pairs with preprocessing and postprocessing pipelines</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 22nd ACM Symposium on Document Engineering</em>, DocEng ’22, pages 1–8, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Zhuosheng Zhang, Junjie Yang, and Hai Zhao. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2001.09694" title="">Retrospective reader for machine reading comprehension</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">CoRR</em>, abs/2001.09694.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2018)</span>
<span class="ltx_bibblock">
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1802.01886" title="">Texygen: A Benchmarking Platform for Text Generation Models</a>.

</span>
<span class="ltx_bibblock">ArXiv:1802.01886 [cs].

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Reward Model Performance</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">To understand the relative contributions of marginal ranking loss and the use of the input when training reward models to discriminate based on difficulty, we trained all four permutations of settings on the whole training split of the comparisons dataset and evaluated on the test split. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A1.T4" title="Table 4 ‣ Appendix A Reward Model Performance ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">4</span></a>, the inclusion of the input text had a very significant impact on performance. This was expected as the difficulty of a question is not independent of the related passage. Surprisingly, marginal ranking loss had a very slight negative impact on reward model performance. We believe this could be due to the fact that features of difficulty are very subtle and the marginal component may have caused too significant adjustments due to higher loss values.</p>
</div>
<figure class="ltx_table" id="A1.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A1.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">Accuracy (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.1.1">RM</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.2.1.2">63.66</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.3.2.1.1">RM-input</span></th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.3.2.2.1">70.69</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.4.3.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.4.3.1.1">RM-margin</span></th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.2">62.39</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.5.4.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.5.4.1.1">RM-input-margin</span></th>
<td class="ltx_td ltx_align_center" id="A1.T4.1.5.4.2">70.38</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Accuracy of reward model variants based on the test split of the comparisons dataset. <span class="ltx_text ltx_font_italic" id="A1.T4.4.1">input</span> indicates that the model was trained with the question and associated text passage as input and <span class="ltx_text ltx_font_italic" id="A1.T4.5.2">margin</span> indicates that marginal ranking loss was used.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7">
<p class="ltx_p" id="A1.F7.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.F7.1.1" style="width:433.6pt;">
<span class="ltx_p" id="A1.F7.1.1.1">Following is a text, a question and an answer. You must determine whether the provided answer is a correct span-extraction response to the question. If there are multiple plausible answers in the text, the answer should be the most relevant or accurate one. If there are multiple equally plausible answers in the text, respond "NO". If the provided answer is incomplete or contains excess information, respond "NO". If the answer does not correctly answer the question, respond "NO". Only if the answer is correct and does not breach the aforementioned requirements, respond with "YES".</span>
<span class="ltx_p" id="A1.F7.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.F7.1.1.2.1">Text</span>: … Upon its arrival in Canberra, the Olympic flame was presented by Chinese officials to local Aboriginal elder Agnes Shea, of the Ngunnawal people. She, in turn, offered them a message stick …</span>
<span class="ltx_p" id="A1.F7.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.F7.1.1.3.1">Question</span>: Who received the flame from Chinese officials in Canberra?</span>
<span class="ltx_p" id="A1.F7.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.F7.1.1.4.1">Answer</span>: Agnes Shea</span>
<span class="ltx_p" id="A1.F7.1.1.5">Respond with only "YES" or "NO" in response to this task. Do NOT provide any other text or reasoning.</span>
</span>
</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Example prompt and response to GPT-4o (gpt-4o as of 1st June 2024) and Gemini-1.5-pro (gemini-1.5-pro as of 1st June 2024).</figcaption>
</figure>
<figure class="ltx_table" id="A1.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A1.T5.5.5.6"><span class="ltx_text ltx_font_bold" id="A1.T5.5.5.6.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A1.T5.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1">Answerable (<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.1.m1.1a"><mo id="A1.T5.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.m1.1b"><ci id="A1.T5.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A1.T5.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T5.2.2.2.1">Unanswerable (<math alttext="\downarrow" class="ltx_Math" display="inline" id="A1.T5.2.2.2.1.m1.1"><semantics id="A1.T5.2.2.2.1.m1.1a"><mo id="A1.T5.2.2.2.1.m1.1.1" stretchy="false" xref="A1.T5.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.1.m1.1b"><ci id="A1.T5.2.2.2.1.m1.1.1.cmml" xref="A1.T5.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.1.m1.1d">↓</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A1.T5.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T5.3.3.3.1">Undetermined (<math alttext="\downarrow" class="ltx_Math" display="inline" id="A1.T5.3.3.3.1.m1.1"><semantics id="A1.T5.3.3.3.1.m1.1a"><mo id="A1.T5.3.3.3.1.m1.1.1" stretchy="false" xref="A1.T5.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.1.m1.1b"><ci id="A1.T5.3.3.3.1.m1.1.1.cmml" xref="A1.T5.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.3.1.m1.1d">↓</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.5.5.5"><span class="ltx_text ltx_font_bold" id="A1.T5.5.5.5.2">Cohen’s <math alttext="\kappa" class="ltx_Math" display="inline" id="A1.T5.4.4.4.1.m1.1"><semantics id="A1.T5.4.4.4.1.m1.1a"><mi id="A1.T5.4.4.4.1.m1.1.1" xref="A1.T5.4.4.4.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="A1.T5.4.4.4.1.m1.1b"><ci id="A1.T5.4.4.4.1.m1.1.1.cmml" xref="A1.T5.4.4.4.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.4.4.1.m1.1c">\kappa</annotation><annotation encoding="application/x-llamapun" id="A1.T5.4.4.4.1.m1.1d">italic_κ</annotation></semantics></math> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.5.5.5.2.m2.1"><semantics id="A1.T5.5.5.5.2.m2.1a"><mo id="A1.T5.5.5.5.2.m2.1.1" stretchy="false" xref="A1.T5.5.5.5.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T5.5.5.5.2.m2.1b"><ci id="A1.T5.5.5.5.2.m2.1.1.cmml" xref="A1.T5.5.5.5.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.5.5.5.2.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.5.5.5.2.m2.1d">↑</annotation></semantics></math>)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T5.5.6.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.5.6.1.1.1">ZeroShot</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.5.6.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.5.6.1.2.1">0.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.5.6.1.3"><span class="ltx_text ltx_font_bold" id="A1.T5.5.6.1.3.1">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.5.6.1.4"><span class="ltx_text ltx_font_bold" id="A1.T5.5.6.1.4.1">0.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.5.6.1.5">0.61</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.7.2.1"><span class="ltx_text ltx_font_bold" id="A1.T5.5.7.2.1.1">SFT</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.7.2.2">0.64</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.7.2.3">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.7.2.4">0.16</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.7.2.5"><span class="ltx_text ltx_font_bold" id="A1.T5.5.7.2.5.1">0.62</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.8.3.1"><span class="ltx_text ltx_font_bold" id="A1.T5.5.8.3.1.1">PPO</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.8.3.2">0.64</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.8.3.3">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.8.3.4">0.16</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.8.3.5"><span class="ltx_text ltx_font_bold" id="A1.T5.5.8.3.5.1">0.62</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.9.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.9.4.1"><span class="ltx_text ltx_font_bold" id="A1.T5.5.9.4.1.1">PPO-input</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.9.4.2">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.9.4.3">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.9.4.4">0.18</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.9.4.5">0.58</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.10.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.10.5.1"><span class="ltx_text ltx_font_bold" id="A1.T5.5.10.5.1.1">PPO-margin</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.10.5.2">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.10.5.3">0.19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.10.5.4">0.19</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.10.5.5">0.56</td>
</tr>
<tr class="ltx_tr" id="A1.T5.5.11.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T5.5.11.6.1"><span class="ltx_text ltx_font_bold" id="A1.T5.5.11.6.1.1">PPO-input-margin</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.11.6.2">0.63</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.11.6.3">0.21</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.5.11.6.4">0.16</td>
<td class="ltx_td ltx_align_center" id="A1.T5.5.11.6.5"><span class="ltx_text ltx_font_bold" id="A1.T5.5.11.6.5.1">0.62</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Results of answerability task posed to GPT-4o and Gemini-1.5-pro. Results represent the proportion of samples that are answerable, unanswerable and undecided, taken from those samples which passed the format critic.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Obtaining Zero-Shot Model Generations</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">To obtain zero-shot generations, we adopt a slightly different approach. To avoid overconstaining the output of the model, we adopted a two-stage process. LLaMa-2-7b-chat was first tasked with generating a question-answer pair based on the text, unconstrained. We then passed this output back into the model with the task of extracting the question and answer components and placed them into a JSON file with the keys <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">question</span> and <span class="ltx_text ltx_font_italic" id="A2.p1.1.2">answer</span>. We used the same, high temperature of 0.9 for generating the samples and a much lower temperature of 0.2 for extracting into a JSON to reduce the chance of models altering the generated sequences while structuring them.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>API-Based LLM Answerability Annotation</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">To ensure that we evaluate performance on as high-quality questions as possible, we extract only those questions deemed <span class="ltx_text ltx_font_italic" id="A3.p1.1.1">answerable</span>, by our definition, by both GPT-4o and Gemini-1.5-pro. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#A1.T5" title="Table 5 ‣ Appendix A Reward Model Performance ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">5</span></a> shows that the zero-shot samples had the highest rate of predicted answerability; each other variant shows very consistent rates of answerability. This outcome should be tempered by the results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08289v1#S5.F5" title="Figure 5 ‣ 5.1 Error Analysis ‣ 5 Results and Discussion ‣ Increasing the Difficulty of Automatically Generated Questions via Reinforcement Learning with Synthetic Preference"><span class="ltx_text ltx_ref_tag">5</span></a> which indicates that the zero-shot model had an extremely high failure rate in many other regards.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 10 11:55:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
