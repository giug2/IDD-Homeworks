<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning</title>
<!--Generated on Tue Sep 24 08:52:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.15879v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S1" title="In Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S2" title="In Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S2.SS1" title="In 2 Data ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S2.SS2" title="In 2 Data ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data Pre-processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3" title="In Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>NMT System</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3.SS1" title="In 3 NMT System ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>System Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3.SS2" title="In 3 NMT System ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Regularized Dropout</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3.SS3" title="In 3 NMT System ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Diversification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3.SS4" title="In 3 NMT System ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Forward Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3.SS5" title="In 3 NMT System ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Back Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3.SS6" title="In 3 NMT System ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Denoise</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3.SS7" title="In 3 NMT System ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Transductive Ensemble Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S3.SS8" title="In 3 NMT System ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.8 </span>Transfer Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S4" title="In Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S4.SS1" title="In 4 Experiment ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S4.SS2" title="In 4 Experiment ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S5" title="In Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document"> Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Bin Wei, Jiawei Zhen, Zongyao Li, Zhanglin Wu, Daimeng Wei, 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id1.1.id1">Jiaxin Guo, Zhiqiang Rao, Shaojun Li, Yuanchang Luo, Hengchao Shang,</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id2.2.id2">
Jinlong Yang, Yuhao Xie, Hao Yang

<br class="ltx_break"/></span>Huawei Translation Service Center, Beijing, China
<br class="ltx_break"/>{weibin29, zhengjiawei15, lizongyao, wuzhanglin2, weidaimeng, 
<br class="ltx_break"/>guojiaxin1,raozhiqiang, lishaojun18, luoyuanchang1, shanghengchao,
<br class="ltx_break"/>yangjinlong7, xieyuhao2, yanghao30}@huawei.com

<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">This paper introduces the submission by Huawei Translation Center (HW-TSC) to the WMT24 Indian Languages Machine Translation (MT) Shared Task. To develop a reliable machine translation system for low-resource Indian languages, we employed two distinct knowledge transfer strategies, taking into account the characteristics of the language scripts and the support available from existing open-source models for Indian languages. For Assamese(as) and Manipuri(mn), we fine-tuned the existing IndicTrans2<cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib7" title="">2023</a>)</cite> open-source model to enable bidirectional translation between English and these languages. For Khasi (kh) and Mizo (mz), We trained a multilingual model as a baseline using bilingual data from these four language pairs, along with an additional about 8kw English-Bengali bilingual data, all of which share certain linguistic features. This was followed by fine-tuning to achieve bidirectional translation between English and Khasi, as well as English and Mizo. Our transfer learning experiments produced impressive results: 23.5 BLEU for en→as, 31.8 BLEU for en→mn, 36.2 BLEU for as→en, and 47.9 BLEU for mn→en on their respective test sets. Similarly, the multilingual model transfer learning experiments yielded impressive outcomes, achieving 19.7 BLEU for en→kh, 32.8 BLEU for en→mz, 16.1 BLEU for kh→en, and 33.9 BLEU for mz→en on their respective test sets. These results not only highlight the effectiveness of transfer learning techniques for low-resource languages but also contribute to advancing machine translation capabilities for low-resource Indian languages.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the realm of machine translation, Neural Machine Translation (NMT) has become the dominant technology, as confirmed by previous research. However, training NMT models requires large amounts of data, which presents a significant challenge when dealing with low-resource languages. To tackle this challenge, we employed transfer learning, a well-established approach that enhances model performance by transferring knowledge gained from one task to other related tasks. To improve translation capabilities for low-resource languages, we faced the challenge of limited bilingual resources for Indian languages. To overcome this issue, we trained a multilingual model using not only all the bilingual data provided for the task but also additional Bengali data. Additionally, we examined the languages supported by the existing IndicTrans2<cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib7" title="">2023</a>)</cite> open-source model and conducted a comparative analysis. Based on our findings, we selected different baseline models for knowledge transfer depending on the language pair: for Assamese and Manipuri, we used the IndicTrans2 model as the baseline, while for Khasi and Mizo, we trained multilingual model by ourselves as the baseline. This approach enabled us to effectively leverage existing resources while addressing the specific challenges associated with each language pair.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">IndicTrans2 is the first open-source transformer-based multilingual NMT model that supports high-quality translations across all the 22 scheduled Indic languages. It was trained on the extensive Bharat Parallel Corpus Collection (BPCC), a publicly accessible repository encompassing both pre-existing and freshly curated data for all 22 scheduled Indian languages, this model boasts a comprehensive understanding of the linguistic diversity within the Indian subcontinent. To enhance its linguistic prowess, IndicTrans2 has undergone auxiliary training utilizing the rich resource of back-translated monolingual data. The model was then trained on human-annotated data to achieve further improvements. We used this model in the first two subtasks and fine-tuned it on the training data provided by WMT24. By adopting this approach, we aim to capitalize on the acquired knowledge during training to significantly bolster the performance of the model in the specific translation task at hand. The fine-tuned IndicTrans2 achieves good scores, so we are using it for our final submission in the first two subtasks.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">For the multilingual model, we first utilized resources from Bengali. The choice of Bengali was based on its belonging to the Indo-Aryan branch, its linguistic feature similarities with some of the target low-resource languages, and its relatively rich available data. By introducing Bengali data, we aimed to enable the model to learn features potentially shared with the target languages, thereby laying a foundation for processing other related languages. Next, we integrated all available bilingual data from Indic language MT track. This included parallel corpora between various Indian languages and English. Although the data for each language pair might be limited individually, the combined dataset offered diverse learning samples. We believe that this integration of multilingual data helps the model capture both the commonalities and differences among different Indian languages. Based on this carefully selected and integrated data, we trained a multilingual model. The design goal of this model was to handle translation tasks for multiple Indian languages simultaneously, using the commonalities between languages to compensate for the scarcity of data in any single language. Through this approach, we expect the model to learn more generalized language representations and translation knowledge under resource constraints, leading to improved performance on Khasi and Mizo translation tasks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Ultimately, we adopted a differentiated strategy for knowledge transfer. This approach thoroughly considered the characteristics of each language to achieve optimal transfer effects. In Section 2, we will discuss the details of the data, the methods and processes used for data pre-processing. Section 3 will cover the overall architecture and training strategies of the NMT system, including a detailed account of the various optimization methods. In Section 4, we will present the experimental parameters, results, and their analysis. The final section will summarize the key findings of the paper.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Data</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Details</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We have fine-tuned the model using the WMT24 corpus. Additionally, we used 2M monolingual english dataset to do BT and FT. The amount of data we used is shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S2.T1" title="Table 1 ‣ 2.1 Data Details ‣ 2 Data ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_block ltx_pruned_first" id="S2.T1.2">
<div class="ltx_para ltx_noindent ltx_align_center" id="S2.T1.2.p2">
<p class="ltx_p" id="S2.T1.2.p2.1"><span class="ltx_text ltx_inline-block" id="S2.T1.2.p2.1.1" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S2.T1.2.p2.1.1.1" style="width:228.1pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S2.T1.2.p2.1.1.1.1"><span class="ltx_text" id="S2.T1.2.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.2.p2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.1.1.1">language pairs</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.1.1.2">bitext data</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.1.1.3">monolingual data</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.2.1.1">en-as</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.2.1.2">50K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.2.1.3">en: 2M, as: 2.62M</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.3.2.1">en-mn</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.3.2.2">21K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.3.2.3">en: 2M, mn: 2.14M</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.4.3.1">en-kh</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.4.3.2">24K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.4.3.3">en: 2M, kh: 182K</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.5.4.1">en-mz</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.5.4.2">50K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.5.4.3">en: 2M, mz: 1.9M</span></span>
</span>
</span></span></span>
</span></span></span></p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">Table 1: </span>Bilingual and monolingual used for training NMT models.</figcaption>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Pre-processing</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Our data pre-processing methods for NMT include:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Remove duplicate sentences or sentence pairs.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Convert full-width symbols to half-width.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Use fasttext<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fastText" title="">https://github.com/facebookresearch/fastText</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Joulin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib10" title="">2016</a>)</cite> to filter other language sentences.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Use mosesdecoder<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/moses-smt/mosesdecoder" title="">https://github.com/moses-smt/mosesdecoder</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Koehn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib12" title="">2007</a>)</cite> to normalize English punctuation.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1">Filter out sentences with more than 150 words.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i6.p1">
<p class="ltx_p" id="S2.I1.i6.p1.1">Use fast-align <cite class="ltx_cite ltx_citemacro_cite">Dyer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib4" title="">2013</a>)</cite> to filter sentence pairs with poor alignment.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i7.p1">
<p class="ltx_p" id="S2.I1.i7.p1.1">Sentencepiece<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/sentencepiece" title="">https://github.com/google/sentencepiece</a></span></span></span> (SPM) <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib13" title="">2018</a>)</cite> is used to perform subword segmentation, and the vocabulary size is set to 32K.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Since there may be some semantically dissimilar sentence pairs in bilingual data, we use LaBSE<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/sentence-transformers/LaBSE" title="">https://huggingface.co/sentence-transformers/LaBSE</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib6" title="">2022</a>)</cite> to calculate the semantic similarity of each bilingual sentence pair, and exclude bilingual sentence pairs with a similarity score lower than 0.75 from our training corpus.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="236" id="S2.F1.g1" src="extracted/5875482/NMT_FLOW.png" width="354"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall training flow of NMT system.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>NMT System</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>System Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We use Transformer <cite class="ltx_cite ltx_citemacro_cite">Vaswani (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib20" title="">2017</a>)</cite> as our neural machine translation (NMT)<cite class="ltx_cite ltx_citemacro_cite">Bahdanau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib2" title="">2014</a>)</cite> model architecture. For the first two subtasks(en-as, en-mn), we use the IndicTrans2<cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib7" title="">2023</a>)</cite> model as our baseline model, which is a deep Transformer architecture with 18-layers encoder and 18-layers decoder. with the latter two subtasks(en-kh, en-mz), we trained a multilingual model as our baseline model, which is a deep Transformer architecture with 35-layers encoder and 3-layers decoder.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S2.F1" title="Figure 1 ‣ 2.2 Data Pre-processing ‣ 2 Data ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overall training flow of NMT system. Referred to previous work <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib22" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib23" title="">2022</a>); Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib26" title="">2023</a>)</cite>, We use training strategies such as regularized dropout (R-Drop) <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib25" title="">2021</a>)</cite>, data diversification (DD) <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib14" title="">2020</a>)</cite>, forward translation FT) <cite class="ltx_cite ltx_citemacro_cite">Abdulmumin (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib1" title="">2021</a>)</cite>, back translation (BT) <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib17" title="">2016</a>)</cite>, denoise, Transfer learning(TL) and transductive ensemble learning (TEL) <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib21" title="">2020</a>)</cite> for training.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Regularized Dropout</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Regularized Dropout (R-Drop)<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/dropreg/R-Drop" title="">https://github.com/dropreg/R-Drop</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib25" title="">2021</a>)</cite> is a simple yet more effective alternative to regularize the training inconsistency induced by dropout <cite class="ltx_cite ltx_citemacro_cite">Srivastava et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib18" title="">2014</a>)</cite>. Concretely, in each mini-batch training, each data sample goes through the forward pass twice, and each pass is processed by a different sub model by randomly dropping out some hidden units. R-Drop forces the two distributions for the same data sample outputted by the two sub models to be consistent with each other, through minimizing the bidirectional Kullback-Leibler (KL) divergence <cite class="ltx_cite ltx_citemacro_cite">Van Erven and Harremos (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib19" title="">2014</a>)</cite> between the two distributions. That is, R-Drop regularizes the outputs of two sub models randomly sampled from dropout for each data sample in training. In this way, the inconsistency between the training and inference stage can be alleviated.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Diversification</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Data Diversification (DD) <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib14" title="">2020</a>)</cite> is a data augmentation method to boost NMT performance. It diversifies the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset which the final NMT model is trained on. DD is applicable to all NMT models. It does not require extra monolingual data, nor does it add more parameters. To conserve training resources, we only use one forward model and one backward model to diversify the training data.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Forward Translation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Forward translation (FT) <cite class="ltx_cite ltx_citemacro_cite">Abdulmumin (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib1" title="">2021</a>)</cite>, also known as self-training, is one of the most commonly used data augmentation methods. FT has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. Generally, FT is performed in three steps: (1) randomly sample a subset from the large-scale source monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Back Translation</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">An effective method to improve NMT with target monolingual data is to augment the parallel training data with back translation (BT) <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib17" title="">2016</a>); Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib24" title="">2023</a>)</cite>. There are many published works that expand the understanding of BT and investigate methods for generating synthetic source sentences. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib5" title="">Edunov et al.</a></cite> (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib5" title="">2018</a></cite>) find that back translations obtained via sampling or noised beam outputs are more effective than back translations generated by beam or greedy search in most scenarios. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib3" title="">Caswell et al.</a></cite> (<cite class="ltx_cite ltx_citemacro_citeyear"><a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib3" title="">2019</a></cite>) show that the main role of such noised beam outputs is not to diversify the source side, but simply to tell the model that the given source is synthetic. Therefore, they propose a simpler alternative strategy: Tagged BT. This method uses an extra token to mark back translated source sentences, which generally outperforms noised BT <cite class="ltx_cite ltx_citemacro_cite">Edunov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib5" title="">2018</a>)</cite>. For better joint use with FT, we use sampling back translation (ST) <cite class="ltx_cite ltx_citemacro_cite">Edunov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib5" title="">2018</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Denoise</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">In machine translation, denoising improves translation quality by removing noise from the training data, such as inaccurate translations, grammatical errors, or unnatural sentence structures, allowing the model to focus on high-quality data and produce more accurate and fluent translations. Additionally, denoising enhances the model’s robustness by eliminating noisy data, which helps the model better learn the target language’s patterns, reducing errors and leading to more stable and reliable performance across diverse inputs. It also optimizes training efficiency by decreasing the amount of data the model needs to process, particularly by filtering out low-quality data, which results in a cleaner and more consistent dataset and can shorten the overall training time. Moreover, denoising reduces error propagation by preventing the model from learning incorrect language patterns, thereby minimizing the accumulation and spread of errors in generated translations. Finally, it enhances the model’s generalization ability, as denoised data is more representative, enabling the model to better adapt to different types of input sentences and improving its performance in real-world applications. Through denoising, machine translation models can more effectively utilize high-quality data, leading to superior translation outcomes and greater overall model stability.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Transductive Ensemble Learning</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">Ensemble learning <cite class="ltx_cite ltx_citemacro_cite">Garmash and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib8" title="">2016</a>)</cite>, which aggregates multiple diverse models for inference, is a common practice to improve the performance of machine learning models. However, it has been observed that the conventional ensemble methods only bring marginal improvement for NMT when individual models are strong or there are a large number of individual models. Transductive Ensemble Learning (TEL) <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib27" title="">2019</a>)</cite> studies how to effectively aggregate multiple NMT models under the transductive setting where the source sentences of the test set are known. TEL uses all individual models to translate the source test set into the target language space and then finetune a strong model on the translated synthetic data, which significantly boosts strong individual models and benefits a lot from more individual models.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8 </span>Transfer Learning</h3>
<div class="ltx_para" id="S3.SS8.p1">
<p class="ltx_p" id="S3.SS8.p1.1">Transfer learning(TL) is a machine learning technique where a model trained on one task is adapted for a second related task. Instead of starting the training of a new model from scratch, transfer learning leverages the knowledge learned from the first task to improve learning on the second task. For Assamese(as) and Manipuri(mn), We have used IndicTrans2<cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib7" title="">2023</a>)</cite>, a powerful model that performs well for English-to-Indic and Indic-to English translation for 22 scheduled Indian languages. This knowledge can be used to translate other Indian languages to and from English. Our approach entailed the fine-tuning of this model, leveraging the parallel corpus provided by the WMT24 for the Indic MT task. This fine-tuning process equipped the model with the expertise required to proficiently translate Assamese and Manipuri to and from English, ultimately yielding the most outstanding results. Similarly, for Khasi and Mizo, we trained a multilingual model as the baseline. We also applied transfer learning techniques to enhance the baseline model using data specific to these language pairs. The results on both the test and dev sets were highly encouraging.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Settings</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.5">We use Transformer architecture in all the subtasks. For the first two subtasks, we use IndicTrans2 <cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib7" title="">2023</a>)</cite> as our baseline model, which is a deep Transformer architecture with 18-layers encoder and 18-layers decoder. With the latter subtasks, the model is also a Transformer architecture with 35-layers encoder and 3-layers decoder. For the first two subtasks, our models apply Adam <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib11" title="">2014</a>)</cite> as optimizer to update the parameters with <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">β</mi><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝛽</ci><cn id="S4.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\beta_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> = 0.9 and <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">β</mi><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">𝛽</ci><cn id="S4.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\beta_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> = 0.98. We employ a warm-up learning rate of <math alttext="10^{-7}" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><msup id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mn id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml"><mo id="S4.SS1.p1.3.m3.1.1.3a" xref="S4.SS1.p1.3.m3.1.1.3.cmml">−</mo><mn id="S4.SS1.p1.3.m3.1.1.3.2" xref="S4.SS1.p1.3.m3.1.1.3.2.cmml">7</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">superscript</csymbol><cn id="S4.SS1.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS1.p1.3.m3.1.1.2">10</cn><apply id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3"><minus id="S4.SS1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS1.p1.3.m3.1.1.3"></minus><cn id="S4.SS1.p1.3.m3.1.1.3.2.cmml" type="integer" xref="S4.SS1.p1.3.m3.1.1.3.2">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">10^{-7}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT</annotation></semantics></math> for 2000 update steps and a learning rate of <math alttext="3\ast 10^{-5}" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mn id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">3</mn><mo id="S4.SS1.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.4.m4.1.1.1.cmml">∗</mo><msup id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml"><mn id="S4.SS1.p1.4.m4.1.1.3.2" xref="S4.SS1.p1.4.m4.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p1.4.m4.1.1.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml"><mo id="S4.SS1.p1.4.m4.1.1.3.3a" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p1.4.m4.1.1.3.3.2" xref="S4.SS1.p1.4.m4.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><ci id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1">∗</ci><cn id="S4.SS1.p1.4.m4.1.1.2.cmml" type="integer" xref="S4.SS1.p1.4.m4.1.1.2">3</cn><apply id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3">superscript</csymbol><cn id="S4.SS1.p1.4.m4.1.1.3.2.cmml" type="integer" xref="S4.SS1.p1.4.m4.1.1.3.2">10</cn><apply id="S4.SS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3"><minus id="S4.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3"></minus><cn id="S4.SS1.p1.4.m4.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.p1.4.m4.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">3\ast 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">3 ∗ 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>. For normalization, we use a dropout value of 0.2 and normalize the probabilities using smoothed label cross-entropy. We use GeLU activations <cite class="ltx_cite ltx_citemacro_cite">Hendrycks and Gimpel (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib9" title="">2016</a>)</cite> for better learning. For the latter subtasks, parameter update frequency is 2, and learning rate is 5e-4. The number of warmup steps is 4000, and model is saved every 1000 steps. R-Drop <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib25" title="">2021</a>)</cite> is used in model training for all subtasks, and we set <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m5.1"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.1d">italic_λ</annotation></semantics></math> to 5.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We use the scareBLEU library to calculate our BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib15" title="">2002</a>)</cite> and ChrF <cite class="ltx_cite ltx_citemacro_cite">Popović (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib16" title="">2015</a>)</cite> scores with a word order of 2.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Regarding this four language pair directions, we use Regularized Dropout, Bidirectional Training, Data Diversification, Forward Translation, Back Translation, Alternated Training, Curriculum Learning, and Transductive Ensemble Learning. The evaluation results of four language pair directions NMT system on WMT24 Indic MT test and dev set are shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S4.T2" title="Table 2 ‣ 4.2 Results ‣ 4 Experiment ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_tag">2</span></a> and Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S4.T3" title="Table 3 ‣ 4.2 Results ‣ 4 Experiment ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S4.T2" title="Table 2 ‣ 4.2 Results ‣ 4 Experiment ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_tag">2</span></a>, IndicTrans2<cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#bib.bib7" title="">2023</a>)</cite> provides a strong baseline. Fine-tuning the model with FT, BT, and bitext data leads to significant improvements, particularly in the en-mn direction, where the BLEU score increases by nearly 20 points over the baseline on the test and dev set. This improvement is largely attributed to Data Diversification. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15879v1#S4.T3" title="Table 3 ‣ 4.2 Results ‣ 4 Experiment ‣ Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning"><span class="ltx_text ltx_ref_tag">3</span></a> further illustrates that FT and BT data contribute the most to model performance, especially in the en-mz direction, which sees an increase of nearly six BLEU points compared to the multilingual baseline. Even after enhancing the model with BT and FT data, adding filtered high-quality bilingual data results in an average gain of about one BLEU point, highlighting the critical role of data quality. Finally, we all use TEL technique to obtain a good result, the improvement is very small, almost less than one bleu score.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_block ltx_pruned_first" id="S4.T2.2">
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T2.2.p2">
<p class="ltx_p" id="S4.T2.2.p2.4"><span class="ltx_text ltx_inline-block" id="S4.T2.2.p2.4.4" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.2.p2.4.4.4.4" style="width:417.5pt;height:306pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T2.2.p2.4.4.4.4.4"><span class="ltx_text" id="S4.T2.2.p2.4.4.4.4.4.4">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.p2.4.4.4.4.4.4.4">
<span class="ltx_thead">
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.5.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.2.p2.4.4.4.4.4.4.4.5.1.1">Language-pair</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.2.p2.4.4.4.4.4.4.4.5.1.2">Training strategies</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.p2.4.4.4.4.4.4.4.5.1.3">Bleu(test)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.p2.4.4.4.4.4.4.4.5.1.4">ChrF2(test)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.p2.4.4.4.4.4.4.4.5.1.5">Bleu(dev)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.p2.4.4.4.4.4.4.4.5.1.6">ChrF2(dev)</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S4.T2.2.p2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1d">→</annotation></semantics></math>as</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.2">IndicTrans2 baseline</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.3">18.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.4">51.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.5">14.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.1.1.1.1.1.1.1.1.6">44.8</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.6.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.6.1.1">+ DD, FT, BT</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.6.1.2">22.9</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.6.1.3">52.5</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.6.1.4">21.1</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.6.1.5">47.7</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.7.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.7.2.1">+ denoise</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.7.2.2">23.3</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.7.2.3">53.1</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.7.2.4">22.5</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.7.2.5">48.9</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.8.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.8.3.1">+ TEL</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.8.3.2">23.5</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.8.3.3">53.2</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.8.3.4">22.8</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.8.3.5">49.0</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1" stretchy="false" xref="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1d">→</annotation></semantics></math>mn</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.2">IndicTrans2 baseline</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.3">11.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.4">48.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.5">11.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.2.2.2.2.2.2.2.2.6">48.5</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.9.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.9.4.1">+ DD, FT, BT</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.9.4.2">30.9</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.9.4.3">62.8</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.9.4.4">31.1</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.9.4.5">63.4</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.10.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.10.5.1">+ denoise</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.10.5.2">31.7</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.10.5.3">64.7</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.10.5.4">31.7</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.10.5.5">64.9</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.11.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.11.6.1">+ TEL</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.11.6.2">31.8</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.11.6.3">64.6</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.11.6.4">31.6</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.11.6.5">64.9</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.3.3.3.3.3.3.3.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1"><span class="ltx_text" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1">as<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1"><semantics id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1a"><mo id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1" stretchy="false" xref="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1b"><ci id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml" xref="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1d">→</annotation></semantics></math>en</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.2">IndicTrans2 baseline</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.3">29.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.4">56.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.5">25.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.3.3.3.3.3.3.3.3.6">49.3</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.12.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.12.7.1">+ DD, FT, BT</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.12.7.2">35.8</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.12.7.3">58.6</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.12.7.4">35.0</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.12.7.5">54.5</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.13.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.13.8.1">+ denoise</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.13.8.2">36.1</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.13.8.3">58.6</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.13.8.4">34.8</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.13.8.5">54.6</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.14.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.14.9.1">+ TEL</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.14.9.2">36.2</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.14.9.3">59.4</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.14.9.4">33.7</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.14.9.5">54.2</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1"><span class="ltx_text" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1">mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1"><semantics id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1a"><mo id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1" stretchy="false" xref="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1b"><ci id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml" xref="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1d">→</annotation></semantics></math>en</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.2">IndicTrans2 baseline</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.3">32.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.4">62.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.5">33.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.p2.4.4.4.4.4.4.4.4.6">61.8</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.15.10">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.15.10.1">+ DD, FT, BT</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.15.10.2">47.5</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.15.10.3">70.8</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.15.10.4">47.0</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.15.10.5">69.7</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.16.11">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.p2.4.4.4.4.4.4.4.16.11.1">+ denoise</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.16.11.2">47.7</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.16.11.3">70.8</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.16.11.4">47.2</span>
<span class="ltx_td ltx_align_center" id="S4.T2.2.p2.4.4.4.4.4.4.4.16.11.5">69.7</span></span>
<span class="ltx_tr" id="S4.T2.2.p2.4.4.4.4.4.4.4.17.12">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.2.p2.4.4.4.4.4.4.4.17.12.1">+ TEL</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.p2.4.4.4.4.4.4.4.17.12.2">47.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.p2.4.4.4.4.4.4.4.17.12.3">70.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.p2.4.4.4.4.4.4.4.17.12.4">47.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.p2.4.4.4.4.4.4.4.17.12.5">69.8</span></span>
</span>
</span></span></span>
</span></span></span></p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">Table 2: </span>The results of en-as and en-mn language pairs on the test and dev set.</figcaption>
</div>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_block ltx_pruned_first" id="S4.T3.2">
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T3.2.p2">
<p class="ltx_p" id="S4.T3.2.p2.4"><span class="ltx_text ltx_inline-block" id="S4.T3.2.p2.4.4" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.2.p2.4.4.4.4" style="width:418.2pt;height:306pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T3.2.p2.4.4.4.4.4"><span class="ltx_text" id="S4.T3.2.p2.4.4.4.4.4.4">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.2.p2.4.4.4.4.4.4.4">
<span class="ltx_thead">
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.5.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.2.p2.4.4.4.4.4.4.4.5.1.1">Language-pair</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.2.p2.4.4.4.4.4.4.4.5.1.2">Training strategies</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.p2.4.4.4.4.4.4.4.5.1.3">Bleu(test)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.p2.4.4.4.4.4.4.4.5.1.4">ChrF2(test)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.p2.4.4.4.4.4.4.4.5.1.5">Bleu(dev)</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.p2.4.4.4.4.4.4.4.5.1.6">ChrF2(dev)</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S4.T3.2.p2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1d">→</annotation></semantics></math>kh</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.2">multilingual baseline</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.3">17.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.4">40.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.5">17.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.1.1.1.1.1.1.1.1.6">39.7</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.6.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.6.1.1">+ DD, FT, BT</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.6.1.2">18.1</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.6.1.3">41.8</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.6.1.4">17.9</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.6.1.5">41.3</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.7.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.7.2.1">+ denoise</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.7.2.2">19.5</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.7.2.3">43.3</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.7.2.4">19.2</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.7.2.5">42.7</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.8.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.8.3.1">+ TEL</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.8.3.2">19.7</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.8.3.3">43.5</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.8.3.4">19.3</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.8.3.5">42.8</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1" stretchy="false" xref="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1d">→</annotation></semantics></math>mz</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.2">multilingual baseline</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.3">25.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.4">51.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.5">22.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.2.2.2.2.2.2.2.2.6">46.6</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.9.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.9.4.1">+ DD, FT, BT</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.9.4.2">30.8</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.9.4.3">55.7</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.9.4.4">25.2</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.9.4.5">49.1</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.10.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.10.5.1">+ denoise</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.10.5.2">32.5</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.10.5.3">57.1</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.10.5.4">25.4</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.10.5.5">49.3</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.11.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.11.6.1">+ TEL</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.11.6.2">32.8</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.11.6.3">57.3</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.11.6.4">25.7</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.11.6.5">49.4</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.3.3.3.3.3.3.3.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1"><span class="ltx_text" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1">kh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1"><semantics id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1a"><mo id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1" stretchy="false" xref="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1b"><ci id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml" xref="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1d">→</annotation></semantics></math>en</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.2">multilingual baseline</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.3">15.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.4">37.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.5">15.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.3.3.3.3.3.3.3.3.6">38.1</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.12.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.12.7.1">+ DD, FT, BT</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.12.7.2">15.8</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.12.7.3">37.8</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.12.7.4">15.0</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.12.7.5">38.3</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.13.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.13.8.1">+ denoise</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.13.8.2">15.9</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.13.8.3">38.5</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.13.8.4">15.5</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.13.8.5">39.0</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.14.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.14.9.1">+ TEL</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.14.9.2">16.1</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.14.9.3">38.8</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.14.9.4">15.6</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.14.9.5">39.2</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_4" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1"><span class="ltx_text" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1">mz<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1"><semantics id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1a"><mo id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1" stretchy="false" xref="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1b"><ci id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml" xref="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1d">→</annotation></semantics></math>en</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.2">multilingual baseline</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.3">26.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.4">48.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.5">22.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.p2.4.4.4.4.4.4.4.4.6">44.0</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.15.10">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.15.10.1">+ DD, FT, BT</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.15.10.2">32.9</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.15.10.3">52.2</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.15.10.4">25.0</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.15.10.5">45.4</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.16.11">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.p2.4.4.4.4.4.4.4.16.11.1">+ denoise</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.16.11.2">33.7</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.16.11.3">52.2</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.16.11.4">25.8</span>
<span class="ltx_td ltx_align_center" id="S4.T3.2.p2.4.4.4.4.4.4.4.16.11.5">46.5</span></span>
<span class="ltx_tr" id="S4.T3.2.p2.4.4.4.4.4.4.4.17.12">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.2.p2.4.4.4.4.4.4.4.17.12.1">+ TEL</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.p2.4.4.4.4.4.4.4.17.12.2">33.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.p2.4.4.4.4.4.4.4.17.12.3">52.7</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.p2.4.4.4.4.4.4.4.17.12.4">26.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.p2.4.4.4.4.4.4.4.17.12.5">46.7</span></span>
</span>
</span></span></span>
</span></span></span></p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">Table 3: </span>The results of en-kh and en-mz language pairs on the test and dev set.</figcaption>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper presents the submission of HW-TSC to the WMT24 Indic MT Task. For the first two subtasks, we use IndicTrans2 as our baseline model to fine-tune it with corpus provided by WMT24 on the en-as and en-mn language pairs, which achieves remarkable performance. For the latter two subtasks, we train a multilingual model on the en-kh and en-mz language pairs, and then use training strategies such as R-Drop, DD, FT, BT, denoise and TEL to train the NMT model based on the deep Transformer-big architecture. By applying these training strategies, our submission achieved a competitive result in the final evaluation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdulmumin (2021)</span>
<span class="ltx_bibblock">
Idris Abdulmumin. 2021.

</span>
<span class="ltx_bibblock">Enhanced back-translation for low resource neural machine translation using self-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Information and Communication Technology and Applications: Third International Conference, ICTA 2020, Minna, Nigeria, November 24–27, 2020, Revised Selected Papers</em>, volume 1350, page 355. Springer Nature.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2014)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:11212020" title="">Neural machine translation by jointly learning to align and translate</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">CoRR</em>, abs/1409.0473.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caswell et al. (2019)</span>
<span class="ltx_bibblock">
Isaac Caswell, Ciprian Chelba, and David Grangier. 2019.

</span>
<span class="ltx_bibblock">Tagged back-translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</em>, pages 53–63.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dyer et al. (2013)</span>
<span class="ltx_bibblock">
Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013.

</span>
<span class="ltx_bibblock">A simple, fast, and effective reparameterization of ibm model 2.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 644–648.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et al. (2018)</span>
<span class="ltx_bibblock">
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018.

</span>
<span class="ltx_bibblock">Understanding back-translation at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, page 489. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2022)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022.

</span>
<span class="ltx_bibblock">Language-agnostic bert sentence embedding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 878–891.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gala et al. (2023)</span>
<span class="ltx_bibblock">
Jay Gala, Pranjal A Chitale, Raghavan AK, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, et al. 2023.

</span>
<span class="ltx_bibblock">Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2305.16307</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garmash and Monz (2016)</span>
<span class="ltx_bibblock">
Ekaterina Garmash and Christof Monz. 2016.

</span>
<span class="ltx_bibblock">Ensemble learning for multi-source neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</em>, pages 1409–1418.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks and Gimpel (2016)</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel. 2016.

</span>
<span class="ltx_bibblock">Bridging nonlinearities and stochastic regularizers with gaussian error linear units. corr abs/1606.08415 (2016).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1606.08415</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et al. (2016)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016.

</span>
<span class="ltx_bibblock">Fasttext.zip: Compressing text classification models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1612.03651</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba. 2014.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:1412.6980</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et al. (2007)</span>
<span class="ltx_bibblock">
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P07-2045" title="">Moses: Open source toolkit for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</em>, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">EMNLP 2018</em>, page 66.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2020)</span>
<span class="ltx_bibblock">
Xuan-Phi Nguyen, Shafiq Joty, Wu Kui, and Ai Ti Aw. 2020.

</span>
<span class="ltx_bibblock">Data diversification: a simple strategy for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</em>, pages 10018–10029.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2015)</span>
<span class="ltx_bibblock">
Maja Popović. 2015.

</span>
<span class="ltx_bibblock">chrf: character n-gram f-score for automatic mt evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the tenth workshop on statistical machine translation</em>, pages 392–395.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock">Improving neural machine translation models with monolingual data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">54th Annual Meeting of the Association for Computational Linguistics</em>, pages 86–96. Association for Computational Linguistics (ACL).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2014)</span>
<span class="ltx_bibblock">
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014.

</span>
<span class="ltx_bibblock">Dropout: a simple way to prevent neural networks from overfitting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">The journal of machine learning research</em>, 15(1):1929–1958.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Erven and Harremos (2014)</span>
<span class="ltx_bibblock">
Tim Van Erven and Peter Harremos. 2014.

</span>
<span class="ltx_bibblock">Rényi divergence and kullback-leibler divergence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">IEEE Transactions on Information Theory</em>, 60(7):3797–3820.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1706.03762</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Yiren Wang, Lijun Wu, Yingce Xia, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. 2020.

</span>
<span class="ltx_bibblock">Transductive ensemble learning for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 34, pages 6291–6298.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Daimeng Wei, Zongyao Li, Zhanglin Wu, Zhengzhe Yu, Xiaoyu Chen, Hengchao Shang, Jiaxin Guo, Minghan Wang, Lizhi Lei, Min Zhang, et al. 2021.

</span>
<span class="ltx_bibblock">Hw-tsc’s participation in the wmt 2021 news translation shared task.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 225–231.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Daimeng Wei, Zhiqiang Rao, Zhanglin Wu, Shaojun Li, Yuanchang Luo, Yuhao Xie, Xiaoyu Chen, Hengchao Shang, Zongyao Li, Zhengzhe Yu, et al. 2022.

</span>
<span class="ltx_bibblock">Hw-tsc’s submissions to the wmt 2022 general machine translation shared task.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the Seventh Conference on Machine Translation</em>, pages 403–410.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Daimeng Wei, Zhanglin Wu, Hengchao Shang, Zongyao Li, Minghan Wang, Jiaxin Guo, Xiaoyu Chen, Zhengzhe Yu, and Hao Yang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.acl-long.441" title="">Text style transfer back-translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em>, pages 7944–7959, Toronto, Canada.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu, et al. 2021.

</span>
<span class="ltx_bibblock">R-drop: Regularized dropout for neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Advances in Neural Information Processing Systems</em>, 34:10890–10905.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Zhanglin Wu, Daimeng Wei, Zongyao Li, Zhengzhe Yu, Shaojun Li, Xiaoyu Chen, Hengchao Shang, Jiaxin Guo, Yuhao Xie, Lizhi Lei, et al. 2023.

</span>
<span class="ltx_bibblock">The path to continuous domain adaptation improvements by hw-tsc for the wmt23 biomedical translation shared task.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 271–274.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. 2019.

</span>
<span class="ltx_bibblock">Curriculum learning for domain adaptation in neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of NAACL-HLT</em>, pages 1903–1915.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 08:52:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
