<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Survey on Drowsiness Detection – Modern Applications and Methods</title>
<!--Generated on Fri Aug 23 11:14:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Drowsiness detection,  Fatigue detection,  Drowsiness and safety,  Public Transportation,  Sleep analysis and drowsiness
" lang="en" name="keywords"/>
<base href="/html/2408.12990v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S1" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S2" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Areas of Applications</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S2.SS0.SSS0.Px1" title="In II Areas of Applications ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">Drowsiness detection in workplace and secured areas</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S2.SS0.SSS0.Px2" title="In II Areas of Applications ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">Drowsiness detection in healthcare sectors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S2.SS0.SSS0.Px3" title="In II Areas of Applications ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">Drowsiness detection at public transportation and aviation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S2.SS0.SSS0.Px4" title="In II Areas of Applications ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">Drowsiness Detection in smart home context</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S3" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Measuring Technology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S3.SS1" title="In III Measuring Technology ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">How does Electroencephalogram work for drowsiness detection?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S3.SS2" title="In III Measuring Technology ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">How does Electrocardiogram work for drowsiness detection?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S3.SS3" title="In III Measuring Technology ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">How does a vision-based drowsiness detection system work?</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Modern Applications and Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.SS1" title="In IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">EEG-based Drowsiness Detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.SS2" title="In IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">ECG-based Drowsiness Detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.SS3" title="In IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Vision-based Drowsiness Detection</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S5" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Widely Used Databases</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S6" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Performance and Evaluation Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Technical and Practical Limitations</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS1" title="In VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Limitations on vision-based technique</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS1.SSS0.Px1" title="In VII-A Limitations on vision-based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">From the database perspective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS1.SSS0.Px2" title="In VII-A Limitations on vision-based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">From the algorithmic perspective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS1.SSS0.Px3" title="In VII-A Limitations on vision-based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">From the biased data perspective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS1.SSS0.Px4" title="In VII-A Limitations on vision-based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">From on-site hardware limitation perspective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS1.SSS0.Px5" title="In VII-A Limitations on vision-based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">From data synthesis perspective</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS2" title="In VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Limitations on physiological signal based technique</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS2.SSS0.Px1" title="In VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">From motion artifacts perspective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS2.SSS0.Px2" title="In VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">From data transmission perspective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.SS2.SSS0.Px3" title="In VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title">From the multimodal perspective</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S8" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Potential Directions for Research</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S9" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IX </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S10" title="In A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">X </span><span class="ltx_text ltx_font_smallcaps">Biography Section</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey on Drowsiness Detection – Modern Applications and Methods</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Biying Fu, Fadi Boutros, Chin-Teng Lin and Naser Damer
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Drowsiness detection holds paramount importance in ensuring safety in workplaces or behind the wheel, enhancing productivity, and healthcare across diverse domains. Therefore accurate and real-time drowsiness detection plays a critical role in preventing accidents, enhancing safety, and ultimately saving lives across various sectors and scenarios. This comprehensive review explores the significance of drowsiness detection in various areas of application, transcending the conventional focus solely on driver drowsiness detection. We delve into the current methodologies, challenges, and technological advancements in drowsiness detection schemes, considering diverse contexts such as public transportation, healthcare, workplace safety, and beyond. By examining the multifaceted implications of drowsiness, this work contributes to a holistic understanding of its impact and the crucial role of accurate and real-time detection techniques in enhancing safety and performance. We identified weaknesses in current algorithms and limitations in existing research such as accurate and real-time detection, stable data transmission, and building bias-free systems. Our survey frames existing works and leads to practical recommendations like mitigating the bias issue by using synthetic data, overcoming the hardware limitations with model compression, and leveraging fusion to boost model performance. This is a pioneering work to survey the topic of drowsiness detection in such an entirely and not only focusing on one single aspect. We consider the topic of drowsiness detection as a dynamic and evolving field, presenting numerous opportunities for further exploration.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Drowsiness detection, Fatigue detection, Drowsiness and safety, Public Transportation, Sleep analysis and drowsiness

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Drowsiness detection aims at detecting the early symptoms of individual drowsiness using physiological (EEG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib1" title="">1</a>]</cite>, ECG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib2" title="">2</a>]</cite>) and visible behavioural (eye blinking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib3" title="">3</a>]</cite> and eye closure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib4" title="">4</a>]</cite>) indications. The accurate and real-time identification of drowsiness detection is crucial for multiple reasons. One of the most prominent use-cases is driver and road safety; drowsy driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib5" title="">5</a>]</cite> is a dominant cause of accidents, injuries, and fatalities on the road. The National Highway Traffic Safety Administration estimated that up to 20% of the annual traffic deaths were attributed to driver drowsiness in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib6" title="">6</a>]</cite>. An early alert can help prevent accidents caused by impaired reaction times of a drowsy driver. Under the setting of workplace safety, such as employees working in jobs like e.g., operating heavy machinery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib7" title="">7</a>]</cite>, industrial equipment, or in medical establishment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib10" title="">10</a>]</cite>, drowsiness can lead to accidents that jeopardize worker’s safety or patient safety. In addition, drowsiness could negatively impact the cognitive function and productivity of shift workers with long working hours. Even in healthcare, monitoring the drowsiness of patients, especially those with sleep disorders or undergoing medical treatments, ensures their well-being and helps healthcare providers make informed decisions about accurate treatment plans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib15" title="">15</a>]</cite>. Finally, drowsiness detection is crucial in sectors such as aviation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib17" title="">17</a>]</cite> and public transportation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib20" title="">20</a>]</cite>, where a drowsy operator can compromise passenger safety.
The attentiveness of the driver significantly impacts railway safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib19" title="">19</a>]</cite>. Incidents involving high-speed trains can result in severe consequences, as evidenced by the most catastrophic railway accident in Chinese history occurring on 23 July 2011, resulting in 40 fatalities and at least 192 injuries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib21" title="">21</a>]</cite>. Therefore, finding ways to unobtrusively detect driver drowsiness operating these high-speed trains is important. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the application areas that can benefit greatly from accurate and real-time drowsiness detection.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="379" id="S1.F1.g1" src="extracted/5809467/drowsiness_detection_areas.jpg" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Depicts areas of applications in urgent need of accurate and real-time drowsiness detection task.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Drowsiness detection is thus vital for safety, preventing accidents caused by impaired attention and reaction times. It improves health by identifying sleep disorders and enhances productivity in various settings. After motivating the importance of drowsiness detection, our work preliminary focused on showing modern applications and methods on drowsiness detection. We are the first work concluding multiple aspects and areas of drowsiness detection not only focusing especially on driver drowsiness detection. Current surveys on drowsiness detection mainly focused on presenting either techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib25" title="">25</a>]</cite> or systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib27" title="">27</a>]</cite> on driver drowsiness detection. Less focus was put on the general use-cases of drowsiness detection in such a broad application area and its faced challenges.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="309" id="S1.F2.g1" src="extracted/5809467/categorization.jpg" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall structure of the surveyed advancements in drowsiness detection. This primary framework illustrates the three measuring techniques utilized in drowsiness detection, along with their respective attributes and methodologies. From the considered topology, we first distinguish between drowsiness detection based on physiological signals and monitoring through vision. Based on these classification, we assign different attributes to individual measuring techniques. The sensing modalities for EEG and ECG are not mutually exclusive and can be divided under wired and wireless application. While under the vision-approach, we now consider static investigation for single frame and dynamic investigation for multiple frames and the unobtrusiveness for the general setup. Abbreviations like HRV stands for heartrate variability, PRV for pulserate variability, QRS for morphological components of a heartbeat, LF/HF for certain frequency bands, and finally PERCLOS for percentage of eye closure. The sensing modality demonstrates the potential realization of applications. The bottom row outlines the typical areas of application for drowsiness detection across all measuring techniques.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S1.F2" title="Figure 2 ‣ I Introduction ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">2</span></a> depicts a primary framework of this survey, illustrating the three measuring techniques and their respective attributes used for detection, sensing modalities, and typical application areas across all measuring techniques. and provides a clear view of the structure of our work. Based on our proposed topology, we differentiate between drowsiness detection relying on precise physiological signals and monitoring through vision-based approaches. Biological processes manifest themselves in brain and heart activities, that can lead to distinct changes in specific attributes induced by drowsiness. Consequently, we can assign different attributes to individual measuring techniques based on our classification. The sensing modality for physiological signals typically involves electrode-based methods. However, there is a shift towards wearable and wireless communication technologies in this regard to enhance user acceptance. Monitoring through vision is in general more unobtrusive and user friendlier, focusing on relevant attributes such as facial features, eye closures, and head poses, among others. We view single frame as static and consecutive frames as dynamic sensing modality in this context.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The structure of our work is outline in the following. In Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S2" title="II Areas of Applications ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">II</span></a>, we discuss recent works within each of the application areas. In Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S3" title="III Measuring Technology ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">III</span></a> we first introduce three general techniques typically used for drowsiness detection under two main categories, i.e. physiological signal- and vision-based approaches. In Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4" title="IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">IV</span></a>, we extend the techniques with relevant works. In Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S5" title="V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">V</span></a> we provide a list of public benchmarks used to evaluate or design algorithms for drowsiness detection both in form of time series and image- or video-sequence-based data. In Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S6" title="VI Performance and Evaluation Metrics ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">VI</span></a>, we exclusively explained the performance and evaluation metrics used for assessing the algorithm’s effectiveness in state-of-the-art (SOTA) works. We then discuss and reveal the existing limitations within this research area categorized under physiological and vision-based approaches and present potential solutions and actionable suggestions in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7" title="VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">VII</span></a>. In Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S8" title="VIII Potential Directions for Research ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">VIII</span></a>, we summarize the current research gaps and provide potential future research directions. Finally, we conclude our work by summarizing the major findings in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S9" title="IX Conclusion ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">IX</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Areas of Applications</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we first start with relevant works for drowsiness detection grouped by applications. Beyond the scope of driver’s drowsiness detection, drowsiness detection can take place in several other aspects of our daily life, as it has far-reaching effects ranging from affecting our quality at work, influencing our healthcare, or clouding our ability at working on cognitive tasks.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Drowsiness detection in workplace and secured areas</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Drowsiness and sleepiness in the workplace are two of the major risks of modern society <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib29" title="">29</a>]</cite>. Well-rested and alert employees are fundamental for better productivity and creativity, while excessive fatigue not only reduces efficiency but also can pose a risk at workplaces, thus fatigue management in the workplace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib30" title="">30</a>]</cite> aims to enhance worker health and well-being both on and off the workspace.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">One example of a mitigating safety issue caused by drowsiness at the workplace is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib7" title="">7</a>]</cite>. The work by Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib7" title="">7</a>]</cite> focused on improving the performance of drowsiness detection for crane operators by using hybrid deep neural networks connecting both spatial and temporal features from videos. The primary contributions of this study involved extending drowsiness detection beyond vehicle drivers to crane operators, introducing relevant facial features as indicators for detection. In addition to their customized dataset on simulated crane operation scenarios, the research provided recommendations for gathering extensive and publicly accessible realistic drowsiness datasets tailored to crane operators.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.1">Sectors engaged in safety-critical endeavors, including the oil and gas industry, exhibit a vested interest in monitoring biological markers to avert human errors and enhance process safety, thereby enhancing their readiness for emergency situations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib31" title="">31</a>]</cite>. Within this context, the reliability of human performance assumes a pivotal role in preventing potential catastrophic incidents stemming from human factors, such as worker fatigue. Ramos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib32" title="">32</a>]</cite> introduced an ensemble approach for fully automated drowsiness detection utilizing Electroencephalogram (EEG) signals.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p4">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p4.1">Even in office environments across various industries, persistent drowsiness poses a significant challenge, resulting in occupational fatigue due to excessive work demands, particularly when precise operational output is essential. Presently, companies are increasingly dedicating resources to monitor their workplaces, aiming to uphold employees’ optimal working states and sustain a desirable level of productivity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib28" title="">28</a>]</cite>. Natnithikarat et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib33" title="">33</a>]</cite> proposed one approach to detect the drowsiness state of an office worker that combines Biometric characteristics like keyboard keystrokes and mouse movement along with eye tracking during office-related tasks. The objective was to identify and assess drowsiness based on the self-reported Karolinska sleepiness scale (KSS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib34" title="">34</a>]</cite> questionnaire. The study identified a correlation between the anticipated level of drowsiness inferred from the biometric data and the estimated KSS score provided by the users. These findings suggest the viability of using the proposed method to effectively detect the level of drowsiness among office workers.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Drowsiness detection in healthcare sectors</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Modern industries, hospitals, and many other essential sectors require shift work to maintain productivity and profit<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib36" title="">36</a>]</cite>. However shift work with its induced drowsiness and fatigue in shift workers has proved to be the source of human error and can lead to a number of accidents, catastrophes, and health-related problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib37" title="">37</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">In healthcare sectors, Geiger-Brown et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib8" title="">8</a>]</cite> revealed in their study that nurses frequently express fatigue and dissatisfaction with the quality of sleep when engaged in 12-hour shifts. This study examines the sleep patterns, levels of sleepiness, fatigue, and neuro-behavioral performance across three consecutive 12-hour shifts (both day and night) for hospital nurses. The findings indicate that nurses accumulate a substantial sleep deficit during successive 12-hour shifts, leading to increased fatigue, sleepiness, and decreased attention. Other studies investigating the subjective sleep quality and daytime sleepiness among nursing staff can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib10" title="">10</a>]</cite>. As a result, it becomes imperative to promptly and accurately identify fatigue in real-time to mitigate potential instances of malpractice stemming from fatigue-related issues in the medical domain.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p3.1">Chervin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib38" title="">38</a>]</cite> conducted an investigation in 2000, focusing on the relation of sleepiness, fatigue, tiredness, and lack of energy in individuals to obstructive sleep apnea. The study encompassed a comprehensive analysis involving 190 participants, including 117 males (M) and 73 females (F), within a university-affiliated sleep laboratory. Data were derived from both sleep studies and questionnaires. The study’s findings suggested that complaints regarding fatigue, tiredness, and reduced energy levels could hold comparable significance to reports of sleepiness among obstructive sleep apnea patients. Notably, female patients seemed to express such concerns more frequently than their male counterparts.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Drowsiness detection at public transportation and aviation</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">In the public transportation sector, accurate and real-time drivers’ drowsiness detection can save human lives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib20" title="">20</a>]</cite>. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib39" title="">39</a>]</cite> proposed a non-parametric solution for detecting the cognitive state of pilots by utilizing a 64-channel brain EEG signal. They developed 2D brain maps from these spatially distributed 3D multichannel EEG signals and extracted useful feature maps for developing algorithms to detect fatigue in pilots. Another work by Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib16" title="">16</a>]</cite> showcased drowsiness detection within the aviation industry too. They utilized EEG signals obtained from a standard aviation headset to identify the drowsiness levels of pilots. The researchers integrated the Seeing Machines driver monitoring system
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Seeing Machines driver monitoring system: https://seeingmachines.com/</span></span></span> with electrooculogram (EOG) data to localize microsleep events and investigated unique characteristics in EEG spectral patterns during these events. Simultaneous recordings of EEG, EOG, and facial behavior data were taken from 16 pilots during simulated flights. Their study demonstrated useful features from the EEG signals and confirmed the effectiveness of drowsiness detection by embedding EEG electrodes within the commonly used aviation headset.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib18" title="">18</a>]</cite> targeted drivers operating high-speed trains. They proposed a driver’s drowsiness detection system for high-speed train safety based on monitoring train driver’s vigilance using a wireless wearable EEG. The proposed system includes three stages ranging from wireless data collection and driver vigilance detection to pushing early alert messages to drivers. An 8-channel wireless wearable brain-computer interface is used to unobtrusively collect the locomotive driver’s brain EEG signal, while the driver is simultaneously operating a high-speed train.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p3.1">Multi-modal fusion of multiple physiological signals and leveraging deep learning techniques for driver’s drowsiness detection for high-speed rail operators can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib19" title="">19</a>]</cite>. One research specifically deals with drowsiness detection from facial clues with occluded face images of railway drivers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib20" title="">20</a>]</cite>. This study was conducted in the post-covid phase to work with the autonomous-rail rapid transit system in China railway. This research was built upon facial thermal imaging and also included environmental information for detection.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Drowsiness Detection in smart home context</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p1.1">Beyond workspace applications, modern homes with integrated smart technologies also intend to enhance the user’s living experience in domestic areas. The smart home application with the smart mirror is used as an example to assess residents’ well-being over time to improve their lifestyle through user-centered guidance. Facial clues indicating user’s emotional states like stress, fatigue and anxiety are targeted in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p2.1">Elderly fall due to drowsiness was studied in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib41" title="">41</a>]</cite>. Based on facial landmarks and eye openness, Kumar et al. further analyzed sleep patterns in order to help to predict the physical condition of the elderly and to avoid emergency situations such as falls. The main contribution of this work is to predict the health condition of the elderly by leveraging machine learning models and their results were verified on real-world scenarios while maintaining good accuracy (Acc).</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p3.1">Another proof of concept work focusing on real-time drowsiness detection for elderly care is in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib42" title="">42</a>]</cite>. The study is based on video feeds to extract visible facial clues, such as yawning, eyelid and head movement over time which are related to drowsiness detection. Classification simply based on eyelid and mouth status already achieved an accuracy between 94.3%-97.2%.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p4">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p4.1">In this survey, we focus on showing modern application areas and methods in terms of drowsiness detection not only limited to driver’s drowsiness detection but extended to cover much broader possible scenarios and detection under more general and diverse purposes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Measuring Technology</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we present the three most popular measuring techniques for drowsiness detection. These measuring techniques can be categorized under physiological sensing (which uses either EEG or ECG) and vision-based sensing. Under physiological sensing, researchers aim to capture numerous biological signals of each individual, such as heart rate variability, muscle movement, and brain wave activities. From these biological signals, unique biological markers are extracted to detect drowsiness. Under vision-based capture, visual feeds are leveraged to derive facial expressions and eye blinking status, among other clues, to give an indication of drowsiness.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">How does Electroencephalogram work for drowsiness detection?</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Electroencephalogram (EEG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib43" title="">43</a>]</cite> measures the electrical activity of the brain. It involves the recording of the brain’s electrical signals using electrodes placed on the scalp. EEG is a non-invasive and painless procedure used to study brain activity, diagnose certain brain disorders, and monitor brain health during medical treatments.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">EEG-based drowsiness detection systems are just one component of an overall driver safety system. During a driving simulation or driving a vehicle in a controlled environment, the EEG system continuously records brain activity throughout the driving session. From the raw multi-channel EEG data, relevant features are extracted that correlate with drowsiness. These features often include changes in different brainwave frequencies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib44" title="">44</a>]</cite>, asymmetry in frequency bands <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib45" title="">45</a>]</cite>, or power spectral density <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib47" title="">47</a>]</cite>. Similarly, these features are used to train a machine learning model detecting the real-time state of the driver as in either awake or drowsy state. Mardi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib48" title="">48</a>]</cite> demonstrated that the brain exhibits its lowest levels of activity and complexity when a driver experiences drowsiness. Consequently, individuals in such a state lose their concentration and control, thereby hindering their ability to respond promptly to stimuli.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">EEG is a valuable tool in neuroscience and clinical settings, providing insights into brain function and helping diagnose and manage various neurological conditions. Its non-invasiveness and ability to capture real-time brain activity make it a widely used method for studying brain health and understanding brain-related disorders. EEG also serves as the ’gold standard’ and is extensively applied to indicate the transition between wakefulness and sleepiness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib49" title="">49</a>]</cite>. Spontaneous alpha activity detected in EEG signals can indicate different underlying physiological process. Spontaneous alpha activity refers to rhythmic electrical oscillations in the brain that occur predominantly in the alpha frequency range, typically between 8 and 13 Hz <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib50" title="">50</a>]</cite>. According to Cantero et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib50" title="">50</a>]</cite>, alpha activity in this range can be related to processes like wakefulness, drowsiness period, and REM sleep phase. Ogilvie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib51" title="">51</a>]</cite> investigated the process of falling sleep in relation to the alpha activities being recorded and associated the alpha wave disappearance to be related to stage I sleep. Kleitman and his students <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib52" title="">52</a>]</cite> also examined the correlation between muscle relaxation and EEG activity, observing a drop of a hand-held spool between 0.5 and 25 seconds after the alpha wave had vanished.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">How does Electrocardiogram work for drowsiness detection?</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The most effective outcomes in driver drowsiness detection have been attained through these electrode based instruments including EEG and EOG measurements to date<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib53" title="">53</a>]</cite>, thus establishing them as the prevailing standard in this research domain. Nevertheless, implementing driver monitoring using EEG measurement proves impractical. In contrast, Electrocardiogram (ECG) recording offers a more feasible alternative, given its ease of capture, significantly larger magnitude, and lower susceptibility to noise interference.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Sleep is a complex state marked by important changes in the autonomic modulation of the cardiovascular activity as investigated by Viola et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib54" title="">54</a>]</cite>. Heart rate variability (HRV) undergoes substantial alterations across different sleep stages, reflecting a prevailing parasympathetic influence on the heart during non-rapid eye movement (NREM) sleep, while displaying heightened sympathetic activity during rapid eye movement (REM) sleep. In addition, respiration also undergoes notable changes, deepening and becoming more regular during deep sleep and shallower and more frequent during REM sleep. These effects were thoroughly investigated by Cabiddu et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib55" title="">55</a>]</cite> and thus making the ECG measurement a well-suited technique for investigating sleep analysis and drowsiness detection.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">ECG is a medical instrument to record the electrical activity of the heart over a period of time. The ECG provides valuable information about the heart’s rhythm, rate, and overall electrical activity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib56" title="">56</a>]</cite>. To perform an ECG, a set of electrodes is placed on the patient’s skin at specific locations. The electrodes are connected to an electrocardiograph which detects and amplifies the electric signals generated by the heart. The recorded electrical impulses are time series representing the movement of the heart’s chambers during each heartbeat. A cardiologist or a trained technician can analyze the ECG recording to identify abnormalities or irregularities that can indicate various heart conditions or problems. ECG signals can exhibit various abnormalities indicative of heart related deceases, including arrhythmia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib57" title="">57</a>]</cite>, ST-segment abnormalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib58" title="">58</a>]</cite> as indicators for hear attack, T-wave abnormalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib59" title="">59</a>]</cite> and QT interval prolongation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib60" title="">60</a>]</cite> indicative of electrolyte imbalances, QRS complex abnormalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib61" title="">61</a>]</cite> indicative for bundle branch blocks, and other artifact or noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib62" title="">62</a>]</cite> among others.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The ECG is a non-invasive and painless measuring procedure, making it a widely used tool for diagnosing heart-related issues. In sleep studies, ECG can be used to record heart rate variations providing hints for sleep disorders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib64" title="">64</a>]</cite> or sleep apnea <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib15" title="">15</a>]</cite> detection. Nowadays, simplified versions of ECGs exist, such as portable ECG devices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib67" title="">67</a>]</cite> based on Bluetooth transmission or wearable sensors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib69" title="">69</a>]</cite> for ECG measurement, making the application of ECG in everyday life possible. There are several studies linking ECG-based features to drowsiness detection as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib14" title="">14</a>]</cite>. Most of these features are based on detecting heart rate variability and are commonly derived from the frequency and spectrum domains.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">How does a vision-based drowsiness detection system work?</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Both previous methods, which rely on biosignals, are well-suited for laboratory conditions but are not very practical in real-world driving scenarios. This is because, while driving on the road, both the movement and the dynamic environment significantly impact performance and pose challenges to extracting features from captured biosignals. Both measures would require individuals sitting still and wearing cables or head-mounted devices. Thus, we motivate for vision-based approaches overcoming the restrictions of wired installation or head-mounted devices, which might effect usability in some use-cases. Vision-based drowsiness detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib4" title="">4</a>]</cite> refers to a technique that uses visual information, such as facial expressions and eye movements, to identify signs of drowsiness in individuals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib70" title="">70</a>]</cite>. This method is commonly employed in various domains, including driver fatigue monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib3" title="">3</a>]</cite>, operator alertness assessment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib7" title="">7</a>]</cite>, and other safety scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib18" title="">18</a>]</cite> where detecting drowsiness is crucial for safety and performance. Research linking facial muscle movement to different levels of muscle fatigue can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib71" title="">71</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The pipeline of vision-based drowsiness detection often includes the following steps. During the image/video acquisition stage, a camera mostly installed on dashboards captures real-time visual data from the occupant, such as facial images or eye movement. Relevant facial features are extracted from the acquired images or video frames. Typical features involve facial landmarks, eye closures, head movements, and changes in gaze direction. These features are used to train a machine learning model often leading to a binary classification model to determine the binary states of awake or drowsiness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib72" title="">72</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Vision-based drowsiness detection systems offer real-time monitoring capabilities and can be integrated into various applications, such as in-vehicle driver assistance systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib65" title="">65</a>]</cite> or workplace safety monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib7" title="">7</a>]</cite>. They play a crucial role in enhancing safety and reducing the risk of accidents caused by drowsy or fatigued individuals.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Modern Applications and Methods</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we provide a comprehensive survey of recent relevant research, which is divided into three main methods for detecting drowsiness using either physiological signals or visual sampling. We distinguish between these methods based on the variables measured. ECG signals use low-dimensional heartbeat signals, EEG signals use multichannel brain activity signals, and visual feeds provide image data. Each method is summarized with a table containing the investigated research works, which is later discussed. Thereby, we consider various aspects, such as year of publication, area of use, specific algorithms developed, evaluation database and its properties, performance, and conclude with special remarks.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">EEG-based Drowsiness Detection</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">EEG signals are often used to detect the mental stress of patients <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib73" title="">73</a>]</cite> but are also one of the physiological signals used to derive the drowsiness state <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib74" title="">74</a>]</cite>. For detecting the EEG signals, electrodes are detached from the skin directly thus allowing clear signal acquisition. Earlier works as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib32" title="">32</a>]</cite> applied traditional machine learning approaches with handcrafted features extracted from these physiological signals. The development in the last few years moved to more advanced approaches based on Deep Q-Learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib75" title="">75</a>]</cite> or deep learning in general <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib76" title="">76</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Traditional machine learning uses handcrafted feature extracted from the EEG power spectrum density to build efficient models for drowsiness detection. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib18" title="">18</a>]</cite> applied a support vector machine (SVM) on Fast Fourier Transformation (FFT) features as a binary classifier. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib16" title="">16</a>]</cite> investigated unique characteristics from EEG spectral patterns during micro-sleep events. Ramo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib32" title="">32</a>]</cite> leveraged data from five diverse EEG signal channels and employed ensemble learning techniques such as bagging to construct a robust and more precise drowsiness detection system. The efficacy of the system was validated using the DROZY database <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib77" title="">77</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Spatio-temporal convolution served as the cornerstone for successive deep learning-based approaches to derive both the sequential and spatial characteristic features from this physiological signal. Jeong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib17" title="">17</a>]</cite> extended the binary classification task of the drowsiness state to a more fine-grained classification of five drowsiness levels from EEG signals. They stated to be the first work providing such a detailed classification of drowsiness levels using only EEG signals. They acquired EEG data from ten pilots in a simulated night flight environment. They proposed a deep spatio-temporal convolutional bidirectional long short-term memory network (DSTCLN) model. The classification performance is evaluated using the Karolinska sleepiness scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib34" title="">34</a>]</cite> for two mental states and five drowsiness levels. Results demonstrated the feasibility of their proposed fine-grained drowsiness classification.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Similarly, Cui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib78" title="">78</a>]</cite> improved the subject-independent drowsiness recognition from single-channel EEG with an interpretable CNN-LSTM model. In this work, authors put more effort on the explainability of the proposed deep learning models by revealing the network’s decision with respect to the input data. Results showed a model average accuracy of 72.97% on 11 subjects for leave-one-out subject-independent detection on a public dataset <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Project page with access to data: https://figshare.com/articles/dataset/Multi-channel_EEG_recordings_during_a_sustained-attention_driving_task/6427334</span></span></span>. They stated that their proposed method surpasses conventional baseline methods and other SOTA deep learning methods till publication. Similar network architecture is leveraged by Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib79" title="">79</a>]</cite> in a more recent work. This work investigated the optimal length of input time series for more accurate detection of drowsiness at multiple levels (awake, sleep, and drowsy), while few studies have seriously considered this feature before.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Paulo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib80" title="">80</a>]</cite> investigated two approaches for drowsiness detection using EEG signals during a sustained-attention driving task. The study focused on pre-event time windows and addressed the challenge of cross-subject zero calibration. EEG signals are known for their low signal-to-noise ratio and individual differences between subjects, thus requiring individual calibration cycles. To tackle this issue, the researchers employed spatio-temporal image encoding representations in the form of recurrence plots for classification using deep CNN. The results obtained from a public dataset of 27 subjects showed the effectiveness of their cross-subject zero calibration approach, highlighting its success in drowsiness detection. Similarly, Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib81" title="">81</a>]</cite> targeted the same issue by only requiring a few subject-specific calibrations to adjust for a new subject. They provided an online and multiview setup to enforce the consistencies across different views in both source and target domains, and thus, making the system in general more robust. In addition, online training makes the proposed application more suitable for practical requirements. Recent follow-up work focusing on cross-subject investigations was presented by Cui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib82" title="">82</a>]</cite>. In this work, the focus further lies in the interpretability of drowsiness detection schemes and automatic feature selections from EEG signals.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">A reinforcement learning-based method for the task of drowsiness detection is introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib75" title="">75</a>]</cite>. Ming et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib75" title="">75</a>]</cite> leveraged deep Q-learning to analyze EEG dataset collected during simulated driving to estimate driver drowsiness state. The main research is to relate certain characteristics of the EEG data to better derive the response time in order to indirectly estimate the driver’s drowsiness state. Their results showed superior performance compared to supervised learning and is promising for real applications.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">The most current research by Zhuang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib83" title="">83</a>]</cite> leveraged Graph Neural Networks (GNNs) for EEG-based driver drowsiness detection in real-time. Their results surpassed the accuracy of other CNNs and graph generation methods based on drowsiness detection schemes. They proposed a GNN-based network with a self-attention mechanism that can focus on developing task-relevant connectivity networks via end-to-end learning. In addition, the authors leveraged a squeeze-and-excitation (SE) block to select the most relevant features and feature bands for drivers’ drowsiness detection. This block is shown both to improve the classification accuracy and the model’s interpretability.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">From the data augmentation point of view, Chaabene et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib76" title="">76</a>]</cite> introduced an EEG-based drowsiness detection system based on deep learning networks. The system follows a two-stage framework, encompassing (i) data acquisition and (ii) model analysis. For data collection, the authors employed a wearable Emotiv EPOC + headset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib84" title="">84</a>]</cite>, recording EEG signals from 14 channels along with signal annotations. Data augmentation techniques were implemented to prevent the proposed model from overfitting. The chosen deep learning architecture in this study used a CNN network. A self-collected dataset containing 42 records of six men and eight women aged between 14 and 64 with normal mental health are used for evaluation. The outcomes exhibited a noteworthy accuracy of 90.42% in binary classification for distinguishing drowsy and awake states. Compared to alternative research, the proposed approach demonstrated its efficacy and efficiency.</p>
</div>
<div class="ltx_para" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.1">From the distributed system point of view, Qin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib1" title="">1</a>]</cite> proposed a driver’s drowsiness state detection system using EEG signals. They increased the accuracy of their system by using Federated Learning (FL) and CNN. FL is used to accumulate knowledge from the data of different clients under privacy protecting mechanism and CNN is used to identify and explain the driver’s drowsiness state. However, they evaluated their method only on a relatively small amount of private database consisting of 11 subjects.</p>
</div>
<div class="ltx_para" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1">Another more difficult and pressing issue is the detection of microsleep events (MSE). MSE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib85" title="">85</a>]</cite> refers to abrupt and non-anticipated lapses of attention experienced by individuals, typically resulting from drowsiness and monotony. MSE can serve as objective indicators of excessive daytime sleepiness and can be characterized by a non-anticipated brief period of sleep lasting between 2 and 30 seconds, occurring amidst ongoing wakefulness as investigated by Carskadon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib86" title="">86</a>]</cite> in the Encyclopedia of sleep and dreaming. Microsleep accounts for an annual loss of nearly 150 million dollars due to diminished daily work performance and vehicular accidents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib87" title="">87</a>]</cite>. Assessing an individual’s level of sleepiness and detecting the onset of microsleep is thus crucial for tasks demanding sustained focus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib88" title="">88</a>]</cite>, such as driving or operating machinery during nighttime hours, where falling asleep poses high risks. In recent years, this subject has received widespread attention from governmental bodies, the public, and the research community alike <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib89" title="">89</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.1">Such subtle events like microsleep episodes are very hard to recognize. The group of Golz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib85" title="">85</a>]</cite> worked extensively on detecting microsleep episodes from EEG and EOG data. To achieve detection, signals coming from heterogeneous sources are processed, such as the brain electric activity captured by EEG data, variation in the pupil size, and eye and eyelid movements captured by EOG data. By combining the spectral and the state space, both linear and non-linear features are considered. The binary decision networks between MSE and non-MSE are based on a support vector machines (SMV) and a learning vector quantization (LVQ) scheme. However, pupil adaptation through light stimuli could affect the accuracy of detection.</p>
</div>
<div class="ltx_para" id="S4.SS1.p12">
<p class="ltx_p" id="S4.SS1.p12.1">Pham et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib90" title="">90</a>]</cite> proposed a more flexible and mobile application by introducing WAKE, which is a behind-the-ear wearable device for microsleep detection. They utilized bone-conduction headphones to gather biosignals including brain activity, eye movements, facial muscle contractions, and galvanic responses from the area behind the user’s ears. Their findings demonstrated that WAKE effectively suppressed motion and environmental noise in real-time by 9.74-19.74 dB during activities such as walking, driving, or being in various environments, ensuring reliable capture of the biosignals. A preliminary training conducted on 19 sleep-deprived and narcoleptic subjects demonstrated an average precision and recall of 76% and 85% on an unseen subject with leave one subject out cross validation technique.</p>
</div>
<div class="ltx_para" id="S4.SS1.p13">
<p class="ltx_p" id="S4.SS1.p13.1">A way to detect microsleep with deep learning architectures even with less training data is proposed by Chougule et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib91" title="">91</a>]</cite>. Their model uses the attention-based mechanism, which combines the advantages of the wavelet transform with the Short Time Fourier Transform (STFT) Spectogram. By separating ”time-dependent” and ”time-independent” parts, the deep learning model is more robust to capture both the sequence features and simultaneously learn the relationships between epochs. Only a single electrode EEG signal was used to achieve greater social acceptability. The training and evaluation are performed on the public Maintenance of Wakefulness Test (MWT) dataset <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>MWT dataset: https://sites.google.com/view/utarldd/home</span></span></span>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p14">
<p class="ltx_p" id="S4.SS1.p14.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T1" title="TABLE I ‣ IV-A EEG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the investigated works using EEG signals for drowsiness detection in various application scenarios. Sensing modality covers both wired and wireless wearable devices targeting non-intrusive applications. References to the public databases are provided in the footnote. From Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T1" title="TABLE I ‣ IV-A EEG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">I</span></a>, we notice that all recent works starting from 2021 utilized deep learning-based approaches to mitigate the investigation of handcrafted features. We further observed that the most predominant evaluation dataset for physiological signal-based drowsiness detection is the DROZY dataset.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>summarizes recent works performing drowsiness detection based on EEG signals.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:433.6pt;height:337.1pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-325.6pt,252.9pt) scale(0.399681303831824,0.399681303831824) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.1.1" style="font-size:90%;">work</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.2.1" style="font-size:90%;">year</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.3.1" style="font-size:90%;">area of use</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.4.1" style="font-size:90%;">algorithm</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.5.1" style="font-size:90%;">database</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.6.1" style="font-size:90%;">subjects/session</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.7.1" style="font-size:90%;">performance</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.8.1" style="font-size:90%;">remarks</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.3.2.1.1" style="font-size:90%;">Golz et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.3.2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib85" title="">85</a><span class="ltx_text" id="S4.T1.1.1.3.2.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.2.1" style="font-size:90%;">2007</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.3.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.3.1" style="font-size:90%;">microsleep events</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.3.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.3.2.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.3.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.3.2.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.4.1.1.1.1" style="font-size:90%;">support vector machine</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.3.2.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.4.1.2.1.1" style="font-size:90%;">Learning vector quantization</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.3.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.3.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.6.1" style="font-size:90%;">23 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.3.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.7.1" style="font-size:90%;">test errors = 9%</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.3.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.3.2.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.3.2.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.3.2.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.8.1.1.1.1" style="font-size:90%;">Biosignals from EEG and variations in</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.3.2.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.8.1.2.1.1" style="font-size:90%;">pupil size and eye movements</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.3.2.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.3.2.8.1.3.1.1" style="font-size:90%;">simulated driving, sleep deprivated subjects</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.4.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.4.3.1.1" style="font-size:90%;">Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.4.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib18" title="">18</a><span class="ltx_text" id="S4.T1.1.1.4.3.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.4.3.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.2.1" style="font-size:90%;">2017</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.4.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.4.3.3.1">
<tr class="ltx_tr" id="S4.T1.1.1.4.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.3.1.1.1.1" style="font-size:90%;">high-speed train</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.3.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.3.1.2.1.1" style="font-size:90%;">operators</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.4.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.4.3.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.4.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.4.1.1.1.1" style="font-size:90%;">SVM builds on FFT</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.4.1.2.1.1" style="font-size:90%;">features extracted from</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.4.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.4.1.3.1.1" style="font-size:90%;">EEG power spectrum</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.4.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.4.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.6.1" style="font-size:90%;">10 drivers</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.4.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.4.3.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.4.3.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.7.1.1.1.1" style="font-size:90%;">precision=90.79%,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.7.1.2.1.1" style="font-size:90%;">sensitivity=86.80%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.4.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.4.3.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.4.3.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.8.1.1.1.1" style="font-size:90%;">awake/drowsy, wearable</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.8.1.2.1.1" style="font-size:90%;">BCI model for EEGs,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.8.1.3.1.1" style="font-size:90%;">wireless data transmission,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3.8.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.4.3.8.1.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.4.3.8.1.4.1.1" style="font-size:90%;">virtual driving environment</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.5.4.1.1" style="font-size:90%;">Jeong et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.5.4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib17" title="">17</a><span class="ltx_text" id="S4.T1.1.1.5.4.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.2.1" style="font-size:90%;">2019</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.3.1" style="font-size:90%;">aviation, pilots</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.5.4.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.5.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.5.4.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.4.1.1.1.1" style="font-size:90%;">deep spatio-temporal</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.5.4.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.4.1.2.1.1" style="font-size:90%;">convolution</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.6.1" style="font-size:90%;">9 M, 1 F</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.5.4.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.5.4.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.5.4.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.7.1.1.1.1" style="font-size:90%;">2 states Acc = 0.87</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.4.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.5.4.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.7.1.2.1.1" style="font-size:90%;">5 levels Acc = 0.69</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.5.4.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.5.4.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.5.4.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.8.1.1.1.1" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.4.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.5.4.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.5.4.8.1.2.1.1" style="font-size:90%;">more fine-grained classification</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.6.5.1.1" style="font-size:90%;">Wang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.6.5.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib16" title="">16</a><span class="ltx_text" id="S4.T1.1.1.6.5.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.6.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.2.1" style="font-size:90%;">2019</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.6.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.6.5.3.1">
<tr class="ltx_tr" id="S4.T1.1.1.6.5.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.6.5.3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.3.1.1.1.1" style="font-size:90%;">aviation, public</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.5.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.6.5.3.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.3.1.2.1.1" style="font-size:90%;">transportation</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.6.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.6.5.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.6.5.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.6.5.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.4.1.1.1.1" style="font-size:90%;">handcrafted features</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.5.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.6.5.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.4.1.2.1.1" style="font-size:90%;">from EEG, EOG, and</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.5.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.6.5.4.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.4.1.3.1.1" style="font-size:90%;">facial data</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.6.5.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.6.5.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.6.1" style="font-size:90%;">16 pilots</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.6.5.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.7.1" style="font-size:90%;">find operational features</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.6.5.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.6.5.8.1" style="font-size:90%;">aviation headset equipped with sensors</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.7.6.1.1">
<tr class="ltx_tr" id="S4.T1.1.1.7.6.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.1.1.1.1.1" style="font-size:90%;">Natnithikarat</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.1.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.7.6.1.1.2.1.1" style="font-size:90%;">et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.7.6.1.1.2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib33" title="">33</a><span class="ltx_text" id="S4.T1.1.1.7.6.1.1.2.1.3.2" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.7.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.2.1" style="font-size:90%;">2019</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.7.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.3.1" style="font-size:90%;">office employees</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.7.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.7.6.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.7.6.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.4.1.1.1.1" style="font-size:90%;">linear regression task,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.4.1.2.1.1" style="font-size:90%;">PCA, SVM</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.7.6.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.7.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.7.6.6.1">
<tr class="ltx_tr" id="S4.T1.1.1.7.6.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.6.1.1.1.1" style="font-size:90%;">18 (15 M, 3 F),</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.6.1.2.1.1" style="font-size:90%;">1h office work</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.7.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.7.6.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.7.6.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.7.1.1.1.1" style="font-size:90%;">keystroke, mouse move</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.7.1.2.1.1" style="font-size:90%;">and self-evaluated KSS</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.7.1.3.1.1" style="font-size:90%;">correlation to drowsiness</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.7.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.7.6.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.7.6.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.8.1.1.1.1" style="font-size:90%;">keyboard, mouse events,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.8.1.2.1.1" style="font-size:90%;">eye-tracking, EEG+ECG</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.6.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.7.6.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.7.6.8.1.3.1.1" style="font-size:90%;">as reference</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.8.7.1.1" style="font-size:90%;">Pham et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.8.7.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib90" title="">90</a><span class="ltx_text" id="S4.T1.1.1.8.7.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.2.1" style="font-size:90%;">2020</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.8.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.3.1" style="font-size:90%;">microsleep events</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.8.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.8.7.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.8.7.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.8.7.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.4.1.1.1.1" style="font-size:90%;">feature engineering + classifiers</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.7.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.8.7.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.4.1.2.1.1" style="font-size:90%;">deep learnning on raw data</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.8.7.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.8.7.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.6.1" style="font-size:90%;">19 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.8.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.8.7.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.8.7.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.8.7.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.7.1.1.1.1" style="font-size:90%;">Avg precision = 76%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.7.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.8.7.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.7.1.2.1.1" style="font-size:90%;">recall=85%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.8.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.8.7.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.8.7.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.8.7.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.8.1.1.1.1" style="font-size:90%;">Headphones as wearable design</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.7.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.8.7.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.8.7.8.1.2.1.1" style="font-size:90%;">noise mitigation and uses EEG+EOG+EMG</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.9.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.9.8.1.1" style="font-size:90%;">Paulo et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.9.8.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib80" title="">80</a><span class="ltx_text" id="S4.T1.1.1.9.8.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.9.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.8.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.9.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.8.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.9.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.9.8.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.9.8.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.9.8.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.8.4.1.1.1.1" style="font-size:90%;">spatio-temporal en-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.9.8.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.9.8.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.8.4.1.2.1.1" style="font-size:90%;">coding CNN classifier</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.9.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.8.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.9.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.8.6.1" style="font-size:90%;">27 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.9.8.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.8.7.1" style="font-size:90%;">LOO-CV Acc=75.87%</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.9.8.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.9.8.8.1" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.10.9.1.1" style="font-size:90%;">Chaabene et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.10.9.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib76" title="">76</a><span class="ltx_text" id="S4.T1.1.1.10.9.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.9.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.10.9.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.10.9.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.10.9.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.4.1.1.1.1" style="font-size:90%;">DL-based two-stage,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.10.9.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.10.9.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.4.1.2.1.1" style="font-size:90%;">networks</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.9.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.10.9.6.1">
<tr class="ltx_tr" id="S4.T1.1.1.10.9.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.10.9.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.6.1.1.1.1" style="font-size:90%;">6 M, 8 F,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.10.9.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.10.9.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.6.1.2.1.1" style="font-size:90%;">42 records</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.9.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.7.1" style="font-size:90%;">Acc=90.42%</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.9.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.10.9.8.1" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.11.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.11.10.1.1" style="font-size:90%;">Cui et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.11.10.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib78" title="">78</a><span class="ltx_text" id="S4.T1.1.1.11.10.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.11.10.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.11.10.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.11.10.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.11.10.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.11.10.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.11.10.4.1" style="font-size:90%;">CNN-LSTM model</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.11.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.11.10.5.1" style="font-size:90%;">public</span><span class="ltx_ERROR undefined" id="S4.T1.1.1.11.10.5.2">\footref</span><span class="ltx_text" id="S4.T1.1.1.11.10.5.3" style="font-size:90%;">fn:eeg</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.11.10.5.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib92" title="">92</a><span class="ltx_text" id="S4.T1.1.1.11.10.5.5.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.11.10.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.11.10.6.1" style="font-size:90%;">11 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.11.10.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.11.10.7.1" style="font-size:90%;">Avg Acc=72.97% (LOO)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.11.10.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.11.10.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.11.10.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.11.10.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.11.10.8.1.1.1.1" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.11.10.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.11.10.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.11.10.8.1.2.1.1" style="font-size:90%;">cross subject recognition</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.12.11.1.1" style="font-size:90%;">Cui et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.12.11.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib82" title="">82</a><span class="ltx_text" id="S4.T1.1.1.12.11.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.11.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.12.11.2.1" style="font-size:90%;">2022</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.11.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.12.11.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.11.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.12.11.4.1" style="font-size:90%;">interpretable CNN</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.11.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.12.11.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.12.11.6.1" style="font-size:90%;">11 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.11.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.12.11.7.1" style="font-size:90%;">Avg Acc=78.35% (LOO)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.12.11.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.12.11.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.12.11.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.12.11.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.12.11.8.1.1.1.1" style="font-size:90%;">automatic feature selection from EEG features;</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.12.11.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.12.11.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.12.11.8.1.2.1.1" style="font-size:90%;">cross subject recognition</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.13.12.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.13.12.1.1" style="font-size:90%;">Ming et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.13.12.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib75" title="">75</a><span class="ltx_text" id="S4.T1.1.1.13.12.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.13.12.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.13.12.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.13.12.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.4.1" style="font-size:90%;">Deep Q-Learning</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.13.12.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.13.12.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.6.1" style="font-size:90%;">37 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.13.12.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.13.12.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.13.12.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.13.12.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.7.1.1.1.1" style="font-size:90%;">use DQN to derive</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.12.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.13.12.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.7.1.2.1.1" style="font-size:90%;">response time from</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.12.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.13.12.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.7.1.3.1.1" style="font-size:90%;">EEG data</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.13.12.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.13.12.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.13.12.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.13.12.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.8.1.1.1.1" style="font-size:90%;">Relate EEG characteristics to</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.12.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.13.12.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.8.1.2.1.1" style="font-size:90%;">response time to indrectly</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.12.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.13.12.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.13.12.8.1.3.1.1" style="font-size:90%;">estimate drowsiness states.</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.1.2.1" style="font-size:90%;">Ramos et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.1.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib32" title="">32</a><span class="ltx_text" id="S4.T1.1.1.1.2.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.3.1" style="font-size:90%;">2022</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.4.1.1.1.1" style="font-size:90%;">sectors engaged in</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.4.1.2.1.1" style="font-size:90%;">safety critical end-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.4.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.4.1.3.1.1" style="font-size:90%;">eavors, oil&amp;gas</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.5.1">
<tr class="ltx_tr" id="S4.T1.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.5.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.5.1.1.1.1" style="font-size:90%;">multiple channel EEGs,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.5.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.5.1.2.1.1" style="font-size:90%;">ensemble machine</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.5.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.5.1.3.1.1" style="font-size:90%;">learning</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.1.6.1" style="font-size:90%;">DROZY</span><span class="ltx_ERROR undefined" id="S4.T1.1.1.1.6.2">\footref</span><span class="ltx_text" id="S4.T1.1.1.1.6.3" style="font-size:90%;">fn:drozy</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.7.1" style="font-size:90%;">14 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.1.1">
<tr class="ltx_tr" id="S4.T1.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.1.1.1.1.1.1" style="font-size:90%;">Accuracy</span><math alttext="\geq" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S4.T1.1.1.1.1.1.1.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.1.m1.1b"><geq id="S4.T1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.1.1.1.m1.1d">≥</annotation></semantics></math><span class="ltx_text" id="S4.T1.1.1.1.1.1.1.1.2" style="font-size:90%;"> 90% for</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.1.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.1.1.2.1.1" style="font-size:90%;">specific subjects and</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.1.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.1.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.1.1.3.1.1" style="font-size:90%;">dedicated models</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.1.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.1.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.8.1.1.1.1" style="font-size:90%;">awake/drowsy,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.8.1.2.1.1" style="font-size:90%;">considered different setups</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.1.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.1.8.1.3.1.1" style="font-size:90%;">for evaluation.</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.14.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.14.13.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.14.13.1.1" style="font-size:90%;">Chougule et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.14.13.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib91" title="">91</a><span class="ltx_text" id="S4.T1.1.1.14.13.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.14.13.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.14.13.2.1" style="font-size:90%;">2022</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.14.13.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.14.13.3.1" style="font-size:90%;">microsleep events</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.14.13.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.14.13.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.14.13.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.14.13.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.14.13.4.1.1.1.1" style="font-size:90%;">attention-based method</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.14.13.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.14.13.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.14.13.4.1.2.1.1" style="font-size:90%;">combine STFT+Wavelets</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.14.13.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.14.13.5.1" style="font-size:90%;">MWT dataset </span><span class="ltx_ERROR undefined" id="S4.T1.1.1.14.13.5.2">\footref</span><span class="ltx_text" id="S4.T1.1.1.14.13.5.3" style="font-size:90%;">fn:mwt</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.14.13.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.14.13.6.1" style="font-size:90%;">64 (27M,37F)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.14.13.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.14.13.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.14.13.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.14.13.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.14.13.7.1.1.1.1" style="font-size:90%;">train acc=92%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.14.13.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.14.13.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.14.13.7.1.2.1.1" style="font-size:90%;">test acc=89.9%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.14.13.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.14.13.8.1" style="font-size:90%;">One electrode EEG for more user acceptance</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.15.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.15.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.15.14.1.1" style="font-size:90%;">Qin et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.15.14.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib1" title="">1</a><span class="ltx_text" id="S4.T1.1.1.15.14.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.15.14.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.2.1" style="font-size:90%;">2023</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.15.14.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.15.14.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.15.14.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.15.14.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.15.14.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.4.1.1.1.1" style="font-size:90%;">federated learning</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.15.14.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.15.14.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.4.1.2.1.1" style="font-size:90%;">CNN classifier</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.15.14.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.15.14.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.6.1" style="font-size:90%;">11 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.15.14.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.15.14.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.15.14.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.15.14.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.7.1.1.1.1" style="font-size:90%;">avg Acc=73.56%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.15.14.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.15.14.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.7.1.2.1.1" style="font-size:90%;">F1-score=73.26%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.15.14.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.15.14.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.7.1.3.1.1" style="font-size:90%;">AUC=78.23%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.15.14.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.15.14.8.1" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.16.15">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.16.15.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.16.15.1.1" style="font-size:90%;">Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.16.15.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib83" title="">83</a><span class="ltx_text" id="S4.T1.1.1.16.15.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.16.15.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.2.1" style="font-size:90%;">2023</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.16.15.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.16.15.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.16.15.4.1">
<tr class="ltx_tr" id="S4.T1.1.1.16.15.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.16.15.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.4.1.1.1.1" style="font-size:90%;">Graph Neural network</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.16.15.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.16.15.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.4.1.2.1.1" style="font-size:90%;">with attention</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.16.15.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.5.1" style="font-size:90%;">public</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.16.15.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.16.15.6.1">
<tr class="ltx_tr" id="S4.T1.1.1.16.15.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.16.15.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.6.1.1.1.1" style="font-size:90%;">27 subjects</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.16.15.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.16.15.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.6.1.2.1.1" style="font-size:90%;">62+ sessions</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.16.15.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.16.15.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.16.15.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.16.15.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.7.1.1.1.1" style="font-size:90%;">Acc=72.6%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.16.15.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.16.15.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.7.1.2.1.1" style="font-size:90%;">F1 = 70.7%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.16.15.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.16.15.8.1" style="font-size:90%;">awake/drowsy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.17.16">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.1.17.16.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T1.1.1.17.16.1.1" style="font-size:90%;">Lee et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.1.17.16.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib79" title="">79</a><span class="ltx_text" id="S4.T1.1.1.17.16.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.1.17.16.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.2.1" style="font-size:90%;">2023</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.1.17.16.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.3.1" style="font-size:90%;">drowsiness detection</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.1.17.16.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.4.1" style="font-size:90%;">LSTM-CNN model</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.1.17.16.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.1.17.16.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.6.1" style="font-size:90%;">19 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.1.17.16.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.17.16.7.1">
<tr class="ltx_tr" id="S4.T1.1.1.17.16.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.17.16.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.7.1.1.1.1" style="font-size:90%;">F1=95% (4000ms)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.17.16.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.17.16.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.7.1.2.1.1" style="font-size:90%;">Acc=85.6% (500ms)</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.1.17.16.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1.17.16.8.1">
<tr class="ltx_tr" id="S4.T1.1.1.17.16.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.17.16.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.8.1.1.1.1" style="font-size:90%;">multistage consciousness</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.17.16.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.17.16.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.8.1.2.1.1" style="font-size:90%;">(awake, sleep, drowsiness)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.17.16.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.1.1.17.16.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.1.17.16.8.1.3.1.1" style="font-size:90%;">auditory stimuli and button responses</span></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">ECG-based Drowsiness Detection</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The ECG is another measurement on the skin to record the heartbeat variability. This physiological signal is also often applied for driver drowsiness detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib93" title="">93</a>]</cite>. Different to multi-channel EEG signals, ECG does not need to be placed on the scalp, thus can be more suitable for drowsiness detection under more relaxed and natural conditions. Interestingly, the trend goes beyond the development of the algorithms and also affects the design of the sensors. The latest trend shows a shift from traditional, stationary medical placement to a more convenient, and wearable design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Takalokastari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib94" title="">94</a>]</cite> carried out real-time drowsiness detection utilizing a wireless sensor node connected to a wearable ECG sensor. They build a binary classification method based on extracted features from the ECG signal, which was sampled at 100Hz, to distinguish between awake and drowsy states. The wireless transmission of data facilitated the forwarding of information to a server PC. Notably, the QRS complex in the ECG signal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib95" title="">95</a>]</cite> offered valuable features that could aid in the diagnosis of various cardiovascular conditions. The process of drowsiness detection often involves the analysis of R peaks, R-R intervals, the interval between R and S peaks, and the duration of the QRS complex. Another work by Martins et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib96" title="">96</a>]</cite> conducted a comprehensive review that examined the latest research on fatigue detection and monitoring using wearable devices. Wearable devices offer a significant advantage by facilitating continuous and long-term monitoring of biomedical signals with comfort and non-intrusiveness. However, the study also identified distinct challenges associated with using wearable devices for fatigue monitoring.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Later, Shebakova focused on the inter-person variability of the detection scheme. Sherbakova et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib97" title="">97</a>]</cite> performed a thorough analysis of ECG signal for driver drowsiness detection. The authors stated that the threshold of drowsiness based purely on individual’s heart rate can vary for different people. Possible causes that could affect the threshold of drowsiness include the individual’s current postures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib98" title="">98</a>]</cite> or different cognitive states <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib99" title="">99</a>]</cite> among other factors. Relevant works showed that the heart rate is significantly different when individuals are supine or upright <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib98" title="">98</a>]</cite>. Difference in heart rate is also observable during the sleep phase in a high or low worrier state <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib99" title="">99</a>]</cite>. However, research demonstrated that parameters of HRV change over time depend on the current state (i.e. during wakefulness, drowsiness, and stress). Therefore, the authors suggested using the analysis of three ECG parameters, including heart rate (HR), LF/HF, and the Baevsky stress index <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib100" title="">100</a>]</cite> as robust indications for drowsiness detection. GPRS data transmission allows the processing and storage of ECG signals on a powerful server.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.2">Ke et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib101" title="">101</a>]</cite> proposed a drowsiness detection system using heartbeat detection from Android-based handheld devices. ECG signal acquired from a sensor is first transferred via Bluetooth to an Android device. The system extracted meaningful information from the ECG signal and indicative features are calculated from the power ratio after applying the hamming window and the Fourier transformation. Data was collected from a male and female test subject both in the awake state as well as in the asleep state. Evaluation results revealed a correlation between the state of drowsiness with a decreasing trend in the ratio (LF/HF). LF band stands for low-frequency component ranging from 0.04<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mo id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">∼</annotation></semantics></math>0.15Hz and HF band stands for high-frequency component ranging from 0.15<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2.1"><semantics id="S4.SS2.p4.2.m2.1a"><mo id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.2.m2.1d">∼</annotation></semantics></math>0.4Hz. Each controls certain functionalities in the vegetative nervous system.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">A more extended investigation of ECG-based drowsiness detection study was conducted by Fujiwara et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib102" title="">102</a>]</cite>. They proposed a detection algorithm based on heart rate variability (HRV) analysis and validated their method by comparing it with EEG-based sleep assessment. Eight features of heart rate variability are monitored to detect known abnormalities in the signal. During the experimental phase, data were collected from 34 participants in a driving simulator and their sleep stages were labeled by a sleep specialist. Results show that sleepiness was detected in 12 of 13 pre-N1 episodes before sleep onset.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">The work by Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib2" title="">2</a>]</cite> introduced a deep learning based approach for drowsiness detection. The authors investigated the robust and deterministic pattern of HRV signals collected from wearable ECG or photolethysmogram (PPG) sensors for driver drowsiness detection. Challenges of using wearable adds additional moving artifacts to collected time series. These motion artefacts can be alleviated. Three types of recurrence plots are generated as input features to a CNN for the binary classification of drowsy and awake state. An experimental dataset was collected under a virtual driving environment to evaluate the proposed measures.</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">With a notable surge in motorcycle traffic accidents, frequently leading to serious consequences and a significant loss of life, research on driver drowsiness detection for motorcyclists becomes more relevant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib103" title="">103</a>]</cite>. Motorcyclists are often more vulnerable compared to car drivers in case of accidents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib104" title="">104</a>]</cite>. To address this concern, Fahrurrasyid et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib105" title="">105</a>]</cite> introduced an innovative solution: a smart helmet integrated with various sensors. These sensors monitor psychological signals, including heartbeats, alongside a GPS module, GSM module, and alert push notifications. The study’s experiment, involving 10 participants, demonstrated the helmet’s capability to identify drowsiness and send alerts when the heart-rate drops below 60 bpm. This data is accessible in real-time, while the helmet also employs the Google Maps application to track the precise location of the incident.</p>
</div>
<div class="ltx_para" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1">Latest interesting work by Heydari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib106" title="">106</a>]</cite> introduced a technique to identify driver drowsiness by monitoring the pulse rate variability (PRV) measured on a finger. They analyzed finger pulse data which are derived from PPG signals, focusing on features within the pulse rate variability that exhibit notable changes during drowsiness. Findings reveal that the variability values, along with their averages, increased before the onset of sleepiness. Additionally, it was observed that the standard deviation of all peak-to-peak intervals notably decreases during drowsiness. Also, an increase in the values of the Root Mean Square of Successive Differences (RMSSD) is observed during the drowsiness stage. The authors suggested a purely conceptual design to integrate PPG sensors into a steering wheel to detect the driver’s finger pulse rate, offering a viable and non-invasive means for detecting driver drowsiness.</p>
</div>
<div class="ltx_para" id="S4.SS2.p9">
<p class="ltx_p" id="S4.SS2.p9.1">Due to the subtlety of microsleeps, these events are usually recorded using EEG or EOG signals from subtle eye muscle movements. Towards that, Lenis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib107" title="">107</a>]</cite> proposed a work investigating MSEs during a car driving simulation using ECG features. In this work, morphological and rhythmical features before and after a MSE are extracted from the ECG signals and analyzed towards baseline. The findings suggested that detecting (or predicting) MSE solely based on the ECG is not feasible. However, when MSE is present, noticeable differences in both the rhythmic and morphological features were observed compared to those calculated for the reference signal in the absence of sleepiness.</p>
</div>
<div class="ltx_para" id="S4.SS2.p10">
<p class="ltx_p" id="S4.SS2.p10.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T2" title="TABLE II ‣ IV-B ECG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">II</span></a> summarizes the investigated works using ECG signals for drowsiness detection in various application scenarios. Sensing modality covers both wired and wireless wearable devices targeting non-intrusive applications. References to the public databases are provided in the footnote if available. From Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T2" title="TABLE II ‣ IV-B ECG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">II</span></a>, we notice that beyond the development of algorithmic choices, i.e., starting from a more heuristic pattern generation to more advanced recurrent and CNN-based methods, the trend also goes to the evaluation of larger groups with more subjects and in the design choices of more flexible and wearable sensors introduce their own individual pros and cons.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>summarizes recent works performing drowsiness detection based on ECG signals.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:199.9pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-305.5pt,140.5pt) scale(0.41513531239335,0.41513531239335) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1" style="font-size:90%;">work</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.2.1" style="font-size:90%;">year</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.3.1" style="font-size:90%;">area of use</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.4.1" style="font-size:90%;">algorithm</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.5.1" style="font-size:90%;">database</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.6.1" style="font-size:90%;">subjects/session</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.7.1" style="font-size:90%;">performance</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.8.1" style="font-size:90%;">remarks</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.2.1.1.1" style="font-size:90%;">Takalokastari et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.2.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib94" title="">94</a><span class="ltx_text" id="S4.T2.1.1.2.1.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.2.1" style="font-size:90%;">2011</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.2.1.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.2.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.2.1.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.4.1.1.1.1" style="font-size:90%;">heart rate variability</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.2.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.2.1.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.4.1.2.1.1" style="font-size:90%;">handcrafted, heuristic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.7.1" style="font-size:90%;">find operation thresholds</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.8.1" style="font-size:90%;">wireless sensor node, wearable ECG</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.3.2.1.1" style="font-size:90%;">Sherbakova et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.3.2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib97" title="">97</a><span class="ltx_text" id="S4.T2.1.1.3.2.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.2.1" style="font-size:90%;">2015</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.3.1" style="font-size:90%;">public transportation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.3.2.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.3.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.3.2.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.4.1.1.1.1" style="font-size:90%;">heart rate variability</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.3.2.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.4.1.2.1.1" style="font-size:90%;">handcrafted, heuristic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.7.1" style="font-size:90%;">find operation thresholds</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.3.2.8.1">
<tr class="ltx_tr" id="S4.T2.1.1.3.2.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.3.2.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.8.1.1.1.1" style="font-size:90%;">portable device for 1 lead ECG,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.3.2.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.8.1.2.1.1" style="font-size:90%;">wireless data transmission</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.4.3.1.1" style="font-size:90%;">Ke et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.4.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib101" title="">101</a><span class="ltx_text" id="S4.T2.1.1.4.3.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.3.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.2.1" style="font-size:90%;">2016</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.4.3.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.4.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.4.3.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.4.1.1.1.1" style="font-size:90%;">HRV, FFT</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.4.3.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.4.1.2.1.1" style="font-size:90%;">handcrafted, heuristic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.4.3.6.1">
<tr class="ltx_tr" id="S4.T2.1.1.4.3.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.4.3.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.6.1.1.1.1" style="font-size:90%;">1 M, 1 F,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.4.3.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.6.1.2.1.1" style="font-size:90%;">120 minutes</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.3.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.7.1" style="font-size:90%;">find operation thresholds</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4.3.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.8.1" style="font-size:90%;">handheld devices, wireless transmission</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.5.4.1.1" style="font-size:90%;">Lenis et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.5.4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib107" title="">107</a><span class="ltx_text" id="S4.T2.1.1.5.4.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.2.1" style="font-size:90%;">2016</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.3.1" style="font-size:90%;">microsleep event</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.5.4.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.5.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.5.4.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.4.1.1.1.1" style="font-size:90%;">ECG features before</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.5.4.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.4.1.2.1.1" style="font-size:90%;">and after MSE</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.5.4.6.1">
<tr class="ltx_tr" id="S4.T2.1.1.5.4.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.5.4.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.6.1.1.1.1" style="font-size:90%;">7 subjects;</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.5.4.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.6.1.2.1.1" style="font-size:90%;">14 records</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.5.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.5.4.8.1">
<tr class="ltx_tr" id="S4.T2.1.1.5.4.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.5.4.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.8.1.1.1.1" style="font-size:90%;">finding significant changes</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.5.4.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.8.1.2.1.1" style="font-size:90%;">in heart rate variability</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.5.4.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.8.1.3.1.1" style="font-size:90%;">around MSE</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.6.5.1.1" style="font-size:90%;">Fujiwara et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.6.5.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib102" title="">102</a><span class="ltx_text" id="S4.T2.1.1.6.5.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.2.1" style="font-size:90%;">2018</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.6.5.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.6.5.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.6.5.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.4.1.1.1.1" style="font-size:90%;">heart rate variability</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.6.5.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.4.1.2.1.1" style="font-size:90%;">handcrafted features</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.5.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.5.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.6.1" style="font-size:90%;">34 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.6.5.7.1">
<tr class="ltx_tr" id="S4.T2.1.1.6.5.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.6.5.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.7.1.1.1.1" style="font-size:90%;">12 out of 13 pre-N1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.6.5.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.7.1.2.1.1" style="font-size:90%;">episodes prior sleep</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.6.5.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.7.1.3.1.1" style="font-size:90%;">onsets detected</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5.7.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.6.5.7.1.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.7.1.4.1.1" style="font-size:90%;">FP=1.7 times per hour</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.6.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.6.5.8.1">
<tr class="ltx_tr" id="S4.T2.1.1.6.5.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.6.5.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.8.1.1.1.1" style="font-size:90%;">data labeled by sleep specialist</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.6.5.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.8.1.2.1.1" style="font-size:90%;">detection of sleep onsets</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.7.6.1.1" style="font-size:90%;">Lee et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.7.6.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib2" title="">2</a><span class="ltx_text" id="S4.T2.1.1.7.6.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.2.1" style="font-size:90%;">2019</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.7.6.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.7.6.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.7.6.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.4.1.1.1.1" style="font-size:90%;">Recurrent methods</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.6.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.7.6.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.4.1.2.1.1" style="font-size:90%;">CNN classification</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.6.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.7.6.6.1">
<tr class="ltx_tr" id="S4.T2.1.1.7.6.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.7.6.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.6.1.1.1.1" style="font-size:90%;">6 subjects,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.6.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.7.6.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.6.1.2.1.1" style="font-size:90%;">22 recordings</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.7.6.7.1">
<tr class="ltx_tr" id="S4.T2.1.1.7.6.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.7.6.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.7.1.1.1.1" style="font-size:90%;">ReLU-RP CNN:</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.6.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.7.6.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.7.1.2.1.1" style="font-size:90%;">ECG Acc = 70%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.6.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.7.6.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.7.1.3.1.1" style="font-size:90%;">PPG Acc = 64%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.6.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.8.1" style="font-size:90%;">wearable ECG + PPG sensors</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.8.7.1.1" style="font-size:90%;">Fahrurrasyid et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.8.7.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib105" title="">105</a><span class="ltx_text" id="S4.T2.1.1.8.7.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.2.1" style="font-size:90%;">2022</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.3.1" style="font-size:90%;">motorcycle driver</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.8.7.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.8.7.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.8.7.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.4.1.1.1.1" style="font-size:90%;">heart rate variability</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8.7.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.8.7.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.4.1.2.1.1" style="font-size:90%;">handcrafted, heuristic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.7.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.7.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.6.1" style="font-size:90%;">10 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.7.1" style="font-size:90%;">find operation thresholds</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.8.7.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.8.1" style="font-size:90%;">a smart helmet equipped with sensors</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.9.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.9.8.1.1" style="font-size:90%;">Heydari et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.9.8.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib106" title="">106</a><span class="ltx_text" id="S4.T2.1.1.9.8.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.9.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.2.1" style="font-size:90%;">2022</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.9.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.9.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.9.8.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.9.8.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.9.8.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.4.1.1.1.1" style="font-size:90%;">pulse rate variability</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9.8.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.9.8.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.4.1.2.1.1" style="font-size:90%;">of a finger from PPG</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.9.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.9.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.6.1" style="font-size:90%;">10 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.9.8.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.7.1" style="font-size:90%;">relevant features selection</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.9.8.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.9.8.8.1">
<tr class="ltx_tr" id="S4.T2.1.1.9.8.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.9.8.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.8.1.1.1.1" style="font-size:90%;">awake/drowsy, heuristic features from time,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9.8.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.9.8.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.8.1.2.1.1" style="font-size:90%;">frequency domain, and nonlinear analysis</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.10.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T2.1.1.10.9.1.1" style="font-size:90%;">Hasan et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.10.9.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib108" title="">108</a><span class="ltx_text" id="S4.T2.1.1.10.9.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.10.9.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.2.1" style="font-size:90%;">2024</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.10.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.10.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.10.9.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.10.9.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.10.9.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.4.1.1.1.1" style="font-size:90%;">explainable ML in</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10.9.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.10.9.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.4.1.2.1.1" style="font-size:90%;">multimodal system</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.10.9.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.10.9.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.6.1" style="font-size:90%;">35 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.10.9.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.10.9.7.1">
<tr class="ltx_tr" id="S4.T2.1.1.10.9.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.10.9.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.7.1.1.1.1" style="font-size:90%;">sensitivity=70.3%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10.9.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.10.9.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.7.1.2.1.1" style="font-size:90%;">specificity=82.2%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10.9.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.10.9.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.7.1.3.1.1" style="font-size:90%;">Acc=80.1%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.1.10.9.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.10.9.8.1">
<tr class="ltx_tr" id="S4.T2.1.1.10.9.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.10.9.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.8.1.1.1.1" style="font-size:90%;">validation techniques for black box model</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10.9.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.1.10.9.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T2.1.1.10.9.8.1.2.1.1" style="font-size:90%;">combining EEG+EOG+ECG signals</span></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Vision-based Drowsiness Detection</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Vision-based drowsiness detection is intended to be non-invasive and non-intrusive. Unlike physiological signals, this method does not require close contact with the subject. It operates remotely and does not necessitate physical attachment or direct interaction with the individual being monitored. Most relevant features for vision-based drowsiness detection are focused on the facial attributes<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib109" title="">109</a>]</cite>, such as eye blinking, eye aspect ratio or facial expressions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib110" title="">110</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib70" title="">70</a>]</cite> such as yawning or mouth opening which indicates the level of drowsiness. In contrast to the works developed based on physiological signals from previous sub-sections, it exists more official databases for the development and evaluation of vision-based drowsiness detection schemes.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Earlier work by Garcia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib4" title="">4</a>]</cite> proposed a vision-based drowsiness detector under real driving conditions. An infrared camera is placed in front to capture the driver’s face and to obtain drowsiness clues from their eyes closure. Three stages of processing include face and eye detection, pupil position detection, and illumination adaptation. Finally, the PERCLOS features are extracted to relate them to the drowsiness state. An outdoor database of several experiments over 25 driving hours was generated as the evaluation dataset. Results of the binary classification for awake and fatigue states showed a specificity, sensitivity, and recall of 92.23%, 79.84%, and 90.68% respectively.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">In this research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib111" title="">111</a>]</cite>, Yu et al. introduced an innovative approach for drowsiness detection, utilizing three main steps for simultaneous representation learning, scene understanding, and feature fusion. They extracted and learned spatio-temporal representations from consecutive frames and employed scene conditional understanding and fusion techniques to enhance the accuracy of drowsiness detection. To evaluate their method’s performance, they tested it on the NTHU-DDD dataset. Results showed a validation accuracy of 88% and an F1-score of 0.712. However, the limitation of the proposed model is its generalizability. Since it is trained on NTHU-DDD dataset, it may not be directly applicable to scenarios that deviate from the trained conditions.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Deng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib3" title="">3</a>]</cite> proposed a system called DriCare which unobtrusively detects the driver’s fatigue status clues, such as yawning, blinking, and duration of eye closure, based on video images. They introduced a face-tracking algorithm to improve the tracking accuracy and designed a new detection scheme for facial regions based on 68 facial landmarks which are leveraged to access the driver’s state. By fusion features of the eyes and mouth, DriCare achieved an accuracy of 92% on the YawDD database.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">To mitigate the problem of changing illumination under real driving conditions in a car, Bakheet et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib109" title="">109</a>]</cite> proposed an improved histogram of oriented gradients (HOG) features combined with a naive Bayesian classification to detect driver drowsiness. The experimental outcomes on the publicly accessible NTHU-DDD dataset demonstrated that the proposed framework has the potential to compete strongly with several SOTA baselines. Results showed an average accuracy of 85.62%. However, the model could have the same shortcomings of missing generalizability.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1">Vijay et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib110" title="">110</a>]</cite> presented a vision-based, two-stage pipeline for real-time driver drowsiness detection using Facial Action Units (FAUs). FAUs can represent facial expression-related movements in facial muscle groups. In the first stage, they employed CNN for detecting FAUs. The second stage utilized an Extreme Gradient Boosting (XGBoost) classifier for drowsiness detection. To model user-specific behavior, individual classifiers were trained. This approach achieved high accuracy in real-time using only a small amount of data and short training time.</p>
</div>
<div class="ltx_para" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1">Liu’s work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib7" title="">7</a>]</cite> focused on the drowsiness detection of crane operators by using deep neural networks leveraging both spatial features and temporal features. They combined the spatial feature extraction with CNN and temporal feature extraction with a Long-short-term-memory (LSTM) network. The authors collected facial videos from licensed crane operators under simulated crane operation scenarios and created a large and public fatigue dataset especially tailored for crane operators. They trained their model on three public vehicle driver datasets, NTHU-DDD, UTA-RLDD, and YawnDD, with human-verified labels at the frame and minute segment levels.</p>
</div>
<div class="ltx_para" id="S4.SS3.p8">
<p class="ltx_p" id="S4.SS3.p8.1">Ahmed et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib70" title="">70</a>]</cite> proposed an approach combining visual features from both eyes and mouth regions extracted from two separate CNNs for driver drowsiness detection. The weights of each stream are further trained on a single-layer perceptron to output the final prediction of drowsiness or non-drowsiness detection. The strength of the ensemble structure is demonstrated over single-stream processing using one of the two face areas. This model is evaluated on the NTHU-DDD video dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib112" title="">112</a>]</cite> with an accuracy of 97.1 % and showed robustness over variations in pose and illumination. More recent multi-stream classification networks combining both spatial and spatio-temporal features is proposed by Pandey et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib113" title="">113</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p9">
<p class="ltx_p" id="S4.SS3.p9.1">Krishna et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib114" title="">114</a>]</cite> proposed to build a driver drowsiness detection framework by fusion object detection and using global attention. Thus, the authors leveraged vision transformers and YoloV5 detectors in their proposed framework. This work aims to capture more complicated driver behaviour features from images compared to current CNN-based methods in this field. The framework is evaluated on the public dataset UTA-RLDD and further validated on a custom dataset of 39 participants collected under various light conditions. On both datasets, the proposed method showcased promising results in terms of high accuracy. The authors claimed the significance of their proposed framework for practical applications in smart transportation systems.</p>
</div>
<div class="ltx_para" id="S4.SS3.p10">
<p class="ltx_p" id="S4.SS3.p10.1">Tamanani et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib115" title="">115</a>]</cite> proposed a new driver’s vigilance detection system based on deep learning methods on facial region diagnosis using the Haar-cascade method and CNN for classification. Evaluation is performed on the UTA-RLDD dataset with five-fold cross-validation. Results showed an accuracy of 96.8% which is higher than most previously reported algorithms. Another customized dataset with 10 subjects under different light conditions was collected to evaluate the generalizability of the proposed method.</p>
</div>
<div class="ltx_para" id="S4.SS3.p11">
<p class="ltx_p" id="S4.SS3.p11.1">In addition to the detection of eye blinking and yawning in a visual image, Khan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib72" title="">72</a>]</cite> further considered another visual feature as an indicator of ’distraction’ by detecting the driver looking sideways for a certain duration of times, i.e. more than 3 seconds. This feature is calculated by determining the Euclidean distances from both ears to nose tip, which builds a triangle, and the difference of both distances is related to side looking. Side looking face will cause an increase in this difference measure compared to frontal view. To evaluate the proposed method, experiments were conducted on a self-collected dataset containing 50 subjects. Results showed a precision, recall, and F1-score of 0.89, 0.98, and 0.93 respectively.</p>
</div>
<div class="ltx_para" id="S4.SS3.p12">
<p class="ltx_p" id="S4.SS3.p12.1">Most vision-based detection methods primarily focus on frontal faces and struggle to handle various head poses encountered in real driving scenarios. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib116" title="">116</a>]</cite> dealt with the challenge by proposing a network to accurately detect driver drowsiness from various viewing angles combining transfer learning and population-based sampling strategy (TLPSN). The population-based sampling strategy is adopted to curate a new training set from data captured in a driver-in-the-loop platform. The results demonstrated that the proposed method has strong robustness to the variation of pose while maintaining high accuracy. In addition, transfer learning significantly improves the generalizability of the model.</p>
</div>
<div class="ltx_para" id="S4.SS3.p13">
<p class="ltx_p" id="S4.SS3.p13.1">Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib117" title="">117</a>]</cite> recently introduced a novel network also designed for detecting driver yawning across arbitrary poses in video. The network comprises three key components: a Geometric-based Key-frame Selection Module (GK-Module), a Face Frontalization with Warp Attention Module (FF-Module), and a dual-channel classifier for Head Pose &amp; Facial Action Fusion Module (HF-Module). Extensive experiments demonstrate that the proposed JHPFA-Net achieves SOTA performance compared to several representative methods on the public YawDD benchmark. Moreover, it exhibits excellent performance in real-time applications.</p>
</div>
<div class="ltx_para" id="S4.SS3.p14">
<p class="ltx_p" id="S4.SS3.p14.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T3" title="TABLE III ‣ IV-C Vision-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">III</span></a> summarizes the works using images or video data for drowsiness detection in various application scenarios. Vision-based drowsiness detection approaches are non-intrusive and remote, focusing mostly on facial clues, eye blink rate, or head positions. References to the public databases used as benchmarks are provided in the footnote. From Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T3" title="TABLE III ‣ IV-C Vision-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">III</span></a>, we noticed that there exist more public databases for the development and evaluation of vision-based drowsiness detection schemes compared to physiological signals. A similar trend from traditional machine learning to deep learning-based methods for drowsiness detection can be observed over time. Sequence models and attention-based vision transformers represent the latest advancements in deep learning-based drowsiness detection schemes.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>summarizes recent works performing drowsiness detection based on vision.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:335.5pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-286.3pt,221.2pt) scale(0.430968698515999,0.430968698515999) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1" style="font-size:90%;">work</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.2.1" style="font-size:90%;">year</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.3.1" style="font-size:90%;">area of use</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.4.1" style="font-size:90%;">algorithm</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.5.1" style="font-size:90%;">database</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.6.1" style="font-size:90%;">subjects/session</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.7.1" style="font-size:90%;">performance</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.1.1.1.1.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.8.1" style="font-size:90%;">remarks</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.2.2.1.1" style="font-size:90%;">Garcia et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.2.2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib4" title="">4</a><span class="ltx_text" id="S4.T3.1.1.2.2.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.2.1" style="font-size:90%;">2012</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.2.2.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.2.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.4.1.1.1.1" style="font-size:90%;">face, eye, pupil detection</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.4.1.2.1.1" style="font-size:90%;">illumination adaptation</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.4.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.4.1.3.1.1" style="font-size:90%;">heuristic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.2.2.6.1">
<tr class="ltx_tr" id="S4.T3.1.1.2.2.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.6.1.1.1.1" style="font-size:90%;">10 subjects, 30 h</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.6.1.2.1.1" style="font-size:90%;">driving, 1296 awake</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2.6.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.6.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.6.1.3.1.1" style="font-size:90%;">min., 504 fatigue min.</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.2.2.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.2.2.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.7.1.1.1.1" style="font-size:90%;">Specificity=92.23%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.7.1.2.1.1" style="font-size:90%;">Sensitivity=79.84%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.2.2.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.7.1.3.1.1" style="font-size:90%;">Recall=90.68%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.2.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.8.1" style="font-size:90%;">under real drive conditions</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.3.3.1.1" style="font-size:90%;">Yu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.3.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib111" title="">111</a><span class="ltx_text" id="S4.T3.1.1.3.3.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.2.1" style="font-size:90%;">2017</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.3.3.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.3.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.3.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.4.1.1.1.1" style="font-size:90%;">representation learning,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.3.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.4.1.2.1.1" style="font-size:90%;">scene understanding,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.3.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.3.4.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.4.1.3.1.1" style="font-size:90%;">feature fusion</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.3.3.5.1" style="font-size:90%;">NTHU-DDD </span><span class="ltx_ERROR undefined" id="S4.T3.1.1.3.3.5.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.3.3.5.3" style="font-size:90%;">fn:nthu</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.6.1" style="font-size:90%;">36 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.3.3.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.3.3.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.3.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.7.1.1.1.1" style="font-size:90%;">validation Acc=88%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.3.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.3.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.7.1.2.1.1" style="font-size:90%;">F1-score = 0.712</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.3.3.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.3.3.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.3.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.8.1.1.1.1" style="font-size:90%;">feature sparsity in fusion model,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.3.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.3.3.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.8.1.2.1.1" style="font-size:90%;">framework may not generalize</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.4.4.1.1" style="font-size:90%;">Deng et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.4.4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib3" title="">3</a><span class="ltx_text" id="S4.T3.1.1.4.4.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.2.1" style="font-size:90%;">2019</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.4.4.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.4.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.4.4.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.4.1.1.1.1" style="font-size:90%;">imroved face tracking,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.4.4.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.4.1.2.1.1" style="font-size:90%;">features from face regions</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.4.4.5.1" style="font-size:90%;">YawDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.4.4.5.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.4.4.5.3" style="font-size:90%;">fn:yawdd</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.6.1" style="font-size:90%;">107 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.7.1" style="font-size:90%;">Acc = 92%</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.4.4.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.4.4.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.4.4.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.8.1.1.1.1" style="font-size:90%;">real-time system,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4.4.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.4.4.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.8.1.2.1.1" style="font-size:90%;">applicable to different circumstances</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.5.5.1.1" style="font-size:90%;">Bakheet et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.5.5.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib109" title="">109</a><span class="ltx_text" id="S4.T3.1.1.5.5.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.5.5.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.5.5.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.5.5.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.4.1.1.1.1" style="font-size:90%;">HOG feature</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5.5.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.5.5.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.4.1.2.1.1" style="font-size:90%;">Naive Bayesian Classifier</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.5.5.5.1" style="font-size:90%;">NTHU-DDD </span><span class="ltx_ERROR undefined" id="S4.T3.1.1.5.5.5.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.5.5.5.3" style="font-size:90%;">fn:nthu</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.6.1" style="font-size:90%;">36 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.5.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.7.1" style="font-size:90%;">Acc = 85.62%</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.5.5.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.5.5.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.5.5.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.8.1.1.1.1" style="font-size:90%;">limitation of generalizability,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5.5.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.5.5.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.8.1.2.1.1" style="font-size:90%;">need more diverse datasets</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.6.6.1.1" style="font-size:90%;">Vijay et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.6.6.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib110" title="">110</a><span class="ltx_text" id="S4.T3.1.1.6.6.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.6.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.6.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.6.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.6.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.6.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.6.6.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.6.6.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.6.6.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.6.4.1.1.1.1" style="font-size:90%;">CNN for Facial Action</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6.6.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.6.6.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.6.4.1.2.1.1" style="font-size:90%;">Units, Extreme Gradient</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6.6.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.6.6.4.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.6.4.1.3.1.1" style="font-size:90%;">Boosting Classifier</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.6.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.6.6.5.1" style="font-size:90%;">NTHU-DDD </span><span class="ltx_ERROR undefined" id="S4.T3.1.1.6.6.5.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.6.6.5.3" style="font-size:90%;">fn:nthu</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.6.6.1" style="font-size:90%;">36 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.6.6.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.6.7.1" style="font-size:90%;">Acc =96%</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.6.6.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.6.8.1" style="font-size:90%;">subject-specific classification</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.7.7.1.1" style="font-size:90%;">Liu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.7.7.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib7" title="">7</a><span class="ltx_text" id="S4.T3.1.1.7.7.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.7.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.7.7.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.7.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.7.7.3.1" style="font-size:90%;">crane operator</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.7.7.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.7.7.4.1" style="font-size:90%;">LSTM + CNN</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.7.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.7.7.5.1">
<tr class="ltx_tr" id="S4.T3.1.1.7.7.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.7.7.5.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.7.7.5.1.1.1.1" style="font-size:90%;">NTHU</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.7.7.5.1.1.1.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.7.7.5.1.1.1.3" style="font-size:90%;">fn:nthu, UTA</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.7.7.5.1.1.1.4">\footref</span><span class="ltx_text" id="S4.T3.1.1.7.7.5.1.1.1.5" style="font-size:90%;">fn:uta,</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7.7.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.7.7.5.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.7.7.5.1.2.1.1" style="font-size:90%;">YawnDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.7.7.5.1.2.1.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.7.7.5.1.2.1.3" style="font-size:90%;">fn:yawdd, Custom</span>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.7.7.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.7.7.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.7.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.7.7.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.7.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.7.7.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.7.7.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.7.7.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.7.7.8.1.1.1.1" style="font-size:90%;">simulated crane operation,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7.7.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.7.7.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.7.7.8.1.2.1.1" style="font-size:90%;">made their database public</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.8.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.8.8.1.1" style="font-size:90%;">Ahmed et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.8.8.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib70" title="">70</a><span class="ltx_text" id="S4.T3.1.1.8.8.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.8.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.8.8.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.8.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.8.8.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.8.8.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.8.8.4.1" style="font-size:90%;">two streams CNNs</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.8.8.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.8.8.5.1" style="font-size:90%;">NTHU-DDD </span><span class="ltx_ERROR undefined" id="S4.T3.1.1.8.8.5.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.8.8.5.3" style="font-size:90%;">fn:nthu</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.8.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.8.8.6.1" style="font-size:90%;">36 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.8.8.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.8.8.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.8.8.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.8.8.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.8.8.7.1.1.1.1" style="font-size:90%;">Evaluation dataset</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8.8.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.8.8.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.8.8.7.1.2.1.1" style="font-size:90%;">Acc=97.1%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.8.8.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.8.8.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.8.8.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.8.8.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.8.8.8.1.1.1.1" style="font-size:90%;">robust over variations</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8.8.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.8.8.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.8.8.8.1.2.1.1" style="font-size:90%;">in pose and illumination</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.9.9.1.1" style="font-size:90%;">Mou et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.9.9.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib118" title="">118</a><span class="ltx_text" id="S4.T3.1.1.9.9.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.9.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.9.9.2.1" style="font-size:90%;">2021</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.9.9.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.9.9.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.9.9.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.9.9.4.1.1.1.1" style="font-size:90%;">IsoSSL-MoCo</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.9.9.5.1">
<tr class="ltx_tr" id="S4.T3.1.1.9.9.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.5.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.9.9.5.1.1.1.1" style="font-size:90%;">NTHU-DDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.9.9.5.1.1.1.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.9.9.5.1.1.1.3" style="font-size:90%;">fn:nthu</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9.9.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.5.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.9.9.5.1.2.1.1" style="font-size:90%;">YawDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.9.9.5.1.2.1.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.9.9.5.1.2.1.3" style="font-size:90%;">fn:yawdd</span>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.9.9.6.1">
<tr class="ltx_tr" id="S4.T3.1.1.9.9.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.9.9.6.1.1.1.1" style="font-size:90%;">36 subjects</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9.9.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.9.9.6.1.2.1.1" style="font-size:90%;">107 subjects</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.9.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.9.9.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.9.9.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.9.9.7.1.1.1.1" style="font-size:90%;">Acc=93.71%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9.9.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.9.9.7.1.2.1.1" style="font-size:90%;">Acc=98.65%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.9.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.9.9.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.9.9.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.9.9.8.1.1.1.1" style="font-size:90%;">pretrain on MRL dataset</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.9.9.8.1.1.1.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.9.9.8.1.1.1.3" style="font-size:90%;">fn:mrl + NTHU-DDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.9.9.8.1.1.1.4">\footref</span><span class="ltx_text" id="S4.T3.1.1.9.9.8.1.1.1.5" style="font-size:90%;">fn:nthu</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9.9.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.9.9.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.9.9.8.1.2.1.1" style="font-size:90%;">leveraging self-supervised learning</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.10.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.10.10.1.1" style="font-size:90%;">Krishna et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.10.10.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib114" title="">114</a><span class="ltx_text" id="S4.T3.1.1.10.10.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.10.10.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.2.1" style="font-size:90%;">2022</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.10.10.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.3.1" style="font-size:90%;">smart transportation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.10.10.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.4.1" style="font-size:90%;">vision transformers + Yolov5</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.10.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.10.10.5.1">
<tr class="ltx_tr" id="S4.T3.1.1.10.10.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.5.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.10.10.5.1.1.1.1" style="font-size:90%;">UTA-RLDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.10.10.5.1.1.1.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.10.10.5.1.1.1.3" style="font-size:90%;">fn:uta</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10.10.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.5.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.5.1.2.1.1" style="font-size:90%;">Custom</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.10.10.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.10.10.6.1">
<tr class="ltx_tr" id="S4.T3.1.1.10.10.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.6.1.1.1.1" style="font-size:90%;">60 subjects,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10.10.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.6.1.2.1.1" style="font-size:90%;">39 subjects</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.10.10.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.10.10.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.10.10.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.7.1.1.1.1" style="font-size:90%;">train Acc = 96.2%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10.10.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.7.1.2.1.1" style="font-size:90%;">valid Acc = 97.4%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10.10.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.7.1.3.1.1" style="font-size:90%;">custom Acc = 95.5%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.10.10.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.10.10.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.10.10.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.8.1.1.1.1" style="font-size:90%;">attention-based model + object detection</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10.10.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.10.10.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.10.10.8.1.2.1.1" style="font-size:90%;">vigilent/drowsy detection</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.11.11.1.1" style="font-size:90%;">Chen et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.11.11.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib116" title="">116</a><span class="ltx_text" id="S4.T3.1.1.11.11.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.11.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.2.1" style="font-size:90%;">2022</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.11.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.11.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.11.11.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.11.11.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.11.11.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.4.1.1.1.1" style="font-size:90%;">multiple viewing angle</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11.11.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.11.11.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.4.1.2.1.1" style="font-size:90%;">sampling strategy for</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11.11.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.11.11.4.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.4.1.3.1.1" style="font-size:90%;">data augmentation</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.11.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.6.1" style="font-size:90%;">5 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.11.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.11.11.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.11.11.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.11.11.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.7.1.1.1.1" style="font-size:90%;">Acc = 97.5%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11.11.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.11.11.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.7.1.2.1.1" style="font-size:90%;">F1 = 97.5%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.11.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.11.11.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.11.11.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.11.11.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.8.1.1.1.1" style="font-size:90%;">simulated driving,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11.11.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.11.11.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.8.1.2.1.1" style="font-size:90%;">sleep deprivated subjects</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11.11.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.11.11.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.11.11.8.1.3.1.1" style="font-size:90%;">small-scale dataset</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.12.12.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.12.12.1.1" style="font-size:90%;">Tamanani et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.12.12.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib115" title="">115</a><span class="ltx_text" id="S4.T3.1.1.12.12.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.12.12.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.2.1" style="font-size:90%;">2023</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.12.12.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.3.1" style="font-size:90%;">public transportation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.12.12.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.12.12.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.12.12.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.4.1.1.1.1" style="font-size:90%;">facial region diagnosis</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.4.1.2.1.1" style="font-size:90%;">(with Haar-cascade),</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.4.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.4.1.3.1.1" style="font-size:90%;">CNN classification</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.12.12.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.12.12.5.1">
<tr class="ltx_tr" id="S4.T3.1.1.12.12.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.5.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.12.12.5.1.1.1.1" style="font-size:90%;">UTA-RLDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.12.12.5.1.1.1.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.12.12.5.1.1.1.3" style="font-size:90%;">fn:uta ,</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.5.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.5.1.2.1.1" style="font-size:90%;">private</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.12.12.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.12.12.6.1">
<tr class="ltx_tr" id="S4.T3.1.1.12.12.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.6.1.1.1.1" style="font-size:90%;">60 subjects,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.6.1.2.1.1" style="font-size:90%;">10 subjects</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.12.12.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.12.12.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.12.12.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.7.1.1.1.1" style="font-size:90%;">avg Acc = 0.918</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.7.1.2.1.1" style="font-size:90%;">precision = 0.928</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.7.1.3.1.1" style="font-size:90%;">recall = 0.920</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12.7.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.7.1.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.7.1.4.1.1" style="font-size:90%;">F1-score = 0.920</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.12.12.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.12.12.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.12.12.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.8.1.1.1.1" style="font-size:90%;">authors evaluated their method</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.12.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.12.12.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.12.12.8.1.2.1.1" style="font-size:90%;">on a customized dataset</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.13.13.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.13.13.1.1" style="font-size:90%;">Khan et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.13.13.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib72" title="">72</a><span class="ltx_text" id="S4.T3.1.1.13.13.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.13.13.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.2.1" style="font-size:90%;">2023</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.13.13.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.3.1" style="font-size:90%;">public transportation</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.13.13.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.13.13.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.13.13.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.4.1.1.1.1" style="font-size:90%;">handcrafted features,</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13.13.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.4.1.2.1.1" style="font-size:90%;">heuristics</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.13.13.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.5.1" style="font-size:90%;">private</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.13.13.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.13.13.6.1">
<tr class="ltx_tr" id="S4.T3.1.1.13.13.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.6.1.1.1.1" style="font-size:90%;">50 subjects</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13.13.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.6.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.6.1.2.1.1" style="font-size:90%;">(33 M, 17 F)</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.13.13.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.13.13.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.13.13.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.7.1.1.1.1" style="font-size:90%;">Precision = 0.89</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13.13.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.7.1.2.1.1" style="font-size:90%;">Recall = 0.98</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13.13.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.7.1.3.1.1" style="font-size:90%;">F1-score =0.93</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.13.13.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.13.13.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.13.13.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.8.1.1.1.1" style="font-size:90%;">IoT-based Non-Intrusive</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13.13.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.13.13.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.13.13.8.1.2.1.1" style="font-size:90%;">to enhance Road Safety</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.14.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.14.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.14.14.1.1" style="font-size:90%;">Pandey et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.14.14.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib113" title="">113</a><span class="ltx_text" id="S4.T3.1.1.14.14.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.14.14.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.2.1" style="font-size:90%;">2023</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.14.14.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.14.14.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.14.14.4.1">
<tr class="ltx_tr" id="S4.T3.1.1.14.14.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.14.14.4.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.4.1.1.1.1" style="font-size:90%;">mutlistream classifcation</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.14.14.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.14.14.4.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.4.1.2.1.1" style="font-size:90%;">YoLov3 + LSTM</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.14.14.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.14.14.5.1" style="font-size:90%;">UTA-RLDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.14.14.5.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.14.14.5.3" style="font-size:90%;">fn:uta</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.14.14.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.6.1" style="font-size:90%;">60 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.14.14.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.14.14.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.14.14.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.14.14.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.7.1.1.1.1" style="font-size:90%;">Acc=97.5%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.14.14.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.14.14.8.1">
<tr class="ltx_tr" id="S4.T3.1.1.14.14.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.14.14.8.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.8.1.1.1.1" style="font-size:90%;">Spatio-temporal feature;</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.14.14.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.14.14.8.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.8.1.2.1.1" style="font-size:90%;">TransGAN; YOLOv3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.14.14.8.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.14.14.8.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.14.14.8.1.3.1.1" style="font-size:90%;">Temporal feature; LSTM</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.15.15">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.1.1.15.15.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.15.15.1.1" style="font-size:90%;">Lu et al </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.1.1.15.15.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib117" title="">117</a><span class="ltx_text" id="S4.T3.1.1.15.15.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.1.1.15.15.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.15.15.2.1" style="font-size:90%;">2023</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.1.1.15.15.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.15.15.3.1" style="font-size:90%;">driver drowsiness</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.1.1.15.15.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.15.15.4.1" style="font-size:90%;">DL with 3 modules</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.1.1.15.15.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.15.15.5.1" style="font-size:90%;">YawDD</span><span class="ltx_ERROR undefined" id="S4.T3.1.1.15.15.5.2">\footref</span><span class="ltx_text" id="S4.T3.1.1.15.15.5.3" style="font-size:90%;">fn:yawdd</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.1.1.15.15.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.15.15.6.1" style="font-size:90%;">107 subjects</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.1.1.15.15.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.15.15.7.1">
<tr class="ltx_tr" id="S4.T3.1.1.15.15.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.15.15.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.15.15.7.1.1.1.1" style="font-size:90%;">Acc=86.7%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.15.15.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.15.15.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.15.15.7.1.2.1.1" style="font-size:90%;">Precision = 91%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.15.15.7.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.15.15.7.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.15.15.7.1.3.1.1" style="font-size:90%;">F1-score =88.8%</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.1.1.15.15.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.1.1.15.15.8.1" style="font-size:90%;">targets arbitrary head poses</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Widely Used Databases</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section summarizes popular databases used for drowsiness detection based on visual, mutli-channel EEG, and ECG signals. This discussion stressed on available databases and put less focus on small-scale private databases. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S5.T4" title="TABLE IV ‣ V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">IV</span></a> contains an overview of these publicly available databases for drowsiness detection research. A link to the individual database is given in the footnote for easier access. In Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S5.T4" title="TABLE IV ‣ V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">IV</span></a>, we target this tabular representation from various aspects, including the database name, year of publication (chronologically ordered), detection modality, number of subjects/sessions included, its label annotation, application area, and conclude with specific remarks. The majority of the cited databases here are used for driver drowsiness detection but can be extended to general drowsiness detection tasks because of the diversity of the capture environments.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The University of Texas at Arlington created the Real-Life Drowsiness Dataset<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>UTA-RLDD: https://sites.google.com/view/utarldd/home</span></span></span> (UTA-RLDD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib119" title="">119</a>]</cite> targeting the task of multi-stage drowsiness detection. The dataset contains both easily visible cases and subtle cases where the drowsiness level is at an early stage and the detection is strongly related to subtle micro-expressions. The creators claimed that the UTA-RLDD dataset is the largest to date realistic drowsiness dataset. It consists of around 30 hours of RGB videos of 60 healthy participants, where each participant collected three different classes including alertness, low vigilance, and drowsiness. It contains variations in gender, ethnicity, age, acquisition device, and also accessories like glasses, facial hair and different viewing angles in different real-life environments and backgrounds.
The position of the acquisition device is placed such that both eyes are visible and the device is within one arm length away from the subject. This database is often used to evaluate algorithms for driver’s drowsiness detection with vision-based approaches<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib115" title="">115</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The academic NTHU-DDD dataset<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>NTHU-DDD: http://cv.cs.nthu.edu.tw/php/callforpaper/datasets/DDD/</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib112" title="">112</a>]</cite> was collected by NTHU Computer Vision Lab at National Tsing Hua University. This is another video dataset for detecting driver’s drowsiness. They used active infrared illumination to alleviate the poor illumination issue. All videos were captured by a stand-alone surveillance camera D-Link DCS-932L with a resolution of 640x480 pixels. The capture device is placed on the top left to emulate the position in the car without blocking the driver’s view. Each subject recorded two different sessions including day-time and night-time sessions. Subjects are asked to perform actions indicating different drowsiness levels while playing a plain driving game, such as normal driving, slow blink rate, yawning, falling asleep and bursting out laughing. The dataset includes variations in illumination, scenarios, skin color, gender, age and also differences in hairstyles, clothing and glasses/sunglasses to cover the most realistic driving scenarios. Compared to UTA-RLDD, although this database was also recorded under indoor settings, its applications are much more limited, because the use-case collected here is more controlled.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The YawDD dataset<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>YawDD: https://traces.cs.umass.edu/index.php/Mmsys/Mmsys</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib120" title="">120</a>]</cite> is recorded by an in-car camera capturing the driver’s facial characteristics in an actual car during talking, singing, being silent, and yawning. It contains two different camera installations: (1) under the front mirror and (2) on the driver’s dashboard. In total 322 RGB videos were recorded with large variations in illumination, gender, ethnicity, and with and without glasses/sunglasses. Additional 29 videos, one for each subject, are added under second setup containing all sequences of performed actions. Compared to UTA-RLDD or NTHU-DDD this database is more realistic in the data acquisition, as the videos are collected in a real vehicle with individuals sitting behind the wheel. This allows the design of drowsiness detection algorithm under more realistic scenarios. The challenge in this database accounts for the placement of the camera under the mirror. Under this viewing angle face detection is much harder.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Media Research Lab from the Technical University of Ostrava proposed the MRL Eye Dataset<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Project page MRL Eye Dataset: http://mrl.cs.vsb.cz/eyedataset</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib121" title="">121</a>]</cite> which could be used to detect eye blinking and relate this to drowsiness detection by combining eye blinking rate to drowsiness levels. This database was not originally designed for drowsiness detection. But it can be leveraged to detect visual features of the human eyes related to drowsiness. It is a large-scale dataset of human eye images and consists of 84,898 images from 37 individuals, while 33 male and 4 female subjects are included. This dataset further modulated variations in capture devices, capture spectra (RGB + IR), lightning conditions, image resolutions and eye openness levels. Unlike previous databases, this database restricts the area of faces to only the eye regions making the detection of subtle facial micro-expressions impossible.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">The FatigueView dataset<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Project page FatigueView Dataset: https://fatigueview.github.io/#/</span></span></span>, introduced by Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib122" title="">122</a>]</cite>, is a large-scaled dataset of videos designed for driver’s drowsiness detection. This dataset boasts practicality, diversity, and a wide range of environments. It encompasses images captured by both RGB and infrared cameras, positioned in five different angles. The dataset includes genuine instances of drowsy driving and displays various visual indications of different drowsiness levels. Notably, the dataset features a substantial 17,403 instances of yawning, significantly surpassing current widely used datasets. The authors evaluated the dataset using SOTA algorithms, therefore establishing numerous baseline results that can guide future algorithm advancements. Unlike the NTHU-DDD, this database is collected in a single office environment by sitting the participants on an upholstered office chair. While the acquisition environment is not diverse enough compared to NTHU-DDD or UTA-RLDD, it covers a much broader viewing angle.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">The National Cheng Kung University Driver Drowsiness Dataset (NCKUDD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib123" title="">123</a>]</cite> consists of videos from a total of 25 participants captured during a normal driving condition. The camera is placed in front of the driver to unobtrusively capture the driver’s facial expression. Recordings contain both daylight and dark environments and include normal, sleepy, distracted, talking/eating scenarios while driving, talking on the phone while driving, and other abnormal driving patterns. Compared to other vision-based databases so far, this database may contain footage of participants maneuvering the car for real. While this fact might be considered advantages, it may include situations that endanger the driver’s safety.</p>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p" id="S5.p8.1">A novel database is introduced by Martin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib124" title="">124</a>]</cite> called the Driver&amp;Act dataset<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Project page Drive&amp;Act: https://driveandact.com/</span></span></span>, uniquely designated for fine-grained recognition of driver behavior in autonomous vehicles. This comprehensive dataset encompasses more than 9.6 million frames spanning 12 hours, capturing individuals engaged in distracting actions during both manual and automated driving scenarios. The dataset utilizes a capture device that integrates RGB, IR, depth, and 3D body pose data from six different perspectives. Among its objectives, this database particularly aims to excel in the identification of intricate actions and features a multi-modal approach to activity recognition. As opposed to other drowsiness detection databases, this database specifically target the use-case of autonomous driving and was not originally addressed for drowsiness detection only.</p>
</div>
<div class="ltx_para" id="S5.p9">
<p class="ltx_p" id="S5.p9.1">Ortega et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib125" title="">125</a>]</cite> proposed DMD, which is another large-scale multi-modal driver monitoring dataset<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Project page DMD dataset: https://dmd.vicomtech.org/</span></span></span> for attention and alertness analysis. This is an extensive dataset with 37 drivers which includes real and simulated driving scenarios. Targeted tasks contain levels of distraction, gaze allocation, drowsiness detection, hands-wheel interaction and context data. It contains in total 41 hours of RGB, depth and infrared videos from 3 cameras capturing face, body, and hands. Compared to other existing similar datasets, the authors motivated their proposed database to be more extensive, diverse, and multi-purpose. Unlike UTA-RLDD, NTHU-DDD or YawDD, this database does not consider drowsiness as the main focus of distraction and considers much more diverse activities causing the drivers to shift their attention from the road, such as talking on the phone, playing with the car interface, or typing into a car navigation system.</p>
</div>
<div class="ltx_para" id="S5.p10">
<p class="ltx_p" id="S5.p10.1">Recently, Yilmaz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib126" title="">126</a>]</cite> introduced a novel dataset, termed SUST-DDD<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>SUST-DDD: https://www.kaggle.com/datasets/esrakavalci/sust-ddd</span></span></span>, aimed at driver drowsiness detection. This dataset is meticulously curated and benchmarked using a range of DL techniques to effectively predict driver drowsiness. The dataset compilation involved 19 participants who were instructed to record themselves while driving using their personal cell phones positioned in front of the driver’s seat. To closely replicate real-world driving scenarios, participants were required to operate their own vehicles and utilize their individual phones. Notably, this dataset encapsulates genuine driving situations encompassing diverse lighting conditions, distinct video sizes, and varying resolutions due to each participant’s unique phone specifications. This database is most similar to YawDD and NCKUDD, however with the difference that both YawDD and SUST-DDD use user-specific cell phones, while only SUST-DDD includes real driving scenarios. The difference between NCKUDD and SUST-DDD is that NCKUDD uses a camera installed in front of the driver, while in SUST-DDD custom cell phones are used as recording devices within custom vehicles to simulate a more natural driving experience.</p>
</div>
<div class="ltx_para" id="S5.p11">
<p class="ltx_p" id="S5.p11.1">The ULg Multimodality Drowsiness Database<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Project page DROZY: http://www.drozy.ulg.ac.be/</span></span></span> (DROZY) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib77" title="">77</a>]</cite> is a database which is not only based on vision capture to detect drowsiness. Here, a total of 14 subjects (3 males, 11 females) have participated in the data collection process. Experiments were conducted in a quite and isolated laboratory environment. Participants have no sleeping disorders and a sleep diary was kept individually. Tea and coffee were avoided during the acquisition process to keep the subjects really drowsy instead of faking it. In this database, four different electrical bio-signals such as electroencephalogram (EEG), electroculography (EOG), electrocardiogram (EKG) and electromyogram (EMG) were acquired. Similar to UTA-RLDD, this database was also not specifically designated to target driver’s drowsiness detection but includes much more sensory inputs especially measuring physiological entities compared to UTA-RLDD.</p>
</div>
<div class="ltx_para" id="S5.p12">
<p class="ltx_p" id="S5.p12.1">Last but not least, another EEG-based database is proposed by Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib92" title="">92</a>]</cite> in Multichannel EEG<span class="ltx_ERROR undefined" id="S5.p12.1.1">\footref</span>fn:eeg. They collected 32-channel EEG data during a sustained-attention driving task for drivers’ drowsiness detection. In Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S5.T4" title="TABLE IV ‣ V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">IV</span></a>, we referred this database to the Multi-channel EEG recordings. It consisted of more than 62 sessions for 27 subjects. Each session includes a 90-minute sustained-attention task in which an immersive driving scenario is simulated. The participants were asked to drive on a four line highway and keep the car in the center of the lane. Random car drifts are induced expecting the participants to respond accordingly. The driver’s drowsiness is inferred from the required response time in this lane-keeping task. This dataset is specifically collected for targeting the driver’s drowsiness detection task.</p>
</div>
<div class="ltx_para" id="S5.p13">
<p class="ltx_p" id="S5.p13.1">In this section, we have seen a wide range of databases that motivate research on drowsiness detection. Both visual and biosignal-based databases are investigated here. While most databases, such as NTHU-DDD, YawDD, FatigueView, NCKUDD, Drive &amp; Act, DMD, and SUST-DDD, are intended for driver drowsiness detection, other databases, such as UTA-RLDD, MRL Eye Dataset, and ULg Dataset, are targeting the more general drowsiness detection tasks. Especially the UTA-RLDD database is collected under unconstrained real-world indoor scenarios, which can be used to develop drowsiness detection applications more general for smart environments. But also NHU-DDD collected under indoor scenarios while sitting can be extended to build general drowsiness detection applications for smart environments as well.</p>
</div>
<div class="ltx_table ltx_transformed_outer" id="S5.T4" style="width:13297.2pt;height:450.3pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:450.3pt;transform:translate(6423.46pt,6424.46pt) rotate(-90deg) ;"><figure>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>summarizes databases used for drowsiness detection. Most of them are used for driver drowsiness detection but can be easily extended to general drowsiness detection framework for smart environments, as they are collected under various indoor scenarios.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T4.1" style="width:433.6pt;height:13296.2pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.5pt,46.0pt) scale(0.993129036614105,0.993129036614105) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.1.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.1.1" style="font-size:90%;">database</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.2.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.2.1.1.1" style="font-size:90%;">year</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.3.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.3.1.1.1" style="font-size:90%;">measrung technique</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.4.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.4.1.1.1" style="font-size:90%;">attributes</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.5.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.5.1.1.1" style="font-size:90%;">sensing modality</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.6.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.6.1.1.1" style="font-size:90%;">area of application</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.7.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.7.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.7.1.1.1" style="font-size:90%;">subjects</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.8.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.8.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.8.1.1.1" style="font-size:90%;">sessions</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.9.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.9.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.9.1.1.1" style="font-size:90%;">classes</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.10.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.10.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.10.1.1.1" style="font-size:90%;">specific remarks</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.2.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.1.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.1.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.1.1.1.1" style="font-size:90%;">YawDD</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.2.1.1.1.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.2.1.1.1.1.3" style="font-size:90%;">fn:yawdd </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.2.1.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib120" title="">120</a><span class="ltx_text" id="S5.T4.1.1.2.1.1.1.1.5.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.2.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.2.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.2.1.1.1" style="font-size:90%;">2014</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.3.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.3.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.2.1.4.1">
<tr class="ltx_tr" id="S5.T4.1.1.2.1.4.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.2.1.4.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.2.1.4.1.1.1.1" style="font-size:90%;">yawning,</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.2.1.4.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.2.1.4.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.2.1.4.1.2.1.1" style="font-size:90%;">open mouth</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.5.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.5.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.6.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.6.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.6.1.1.1" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.7.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.7.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.7.1.1.1" style="font-size:90%;">29</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.8.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.8.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.8.1.1.1" style="font-size:90%;">342 RGB videos</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.9.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.9.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.9.1.1.1" style="font-size:90%;">normal, talking/singing, yawing, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.2.1.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib112" title="">112</a><span class="ltx_text" id="S5.T4.1.1.2.1.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.2.1.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.2.1.10.1">
<span class="ltx_p" id="S5.T4.1.1.2.1.10.1.1"><span class="ltx_text" id="S5.T4.1.1.2.1.10.1.1.1" style="font-size:90%;">recorded in an actual parked car</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.3.2.1.1">
<span class="ltx_p" id="S5.T4.1.1.3.2.1.1.1"><span class="ltx_text" id="S5.T4.1.1.3.2.1.1.1.1" style="font-size:90%;">NTHU-DDD</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.3.2.1.1.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.3.2.1.1.1.3" style="font-size:90%;">fn:nthu </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.3.2.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib112" title="">112</a><span class="ltx_text" id="S5.T4.1.1.3.2.1.1.1.5.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.3.2.2.1">
<span class="ltx_p" id="S5.T4.1.1.3.2.2.1.1"><span class="ltx_text" id="S5.T4.1.1.3.2.2.1.1.1" style="font-size:90%;">2017</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.3.2.3.1">
<span class="ltx_p" id="S5.T4.1.1.3.2.3.1.1"><span class="ltx_text" id="S5.T4.1.1.3.2.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.3.2.4.1">
<tr class="ltx_tr" id="S5.T4.1.1.3.2.4.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.2.4.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.3.2.4.1.1.1.1" style="font-size:90%;">head pose,</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.2.4.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.2.4.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.3.2.4.1.2.1.1" style="font-size:90%;">yawning,</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.2.4.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.2.4.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.3.2.4.1.3.1.1" style="font-size:90%;">eye closure</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.3.2.5.1">
<span class="ltx_p" id="S5.T4.1.1.3.2.5.1.1"><span class="ltx_text" id="S5.T4.1.1.3.2.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.3.2.6.1">
<tr class="ltx_tr" id="S5.T4.1.1.3.2.6.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.2.6.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.3.2.6.1.1.1.1" style="font-size:90%;">driver drowsiness,</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.2.6.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.2.6.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.3.2.6.1.2.1.1" style="font-size:90%;">while sitting on a</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.2.6.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.2.6.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.3.2.6.1.3.1.1" style="font-size:90%;">chair indoor</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.3.2.7.1">
<span class="ltx_p" id="S5.T4.1.1.3.2.7.1.1"><span class="ltx_text" id="S5.T4.1.1.3.2.7.1.1.1" style="font-size:90%;">22</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.3.2.8.1">
<span class="ltx_p" id="S5.T4.1.1.3.2.8.1.1"><span class="ltx_text" id="S5.T4.1.1.3.2.8.1.1.1" style="font-size:90%;">360 RGB videos</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.3.2.9.1">
<span class="ltx_p" id="S5.T4.1.1.3.2.9.1.1"><span class="ltx_text" id="S5.T4.1.1.3.2.9.1.1.1" style="font-size:90%;">drowsiness and non-drowsiness, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.3.2.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib110" title="">110</a><span class="ltx_text" id="S5.T4.1.1.3.2.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.3.2.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.3.2.10.1">
<span class="ltx_p" id="S5.T4.1.1.3.2.10.1.1"><span class="ltx_text" id="S5.T4.1.1.3.2.10.1.1.1" style="font-size:90%;">controlled indoor scenarios, under simulations, contains variations for most realistic driving scenarios</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.4.3.1.1">
<span class="ltx_p" id="S5.T4.1.1.4.3.1.1.1"><span class="ltx_text" id="S5.T4.1.1.4.3.1.1.1.1" style="font-size:90%;">MRL Eye Dataset</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.4.3.1.1.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.4.3.1.1.1.3" style="font-size:90%;">fn:mrl </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.4.3.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib121" title="">121</a><span class="ltx_text" id="S5.T4.1.1.4.3.1.1.1.5.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.4.3.2.1">
<span class="ltx_p" id="S5.T4.1.1.4.3.2.1.1"><span class="ltx_text" id="S5.T4.1.1.4.3.2.1.1.1" style="font-size:90%;">2018</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.4.3.3.1">
<span class="ltx_p" id="S5.T4.1.1.4.3.3.1.1"><span class="ltx_text" id="S5.T4.1.1.4.3.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.4.3.4.1">
<tr class="ltx_tr" id="S5.T4.1.1.4.3.4.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.4.3.4.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.4.3.4.1.1.1.1" style="font-size:90%;">eye blink,</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.3.4.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.4.3.4.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.4.3.4.1.2.1.1" style="font-size:90%;">eye openess</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.4.3.5.1">
<span class="ltx_p" id="S5.T4.1.1.4.3.5.1.1"><span class="ltx_text" id="S5.T4.1.1.4.3.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.4.3.6.1">
<span class="ltx_p" id="S5.T4.1.1.4.3.6.1.1"><span class="ltx_text" id="S5.T4.1.1.4.3.6.1.1.1" style="font-size:90%;">general drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.4.3.7.1">
<span class="ltx_p" id="S5.T4.1.1.4.3.7.1.1"><span class="ltx_text" id="S5.T4.1.1.4.3.7.1.1.1" style="font-size:90%;">37</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.4.3.8.1">
<tr class="ltx_tr" id="S5.T4.1.1.4.3.8.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.4.3.8.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.4.3.8.1.1.1.1" style="font-size:90%;">85000 different</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.3.8.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.4.3.8.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.4.3.8.1.2.1.1" style="font-size:90%;">eye regions</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.4.3.9.1">
<tr class="ltx_tr" id="S5.T4.1.1.4.3.9.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.4.3.9.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.4.3.9.1.1.1.1" style="font-size:90%;">driver drowsiness,</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.3.9.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.4.3.9.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.4.3.9.1.2.1.1" style="font-size:90%;">gaze direction,</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.3.9.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.4.3.9.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S5.T4.1.1.4.3.9.1.3.1.1" style="font-size:90%;">used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.4.3.9.1.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib127" title="">127</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib128" title="">128</a><span class="ltx_text" id="S5.T4.1.1.4.3.9.1.3.1.3.2" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.4.3.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.4.3.10.1">
<span class="ltx_p" id="S5.T4.1.1.4.3.10.1.1"><span class="ltx_text" id="S5.T4.1.1.4.3.10.1.1.1" style="font-size:90%;">contains only eye regions not faces, impossible to detect facial expressions</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.1.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.1.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.1.1.1.1" style="font-size:90%;">UTA-RLDD</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.5.4.1.1.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.5.4.1.1.1.3" style="font-size:90%;">fn:uta </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.5.4.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib119" title="">119</a><span class="ltx_text" id="S5.T4.1.1.5.4.1.1.1.5.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.2.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.2.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.2.1.1.1" style="font-size:90%;">2019</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.3.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.3.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.4.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.4.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.4.1.1.1" style="font-size:90%;">facial expressions extreme to subtle</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.5.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.5.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.5.4.6.1">
<tr class="ltx_tr" id="S5.T4.1.1.5.4.6.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.5.4.6.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.5.4.6.1.1.1.1" style="font-size:90%;">general drowsiness</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5.4.6.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.5.4.6.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.5.4.6.1.2.1.1" style="font-size:90%;">driver drowsiness</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.7.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.7.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.7.1.1.1" style="font-size:90%;">60</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.8.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.8.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.8.1.1.1" style="font-size:90%;">180 RGB videos</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.9.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.9.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.9.1.1.1" style="font-size:90%;">alert, low vigilance, drowsiness, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.5.4.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib115" title="">115</a><span class="ltx_text" id="S5.T4.1.1.5.4.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.5.4.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.5.4.10.1">
<span class="ltx_p" id="S5.T4.1.1.5.4.10.1.1"><span class="ltx_text" id="S5.T4.1.1.5.4.10.1.1.1" style="font-size:90%;">contains variations in gender, ethnicity, age, acquisition device and accessories, can be used for general drowsiness detection</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.6.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.6.5.1.1">
<tr class="ltx_tr" id="S5.T4.1.1.6.5.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.6.5.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.6.5.1.1.1.1.1" style="font-size:90%;">NCK-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.6.5.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.6.5.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S5.T4.1.1.6.5.1.1.2.1.1" style="font-size:90%;">UDD </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.6.5.1.1.2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib123" title="">123</a><span class="ltx_text" id="S5.T4.1.1.6.5.1.1.2.1.3.2" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.2.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.2.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.2.1.1.1" style="font-size:90%;">2019</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.3.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.3.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.4.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.4.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.4.1.1.1" style="font-size:90%;">facial expression, eye openess, mouth open</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.5.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.5.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.6.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.6.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.6.1.1.1" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.7.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.7.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.7.1.1.1" style="font-size:90%;">25</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.8.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.8.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.8.1.1.1" style="font-size:90%;">643 total events</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.9.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.9.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.9.1.1.1" style="font-size:90%;">normal, sleepy, distracted, and various other activities while driving, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.6.5.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib129" title="">129</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib130" title="">130</a><span class="ltx_text" id="S5.T4.1.1.6.5.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.6.5.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.6.5.10.1">
<span class="ltx_p" id="S5.T4.1.1.6.5.10.1.1"><span class="ltx_text" id="S5.T4.1.1.6.5.10.1.1.1" style="font-size:90%;">contains real driving conditions possibly endangering the drivers</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.7.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.7.6.1.1">
<tr class="ltx_tr" id="S5.T4.1.1.7.6.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.7.6.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.7.6.1.1.1.1.1" style="font-size:90%;">Fatigue-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.7.6.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.7.6.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S5.T4.1.1.7.6.1.1.2.1.1" style="font-size:90%;">View</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.7.6.1.1.2.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.7.6.1.1.2.1.3" style="font-size:90%;">fn:fatigue </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.7.6.1.1.2.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib122" title="">122</a><span class="ltx_text" id="S5.T4.1.1.7.6.1.1.2.1.5.2" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.2.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.2.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.2.1.1.1" style="font-size:90%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.3.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.3.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.4.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.4.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.4.1.1.1" style="font-size:90%;">head pose, eye openess, mouth closed</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.5.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.5.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.6.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.6.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.6.1.1.1" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.7.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.7.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.7.1.1.1" style="font-size:90%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.8.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.8.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.8.1.1.1" style="font-size:90%;">17403 yawning sets</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.9.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.9.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.9.1.1.1" style="font-size:90%;">drowsy, normal, nodding, and stretching, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.7.6.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib131" title="">131</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib132" title="">132</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib133" title="">133</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib134" title="">134</a><span class="ltx_text" id="S5.T4.1.1.7.6.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.7.6.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.7.6.10.1">
<span class="ltx_p" id="S5.T4.1.1.7.6.10.1.1"><span class="ltx_text" id="S5.T4.1.1.7.6.10.1.1.1" style="font-size:90%;">office environment indoor, sitting on office chairs, simulation</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.8.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.1.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.1.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.1.1.1.1" style="font-size:90%;">Drive &amp; Act</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.8.7.1.1.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.8.7.1.1.1.3" style="font-size:90%;">fn:driveAct </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.8.7.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib124" title="">124</a><span class="ltx_text" id="S5.T4.1.1.8.7.1.1.1.5.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.2.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.2.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.2.1.1.1" style="font-size:90%;">2020</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.3.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.3.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.4.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.4.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.4.1.1.1" style="font-size:90%;">facial expression, head pose, eye open, mouth open</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.5.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.5.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.6.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.6.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.6.1.1.1" style="font-size:90%;">autonomous driving, includes driver drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.7.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.7.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.7.1.1.1" style="font-size:90%;">15</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.8.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.8.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.8.1.1.1" style="font-size:90%;">12 hours of RGB, depth, IR videos, 3D body pose from 6 different views</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.9.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.9.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.9.1.1.1" style="font-size:90%;">diverse distracting activities during both manual and automated driving conditions, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.8.7.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib135" title="">135</a><span class="ltx_text" id="S5.T4.1.1.8.7.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.8.7.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.8.7.10.1">
<span class="ltx_p" id="S5.T4.1.1.8.7.10.1.1"><span class="ltx_text" id="S5.T4.1.1.8.7.10.1.1.1" style="font-size:90%;">specific for autonomous driving not specific for driver drowsiness. Database is more extensive, diverse, and multi-purpose.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.9.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.1.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.1.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.1.1.1.1" style="font-size:90%;">DMD</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.9.8.1.1.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.9.8.1.1.1.3" style="font-size:90%;">fn:dmd</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.9.8.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib125" title="">125</a><span class="ltx_text" id="S5.T4.1.1.9.8.1.1.1.5.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.2.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.2.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.2.1.1.1" style="font-size:90%;">2020</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.3.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.3.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.4.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.4.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.4.1.1.1" style="font-size:90%;">facial expression, mouth open, eyes open, head pose, yawning</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.5.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.5.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.6.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.6.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.6.1.1.1" style="font-size:90%;">driver distraction, recognition includes but not only for drowsiness detection</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.7.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.7.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.7.1.1.1" style="font-size:90%;">37</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.8.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.8.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.8.1.1.1" style="font-size:90%;">41 hours of RGB, depth, IR videos</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.9.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.9.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.9.1.1.1" style="font-size:90%;">levels of distraction, gaze allocation, hands-wheel interaction, and context data, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.9.8.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib136" title="">136</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib137" title="">137</a><span class="ltx_text" id="S5.T4.1.1.9.8.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.9.8.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.9.8.10.1">
<span class="ltx_p" id="S5.T4.1.1.9.8.10.1.1"><span class="ltx_text" id="S5.T4.1.1.9.8.10.1.1.1" style="font-size:90%;">multimodal dataset for different driver scenarios. Not considering drowsiness as the only distraction factor for drivers</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.10.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.1.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.1.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.1.1.1.1" style="font-size:90%;">SUST-DDD</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.10.9.1.1.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.10.9.1.1.1.3" style="font-size:90%;">fn:sust </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.10.9.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib126" title="">126</a><span class="ltx_text" id="S5.T4.1.1.10.9.1.1.1.5.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.2.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.2.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.2.1.1.1" style="font-size:90%;">2022</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.3.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.3.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.3.1.1.1" style="font-size:90%;">vision-based</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.4.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.4.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.4.1.1.1" style="font-size:90%;">facial expression, eye openess, mouth open</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.5.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.5.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.5.1.1.1" style="font-size:90%;">remote</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.6.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.6.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.6.1.1.1" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.7.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.7.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.7.1.1.1" style="font-size:90%;">19</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.8.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.8.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.8.1.1.1" style="font-size:90%;">2074 RGB videos</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.9.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.9.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.9.1.1.1" style="font-size:90%;">drowsiness and non-drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.10.9.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.10.9.10.1">
<span class="ltx_p" id="S5.T4.1.1.10.9.10.1.1"><span class="ltx_text" id="S5.T4.1.1.10.9.10.1.1.1" style="font-size:90%;">recorded under real driving scenarios, participants use their own vehicle and phones to mimic most natural and comfortable driving condition</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.11.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.1.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.1.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.1.1.1.1" style="font-size:90%;">DROZY</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.11.10.1.1.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.11.10.1.1.1.3" style="font-size:90%;">fn:drozy </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.11.10.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib77" title="">77</a><span class="ltx_text" id="S5.T4.1.1.11.10.1.1.1.5.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.2.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.2.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.2.1.1.1" style="font-size:90%;">2016</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.11.10.3.1">
<tr class="ltx_tr" id="S5.T4.1.1.11.10.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.11.10.3.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.11.10.3.1.1.1.1" style="font-size:90%;">vision-based</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.11.10.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.11.10.3.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.11.10.3.1.2.1.1" style="font-size:90%;">physiological</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.4.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.4.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.4.1.1.1" style="font-size:90%;">facial expression, heartbeat variability</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.5.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.5.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.5.1.1.1" style="font-size:90%;">remote, wired</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.6.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.6.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.6.1.1.1" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.7.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.7.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.7.1.1.1" style="font-size:90%;">14</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.8.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.8.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.8.1.1.1" style="font-size:90%;">3064 examples</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.9.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.9.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.9.1.1.1" style="font-size:90%;">drowsiness and non-drowsiness, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.11.10.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib138" title="">138</a><span class="ltx_text" id="S5.T4.1.1.11.10.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T4.1.1.11.10.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.11.10.10.1">
<span class="ltx_p" id="S5.T4.1.1.11.10.10.1.1"><span class="ltx_text" id="S5.T4.1.1.11.10.10.1.1.1" style="font-size:90%;">recorded in real driving scenarios, participants use their own vehicle and phones to mimic most natural and comfortable driving condition</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.12.11">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.12.11.1.1">
<tr class="ltx_tr" id="S5.T4.1.1.12.11.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.12.11.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.12.11.1.1.1.1.1" style="font-size:90%;">Multi</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.12.11.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.12.11.1.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.12.11.1.1.2.1.1" style="font-size:90%;">channel</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.12.11.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.12.11.1.1.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S5.T4.1.1.12.11.1.1.3.1.1" style="font-size:90%;">EEG</span><span class="ltx_ERROR undefined" id="S5.T4.1.1.12.11.1.1.3.1.2">\footref</span><span class="ltx_text" id="S5.T4.1.1.12.11.1.1.3.1.3" style="font-size:90%;">fn:eeg </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.12.11.1.1.3.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib92" title="">92</a><span class="ltx_text" id="S5.T4.1.1.12.11.1.1.3.1.5.2" style="font-size:90%;">]</span></cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.2" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.12.11.2.1">
<span class="ltx_p" id="S5.T4.1.1.12.11.2.1.1"><span class="ltx_text" id="S5.T4.1.1.12.11.2.1.1.1" style="font-size:90%;">2019</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.3" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.12.11.3.1">
<span class="ltx_p" id="S5.T4.1.1.12.11.3.1.1"><span class="ltx_text" id="S5.T4.1.1.12.11.3.1.1.1" style="font-size:90%;">physiological</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.4" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.12.11.4.1">
<span class="ltx_p" id="S5.T4.1.1.12.11.4.1.1"><span class="ltx_text" id="S5.T4.1.1.12.11.4.1.1.1" style="font-size:90%;">EEG-based deviation in RT during lane-keeping task</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.5" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.12.11.5.1">
<span class="ltx_p" id="S5.T4.1.1.12.11.5.1.1"><span class="ltx_text" id="S5.T4.1.1.12.11.5.1.1.1" style="font-size:90%;">wired</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.6" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.12.11.6.1">
<span class="ltx_p" id="S5.T4.1.1.12.11.6.1.1"><span class="ltx_text" id="S5.T4.1.1.12.11.6.1.1.1" style="font-size:90%;">driver drowsiness</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.7" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.12.11.7.1">
<span class="ltx_p" id="S5.T4.1.1.12.11.7.1.1"><span class="ltx_text" id="S5.T4.1.1.12.11.7.1.1.1" style="font-size:90%;">27</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.8" style="padding-left:1.0pt;padding-right:1.0pt;">
<table class="ltx_tabular ltx_align_top" id="S5.T4.1.1.12.11.8.1">
<tr class="ltx_tr" id="S5.T4.1.1.12.11.8.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.12.11.8.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.12.11.8.1.1.1.1" style="font-size:90%;">62 sessions derived</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.12.11.8.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S5.T4.1.1.12.11.8.1.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S5.T4.1.1.12.11.8.1.2.1.1" style="font-size:90%;">from 90 min. task</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.9" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.12.11.9.1">
<span class="ltx_p" id="S5.T4.1.1.12.11.9.1.1"><span class="ltx_text" id="S5.T4.1.1.12.11.9.1.1.1" style="font-size:90%;">deviation onset, response onset, response offset, used in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T4.1.1.12.11.9.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib78" title="">78</a><span class="ltx_text" id="S5.T4.1.1.12.11.9.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t" id="S5.T4.1.1.12.11.10" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.12.11.10.1">
<span class="ltx_p" id="S5.T4.1.1.12.11.10.1.1"><span class="ltx_text" id="S5.T4.1.1.12.11.10.1.1.1" style="font-size:90%;">23-channel EEG under simulated driving condition</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure></div></div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Performance and Evaluation Metrics</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we present common evaluation metrics used to compare SOTA drowsiness detection algorithms. Popular evaluation metrics for this classification task include accuracy (Acc) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib110" title="">110</a>]</cite>, precision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib72" title="">72</a>]</cite>, recall <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib72" title="">72</a>]</cite>, and F1-score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib72" title="">72</a>]</cite> defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Precision=\cfrac{TP}{TP+FP}," class="ltx_Math" display="block" id="S6.E1.m1.1"><semantics id="S6.E1.m1.1a"><mrow id="S6.E1.m1.1.1.1" xref="S6.E1.m1.1.1.1.1.cmml"><mrow id="S6.E1.m1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.cmml"><mrow id="S6.E1.m1.1.1.1.1.2" xref="S6.E1.m1.1.1.1.1.2.cmml"><mi id="S6.E1.m1.1.1.1.1.2.2" xref="S6.E1.m1.1.1.1.1.2.2.cmml">P</mi><mo id="S6.E1.m1.1.1.1.1.2.1" xref="S6.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.2.3" xref="S6.E1.m1.1.1.1.1.2.3.cmml">r</mi><mo id="S6.E1.m1.1.1.1.1.2.1a" xref="S6.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.2.4" xref="S6.E1.m1.1.1.1.1.2.4.cmml">e</mi><mo id="S6.E1.m1.1.1.1.1.2.1b" xref="S6.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.2.5" xref="S6.E1.m1.1.1.1.1.2.5.cmml">c</mi><mo id="S6.E1.m1.1.1.1.1.2.1c" xref="S6.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.2.6" xref="S6.E1.m1.1.1.1.1.2.6.cmml">i</mi><mo id="S6.E1.m1.1.1.1.1.2.1d" xref="S6.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.2.7" xref="S6.E1.m1.1.1.1.1.2.7.cmml">s</mi><mo id="S6.E1.m1.1.1.1.1.2.1e" xref="S6.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.2.8" xref="S6.E1.m1.1.1.1.1.2.8.cmml">i</mi><mo id="S6.E1.m1.1.1.1.1.2.1f" xref="S6.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.2.9" xref="S6.E1.m1.1.1.1.1.2.9.cmml">o</mi><mo id="S6.E1.m1.1.1.1.1.2.1g" xref="S6.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.2.10" xref="S6.E1.m1.1.1.1.1.2.10.cmml">n</mi></mrow><mo id="S6.E1.m1.1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S6.E1.m1.1.1.1.1.3" xref="S6.E1.m1.1.1.1.1.3.cmml"><mrow id="S6.E1.m1.1.1.1.1.3.2" xref="S6.E1.m1.1.1.1.1.3.2.cmml"><mi id="S6.E1.m1.1.1.1.1.3.2.2" xref="S6.E1.m1.1.1.1.1.3.2.2.cmml">T</mi><mo id="S6.E1.m1.1.1.1.1.3.2.1" xref="S6.E1.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.3.2.3" xref="S6.E1.m1.1.1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S6.E1.m1.1.1.1.1.3.3" xref="S6.E1.m1.1.1.1.1.3.3.cmml"><mrow id="S6.E1.m1.1.1.1.1.3.3.2" xref="S6.E1.m1.1.1.1.1.3.3.2.cmml"><mi id="S6.E1.m1.1.1.1.1.3.3.2.2" xref="S6.E1.m1.1.1.1.1.3.3.2.2.cmml">T</mi><mo id="S6.E1.m1.1.1.1.1.3.3.2.1" xref="S6.E1.m1.1.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.3.3.2.3" xref="S6.E1.m1.1.1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S6.E1.m1.1.1.1.1.3.3.1" xref="S6.E1.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S6.E1.m1.1.1.1.1.3.3.3" xref="S6.E1.m1.1.1.1.1.3.3.3.cmml"><mi id="S6.E1.m1.1.1.1.1.3.3.3.2" xref="S6.E1.m1.1.1.1.1.3.3.3.2.cmml">F</mi><mo id="S6.E1.m1.1.1.1.1.3.3.3.1" xref="S6.E1.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S6.E1.m1.1.1.1.1.3.3.3.3" xref="S6.E1.m1.1.1.1.1.3.3.3.3.cmml">P</mi></mrow></mrow></mfrac></mrow><mo id="S6.E1.m1.1.1.1.2" xref="S6.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.1b"><apply id="S6.E1.m1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1"><eq id="S6.E1.m1.1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1.1"></eq><apply id="S6.E1.m1.1.1.1.1.2.cmml" xref="S6.E1.m1.1.1.1.1.2"><times id="S6.E1.m1.1.1.1.1.2.1.cmml" xref="S6.E1.m1.1.1.1.1.2.1"></times><ci id="S6.E1.m1.1.1.1.1.2.2.cmml" xref="S6.E1.m1.1.1.1.1.2.2">𝑃</ci><ci id="S6.E1.m1.1.1.1.1.2.3.cmml" xref="S6.E1.m1.1.1.1.1.2.3">𝑟</ci><ci id="S6.E1.m1.1.1.1.1.2.4.cmml" xref="S6.E1.m1.1.1.1.1.2.4">𝑒</ci><ci id="S6.E1.m1.1.1.1.1.2.5.cmml" xref="S6.E1.m1.1.1.1.1.2.5">𝑐</ci><ci id="S6.E1.m1.1.1.1.1.2.6.cmml" xref="S6.E1.m1.1.1.1.1.2.6">𝑖</ci><ci id="S6.E1.m1.1.1.1.1.2.7.cmml" xref="S6.E1.m1.1.1.1.1.2.7">𝑠</ci><ci id="S6.E1.m1.1.1.1.1.2.8.cmml" xref="S6.E1.m1.1.1.1.1.2.8">𝑖</ci><ci id="S6.E1.m1.1.1.1.1.2.9.cmml" xref="S6.E1.m1.1.1.1.1.2.9">𝑜</ci><ci id="S6.E1.m1.1.1.1.1.2.10.cmml" xref="S6.E1.m1.1.1.1.1.2.10">𝑛</ci></apply><apply id="S6.E1.m1.1.1.1.1.3.cmml" xref="S6.E1.m1.1.1.1.1.3"><csymbol cd="latexml" id="S6.E1.m1.1.1.1.1.3.1.cmml" xref="S6.E1.m1.1.1.1.1.3">continued-fraction</csymbol><apply id="S6.E1.m1.1.1.1.1.3.2.cmml" xref="S6.E1.m1.1.1.1.1.3.2"><times id="S6.E1.m1.1.1.1.1.3.2.1.cmml" xref="S6.E1.m1.1.1.1.1.3.2.1"></times><ci id="S6.E1.m1.1.1.1.1.3.2.2.cmml" xref="S6.E1.m1.1.1.1.1.3.2.2">𝑇</ci><ci id="S6.E1.m1.1.1.1.1.3.2.3.cmml" xref="S6.E1.m1.1.1.1.1.3.2.3">𝑃</ci></apply><apply id="S6.E1.m1.1.1.1.1.3.3.cmml" xref="S6.E1.m1.1.1.1.1.3.3"><plus id="S6.E1.m1.1.1.1.1.3.3.1.cmml" xref="S6.E1.m1.1.1.1.1.3.3.1"></plus><apply id="S6.E1.m1.1.1.1.1.3.3.2.cmml" xref="S6.E1.m1.1.1.1.1.3.3.2"><times id="S6.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S6.E1.m1.1.1.1.1.3.3.2.1"></times><ci id="S6.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S6.E1.m1.1.1.1.1.3.3.2.2">𝑇</ci><ci id="S6.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S6.E1.m1.1.1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S6.E1.m1.1.1.1.1.3.3.3.cmml" xref="S6.E1.m1.1.1.1.1.3.3.3"><times id="S6.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S6.E1.m1.1.1.1.1.3.3.3.1"></times><ci id="S6.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S6.E1.m1.1.1.1.1.3.3.3.2">𝐹</ci><ci id="S6.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S6.E1.m1.1.1.1.1.3.3.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.1c">Precision=\cfrac{TP}{TP+FP},</annotation><annotation encoding="application/x-llamapun" id="S6.E1.m1.1d">italic_P italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n = continued-fraction start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_P end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Recall=\cfrac{TP}{TP+FN}," class="ltx_Math" display="block" id="S6.E2.m1.1"><semantics id="S6.E2.m1.1a"><mrow id="S6.E2.m1.1.1.1" xref="S6.E2.m1.1.1.1.1.cmml"><mrow id="S6.E2.m1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.cmml"><mrow id="S6.E2.m1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.2.cmml"><mi id="S6.E2.m1.1.1.1.1.2.2" xref="S6.E2.m1.1.1.1.1.2.2.cmml">R</mi><mo id="S6.E2.m1.1.1.1.1.2.1" xref="S6.E2.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E2.m1.1.1.1.1.2.3" xref="S6.E2.m1.1.1.1.1.2.3.cmml">e</mi><mo id="S6.E2.m1.1.1.1.1.2.1a" xref="S6.E2.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E2.m1.1.1.1.1.2.4" xref="S6.E2.m1.1.1.1.1.2.4.cmml">c</mi><mo id="S6.E2.m1.1.1.1.1.2.1b" xref="S6.E2.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E2.m1.1.1.1.1.2.5" xref="S6.E2.m1.1.1.1.1.2.5.cmml">a</mi><mo id="S6.E2.m1.1.1.1.1.2.1c" xref="S6.E2.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E2.m1.1.1.1.1.2.6" xref="S6.E2.m1.1.1.1.1.2.6.cmml">l</mi><mo id="S6.E2.m1.1.1.1.1.2.1d" xref="S6.E2.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S6.E2.m1.1.1.1.1.2.7" xref="S6.E2.m1.1.1.1.1.2.7.cmml">l</mi></mrow><mo id="S6.E2.m1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S6.E2.m1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.3.cmml"><mrow id="S6.E2.m1.1.1.1.1.3.2" xref="S6.E2.m1.1.1.1.1.3.2.cmml"><mi id="S6.E2.m1.1.1.1.1.3.2.2" xref="S6.E2.m1.1.1.1.1.3.2.2.cmml">T</mi><mo id="S6.E2.m1.1.1.1.1.3.2.1" xref="S6.E2.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E2.m1.1.1.1.1.3.2.3" xref="S6.E2.m1.1.1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S6.E2.m1.1.1.1.1.3.3" xref="S6.E2.m1.1.1.1.1.3.3.cmml"><mrow id="S6.E2.m1.1.1.1.1.3.3.2" xref="S6.E2.m1.1.1.1.1.3.3.2.cmml"><mi id="S6.E2.m1.1.1.1.1.3.3.2.2" xref="S6.E2.m1.1.1.1.1.3.3.2.2.cmml">T</mi><mo id="S6.E2.m1.1.1.1.1.3.3.2.1" xref="S6.E2.m1.1.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S6.E2.m1.1.1.1.1.3.3.2.3" xref="S6.E2.m1.1.1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S6.E2.m1.1.1.1.1.3.3.1" xref="S6.E2.m1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S6.E2.m1.1.1.1.1.3.3.3" xref="S6.E2.m1.1.1.1.1.3.3.3.cmml"><mi id="S6.E2.m1.1.1.1.1.3.3.3.2" xref="S6.E2.m1.1.1.1.1.3.3.3.2.cmml">F</mi><mo id="S6.E2.m1.1.1.1.1.3.3.3.1" xref="S6.E2.m1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S6.E2.m1.1.1.1.1.3.3.3.3" xref="S6.E2.m1.1.1.1.1.3.3.3.3.cmml">N</mi></mrow></mrow></mfrac></mrow><mo id="S6.E2.m1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.1b"><apply id="S6.E2.m1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1"><eq id="S6.E2.m1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1"></eq><apply id="S6.E2.m1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.2"><times id="S6.E2.m1.1.1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.1.1.2.1"></times><ci id="S6.E2.m1.1.1.1.1.2.2.cmml" xref="S6.E2.m1.1.1.1.1.2.2">𝑅</ci><ci id="S6.E2.m1.1.1.1.1.2.3.cmml" xref="S6.E2.m1.1.1.1.1.2.3">𝑒</ci><ci id="S6.E2.m1.1.1.1.1.2.4.cmml" xref="S6.E2.m1.1.1.1.1.2.4">𝑐</ci><ci id="S6.E2.m1.1.1.1.1.2.5.cmml" xref="S6.E2.m1.1.1.1.1.2.5">𝑎</ci><ci id="S6.E2.m1.1.1.1.1.2.6.cmml" xref="S6.E2.m1.1.1.1.1.2.6">𝑙</ci><ci id="S6.E2.m1.1.1.1.1.2.7.cmml" xref="S6.E2.m1.1.1.1.1.2.7">𝑙</ci></apply><apply id="S6.E2.m1.1.1.1.1.3.cmml" xref="S6.E2.m1.1.1.1.1.3"><csymbol cd="latexml" id="S6.E2.m1.1.1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.1.1.3">continued-fraction</csymbol><apply id="S6.E2.m1.1.1.1.1.3.2.cmml" xref="S6.E2.m1.1.1.1.1.3.2"><times id="S6.E2.m1.1.1.1.1.3.2.1.cmml" xref="S6.E2.m1.1.1.1.1.3.2.1"></times><ci id="S6.E2.m1.1.1.1.1.3.2.2.cmml" xref="S6.E2.m1.1.1.1.1.3.2.2">𝑇</ci><ci id="S6.E2.m1.1.1.1.1.3.2.3.cmml" xref="S6.E2.m1.1.1.1.1.3.2.3">𝑃</ci></apply><apply id="S6.E2.m1.1.1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3"><plus id="S6.E2.m1.1.1.1.1.3.3.1.cmml" xref="S6.E2.m1.1.1.1.1.3.3.1"></plus><apply id="S6.E2.m1.1.1.1.1.3.3.2.cmml" xref="S6.E2.m1.1.1.1.1.3.3.2"><times id="S6.E2.m1.1.1.1.1.3.3.2.1.cmml" xref="S6.E2.m1.1.1.1.1.3.3.2.1"></times><ci id="S6.E2.m1.1.1.1.1.3.3.2.2.cmml" xref="S6.E2.m1.1.1.1.1.3.3.2.2">𝑇</ci><ci id="S6.E2.m1.1.1.1.1.3.3.2.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S6.E2.m1.1.1.1.1.3.3.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3.3"><times id="S6.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S6.E2.m1.1.1.1.1.3.3.3.1"></times><ci id="S6.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S6.E2.m1.1.1.1.1.3.3.3.2">𝐹</ci><ci id="S6.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3.3.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.1c">Recall=\cfrac{TP}{TP+FN},</annotation><annotation encoding="application/x-llamapun" id="S6.E2.m1.1d">italic_R italic_e italic_c italic_a italic_l italic_l = continued-fraction start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_N end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S6.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F1-score=2\times\cfrac{precision\times recall}{precision+recall}," class="ltx_Math" display="block" id="S6.E3.m1.1"><semantics id="S6.E3.m1.1a"><mrow id="S6.E3.m1.1.1.1" xref="S6.E3.m1.1.1.1.1.cmml"><mrow id="S6.E3.m1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.cmml"><mrow id="S6.E3.m1.1.1.1.1.2" xref="S6.E3.m1.1.1.1.1.2.cmml"><mrow id="S6.E3.m1.1.1.1.1.2.2" xref="S6.E3.m1.1.1.1.1.2.2.cmml"><mi id="S6.E3.m1.1.1.1.1.2.2.2" xref="S6.E3.m1.1.1.1.1.2.2.2.cmml">F</mi><mo id="S6.E3.m1.1.1.1.1.2.2.1" xref="S6.E3.m1.1.1.1.1.2.2.1.cmml">⁢</mo><mn id="S6.E3.m1.1.1.1.1.2.2.3" xref="S6.E3.m1.1.1.1.1.2.2.3.cmml">1</mn></mrow><mo id="S6.E3.m1.1.1.1.1.2.1" xref="S6.E3.m1.1.1.1.1.2.1.cmml">−</mo><mrow id="S6.E3.m1.1.1.1.1.2.3" xref="S6.E3.m1.1.1.1.1.2.3.cmml"><mi id="S6.E3.m1.1.1.1.1.2.3.2" xref="S6.E3.m1.1.1.1.1.2.3.2.cmml">s</mi><mo id="S6.E3.m1.1.1.1.1.2.3.1" xref="S6.E3.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.2.3.3" xref="S6.E3.m1.1.1.1.1.2.3.3.cmml">c</mi><mo id="S6.E3.m1.1.1.1.1.2.3.1a" xref="S6.E3.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.2.3.4" xref="S6.E3.m1.1.1.1.1.2.3.4.cmml">o</mi><mo id="S6.E3.m1.1.1.1.1.2.3.1b" xref="S6.E3.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.2.3.5" xref="S6.E3.m1.1.1.1.1.2.3.5.cmml">r</mi><mo id="S6.E3.m1.1.1.1.1.2.3.1c" xref="S6.E3.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.2.3.6" xref="S6.E3.m1.1.1.1.1.2.3.6.cmml">e</mi></mrow></mrow><mo id="S6.E3.m1.1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S6.E3.m1.1.1.1.1.3" xref="S6.E3.m1.1.1.1.1.3.cmml"><mn id="S6.E3.m1.1.1.1.1.3.2" xref="S6.E3.m1.1.1.1.1.3.2.cmml">2</mn><mo id="S6.E3.m1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S6.E3.m1.1.1.1.1.3.1.cmml">×</mo><mfrac id="S6.E3.m1.1.1.1.1.3.3" xref="S6.E3.m1.1.1.1.1.3.3.cmml"><mrow id="S6.E3.m1.1.1.1.1.3.3.2" xref="S6.E3.m1.1.1.1.1.3.3.2.cmml"><mrow id="S6.E3.m1.1.1.1.1.3.3.2.2" xref="S6.E3.m1.1.1.1.1.3.3.2.2.cmml"><mrow id="S6.E3.m1.1.1.1.1.3.3.2.2.2" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.cmml"><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.2" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.2.cmml">p</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.3" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.3.cmml">r</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1a" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.4" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.4.cmml">e</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1b" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.5" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.5.cmml">c</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1c" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.6" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.6.cmml">i</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1d" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.7" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.7.cmml">s</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1e" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.8" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.8.cmml">i</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1f" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.9" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.9.cmml">o</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1g" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.2.10" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.10.cmml">n</mi></mrow><mo id="S6.E3.m1.1.1.1.1.3.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S6.E3.m1.1.1.1.1.3.3.2.2.1.cmml">×</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.2.3" xref="S6.E3.m1.1.1.1.1.3.3.2.2.3.cmml">r</mi></mrow><mo id="S6.E3.m1.1.1.1.1.3.3.2.1" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.3" xref="S6.E3.m1.1.1.1.1.3.3.2.3.cmml">e</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.1a" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.4" xref="S6.E3.m1.1.1.1.1.3.3.2.4.cmml">c</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.1b" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.5" xref="S6.E3.m1.1.1.1.1.3.3.2.5.cmml">a</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.1c" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.6" xref="S6.E3.m1.1.1.1.1.3.3.2.6.cmml">l</mi><mo id="S6.E3.m1.1.1.1.1.3.3.2.1d" xref="S6.E3.m1.1.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.2.7" xref="S6.E3.m1.1.1.1.1.3.3.2.7.cmml">l</mi></mrow><mrow id="S6.E3.m1.1.1.1.1.3.3.3" xref="S6.E3.m1.1.1.1.1.3.3.3.cmml"><mrow id="S6.E3.m1.1.1.1.1.3.3.3.2" xref="S6.E3.m1.1.1.1.1.3.3.3.2.cmml"><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.2" xref="S6.E3.m1.1.1.1.1.3.3.3.2.2.cmml">p</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.2.1" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.3" xref="S6.E3.m1.1.1.1.1.3.3.3.2.3.cmml">r</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.2.1a" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.4" xref="S6.E3.m1.1.1.1.1.3.3.3.2.4.cmml">e</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.2.1b" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.5" xref="S6.E3.m1.1.1.1.1.3.3.3.2.5.cmml">c</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.2.1c" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.6" xref="S6.E3.m1.1.1.1.1.3.3.3.2.6.cmml">i</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.2.1d" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.7" xref="S6.E3.m1.1.1.1.1.3.3.3.2.7.cmml">s</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.2.1e" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.8" xref="S6.E3.m1.1.1.1.1.3.3.3.2.8.cmml">i</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.2.1f" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.9" xref="S6.E3.m1.1.1.1.1.3.3.3.2.9.cmml">o</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.2.1g" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.2.10" xref="S6.E3.m1.1.1.1.1.3.3.3.2.10.cmml">n</mi></mrow><mo id="S6.E3.m1.1.1.1.1.3.3.3.1" xref="S6.E3.m1.1.1.1.1.3.3.3.1.cmml">+</mo><mrow id="S6.E3.m1.1.1.1.1.3.3.3.3" xref="S6.E3.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.2" xref="S6.E3.m1.1.1.1.1.3.3.3.3.2.cmml">r</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.3.1" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.3" xref="S6.E3.m1.1.1.1.1.3.3.3.3.3.cmml">e</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.3.1a" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.4" xref="S6.E3.m1.1.1.1.1.3.3.3.3.4.cmml">c</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.3.1b" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.5" xref="S6.E3.m1.1.1.1.1.3.3.3.3.5.cmml">a</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.3.1c" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.6" xref="S6.E3.m1.1.1.1.1.3.3.3.3.6.cmml">l</mi><mo id="S6.E3.m1.1.1.1.1.3.3.3.3.1d" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S6.E3.m1.1.1.1.1.3.3.3.3.7" xref="S6.E3.m1.1.1.1.1.3.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow></mrow><mo id="S6.E3.m1.1.1.1.2" xref="S6.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E3.m1.1b"><apply id="S6.E3.m1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1"><eq id="S6.E3.m1.1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1.1.1"></eq><apply id="S6.E3.m1.1.1.1.1.2.cmml" xref="S6.E3.m1.1.1.1.1.2"><minus id="S6.E3.m1.1.1.1.1.2.1.cmml" xref="S6.E3.m1.1.1.1.1.2.1"></minus><apply id="S6.E3.m1.1.1.1.1.2.2.cmml" xref="S6.E3.m1.1.1.1.1.2.2"><times id="S6.E3.m1.1.1.1.1.2.2.1.cmml" xref="S6.E3.m1.1.1.1.1.2.2.1"></times><ci id="S6.E3.m1.1.1.1.1.2.2.2.cmml" xref="S6.E3.m1.1.1.1.1.2.2.2">𝐹</ci><cn id="S6.E3.m1.1.1.1.1.2.2.3.cmml" type="integer" xref="S6.E3.m1.1.1.1.1.2.2.3">1</cn></apply><apply id="S6.E3.m1.1.1.1.1.2.3.cmml" xref="S6.E3.m1.1.1.1.1.2.3"><times id="S6.E3.m1.1.1.1.1.2.3.1.cmml" xref="S6.E3.m1.1.1.1.1.2.3.1"></times><ci id="S6.E3.m1.1.1.1.1.2.3.2.cmml" xref="S6.E3.m1.1.1.1.1.2.3.2">𝑠</ci><ci id="S6.E3.m1.1.1.1.1.2.3.3.cmml" xref="S6.E3.m1.1.1.1.1.2.3.3">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.2.3.4.cmml" xref="S6.E3.m1.1.1.1.1.2.3.4">𝑜</ci><ci id="S6.E3.m1.1.1.1.1.2.3.5.cmml" xref="S6.E3.m1.1.1.1.1.2.3.5">𝑟</ci><ci id="S6.E3.m1.1.1.1.1.2.3.6.cmml" xref="S6.E3.m1.1.1.1.1.2.3.6">𝑒</ci></apply></apply><apply id="S6.E3.m1.1.1.1.1.3.cmml" xref="S6.E3.m1.1.1.1.1.3"><times id="S6.E3.m1.1.1.1.1.3.1.cmml" xref="S6.E3.m1.1.1.1.1.3.1"></times><cn id="S6.E3.m1.1.1.1.1.3.2.cmml" type="integer" xref="S6.E3.m1.1.1.1.1.3.2">2</cn><apply id="S6.E3.m1.1.1.1.1.3.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3"><csymbol cd="latexml" id="S6.E3.m1.1.1.1.1.3.3.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3">continued-fraction</csymbol><apply id="S6.E3.m1.1.1.1.1.3.3.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2"><times id="S6.E3.m1.1.1.1.1.3.3.2.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.1"></times><apply id="S6.E3.m1.1.1.1.1.3.3.2.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2"><times id="S6.E3.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.1"></times><apply id="S6.E3.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2"><times id="S6.E3.m1.1.1.1.1.3.3.2.2.2.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.1"></times><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.2">𝑝</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.3">𝑟</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.4.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.4">𝑒</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.5.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.5">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.6.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.6">𝑖</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.7.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.7">𝑠</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.8.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.8">𝑖</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.9.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.9">𝑜</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.2.10.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.2.10">𝑛</ci></apply><ci id="S6.E3.m1.1.1.1.1.3.3.2.2.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.2.3">𝑟</ci></apply><ci id="S6.E3.m1.1.1.1.1.3.3.2.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.3">𝑒</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.4.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.4">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.5.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.5">𝑎</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.6.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.6">𝑙</ci><ci id="S6.E3.m1.1.1.1.1.3.3.2.7.cmml" xref="S6.E3.m1.1.1.1.1.3.3.2.7">𝑙</ci></apply><apply id="S6.E3.m1.1.1.1.1.3.3.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3"><plus id="S6.E3.m1.1.1.1.1.3.3.3.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.1"></plus><apply id="S6.E3.m1.1.1.1.1.3.3.3.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2"><times id="S6.E3.m1.1.1.1.1.3.3.3.2.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.1"></times><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.2">𝑝</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.3">𝑟</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.4.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.4">𝑒</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.5.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.5">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.6.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.6">𝑖</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.7.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.7">𝑠</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.8.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.8">𝑖</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.9.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.9">𝑜</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.2.10.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.2.10">𝑛</ci></apply><apply id="S6.E3.m1.1.1.1.1.3.3.3.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3"><times id="S6.E3.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.1"></times><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.2">𝑟</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.3">𝑒</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.4.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.4">𝑐</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.5.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.5">𝑎</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.6.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.6">𝑙</ci><ci id="S6.E3.m1.1.1.1.1.3.3.3.3.7.cmml" xref="S6.E3.m1.1.1.1.1.3.3.3.3.7">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E3.m1.1c">F1-score=2\times\cfrac{precision\times recall}{precision+recall},</annotation><annotation encoding="application/x-llamapun" id="S6.E3.m1.1d">italic_F 1 - italic_s italic_c italic_o italic_r italic_e = 2 × continued-fraction start_ARG italic_p italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n × italic_r italic_e italic_c italic_a italic_l italic_l end_ARG start_ARG italic_p italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n + italic_r italic_e italic_c italic_a italic_l italic_l end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S6.p1.2">where TP stands for true positive accounting for the number of correct drowsiness predictions, FP stands for false positive accounting for the number of incorrect drowsiness predictions (type I error), TN stands for true negative accounting for the number of correct non-drowsiness predictions and finally, FN stands for false negative accounting the number of incorrect non-drowsiness prediction (type II errors). Detailed results in terms of accuracy and F1-score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib109" title="">109</a>]</cite> are calculated for assessing the detection performance. The mathematical formulation of accuracy is given as:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Accuracy=\left(\cfrac{TP+TN}{TP+FN+TN+FP}\right)." class="ltx_Math" display="block" id="S6.E4.m1.2"><semantics id="S6.E4.m1.2a"><mrow id="S6.E4.m1.2.2.1" xref="S6.E4.m1.2.2.1.1.cmml"><mrow id="S6.E4.m1.2.2.1.1" xref="S6.E4.m1.2.2.1.1.cmml"><mrow id="S6.E4.m1.2.2.1.1.2" xref="S6.E4.m1.2.2.1.1.2.cmml"><mi id="S6.E4.m1.2.2.1.1.2.2" xref="S6.E4.m1.2.2.1.1.2.2.cmml">A</mi><mo id="S6.E4.m1.2.2.1.1.2.1" xref="S6.E4.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E4.m1.2.2.1.1.2.3" xref="S6.E4.m1.2.2.1.1.2.3.cmml">c</mi><mo id="S6.E4.m1.2.2.1.1.2.1a" xref="S6.E4.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E4.m1.2.2.1.1.2.4" xref="S6.E4.m1.2.2.1.1.2.4.cmml">c</mi><mo id="S6.E4.m1.2.2.1.1.2.1b" xref="S6.E4.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E4.m1.2.2.1.1.2.5" xref="S6.E4.m1.2.2.1.1.2.5.cmml">u</mi><mo id="S6.E4.m1.2.2.1.1.2.1c" xref="S6.E4.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E4.m1.2.2.1.1.2.6" xref="S6.E4.m1.2.2.1.1.2.6.cmml">r</mi><mo id="S6.E4.m1.2.2.1.1.2.1d" xref="S6.E4.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E4.m1.2.2.1.1.2.7" xref="S6.E4.m1.2.2.1.1.2.7.cmml">a</mi><mo id="S6.E4.m1.2.2.1.1.2.1e" xref="S6.E4.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E4.m1.2.2.1.1.2.8" xref="S6.E4.m1.2.2.1.1.2.8.cmml">c</mi><mo id="S6.E4.m1.2.2.1.1.2.1f" xref="S6.E4.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E4.m1.2.2.1.1.2.9" xref="S6.E4.m1.2.2.1.1.2.9.cmml">y</mi></mrow><mo id="S6.E4.m1.2.2.1.1.1" xref="S6.E4.m1.2.2.1.1.1.cmml">=</mo><mrow id="S6.E4.m1.2.2.1.1.3.2" xref="S6.E4.m1.1.1.cmml"><mo id="S6.E4.m1.2.2.1.1.3.2.1" xref="S6.E4.m1.1.1.cmml">(</mo><mfrac id="S6.E4.m1.1.1" xref="S6.E4.m1.1.1.cmml"><mrow id="S6.E4.m1.1.1.2" xref="S6.E4.m1.1.1.2.cmml"><mrow id="S6.E4.m1.1.1.2.2" xref="S6.E4.m1.1.1.2.2.cmml"><mi id="S6.E4.m1.1.1.2.2.2" xref="S6.E4.m1.1.1.2.2.2.cmml">T</mi><mo id="S6.E4.m1.1.1.2.2.1" xref="S6.E4.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S6.E4.m1.1.1.2.2.3" xref="S6.E4.m1.1.1.2.2.3.cmml">P</mi></mrow><mo id="S6.E4.m1.1.1.2.1" xref="S6.E4.m1.1.1.2.1.cmml">+</mo><mrow id="S6.E4.m1.1.1.2.3" xref="S6.E4.m1.1.1.2.3.cmml"><mi id="S6.E4.m1.1.1.2.3.2" xref="S6.E4.m1.1.1.2.3.2.cmml">T</mi><mo id="S6.E4.m1.1.1.2.3.1" xref="S6.E4.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S6.E4.m1.1.1.2.3.3" xref="S6.E4.m1.1.1.2.3.3.cmml">N</mi></mrow></mrow><mrow id="S6.E4.m1.1.1.3" xref="S6.E4.m1.1.1.3.cmml"><mrow id="S6.E4.m1.1.1.3.2" xref="S6.E4.m1.1.1.3.2.cmml"><mi id="S6.E4.m1.1.1.3.2.2" xref="S6.E4.m1.1.1.3.2.2.cmml">T</mi><mo id="S6.E4.m1.1.1.3.2.1" xref="S6.E4.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E4.m1.1.1.3.2.3" xref="S6.E4.m1.1.1.3.2.3.cmml">P</mi></mrow><mo id="S6.E4.m1.1.1.3.1" xref="S6.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S6.E4.m1.1.1.3.3" xref="S6.E4.m1.1.1.3.3.cmml"><mi id="S6.E4.m1.1.1.3.3.2" xref="S6.E4.m1.1.1.3.3.2.cmml">F</mi><mo id="S6.E4.m1.1.1.3.3.1" xref="S6.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E4.m1.1.1.3.3.3" xref="S6.E4.m1.1.1.3.3.3.cmml">N</mi></mrow><mo id="S6.E4.m1.1.1.3.1a" xref="S6.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S6.E4.m1.1.1.3.4" xref="S6.E4.m1.1.1.3.4.cmml"><mi id="S6.E4.m1.1.1.3.4.2" xref="S6.E4.m1.1.1.3.4.2.cmml">T</mi><mo id="S6.E4.m1.1.1.3.4.1" xref="S6.E4.m1.1.1.3.4.1.cmml">⁢</mo><mi id="S6.E4.m1.1.1.3.4.3" xref="S6.E4.m1.1.1.3.4.3.cmml">N</mi></mrow><mo id="S6.E4.m1.1.1.3.1b" xref="S6.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S6.E4.m1.1.1.3.5" xref="S6.E4.m1.1.1.3.5.cmml"><mi id="S6.E4.m1.1.1.3.5.2" xref="S6.E4.m1.1.1.3.5.2.cmml">F</mi><mo id="S6.E4.m1.1.1.3.5.1" xref="S6.E4.m1.1.1.3.5.1.cmml">⁢</mo><mi id="S6.E4.m1.1.1.3.5.3" xref="S6.E4.m1.1.1.3.5.3.cmml">P</mi></mrow></mrow></mfrac><mo id="S6.E4.m1.2.2.1.1.3.2.2" xref="S6.E4.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S6.E4.m1.2.2.1.2" lspace="0em" xref="S6.E4.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E4.m1.2b"><apply id="S6.E4.m1.2.2.1.1.cmml" xref="S6.E4.m1.2.2.1"><eq id="S6.E4.m1.2.2.1.1.1.cmml" xref="S6.E4.m1.2.2.1.1.1"></eq><apply id="S6.E4.m1.2.2.1.1.2.cmml" xref="S6.E4.m1.2.2.1.1.2"><times id="S6.E4.m1.2.2.1.1.2.1.cmml" xref="S6.E4.m1.2.2.1.1.2.1"></times><ci id="S6.E4.m1.2.2.1.1.2.2.cmml" xref="S6.E4.m1.2.2.1.1.2.2">𝐴</ci><ci id="S6.E4.m1.2.2.1.1.2.3.cmml" xref="S6.E4.m1.2.2.1.1.2.3">𝑐</ci><ci id="S6.E4.m1.2.2.1.1.2.4.cmml" xref="S6.E4.m1.2.2.1.1.2.4">𝑐</ci><ci id="S6.E4.m1.2.2.1.1.2.5.cmml" xref="S6.E4.m1.2.2.1.1.2.5">𝑢</ci><ci id="S6.E4.m1.2.2.1.1.2.6.cmml" xref="S6.E4.m1.2.2.1.1.2.6">𝑟</ci><ci id="S6.E4.m1.2.2.1.1.2.7.cmml" xref="S6.E4.m1.2.2.1.1.2.7">𝑎</ci><ci id="S6.E4.m1.2.2.1.1.2.8.cmml" xref="S6.E4.m1.2.2.1.1.2.8">𝑐</ci><ci id="S6.E4.m1.2.2.1.1.2.9.cmml" xref="S6.E4.m1.2.2.1.1.2.9">𝑦</ci></apply><apply id="S6.E4.m1.1.1.cmml" xref="S6.E4.m1.2.2.1.1.3.2"><csymbol cd="latexml" id="S6.E4.m1.1.1.1.cmml" xref="S6.E4.m1.2.2.1.1.3.2">continued-fraction</csymbol><apply id="S6.E4.m1.1.1.2.cmml" xref="S6.E4.m1.1.1.2"><plus id="S6.E4.m1.1.1.2.1.cmml" xref="S6.E4.m1.1.1.2.1"></plus><apply id="S6.E4.m1.1.1.2.2.cmml" xref="S6.E4.m1.1.1.2.2"><times id="S6.E4.m1.1.1.2.2.1.cmml" xref="S6.E4.m1.1.1.2.2.1"></times><ci id="S6.E4.m1.1.1.2.2.2.cmml" xref="S6.E4.m1.1.1.2.2.2">𝑇</ci><ci id="S6.E4.m1.1.1.2.2.3.cmml" xref="S6.E4.m1.1.1.2.2.3">𝑃</ci></apply><apply id="S6.E4.m1.1.1.2.3.cmml" xref="S6.E4.m1.1.1.2.3"><times id="S6.E4.m1.1.1.2.3.1.cmml" xref="S6.E4.m1.1.1.2.3.1"></times><ci id="S6.E4.m1.1.1.2.3.2.cmml" xref="S6.E4.m1.1.1.2.3.2">𝑇</ci><ci id="S6.E4.m1.1.1.2.3.3.cmml" xref="S6.E4.m1.1.1.2.3.3">𝑁</ci></apply></apply><apply id="S6.E4.m1.1.1.3.cmml" xref="S6.E4.m1.1.1.3"><plus id="S6.E4.m1.1.1.3.1.cmml" xref="S6.E4.m1.1.1.3.1"></plus><apply id="S6.E4.m1.1.1.3.2.cmml" xref="S6.E4.m1.1.1.3.2"><times id="S6.E4.m1.1.1.3.2.1.cmml" xref="S6.E4.m1.1.1.3.2.1"></times><ci id="S6.E4.m1.1.1.3.2.2.cmml" xref="S6.E4.m1.1.1.3.2.2">𝑇</ci><ci id="S6.E4.m1.1.1.3.2.3.cmml" xref="S6.E4.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S6.E4.m1.1.1.3.3.cmml" xref="S6.E4.m1.1.1.3.3"><times id="S6.E4.m1.1.1.3.3.1.cmml" xref="S6.E4.m1.1.1.3.3.1"></times><ci id="S6.E4.m1.1.1.3.3.2.cmml" xref="S6.E4.m1.1.1.3.3.2">𝐹</ci><ci id="S6.E4.m1.1.1.3.3.3.cmml" xref="S6.E4.m1.1.1.3.3.3">𝑁</ci></apply><apply id="S6.E4.m1.1.1.3.4.cmml" xref="S6.E4.m1.1.1.3.4"><times id="S6.E4.m1.1.1.3.4.1.cmml" xref="S6.E4.m1.1.1.3.4.1"></times><ci id="S6.E4.m1.1.1.3.4.2.cmml" xref="S6.E4.m1.1.1.3.4.2">𝑇</ci><ci id="S6.E4.m1.1.1.3.4.3.cmml" xref="S6.E4.m1.1.1.3.4.3">𝑁</ci></apply><apply id="S6.E4.m1.1.1.3.5.cmml" xref="S6.E4.m1.1.1.3.5"><times id="S6.E4.m1.1.1.3.5.1.cmml" xref="S6.E4.m1.1.1.3.5.1"></times><ci id="S6.E4.m1.1.1.3.5.2.cmml" xref="S6.E4.m1.1.1.3.5.2">𝐹</ci><ci id="S6.E4.m1.1.1.3.5.3.cmml" xref="S6.E4.m1.1.1.3.5.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E4.m1.2c">Accuracy=\left(\cfrac{TP+TN}{TP+FN+TN+FP}\right).</annotation><annotation encoding="application/x-llamapun" id="S6.E4.m1.2d">italic_A italic_c italic_c italic_u italic_r italic_a italic_c italic_y = ( continued-fraction start_ARG italic_T italic_P + italic_T italic_N end_ARG start_ARG italic_T italic_P + italic_F italic_N + italic_T italic_N + italic_F italic_P end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In the works investigated in section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4" title="IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">IV</span></a>, accuracy is used as the primary performance measure. However, caution should be taken with unbalanced class distributions between drowsiness and alert labels. In cases where the evaluation data is imbalanced with respect to these labels, we advocate the use of the balanced accuracy that takes this imbalance in the class distribution into account. The mathematical equation is given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Balanced-Accuracy=0.5*\left(\cfrac{TP}{TP+FN}+\cfrac{TN}{TN+FP}\right)." class="ltx_Math" display="block" id="S6.E5.m1.1"><semantics id="S6.E5.m1.1a"><mrow id="S6.E5.m1.1.1.1" xref="S6.E5.m1.1.1.1.1.cmml"><mrow id="S6.E5.m1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.cmml"><mrow id="S6.E5.m1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.3.cmml"><mrow id="S6.E5.m1.1.1.1.1.3.2" xref="S6.E5.m1.1.1.1.1.3.2.cmml"><mi id="S6.E5.m1.1.1.1.1.3.2.2" xref="S6.E5.m1.1.1.1.1.3.2.2.cmml">B</mi><mo id="S6.E5.m1.1.1.1.1.3.2.1" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.2.3" xref="S6.E5.m1.1.1.1.1.3.2.3.cmml">a</mi><mo id="S6.E5.m1.1.1.1.1.3.2.1a" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.2.4" xref="S6.E5.m1.1.1.1.1.3.2.4.cmml">l</mi><mo id="S6.E5.m1.1.1.1.1.3.2.1b" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.2.5" xref="S6.E5.m1.1.1.1.1.3.2.5.cmml">a</mi><mo id="S6.E5.m1.1.1.1.1.3.2.1c" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.2.6" xref="S6.E5.m1.1.1.1.1.3.2.6.cmml">n</mi><mo id="S6.E5.m1.1.1.1.1.3.2.1d" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.2.7" xref="S6.E5.m1.1.1.1.1.3.2.7.cmml">c</mi><mo id="S6.E5.m1.1.1.1.1.3.2.1e" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.2.8" xref="S6.E5.m1.1.1.1.1.3.2.8.cmml">e</mi><mo id="S6.E5.m1.1.1.1.1.3.2.1f" xref="S6.E5.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.2.9" xref="S6.E5.m1.1.1.1.1.3.2.9.cmml">d</mi></mrow><mo id="S6.E5.m1.1.1.1.1.3.1" xref="S6.E5.m1.1.1.1.1.3.1.cmml">−</mo><mrow id="S6.E5.m1.1.1.1.1.3.3" xref="S6.E5.m1.1.1.1.1.3.3.cmml"><mi id="S6.E5.m1.1.1.1.1.3.3.2" xref="S6.E5.m1.1.1.1.1.3.3.2.cmml">A</mi><mo id="S6.E5.m1.1.1.1.1.3.3.1" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.3.3" xref="S6.E5.m1.1.1.1.1.3.3.3.cmml">c</mi><mo id="S6.E5.m1.1.1.1.1.3.3.1a" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.3.4" xref="S6.E5.m1.1.1.1.1.3.3.4.cmml">c</mi><mo id="S6.E5.m1.1.1.1.1.3.3.1b" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.3.5" xref="S6.E5.m1.1.1.1.1.3.3.5.cmml">u</mi><mo id="S6.E5.m1.1.1.1.1.3.3.1c" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.3.6" xref="S6.E5.m1.1.1.1.1.3.3.6.cmml">r</mi><mo id="S6.E5.m1.1.1.1.1.3.3.1d" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.3.7" xref="S6.E5.m1.1.1.1.1.3.3.7.cmml">a</mi><mo id="S6.E5.m1.1.1.1.1.3.3.1e" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.3.8" xref="S6.E5.m1.1.1.1.1.3.3.8.cmml">c</mi><mo id="S6.E5.m1.1.1.1.1.3.3.1f" xref="S6.E5.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.3.3.9" xref="S6.E5.m1.1.1.1.1.3.3.9.cmml">y</mi></mrow></mrow><mo id="S6.E5.m1.1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S6.E5.m1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.cmml"><mn id="S6.E5.m1.1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.1.3.cmml">0.5</mn><mo id="S6.E5.m1.1.1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S6.E5.m1.1.1.1.1.1.2.cmml">∗</mo><mrow id="S6.E5.m1.1.1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S6.E5.m1.1.1.1.1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S6.E5.m1.1.1.1.1.1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.cmml"><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">T</mi><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">P</mi></mrow><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml">T</mi><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml">P</mi></mrow><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">+</mo><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.2.cmml">F</mi><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.3.cmml">N</mi></mrow></mrow></mfrac><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S6.E5.m1.1.1.1.1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.cmml"><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">T</mi><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">N</mi></mrow><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.2.cmml">T</mi><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.3.cmml">N</mi></mrow><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">+</mo><mrow id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">F</mi><mo id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.1" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml">P</mi></mrow></mrow></mfrac></mrow><mo id="S6.E5.m1.1.1.1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S6.E5.m1.1.1.1.2" lspace="0em" xref="S6.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E5.m1.1b"><apply id="S6.E5.m1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1"><eq id="S6.E5.m1.1.1.1.1.2.cmml" xref="S6.E5.m1.1.1.1.1.2"></eq><apply id="S6.E5.m1.1.1.1.1.3.cmml" xref="S6.E5.m1.1.1.1.1.3"><minus id="S6.E5.m1.1.1.1.1.3.1.cmml" xref="S6.E5.m1.1.1.1.1.3.1"></minus><apply id="S6.E5.m1.1.1.1.1.3.2.cmml" xref="S6.E5.m1.1.1.1.1.3.2"><times id="S6.E5.m1.1.1.1.1.3.2.1.cmml" xref="S6.E5.m1.1.1.1.1.3.2.1"></times><ci id="S6.E5.m1.1.1.1.1.3.2.2.cmml" xref="S6.E5.m1.1.1.1.1.3.2.2">𝐵</ci><ci id="S6.E5.m1.1.1.1.1.3.2.3.cmml" xref="S6.E5.m1.1.1.1.1.3.2.3">𝑎</ci><ci id="S6.E5.m1.1.1.1.1.3.2.4.cmml" xref="S6.E5.m1.1.1.1.1.3.2.4">𝑙</ci><ci id="S6.E5.m1.1.1.1.1.3.2.5.cmml" xref="S6.E5.m1.1.1.1.1.3.2.5">𝑎</ci><ci id="S6.E5.m1.1.1.1.1.3.2.6.cmml" xref="S6.E5.m1.1.1.1.1.3.2.6">𝑛</ci><ci id="S6.E5.m1.1.1.1.1.3.2.7.cmml" xref="S6.E5.m1.1.1.1.1.3.2.7">𝑐</ci><ci id="S6.E5.m1.1.1.1.1.3.2.8.cmml" xref="S6.E5.m1.1.1.1.1.3.2.8">𝑒</ci><ci id="S6.E5.m1.1.1.1.1.3.2.9.cmml" xref="S6.E5.m1.1.1.1.1.3.2.9">𝑑</ci></apply><apply id="S6.E5.m1.1.1.1.1.3.3.cmml" xref="S6.E5.m1.1.1.1.1.3.3"><times id="S6.E5.m1.1.1.1.1.3.3.1.cmml" xref="S6.E5.m1.1.1.1.1.3.3.1"></times><ci id="S6.E5.m1.1.1.1.1.3.3.2.cmml" xref="S6.E5.m1.1.1.1.1.3.3.2">𝐴</ci><ci id="S6.E5.m1.1.1.1.1.3.3.3.cmml" xref="S6.E5.m1.1.1.1.1.3.3.3">𝑐</ci><ci id="S6.E5.m1.1.1.1.1.3.3.4.cmml" xref="S6.E5.m1.1.1.1.1.3.3.4">𝑐</ci><ci id="S6.E5.m1.1.1.1.1.3.3.5.cmml" xref="S6.E5.m1.1.1.1.1.3.3.5">𝑢</ci><ci id="S6.E5.m1.1.1.1.1.3.3.6.cmml" xref="S6.E5.m1.1.1.1.1.3.3.6">𝑟</ci><ci id="S6.E5.m1.1.1.1.1.3.3.7.cmml" xref="S6.E5.m1.1.1.1.1.3.3.7">𝑎</ci><ci id="S6.E5.m1.1.1.1.1.3.3.8.cmml" xref="S6.E5.m1.1.1.1.1.3.3.8">𝑐</ci><ci id="S6.E5.m1.1.1.1.1.3.3.9.cmml" xref="S6.E5.m1.1.1.1.1.3.3.9">𝑦</ci></apply></apply><apply id="S6.E5.m1.1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1.1.1"><times id="S6.E5.m1.1.1.1.1.1.2.cmml" xref="S6.E5.m1.1.1.1.1.1.2"></times><cn id="S6.E5.m1.1.1.1.1.1.3.cmml" type="float" xref="S6.E5.m1.1.1.1.1.1.3">0.5</cn><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1"><plus id="S6.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S6.E5.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2">continued-fraction</csymbol><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.2">𝑇</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.2.3">𝑃</ci></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3"><plus id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.1"></plus><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.2">𝑇</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.2.3">𝑃</ci></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.2">𝐹</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.2.3.3.3">𝑁</ci></apply></apply></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="latexml" id="S6.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3">continued-fraction</csymbol><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.2">𝑇</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.2.3">𝑁</ci></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3"><plus id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.1"></plus><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.2">𝑇</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.2.3">𝑁</ci></apply><apply id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3"><times id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.1"></times><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.2">𝐹</ci><ci id="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1.1.3.3.3.3">𝑃</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E5.m1.1c">Balanced-Accuracy=0.5*\left(\cfrac{TP}{TP+FN}+\cfrac{TN}{TN+FP}\right).</annotation><annotation encoding="application/x-llamapun" id="S6.E5.m1.1d">italic_B italic_a italic_l italic_a italic_n italic_c italic_e italic_d - italic_A italic_c italic_c italic_u italic_r italic_a italic_c italic_y = 0.5 ∗ ( continued-fraction start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_N end_ARG + continued-fraction start_ARG italic_T italic_N end_ARG start_ARG italic_T italic_N + italic_F italic_P end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Further metrics include area under the receiver operating characteristic (ROC) Curve (AUC-ROC) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib139" title="">139</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib140" title="">140</a>]</cite> which evaluates the classifier’s ability to distinguish between drowsy and non-drowsy instances across different threshold values and Mean Squared Error (MSE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib141" title="">141</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib142" title="">142</a>]</cite> or Root Mean Squared Error (RMSE)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib143" title="">143</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib144" title="">144</a>]</cite> which quantifies the average squared differences between predicted drowsiness levels and actual levels. Dua et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib145" title="">145</a>]</cite> reported their results further in terms of a confusion matrix. This tabular representation comprises all four metrics (TP, TN, FP, and FN) representing the performance of the binary drowsiness detection classifier. Based on the four metrics, the authors also provide evaluation metrics such as <span class="ltx_text ltx_font_italic" id="S6.p3.1.1">sensitivity, specificity, precision</span>, and <span class="ltx_text ltx_font_italic" id="S6.p3.1.2">F1-score</span>. The mathematical formulation for <span class="ltx_text ltx_font_italic" id="S6.p3.1.3">specificity</span> is given in equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S6.E6" title="In VI Performance and Evaluation Metrics ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">6</span></a>) as:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Specificity=\left(\cfrac{TN}{TN+FP}\right)." class="ltx_Math" display="block" id="S6.E6.m1.2"><semantics id="S6.E6.m1.2a"><mrow id="S6.E6.m1.2.2.1" xref="S6.E6.m1.2.2.1.1.cmml"><mrow id="S6.E6.m1.2.2.1.1" xref="S6.E6.m1.2.2.1.1.cmml"><mrow id="S6.E6.m1.2.2.1.1.2" xref="S6.E6.m1.2.2.1.1.2.cmml"><mi id="S6.E6.m1.2.2.1.1.2.2" xref="S6.E6.m1.2.2.1.1.2.2.cmml">S</mi><mo id="S6.E6.m1.2.2.1.1.2.1" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.3" xref="S6.E6.m1.2.2.1.1.2.3.cmml">p</mi><mo id="S6.E6.m1.2.2.1.1.2.1a" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.4" xref="S6.E6.m1.2.2.1.1.2.4.cmml">e</mi><mo id="S6.E6.m1.2.2.1.1.2.1b" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.5" xref="S6.E6.m1.2.2.1.1.2.5.cmml">c</mi><mo id="S6.E6.m1.2.2.1.1.2.1c" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.6" xref="S6.E6.m1.2.2.1.1.2.6.cmml">i</mi><mo id="S6.E6.m1.2.2.1.1.2.1d" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.7" xref="S6.E6.m1.2.2.1.1.2.7.cmml">f</mi><mo id="S6.E6.m1.2.2.1.1.2.1e" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.8" xref="S6.E6.m1.2.2.1.1.2.8.cmml">i</mi><mo id="S6.E6.m1.2.2.1.1.2.1f" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.9" xref="S6.E6.m1.2.2.1.1.2.9.cmml">c</mi><mo id="S6.E6.m1.2.2.1.1.2.1g" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.10" xref="S6.E6.m1.2.2.1.1.2.10.cmml">i</mi><mo id="S6.E6.m1.2.2.1.1.2.1h" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.11" xref="S6.E6.m1.2.2.1.1.2.11.cmml">t</mi><mo id="S6.E6.m1.2.2.1.1.2.1i" xref="S6.E6.m1.2.2.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.2.2.1.1.2.12" xref="S6.E6.m1.2.2.1.1.2.12.cmml">y</mi></mrow><mo id="S6.E6.m1.2.2.1.1.1" xref="S6.E6.m1.2.2.1.1.1.cmml">=</mo><mrow id="S6.E6.m1.2.2.1.1.3.2" xref="S6.E6.m1.1.1.cmml"><mo id="S6.E6.m1.2.2.1.1.3.2.1" xref="S6.E6.m1.1.1.cmml">(</mo><mfrac id="S6.E6.m1.1.1" xref="S6.E6.m1.1.1.cmml"><mrow id="S6.E6.m1.1.1.2" xref="S6.E6.m1.1.1.2.cmml"><mi id="S6.E6.m1.1.1.2.2" xref="S6.E6.m1.1.1.2.2.cmml">T</mi><mo id="S6.E6.m1.1.1.2.1" xref="S6.E6.m1.1.1.2.1.cmml">⁢</mo><mi id="S6.E6.m1.1.1.2.3" xref="S6.E6.m1.1.1.2.3.cmml">N</mi></mrow><mrow id="S6.E6.m1.1.1.3" xref="S6.E6.m1.1.1.3.cmml"><mrow id="S6.E6.m1.1.1.3.2" xref="S6.E6.m1.1.1.3.2.cmml"><mi id="S6.E6.m1.1.1.3.2.2" xref="S6.E6.m1.1.1.3.2.2.cmml">T</mi><mo id="S6.E6.m1.1.1.3.2.1" xref="S6.E6.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S6.E6.m1.1.1.3.2.3" xref="S6.E6.m1.1.1.3.2.3.cmml">N</mi></mrow><mo id="S6.E6.m1.1.1.3.1" xref="S6.E6.m1.1.1.3.1.cmml">+</mo><mrow id="S6.E6.m1.1.1.3.3" xref="S6.E6.m1.1.1.3.3.cmml"><mi id="S6.E6.m1.1.1.3.3.2" xref="S6.E6.m1.1.1.3.3.2.cmml">F</mi><mo id="S6.E6.m1.1.1.3.3.1" xref="S6.E6.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.E6.m1.1.1.3.3.3" xref="S6.E6.m1.1.1.3.3.3.cmml">P</mi></mrow></mrow></mfrac><mo id="S6.E6.m1.2.2.1.1.3.2.2" xref="S6.E6.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S6.E6.m1.2.2.1.2" lspace="0em" xref="S6.E6.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E6.m1.2b"><apply id="S6.E6.m1.2.2.1.1.cmml" xref="S6.E6.m1.2.2.1"><eq id="S6.E6.m1.2.2.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1"></eq><apply id="S6.E6.m1.2.2.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.2"><times id="S6.E6.m1.2.2.1.1.2.1.cmml" xref="S6.E6.m1.2.2.1.1.2.1"></times><ci id="S6.E6.m1.2.2.1.1.2.2.cmml" xref="S6.E6.m1.2.2.1.1.2.2">𝑆</ci><ci id="S6.E6.m1.2.2.1.1.2.3.cmml" xref="S6.E6.m1.2.2.1.1.2.3">𝑝</ci><ci id="S6.E6.m1.2.2.1.1.2.4.cmml" xref="S6.E6.m1.2.2.1.1.2.4">𝑒</ci><ci id="S6.E6.m1.2.2.1.1.2.5.cmml" xref="S6.E6.m1.2.2.1.1.2.5">𝑐</ci><ci id="S6.E6.m1.2.2.1.1.2.6.cmml" xref="S6.E6.m1.2.2.1.1.2.6">𝑖</ci><ci id="S6.E6.m1.2.2.1.1.2.7.cmml" xref="S6.E6.m1.2.2.1.1.2.7">𝑓</ci><ci id="S6.E6.m1.2.2.1.1.2.8.cmml" xref="S6.E6.m1.2.2.1.1.2.8">𝑖</ci><ci id="S6.E6.m1.2.2.1.1.2.9.cmml" xref="S6.E6.m1.2.2.1.1.2.9">𝑐</ci><ci id="S6.E6.m1.2.2.1.1.2.10.cmml" xref="S6.E6.m1.2.2.1.1.2.10">𝑖</ci><ci id="S6.E6.m1.2.2.1.1.2.11.cmml" xref="S6.E6.m1.2.2.1.1.2.11">𝑡</ci><ci id="S6.E6.m1.2.2.1.1.2.12.cmml" xref="S6.E6.m1.2.2.1.1.2.12">𝑦</ci></apply><apply id="S6.E6.m1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.3.2"><csymbol cd="latexml" id="S6.E6.m1.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.3.2">continued-fraction</csymbol><apply id="S6.E6.m1.1.1.2.cmml" xref="S6.E6.m1.1.1.2"><times id="S6.E6.m1.1.1.2.1.cmml" xref="S6.E6.m1.1.1.2.1"></times><ci id="S6.E6.m1.1.1.2.2.cmml" xref="S6.E6.m1.1.1.2.2">𝑇</ci><ci id="S6.E6.m1.1.1.2.3.cmml" xref="S6.E6.m1.1.1.2.3">𝑁</ci></apply><apply id="S6.E6.m1.1.1.3.cmml" xref="S6.E6.m1.1.1.3"><plus id="S6.E6.m1.1.1.3.1.cmml" xref="S6.E6.m1.1.1.3.1"></plus><apply id="S6.E6.m1.1.1.3.2.cmml" xref="S6.E6.m1.1.1.3.2"><times id="S6.E6.m1.1.1.3.2.1.cmml" xref="S6.E6.m1.1.1.3.2.1"></times><ci id="S6.E6.m1.1.1.3.2.2.cmml" xref="S6.E6.m1.1.1.3.2.2">𝑇</ci><ci id="S6.E6.m1.1.1.3.2.3.cmml" xref="S6.E6.m1.1.1.3.2.3">𝑁</ci></apply><apply id="S6.E6.m1.1.1.3.3.cmml" xref="S6.E6.m1.1.1.3.3"><times id="S6.E6.m1.1.1.3.3.1.cmml" xref="S6.E6.m1.1.1.3.3.1"></times><ci id="S6.E6.m1.1.1.3.3.2.cmml" xref="S6.E6.m1.1.1.3.3.2">𝐹</ci><ci id="S6.E6.m1.1.1.3.3.3.cmml" xref="S6.E6.m1.1.1.3.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E6.m1.2c">Specificity=\left(\cfrac{TN}{TN+FP}\right).</annotation><annotation encoding="application/x-llamapun" id="S6.E6.m1.2d">italic_S italic_p italic_e italic_c italic_i italic_f italic_i italic_c italic_i italic_t italic_y = ( continued-fraction start_ARG italic_T italic_N end_ARG start_ARG italic_T italic_N + italic_F italic_P end_ARG ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S6.p3.2">It is noteworthy, that the term <span class="ltx_text ltx_font_italic" id="S6.p3.2.1">sensitivity</span> is commonly used as a synonym to the term <span class="ltx_text ltx_font_italic" id="S6.p3.2.2">recall</span> and thus there is no need to duplicate the equation. Some works also simply reported the ”Accuracy” of their algorithm. <span class="ltx_text ltx_font_italic" id="S6.p3.2.3">Accuracy</span> in this sense is dependent on the FN and FP, but unlike <span class="ltx_text ltx_font_italic" id="S6.p3.2.4">F1-score</span>, does not consider the possible imbalance in the data. Even though it is not reported in detail in most works, the accuracy can be given simply by the ratio of misclassified samples to all evaluated samples, regardless of their ground-truth label balance.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.4">Another performance metric, when driver drowsiness detection is considered as a regression task instead of a classification task, is the case to predict the accuracy of continuous drowsiness level. Such a performance measure proposed by Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib146" title="">146</a>]</cite> is called the drowsiness index (DI). They measured the difference between the individual response time (RT) and the true alert state (alert RT) in a simulated lane-keeping task. The formulation is given by equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S6.E7" title="In VI Performance and Evaluation Metrics ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">7</span></a>) as:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="DI=max\biggl{(}0,\cfrac{1-e^{-\alpha(\tau-\tau_{0})}}{1+e^{-\alpha(\tau-\tau_{%
0})}}\biggr{)}," class="ltx_Math" display="block" id="S6.E7.m1.4"><semantics id="S6.E7.m1.4a"><mrow id="S6.E7.m1.4.4.1" xref="S6.E7.m1.4.4.1.1.cmml"><mrow id="S6.E7.m1.4.4.1.1" xref="S6.E7.m1.4.4.1.1.cmml"><mrow id="S6.E7.m1.4.4.1.1.2" xref="S6.E7.m1.4.4.1.1.2.cmml"><mi id="S6.E7.m1.4.4.1.1.2.2" xref="S6.E7.m1.4.4.1.1.2.2.cmml">D</mi><mo id="S6.E7.m1.4.4.1.1.2.1" xref="S6.E7.m1.4.4.1.1.2.1.cmml">⁢</mo><mi id="S6.E7.m1.4.4.1.1.2.3" xref="S6.E7.m1.4.4.1.1.2.3.cmml">I</mi></mrow><mo id="S6.E7.m1.4.4.1.1.1" xref="S6.E7.m1.4.4.1.1.1.cmml">=</mo><mrow id="S6.E7.m1.4.4.1.1.3" xref="S6.E7.m1.4.4.1.1.3.cmml"><mi id="S6.E7.m1.4.4.1.1.3.2" xref="S6.E7.m1.4.4.1.1.3.2.cmml">m</mi><mo id="S6.E7.m1.4.4.1.1.3.1" xref="S6.E7.m1.4.4.1.1.3.1.cmml">⁢</mo><mi id="S6.E7.m1.4.4.1.1.3.3" xref="S6.E7.m1.4.4.1.1.3.3.cmml">a</mi><mo id="S6.E7.m1.4.4.1.1.3.1a" xref="S6.E7.m1.4.4.1.1.3.1.cmml">⁢</mo><mi id="S6.E7.m1.4.4.1.1.3.4" xref="S6.E7.m1.4.4.1.1.3.4.cmml">x</mi><mo id="S6.E7.m1.4.4.1.1.3.1b" xref="S6.E7.m1.4.4.1.1.3.1.cmml">⁢</mo><mrow id="S6.E7.m1.4.4.1.1.3.5.2" xref="S6.E7.m1.4.4.1.1.3.5.1.cmml"><mo id="S6.E7.m1.4.4.1.1.3.5.2.1" maxsize="210%" minsize="210%" xref="S6.E7.m1.4.4.1.1.3.5.1.cmml">(</mo><mn id="S6.E7.m1.3.3" xref="S6.E7.m1.3.3.cmml">0</mn><mo id="S6.E7.m1.4.4.1.1.3.5.2.2" xref="S6.E7.m1.4.4.1.1.3.5.1.cmml">,</mo><mfrac id="S6.E7.m1.2.2" xref="S6.E7.m1.2.2.cmml"><mrow id="S6.E7.m1.1.1.1" xref="S6.E7.m1.1.1.1.cmml"><mn id="S6.E7.m1.1.1.1.3" xref="S6.E7.m1.1.1.1.3.cmml">1</mn><mo id="S6.E7.m1.1.1.1.2" xref="S6.E7.m1.1.1.1.2.cmml">−</mo><msup id="S6.E7.m1.1.1.1.4" xref="S6.E7.m1.1.1.1.4.cmml"><mi id="S6.E7.m1.1.1.1.4.2" xref="S6.E7.m1.1.1.1.4.2.cmml">e</mi><mrow id="S6.E7.m1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.cmml"><mo id="S6.E7.m1.1.1.1.1.1a" xref="S6.E7.m1.1.1.1.1.1.cmml">−</mo><mrow id="S6.E7.m1.1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.1.cmml"><mi id="S6.E7.m1.1.1.1.1.1.1.3" xref="S6.E7.m1.1.1.1.1.1.1.3.cmml">α</mi><mo id="S6.E7.m1.1.1.1.1.1.1.2" xref="S6.E7.m1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S6.E7.m1.1.1.1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S6.E7.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E7.m1.1.1.1.1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.E7.m1.1.1.1.1.1.1.1.1.1.2" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.2.cmml">τ</mi><mo id="S6.E7.m1.1.1.1.1.1.1.1.1.1.1" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">τ</mi><mn id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S6.E7.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msup></mrow><mrow id="S6.E7.m1.2.2.2" xref="S6.E7.m1.2.2.2.cmml"><mn id="S6.E7.m1.2.2.2.3" xref="S6.E7.m1.2.2.2.3.cmml">1</mn><mo id="S6.E7.m1.2.2.2.2" xref="S6.E7.m1.2.2.2.2.cmml">+</mo><msup id="S6.E7.m1.2.2.2.4" xref="S6.E7.m1.2.2.2.4.cmml"><mi id="S6.E7.m1.2.2.2.4.2" xref="S6.E7.m1.2.2.2.4.2.cmml">e</mi><mrow id="S6.E7.m1.2.2.2.1.1" xref="S6.E7.m1.2.2.2.1.1.cmml"><mo id="S6.E7.m1.2.2.2.1.1a" xref="S6.E7.m1.2.2.2.1.1.cmml">−</mo><mrow id="S6.E7.m1.2.2.2.1.1.1" xref="S6.E7.m1.2.2.2.1.1.1.cmml"><mi id="S6.E7.m1.2.2.2.1.1.1.3" xref="S6.E7.m1.2.2.2.1.1.1.3.cmml">α</mi><mo id="S6.E7.m1.2.2.2.1.1.1.2" xref="S6.E7.m1.2.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S6.E7.m1.2.2.2.1.1.1.1.1" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml"><mo id="S6.E7.m1.2.2.2.1.1.1.1.1.2" stretchy="false" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E7.m1.2.2.2.1.1.1.1.1.1" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml"><mi id="S6.E7.m1.2.2.2.1.1.1.1.1.1.2" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.2.cmml">τ</mi><mo id="S6.E7.m1.2.2.2.1.1.1.1.1.1.1" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.1.cmml">−</mo><msub id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.cmml"><mi id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.2" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.2.cmml">τ</mi><mn id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.3" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S6.E7.m1.2.2.2.1.1.1.1.1.3" stretchy="false" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msup></mrow></mfrac><mo id="S6.E7.m1.4.4.1.1.3.5.2.3" maxsize="210%" minsize="210%" xref="S6.E7.m1.4.4.1.1.3.5.1.cmml">)</mo></mrow></mrow></mrow><mo id="S6.E7.m1.4.4.1.2" xref="S6.E7.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E7.m1.4b"><apply id="S6.E7.m1.4.4.1.1.cmml" xref="S6.E7.m1.4.4.1"><eq id="S6.E7.m1.4.4.1.1.1.cmml" xref="S6.E7.m1.4.4.1.1.1"></eq><apply id="S6.E7.m1.4.4.1.1.2.cmml" xref="S6.E7.m1.4.4.1.1.2"><times id="S6.E7.m1.4.4.1.1.2.1.cmml" xref="S6.E7.m1.4.4.1.1.2.1"></times><ci id="S6.E7.m1.4.4.1.1.2.2.cmml" xref="S6.E7.m1.4.4.1.1.2.2">𝐷</ci><ci id="S6.E7.m1.4.4.1.1.2.3.cmml" xref="S6.E7.m1.4.4.1.1.2.3">𝐼</ci></apply><apply id="S6.E7.m1.4.4.1.1.3.cmml" xref="S6.E7.m1.4.4.1.1.3"><times id="S6.E7.m1.4.4.1.1.3.1.cmml" xref="S6.E7.m1.4.4.1.1.3.1"></times><ci id="S6.E7.m1.4.4.1.1.3.2.cmml" xref="S6.E7.m1.4.4.1.1.3.2">𝑚</ci><ci id="S6.E7.m1.4.4.1.1.3.3.cmml" xref="S6.E7.m1.4.4.1.1.3.3">𝑎</ci><ci id="S6.E7.m1.4.4.1.1.3.4.cmml" xref="S6.E7.m1.4.4.1.1.3.4">𝑥</ci><interval closure="open" id="S6.E7.m1.4.4.1.1.3.5.1.cmml" xref="S6.E7.m1.4.4.1.1.3.5.2"><cn id="S6.E7.m1.3.3.cmml" type="integer" xref="S6.E7.m1.3.3">0</cn><apply id="S6.E7.m1.2.2.cmml" xref="S6.E7.m1.2.2"><csymbol cd="latexml" id="S6.E7.m1.2.2.3.cmml" xref="S6.E7.m1.2.2">continued-fraction</csymbol><apply id="S6.E7.m1.1.1.1.cmml" xref="S6.E7.m1.1.1.1"><minus id="S6.E7.m1.1.1.1.2.cmml" xref="S6.E7.m1.1.1.1.2"></minus><cn id="S6.E7.m1.1.1.1.3.cmml" type="integer" xref="S6.E7.m1.1.1.1.3">1</cn><apply id="S6.E7.m1.1.1.1.4.cmml" xref="S6.E7.m1.1.1.1.4"><csymbol cd="ambiguous" id="S6.E7.m1.1.1.1.4.1.cmml" xref="S6.E7.m1.1.1.1.4">superscript</csymbol><ci id="S6.E7.m1.1.1.1.4.2.cmml" xref="S6.E7.m1.1.1.1.4.2">𝑒</ci><apply id="S6.E7.m1.1.1.1.1.1.cmml" xref="S6.E7.m1.1.1.1.1.1"><minus id="S6.E7.m1.1.1.1.1.1.2.cmml" xref="S6.E7.m1.1.1.1.1.1"></minus><apply id="S6.E7.m1.1.1.1.1.1.1.cmml" xref="S6.E7.m1.1.1.1.1.1.1"><times id="S6.E7.m1.1.1.1.1.1.1.2.cmml" xref="S6.E7.m1.1.1.1.1.1.1.2"></times><ci id="S6.E7.m1.1.1.1.1.1.1.3.cmml" xref="S6.E7.m1.1.1.1.1.1.1.3">𝛼</ci><apply id="S6.E7.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1"><minus id="S6.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S6.E7.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.2">𝜏</ci><apply id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.2">𝜏</ci><cn id="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S6.E7.m1.1.1.1.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply></apply></apply></apply><apply id="S6.E7.m1.2.2.2.cmml" xref="S6.E7.m1.2.2.2"><plus id="S6.E7.m1.2.2.2.2.cmml" xref="S6.E7.m1.2.2.2.2"></plus><cn id="S6.E7.m1.2.2.2.3.cmml" type="integer" xref="S6.E7.m1.2.2.2.3">1</cn><apply id="S6.E7.m1.2.2.2.4.cmml" xref="S6.E7.m1.2.2.2.4"><csymbol cd="ambiguous" id="S6.E7.m1.2.2.2.4.1.cmml" xref="S6.E7.m1.2.2.2.4">superscript</csymbol><ci id="S6.E7.m1.2.2.2.4.2.cmml" xref="S6.E7.m1.2.2.2.4.2">𝑒</ci><apply id="S6.E7.m1.2.2.2.1.1.cmml" xref="S6.E7.m1.2.2.2.1.1"><minus id="S6.E7.m1.2.2.2.1.1.2.cmml" xref="S6.E7.m1.2.2.2.1.1"></minus><apply id="S6.E7.m1.2.2.2.1.1.1.cmml" xref="S6.E7.m1.2.2.2.1.1.1"><times id="S6.E7.m1.2.2.2.1.1.1.2.cmml" xref="S6.E7.m1.2.2.2.1.1.1.2"></times><ci id="S6.E7.m1.2.2.2.1.1.1.3.cmml" xref="S6.E7.m1.2.2.2.1.1.1.3">𝛼</ci><apply id="S6.E7.m1.2.2.2.1.1.1.1.1.1.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1"><minus id="S6.E7.m1.2.2.2.1.1.1.1.1.1.1.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.1"></minus><ci id="S6.E7.m1.2.2.2.1.1.1.1.1.1.2.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.2">𝜏</ci><apply id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.2">𝜏</ci><cn id="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S6.E7.m1.2.2.2.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply></apply></apply></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E7.m1.4c">DI=max\biggl{(}0,\cfrac{1-e^{-\alpha(\tau-\tau_{0})}}{1+e^{-\alpha(\tau-\tau_{%
0})}}\biggr{)},</annotation><annotation encoding="application/x-llamapun" id="S6.E7.m1.4d">italic_D italic_I = italic_m italic_a italic_x ( 0 , continued-fraction start_ARG 1 - italic_e start_POSTSUPERSCRIPT - italic_α ( italic_τ - italic_τ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_α ( italic_τ - italic_τ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S6.p4.3">where <math alttext="\tau" class="ltx_Math" display="inline" id="S6.p4.1.m1.1"><semantics id="S6.p4.1.m1.1a"><mi id="S6.p4.1.m1.1.1" xref="S6.p4.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.p4.1.m1.1b"><ci id="S6.p4.1.m1.1.1.cmml" xref="S6.p4.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S6.p4.1.m1.1d">italic_τ</annotation></semantics></math> denotes the RT of the given lane-departure event, <math alttext="a" class="ltx_Math" display="inline" id="S6.p4.2.m2.1"><semantics id="S6.p4.2.m2.1a"><mi id="S6.p4.2.m2.1.1" xref="S6.p4.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S6.p4.2.m2.1b"><ci id="S6.p4.2.m2.1.1.cmml" xref="S6.p4.2.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.2.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S6.p4.2.m2.1d">italic_a</annotation></semantics></math> is a constant, and <math alttext="\tau_{0}" class="ltx_Math" display="inline" id="S6.p4.3.m3.1"><semantics id="S6.p4.3.m3.1a"><msub id="S6.p4.3.m3.1.1" xref="S6.p4.3.m3.1.1.cmml"><mi id="S6.p4.3.m3.1.1.2" xref="S6.p4.3.m3.1.1.2.cmml">τ</mi><mn id="S6.p4.3.m3.1.1.3" xref="S6.p4.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S6.p4.3.m3.1b"><apply id="S6.p4.3.m3.1.1.cmml" xref="S6.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S6.p4.3.m3.1.1.1.cmml" xref="S6.p4.3.m3.1.1">subscript</csymbol><ci id="S6.p4.3.m3.1.1.2.cmml" xref="S6.p4.3.m3.1.1.2">𝜏</ci><cn id="S6.p4.3.m3.1.1.3.cmml" type="integer" xref="S6.p4.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.3.m3.1c">\tau_{0}</annotation><annotation encoding="application/x-llamapun" id="S6.p4.3.m3.1d">italic_τ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> denotes the true alert RT. In their work, they continuously predict this score and related this measure to the continuous driver’s drowsiness level.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">Paulo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib80" title="">80</a>]</cite> also used a leave-one-subject-out cross-validation strategy to validate their proposed classification method. Because the authors intended to perform classification without individual-dependent calibration from EEG signals, they first carried out the validation at the subject level to understand the individual contribution. By further selecting groups of subjects with major individual contributions, a better cross-subject generalizable model is created.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">These metrics in this section help to assess the effectiveness, accuracy, and reliability of drowsiness detection systems and algorithms, enabling researchers and developers to improve their models for better real-world applicability and safety. The performance metrics used are also reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T1" title="TABLE I ‣ IV-A EEG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">I</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T2" title="TABLE II ‣ IV-B ECG-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">II</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S4.T3" title="TABLE III ‣ IV-C Vision-based Drowsiness Detection ‣ IV Modern Applications and Methods ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">III</span></a>, which show the achieved performance on bench-marking datasets for each of the three measurement techniques considered.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Technical and Practical Limitations</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Application areas of drowsiness detection are broad. It is of paramount importance across diverse domains due to its profound impact on safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib20" title="">20</a>]</cite>, productivity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib30" title="">30</a>]</cite>, and healthcare<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib10" title="">10</a>]</cite>. As extensively studied in this survey, whether on the road, in workplaces, or in critical operational environments, the ability to accurately identify and mitigate drowsiness can have far-reaching implications. In this section, we focus on identifying certain weaknesses in current algorithms and uncovering limitations in existing research. We do this with respect to two main categories, i.e., vision-based and physiological signal-based approaches.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.4.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.5.2">Limitations on vision-based technique</span>
</h3>
<section class="ltx_paragraph" id="S7.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">From the database perspective</h4>
<div class="ltx_para" id="S7.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS1.SSS0.Px1.p1.1">The increasing number of public benchmark databases for driver drowsiness detection is partially due to the rising number of fatal traffic accidents, but also following the trend towards autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib124" title="">124</a>]</cite>. The National Highway Traffic Safety Administration (NHTSA) published the Drowsy Driving Research and Program Plan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib6" title="">6</a>]</cite> in 2016 estimating that 2% to 20% of annual traffic deaths are attributable to driver drowsiness. As we have observed in previous Section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S5" title="V Widely Used Databases ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">V</span></a>, there are several vision-based databases that can be leveraged as benchmarking systems for developing driver drowsiness detection solutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib120" title="">120</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib112" title="">112</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib119" title="">119</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib122" title="">122</a>]</cite>. However, a major limitation is that these databases often contain only simulation data or were collected under strictly controlled environments, such as indoors or in parked vehicles as reviewed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib5" title="">5</a>]</cite>. This makes developing solutions that would work in real life scenarios more challenging. Additionally, it makes estimating the performance in real deployment questionable as the data does not represent all the variations in such scenarios. To alleviate this problem, the trend moves from simulation data to real-world data. Additional databases have been curated that have been collected under real driving conditions and incorporated various aspects of the real environment, such as different lightening and road physics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib123" title="">123</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib124" title="">124</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib126" title="">126</a>]</cite>. However, it is not only tedious but also risky to collect data in a moving vehicle and it is difficult to capture all variability mimicking a real environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib126" title="">126</a>]</cite>. In addition, problems such as occlusion-free capturing and accurate labeling also play an important role, which lead to works as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib119" title="">119</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib147" title="">147</a>]</cite> dealing with capturing of realistic and diverse drowsiness data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">From the algorithmic perspective</h4>
<div class="ltx_para" id="S7.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S7.SS1.SSS0.Px2.p1.1">Most vision-based driver drowsiness detection schemes focus on observing the individual’s facial attributes, such as face expression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib148" title="">148</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib149" title="">149</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib150" title="">150</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib151" title="">151</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib152" title="">152</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib153" title="">153</a>]</cite>, head position <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib154" title="">154</a>]</cite>, pupil diameter state, eye blink and eye movement (PERCLOS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib155" title="">155</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib156" title="">156</a>]</cite>. With the advancement of deep learning, more accurate and efficient extraction of face and facial landmarks becomes possible, making the drowsiness detection on facial features more accurate and real-time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib157" title="">157</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib158" title="">158</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib70" title="">70</a>]</cite>. Popular face detection methods include MTCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib159" title="">159</a>]</cite> and RetinaFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib160" title="">160</a>]</cite> that allow more precise and accurate face detection under more challenging environments. Other approaches like accurate facial landmark detection in the wild <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib161" title="">161</a>]</cite> also help to improve the detection of fine-grained facial expressions under varying and challenging situations.
For integrating face recognition on devices with limited hardware resources, previous works leveraged extremely lightweight face recognition networks from knowledge distillation or model quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib162" title="">162</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib163" title="">163</a>]</cite>. To overcome the limitation of occlusion in vision-based drowsiness detection, research can benefit from methods developed for improving face detection performance under masked faces as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib164" title="">164</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib165" title="">165</a>]</cite>. Working on robust vision-based algorithms coping with variety of challenges faced under real life scenarios is thus a very promising future research direction. Multi angles processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib116" title="">116</a>]</cite> and key frames selection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib117" title="">117</a>]</cite> are also the upcoming challenges for video processing in vision-based drowsiness detection.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">From the biased data perspective</h4>
<div class="ltx_para" id="S7.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S7.SS1.SSS0.Px3.p1.1">Proper datasets play a pivotal role in the training of deep neural networks. When datasets lack representativeness, trained models can become biased and struggle to generalize to real-world scenarios. This concern is particularly pronounced in models trained within specific cultural contexts, potentially leading to inadequate generalization due to limited racial diversity representation. This challenge is amplified in the context of driver drowsiness detection, where publicly available vision-based datasets often focus on specific ethnic groups, resulting in an incomplete picture. Ngxande et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib166" title="">166</a>]</cite> addressed this issue by utilizing a GAN-based method for data augmentation. They used a population bias visualization strategy to group similar facial attributes and highlight the model weaknesses in such samples. The approach involved fine-tuning the CNN model using a sampling technique for faces with subpar performance. Experimental outcomes demonstrated the effectiveness of this approach in enhancing driver drowsiness detection for ethnic groups that were underrepresented. Under the same context of data, studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib167" title="">167</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib168" title="">168</a>]</cite> dealing with how to induce drowsiness and collecting realistic drowsy driving data both in real traffic and under simulation are thus very important for developing robust detection algorithms.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">From on-site hardware limitation perspective</h4>
<div class="ltx_para" id="S7.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S7.SS1.SSS0.Px4.p1.1">The requirement for a real-time and on-site driver monitoring system is crucial to avert motor vehicle accidents attributed to driver inattentiveness or drowsiness. However, onboard hardware often possesses limited computational resources. Recent advancements in deep learning, particularly model compression and distillation techniques, have made it feasible to construct compact yet highly accurate models on embedded systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib170" title="">170</a>]</cite>, such as those integrated into vehicles. Reddy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib24" title="">24</a>]</cite> proposed employing model compression to transition from a resource-intensive baseline model to a lightweight model suitable for deployment on an embedded board device. The suggested model based on facial landmarks achieved an accuracy of 89.5% for a 3-classes classification task, operating at a speed of 14.9 frames per second (FPS) on the Jetson TK1 platform. With the European Union (EU) mandating the introduction of driver drowsiness and alertness warning (DDAW) systems for all new vehicles from 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib167" title="">167</a>]</cite>, the development and installation of accurate and resource-efficient algorithms to detect drowsiness in real time in the vehicle is becoming an urgent issue.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS1.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">From data synthesis perspective</h4>
<div class="ltx_para" id="S7.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S7.SS1.SSS0.Px5.p1.1">Because of the considerable expenses associated with dataset acquisition and the lack of adequate datasets discussed earlier, we propose to use synthetic data to study the common characteristics and the various hidden impacts in data for drowsiness detection. Inspired by the insights gained from the synthetic data, we can further extend to other downstream tasks. This also allows us to uncover different causal aspects for drowsiness detection. Most works focus on finding the correlations in signal variability to the different states of drowsiness, but less on the causality aspects. We consider this to be a very promising research direction with regard to explainability. Kong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib171" title="">171</a>]</cite> proposed to use Granger Causality Network to investigate driver fatigue and alertness state in EEG signals in 2015. The whole experiment included twelve young and healthy participants by recording their mental states under different simulated driving conditions and the data was analyzed by using Granger-Causality-based brain effective networks. Using such foundation models for other tasks, such as training face recognition solutions, have already gained increased interest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib173" title="">173</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.4.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.5.2">Limitations on physiological signal based technique</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Physiological signal based techniques shares, to some degree, most of the limitations related to vision based techniques discussed above. Here, we focus on these limitations specifically linked to the nature of physiological signal based technique.</p>
</div>
<section class="ltx_paragraph" id="S7.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">From motion artifacts perspective</h4>
<div class="ltx_para" id="S7.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS2.SSS0.Px1.p1.1">Multiple studies have investigated the correlation between EEG and driving behavior <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib78" title="">78</a>]</cite>, consistently highlighting EEG as the most predictive physiological indicator of drowsiness. The fluctuation in band activities within EEG signals in the spectral-frequency domain offers dominant insights into varying levels of drowsiness. In contrast to vision-based detection methods that often identify drowsiness after the onset of actual sleep or during advanced drowsiness stages, technologies based on physiological signals enable the early alerting of drivers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib5" title="">5</a>]</cite>, averting potentially catastrophic accidents. However, a challenge arises from relevant signal extraction from motion artifacts by capturing biological signals either due to the motion of vehicles, individuals, or remote capture. This makes developing robust solutions what would work in real driving scenarios more challenging. Therefore, there are extensive approaches dealing with the removal of motion artifacts from EEG recordings. Relevant works include using methods like signal decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib174" title="">174</a>]</cite>, wavelet decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib175" title="">175</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib176" title="">176</a>]</cite>, and detrended fluctuation analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib177" title="">177</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">From data transmission perspective</h4>
<div class="ltx_para" id="S7.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S7.SS2.SSS0.Px2.p1.1">Physiological signals have demonstrated their stability, reliability and precision, as they are less influenced by external factors like e.g., occlusions or variability in lightening, resulting in fewer false positive detections <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib178" title="">178</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib179" title="">179</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib180" title="">180</a>]</cite>. Nevertheless, such physiological sensing methods involving cables or wired electrodes can be obtrusive and inconvenient for signal capture. Consequently, there is a noticeable trend towards wireless sensing and communication to alleviate these issues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib94" title="">94</a>]</cite>. The hurdle lies in maintaining consistent connectivity and addressing weak signal strength in wireless setups. These challenges even exist under laboratory setup and become even more pronounced in uncontrolled environments, particularly when utilizing wearable platforms with a limited count of dry electrodes as stated by Gerwin Schalk<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib181" title="">181</a>]</cite>, a neuroscientist at New York State Department of Health’s Wadsworth Center. Stable wireless data transmission and communication are thus a necessary requirement for developing high performance solutions. Signal imputation in case of data leakage is another solution to address the issue besides implementing fusion techniques. It is known that deep learning based models are often used to detect outliers or handle missing data both in handling image data or time-series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib182" title="">182</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib183" title="">183</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib184" title="">184</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">From the multimodal perspective</h4>
<div class="ltx_para" id="S7.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S7.SS2.SSS0.Px3.p1.1">Often drowsiness detection scheme based on only one modality is not robust enough and thus a fusion of several complementary modalities or methods would lead to a better overall system performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib185" title="">185</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib187" title="">187</a>]</cite>. Samiee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib188" title="">188</a>]</cite> introduced data fusion using image-based features and driver-vehicle interaction as a strategy to address the issue of signal loss and boost the overall resilience of the driver drowsiness detection system. The outcome underlined the system’s primary strengths, which encompass dependable and reliable detection and a capacity to withstand input signal losses effectively. Sedik et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib188" title="">188</a>]</cite> investigated sensor fusion techniques in their research. Their approach involves integrating EEG, EOG, ECG, and EMG signals, resulting in enhanced system accuracy, faster detection time, and a robust drowsiness detection scheme. Signal processing techniques fuse FFT and Discrete Wavelet Transform that are applied for feature extraction and noise reduction. Various machine learning and deep learning classifiers are utilized for both multi-class and binary-class classifications. The proposed methodologies are validated through simulations in two scenarios addressing these classification tasks. The outcomes demonstrate that the proposed models exhibit high performance in detecting drowsiness state from medical signals, achieving detection accuracy of 90% and 96% for the multi-class and binary-class scenarios, respectively.
Recent works on multi-modal drowsiness detection systems also focused on the explainability of the algorithms it used as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib108" title="">108</a>]</cite>. The interpretability of such models would strengthen confidence in the detection systems and increase their reliability, which is particularly important as the EU will make the installation of DDAW systems in new vehicles a legal requirement from 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib167" title="">167</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S7.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Benchmark drowsiness detection system with respect to features. Label annotations: x negative, o neutral, and + positive. We note that this assessment is based on current existing research and the negativity in any aspect of any category does not represent the potential of this category but is seen as a future research challenge. Therefore, this table is dynamic and can be changed with future research efforts.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T5.1.1.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.1.1.2.1">
<tr class="ltx_tr" id="S7.T5.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.2.1.1.1.1">data</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.2.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.2.1.2.1.1">accessability</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.3.1">unobstrusive</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.1.1.4.1">
<tr class="ltx_tr" id="S7.T5.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.4.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.4.1.1.1.1">processing</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.4.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.4.1.2.1.1">complexity</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.1.1.5.1">
<tr class="ltx_tr" id="S7.T5.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.5.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.5.1.1.1.1">calibration</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.5.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.5.1.2.1.1">complexity</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.6" style="padding-left:3.0pt;padding-right:3.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.1.1.6.1">
<tr class="ltx_tr" id="S7.T5.1.1.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.6.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.6.1.1.1.1">noise</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.1.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.6.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.6.1.2.1.1">coupling</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.1.1.7.1">
<tr class="ltx_tr" id="S7.T5.1.1.1.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.7.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.7.1.1.1.1">motion</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.1.1.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.7.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.7.1.2.1.1">artifect</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.8.1">occlusion</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.9" style="padding-left:3.0pt;padding-right:3.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.1.1.9.1">
<tr class="ltx_tr" id="S7.T5.1.1.1.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.9.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.9.1.1.1.1">transmission</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.1.1.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.9.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.9.1.2.1.1">stability</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T5.1.1.1.10" style="padding-left:3.0pt;padding-right:3.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.1.1.10.1">
<tr class="ltx_tr" id="S7.T5.1.1.1.10.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.10.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.10.1.1.1.1">detection</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.1.1.10.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S7.T5.1.1.1.10.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.10.1.2.1.1">accuracy</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S7.T5.1.1.1.11" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.11.1">practability</span></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">EEG</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.8" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.9" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T5.1.2.2.10" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T5.1.2.2.11" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">-wearable</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.8" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.9" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.3.3.10" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left" id="S7.T5.1.3.3.11" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">ECG</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.8" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.9" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.4.4.10" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left" id="S7.T5.1.4.4.11" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">-wearable</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.8" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.9" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T5.1.5.5.10" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left" id="S7.T5.1.5.5.11" style="padding-left:3.0pt;padding-right:3.0pt;">o</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">Vision</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.8" style="padding-left:3.0pt;padding-right:3.0pt;">x</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.9" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T5.1.6.6.10" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S7.T5.1.6.6.11" style="padding-left:3.0pt;padding-right:3.0pt;">+</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S7.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>The diagram highlights the sensor types utilized for various drowsiness detection applications reported in previous works. This allows us to promptly pinpoint areas where specific sensor types are absent, promoting future research in those domains. This table does not limit the research in the field of drowsiness detection and is thus dynamic and can be changed with future research efforts.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T6.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T6.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.1.1">Application areas</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T6.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.2.1">Public transportation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T6.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.3.1">Aviation transportation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T6.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.4.1">Driver Monitoring</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T6.1.1.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.5.1">Workplace Safety</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T6.1.1.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.6.1">Healthcare</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S7.T6.1.1.1.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S7.T6.1.1.1.7.1">Smart Homes</span></td>
</tr>
<tr class="ltx_tr" id="S7.T6.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T6.1.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">EEG</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T6.1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T6.1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T6.1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T6.1.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T6.1.2.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_border_t" id="S7.T6.1.2.2.7" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S7.T6.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T6.1.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">ECG</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T6.1.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T6.1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T6.1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_border_r" id="S7.T6.1.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T6.1.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td" id="S7.T6.1.3.3.7" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S7.T6.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T6.1.4.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">Vision</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T6.1.4.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T6.1.4.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T6.1.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T6.1.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S7.T6.1.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S7.T6.1.4.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Potential Directions for Research</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.T5" title="TABLE V ‣ From the multimodal perspective ‣ VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">V</span></a> summarizes the limitations described in section <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7" title="VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">VII</span></a> and provides a better overview of the relative strengths and weaknesses of the current sensing modalities and algorithms for drowsiness detection. It’s important to emphasize that this assessment relies on investigated research works in this survey. Any negative aspect identified within a category should not be interpreted as a reflection of its potential; rather, it’s viewed as a future research challenge.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Biosignals captured with EEG and EOG measurements are more precise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib107" title="">107</a>]</cite>, but less appropriate for use-cases as driver’s drowsiness detection. Both vehicle and subject motion would strongly affect the performance and the accuracy of the feature extraction process of most proposed methods in the literature. Trends towards wearable sensors reduced the limitation of electrode-based applications of the EEG and EOG approaches to certain extent. However, headphone-based devices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib90" title="">90</a>]</cite> including the biosignal measurements are also not practicable for real world driving scenarios. Drivers are not safe wearing headphone devices while driving. Therefore, future research directions involve the development of more robust algorithms mitigating these motion artifacts (e.g., by using de-trending, or signal decomposition techniques) or require more appropriate setups without affecting the drivers while driving under real world scenarios. This can include automatic learnable augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib189" title="">189</a>]</cite> techniques that would enrich the training data with more realistic variation.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Confusion may arise between the terms driver fatigue detection and driver drowsiness detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib190" title="">190</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib190" title="">190</a>]</cite>.
Fatigue state describes the degree of fatigue, which may not necessarily involve drowsiness or subject falling asleep. Many individuals can experience fatigue while still remaining cognitive vigilant and are still capable of driving safely. Therefore, it is essential to prioritize the detection of drowsiness, as drivers in this state are unconscious while driving. To tackle this issue, heart rate variability (HRV) derived from ECG-signals are often employed as a feature for detection. However, ECG signals in relation towards drowsiness are often hard to draw. Changes in the parasympathetic and sympathetic activity of the body can be related to different biological processes beyond drowsiness detection. Several works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib107" title="">107</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib96" title="">96</a>]</cite> investigated the discriminative power of ECG-based features towards drowsiness and future trends hint towards more complex fusion methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib191" title="">191</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib85" title="">85</a>]</cite> or more generalized self-learning models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib118" title="">118</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.T5" title="TABLE V ‣ From the multimodal perspective ‣ VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">V</span></a> indicates a clear advantage of vision-based solutions over a wide range of challenges faced by other two investigated measures, but also emphasized that such solutions are strongly affected by any application scenario where the measured subject might be occluded or under less favorable capture conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib109" title="">109</a>]</cite>. Features considered like the PERCLOS indicates the percentage of eye closures. This features strongly relates to the accuracy and efficiency of face detection and facial landmark detection. Video-based approaches of drowsiness detection often combine techniques for keyframe selections and final drowsiness state classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib117" title="">117</a>]</cite>. Other works intend to face the challenge of variation in head poses as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib116" title="">116</a>]</cite>. Therefore, we believe that future research directions in vision-based approaches should consider these relevant topics, such as accurate and fast keyframe selections and creating more robust algorithms dealing with the dynamic challenges posed in a real-world setting.</p>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1">Finally, Table <a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#S7.T6" title="TABLE VI ‣ From the multimodal perspective ‣ VII-B Limitations on physiological signal based technique ‣ VII Technical and Practical Limitations ‣ A Survey on Drowsiness Detection – Modern Applications and Methods"><span class="ltx_text ltx_ref_tag">VI</span></a> shows the investigated measuring modalities successfully associated with different application areas. This allows researchers to have a clearer view to position their future research in the missing areas of drowsiness detection. However, this table should not limit the research in the field of drowsiness detection and is thus dynamic and can be changed with future research efforts.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span class="ltx_text ltx_font_smallcaps" id="S9.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">Detecting drowsiness is of high significance in guaranteeing safety in various domains such as in workplaces requiring high concentration of employees, under real driving situations, in public transportation, and for aviation. Alert and well-rested employees also lead to enhanced productivity and better personal healthcare. This work is a pioneering work covering a wide application area of drowsiness detection with its modern applications and methods. We categorized our researched works into three measuring techniques, with multi-channel EEG signals, ECG signals, and vision-based detection schemes. We summarized and compared popular benchmarking databases and common evaluation metrics used to assess the performance of the developed drowsiness detection algorithms.</p>
</div>
<div class="ltx_para" id="S9.p2">
<p class="ltx_p" id="S9.p2.1">We identified strengths and weaknesses in current algorithms and discussed the limitations of current research categorized under both physiological-based and vision-based approaches. We pinpointed challenges in accurate and real-time detection, in stable data transmission using wireless sensing technologies, and in building a bias-free system among others. We provide possible solutions like mitigating the bias by using synthetic, adversarial data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib166" title="">166</a>]</cite> or data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.12990v1#bib.bib192" title="">192</a>]</cite> techniques. Overcoming the hardware limitations requires model compression techniques to build small-scale but still highly accurate models, and leveraging the fusion of complementary modalities, methods, or sensors to lead to more robust and accurate detection resilient to noise or data loss. Finally, we believe that drowsiness detection remains an actively evolving field with abundant opportunities to explore, both when it comes to sensing technology and algorithmic development. The primary goal of this work is to provide an initial comprehensive survey of drowsiness detection within contemporary applications and methodologies.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research work has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
X. Qin, Y. Niu, H. Zhou, X. Li, W. Jia, and Y. Zheng, “Driver drowsiness EEG detection based on tree federated learning and interpretable network,” <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Int. J. Neural Syst.</span>, vol. 33, no. 3, pp. 2350009:1–2350009:17, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. Lee, J. Lee, and M. Shin, “Using wearable ECG/PPG sensors for driver drowsiness detection based on distinguishable pattern of recurrence plots,” <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Electronics</span>, vol. 8, no. 2, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
W. Deng and R. Wu, “Real-time driver-drowsiness detection system using facial features,” <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">IEEE Access</span>, vol. 7, pp. 118727–118738, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
I. G. Daza, S. Bronte, L. M. Bergasa, J. Almazán, and J. J. Y. Torres, “Vision-based drowsiness detector for real driving conditions,” in <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">2012 IEEE Intelligent Vehicles Symposium, IV 2012, Alcal de Henares, Madrid, Spain, June 3-7, 2012</span>, pp. 618–623, IEEE, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Sahayadhas, K. Sundaraj, and M. Murugappan, “Detecting driver drowsiness based on sensors: A review,” <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Sensors</span>, vol. 12, no. 12, pp. 16937–16953, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
National Highway Traffic Safety Administration, “NHTSA drowsy driving research and program plan.” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.nhtsa.gov/sites/nhtsa.gov/files/drowsydriving_strategicplan_030316.pdf</span>, 2016.

</span>
<span class="ltx_bibblock">DOT publication HS 812 252. Accessed: (Februar 2, 2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P. Liu, H.-L. Chi, X. Li, and J. Guo, “Effects of dataset characteristics on the performance of fatigue detection for crane operators using hybrid deep neural networks,” <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Automation in Construction</span>, vol. 132, p. 103901, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Geiger-Brown, V. E. Rogers, A. M. Trinkoff, R. L. Kane, R. B. Bausell, and S. M. Scharf, “Sleep, sleepiness, fatigue, and performance of 12-hour-shift nurses,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Chronobiology international</span>, vol. 29, no. 2, pp. 211–219, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Mantzanas, D. Chrysikos, I. Giannakodimos, G. Chelidonis, P. Drymousi, E. Pechlivanidou, E. Pikoulis, N. Basios, C. G. Zografos, G. C. Zografos, <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">et al.</span>, “Subjective sleep quality and daytime sleepiness among greek nursing staff: A multicenter cross-sectional study,” <span class="ltx_text ltx_font_italic" id="bib.bib9.2.2">Health &amp; Research Journal</span>, vol. 8, no. 3, pp. 214–224, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C. Juan-García, M. Plaza-Carmona, and N. Fernández-Martínez, “Sleep analysis in emergency nurses’ department,” <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Revista da Associação Médica Brasileira</span>, vol. 67, pp. 862–867, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. Woodard, J. Adornetti, J. M. Nogales, M. Foster, L. Leask, R. McGee, M. Carlucci, S. Crowley, and A. Wolfson, “0064 youth sleep-wake experience in juvenile justice facilities: A descriptive analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Sleep</span>, vol. 45, no. Supplement_1, pp. A29–A30, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. A. Kamran, M. M. N. Mannan, and M. Y. Jeong, “Drowsiness, fatigue and poor sleep’s causes and detection: A comprehensive study,” <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">IEEE Access</span>, vol. 7, pp. 167172–167186, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Papadelis, Z. Chen, C. Kourtidou-Papadeli, P. D. Bamidis, I. Chouvarda, E. Bekiaris, and N. Maglaveras, “Monitoring sleepiness with on-board electrophysiological recordings for preventing sleep-deprived traffic accidents,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Clinical Neurophysiology</span>, vol. 118, no. 9, pp. 1906–1922, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L. N. Boyle, J. Tippin, A. Paul, and M. Rizzo, “Driver performance in the moments surrounding a microsleep,” <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Transportation research part F: traffic psychology and behaviour</span>, vol. 11, no. 2, pp. 126–136, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Sharma and K. K. Sharma, “An algorithm for sleep apnea detection from single-lead ECG using hermite basis functions,” <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Comput. Biol. Medicine</span>, vol. 77, pp. 116–124, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Wang, B. Guragain, A. K. Verma, L. Archer, S. Majumder, A. Mohamud, E. Flaherty-Woods, G. Shapiro, M. Almashor, M. Lenné, R. Myers, J. Kuo, S. Yang, N. Wilson, and K. Tavakolian, “Spectral analysis of EEG during microsleep events annotated via driver monitoring system to characterize drowsiness,” <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE Trans. Aerosp. Electron. Syst.</span>, vol. 56, no. 2, pp. 1346–1356, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J.-H. Jeong, B.-W. Yu, D.-H. Lee, and S.-W. Lee, “Classification of drowsiness levels based on a deep spatio-temporal convolutional bidirectional lstm network using electroencephalography signals,” <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Brain sciences</span>, vol. 9, no. 12, p. 348, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Zhang, J. Li, Y. Liu, Z. Zhang, Z. Wang, D. Luo, X. Zhou, M. Zhu, W. Salman, G. Hu, and C. Wang, “Design of a fatigue detection system for high-speed trains based on driver vigilance using a wireless wearable EEG,” <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Sensors</span>, vol. 17, no. 3, p. 486, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Chen, H. Li, L. Han, J. Wu, A. Azam, and Z. Zhang, “Driver vigilance detection for high-speed rail using fusion of multiple physiological signals and deep learning,” <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Appl. Soft Comput.</span>, vol. 123, p. 108982, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z. Zhou, Z. Fang, J. Wang, J. Chen, H. Li, L. Han, and Z. Zhang, “Driver vigilance detection based on deep learning with fused thermal image information for public transportation,” <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Eng. Appl. Artif. Intell.</span>, vol. 124, p. 106604, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Fan, Z. Li, J. Pei, H. Li, and J. Sun, “Applying systems thinking approach to accident analysis in china: Case study of “7.23” yong-tai-wen high-speed train accident,” <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Safety science</span>, vol. 76, pp. 190–201, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M. Ramzan, H. U. Khan, S. M. Awan, A. Ismail, M. Ilyas, and A. Mahmood, “A survey on state-of-the-art drowsiness detection techniques,” <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">IEEE Access</span>, vol. 7, pp. 61904–61919, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. S. D. Pandilwar and M. More, “Survey paper for real time car driver drowsiness detection using machine learning approach,” <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">International Journal for Research in Applied Science and Engineering Technology</span>, p. 4318–4323, Jul 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
B. Reddy, Y. Kim, S. Yun, C. Seo, and J. Jang, “Real-time driver drowsiness detection for embedded system using model compression of deep neural networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017</span>, pp. 438–445, IEEE Computer Society, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Othmani, A. Q. M. Sabri, S. Aslan, F. Chaieb, H. Rameh, R. Alfred, and D. Cohen, “EEG-based neural networks approaches for fatigue and drowsiness detection: A survey,” <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Neurocomputing</span>, p. 126709, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Sahayadhas, K. Sundaraj, and M. Murugappan, “Detecting driver drowsiness based on sensors: A review,” <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Sensors</span>, vol. 12, no. 12, pp. 16937–16953, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. A. El-Nabi, W. El-Shafai, E.-S. M. El-Rabaie, K. F. Ramadan, F. E. Abd El-Samie, and S. Mohsen, “Machine learning and deep learning techniques for driver fatigue and drowsiness detection: a review,” <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Multimedia Tools and Applications</span>, vol. 83, no. 3, pp. 9441–9477, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. A. Caldwell, J. L. Caldwell, L. A. Thompson, and H. R. Lieberman, “Fatigue and its management in the workplace,” <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Neuroscience &amp; Biobehavioral Reviews</span>, vol. 96, pp. 272–289, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. E. Lerman, E. Eskin, D. J. Flower, E. C. George, B. Gerson, N. Hartenbaum, S. R. Hursh, and M. Moore-Ede, “Fatigue risk management in the workplace,” vol. 54, no. 2, pp. 231–258, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
K. Sadeghniiat-Haghighi and Z. Yazdi, “Fatigue management in the workplace,” <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Industrial psychiatry journal</span>, vol. 24, no. 1, p. 12, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Astaras, P. D. Bamidis, C. Kourtidou-Papadeli, and N. Maglaveras, “Biomedical real-time monitoring in restricted and safety-critical environments,” <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Hippokratia</span>, vol. 12, no. Suppl 1, p. 10, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
P. M. Ramos, C. B. Maior, M. C. Moura, and I. D. Lins, “Automatic drowsiness detection for safety-critical operations using ensemble models and EEG signals,” <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Process Safety and Environmental Protection</span>, vol. 164, pp. 566–581, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. Natnithikarat, S. Lamyai, P. Leelaarporn, N. Kunaseth, P. Autthasan, T. Wisutthisen, and T. Wilaiprasitporn, “Drowsiness detection for office-based workload with mouse and keyboard data,” in <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">2019 12th Biomedical Engineering International Conference (BMEiCON)</span>, pp. 1–4, IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Kaida, M. Takahashi, T. Åkerstedt, A. Nakata, Y. Otsuka, T. Haratani, and K. Fukasawa, “Validation of the karolinska sleepiness scale against performance and EEG variables,” <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Clinical neurophysiology</span>, vol. 117, no. 7, pp. 1574–1581, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Ummul and K. Rao, “Shift work and fatigue,” <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Journal of Environ Sci Toxicol Food Technol</span>, vol. 1, no. 3, pp. 17–21, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
B. J. Thompson, M. S. Stock, and V. K. Banuelas, “Effects of accumulating work shifts on performance-based fatigue using multiple strength measurements in day and night shift nurses and aides,” <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Hum. Factors</span>, vol. 59, no. 3, pp. 346–356, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
T. Åkerstedt, “Sleepiness as a consequence of shift work,” <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Sleep</span>, vol. 11, no. 1, pp. 17–34, 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R. D. Chervin, “Sleepiness, fatigue, tiredness, and lack of energy in obstructive sleep apnea,” <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Chest</span>, vol. 118, no. 2, pp. 372–379, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
E. Q. Wu, C. Lin, L. Zhu, Z. Tang, Y. Jie, and G. Zhou, “Fatigue detection of pilots’ brain through brains cognitive map and multilayer latent incremental learning model,” <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">IEEE Trans. Cybern.</span>, vol. 52, no. 11, pp. 12302–12314, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
P. Henríquez, B. J. Matuszewski, Y. Andreu, L. Bastiani, S. Colantonio, G. Coppini, M. D’Acunto, R. Favilla, D. Germanese, D. Giorgi, P. Marraccini, M. Martinelli, M. Morales, M. A. Pascali, M. Righi, O. Salvetti, M. Larsson, T. Strömberg, L. Randeberg, A. Bjorgan, G. A. Giannakakis, M. Pediaditis, F. Chiarugi, E. Christinaki, K. Marias, and M. Tsiknakis, “Mirror mirror on the wall… an unobtrusive intelligent multisensory mirror for well-being status self-assessment and visualization,” <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">IEEE Trans. Multim.</span>, vol. 19, no. 7, pp. 1467–1481, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
V. Kumar, N. Badal, and R. Mishra, “Elderly fall due to drowsiness: Detection and prevention using machine learning and iot,” <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Modern Physics Letters B</span>, vol. 35, no. 07, p. 2150120, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
B. Bačić and J. Zhang, “Towards real-time drowsiness detection for elderly care,” in <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA)</span>, pp. 1–6, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M. Teplan <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">et al.</span>, “Fundamentals of EEG measurement,” <span class="ltx_text ltx_font_italic" id="bib.bib43.2.2">Measurement science review</span>, vol. 2, no. 2, pp. 1–11, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D. Jiang, Y. Lu, Y. Ma, and Y. Wang, “Robust sleep stage classification with single-channel EEG signals using multimodal decomposition and hmm-based refinement,” <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Expert Syst. Appl.</span>, vol. 121, pp. 188–203, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T. L. T. da Silveira, A. J. Kozakevicius, and C. R. Rodrigues, “Automated drowsiness detection through wavelet packet analysis of a single EEG channel,” <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Expert Syst. Appl.</span>, vol. 55, pp. 559–565, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
B. A. GEERING, P. ACHERMANN, F. EGGIMANN, and A. A. BORBÉLY, “Period-amplitude analysis and power spectral analysis: a comparison based on all-night sleep eeg recordings,” <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Journal of sleep research</span>, vol. 2, no. 3, pp. 121–129, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
F. K. Onay and C. Köse, “Power spectral density analysis in alfa, beta and gamma frequency bands for classification of motor EEG signals,” in <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">27th Signal Processing and Communications Applications Conference, SIU 2019, Sivas, Turkey, April 24-26, 2019</span>, pp. 1–4, IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Z. Mardi, S. N. M. Ashtiani, and M. Mikaili, “Eeg-based drowsiness detection for safe driving using chaotic features and statistical tests,” <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Journal of medical signals and sensors</span>, vol. 1, no. 2, p. 130, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Y. Jiao, Y. Deng, Y. Luo, and B.-L. Lu, “Driver sleepiness detection from EEG and EOG signals using gan and lstm networks,” <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Neurocomputing</span>, vol. 408, pp. 100–111, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
J. L. Cantero, M. Atienza, and R. M. Salas, “Human alpha oscillations in wakefulness, drowsiness period, and rem sleep: different electroencephalographic phenomena within the alpha band,” <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Neurophysiologie Clinique/Clinical Neurophysiology</span>, vol. 32, no. 1, pp. 54–71, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
R. D. Ogilvie, “The process of falling asleep,” <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">Sleep medicine reviews</span>, vol. 5, no. 3, pp. 247–270, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
H. Blake, R. W. Gerard, and N. Kleitman, “Factors influencing brain potentials during sleep,” <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Journal of Neurophysiology</span>, vol. 2, no. 1, pp. 48–60, 1939.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
C. Heinze, C. Hütterer, T. Schnupp, G. Lenis, and M. Golz, “Drowsiness discrimination in an overnight driving simulation on the basis of rr and qt intervals,” <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Current Directions in Biomedical Engineering</span>, vol. 3, no. 2, pp. 563–567, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
A. U. Viola, E. Tobaldini, S. L. Chellappa, K. R. Casali, A. Porta, and N. Montano, “Short-term complexity of cardiac autonomic control during sleep: Rem as a potential risk factor for cardiovascular system in aging,” <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">PloS one</span>, vol. 6, no. 4, p. e19002, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
R. Cabiddu, S. Cerutti, G. Viardot, S. Werner, and A. M. Bianchi, “Modulation of the sympatho-vagal balance during sleep: frequency domain study of heart rate variability and respiration,” <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Frontiers in physiology</span>, vol. 3, p. 45, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
M. AlGhatrif and J. Lindsay, “A brief review: history to understand fundamentals of electrocardiography,” <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Journal of community hospital internal medicine perspectives</span>, vol. 2, no. 1, p. 14383, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
M. Hammad, A. Maher, K. Wang, F. Jiang, and M. Amrani, “Detection of abnormal heart conditions based on characteristics of ECG signals,” <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">Measurement</span>, vol. 125, pp. 634–644, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
W. J. Brady, A. D. Perron, M. L. Martin, C. Beagle, and T. P. Aufderheide, “Cause of ST segment abnormality in ED chest pain patients,” <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">The American journal of emergency medicine</span>, vol. 19, no. 1, pp. 25–28, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
A. Kumar and D. M. Lloyd-Jones, “Clinical significance of minor nonspecific ST-segment and T-wave abnormalities in asymptomatic subjects: a systematic review,” <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Cardiology in review</span>, vol. 15, no. 3, pp. 133–142, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
K. E. Trinkley, R. Lee Page, H. Lien, K. Yamanouye, and J. E. Tisdale, “QT interval prolongation and the risk of torsades de pointes: essentials for clinicians,” <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">Current medical research and opinion</span>, vol. 29, no. 12, pp. 1719–1726, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
D. G. Strauss and R. H. Selvester, “The QRS complex—a biomarker that “images” the heart: Qrs scores to quantify myocardial scar in the presence of normal and abnormal ventricular conduction,” <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Journal of electrocardiology</span>, vol. 42, no. 1, pp. 85–96, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
R. Kher <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">et al.</span>, “Signal processing techniques for removing noise from ECG signals,” <span class="ltx_text ltx_font_italic" id="bib.bib62.2.2">J. Biomed. Eng. Res</span>, vol. 3, no. 101, pp. 1–9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
P. K. Stein and Y. Pu, “Heart rate variability, sleep and sleep disorders,” <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">Sleep medicine reviews</span>, vol. 16, no. 1, pp. 47–66, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
T. Penzel, J. W. Kantelhardt, R. P. Bartsch, M. Riedl, J. F. Kraemer, N. Wessel, C. Garcia, M. Glos, I. Fietze, and C. Schöbel, “Modulations of heart rate, ECG, and cardio-respiratory coupling observed in polysomnography,” <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">Frontiers in physiology</span>, vol. 7, p. 460, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
A. H. Khandoker, M. Palaniswami, and C. K. Karmakar, “Support vector machines for automated recognition of obstructive sleep apnea syndrome from ECG recordings,” <span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">IEEE Trans. Inf. Technol. Biomed.</span>, vol. 13, no. 1, pp. 37–48, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
G. B. Papini, P. Fonseca, J. Margarito, M. M. van Gilst, S. Overeem, J. W. M. Bergmans, and R. Vullings, “On the generalizability of ECG-based obstructive sleep apnea monitoring: merits and limitations of the apnea-ecg database,” in <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2018, Honolulu, HI, USA, July 18-21, 2018</span>, pp. 6022–6025, IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
A. Dharma, P. Sihombing, H. Mawengkang, S. Efendi, and A. R. Crispin, “Development and evaluation of a portable ECG monitoring system for automated classification of normal and abnormal ECG signal using random forest with xgboost,” in <span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">29th International Conference on Telecommunications, ICT 2023, Toba, Indonesia, November 8-9, 2023</span>, pp. 1–5, IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
S. Ramasamy and A. Balan, “Wearable sensors for ECG measurement: a review,” <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">Sensor Review</span>, vol. 38, no. 4, pp. 412–419, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
H. Li, Z. An, S. Zuo, W. Zhu, Z. Zhang, S. Zhang, C. Zhang, W. Song, Q. Mao, Y. Mu, E. Li, and J. D. P. García, “Artificial intelligence-enabled ECG algorithm based on improved residual network for wearable ECG,” <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">Sensors</span>, vol. 21, no. 18, p. 6043, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
M. Ahmed, S. Masood, M. Ahmad, and A. A. A. El-Latif, “Intelligent driver drowsiness detection for traffic safety based on multi CNN deep model and facial subsampling,” <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">IEEE Trans. Intell. Transp. Syst.</span>, vol. 23, no. 10, pp. 19743–19752, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
M. C. Uchida, R. Carvalho, V. D. Tessutti, R. F. P. Bacurau, H. J. Coelho-Júnior, L. P. Capelo, H. P. Ramos, M. C. d. Santos, L. F. M. Teixeira, and P. H. Marchetti, “Identification of muscle fatigue by tracking facial expressions,” <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">PLoS One</span>, vol. 13, no. 12, p. e0208834, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
M. A. Khan, T. Nawaz, U. S. Khan, A. Hamza, and N. Rashid, “IoT-Based Non-Intrusive automated driver drowsiness monitoring framework for logistics and public transport applications to enhance road safety,” <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">IEEE Access</span>, vol. 11, pp. 14385–14397, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
R. Katmah, F. Al-Shargie, U. Tariq, F. Babiloni, F. Al-Mughairbi, and H. Al-Nashash, “A review on mental stress assessment methods using EEG signals,” <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Sensors</span>, vol. 21, no. 15, p. 5043, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
I. Stancin, M. Cifrek, and A. Jovic, “A review of EEG signal features and their application in driver drowsiness detection systems,” <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">Sensors</span>, vol. 21, no. 11, p. 3786, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Y. Ming, D. Wu, Y. Wang, Y. Shi, and C. Lin, “EEG-based drowsiness estimation for driving safety using Deep Q-Learning,” <span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">IEEE Trans. Emerg. Top. Comput. Intell.</span>, vol. 5, no. 4, pp. 583–594, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
S. Chaabene, B. Bouaziz, A. Boudaya, A. Hökelmann, A. Ammar, and L. Chaâri, “Convolutional neural network for drowsiness detection using EEG signals,” <span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">Sensors</span>, vol. 21, no. 5, p. 1734, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Q. Massoz, T. Langohr, C. François, and J. G. Verly, “The ULg multimodality drowsiness database (called DROZY) and examples of use,” in <span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016, Lake Placid, NY, USA, March 7-10, 2016</span>, pp. 1–7, IEEE Computer Society, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
J. Cui, Z. Lan, T. Zheng, Y. Liu, O. Sourina, L. Wang, and W. Müller-Wittig, “Subject-independent drowsiness recognition from single-channel EEG with an interpretable CNN-LSTM model,” in <span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">International Conference on Cyberworlds, CW 2021, Caen, France, September 28-30, 2021</span> (A. Sourin, C. Rosenberger, and O. Sourina, eds.), pp. 201–208, IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
C. Lee and J. An, “LSTM-CNN model of drowsiness detection from multiple consciousness states acquired by eeg,” <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Expert Systems with Applications</span>, vol. 213, p. 119032, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
J. R. Paulo, G. Pires, and U. J. Nunes, “Cross-subject zero calibration driver’s drowsiness detection: Exploring spatiotemporal image encoding of EEG signals for convolutional neural network classification,” <span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">IEEE Transactions on Neural Systems and Rehabilitation Engineering</span>, vol. 29, pp. 905–915, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Y. Jiang, Y. Zhang, C. Lin, D. Wu, and C. Lin, “EEG-based driver drowsiness estimation using an online multi-view and transfer TSK fuzzy system,” <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">IEEE Trans. Intell. Transp. Syst.</span>, vol. 22, no. 3, pp. 1752–1764, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
J. Cui, Z. Lan, O. Sourina, and W. Müller-Wittig, “EEG-based cross-subject driver drowsiness recognition with an interpretable convolutional neural network,” <span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">IEEE Transactions on Neural Networks and Learning Systems</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Z. Zhuang, Y.-K. Wang, Y.-C. Chang, J. Liu, and C.-T. Lin, “A connectivity-aware graph neural network for real-time drowsiness classification,” <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">IEEE Transactions on Neural Systems and Rehabilitation Engineering</span>, vol. PP, pp. 1–1, 11 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
M. Duvinage, T. Castermans, M. Petieau, T. Hoellinger, G. Cheron, and T. Dutoit, “Performance of the emotiv epoc headset for p300-based applications,” <span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">Biomedical engineering online</span>, vol. 12, pp. 1–15, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
M. Golz, D. Sommer, M. Chen, U. Trutschel, and D. Mandic, “Feature fusion for the detection of microsleep events,” <span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">The Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology</span>, vol. 49, pp. 329–342, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
M. A. Carskadon, <span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">Encyclopedia of sleep and dreaming.</span>
</span>
<span class="ltx_bibblock">Macmillan Publishing Co, Inc, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
A. A. of Sleep Medicine <span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">et al.</span>, “Economic burden of undiagnosed sleep apnea in us is nearly $150 b per year,” 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
R. L. DeHart, “The twenty-four hour society: Understanding human limits in a world that never sleeps,” <span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">JAMA</span>, vol. 270, no. 18, pp. 2230–2230, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
M. Golz, D. Sommer, A. Seyfarth, U. Trutschel, and M. Moore-Ede, “Application of vector-based neural networks for the recognition of beginning microsleep episodes with an eyetracking system,” <span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">Computational Intelligence: Methods &amp; Applications</span>, pp. 130–134, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
N. Pham, T. Dinh, Z. Raghebi, T. Kim, N. Bui, P. Nguyen, H. Truong, F. Banaei-Kashani, A. Halbower, T. Dinh, <span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">et al.</span>, “Wake: a behind-the-ear wearable system for microsleep detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib90.2.2">Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services</span>, pp. 404–418, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
A. Chougule, J. Shah, V. Chamola, and S. Kanhere, “Enabling safe ITS: EEG-Based microsleep detection in vanets,” <span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">IEEE Transactions on Intelligent Transportation Systems</span>, vol. 24, no. 12, pp. 15773–15783, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Z. Cao, C.-H. Chuang, J.-K. King, and C.-T. Lin, “Multi-channel EEG recordings during a sustained-attention driving task,” <span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">Scientific data</span>, vol. 6, no. 1, p. 19, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
K. T. Chui, K. F. Tsang, H. R. Chi, B. W. Ling, and C. K. Wu, “An accurate ECG-based transportation safety drowsiness detection scheme,” <span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">IEEE Trans. Ind. Informatics</span>, vol. 12, no. 4, pp. 1438–1452, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
T. Takalokastari, S.-J. Jung, D.-D. Lee, and W.-Y. Chung, “Real time drowsiness detection by a WSN based wearable ecg measurement system,” <span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">Journal of Sensor Science and Technology</span>, vol. 20, 11 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
C. Meyer, J. F. Gavela, and M. Harris, “Combining algorithms in automatic detection of QRS complexes in ECG signals,” <span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">IEEE Trans. Inf. Technol. Biomed.</span>, vol. 10, no. 3, pp. 468–475, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
N. R. Adão Martins, S. Annaheim, C. M. Spengler, and R. M. Rossi, “Fatigue monitoring through wearables: A state-of-the-art review,” <span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">Frontiers in Physiology</span>, vol. 12, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
T. Sherbakova and O. Osipova, “The analysis of an electrocardiosignal in a system of data transmission in control office,” in <span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">2015 International Conference on Antenna Theory and Techniques (ICATT)</span>, pp. 1–2, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
J. Achten and A. E. Jeukendrup, “Heart rate monitoring: applications and limitations,” <span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">Sports medicine</span>, vol. 33, pp. 517–538, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
S. Weise, J. Ong, N. A. Tesler, S. Kim, and W. T. Roth, “Worried sleep: 24-h monitoring in high and low worriers,” <span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">Biological psychology</span>, vol. 94, no. 1, pp. 61–70, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
V. A. Zernov, E. V. Lobanova, E. V. Likhacheva, L. R. Nikolaeva, D. D. Dymarchuk, D. S. Yesenin, N. V. Mizin, A. S. Ognev, and M. Y. Rudenko, “Cardiometric fingerprints of various human ego states,” <span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">Cardiometry</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
K. Ke, M. R. Zulman, H. Wu, Y. Huang, and J. Thiagarajan, “Drowsiness detection system using heartbeat rate in android-based handheld devices,” in <span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">First International Conference on Multimedia and Image Processing, ICMIP 2016, Bandar Seri Begawan, Brunei Darussalam, June 1-3, 2016</span>, pp. 99–103, IEEE, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
K. Fujiwara, E. Abe, K. Kamata, C. Nakayama, Y. Suzuki, T. Yamakawa, T. Hiraoka, M. Kano, Y. Sumi, F. Masuda, M. Matsuo, and H. Kadotani, “Heart rate variability-based driver drowsiness detection and its validation with EEG,” <span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">IEEE Trans. Biomed. Eng.</span>, vol. 66, no. 6, pp. 1769–1778, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
F. L. Mannering and L. L. Grodsky, “Statistical analysis of motorcyclists’ perceived accident risk,” <span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">Accident Analysis &amp; Prevention</span>, vol. 27, no. 1, pp. 21–31, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
H. Ospina-Mateus, L. A. Q. Jiménez, F. J. López-Valdés, S. B. Garcia, L. H. Barrero, and S. S. Sana, “Extraction of decision rules using genetic algorithms and simulated annealing for prediction of severity of traffic accidents by motorcyclists,” <span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">J. Ambient Intell. Humaniz. Comput.</span>, vol. 12, no. 11, pp. 10051–10072, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
F. Fahrurrasyid, G. I. Hapsari, L. Meisaroh, and G. A. Mutiara, “Smart helmet GPS-based for heartbeat drowsiness detection and location tracking,” <span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">Journal of Biomimetics, Biomaterials and Biomedical Engineering</span>, vol. 55, pp. 226–235, 4 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
S. Heydari, A. Ayatollahi, A. Najafi, and M. Poorjafari, “Detection of drowsiness using the pulse rate variability of finger,” <span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">SN Comput. Sci.</span>, vol. 3, no. 5, p. 359, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
G. Lenis, P. Reichensperger, D. Sommer, C. Heinze, M. Golz, and O. Dössel, “Detection of microsleep events in a car driving simulation study using electrocardiographic features,” <span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">Current Directions in Biomedical Engineering</span>, vol. 2, no. 1, pp. 283–287, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
M. M. Hasan, C. N. Watling, and G. S. Larue, “Validation and interpretation of a multimodal drowsiness detection system using explainable machine learning,” <span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">Computer Methods and Programs in Biomedicine</span>, vol. 243, p. 107925, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
S. Bakheet and A. Al-Hamadi, “A framework for instantaneous driver drowsiness detection based on improved HOG features and naïve bayesian classification,” <span class="ltx_text ltx_font_italic" id="bib.bib109.1.1">Brain Sciences</span>, vol. 11, no. 2, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
M. Vijay, N. N. Vinayak, M. Nunna, and N. Subramanyam, “Real-time driver drowsiness detection using facial action units,” in <span class="ltx_text ltx_font_italic" id="bib.bib110.1.1">25th International Conference on Pattern Recognition, ICPR 2020, Virtual Event / Milan, Italy, January 10-15, 2021</span>, pp. 10113–10119, IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
J. Yu, S. Park, S. Lee, and M. Jeon, “Representation learning, scene understanding, and feature fusion for drowsiness detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">Computer Vision - ACCV 2016 Workshops - ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part III</span> (C. Chen, J. Lu, and K. Ma, eds.), vol. 10118 of <span class="ltx_text ltx_font_italic" id="bib.bib111.2.2">Lecture Notes in Computer Science</span>, pp. 165–177, Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
C. Weng, Y. Lai, and S. Lai, “Driver drowsiness detection via a hierarchical temporal deep belief network,” in <span class="ltx_text ltx_font_italic" id="bib.bib112.1.1">Computer Vision - ACCV 2016 Workshops - ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part III</span> (C. Chen, J. Lu, and K. Ma, eds.), vol. 10118 of <span class="ltx_text ltx_font_italic" id="bib.bib112.2.2">Lecture Notes in Computer Science</span>, pp. 117–133, Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
N. N. Pandey and N. B. Muppalaneni, “Dumodds: Dual modeling approach for drowsiness detection based on spatial and spatio-temporal features,” <span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">Engineering Applications of Artificial Intelligence</span>, vol. 119, p. 105759, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
G. S. Krishna, K. Supriya, J. Vardhan, and M. R. K, “Vision transformers and YoloV5 based driver drowsiness detection framework,” <span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">CoRR</span>, vol. abs/2209.01401, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
R. Tamanani, R. Muresan, and A. Al-Dweik, “Estimation of driver vigilance status using real-time facial expression and deep learning,” <span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">IEEE Sensors Letters</span>, vol. 5, no. 5, pp. 1–4, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
J. Chen, Z. Fang, J. Wang, J. Chen, and G. Yin, “A multi-view driver drowsiness detection method using transfer learning and population-based sampling strategy,” in <span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)</span>, pp. 3386–3391, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Y. Lu, C. Liu, F. Chang, H. Liu, and H. Huan, “JHPFA-Net: Joint head pose and facial action network for driver yawning detection across arbitrary poses in videos,” <span class="ltx_text ltx_font_italic" id="bib.bib117.1.1">IEEE Transactions on Intelligent Transportation Systems</span>, vol. 24, no. 11, pp. 11850–11863, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
L. Mou, C. Zhou, P. Xie, P. Zhao, R. Jain, W. Gao, and B. Yin, “Isotropic self-supervised learning for driver drowsiness detection with attention-based multimodal fusion,” <span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">IEEE Transactions on Multimedia</span>, vol. 25, pp. 529–542, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
R. Ghoddoosian, M. Galib, and V. Athitsos, “A realistic dataset and baseline temporal model for early drowsiness detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2019, Long Beach, CA, USA, June 16-20, 2019</span>, pp. 178–187, Computer Vision Foundation / IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
S. Abtahi, M. Omidyeganeh, S. Shirmohammadi, and B. Hariri, “YawDD: a yawning detection dataset,” in <span class="ltx_text ltx_font_italic" id="bib.bib120.1.1">Multimedia Systems Conference 2014, MMSys ’14, Singapore, March 19-21, 2014</span> (R. Zimmermann, ed.), pp. 24–28, ACM, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
R. Fusek, “Pupil localization using geodesic distance,” in <span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">Advances in Visual Computing - 13th International Symposium, ISVC 2018, Las Vegas, NV, USA, November 19-21, 2018, Proceedings</span> (G. Bebis, R. Boyle, B. Parvin, D. Koracin, M. Turek, S. Ramalingam, K. Xu, S. Lin, B. Alsallakh, J. Yang, E. Cuervo, and J. Ventura, eds.), vol. 11241 of <span class="ltx_text ltx_font_italic" id="bib.bib121.2.2">Lecture Notes in Computer Science</span>, pp. 433–444, Springer, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
C. Yang, Z. Yang, W. Li, and J. See, “Fatigueview: A multi-camera video dataset for vision-based drowsiness detection,” <span class="ltx_text ltx_font_italic" id="bib.bib122.1.1">IEEE Trans. Intell. Transp. Syst.</span>, vol. 24, no. 1, pp. 233–246, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
C. Chiou, W. Wang, S. Lu, C. Huang, P. Chung, and Y. Lai, “Driver monitoring using sparse representation with part-based temporal face descriptors,” <span class="ltx_text ltx_font_italic" id="bib.bib123.1.1">IEEE Trans. Intell. Transp. Syst.</span>, vol. 21, no. 1, pp. 346–361, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
M. Martin, A. Roitberg, M. Haurilet, M. Horne, S. Reiß, M. Voit, and R. Stiefelhagen, “Drive&amp;act: A multi-modal dataset for fine-grained driver behavior recognition in autonomous vehicles,” in <span class="ltx_text ltx_font_italic" id="bib.bib124.1.1">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</span>, pp. 2801–2810, IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
J. D. Ortega, N. Kose, P. Cañas, M. Chao, A. Unnervik, M. Nieto, O. Otaegui, and L. Salgado, “DMD: A large-scale multi-modal driver monitoring dataset for attention and alertness analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">Computer Vision - ECCV 2020 Workshops - Glasgow, UK, August 23-28, 2020, Proceedings, Part IV</span> (A. Bartoli and A. Fusiello, eds.), vol. 12538 of <span class="ltx_text ltx_font_italic" id="bib.bib125.2.2">Lecture Notes in Computer Science</span>, pp. 387–405, Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
E. K. Yılmaz and M. A. Akcayol, “Sust-ddd: A real-drive dataset for driver drowsiness detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib126.1.1">Conference of Open Innovations Association (FRUCT)</span>, vol. 6, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
S. A. El-Nabi, W. El-Shafai, E. M. El-Rabaie, K. F. Ramadan, F. E. A. El-Samie, and S. Mohsen, “Machine learning and deep learning techniques for driver fatigue and drowsiness detection: a review,” <span class="ltx_text ltx_font_italic" id="bib.bib127.1.1">Multim. Tools Appl.</span>, vol. 83, no. 3, pp. 9441–9477, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
S. Saurav, P. S. Gidde, R. Saini, and S. Singh, “Real-time eye state recognition using dual convolutional neural network ensemble,” <span class="ltx_text ltx_font_italic" id="bib.bib128.1.1">Journal of Real Time Image Process.</span>, vol. 19, no. 3, pp. 607–622, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Z. Hu, C. Lv, P. Hang, C. Huang, and Y. Xing, “Data-driven estimation of driver attention using calibration-free eye gaze and scene features,” <span class="ltx_text ltx_font_italic" id="bib.bib129.1.1">IEEE Trans. Ind. Electron.</span>, vol. 69, no. 2, pp. 1800–1808, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Y. Ed-Doughmi, N. Idrissi, and Y. Hbali, “Real-time system for driver fatigue detection based on a recurrent neuronal network,” <span class="ltx_text ltx_font_italic" id="bib.bib130.1.1">J. Imaging</span>, vol. 6, no. 3, p. 8, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
F. Song, X. Tan, X. Liu, and S. Chen, “Eyes closeness detection from still images with multi-scale histograms of principal oriented gradients,” <span class="ltx_text ltx_font_italic" id="bib.bib131.1.1">Pattern Recognit.</span>, vol. 47, no. 9, pp. 2825–2838, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
T. Drutarovsky and A. Fogelton, “Eye blink detection using variance of motion vectors,” in <span class="ltx_text ltx_font_italic" id="bib.bib132.1.1">Computer Vision - ECCV 2014 Workshops - Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part III</span> (L. Agapito, M. M. Bronstein, and C. Rother, eds.), vol. 8927 of <span class="ltx_text ltx_font_italic" id="bib.bib132.2.2">Lecture Notes in Computer Science</span>, pp. 436–448, Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Y. Xie, K. Chen, and Y. L. Murphey, “Real-time and robust driver yawning detection with deep neural networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib133.1.1">IEEE Symposium Series on Computational Intelligence, SSCI 2018, Bangalore, India, November 18-21, 2018</span>, pp. 532–538, IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
S. Kawato and J. Ohya, “Real-time detection of nodding and head-shaking by directly detecting and tracking the ”between-eyes”,” in <span class="ltx_text ltx_font_italic" id="bib.bib134.1.1">4th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2000), 26-30 March 2000, Grenoble, France</span>, pp. 40–45, IEEE Computer Society, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
M. Tan, G. Ni, X. Liu, S. Zhang, X. Wu, Y. Wang, and R. Zeng, “Bidirectional posture-appearance interaction network for driver behavior recognition,” <span class="ltx_text ltx_font_italic" id="bib.bib135.1.1">IEEE Trans. Intell. Transp. Syst.</span>, vol. 23, no. 8, pp. 13242–13254, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
T. Abbas, S. F. Ali, M. A. Mohammed, A. Z. Khan, M. J. Awan, A. Majumdar, and O. Thinnukool, “Deep learning approach based on residual neural network and svm classifier for driver’s distraction detection,” <span class="ltx_text ltx_font_italic" id="bib.bib136.1.1">Applied Sciences</span>, vol. 12, no. 13, p. 6626, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
I. Kotseruba and J. K. Tsotsos, “Attention for vision-based assistive and automated driving: A review of algorithms and datasets,” <span class="ltx_text ltx_font_italic" id="bib.bib137.1.1">IEEE Trans. Intell. Transp. Syst.</span>, vol. 23, no. 11, pp. 19907–19928, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
C. B. S. Maior, M. J. das Chagas Moura, J. M. M. Santana, and I. D. Lins, “Real-time classification for autonomous drowsiness detection using eye aspect ratio,” <span class="ltx_text ltx_font_italic" id="bib.bib138.1.1">Expert Syst. Appl.</span>, vol. 158, p. 113505, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
M. Papakostas, K. Das, M. Abouelenien, R. Mihalcea, and M. Burzo, “Distracted and drowsy driving modeling using deep physiological representations and multitask learning,” <span class="ltx_text ltx_font_italic" id="bib.bib139.1.1">Applied Sciences</span>, vol. 11, no. 1, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Y. Albadawi, A. A. Redhaei, and M. Takruri, “Real-time machine learning-based driver drowsiness detection using visual features,” <span class="ltx_text ltx_font_italic" id="bib.bib140.1.1">J. Imaging</span>, vol. 9, no. 5, p. 91, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
C. J. de Naurois, C. Bourdin, A. Stratulat, E. Diaz, and J.-L. Vercher, “Detection and prediction of driver drowsiness using artificial neural network models,” <span class="ltx_text ltx_font_italic" id="bib.bib141.1.1">Accident Analysis &amp; Prevention</span>, vol. 126, pp. 95–104, 2019.

</span>
<span class="ltx_bibblock">10th International Conference on Managing Fatigue: Managing Fatigue to Improve Safety, Wellness, and Effectiveness”.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
G. Li and W. Chung, “Estimation of eye closure degree using EEG sensors and its application in driver drowsiness detection,” <span class="ltx_text ltx_font_italic" id="bib.bib142.1.1">Sensors</span>, vol. 14, no. 9, pp. 17491–17515, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Y. Cui and D. Wu, “EEG-based driver drowsiness estimation using convolutional neural networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib143.1.1">Neural Information Processing: 24th International Conference, ICONIP 2017, Guangzhou, China, November 14-18, 2017, Proceedings, Part II 24</span>, pp. 822–832, Springer, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
S. Hachisuka, “Human and vehicle-driver drowsiness detection by facial expression,” in <span class="ltx_text ltx_font_italic" id="bib.bib144.1.1">2013 International Conference on Biometrics and Kansei Engineering</span>, pp. 320–326, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
M. Dua, Shakshi, R. Singla, S. Raj, and A. Jangra, “Deep CNN models-based ensemble approach to driver drowsiness detection,” <span class="ltx_text ltx_font_italic" id="bib.bib145.1.1">Neural Comput. Appl.</span>, vol. 33, no. 8, pp. 3155–3168, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
C. Wei, Y. Lin, Y. Wang, C. Lin, and T. Jung, “A subject-transfer framework for obviating inter- and intra-subject variability in EEG-based drowsiness detection,” <span class="ltx_text ltx_font_italic" id="bib.bib146.1.1">NeuroImage</span>, vol. 174, pp. 407–419, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
E. Perkins, C. Sitaula, M. Burke, and F. Marzbanrad, “Challenges of driver drowsiness prediction: The remaining steps to implementation,” <span class="ltx_text ltx_font_italic" id="bib.bib147.1.1">IEEE Transactions on Intelligent Vehicles</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
S. Abtahi, B. Hariri, and S. Shirmohammadi, “Driver drowsiness monitoring based on yawning detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib148.1.1">2011 IEEE International Instrumentation and Measurement Technology Conference</span>, pp. 1–4, IEEE, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
A. Ghourabi, H. Ghazouani, and W. Barhoumi, “Driver drowsiness detection based on joint monitoring of yawning, blinking and nodding,” in <span class="ltx_text ltx_font_italic" id="bib.bib149.1.1">16th IEEE International Conference on Intelligent Computer Communication and Processing, ICCP 2020, Cluj-Napoca, Romania, September 3-5, 2020</span> (S. Nedevschi, R. Potolea, and R. R. Slavescu, eds.), pp. 407–414, IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
E. Vural, M. Çetin, A. Erçil, G. Littlewort, M. S. Bartlett, and J. R. Movellan, “Drowsy driver detection through facial movement analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib150.1.1">Human-Computer Interaction, IEEE International Workshop, HCI 2007, Rio de Janeiro, Brazil, October 20, 2007, Proceedings</span> (M. S. Lew, N. Sebe, T. S. Huang, and E. M. Bakker, eds.), vol. 4796 of <span class="ltx_text ltx_font_italic" id="bib.bib150.2.2">Lecture Notes in Computer Science</span>, pp. 6–18, Springer, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
H. Yang, L. Liu, W. Min, X. Yang, and X. Xiong, “Driver yawning detection based on subtle facial action recognition,” <span class="ltx_text ltx_font_italic" id="bib.bib151.1.1">IEEE Trans. Multim.</span>, vol. 23, pp. 572–583, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
B. Yin, X. Fan, and Y. Sun, “Multiscale dynamic features based driver fatigue detection,” <span class="ltx_text ltx_font_italic" id="bib.bib152.1.1">Int. J. Pattern Recognit. Artif. Intell.</span>, vol. 23, no. 3, pp. 575–589, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
X. Fan, B. Yin, and Y. Sun, “Yawning detection based on gabor wavelets and LDA,” <span class="ltx_text ltx_font_italic" id="bib.bib153.1.1">Journal of Beijing Univ Technol</span>, vol. 35, no. 3, pp. 409–413, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
T. Brandt, R. Stemmer, and A. Rakotonirainy, “Affordable visual driver monitoring system for fatigue and monotony,” in <span class="ltx_text ltx_font_italic" id="bib.bib154.1.1">Proceedings of the IEEE International Conference on Systems, Man &amp; Cybernetics: The Hague, Netherlands, 10-13 October 2004</span>, pp. 6451–6456, IEEE, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
R. C. Chang, C. Wang, W. Chen, and C. Chiu, “Drowsiness detection system based on PERCLOS and facial physiological signal,” <span class="ltx_text ltx_font_italic" id="bib.bib155.1.1">Sensors</span>, vol. 22, no. 14, p. 5380, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
L. Wang, X. Wu, B. Ba, and W. Dong, “A vision-based method to detect perclos features,” <span class="ltx_text ltx_font_italic" id="bib.bib156.1.1">Computer Engineering &amp; Science</span>, vol. 6, p. 017, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Z. Zhao, N. Zhou, L. Zhang, H. Yan, Y. Xu, and Z. Zhang, “Driver fatigue detection based on convolutional neural networks using EM-CNN,” <span class="ltx_text ltx_font_italic" id="bib.bib157.1.1">Comput. Intell. Neurosci.</span>, vol. 2020, pp. 7251280:1–7251280:11, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
L. Chen, G. Xin, Y. Liu, and J. Huang, “Driver fatigue detection based on facial key points and LSTM,” <span class="ltx_text ltx_font_italic" id="bib.bib158.1.1">Secur. Commun. Networks</span>, vol. 2021, pp. 5383573:1–5383573:9, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and alignment using multitask cascaded convolutional networks,” <span class="ltx_text ltx_font_italic" id="bib.bib159.1.1">IEEE Signal Process. Lett.</span>, vol. 23, no. 10, pp. 1499–1503, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou, “Retinaface: Single-shot multi-level face localisation in the wild,” in <span class="ltx_text ltx_font_italic" id="bib.bib160.1.1">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020</span>, pp. 5202–5211, Computer Vision Foundation / IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection by deep multi-task learning,” in <span class="ltx_text ltx_font_italic" id="bib.bib161.1.1">Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI</span> (D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, eds.), vol. 8694 of <span class="ltx_text ltx_font_italic" id="bib.bib161.2.2">Lecture Notes in Computer Science</span>, pp. 94–108, Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
F. Boutros, N. Damer, and A. Kuijper, “Quantface: Towards lightweight face recognition by synthetic data low-bit quantization,” in <span class="ltx_text ltx_font_italic" id="bib.bib162.1.1">26th International Conference on Pattern Recognition, ICPR 2022, Montreal, QC, Canada, August 21-25, 2022</span>, pp. 855–862, IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
F. Boutros, P. Siebke, M. Klemt, N. Damer, F. Kirchbuchner, and A. Kuijper, “Pocketnet: Extreme lightweight face recognition network using neural architecture search and multistep knowledge distillation,” <span class="ltx_text ltx_font_italic" id="bib.bib163.1.1">IEEE Access</span>, vol. 10, pp. 46823–46833, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
N. Damer, F. Boutros, M. Süßmilch, M. Fang, F. Kirchbuchner, and A. Kuijper, “Masked face recognition: Human versus machine,” <span class="ltx_text ltx_font_italic" id="bib.bib164.1.1">IET Biom.</span>, vol. 11, no. 5, pp. 512–528, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
F. Boutros, N. Damer, F. Kirchbuchner, and A. Kuijper, “Self-restrained triplet loss for accurate masked face recognition,” <span class="ltx_text ltx_font_italic" id="bib.bib165.1.1">Pattern Recognit.</span>, vol. 124, p. 108473, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
M. Ngxande, J. Tapamo, and M. Burke, “Bias remediation in driver drowsiness detection systems using generative adversarial networks,” <span class="ltx_text ltx_font_italic" id="bib.bib166.1.1">IEEE Access</span>, vol. 8, pp. 55592–55601, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
J. Wörle, B. Metz, and A. Prill, “How to induce drowsiness when testing driver drowsiness and attention warning (DDAW) systems,” <span class="ltx_text ltx_font_italic" id="bib.bib167.1.1">IEEE Transactions on Intelligent Transportation Systems</span>, vol. 24, no. 5, pp. 4758–4764, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
M. Golz, D. Sommer, M. Holzbrecher, and T. Schnupp, “Detection and prediction of driver’s microsleep events,” in <span class="ltx_text ltx_font_italic" id="bib.bib168.1.1">Proc 14th Int Conf Road Safety on Four Continents</span>, vol. 11, Citeseer, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
R. Jabbar, K. Al-Khalifa, M. Kharbeche, W. K. M. Alhajyaseen, M. A. Jafari, and S. Jiang, “Real-time driver drowsiness detection for android application using deep neural networks techniques,” in <span class="ltx_text ltx_font_italic" id="bib.bib169.1.1">The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT 2018) / Affiliated Workshops, May 8-11, 2018, Porto, Portugal</span> (E. M. Shakshuki and A. Yasar, eds.), vol. 130 of <span class="ltx_text ltx_font_italic" id="bib.bib169.2.2">Procedia Computer Science</span>, pp. 400–407, Elsevier, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
R. Jabbar, K. Al-Khalifa, M. Kharbeche, W. K. M. Alhajyaseen, M. A. Jafari, and S. Jiang, “Real-time driver drowsiness detection for android application using deep neural networks techniques,” in <span class="ltx_text ltx_font_italic" id="bib.bib170.1.1">The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT 2018) / Affiliated Workshops, May 8-11, 2018, Porto, Portugal</span> (E. M. Shakshuki and A. Yasar, eds.), vol. 130 of <span class="ltx_text ltx_font_italic" id="bib.bib170.2.2">Procedia Computer Science</span>, pp. 400–407, Elsevier, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
W. Kong, W. Lin, F. Babiloni, S. Hu, and G. Borghini, “Investigating driver fatigue<em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1"> versus</em> alertness using the granger causality network,” <span class="ltx_text ltx_font_italic" id="bib.bib171.2.2">Sensors</span>, vol. 15, no. 8, pp. 19181–19198, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
F. Boutros, V. Struc, J. Fiérrez, and N. Damer, “Synthetic data for face recognition: Current state and future prospects,” <span class="ltx_text ltx_font_italic" id="bib.bib172.1.1">Image Vis. Comput.</span>, vol. 135, p. 104688, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
F. Boutros, J. H. Grebe, A. Kuijper, and N. Damer, “Idiff-face: Synthetic-based face recognition through fizzy identity-conditioned diffusion models,” in <span class="ltx_text ltx_font_italic" id="bib.bib173.1.1">IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023</span>, pp. 19593–19604, IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
S. Debbarma and S. Bhadra, “Signal decomposition method with sensor-fusion for reducing motion artifacts in intra-oral EEG,” in <span class="ltx_text ltx_font_italic" id="bib.bib174.1.1">2023 IEEE SENSORS, Vienna, Austria, October 29 - Nov. 1, 2023</span>, pp. 1–4, IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
M. S. Hossain, M. E. H. Chowdhury, M. B. I. Reaz, S. H. M. Ali, A. A. A. Bakar, S. Kiranyaz, A. Khandakar, M. Alhatou, R. Habib, and M. M. Hossain, “Motion artifacts correction from single-channel EEG and fnirs signals using novel wavelet packet decomposition in combination with canonical correlation analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib175.1.1">Sensors</span>, vol. 22, no. 9, p. 3169, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
P. Gajbhiye, R. K. Tripathy, A. Bhattacharyya, and R. B. Pachori, “Novel approaches for the removal of motion artifact from eeg recordings,” <span class="ltx_text ltx_font_italic" id="bib.bib176.1.1">IEEE Sensors Journal</span>, vol. 19, no. 22, pp. 10600–10608, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
J.-M. Lee, D.-J. Kim, I.-Y. Kim, K.-S. Park, and S. I. Kim, “Detrended fluctuation analysis of eeg in sleep apnea using mit/bih polysomnography data,” <span class="ltx_text ltx_font_italic" id="bib.bib177.1.1">Computers in biology and medicine</span>, vol. 32, no. 1, pp. 37–47, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Q. Ji, Z. Zhu, and P. Lan, “Real-time nonintrusive monitoring and prediction of driver fatigue,” <span class="ltx_text ltx_font_italic" id="bib.bib178.1.1">IEEE Trans. Veh. Technol.</span>, vol. 53, no. 4, pp. 1052–1068, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
H. U. R. Siddiqui, A. A. Saleem, R. Brown, B. Bademci, E. Lee, F. Rustam, and S. E. M. Dudley, “Non-invasive driver drowsiness detection system,” <span class="ltx_text ltx_font_italic" id="bib.bib179.1.1">Sensors</span>, vol. 21, no. 14, p. 4833, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
E. Zilberg, D. Burton, M. Xu, M. Karrar, and S. Lal, “Methodology and initial analysis results for development of non-invasive and hybrid driver drowsiness detection systems,” in <span class="ltx_text ltx_font_italic" id="bib.bib180.1.1">Advances in Broadband Communication and Networks</span>, pp. 309–328, River Publishers, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
G. Schalk, “Decoding details of human functions using electrocorticography,” in <span class="ltx_text ltx_font_italic" id="bib.bib181.1.1">4th International Winter Conference on Brain-Computer Interface, BCI 2016, Gangwon Province, South Korea, February 22-24, 2016</span>, pp. 1–2, IEEE, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
J. Ma, Y. L. Murphey, and H. Zhao, “Real time drowsiness detection based on lateral distance using wavelet transform and neural network,” in <span class="ltx_text ltx_font_italic" id="bib.bib182.1.1">IEEE Symposium Series on Computational Intelligence, SSCI 2015, Cape Town, South Africa, December 7-10, 2015</span>, pp. 411–418, IEEE, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
J. Zhao, C. Rong, C. Lin, and X. Dang, “Multivariate time series data imputation using attention-based mechanism,” <span class="ltx_text ltx_font_italic" id="bib.bib183.1.1">Neurocomputing</span>, vol. 542, p. 126238, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
S. Ryu, M. Kim, and H. Kim, “Denoising autoencoder-based missing value imputation for smart meters,” <span class="ltx_text ltx_font_italic" id="bib.bib184.1.1">IEEE Access</span>, vol. 8, pp. 40656–40666, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
I. G. Daza, L. M. Bergasa, S. Bronte, J. J. Yebes, J. Almazán, and R. Arroyo, “Fusion of optimized indicators from advanced driver assistance systems (ADAS) for driver drowsiness detection,” <span class="ltx_text ltx_font_italic" id="bib.bib185.1.1">Sensors</span>, vol. 14, no. 1, pp. 1106–1131, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
B. Cheng, W. Zhang, Y. Lin, R. Feng, and X. Zhang, “Driver drowsiness detection based on multisource information,” <span class="ltx_text ltx_font_italic" id="bib.bib186.1.1">Human Factors and Ergonomics in Manufacturing &amp; Service Industries</span>, vol. 22, no. 5, pp. 450–467, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
J. Gwak, A. Hirao, and M. Shino, “An investigation of early detection of driver drowsiness using ensemble machine learning based on hybrid sensing,” <span class="ltx_text ltx_font_italic" id="bib.bib187.1.1">Applied Sciences</span>, vol. 10, no. 8, p. 2890, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
S. Samiee, S. Azadi, R. Kazemi, A. Nahvi, and A. Eichberger, “Data fusion to develop a driver drowsiness detection system with robustness to signal loss,” <span class="ltx_text ltx_font_italic" id="bib.bib188.1.1">Sensors</span>, vol. 14, no. 9, pp. 17832–17847, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Z. Yang, R. O. Sinnott, J. Bailey, and Q. Ke, “A survey of automated data augmentation algorithms for deep learning-based image classification tasks,” <span class="ltx_text ltx_font_italic" id="bib.bib189.1.1">Knowledge and Information Systems</span>, vol. 65, no. 7, pp. 2805–2861, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
B. P. Nayak, S. Kar, A. Routray, and A. K. Padhi, “A biomedical approach to retrieve information on driver’s fatigue by integrating eeg, ecg and blood biomarkers during simulated driving session,” in <span class="ltx_text ltx_font_italic" id="bib.bib190.1.1">2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)</span>, pp. 1–6, IEEE, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
K. T. Chui, K. F. Tsang, H. R. Chi, B. W. K. Ling, and C. K. Wu, “An accurate ecg-based transportation safety drowsiness detection scheme,” <span class="ltx_text ltx_font_italic" id="bib.bib191.1.1">IEEE Transactions on Industrial Informatics</span>, vol. 12, no. 4, pp. 1438–1452, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
G. M. Mohamed, S. S. Patel, and N. Naicker, “Data augmentation for deep learning algorithms that perform driver drowsiness detection,” <span class="ltx_text ltx_font_italic" id="bib.bib192.1.1">International Journal of Advanced Computer Science and Applications</span>, vol. 14, pp. 233–248, 02 2023.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">X </span><span class="ltx_text ltx_font_smallcaps" id="S10.5.1">Biography Section</span>
</h2>
<figure class="ltx_float biography" id="S10.1">
<table class="ltx_tabular" id="S10.1.1">
<tr class="ltx_tr" id="S10.1.1.1">
<td class="ltx_td" id="S10.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="S10.1.1.1.1.g1" src="extracted/5809467/biying_gray.jpg" width="92"/></td>
<td class="ltx_td" id="S10.1.1.1.2">
<span class="ltx_inline-block" id="S10.1.1.1.2.1">
<span class="ltx_p" id="S10.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S10.1.1.1.2.1.1.1">Biying Fu</span> 
received the B.Sc. degree and the M.Sc. degree in electrical engineering and information technology from University of Karlsruhe, Germany, in 2011 and 2014 respectively. She finished her Ph.D. in 2020 in informatics at Technical University of Darmstadt, Germany, on the topic ”Sensor Applications for Human Activity Recognition in Smart Environments”. From 2014 till today, she is working with the Fraunhofer Institute for Computer Graphics (IGD), Germany. She is currently working in the department for Smart Living &amp; Biometric Technologies. Starting from 2022, she also works as a half-time Professor in Informatics at Hochschule RheinMain with the major ”Smart Environments”. Her research interest includes intelligent human machine interaction, machine learning, and deep learning for human activity recognition with sensor data.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="S10.2">
<table class="ltx_tabular" id="S10.2.1">
<tr class="ltx_tr" id="S10.2.1.1">
<td class="ltx_td" id="S10.2.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="120" id="S10.2.1.1.1.g1" src="extracted/5809467/fadi_boutros.jpeg" width="100"/></td>
<td class="ltx_td" id="S10.2.1.1.2">
<span class="ltx_inline-block" id="S10.2.1.1.2.1">
<span class="ltx_p" id="S10.2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S10.2.1.1.2.1.1.1">Fadi Boutros</span>  is a scientific researcher at the Fraunhofer IGD and a principal investigator at the National Research Center for Applied Cybersecurity ATHENE, Germany. Fadi received his Ph.D. in computer science from TU Darmstadt (2022) and a master’s degree in ”Distributed Software Systems” from TU Darmstadt (2019). Also, he is participating in the Software Campus program, a management program of the German Federal Ministry of Education and Research (BMBF).
He authored and co-authored several conference and journal papers. His main research interests lie in the fields of biometrics, machine learning, synthetic data, and efficient deep learning. For his scientific work, he received several awards, including the CAST-Förderpreis 2019 award, the IJCB 2022 Qualcomm Audience Choice Award, and the 2022 EAB Biometrics Industry Award from the European Association for Biometrics (EAB) for his Ph.D. dissertation.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="S10.3">
<table class="ltx_tabular" id="S10.3.1">
<tr class="ltx_tr" id="S10.3.1.1">
<td class="ltx_td" id="S10.3.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="S10.3.1.1.1.g1" src="extracted/5809467/CT_Lin_gray.jpg" width="100"/></td>
<td class="ltx_td" id="S10.3.1.1.2">
<span class="ltx_inline-block" id="S10.3.1.1.2.1">
<span class="ltx_p" id="S10.3.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S10.3.1.1.2.1.1.1">Chin-Teng Lin</span>  (Fellow, IEEE) received the B.Sc. degree from National Chiao-Tung University (NCTU), Taiwan in 1986, and holds Master’s and PhD degrees in Electrical Engineering from Purdue University, USA, received in 1989 and 1992, respectively. He is currently a distinguished professor at School of Computer Science and Director of the Human Centric AI (HAI) Centre and Co-Director of the Australian Artificial Intelligence Institute (AAII) within the Faculty of Engineering and Information Technology at the University of Technology Sydney, Australia. He is also an Honorary Chair Professor of Electrical and Computer Engineering at NCTU. For his contributions to biologically inspired information systems, Prof Lin was awarded Fellowship with the IEEE in 2005, and with the International Fuzzy Systems Association (IFSA) in 2012. He received the IEEE Fuzzy Systems Pioneer Award in 2017. He has held notable positions as editor-in-chief of IEEE Transactions on Fuzzy Systems from 2011 to 2016; seats on Board of Governors for the IEEE Circuits and Systems (CAS) Society (2005-2008), IEEE Systems, Man, Cybernetics (SMC) Society (2003-2005), IEEE Computational Intelligence Society (2008-2010); Chair of the IEEE Taipei Section (2009-2010); Chair of IEEE CIS Awards Committee (2022, 2023); Distinguished Lecturer with the IEEE CAS Society (2003-2005) and the CIS Society (2015-2017); Chair of the IEEE CIS Distinguished Lecturer Program Committee (2018-2019); Deputy Editor-in-Chief of IEEE Transactions on Circuits and Systems-II (2006-2008); Program Chair of the IEEE International Conference on Systems, Man, and Cybernetics (2005); and General Chair of the 2011 IEEE International Conference on Fuzzy Systems. He is the co-author of Neural Fuzzy Systems (Prentice-Hall) and the author of Neural Fuzzy Control Systems with Structure and Parameter Learning (World Scientific). He has published more than 450 journal papers including about 210 IEEE journal papers in the areas of neural networks, fuzzy systems, brain-computer interface, multimedia information processing, cognitive neuro-engineering, and human-machine teaming, that have been cited more than 36,000 times. His current h-index is 94, and his i10-index is 436.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="S10.4">
<table class="ltx_tabular" id="S10.4.1">
<tr class="ltx_tr" id="S10.4.1.1">
<td class="ltx_td" id="S10.4.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="S10.4.1.1.1.g1" src="extracted/5809467/Naser_Damer_crop.jpg" width="96"/></td>
<td class="ltx_td" id="S10.4.1.1.2">
<span class="ltx_inline-block" id="S10.4.1.1.2.1">
<span class="ltx_p" id="S10.4.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S10.4.1.1.2.1.1.1">Naser Damer</span>  (Senior Member, IEEE) received the Ph.D. degree in computer science from TU Darmstadt in 2018. He is a Senior Researcher with Fraunhofer IGD. He is a Research Area Co-Coordinator and a Principal Investigator with the National Research Center for Applied Cybersecurity ATHENE, Germany. He lectures on Human and Identity-Centric Machine Learning with TU Darmstadt, Germany. His main research interests lie in the fields of biometrics and human-centric machine learning. He serves as an Associate Editor for Pattern Recognition (Elsevier), the Visual Computer (Springer), and the IEEE Transactions on Information Forensics and Security. He represents the German Institute for Standardization (DIN) in the ISO/IEC SC37 International Biometrics Standardization Committee. He is a member of the organizing teams of several conferences, workshops, and special sessions, including being the program co-chair of BIOSIG and a member of the IEEE Biometrics Council serving on its Technical Activities Committee.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 23 11:14:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
