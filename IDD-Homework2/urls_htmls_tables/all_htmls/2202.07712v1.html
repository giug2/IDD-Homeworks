<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2202.07712] Privacy Preserving Visual Question Answering</title><meta property="og:description" content="We introduce a novel privacy-preserving methodology for performing Visual Question Answering on the edge. Our method constructs a symbolic representation of the visual scene, using a low-complexity computer vision mode…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Privacy Preserving Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Privacy Preserving Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2202.07712">

<!--Generated on Thu Mar  7 18:22:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Privacy Preserving Visual Question Answering
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Cristian-Paul Bara<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This work has been done during an internship at Amazon Alexa NLU</span></span></span>,<sup id="id1.1.id1" class="ltx_sup">1</sup>
Qing Ping, <sup id="id2.2.id2" class="ltx_sup">2</sup>
Abhinav Mathur, <sup id="id3.3.id3" class="ltx_sup">2</sup>
Govind Thattai, <sup id="id4.4.id4" class="ltx_sup">2</sup> 
<br class="ltx_break">Rohith MV, <sup id="id5.5.id5" class="ltx_sup">2</sup>
Gaurav S. Sukhatme <sup id="id6.6.id6" class="ltx_sup">2,3</sup>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">We introduce a novel privacy-preserving methodology for performing Visual Question Answering on the edge. Our method constructs a symbolic representation of the visual scene, using a low-complexity computer vision model that jointly predicts classes, attributes and predicates. This symbolic representation is non-differentiable, which means it cannot be used to recover the original image, thereby keeping the original image private. Our proposed hybrid solution uses a vision model
which is more than 25 times smaller than the current state-of-the-art (SOTA) vision models <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>, and 100 times smaller than end-to-end SOTA VQA models <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. We report detailed error analysis and discuss the trade-offs of using a distilled vision model and a symbolic representation of the visual scene.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The ubiquity of cameras in personal devices, monitoring devices, and appliances has advanced the skill-set of edge devices considerably, enabling them to answer questions through a combination of computer vision and natural language understanding. This is usually done by streaming video snippets to high-performance computing systems. However, as these images may contain private information (unlike a simple proximity sensor or an IR motion sensor), they can potentially compromise privacy. While some personal devices may alleviate these privacy concerns by performing image analysis, voice recognition, and semantic question-answering all on same device by incorporating expensive neural net hardware accelerators, it is impractical for most personal devices to be so equipped. In this paper, we explore whether accurate visual question answering (VQA) about a scene can be achieved in a cost-effective manner while preserving privacy, on devices that lack expensive edge-computation capabilities.</p>
</div>
<figure id="Sx1.F1" class="ltx_figure"><img src="/html/2202.07712/assets/x1.png" id="Sx1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="Sx1.F1.2.1" class="ltx_text ltx_font_bold">VQA model tradeoffs</span> with respect to accuracy, privacy and size. We look at three privacy categories. Not Private where the image is sent to the cloud, at risk where the raw class, and attribute scores as well as the bounding boxes are sent to the cloud, and Private where only symbolic representations of the seen are sent to the cloud. </figcaption>
</figure>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">The problem of answering questions based on an image has been tackled either through a two-stage network that has a region or grid based network feeding into a vision language model downstream to answer the questions, or by using three-stage networks that explicitly generate a symbolic relationship graph from the scene in order to address biases observed in other models <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>; Jiang et al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. While these methods are able to tackle a variety of questions, they have two shortcomings: (a) the model sizes are large - they typically have about 3-4 billion parameters which makes them unsuitable for realization in low-cost devices (b) though they can be split up into distinct image processing units (which could be distilled to run on the device) and question answering stages, the feature representation passed between the stages could potentially be decoded to reconstruct the image, thereby compromising privacy.</p>
</div>
<figure id="Sx1.F2" class="ltx_figure"><img src="/html/2202.07712/assets/x2.png" id="Sx1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="310" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The interconnection of our hybrid model structure. Image adapted from <cite class="ltx_cite ltx_citemacro_citep">(Yu et al. <a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite>.</figcaption>
</figure>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">We look at three levels of privacy as shown in Figure <a href="#Sx1.F1" title="Figure 1 ‣ Introduction ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. First is a setting where the model is trained end-to-end, and an intermediate feature representation is used as a means to bifurcate model complexity. We consider this not to be private since it is conceivable that an adversarial model can be trained to recover the original image. Second, where the model is trained in stages (as opposed to end-to-end), and the prediction distributions for both objects and attributes are used as intermediate representations. While this is a less descriptive representation of the input image than the end-to-end scenario, it still puts a privacy at risk since it is still a differentiable network that conceivably allows the original image to be recovered by model extraction methods. Third, we consider a symbolic representation of the scene. This approach makes the system non-differentiable, and makes it close to impossible to recover the original image, making the representation highly private.</p>
</div>
<figure id="Sx1.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x3.png" id="Sx1.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="332" height="195" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x4.png" id="Sx1.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="332" height="193" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of (a) scenes and questions where Bottom-Up answered incorrectly, but our model answered correctly and (b) scenes and questions where Bottom-Up answered correctly, but our model answered incorrectly.</figcaption>
</figure>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">The novel contributions of this paper are:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">A framework to enable privacy-preserving VQA with a small memory footprint edge-friendly model (reduction in footprint by <math id="Sx1.I1.i1.p1.1.m1.1" class="ltx_math_unparsed" alttext="&gt;25\times" display="inline"><semantics id="Sx1.I1.i1.p1.1.m1.1a"><mrow id="Sx1.I1.i1.p1.1.m1.1b"><mo id="Sx1.I1.i1.p1.1.m1.1.1">&gt;</mo><mn id="Sx1.I1.i1.p1.1.m1.1.2">25</mn><mo lspace="0.222em" id="Sx1.I1.i1.p1.1.m1.1.3">×</mo></mrow><annotation encoding="application/x-tex" id="Sx1.I1.i1.p1.1.m1.1c">&gt;25\times</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">A non-differentiable formulation using a symbolic representation to achieve competitive performance results on Yes/No type questions for VQA.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">An analysis of the trade-offs between privacy and performance.</p>
</div>
</li>
</ul>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">Our results show that even severely limited information about the scene, enables answering a multitude of questions related to presence, attributes, and relationships of objects, while limiting the possibility of an adversarial system to reconstruct the scene. We see a <math id="Sx1.p5.1.m1.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="Sx1.p5.1.m1.1a"><mrow id="Sx1.p5.1.m1.1.1" xref="Sx1.p5.1.m1.1.1.cmml"><mn id="Sx1.p5.1.m1.1.1.2" xref="Sx1.p5.1.m1.1.1.2.cmml">5</mn><mo id="Sx1.p5.1.m1.1.1.1" xref="Sx1.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p5.1.m1.1b"><apply id="Sx1.p5.1.m1.1.1.cmml" xref="Sx1.p5.1.m1.1.1"><csymbol cd="latexml" id="Sx1.p5.1.m1.1.1.1.cmml" xref="Sx1.p5.1.m1.1.1.1">percent</csymbol><cn type="integer" id="Sx1.p5.1.m1.1.1.2.cmml" xref="Sx1.p5.1.m1.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p5.1.m1.1c">5\%</annotation></semantics></math> drop in performance from a visual model with a 25x smaller footprint <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>. The component running on the device is also 100x smaller than the original end to end model <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. The performance drop is also be attributed to the fact that nearly half of the images in the dataset require information outside of what can be seen in the image, to generate accurate answers to the respective queries (Figure <a href="#Sx1.F3" title="Figure 3 ‣ Introduction ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). This also raises questions about how SOTA models end-to-end (without external knowledge) were able to answer these queries. We examine the possibility of captioning systems towards creating a human-understandable level of abstraction of input images. This opens up the possibility of analyzing long-term activities, trends, and anomalies using natural language processing techniques in the future.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Related Work</h2>

<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Visual Question Answering</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">The problem of making sense of visual scenes is one that has been thoroughly pursued by both the Computer Science and Natural Language Processing fields <cite class="ltx_cite ltx_citemacro_citep">(Wu et al. <a href="#bib.bib28" title="" class="ltx_ref">2017</a>; Sun et al. <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer.</p>
</div>
<div id="Sx2.SSx1.p2" class="ltx_para">
<p id="Sx2.SSx1.p2.1" class="ltx_p">The intersection of vision and language is an important topic today. To this end there is a large body of work done to gather and annotate data for the creation of models that successfully fuse the two modalities. Out of these datasets we note that the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al. <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite> enables modeling of relationships between objects. It provides dense annotations, of objects, attributes, and relationships within each image containing over 100K images each having an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. These annotations are canonical in region descriptions and question answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers. The image base for Visual Genome is the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al. <a href="#bib.bib19" title="" class="ltx_ref">2014</a>)</cite>. MS-COCO gathers images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentation to aid in precise object localization. In total there are 328k images with 2.5 million labeled instances on 91 object types in the COCO dataset. Another interesting dataset and benchmark is GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>. It is a dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. They leverage Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. The programs are used to have control over biases and answer distributions.</p>
</div>
<div id="Sx2.SSx1.p3" class="ltx_para">
<p id="Sx2.SSx1.p3.1" class="ltx_p">There is a growing number of benchmarks related to VQA. We look at two of them: two stage and three stage models. Two-stage models usually first extract visual features from the image, and then perform cross-modality fusions between visual features and language features to predict final answers. Among this line of work there are task-specific VQA models that are designed and trained specifically on VQA datasets <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>; Jiang et al. <a href="#bib.bib14" title="" class="ltx_ref">2018</a>; Yu et al. <a href="#bib.bib31" title="" class="ltx_ref">2019</a>, <a href="#bib.bib30" title="" class="ltx_ref">2020</a>; Guo, Xu, and Tao <a href="#bib.bib8" title="" class="ltx_ref">2021</a>; Jiang et al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. There are also joint vision-language models that are pre-trained on large vision-language datasets such as Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al. <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite> and Open Images <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova et al. <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> and then fine-tuned on VQA tasks <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal <a href="#bib.bib24" title="" class="ltx_ref">2019</a>; Lu et al. <a href="#bib.bib20" title="" class="ltx_ref">2019</a>; Li et al. <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Chen et al. <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Zhang et al. <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>. The three-stage models introduce an extra step of generating symbolic scene graph representation for images and symbolic neural module representations for questions, and perform symbolic reasoning between the two representations to predict final answers <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a href="#bib.bib29" title="" class="ltx_ref">2020</a>; Hu et al. <a href="#bib.bib10" title="" class="ltx_ref">2017</a>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="Sx2.SSx1.p4" class="ltx_para">
<p id="Sx2.SSx1.p4.1" class="ltx_p">While much inspirations can be drawn from above-mentioned work, these methods cannot be readily applied to our application which demands user information for privacy preservation. For most two-stage and three-stage models, the first step is usually to use a heavy-weight image feature extraction model to extract visual features. An example is the widely-used Bottom-Up model which is over 1GB footprint <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite> and thus cannot run on the device. In this case, the image has to be uploaded to a high-performance computation infrastructure, which imposes potential privacy risks as the image could be exposed to malicious usage.</p>
</div>
<div id="Sx2.SSx1.p5" class="ltx_para">
<p id="Sx2.SSx1.p5.1" class="ltx_p">One insight we learn from three-stage models is that they are separable modules. The QA models require the generation of a scene graph. Since symbolic outputs, similar in concept to scene graphs, are not differentiable they cannot be used to back-propagate through the model to recover the original image. Taking this into account there is no longer a need to run the whole VQA model on the edge or on dedicated computation infrastructure, but only up to a point where a symbolic representation can be generated on the device and used by downstream models. Towards this goal, if we consider deploying the image feature model on the edge, and uploading less private information to a computation infrastructure, the image feature model has to be light-weight, accurate in predicting symbolic representations such as object names, attributes and relations, and work well with downstream QA models. We consider that this addresses privacy, since it is non-differentiable information and thus cannot be used to reconstruct the original image.</p>
</div>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Vision Model Compression</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">Another important aspect to consider is the compression of neural network models <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al. <a href="#bib.bib5" title="" class="ltx_ref">2017</a>; Choudhary et al. <a href="#bib.bib6" title="" class="ltx_ref">2020</a>; Feng et al. <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Lei et al. <a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite>. Due to the multi-stage nature of most VQA models, there are relatively rare work to compress or distill an entire VQA model as a whole. On the other hand, there are quite some small-footprint vision models such as YOLO<cite class="ltx_cite ltx_citemacro_citep">(Bochkovskiy, Wang, and Liao <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, EfficientDet<cite class="ltx_cite ltx_citemacro_citep">(Tan, Pang, and Le <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> and so on that are trained from scratch with a smaller neural architecture. Yet these models are usually trained on a limited number of object classes (for example, EfficientDet is trained on 90 object MSCOCO classes) and do not support predicting object attributes and relations.</p>
</div>
<div id="Sx2.SSx2.p2" class="ltx_para">
<p id="Sx2.SSx2.p2.1" class="ltx_p">We look at EfficientNet <cite class="ltx_cite ltx_citemacro_citep">(Tan and Le <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> and EfficientDet <cite class="ltx_cite ltx_citemacro_citep">(Tan, Pang, and Le <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> which are compressed substitutes to ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al. <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> and Faster-RCNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al. <a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite> respectively. Convolutional Neural Networks (CNNs) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. EfficientNet proposes a uniform scaling of all dimensions of the model, i.e. depth, width, and resolution. They provide a family of models, EfficientNets, that outperform previous attempts. Their largest model achieves state of the art accuracy on ImageNet while being 8.4 times smaller and 6.1 times faster than its competitors. EfficientDet is an object detection network that builds upon an EfficientNet backbone. A key difference to competing large scale models, like Faster-RCNN, is that it replaces the region proposal network with a Bi-directional Feature Pyramid Network (BiFPN). This allows easy and fast multi-scale feature fusion. They harness EfficientNet’s scaling technique to produce a family of models that, on the COCO dataset, achieves state of the art performance at an up to 9 times smaller size and up to 42 time faster inference than other competing models. For our goal of developing a small-footprint image model that could predict symbolic information robustly, we propose to build a custom vision model that is trained on larger vocabulary of object classes and also predict object attribute information using EfficientDet as the model backbone.</p>
</div>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Methodology</h2>

<figure id="Sx3.F4" class="ltx_figure"><img src="/html/2202.07712/assets/x5.png" id="Sx3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="348" height="114" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Diagram of EfficientDet and additions. Image adapted from <cite class="ltx_cite ltx_citemacro_citep">(Tan, Pang, and Le <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>.</figcaption>
</figure>
<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">With user-facing VQA tasks there is always a concern with the privacy of the collected data. An obvious approach is to obfuscate the input streams. We take inspiration from the architecture of speech to downstream task architectures where there is conversion to a symbolic intermediate representation of the raw input audio measurement, i.e. audio to phonemes and subsequently words, before being sent to a language model for further processing. Current state-of-the-art vision models mostly extract visual features in continuous space and send these features to downstream VQA models. These features could be exploited to recover the original visual information and thus user privacy can be compromised. Instead, we propose to apply obfuscation at input and transform the visual image to symbolic representation first; consisting of object labels, attribute labels and so on, before sending them to downstream VQA models. This ensures that the intermediate symbolic information is not only addressing the privacy issue, but also more controllable in terms of what could be sent to downstream cloud QA models. We introduce this novel perspective in neural network model strategy. Since we implement raw to symbolic transformations for both speech and visual modalities, for the purpose of more control, over filtering out potentially private information, we can consider the symbolic representations as safe with regard to our initial privacy concerns. As such, it is only necessary to deploy the symbolic representation models on user devices and have the larger downstream language and visual QA models run on dedicated high-performance computation infrastructure.</p>
</div>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Model Overview</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.1" class="ltx_p">We chose to split our model into two-stages, a perception stage and a question answering stage, see Figure <a href="#Sx1.F2" title="Figure 2 ‣ Introduction ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. As opposed to other two-stage models like Bottom-Up <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite> or MiniVLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, we simplify the 3 stage graph generating approach where we rely on generating symbolic representations of the scene without going through the higher complexity relationship prediction stage. We use this symbolic output as a privacy granting representation. At a high level the two stages can be instances of any perception models and any downstream question answering model. This structure works as long as symbolic features can be extracted from the perception stage.</p>
</div>
<div id="Sx3.SSx1.p2" class="ltx_para">
<p id="Sx3.SSx1.p2.1" class="ltx_p">We continue with the description of our implementation composed of two stages: an on device image to symbolic representation models with a small footprint and a large scale QA model running on the cloud.</p>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Image Model</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.2" class="ltx_p">The starting point for our image processing module is EfficientDet-D0 <cite class="ltx_cite ltx_citemacro_citep">(Tan, Pang, and Le <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> backbone, due to it’s small size. The original EfficientDet’s base network structure is made up of an EfficientNet processed by a bidirectional feature pyramid network (BiFPN) which is then passed to two output heads. Both of these output heads are a suite of stacked convolutional layers that output the object bounding boxes and the object classes. To note that EfficientDet does not have a region proposal network (RPM) which also accounts for its small footprint. The model will always output <math id="Sx3.SSx2.p1.1.m1.1" class="ltx_Math" alttext="~{}48K" display="inline"><semantics id="Sx3.SSx2.p1.1.m1.1a"><mrow id="Sx3.SSx2.p1.1.m1.1.1" xref="Sx3.SSx2.p1.1.m1.1.1.cmml"><mn id="Sx3.SSx2.p1.1.m1.1.1.2" xref="Sx3.SSx2.p1.1.m1.1.1.2.cmml">48</mn><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.1.m1.1.1.1" xref="Sx3.SSx2.p1.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.SSx2.p1.1.m1.1.1.3" xref="Sx3.SSx2.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.1.m1.1b"><apply id="Sx3.SSx2.p1.1.m1.1.1.cmml" xref="Sx3.SSx2.p1.1.m1.1.1"><times id="Sx3.SSx2.p1.1.m1.1.1.1.cmml" xref="Sx3.SSx2.p1.1.m1.1.1.1"></times><cn type="integer" id="Sx3.SSx2.p1.1.m1.1.1.2.cmml" xref="Sx3.SSx2.p1.1.m1.1.1.2">48</cn><ci id="Sx3.SSx2.p1.1.m1.1.1.3.cmml" xref="Sx3.SSx2.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.1.m1.1c">~{}48K</annotation></semantics></math> objects and the relevant objects are picked if their object class prediction is over a threshold after non maximal suppression (at <math id="Sx3.SSx2.p1.2.m2.1" class="ltx_Math" alttext="IoU" display="inline"><semantics id="Sx3.SSx2.p1.2.m2.1a"><mrow id="Sx3.SSx2.p1.2.m2.1.1" xref="Sx3.SSx2.p1.2.m2.1.1.cmml"><mi id="Sx3.SSx2.p1.2.m2.1.1.2" xref="Sx3.SSx2.p1.2.m2.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.2.m2.1.1.1" xref="Sx3.SSx2.p1.2.m2.1.1.1.cmml">​</mo><mi id="Sx3.SSx2.p1.2.m2.1.1.3" xref="Sx3.SSx2.p1.2.m2.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="Sx3.SSx2.p1.2.m2.1.1.1a" xref="Sx3.SSx2.p1.2.m2.1.1.1.cmml">​</mo><mi id="Sx3.SSx2.p1.2.m2.1.1.4" xref="Sx3.SSx2.p1.2.m2.1.1.4.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx2.p1.2.m2.1b"><apply id="Sx3.SSx2.p1.2.m2.1.1.cmml" xref="Sx3.SSx2.p1.2.m2.1.1"><times id="Sx3.SSx2.p1.2.m2.1.1.1.cmml" xref="Sx3.SSx2.p1.2.m2.1.1.1"></times><ci id="Sx3.SSx2.p1.2.m2.1.1.2.cmml" xref="Sx3.SSx2.p1.2.m2.1.1.2">𝐼</ci><ci id="Sx3.SSx2.p1.2.m2.1.1.3.cmml" xref="Sx3.SSx2.p1.2.m2.1.1.3">𝑜</ci><ci id="Sx3.SSx2.p1.2.m2.1.1.4.cmml" xref="Sx3.SSx2.p1.2.m2.1.1.4">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx2.p1.2.m2.1c">IoU</annotation></semantics></math> default value 50%). There is no information exchanged between the output heads.</p>
</div>
<div id="Sx3.SSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.p2.1" class="ltx_p">To suit our purposes, we made several modifications to the base EfficientDet-D0 network. First, and most importantly, we introduce an attribute prediction network that makes a multi-label classification on the attributes. As in the base model, there is no information exchanges between the three output heads, i.e. attributes are predicted independently of class, as opposed to MiniVLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> where attribute prediction is dependant.</p>
</div>
<div id="Sx3.SSx2.p3" class="ltx_para">
<p id="Sx3.SSx2.p3.1" class="ltx_p">We train our vision model on visual genome on the same set of classes and attributes as previously done <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>. We first train the object classification and localization heads without the attribute head until convergence. Once the classification and localization heads are trained we reintroduce the attribute head and continue training with all three heads till convergence.</p>
</div>
<figure id="Sx3.F5" class="ltx_figure"><img src="/html/2202.07712/assets/x6.png" id="Sx3.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="266" height="331" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Privacy preserving symbolic representations. Top 5 class predictions are chosen and the GloVe embeddings of the class names are concatenated together. Top 5 attribute predictions are chosen and their GloVe embeddings are weighted by their confidence and summed. The top 5 wights are normalized. The bounding boxes are normalized to the image and to the enveloping bounding box. The Class, attribute, and bounding box representations are then concatenated and sent to MCAN. </figcaption>
</figure>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Symbolic Representation</h3>

<figure id="Sx3.T1" class="ltx_table">
<table id="Sx3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T1.1.1.1" class="ltx_tr">
<th id="Sx3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="Sx3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Name</span></th>
<th id="Sx3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="6"><span id="Sx3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Object Detection</span></th>
<th id="Sx3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4"><span id="Sx3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Attribute Prediction</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T1.1.2.1" class="ltx_tr">
<th id="Sx3.T1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="Sx3.T1.1.2.1.2" class="ltx_td ltx_align_right"><span id="Sx3.T1.1.2.1.2.1" class="ltx_text ltx_font_bold">AP</span></td>
<td id="Sx3.T1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_r"><span id="Sx3.T1.1.2.1.3.1" class="ltx_text ltx_font_bold">AR</span></td>
<td id="Sx3.T1.1.2.1.4" class="ltx_td ltx_align_right"><span id="Sx3.T1.1.2.1.4.1" class="ltx_text ltx_font_bold">A</span></td>
<td id="Sx3.T1.1.2.1.5" class="ltx_td ltx_align_right"><span id="Sx3.T1.1.2.1.5.1" class="ltx_text ltx_font_bold">P</span></td>
<td id="Sx3.T1.1.2.1.6" class="ltx_td ltx_align_right"><span id="Sx3.T1.1.2.1.6.1" class="ltx_text ltx_font_bold">R</span></td>
<td id="Sx3.T1.1.2.1.7" class="ltx_td ltx_align_right ltx_border_r"><span id="Sx3.T1.1.2.1.7.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="Sx3.T1.1.2.1.8" class="ltx_td ltx_align_right"><span id="Sx3.T1.1.2.1.8.1" class="ltx_text ltx_font_bold">A</span></td>
<td id="Sx3.T1.1.2.1.9" class="ltx_td ltx_align_right"><span id="Sx3.T1.1.2.1.9.1" class="ltx_text ltx_font_bold">P</span></td>
<td id="Sx3.T1.1.2.1.10" class="ltx_td ltx_align_right"><span id="Sx3.T1.1.2.1.10.1" class="ltx_text ltx_font_bold">R</span></td>
<td id="Sx3.T1.1.2.1.11" class="ltx_td ltx_align_right"><span id="Sx3.T1.1.2.1.11.1" class="ltx_text ltx_font_bold">F1</span></td>
</tr>
<tr id="Sx3.T1.1.3.2" class="ltx_tr">
<th id="Sx3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Bottom-Up</th>
<td id="Sx3.T1.1.3.2.2" class="ltx_td ltx_align_right">32.50</td>
<td id="Sx3.T1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_r">45.67</td>
<td id="Sx3.T1.1.3.2.4" class="ltx_td ltx_align_right">62.53</td>
<td id="Sx3.T1.1.3.2.5" class="ltx_td ltx_align_right">87.02</td>
<td id="Sx3.T1.1.3.2.6" class="ltx_td ltx_align_right">71.24</td>
<td id="Sx3.T1.1.3.2.7" class="ltx_td ltx_align_right ltx_border_r">78.34</td>
<td id="Sx3.T1.1.3.2.8" class="ltx_td ltx_align_right">16.23</td>
<td id="Sx3.T1.1.3.2.9" class="ltx_td ltx_align_right">25.62</td>
<td id="Sx3.T1.1.3.2.10" class="ltx_td ltx_align_right">30.71</td>
<td id="Sx3.T1.1.3.2.11" class="ltx_td ltx_align_right">27.93</td>
</tr>
<tr id="Sx3.T1.1.4.3" class="ltx_tr">
<th id="Sx3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Ours</th>
<td id="Sx3.T1.1.4.3.2" class="ltx_td ltx_align_right">39.30</td>
<td id="Sx3.T1.1.4.3.3" class="ltx_td ltx_align_right ltx_border_r">50.48</td>
<td id="Sx3.T1.1.4.3.4" class="ltx_td ltx_align_right">47.43</td>
<td id="Sx3.T1.1.4.3.5" class="ltx_td ltx_align_right">84.62</td>
<td id="Sx3.T1.1.4.3.6" class="ltx_td ltx_align_right">67.13</td>
<td id="Sx3.T1.1.4.3.7" class="ltx_td ltx_align_right ltx_border_r">74.87</td>
<td id="Sx3.T1.1.4.3.8" class="ltx_td ltx_align_right">33.51</td>
<td id="Sx3.T1.1.4.3.9" class="ltx_td ltx_align_right">64.33</td>
<td id="Sx3.T1.1.4.3.10" class="ltx_td ltx_align_right">41.16</td>
<td id="Sx3.T1.1.4.3.11" class="ltx_td ltx_align_right">50.20</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of the performance on object detection with attribute prediction of Bottom-Up, and EfficientDet. We measure Class agnostic Average Precision (AP) class agnostic Average Recall (AR). For detected objects we also compute the Accuracy (A), Precision (P), Recall (R), and F1 score for both class and attribute prediction. </figcaption>
</figure>
<div id="Sx3.SSx3.p1" class="ltx_para">
<p id="Sx3.SSx3.p1.6" class="ltx_p">The symbolic representation of the visual scene uses different sections of <math id="Sx3.SSx3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{R}^{2048}" display="inline"><semantics id="Sx3.SSx3.p1.1.m1.1a"><msup id="Sx3.SSx3.p1.1.m1.1.1" xref="Sx3.SSx3.p1.1.m1.1.1.cmml"><mi id="Sx3.SSx3.p1.1.m1.1.1.2" xref="Sx3.SSx3.p1.1.m1.1.1.2.cmml">𝐑</mi><mn id="Sx3.SSx3.p1.1.m1.1.1.3" xref="Sx3.SSx3.p1.1.m1.1.1.3.cmml">2048</mn></msup><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.1.m1.1b"><apply id="Sx3.SSx3.p1.1.m1.1.1.cmml" xref="Sx3.SSx3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.1.m1.1.1.1.cmml" xref="Sx3.SSx3.p1.1.m1.1.1">superscript</csymbol><ci id="Sx3.SSx3.p1.1.m1.1.1.2.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.2">𝐑</ci><cn type="integer" id="Sx3.SSx3.p1.1.m1.1.1.3.cmml" xref="Sx3.SSx3.p1.1.m1.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.1.m1.1c">\mathbf{R}^{2048}</annotation></semantics></math> vector to represent the various information categories. As previously stated, we use a set of 1600 object classes and 400 attributes. We use the output predictions for object classes, attributes, and bounding boxes and construct a symbolic representation of the image. For object classes, we select the top five class predictions (by confidence score) and concatenate the GloVe <cite class="ltx_cite ltx_citemacro_citep">(Pennington, Socher, and Manning <a href="#bib.bib21" title="" class="ltx_ref">2014</a>)</cite> embeddings of the names of those classes which gives us a <math id="Sx3.SSx3.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{R}^{1500}" display="inline"><semantics id="Sx3.SSx3.p1.2.m2.1a"><msup id="Sx3.SSx3.p1.2.m2.1.1" xref="Sx3.SSx3.p1.2.m2.1.1.cmml"><mi id="Sx3.SSx3.p1.2.m2.1.1.2" xref="Sx3.SSx3.p1.2.m2.1.1.2.cmml">𝐑</mi><mn id="Sx3.SSx3.p1.2.m2.1.1.3" xref="Sx3.SSx3.p1.2.m2.1.1.3.cmml">1500</mn></msup><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.2.m2.1b"><apply id="Sx3.SSx3.p1.2.m2.1.1.cmml" xref="Sx3.SSx3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.2.m2.1.1.1.cmml" xref="Sx3.SSx3.p1.2.m2.1.1">superscript</csymbol><ci id="Sx3.SSx3.p1.2.m2.1.1.2.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.2">𝐑</ci><cn type="integer" id="Sx3.SSx3.p1.2.m2.1.1.3.cmml" xref="Sx3.SSx3.p1.2.m2.1.1.3">1500</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.2.m2.1c">\mathbf{R}^{1500}</annotation></semantics></math> vector. For attributes we similarly take the top five attributes and compute the sum of the GloVe embeddings of the names of those attributes weighted by their output scores which gives us a <math id="Sx3.SSx3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{R}^{300}" display="inline"><semantics id="Sx3.SSx3.p1.3.m3.1a"><msup id="Sx3.SSx3.p1.3.m3.1.1" xref="Sx3.SSx3.p1.3.m3.1.1.cmml"><mi id="Sx3.SSx3.p1.3.m3.1.1.2" xref="Sx3.SSx3.p1.3.m3.1.1.2.cmml">𝐑</mi><mn id="Sx3.SSx3.p1.3.m3.1.1.3" xref="Sx3.SSx3.p1.3.m3.1.1.3.cmml">300</mn></msup><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.3.m3.1b"><apply id="Sx3.SSx3.p1.3.m3.1.1.cmml" xref="Sx3.SSx3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.3.m3.1.1.1.cmml" xref="Sx3.SSx3.p1.3.m3.1.1">superscript</csymbol><ci id="Sx3.SSx3.p1.3.m3.1.1.2.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.2">𝐑</ci><cn type="integer" id="Sx3.SSx3.p1.3.m3.1.1.3.cmml" xref="Sx3.SSx3.p1.3.m3.1.1.3">300</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.3.m3.1c">\mathbf{R}^{300}</annotation></semantics></math> vector. For bounding boxes we make two representations - first, we normalize the bounding box to the original image and second we normalize the bounding boxes to the encompassing bounding box which gives us a <math id="Sx3.SSx3.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{R}^{8}" display="inline"><semantics id="Sx3.SSx3.p1.4.m4.1a"><msup id="Sx3.SSx3.p1.4.m4.1.1" xref="Sx3.SSx3.p1.4.m4.1.1.cmml"><mi id="Sx3.SSx3.p1.4.m4.1.1.2" xref="Sx3.SSx3.p1.4.m4.1.1.2.cmml">𝐑</mi><mn id="Sx3.SSx3.p1.4.m4.1.1.3" xref="Sx3.SSx3.p1.4.m4.1.1.3.cmml">8</mn></msup><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.4.m4.1b"><apply id="Sx3.SSx3.p1.4.m4.1.1.cmml" xref="Sx3.SSx3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.4.m4.1.1.1.cmml" xref="Sx3.SSx3.p1.4.m4.1.1">superscript</csymbol><ci id="Sx3.SSx3.p1.4.m4.1.1.2.cmml" xref="Sx3.SSx3.p1.4.m4.1.1.2">𝐑</ci><cn type="integer" id="Sx3.SSx3.p1.4.m4.1.1.3.cmml" xref="Sx3.SSx3.p1.4.m4.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.4.m4.1c">\mathbf{R}^{8}</annotation></semantics></math> vector. We make two bounding box representations in order to transmit information global information about the scene as well as relative locations between the bounding boxes. We then concatenate the class, attribute, and bounding box representations together which gives us a <math id="Sx3.SSx3.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{R}^{1808}" display="inline"><semantics id="Sx3.SSx3.p1.5.m5.1a"><msup id="Sx3.SSx3.p1.5.m5.1.1" xref="Sx3.SSx3.p1.5.m5.1.1.cmml"><mi id="Sx3.SSx3.p1.5.m5.1.1.2" xref="Sx3.SSx3.p1.5.m5.1.1.2.cmml">𝐑</mi><mn id="Sx3.SSx3.p1.5.m5.1.1.3" xref="Sx3.SSx3.p1.5.m5.1.1.3.cmml">1808</mn></msup><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.5.m5.1b"><apply id="Sx3.SSx3.p1.5.m5.1.1.cmml" xref="Sx3.SSx3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.5.m5.1.1.1.cmml" xref="Sx3.SSx3.p1.5.m5.1.1">superscript</csymbol><ci id="Sx3.SSx3.p1.5.m5.1.1.2.cmml" xref="Sx3.SSx3.p1.5.m5.1.1.2">𝐑</ci><cn type="integer" id="Sx3.SSx3.p1.5.m5.1.1.3.cmml" xref="Sx3.SSx3.p1.5.m5.1.1.3">1808</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.5.m5.1c">\mathbf{R}^{1808}</annotation></semantics></math> vector. This representation is then padded to create a <math id="Sx3.SSx3.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{R}^{2048}" display="inline"><semantics id="Sx3.SSx3.p1.6.m6.1a"><msup id="Sx3.SSx3.p1.6.m6.1.1" xref="Sx3.SSx3.p1.6.m6.1.1.cmml"><mi id="Sx3.SSx3.p1.6.m6.1.1.2" xref="Sx3.SSx3.p1.6.m6.1.1.2.cmml">𝐑</mi><mn id="Sx3.SSx3.p1.6.m6.1.1.3" xref="Sx3.SSx3.p1.6.m6.1.1.3.cmml">2048</mn></msup><annotation-xml encoding="MathML-Content" id="Sx3.SSx3.p1.6.m6.1b"><apply id="Sx3.SSx3.p1.6.m6.1.1.cmml" xref="Sx3.SSx3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="Sx3.SSx3.p1.6.m6.1.1.1.cmml" xref="Sx3.SSx3.p1.6.m6.1.1">superscript</csymbol><ci id="Sx3.SSx3.p1.6.m6.1.1.2.cmml" xref="Sx3.SSx3.p1.6.m6.1.1.2">𝐑</ci><cn type="integer" id="Sx3.SSx3.p1.6.m6.1.1.3.cmml" xref="Sx3.SSx3.p1.6.m6.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx3.p1.6.m6.1c">\mathbf{R}^{2048}</annotation></semantics></math> vector which is passed to MCAN.
We also implement a BERT embedding structure where the question and the tuples of classes and attributes are all embedded by a pretrained BERT model. The classes and attributes are presented as a sequence of five words and processed to produce an embedding for each tuple, classes and attributes. The question is also embedded as a statement and presented to the downstream QA model.</p>
</div>
</section>
<section id="Sx3.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">QA Model</h3>

<div id="Sx3.SSx4.p1" class="ltx_para">
<p id="Sx3.SSx4.p1.1" class="ltx_p">Our formulation of symbolic inputs could be applied in a plug-and-play manner, meaning all existing QA models that takes in image feature and text inputs could potentially work with our symbolic inputs.</p>
</div>
<div id="Sx3.SSx4.p2" class="ltx_para">
<p id="Sx3.SSx4.p2.1" class="ltx_p">We use the architecture of MCAN <cite class="ltx_cite ltx_citemacro_citep">(Yu et al. <a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> to demonstrate our approach. While we retain the deep co-attention learning, and multimodal fusion and output classifier stages, the novelty of our approach is that we separate the input modality perception stages, specifically the vision model, and run them in a de-coupled fashion. Moreover, we replace the intermediate layer representation from the vision model, originally an adaptation of Faster-RCNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al. <a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite> to predict attributes and subsequently using Bottom-up Attention, with a symbolic representation of the scene. Given an output from a differentiable model it is conceptually possible to reverse-engineer and recreate the original image. The purpose of the symbolic representation is to break the differentiable link from the originally captured image to the input of MCAN, and hence preserve the privacy.</p>
</div>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experimental Results</h2>

<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Datasets</h3>

<div id="Sx4.SSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.p1.1" class="ltx_p">For the custom EfficientDet model, the Visual Genome dataset is used for pretraining with 1600 object classes and 400 attribute classes <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al. <a href="#bib.bib15" title="" class="ltx_ref">2017</a>; Anderson et al. <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>. For the VQA task, the VQA 2.0 dataset is used for evaluating our models <cite class="ltx_cite ltx_citemacro_citep">(Antol et al. <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>. The VQA 2.0 dataset has 0.25M images, 0.76M questions, and 10M answers.</p>
</div>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Experimental Settings</h3>

<div id="Sx4.SSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.p1.1" class="ltx_p">For the custom EfficientDet model, the adamw optimizer is used with an adaptive learning rate starting at 0.001. The model was trained on a 8 GPU P3.8-instance for 3 days for the object classification head, and then trained for 2 days for the attribute classification head. For the VQA model (MCAN), the adamw optimizer is used with an adaptive learning rate, starting at 0.01. The model is trained on a 1 GPU P2-instance for 24 hours.</p>
</div>
</section>
<section id="Sx4.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Model Performance</h3>

<section id="Sx4.SSx3.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Image Model</h4>

<figure id="Sx4.T2" class="ltx_table">
<table id="Sx4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T2.1.1.1" class="ltx_tr">
<td id="Sx4.T2.1.1.1.1" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Visual</span></td>
<td id="Sx4.T2.1.1.1.2" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Privacy</span></td>
<td id="Sx4.T2.1.1.1.3" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Deployability</span></td>
<td id="Sx4.T2.1.1.1.4" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Visual Model</span></td>
<td id="Sx4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">Feature</span></td>
<td id="Sx4.T2.1.1.1.6" class="ltx_td ltx_align_center" colspan="4"><span id="Sx4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">Performance</span></td>
</tr>
<tr id="Sx4.T2.1.2.2" class="ltx_tr">
<td id="Sx4.T2.1.2.2.1" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.2.2.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="Sx4.T2.1.2.2.2" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.2.2.2.1" class="ltx_text ltx_font_bold">Status</span></td>
<td id="Sx4.T2.1.2.2.3" class="ltx_td"></td>
<td id="Sx4.T2.1.2.2.4" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.2.2.4.1" class="ltx_text ltx_font_bold">Num. Param.</span></td>
<td id="Sx4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.2.2.5.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="Sx4.T2.1.2.2.6" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.2.2.6.1" class="ltx_text ltx_font_bold">Overall</span></td>
<td id="Sx4.T2.1.2.2.7" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.2.2.7.1" class="ltx_text ltx_font_bold">Other</span></td>
<td id="Sx4.T2.1.2.2.8" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.2.2.8.1" class="ltx_text ltx_font_bold">Yes/No</span></td>
<td id="Sx4.T2.1.2.2.9" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.2.2.9.1" class="ltx_text ltx_font_bold">Count</span></td>
</tr>
<tr id="Sx4.T2.1.3.3" class="ltx_tr">
<td id="Sx4.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">Bottom-Up</td>
<td id="Sx4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">Not Private</td>
<td id="Sx4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">No</td>
<td id="Sx4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">153M</td>
<td id="Sx4.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Intermediate</td>
<td id="Sx4.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">67.08</td>
<td id="Sx4.T2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">58.41</td>
<td id="Sx4.T2.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">84.73</td>
<td id="Sx4.T2.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">49.19</td>
</tr>
<tr id="Sx4.T2.1.4.4" class="ltx_tr">
<td id="Sx4.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_t">Bottom-Up</td>
<td id="Sx4.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">At risk</td>
<td id="Sx4.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">No</td>
<td id="Sx4.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">153M</td>
<td id="Sx4.T2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Raw</td>
<td id="Sx4.T2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">63.31</td>
<td id="Sx4.T2.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">54.48</td>
<td id="Sx4.T2.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">81.72</td>
<td id="Sx4.T2.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">43.81</td>
</tr>
<tr id="Sx4.T2.1.5.5" class="ltx_tr">
<td id="Sx4.T2.1.5.5.1" class="ltx_td ltx_align_center">Bottom-Up</td>
<td id="Sx4.T2.1.5.5.2" class="ltx_td ltx_align_center">Private</td>
<td id="Sx4.T2.1.5.5.3" class="ltx_td ltx_align_center">No</td>
<td id="Sx4.T2.1.5.5.4" class="ltx_td ltx_align_center">153M</td>
<td id="Sx4.T2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r">Symbolic (GloVe)</td>
<td id="Sx4.T2.1.5.5.6" class="ltx_td ltx_align_center">62.49</td>
<td id="Sx4.T2.1.5.5.7" class="ltx_td ltx_align_center">53.78</td>
<td id="Sx4.T2.1.5.5.8" class="ltx_td ltx_align_center">81.05</td>
<td id="Sx4.T2.1.5.5.9" class="ltx_td ltx_align_center">42.10</td>
</tr>
<tr id="Sx4.T2.1.6.6" class="ltx_tr">
<td id="Sx4.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_t">Ours</td>
<td id="Sx4.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">At Risk</td>
<td id="Sx4.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="Sx4.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">5.75M</td>
<td id="Sx4.T2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Raw</td>
<td id="Sx4.T2.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">56.67</td>
<td id="Sx4.T2.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t">44.19</td>
<td id="Sx4.T2.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">77.98</td>
<td id="Sx4.T2.1.6.6.9" class="ltx_td ltx_align_center ltx_border_t">42.57</td>
</tr>
<tr id="Sx4.T2.1.7.7" class="ltx_tr">
<td id="Sx4.T2.1.7.7.1" class="ltx_td ltx_align_center">Ours</td>
<td id="Sx4.T2.1.7.7.2" class="ltx_td ltx_align_center">Private</td>
<td id="Sx4.T2.1.7.7.3" class="ltx_td ltx_align_center">Yes</td>
<td id="Sx4.T2.1.7.7.4" class="ltx_td ltx_align_center">5.75M</td>
<td id="Sx4.T2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r">Symbolic (GloVe)</td>
<td id="Sx4.T2.1.7.7.6" class="ltx_td ltx_align_center">55.41</td>
<td id="Sx4.T2.1.7.7.7" class="ltx_td ltx_align_center">42.95</td>
<td id="Sx4.T2.1.7.7.8" class="ltx_td ltx_align_center">77.27</td>
<td id="Sx4.T2.1.7.7.9" class="ltx_td ltx_align_center">41.89</td>
</tr>
<tr id="Sx4.T2.1.8.8" class="ltx_tr">
<td id="Sx4.T2.1.8.8.1" class="ltx_td ltx_align_center">Ours</td>
<td id="Sx4.T2.1.8.8.2" class="ltx_td ltx_align_center">Private</td>
<td id="Sx4.T2.1.8.8.3" class="ltx_td ltx_align_center">Yes</td>
<td id="Sx4.T2.1.8.8.4" class="ltx_td ltx_align_center">5.75M</td>
<td id="Sx4.T2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r">Symbolic (BERT)</td>
<td id="Sx4.T2.1.8.8.6" class="ltx_td ltx_align_center">54.17</td>
<td id="Sx4.T2.1.8.8.7" class="ltx_td ltx_align_center">72.10</td>
<td id="Sx4.T2.1.8.8.8" class="ltx_td ltx_align_center">75.25</td>
<td id="Sx4.T2.1.8.8.9" class="ltx_td ltx_align_center">39.16</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of MCAN on VQA 2.0 using the output from Bottom-Up or EfficientDet with raw predictions representations or fully symbolic output. We also provide the performance of MCAN using the intermediate output provided by Bottom-Up as trained end-to-end as measured by us. </figcaption>
</figure>
<div id="Sx4.SSx3.SSSx1.p1" class="ltx_para">
<p id="Sx4.SSx3.SSSx1.p1.1" class="ltx_p">To evaluate the change in performance of our vision model as compared to the reference model, Bottom-Up, we look at three aspects. First we look at class agnostic object detection where we measure Average Precision and Average Recall. Second, among the detected objects we evaluate the accuracy, precision, recall, and F1 score of the class prediction. Third, also for the detected objects, we compute accuracy, precision, recall, and F1 score for the attribute prediction task.</p>
</div>
<div id="Sx4.SSx3.SSSx1.p2" class="ltx_para">
<p id="Sx4.SSx3.SSSx1.p2.1" class="ltx_p">We conduct an evaluation of the vision model itself as compared to the reference model, Bottom-Up ( Table-<a href="#Sx3.T1" title="Table 1 ‣ Symbolic Representation ‣ Methodology ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). We notice the our model detects more objects although among the detected objects EfficientDet has a lower rate of successful classifications. On the contrary, the low footprint model has a much better success rate in predicting attributes.</p>
</div>
</section>
<section id="Sx4.SSx3.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Overall System Performance</h4>

<div id="Sx4.SSx3.SSSx2.p1" class="ltx_para">
<p id="Sx4.SSx3.SSSx2.p1.5" class="ltx_p">Since we use MCAN with minimal modifications we use the metrics described in <cite class="ltx_cite ltx_citemacro_citep">(Yu et al. <a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite>. The experimental setting starts with exported output from a candidate visual network, i.e. EfficientDet with attributes or Bottom-Up. MCAN is trained alone on different types of representations. For Bottom-Up there are three representations. First, we use the intermediate representations as used in <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. These were trained end to end with both Bottom-up and MCAN and used as provided by the authors. Second, we use the raw predictions of the model, i.e classes <math id="Sx4.SSx3.SSSx2.p1.1.m1.1" class="ltx_Math" alttext="\in\textbf{R}^{1600}" display="inline"><semantics id="Sx4.SSx3.SSSx2.p1.1.m1.1a"><mrow id="Sx4.SSx3.SSSx2.p1.1.m1.1.1" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.cmml"><mi id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.2" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.2.cmml"></mi><mo id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.1" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.2" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.2a.cmml">R</mtext><mn id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.3" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.3.cmml">1600</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p1.1.m1.1b"><apply id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1"><in id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.1"></in><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.2">absent</csymbol><apply id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.cmml" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.1.cmml" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3">superscript</csymbol><ci id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.2a.cmml" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.2.cmml" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.2">R</mtext></ci><cn type="integer" id="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.3.cmml" xref="Sx4.SSx3.SSSx2.p1.1.m1.1.1.3.3">1600</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p1.1.m1.1c">\in\textbf{R}^{1600}</annotation></semantics></math> concatenated with attributes <math id="Sx4.SSx3.SSSx2.p1.2.m2.1" class="ltx_Math" alttext="\in\textbf{R}^{400}" display="inline"><semantics id="Sx4.SSx3.SSSx2.p1.2.m2.1a"><mrow id="Sx4.SSx3.SSSx2.p1.2.m2.1.1" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.cmml"><mi id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.2" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.2.cmml"></mi><mo id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.1" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.1.cmml">∈</mo><msup id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.2" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.2a.cmml">R</mtext><mn id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.3" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.3.cmml">400</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p1.2.m2.1b"><apply id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1"><in id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.1"></in><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.2">absent</csymbol><apply id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.cmml" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.1.cmml" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3">superscript</csymbol><ci id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.2a.cmml" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.2.cmml" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.2">R</mtext></ci><cn type="integer" id="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.3.cmml" xref="Sx4.SSx3.SSSx2.p1.2.m2.1.1.3.3">400</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p1.2.m2.1c">\in\textbf{R}^{400}</annotation></semantics></math> and a global (to the image) and relative (among boxes) normalization <math id="Sx4.SSx3.SSSx2.p1.3.m3.1" class="ltx_Math" alttext="\in\textbf{R}^{8}" display="inline"><semantics id="Sx4.SSx3.SSSx2.p1.3.m3.1a"><mrow id="Sx4.SSx3.SSSx2.p1.3.m3.1.1" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.cmml"><mi id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.2" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.2.cmml"></mi><mo id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.1" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.1.cmml">∈</mo><msup id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.2" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.2a.cmml">R</mtext><mn id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.3" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.3.cmml">8</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p1.3.m3.1b"><apply id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1"><in id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.1"></in><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.2">absent</csymbol><apply id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.cmml" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.1.cmml" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3">superscript</csymbol><ci id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.2a.cmml" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.2.cmml" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.2">R</mtext></ci><cn type="integer" id="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.3.cmml" xref="Sx4.SSx3.SSSx2.p1.3.m3.1.1.3.3">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p1.3.m3.1c">\in\textbf{R}^{8}</annotation></semantics></math> which is padded to a vector <math id="Sx4.SSx3.SSSx2.p1.4.m4.1" class="ltx_Math" alttext="\in\textbf{R}^{2048}" display="inline"><semantics id="Sx4.SSx3.SSSx2.p1.4.m4.1a"><mrow id="Sx4.SSx3.SSSx2.p1.4.m4.1.1" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.cmml"><mi id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.2" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.2.cmml"></mi><mo id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.1" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.1.cmml">∈</mo><msup id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.2" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.2a.cmml">R</mtext><mn id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.3" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p1.4.m4.1b"><apply id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1"><in id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.1"></in><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.2">absent</csymbol><apply id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.cmml" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.1.cmml" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3">superscript</csymbol><ci id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.2a.cmml" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.2.cmml" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.2">R</mtext></ci><cn type="integer" id="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.3.cmml" xref="Sx4.SSx3.SSSx2.p1.4.m4.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p1.4.m4.1c">\in\textbf{R}^{2048}</annotation></semantics></math> and presented to MCAN as the image representation. Third and finally, we use our novel symbolic representation. For classes we take the GloVe embeddings of the names of the top 5 class predictions and concatenate them into a vector <math id="Sx4.SSx3.SSSx2.p1.5.m5.1" class="ltx_Math" alttext="\in\textbf{R}^{1500}" display="inline"><semantics id="Sx4.SSx3.SSSx2.p1.5.m5.1a"><mrow id="Sx4.SSx3.SSSx2.p1.5.m5.1.1" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.cmml"><mi id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.2" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.2.cmml"></mi><mo id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.1" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.1.cmml">∈</mo><msup id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.2" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.2a.cmml">R</mtext><mn id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.3" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.3.cmml">1500</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p1.5.m5.1b"><apply id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1"><in id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.1"></in><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.2">absent</csymbol><apply id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.cmml" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.1.cmml" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3">superscript</csymbol><ci id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.2a.cmml" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.2.cmml" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.2">R</mtext></ci><cn type="integer" id="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.3.cmml" xref="Sx4.SSx3.SSSx2.p1.5.m5.1.1.3.3">1500</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p1.5.m5.1c">\in\textbf{R}^{1500}</annotation></semantics></math>. For attributes we take the top 5 predictions and compute a sum, weighted by their confidence, of the GloVe embeddings of the names of the attributes. We also reproduce the same settings with our modification of EfficientDet-D0 with attribute prediction except for the intermediate layer. Since our goal is to break the back-propagation flow to make the representation private, there is no opportunity to train MCAN end to end with EfficientDet to generate the intermediate representations.</p>
</div>
<div id="Sx4.SSx3.SSSx2.p2" class="ltx_para">
<p id="Sx4.SSx3.SSSx2.p2.1" class="ltx_p">When using BERT the question is embedded as a single embedding. This replaces the LSTM in MCAN as well. We notice that the setting using BERT embedding instead of GloVe does not beat the performance of the standard MCAN implementation.</p>
</div>
<figure id="Sx4.T3" class="ltx_table">
<table id="Sx4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T3.1.1.1" class="ltx_tr">
<td id="Sx4.T3.1.1.1.1" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Visual</span></td>
<td id="Sx4.T3.1.1.1.2" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Privacy</span></td>
<td id="Sx4.T3.1.1.1.3" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Deployability</span></td>
<td id="Sx4.T3.1.1.1.4" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Visual Model</span></td>
<td id="Sx4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Feature</span></td>
<td id="Sx4.T3.1.1.1.6" class="ltx_td ltx_align_center" colspan="4"><span id="Sx4.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">Performance</span></td>
</tr>
<tr id="Sx4.T3.1.2.2" class="ltx_tr">
<td id="Sx4.T3.1.2.2.1" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.2.2.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="Sx4.T3.1.2.2.2" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.2.2.2.1" class="ltx_text ltx_font_bold">Status</span></td>
<td id="Sx4.T3.1.2.2.3" class="ltx_td"></td>
<td id="Sx4.T3.1.2.2.4" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.2.2.4.1" class="ltx_text ltx_font_bold">Num. Param.</span></td>
<td id="Sx4.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T3.1.2.2.5.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="Sx4.T3.1.2.2.6" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.2.2.6.1" class="ltx_text ltx_font_bold">Overall</span></td>
<td id="Sx4.T3.1.2.2.7" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.2.2.7.1" class="ltx_text ltx_font_bold">Other</span></td>
<td id="Sx4.T3.1.2.2.8" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.2.2.8.1" class="ltx_text ltx_font_bold">Yes/No</span></td>
<td id="Sx4.T3.1.2.2.9" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.2.2.9.1" class="ltx_text ltx_font_bold">Count</span></td>
</tr>
<tr id="Sx4.T3.1.3.3" class="ltx_tr">
<td id="Sx4.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">Bottom-Up</td>
<td id="Sx4.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">Not Private</td>
<td id="Sx4.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">No</td>
<td id="Sx4.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">153M</td>
<td id="Sx4.T3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Raw</td>
<td id="Sx4.T3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">65.16</td>
<td id="Sx4.T3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">56.72</td>
<td id="Sx4.T3.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">82.49</td>
<td id="Sx4.T3.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">47.25</td>
</tr>
<tr id="Sx4.T3.1.4.4" class="ltx_tr">
<td id="Sx4.T3.1.4.4.1" class="ltx_td ltx_align_center">Bottom-Up</td>
<td id="Sx4.T3.1.4.4.2" class="ltx_td ltx_align_center">At risk</td>
<td id="Sx4.T3.1.4.4.3" class="ltx_td ltx_align_center">No</td>
<td id="Sx4.T3.1.4.4.4" class="ltx_td ltx_align_center">153M</td>
<td id="Sx4.T3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">Symbolic (GloVe)</td>
<td id="Sx4.T3.1.4.4.6" class="ltx_td ltx_align_center">64.58</td>
<td id="Sx4.T3.1.4.4.7" class="ltx_td ltx_align_center">56.32</td>
<td id="Sx4.T3.1.4.4.8" class="ltx_td ltx_align_center">81.91</td>
<td id="Sx4.T3.1.4.4.9" class="ltx_td ltx_align_center">46.01</td>
</tr>
<tr id="Sx4.T3.1.5.5" class="ltx_tr">
<td id="Sx4.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t">Caption Only</td>
<td id="Sx4.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">Private</td>
<td id="Sx4.T3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="Sx4.T3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="Sx4.T3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="Sx4.T3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">57.55</td>
<td id="Sx4.T3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">47.28</td>
<td id="Sx4.T3.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">76.53</td>
<td id="Sx4.T3.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t">41.83</td>
</tr>
<tr id="Sx4.T3.1.6.6" class="ltx_tr">
<td id="Sx4.T3.1.6.6.1" class="ltx_td ltx_align_center">Ours</td>
<td id="Sx4.T3.1.6.6.2" class="ltx_td ltx_align_center">At risk</td>
<td id="Sx4.T3.1.6.6.3" class="ltx_td ltx_align_center">Yes</td>
<td id="Sx4.T3.1.6.6.4" class="ltx_td ltx_align_center">5.75M</td>
<td id="Sx4.T3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r">Raw</td>
<td id="Sx4.T3.1.6.6.6" class="ltx_td ltx_align_center">60.10</td>
<td id="Sx4.T3.1.6.6.7" class="ltx_td ltx_align_center">49.42</td>
<td id="Sx4.T3.1.6.6.8" class="ltx_td ltx_align_center">79.26</td>
<td id="Sx4.T3.1.6.6.9" class="ltx_td ltx_align_center">45.32</td>
</tr>
<tr id="Sx4.T3.1.7.7" class="ltx_tr">
<td id="Sx4.T3.1.7.7.1" class="ltx_td ltx_align_center">Ours</td>
<td id="Sx4.T3.1.7.7.2" class="ltx_td ltx_align_center">Private</td>
<td id="Sx4.T3.1.7.7.3" class="ltx_td ltx_align_center">Yes</td>
<td id="Sx4.T3.1.7.7.4" class="ltx_td ltx_align_center">5.75M</td>
<td id="Sx4.T3.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r">Symbolic (GloVe)</td>
<td id="Sx4.T3.1.7.7.6" class="ltx_td ltx_align_center">59.76</td>
<td id="Sx4.T3.1.7.7.7" class="ltx_td ltx_align_center">49.02</td>
<td id="Sx4.T3.1.7.7.8" class="ltx_td ltx_align_center">79.14</td>
<td id="Sx4.T3.1.7.7.9" class="ltx_td ltx_align_center">44.62</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance of MCAN on VQA 2.0 using captions along side the output from Bottom-Up or EfficientDet with raw predictions representations or fully symbolic output. We also provide the performance of MCAN using captions alone. </figcaption>
</figure>
<div id="Sx4.SSx3.SSSx2.p3" class="ltx_para">
<p id="Sx4.SSx3.SSSx2.p3.9" class="ltx_p">In Table-<a href="#Sx4.T2" title="Table 2 ‣ Image Model ‣ Model Performance ‣ Experimental Results ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we compare the five settings described before to gauge the penalty for the two aspects in question. First, breaking the differentiability of the model, and second, the use of a small footprint model instead of the original state of the art. We start with the reference model which is the end to end trained model consisting of Bottom-Up and MCAN. This model plays the role of our upper bound on performance. The first comparison using less fine-tuned outputs, see Figure <a href="#Sx4.T2" title="Table 2 ‣ Image Model ‣ Model Performance ‣ Experimental Results ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, yet still differentiable. We notice this drops the performance by <math id="Sx4.SSx3.SSSx2.p3.1.m1.1" class="ltx_Math" alttext="3.7\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.1.m1.1a"><mrow id="Sx4.SSx3.SSSx2.p3.1.m1.1.1" xref="Sx4.SSx3.SSSx2.p3.1.m1.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.1.m1.1.1.2" xref="Sx4.SSx3.SSSx2.p3.1.m1.1.1.2.cmml">3.7</mn><mo id="Sx4.SSx3.SSSx2.p3.1.m1.1.1.1" xref="Sx4.SSx3.SSSx2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.1.m1.1b"><apply id="Sx4.SSx3.SSSx2.p3.1.m1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.1.m1.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.1.m1.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="Sx4.SSx3.SSSx2.p3.1.m1.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.1.m1.1.1.2">3.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.1.m1.1c">3.7\%</annotation></semantics></math> <span id="Sx4.SSx3.SSSx2.p3.9.1" class="ltx_text ltx_font_bold">overall</span> while on the specific tasks <math id="Sx4.SSx3.SSSx2.p3.2.m2.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.2.m2.1a"><mrow id="Sx4.SSx3.SSSx2.p3.2.m2.1.1" xref="Sx4.SSx3.SSSx2.p3.2.m2.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.2.m2.1.1.2" xref="Sx4.SSx3.SSSx2.p3.2.m2.1.1.2.cmml">3</mn><mo id="Sx4.SSx3.SSSx2.p3.2.m2.1.1.1" xref="Sx4.SSx3.SSSx2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.2.m2.1b"><apply id="Sx4.SSx3.SSSx2.p3.2.m2.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.2.m2.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.2.m2.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="Sx4.SSx3.SSSx2.p3.2.m2.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.2.m2.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.2.m2.1c">3\%</annotation></semantics></math> on <span id="Sx4.SSx3.SSSx2.p3.9.2" class="ltx_text ltx_font_bold">Yes/No</span> questions, <math id="Sx4.SSx3.SSSx2.p3.3.m3.1" class="ltx_Math" alttext="5.2\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.3.m3.1a"><mrow id="Sx4.SSx3.SSSx2.p3.3.m3.1.1" xref="Sx4.SSx3.SSSx2.p3.3.m3.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.3.m3.1.1.2" xref="Sx4.SSx3.SSSx2.p3.3.m3.1.1.2.cmml">5.2</mn><mo id="Sx4.SSx3.SSSx2.p3.3.m3.1.1.1" xref="Sx4.SSx3.SSSx2.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.3.m3.1b"><apply id="Sx4.SSx3.SSSx2.p3.3.m3.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.3.m3.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.3.m3.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.3.m3.1.1.1">percent</csymbol><cn type="float" id="Sx4.SSx3.SSSx2.p3.3.m3.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.3.m3.1.1.2">5.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.3.m3.1c">5.2\%</annotation></semantics></math> on <span id="Sx4.SSx3.SSSx2.p3.9.3" class="ltx_text ltx_font_bold">count</span> and <math id="Sx4.SSx3.SSSx2.p3.4.m4.1" class="ltx_Math" alttext="4\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.4.m4.1a"><mrow id="Sx4.SSx3.SSSx2.p3.4.m4.1.1" xref="Sx4.SSx3.SSSx2.p3.4.m4.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.4.m4.1.1.2" xref="Sx4.SSx3.SSSx2.p3.4.m4.1.1.2.cmml">4</mn><mo id="Sx4.SSx3.SSSx2.p3.4.m4.1.1.1" xref="Sx4.SSx3.SSSx2.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.4.m4.1b"><apply id="Sx4.SSx3.SSSx2.p3.4.m4.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.4.m4.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.4.m4.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.4.m4.1.1.1">percent</csymbol><cn type="integer" id="Sx4.SSx3.SSSx2.p3.4.m4.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.4.m4.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.4.m4.1c">4\%</annotation></semantics></math> on <span id="Sx4.SSx3.SSSx2.p3.9.4" class="ltx_text ltx_font_bold">other</span>. Next we move to using symbolic representations. When compared to the raw predictions we see a marginal drop in performance with the biggest drop of <math id="Sx4.SSx3.SSSx2.p3.5.m5.1" class="ltx_Math" alttext="1.71\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.5.m5.1a"><mrow id="Sx4.SSx3.SSSx2.p3.5.m5.1.1" xref="Sx4.SSx3.SSSx2.p3.5.m5.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.5.m5.1.1.2" xref="Sx4.SSx3.SSSx2.p3.5.m5.1.1.2.cmml">1.71</mn><mo id="Sx4.SSx3.SSSx2.p3.5.m5.1.1.1" xref="Sx4.SSx3.SSSx2.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.5.m5.1b"><apply id="Sx4.SSx3.SSSx2.p3.5.m5.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.5.m5.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.5.m5.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="Sx4.SSx3.SSSx2.p3.5.m5.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.5.m5.1.1.2">1.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.5.m5.1c">1.71\%</annotation></semantics></math> on <span id="Sx4.SSx3.SSSx2.p3.9.5" class="ltx_text ltx_font_bold">count</span> and less than <math id="Sx4.SSx3.SSSx2.p3.6.m6.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.6.m6.1a"><mrow id="Sx4.SSx3.SSSx2.p3.6.m6.1.1" xref="Sx4.SSx3.SSSx2.p3.6.m6.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.6.m6.1.1.2" xref="Sx4.SSx3.SSSx2.p3.6.m6.1.1.2.cmml">1</mn><mo id="Sx4.SSx3.SSSx2.p3.6.m6.1.1.1" xref="Sx4.SSx3.SSSx2.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.6.m6.1b"><apply id="Sx4.SSx3.SSSx2.p3.6.m6.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.6.m6.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.6.m6.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.6.m6.1.1.1">percent</csymbol><cn type="integer" id="Sx4.SSx3.SSSx2.p3.6.m6.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.6.m6.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.6.m6.1c">1\%</annotation></semantics></math> for everything else and <span id="Sx4.SSx3.SSSx2.p3.9.6" class="ltx_text ltx_font_bold">overall</span>. Next we replace the Bottom-Up visual model with our modified EfficientDet-D0. For the raw prediction representation we see an <span id="Sx4.SSx3.SSSx2.p3.9.7" class="ltx_text ltx_font_bold">overall</span> drop of <math id="Sx4.SSx3.SSSx2.p3.7.m7.1" class="ltx_Math" alttext="6.64\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.7.m7.1a"><mrow id="Sx4.SSx3.SSSx2.p3.7.m7.1.1" xref="Sx4.SSx3.SSSx2.p3.7.m7.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.7.m7.1.1.2" xref="Sx4.SSx3.SSSx2.p3.7.m7.1.1.2.cmml">6.64</mn><mo id="Sx4.SSx3.SSSx2.p3.7.m7.1.1.1" xref="Sx4.SSx3.SSSx2.p3.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.7.m7.1b"><apply id="Sx4.SSx3.SSSx2.p3.7.m7.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.7.m7.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.7.m7.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.7.m7.1.1.1">percent</csymbol><cn type="float" id="Sx4.SSx3.SSSx2.p3.7.m7.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.7.m7.1.1.2">6.64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.7.m7.1c">6.64\%</annotation></semantics></math> drop from the equivalent Bottom-Up configuration. The <span id="Sx4.SSx3.SSSx2.p3.9.8" class="ltx_text ltx_font_bold">other</span> category sees the highest drop at <math id="Sx4.SSx3.SSSx2.p3.8.m8.1" class="ltx_Math" alttext="10.29\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.8.m8.1a"><mrow id="Sx4.SSx3.SSSx2.p3.8.m8.1.1" xref="Sx4.SSx3.SSSx2.p3.8.m8.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.8.m8.1.1.2" xref="Sx4.SSx3.SSSx2.p3.8.m8.1.1.2.cmml">10.29</mn><mo id="Sx4.SSx3.SSSx2.p3.8.m8.1.1.1" xref="Sx4.SSx3.SSSx2.p3.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.8.m8.1b"><apply id="Sx4.SSx3.SSSx2.p3.8.m8.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.8.m8.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.8.m8.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.8.m8.1.1.1">percent</csymbol><cn type="float" id="Sx4.SSx3.SSSx2.p3.8.m8.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.8.m8.1.1.2">10.29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.8.m8.1c">10.29\%</annotation></semantics></math> and the lowest drop for the <span id="Sx4.SSx3.SSSx2.p3.9.9" class="ltx_text ltx_font_bold">count</span> category with <math id="Sx4.SSx3.SSSx2.p3.9.m9.1" class="ltx_Math" alttext="1.24\%" display="inline"><semantics id="Sx4.SSx3.SSSx2.p3.9.m9.1a"><mrow id="Sx4.SSx3.SSSx2.p3.9.m9.1.1" xref="Sx4.SSx3.SSSx2.p3.9.m9.1.1.cmml"><mn id="Sx4.SSx3.SSSx2.p3.9.m9.1.1.2" xref="Sx4.SSx3.SSSx2.p3.9.m9.1.1.2.cmml">1.24</mn><mo id="Sx4.SSx3.SSSx2.p3.9.m9.1.1.1" xref="Sx4.SSx3.SSSx2.p3.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.SSSx2.p3.9.m9.1b"><apply id="Sx4.SSx3.SSSx2.p3.9.m9.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.9.m9.1.1"><csymbol cd="latexml" id="Sx4.SSx3.SSSx2.p3.9.m9.1.1.1.cmml" xref="Sx4.SSx3.SSSx2.p3.9.m9.1.1.1">percent</csymbol><cn type="float" id="Sx4.SSx3.SSSx2.p3.9.m9.1.1.2.cmml" xref="Sx4.SSx3.SSSx2.p3.9.m9.1.1.2">1.24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.SSSx2.p3.9.m9.1c">1.24\%</annotation></semantics></math>. Our target model, distilled and private, as compared to the raw prediction distilled version also shows minimal loss in performance.</p>
</div>
</section>
</section>
<section id="Sx4.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Using Captions</h3>

<div id="Sx4.SSx4.p1" class="ltx_para">
<p id="Sx4.SSx4.p1.1" class="ltx_p">As mentioned previously, our setup is a middle ground between an end-to-end two stage model and a three stage graph generating model, where we only provide information about object class, attributes and location. Saying this we lose information about the holistic structure of the image. To this end we incorporate the use of captions in our model for two reasons. First, it implies a selection of the most relevant objects in the scene and second, captions imply relationships between these selected objects.</p>
</div>
<div id="Sx4.SSx4.p2" class="ltx_para">
<p id="Sx4.SSx4.p2.1" class="ltx_p">We also look into a hypothetical setting where we would have access to caption descriptions of the given scene. We present the results in Table <a href="#Sx4.T3" title="Table 3 ‣ Overall System Performance ‣ Model Performance ‣ Experimental Results ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We use the ground truth captions provided by the MSCOCO dataset as a proxy for the ideal caption generation system. We go through the same comparisons as before though we will skip the intermediate representation configuration. We notice that the low footprint configurations using our modified EfficientDet has the most to benefit from this strategy giving more than a <math id="Sx4.SSx4.p2.1.m1.1" class="ltx_Math" alttext="4\%" display="inline"><semantics id="Sx4.SSx4.p2.1.m1.1a"><mrow id="Sx4.SSx4.p2.1.m1.1.1" xref="Sx4.SSx4.p2.1.m1.1.1.cmml"><mn id="Sx4.SSx4.p2.1.m1.1.1.2" xref="Sx4.SSx4.p2.1.m1.1.1.2.cmml">4</mn><mo id="Sx4.SSx4.p2.1.m1.1.1.1" xref="Sx4.SSx4.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx4.p2.1.m1.1b"><apply id="Sx4.SSx4.p2.1.m1.1.1.cmml" xref="Sx4.SSx4.p2.1.m1.1.1"><csymbol cd="latexml" id="Sx4.SSx4.p2.1.m1.1.1.1.cmml" xref="Sx4.SSx4.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="Sx4.SSx4.p2.1.m1.1.1.2.cmml" xref="Sx4.SSx4.p2.1.m1.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx4.p2.1.m1.1c">4\%</annotation></semantics></math> gain overall and for the count category even beating the equivalent Bottom-Up configurations without captions.</p>
</div>
</section>
<section id="Sx4.SSx5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Error Analysis</h3>

<div id="Sx4.SSx5.p1" class="ltx_para">
<p id="Sx4.SSx5.p1.1" class="ltx_p">We run a series of error analyses where we compare the performance of the symbolic configurations using Bottom-Up and EfficientDet as the visual model. Specifically, we look at the object predictions on images which have associated Yes/No questions where the Bottom-Up based model answers correctly and the EfficientDet based model gives the wrong answer (see examples in Figure <a href="#Sx1.F3" title="Figure 3 ‣ Introduction ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The main observation is that Bottom-Up tends to detect fewer objects and that for a given object Bottom-Up is much more confident in its prediction. However, for the most part, EfficientDet correctly classifies the object and in other cases the correct class is in the top 5 predictions (see Figures <a href="#A1.F8" title="Figure 8 ‣ Object and Attribute Prediction Comparison to BottomUp ‣ Appendix A Appendix ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, and <a href="#A1.F9" title="Figure 9 ‣ Object and Attribute Prediction Comparison to BottomUp ‣ Appendix A Appendix ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Another observation is that among the questions that our model gets wrong, there are a large number of questions that require knowledge extrinsic to the image. We also provide comparisons when our model performs better than Bottom up in Figures <a href="#A1.F6" title="Figure 6 ‣ Object and Attribute Prediction Comparison to BottomUp ‣ Appendix A Appendix ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a href="#A1.F7" title="Figure 7 ‣ Object and Attribute Prediction Comparison to BottomUp ‣ Appendix A Appendix ‣ Privacy Preserving Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Discussion</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">An important aspect to note is that the current symbolic approach proves to be feasible especially for Yes/No and counting questions. These types of questions are important because they could be used to break down a more complex query into more specific questions that can be processed later into an answer to the original question. It is also of note that even with low footprint models the drop in performance on simpler questions is acceptably small. Also, our tests of symbolic representations on larger, better performing models are available once the computational performance is available.</p>
</div>
<div id="Sx5.p2" class="ltx_para">
<p id="Sx5.p2.1" class="ltx_p">We also explored captions as a means to capture the relationships in a scene graph without explicitly and exhaustively generating them. This would bring further holistic information about the entire scene as opposed to the local information provided by object locations, classes, and attributes. Furthermore, captions also make an implicit selection of relevant objects and their separation of from the background. This was reflected in the increased accuracy when captions were used.</p>
</div>
<section id="Sx5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Conclusions</h3>

<div id="Sx5.SSx1.p1" class="ltx_para">
<p id="Sx5.SSx1.p1.1" class="ltx_p">We introduced a flexible VQA architecture that, by the nature of its deployment on current low power devices, will maintain image privacy. The privacy maintaining strategy relies on a symbolic representation which eliminates the possibility of accurately reproducing the originally captured image. Our architecture had two major components. First, a low footprint visual perception model that produces a list of objects with their likely classes and attributes, as well as their locations in the scene. Second, a large scale powerful QA model that harnesses the full capabilities of modern cloud computing. Since the information provided to it cannot be used to recover sensitive information, it is privacy preserving as well.</p>
</div>
</section>
<section id="Sx5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Future Work</h3>

<div id="Sx5.SSx2.p1" class="ltx_para">
<p id="Sx5.SSx2.p1.1" class="ltx_p">As we saw, there is a considerable benefit from incorporation of captions in our model. A logical next step is to design a caption generation module that could be merged into the low footprint vision model; so that it can produce the sentence-type output that gives a broader description of the scene.</p>
</div>
<div id="Sx5.SSx2.p2" class="ltx_para">
<p id="Sx5.SSx2.p2.1" class="ltx_p">Based on our error analysis, an alternate strategy for training EfficientDet would be to aim for matching the output of the Bottom-Up detector. This can be done without additional annotations, i.e. only train as a regression model and use Bottom-Up’s output as the target.
By priming the model to be as close as possible to Bottom-Up and afterwards to actually do the object recognition task.
Finally, this can be done jointly by combining the regression and detection losses together in the same training schema.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and
Zhang, L. 2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 6077–6086.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Antol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zitnick, C. L.; and
Parikh, D. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, 2425–2433.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bochkovskiy, Wang, and Liao (2020)</span>
<span class="ltx_bibblock">
Bochkovskiy, A.; Wang, C.-Y.; and Liao, H.-Y. M. 2020.

</span>
<span class="ltx_bibblock">Yolov4: Optimal speed and accuracy of object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.10934</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Chen, Y.-C.; Li, L.; Yu, L.; Kholy, A. E.; Ahmed, F.; Gan, Z.; Cheng, Y.; and
Liu, J. 2020.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2017)</span>
<span class="ltx_bibblock">
Cheng, Y.; Wang, D.; Zhou, P.; and Zhang, T. 2017.

</span>
<span class="ltx_bibblock">A survey of model compression and acceleration for deep neural
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.09282</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choudhary et al. (2020)</span>
<span class="ltx_bibblock">
Choudhary, T.; Mishra, V.; Goswami, A.; and Sarangapani, J. 2020.

</span>
<span class="ltx_bibblock">A comprehensive survey on model compression and acceleration.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence Review</em>, 53(7): 5113–5155.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2019)</span>
<span class="ltx_bibblock">
Feng, X.; Jiang, Y.; Yang, X.; Du, M.; and Li, X. 2019.

</span>
<span class="ltx_bibblock">Computer vision algorithms and hardware implementations: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Integration</em>, 69: 309–320.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo, Xu, and Tao (2021)</span>
<span class="ltx_bibblock">
Guo, D.; Xu, C.; and Tao, D. 2021.

</span>
<span class="ltx_bibblock">Bilinear graph networks for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 770–778.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2017)</span>
<span class="ltx_bibblock">
Hu, R.; Andreas, J.; Rohrbach, M.; Darrell, T.; and Saenko, K. 2017.

</span>
<span class="ltx_bibblock">Learning to reason: End-to-end module networks for visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, 804–813.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2019)</span>
<span class="ltx_bibblock">
Hu, R.; Rohrbach, A.; Darrell, T.; and Saenko, K. 2019.

</span>
<span class="ltx_bibblock">Language-conditioned graph networks for relational reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, 10294–10303.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Hudson, D. A.; and Manning, C. D. 2019.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, 6700–6709.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2020)</span>
<span class="ltx_bibblock">
Jiang, H.; Misra, I.; Rohrbach, M.; Learned-Miller, E.; and Chen, X. 2020.

</span>
<span class="ltx_bibblock">In defense of grid features for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 10267–10276.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2018)</span>
<span class="ltx_bibblock">
Jiang, Y.; Natarajan, V.; Chen, X.; Rohrbach, M.; Batra, D.; and Parikh, D.
2018.

</span>
<span class="ltx_bibblock">Pythia v0. 1: the winning entry to the vqa challenge 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.09956</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Kravitz, J.; Chen, S.;
Kalantidis, Y.; Li, L.-J.; Shamma, D. A.; et al. 2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123(1): 32–73.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova et al. (2020)</span>
<span class="ltx_bibblock">
Kuznetsova, A.; Rom, H.; Alldrin, N.; Uijlings, J.; Krasin, I.; Pont-Tuset, J.;
Kamali, S.; Popov, S.; Malloci, M.; Kolesnikov, A.; et al. 2020.

</span>
<span class="ltx_bibblock">The open images dataset v4.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 128(7): 1956–1981.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2018)</span>
<span class="ltx_bibblock">
Lei, J.; Gao, X.; Song, J.; Wang, X.; and Song, M. 2018.

</span>
<span class="ltx_bibblock">Survey of deep neural network model compression.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Journal of Software</em>, 29(2): 251–266.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Li, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu, H.; Dong,
L.; Wei, F.; et al. 2020.

</span>
<span class="ltx_bibblock">Oscar: Object-semantics aligned pre-training for vision-language
tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, 121–137. Springer.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.;
Dollár, P.; and Zitnick, C. L. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, 740–755. Springer.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2019)</span>
<span class="ltx_bibblock">
Lu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.02265</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington, Socher, and Manning (2014)</span>
<span class="ltx_bibblock">
Pennington, J.; Socher, R.; and Manning, C. D. 2014.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)</em>, 1532–1543.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015.

</span>
<span class="ltx_bibblock">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal
Networks.

</span>
<span class="ltx_bibblock">In Cortes, C.; Lawrence, N.; Lee, D.; Sugiyama, M.; and Garnett, R.,
eds., <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 28.
Curran Associates, Inc.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2021)</span>
<span class="ltx_bibblock">
Sun, G.; Liang, L.; Li, T.; Yu, B.; Wu, M.; and Zhang, B. 2021.

</span>
<span class="ltx_bibblock">Video question answering: a survey of models and datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Mobile Networks and Applications</em>, 1–34.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Tan, H.; and Bansal, M. 2019.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07490</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le (2019)</span>
<span class="ltx_bibblock">
Tan, M.; and Le, Q. 2019.

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 6105–6114.
PMLR.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan, Pang, and Le (2020)</span>
<span class="ltx_bibblock">
Tan, M.; Pang, R.; and Le, Q. V. 2020.

</span>
<span class="ltx_bibblock">Efficientdet: Scalable and efficient object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, 10781–10790.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Wang, J.; Hu, X.; Zhang, P.; Li, X.; Wang, L.; Zhang, L.; Gao, J.; and Liu, Z.
2020.

</span>
<span class="ltx_bibblock">Minivlm: A smaller and faster vision-language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.06946</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2017)</span>
<span class="ltx_bibblock">
Wu, Q.; Teney, D.; Wang, P.; Shen, C.; Dick, A.; and van den Hengel, A. 2017.

</span>
<span class="ltx_bibblock">Visual question answering: A survey of methods and datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, 163: 21–40.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2020)</span>
<span class="ltx_bibblock">
Yang, X.; Lin, G.; Lv, F.; and Liu, F. 2020.

</span>
<span class="ltx_bibblock">TRRNet: Tiered Relation Reasoning for Compositional Visual Question
Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16</em>, 414–430.
Springer.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2020)</span>
<span class="ltx_bibblock">
Yu, Z.; Cui, Y.; Yu, J.; Wang, M.; Tao, D.; and Tian, Q. 2020.

</span>
<span class="ltx_bibblock">Deep multimodal neural architecture search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on
Multimedia</em>, 3743–3752.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019)</span>
<span class="ltx_bibblock">
Yu, Z.; Yu, J.; Cui, Y.; Tao, D.; and Tian, Q. 2019.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 6281–6290.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J.
2021.

</span>
<span class="ltx_bibblock">Vinvl: Revisiting visual representations in vision-language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 5579–5588.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Object and Attribute Prediction Comparison to BottomUp</h3>

<div id="A1.SSx1.p1" class="ltx_para">
<p id="A1.SSx1.p1.1" class="ltx_p">In the following we show the class and attribute output for both EfficientDet and BottomUp in scenarios where EfficientDet provided output that produces correct results from MCAN but BottomUp did not, and vice-versa. The main observation here is that EfficientDet, although providing good top candidates for both class and attribute prediction, has low confidence when compared to BottomUp. Another observation would be that when EfficientDet provides outputs that lead to wrong answers the questions are likely to require information that is not present in the scene. It is important to remember that these comparable results were generated by a model with a much lower footprint.</p>
</div>
<div id="A1.SSx1.p2" class="ltx_para">
<p id="A1.SSx1.p2.1" class="ltx_p">We provide a list of the top objects considered in the scene along with their Top-5 class and attribute predictions for each object. The question and answers are also provided.</p>
</div>
<figure id="A1.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x7.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="430" height="332" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x8.png" id="A1.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="430" height="332" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison on the output of EfficientDet on the bottom, left when rotated, and Bottom-Up on top, right when rotated. Our answers are correct and BottomUp’s answers are incorrect. Answers: Ours: yes, BottomUp: no</figcaption>
</figure>
<figure id="A1.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x9.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="430" height="332" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x10.png" id="A1.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="430" height="332" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparison on the output of EfficientDet on the bottom, left when rotated, and Bottom-Up on top, right when rotated. Our answers are correct and BottomUp’s answers are incorrect. Answers: Ours: no, BottomUp: yes</figcaption>
</figure>
<figure id="A1.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x11.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="430" height="332" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x12.png" id="A1.F8.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="430" height="332" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Comparison on the output of EfficientDet on the bottom, left when rotated, and Bottom-Up on top, right when rotated. Our answers are incorrect and BottomUp’s answers are correct. Answers: Ours:yes, BottomUp:no</figcaption>
</figure>
<figure id="A1.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x13.png" id="A1.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="430" height="332" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2202.07712/assets/x14.png" id="A1.F9.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="430" height="332" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Comparison on the output of EfficientDet on the bottom, left when rotated, and Bottom-Up on top, right when rotated. Our answers are incorrect and BottomUp’s answers are correct. Answers: Ours:yes, BottomUp:no</figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2202.07711" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2202.07712" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2202.07712">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2202.07712" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2202.07713" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 18:22:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
