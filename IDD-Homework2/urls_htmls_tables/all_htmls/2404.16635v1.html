<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.16635] TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning</title><meta property="og:description" content="Charts are important for presenting and explaining complex data relationships.
Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks. However, the sh…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.16635">

<!--Generated on Sun May  5 22:06:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">TinyChart: Efficient Chart Understanding with 
<br class="ltx_break">Visual Token Merging and Program-of-Thoughts Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liang Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Renmin University of China</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhangliang00@ruc.edu.cn">zhangliang00@ruc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anwen Hu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">Alibaba Group</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:huanwen.haw@alibaba-inc.com">huanwen.haw@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haiyang Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Alibaba Group</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Hangzhou</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shuofeng.xhy@alibaba-inc.com">shuofeng.xhy@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ming Yan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">Alibaba Group</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_city">Hangzhou</span><span id="id12.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ym119608@alibaba-inc.com">ym119608@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yichen Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Renmin University of China</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xu%CB%99yichen@ruc.edu.cn">xu˙yichen@ruc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qin Jin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id16.1.id1" class="ltx_text ltx_affiliation_institution">Renmin University of China</span><span id="id17.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id18.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:qjin@ruc.edu.cn">qjin@ruc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ji Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id19.1.id1" class="ltx_text ltx_affiliation_institution">Alibaba Group</span><span id="id20.2.id2" class="ltx_text ltx_affiliation_city">Hangzhou</span><span id="id21.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zj122146@alibaba-inc.com">zj122146@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fei Huang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id22.1.id1" class="ltx_text ltx_affiliation_institution">Alibaba Group</span><span id="id23.2.id2" class="ltx_text ltx_affiliation_city">Hangzhou</span><span id="id24.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:f.huang@alibaba-inc.com">f.huang@alibaba-inc.com</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id25.id1" class="ltx_p">Charts are important for presenting and explaining complex data relationships.
Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks. However, the sheer size of these models in terms of parameters and computational requirements limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding:
(1) reduce the burden of learning numerical computations through a Program-of-Thoughts (PoT) learning strategy, which trains the model to generate Python programs for numerical calculations, and
(2) reduce lengthy vision feature sequences produced by the vision transformer for high-resolution images through a Vision Token Merging module, which gradually merges most similar vision tokens.
Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on a variety of chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart understanding MLLM with up to 13B parameters such as ChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on ChartQA. It also demonstrates its superior efficiency with higher throughput during inference due to a smaller model scale and more efficient vision encoding. Our code and model are available at <a target="_blank" href="https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart" title="" class="ltx_ref ltx_href">https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.16635/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_square" width="415" height="406" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Our TinyChart-3B outperforms several 13B MLLMs on a variety of chart understanding benchmarks (a), while achieving larger inference throughput (b).</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As an important information source, charts can intuitively visualize data in various visual presentation forms and have become an indispensable part of information dissemination, business decision-making, and academic research <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2024a</a>)</cite>. With the rapid growth of multimodal data, automatically comprehending charts has become a pressing need and received increasing attention from the research community <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>; Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2024</a>)</cite>. Recently, Multimodal Large Language Models (MLLMs) have shown strong capability in comprehending images and following instructions <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib40" title="" class="ltx_ref">2023b</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2024a</a>; Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2024</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023b</a>; Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>; Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2023c</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2023b</a>; Dong et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2024a</a>, <a href="#bib.bib7" title="" class="ltx_ref">b</a>)</cite>. Based on these MLLMs, some recent works <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>; Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>; Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2024a</a>)</cite> further build chart understanding models by collecting and constructing versatile chart comprehension datasets and performing supervised fine-tuning.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, despite their remarkable success, current chart understanding models still face three main limitations: (1) Considerable amount of parameters makes training and deployment challenging. For example, ChartLlama <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> is a model with 13 billion parameters, which is hard to deploy on a single consumer GPU with less than 26GB of VRAMs.
(2) They are prone to errors when tackling questions involving numerical calculations <cite class="ltx_cite ltx_citemacro_citep">(Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>, which are difficult to directly answer without any reasoning steps.
(3) They struggle with efficiently encoding for high-resolution images since the standard vision transformer would produce lengthy feature sequences.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To overcome such limitations in chart understanding, we propose an efficient and powerful MLLM, namely <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">TinyChart</span>. As shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, through the efficient visual encoding and Program-of-Thoughts learning strategy, TinyChart achieves state-of-the-art performances on various chart understanding benchmarks with only 3B parameters, while excelling in faster inference throughput.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">For efficient visual encoding, we propose to merge visual tokens based on the observation that chart images often contain large areas of color and white spaces.
Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Bolya et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>, we adopt a parameter-free Visual Token Merging module inside each vision transformer layer, which aggregates the most similar visual tokens and gradually reduces the length of the visual feature sequence, thus making it possible to efficiently encode high-resolution chart images.
This enables the model to maintain high-resolution chart image input while controlling the computation load.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Moreover, inspired by  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, we propose Program-of-Thoughts learning that enhances the model’s ability to resolve mathematical problems. According to statistics on ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>, 42% of questions for charts require numerical answers, and most existing models struggle to perform numerical question answering <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023c</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>. To learn chart understanding more efficiently, we train the model to generate Python programs for the computation problems step by step. The programs are then passed to a Python interpreter to produce the final answer. To support Program-of-Thoughts learning, we further construct the ChartQA-PoT dataset based on ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>. The QA pairs in our ChartQA-PoT are constructed in two ways: (1) Template-based PoT construction, which generates questions and programs by filling in the manually written templates based on chart data. (2) GPT-based PoT construction, which leverages <span id="S1.p5.1.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib39" title="" class="ltx_ref">2023a</a>)</cite> to generate programs based on human-written questions. Experimental results show that Program-of-Thoughts learning can significantly improve the question-answering, especially numerical question answering ability of TinyChart.
<br class="ltx_break">The main contributions of this work are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce TinyChart, an efficient multimodal chart understanding model, which outperforms several 13B MLLMs and achieves state-of-the-art performances on a variety of chart understanding benchmarks, while excelling in faster inference speed at the same time.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a Program-of-Thoughts (PoT) learning strategy to enhance the model in learning numerical calculation and carefully build a PoT dataset ChartQA-PoT.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We adopt Visual Token Merging for efficient vision encoding, which significantly reduces the length of vision feature sequences and enables the model to encode high-resolution chart images with constrained computing resources.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Chart Understanding</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Chart understanding requires the model to comprehend chart contents and accomplish related tasks specified by the instructions. This field encompasses low-level recognition tasks, such as data extraction <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023a</a>)</cite>, and high-level tasks, such as question-answering (QA) <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>; Methani et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>; Kafle et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>, summarization <cite class="ltx_cite ltx_citemacro_citep">(Kantharaj et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022b</a>; Obeid and Hoque, <a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite>, and re-rendering <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>. As charts often contain OCR text pivotal for data interpretation, and many instructions require the model to perform numerical calculations, chart understanding demands robust text recognition capabilities and computational reasoning from the model. Early approaches <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2019</a>; Methani et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023a</a>; Fu et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2023a</a>; Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> rely on pipeline methods that use off-the-shelf OCR tools or component detectors to transform charts into data tables and other textual representations. They then employ language models to complete the specified tasks. These pipeline approaches, limited by their inability to optimize jointly, were hampered by error accumulation. Recent studies <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2023</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023c</a>; Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>; Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023d</a>)</cite> have shifted towards end-to-end methods based on multimodal large language models. These studies adopt the structure of multimodal large language models <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2024a</a>, <a href="#bib.bib30" title="" class="ltx_ref">2023b</a>; Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2024</a>, <a href="#bib.bib56" title="" class="ltx_ref">2023c</a>; Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> and enhance chart understanding abilities through supervised fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite> with substantial chart instruction data <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>; Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>)</cite>.
Although these models demonstrate improvement in performance, their extensive parameter size prevents them from being easily trained or deployed under resource-constrained scenarios. In this paper, we demonstrate that a 3B MLLM is enough to achieve state-of-the-art performance on several chart understanding tasks.
Meanwhile, it has been well observed that these models are prone to numerical errors <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023c</a>; Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>.
Though <cite class="ltx_cite ltx_citemacro_citet">Meng et al<span class="ltx_text">.</span> (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite> try to construct executable command lines in JSON format based on a template to eliminate numerical errors, we argue that it is insufficient to fully address this issue for two reasons: 1) The executable command lines in JSON format produced by <cite class="ltx_cite ltx_citemacro_citet">Meng et al<span class="ltx_text">.</span> (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite> relies on a specific computational backend, which limits their potential versatility. 2) Template-based programs can only cover rather limited scenarios. Instead, we construct the Program-of-Thoughts learning dataset with the combination of both templates and GPT-generated programs.
This allows the model to more effectively learn how to solve numerical problems.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Multimodal Large Language Model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Multimodal large language models (MLLM) exhibit strong capabilities in visual understanding and instruction following <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib40" title="" class="ltx_ref">2023b</a>; Team et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>. They typically comprise transformer-based visual encoders, large language models, and vision-language connectors <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2024a</a>, <a href="#bib.bib30" title="" class="ltx_ref">2023b</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2024</a>; Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2024</a>, <a href="#bib.bib56" title="" class="ltx_ref">2023c</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2023b</a>; Dong et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2024a</a>; Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2023b</a>)</cite>. These models are generally trained on extensive general image-text data for cross-modal alignment and instruction fine-tuning. Although some studies have showcased a degree of OCR capability in these multimodal large language models <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2024b</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite>, their performance on document and chart understanding benchmarks remains suboptimal due to their low input resolution <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2023a</a>; Dong et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2024b</a>)</cite>. Efforts in the general document domain have attempted to improve the fine-grained understanding capabilities of MLLMs by increasing resolution <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>, segmenting images <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2023a</a>; Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>; Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2024b</a>; Dong et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2024b</a>)</cite>, utilizing frequency domain signals <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>, and introducing additional high-resolution encoders <cite class="ltx_cite ltx_citemacro_citep">(Hong et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>. However, these models often suffer from low efficiency, primarily due to the excessive length of the high-resolution visual sequences. The visual token merging method adopted in this paper can significantly reduce the length of visual feature sequences and relax the computational requirements with high-resolution input.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2404.16635/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="415" height="248" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Overview of TinyChart.</span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2404.16635/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_img_landscape" width="415" height="125" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">(a) Vision transformer layer with Visual Token Merging. (b) Process of the Visual Token Merging.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>TinyChart</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Model Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2. Multimodal Large Language Model ‣ 2. Related Work ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overview framework of our proposed TinyChart. It follows the typical architecture of the multimodal large language model (MLLM), which consists of a vision transformer encoder, a vision-language connector, and a large language model. To encode high-resolution visual input effectively, we insert the visual token merging module inside each vision transformer layer.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Vision Transformer Encoder</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.7" class="ltx_p">The vision transformer encoder aims to encode chart images into vision features. A standard vision transformer <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> first resizes the input image <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">I</annotation></semantics></math> into a fixed resolution and crops the image into patches. Then the patches are treated as vision tokens and processed with transformer encoder layers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite>. Suppose the input image <math id="S3.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="I^{N\times N}" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><msup id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">I</mi><mrow id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.2.m2.1.1.3.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml">×</mo><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.3.cmml">N</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">𝐼</ci><apply id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3"><times id="S3.SS1.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.1"></times><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.2">𝑁</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">I^{N\times N}</annotation></semantics></math> is in resolution <math id="S3.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="N\times N" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mrow id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.3.m3.1.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS1.SSS1.p1.3.m3.1.1.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><apply id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1"><times id="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.1"></times><ci id="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.2">𝑁</ci><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">N\times N</annotation></semantics></math>, and the patch size is <math id="S3.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="P\times P" display="inline"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><mrow id="S3.SS1.SSS1.p1.4.m4.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS1.p1.4.m4.1.1.2" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml">P</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.4.m4.1.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.1.cmml">×</mo><mi id="S3.SS1.SSS1.p1.4.m4.1.1.3" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m4.1b"><apply id="S3.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1"><times id="S3.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.1"></times><ci id="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.2">𝑃</ci><ci id="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m4.1c">P\times P</annotation></semantics></math>, the length of vision tokens would be <math id="S3.SS1.SSS1.p1.5.m5.1" class="ltx_math_unparsed" alttext="(N//P)^{2}" display="inline"><semantics id="S3.SS1.SSS1.p1.5.m5.1a"><msup id="S3.SS1.SSS1.p1.5.m5.1.1"><mrow id="S3.SS1.SSS1.p1.5.m5.1.1.2"><mo stretchy="false" id="S3.SS1.SSS1.p1.5.m5.1.1.2.1">(</mo><mi id="S3.SS1.SSS1.p1.5.m5.1.1.2.2">N</mi><mo rspace="0em" id="S3.SS1.SSS1.p1.5.m5.1.1.2.3">/</mo><mo lspace="0em" id="S3.SS1.SSS1.p1.5.m5.1.1.2.4">/</mo><mi id="S3.SS1.SSS1.p1.5.m5.1.1.2.5">P</mi><mo stretchy="false" id="S3.SS1.SSS1.p1.5.m5.1.1.2.6">)</mo></mrow><mn id="S3.SS1.SSS1.p1.5.m5.1.1.3">2</mn></msup><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.5.m5.1b">(N//P)^{2}</annotation></semantics></math>. Since the standard transformer layer does not reduce the sequence length, the vision transformer finally produces a vision feature in length <math id="S3.SS1.SSS1.p1.6.m6.1" class="ltx_math_unparsed" alttext="(N//P)^{2}" display="inline"><semantics id="S3.SS1.SSS1.p1.6.m6.1a"><msup id="S3.SS1.SSS1.p1.6.m6.1.1"><mrow id="S3.SS1.SSS1.p1.6.m6.1.1.2"><mo stretchy="false" id="S3.SS1.SSS1.p1.6.m6.1.1.2.1">(</mo><mi id="S3.SS1.SSS1.p1.6.m6.1.1.2.2">N</mi><mo rspace="0em" id="S3.SS1.SSS1.p1.6.m6.1.1.2.3">/</mo><mo lspace="0em" id="S3.SS1.SSS1.p1.6.m6.1.1.2.4">/</mo><mi id="S3.SS1.SSS1.p1.6.m6.1.1.2.5">P</mi><mo stretchy="false" id="S3.SS1.SSS1.p1.6.m6.1.1.2.6">)</mo></mrow><mn id="S3.SS1.SSS1.p1.6.m6.1.1.3">2</mn></msup><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.6.m6.1b">(N//P)^{2}</annotation></semantics></math>. In practice, when <math id="S3.SS1.SSS1.p1.7.m7.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.SSS1.p1.7.m7.1a"><mi id="S3.SS1.SSS1.p1.7.m7.1.1" xref="S3.SS1.SSS1.p1.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.7.m7.1b"><ci id="S3.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.7.m7.1c">N</annotation></semantics></math> is large, the vision feature can be very long and inefficient for the language model to handle.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2404.16635/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="415" height="231" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">The demonstration of constructing Template-based PoT (upper half) and GPT-based PoT (lower half) in the ChartQA-PoT dataset.</span></figcaption>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<span id="S3.SS1.SSS1.p2.5" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S3.SS1.SSS1.p2.4" class="ltx_p"><span id="S3.SS1.SSS1.p2.4.1" class="ltx_text ltx_font_bold">Visual Token Merging</span>
Since key information (such as OCR words) in a chart can be unrecognizable in low-resolution images <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2024b</a>)</cite>, high-resolution input is essential for chart understanding. However, charts typically contain a large number of color blocks and blank spaces, where patches are visually similar.
To achieve efficient and effective chart understanding, we apply Visual Token Merging <cite class="ltx_cite ltx_citemacro_citep">(Bolya et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> in each transformer layer. The process of Visual Token Merging is shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2. Multimodal Large Language Model ‣ 2. Related Work ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. By merging the <math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mi id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><ci id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">r</annotation></semantics></math> most similar token pairs, it reduces the length of the vision feature by <math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">r</annotation></semantics></math> in each layer. We measure the similarity between two tokens using the cosine distance between Keys from self-attention following <cite class="ltx_cite ltx_citemacro_citep">(Bolya et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>. As shown in the lower part of Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2. Multimodal Large Language Model ‣ 2. Related Work ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, Vision Token Merger finds the top-<math id="S3.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><mi id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.1b"><ci id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.1c">r</annotation></semantics></math> similar token pairs through bipartite graph matching. It first divides the vision tokens into two disjoint sets. Then, for each token in one set, it finds the most similar tokens in the other set and draws an edge between the two tokens. After that, it only keeps the top-<math id="S3.SS1.SSS1.p2.4.m4.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><mi id="S3.SS1.SSS1.p2.4.m4.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.4.m4.1b"><ci id="S3.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.4.m4.1c">r</annotation></semantics></math> most similar edges and merges the features of the two endpoints through average pooling. Note that not only spatially adjacent visual tokens are subject to merging.
Non-adjacent tokens can also be merged if they belong to different subsets and are similar enough.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<span id="S3.SS1.SSS1.p3.2" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S3.SS1.SSS1.p3.1" class="ltx_p"><span id="S3.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Proportional attention</span> The visual token merging operation aggregates tokens with a similar feature into one. Therefore, it will reduce the proportion of this visual feature in the attention calculation in the following transformer layer, since the number of this feature has decreased. To solve this issue, we let the attention operation consider the actual number of patches <math id="S3.SS1.SSS1.p3.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS1.SSS1.p3.1.m1.1a"><mi id="S3.SS1.SSS1.p3.1.m1.1.1" xref="S3.SS1.SSS1.p3.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.1.m1.1b"><ci id="S3.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.1.m1.1c">s</annotation></semantics></math> represented by each token as follows:</p>
<table id="A1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle\mathrm{Attention}=\mathrm{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}+\log s\right)V" display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">Attention</mi><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">softmax</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mfrac id="S3.E1.m1.1.1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.2.2.1" xref="S3.E1.m1.1.1.1.1.1.1.2.2.1.cmml">​</mo><msup id="S3.E1.m1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.2.cmml">K</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.3.cmml">⊤</mo></msup></mrow><msqrt id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2.cmml">d</mi></msqrt></mfrac></mstyle><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.3a" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">s</mi></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2a" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.4" xref="S3.E1.m1.1.1.1.4.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><ci id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">Attention</ci><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">softmax</ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><divide id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"></divide><apply id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2"><times id="S3.E1.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2">𝑄</ci><apply id="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.2">𝐾</ci><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.3">top</csymbol></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3"><root id="S3.E1.m1.1.1.1.1.1.1.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3"></root><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3.2">𝑑</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><log id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.1"></log><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝑠</ci></apply></apply><ci id="S3.E1.m1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle\mathrm{Attention}=\mathrm{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}+\log s\right)V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.7" class="ltx_p">Where <math id="S3.SS1.SSS1.p4.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><mi id="S3.SS1.SSS1.p4.1.m1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.1.m1.1b"><ci id="S3.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.1.m1.1c">Q</annotation></semantics></math>, <math id="S3.SS1.SSS1.p4.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><mi id="S3.SS1.SSS1.p4.2.m2.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.2.m2.1b"><ci id="S3.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.2.m2.1c">K</annotation></semantics></math>, <math id="S3.SS1.SSS1.p4.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS1.SSS1.p4.3.m3.1a"><mi id="S3.SS1.SSS1.p4.3.m3.1.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.3.m3.1b"><ci id="S3.SS1.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.3.m3.1c">V</annotation></semantics></math> denotes the query, key, and value of self-attention which are linear projected from the hidden states <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite>. By adding <math id="S3.SS1.SSS1.p4.4.m4.1" class="ltx_Math" alttext="\log s" display="inline"><semantics id="S3.SS1.SSS1.p4.4.m4.1a"><mrow id="S3.SS1.SSS1.p4.4.m4.1.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.SSS1.p4.4.m4.1.1.1" xref="S3.SS1.SSS1.p4.4.m4.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S3.SS1.SSS1.p4.4.m4.1.1a" xref="S3.SS1.SSS1.p4.4.m4.1.1.cmml">⁡</mo><mi id="S3.SS1.SSS1.p4.4.m4.1.1.2" xref="S3.SS1.SSS1.p4.4.m4.1.1.2.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.4.m4.1b"><apply id="S3.SS1.SSS1.p4.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1"><log id="S3.SS1.SSS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.1"></log><ci id="S3.SS1.SSS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p4.4.m4.1.1.2">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.4.m4.1c">\log s</annotation></semantics></math> inside <math id="S3.SS1.SSS1.p4.5.m5.1" class="ltx_Math" alttext="\mathrm{softmax}" display="inline"><semantics id="S3.SS1.SSS1.p4.5.m5.1a"><mi id="S3.SS1.SSS1.p4.5.m5.1.1" xref="S3.SS1.SSS1.p4.5.m5.1.1.cmml">softmax</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.5.m5.1b"><ci id="S3.SS1.SSS1.p4.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p4.5.m5.1.1">softmax</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.5.m5.1c">\mathrm{softmax}</annotation></semantics></math>, the token that merged from <math id="S3.SS1.SSS1.p4.6.m6.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS1.SSS1.p4.6.m6.1a"><mi id="S3.SS1.SSS1.p4.6.m6.1.1" xref="S3.SS1.SSS1.p4.6.m6.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.6.m6.1b"><ci id="S3.SS1.SSS1.p4.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p4.6.m6.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.6.m6.1c">s</annotation></semantics></math> patches are duplicated by <math id="S3.SS1.SSS1.p4.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS1.SSS1.p4.7.m7.1a"><mi id="S3.SS1.SSS1.p4.7.m7.1.1" xref="S3.SS1.SSS1.p4.7.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.7.m7.1b"><ci id="S3.SS1.SSS1.p4.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p4.7.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.7.m7.1c">s</annotation></semantics></math> times in the attention calculation <cite class="ltx_cite ltx_citemacro_citep">(Bolya et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Vision-Language Connector</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The vision language connector aims to project the vision features into the embedding space of the large language model. Following <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023b</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2024</a>)</cite>, we implement the vision-language connector as a multiple-layer perceptron with one hidden layer and GeLU <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks and Gimpel, <a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> activation.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Large Language Model</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.3" class="ltx_p">The large language model aims to comprehend both visual features and language instructions, and then generate responses to accomplish chart understanding tasks. It is implemented as a transformer decoder <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite> with a causal attention mask. The training objective of the model is language modeling. Assuming the visual features is <math id="S3.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mi id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><ci id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">V</annotation></semantics></math>, the language instruction is <math id="S3.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mi id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><ci id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">L</annotation></semantics></math>, and the response is <math id="S3.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS3.p1.3.m3.1a"><mi id="S3.SS1.SSS3.p1.3.m3.1.1" xref="S3.SS1.SSS3.p1.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.3.m3.1b"><ci id="S3.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.3.m3.1c">R</annotation></semantics></math>, then the loss function is defined as follows:</p>
<table id="A1.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}=\frac{1}{T}\sum_{i=1}^{T}\mathrm{LLM}(R_{i}|V,L,R_{&lt;i})" display="inline"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml">ℒ</mi><mo id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.3.3.1.3" xref="S3.E2.m1.3.3.1.3.cmml"><mfrac id="S3.E2.m1.3.3.1.3a" xref="S3.E2.m1.3.3.1.3.cmml"><mn id="S3.E2.m1.3.3.1.3.2" xref="S3.E2.m1.3.3.1.3.2.cmml">1</mn><mi id="S3.E2.m1.3.3.1.3.3" xref="S3.E2.m1.3.3.1.3.3.cmml">T</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><munderover id="S3.E2.m1.3.3.1.1.2a" xref="S3.E2.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.2.2.3.2" xref="S3.E2.m1.3.3.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.2.2.3.1" xref="S3.E2.m1.3.3.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.3.3.1.1.2.2.3.3" xref="S3.E2.m1.3.3.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml">T</mi></munderover></mstyle><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml">LLM</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.2.cmml">R</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">V</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">L</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">R</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"></eq><ci id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3">ℒ</ci><apply id="S3.E2.m1.3.3.1.cmml" xref="S3.E2.m1.3.3.1"><times id="S3.E2.m1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1.2"></times><apply id="S3.E2.m1.3.3.1.3.cmml" xref="S3.E2.m1.3.3.1.3"><divide id="S3.E2.m1.3.3.1.3.1.cmml" xref="S3.E2.m1.3.3.1.3"></divide><cn type="integer" id="S3.E2.m1.3.3.1.3.2.cmml" xref="S3.E2.m1.3.3.1.3.2">1</cn><ci id="S3.E2.m1.3.3.1.3.3.cmml" xref="S3.E2.m1.3.3.1.3.3">𝑇</ci></apply><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3"><eq id="S3.E2.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3">𝑇</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></times><ci id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3">LLM</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.2">𝑅</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑉</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝐿</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑅</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3"><lt id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\displaystyle\mathcal{L}=\frac{1}{T}\sum_{i=1}^{T}\mathrm{LLM}(R_{i}|V,L,R_{&lt;i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS3.p1.5" class="ltx_p">Where <math id="S3.SS1.SSS3.p1.4.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.SSS3.p1.4.m1.1a"><mi id="S3.SS1.SSS3.p1.4.m1.1.1" xref="S3.SS1.SSS3.p1.4.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.4.m1.1b"><ci id="S3.SS1.SSS3.p1.4.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.4.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.4.m1.1c">T</annotation></semantics></math> is the number of tokens in <math id="S3.SS1.SSS3.p1.5.m2.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS3.p1.5.m2.1a"><mi id="S3.SS1.SSS3.p1.5.m2.1.1" xref="S3.SS1.SSS3.p1.5.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.5.m2.1b"><ci id="S3.SS1.SSS3.p1.5.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.5.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.5.m2.1c">R</annotation></semantics></math>. Note that we only calculate loss over tokens in the responses following the supervised fine-tuning setting in <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Program-of-Thoughts Learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Program-of-Thoughts (PoT) learning aims to enhance the learning efficiency of models for numerical computation.
In PoT learning, the model is trained to generate executable Python codes as the target of a question. The final answer is obtained by executing the code with a Python interpreter.
Compared to short answers that only contain the calculated values, the Python code includes natural language comments and multi-step reasoning processes, offering a form of learning closely aligned with the pre-training of the large language model.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<span id="S3.SS2.p2.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S3.SS2.p2.2" class="ltx_p"><span id="S3.SS2.p2.2.1" class="ltx_text ltx_font_bold">ChartQA-PoT Dataset</span> To support PoT learning on chart understanding, we construct the ChartQA-PoT dataset based on the training split of ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>. ChartQA-PoT contains 140,584 (question, PoT answer) pairs. Each PoT answer consists of multiple lines of Python code. We provide natural language comments for almost all code lines to explain their behaviors.
We employ two approaches for constructing (question, PoT answer) pairs: Template-based PoT, and GPT-based PoT.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Template-based PoT</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Based on the chart images in ChartQA, we construct template-based (question, PoT answer) pairs. As illustrated in the upper half of Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1.1. Vision Transformer Encoder ‣ 3.1. Model Architecture ‣ 3. TinyChart ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the Template-based PoT is constructed based on human-written templates containing placeholders for both questions and code. The template questions involve common numerical operations such as calculating the sum, average, minimal, and maximum values. We adopt the 40 template questions proposed by PlotQA <cite class="ltx_cite ltx_citemacro_citep">(Methani et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> and manually write their corresponding template Python code to solve them. As shown in the top-left part of Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1.1. Vision Transformer Encoder ‣ 3.1. Model Architecture ‣ 3. TinyChart ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the template code consists of several variable assignment operations with NumPy <cite class="ltx_cite ltx_citemacro_citep">(van der Walt et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2011</a>)</cite> functions to perform calculations. The beginning steps usually involve extracting the relevant data from the chart and assigning them to variables. The final computed result is stored in a variable named ”Answer”. For each placeholder in the template, we identify all possible values from the data table of the chart and randomly select one to fill in the placeholder. After removing incorrect or unreasonable filled-ins using rule-based methods, we finally successfully construct 119,281 (question, PoT pairs) over 17,498 images from ChartQA.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>GPT-based PoT</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Although the template-based method allows for the construction of a large number of question-answer pairs, the diversity of these pairs is limited due to the fixed templates. To improve the generalization ability of PoT learning, we have additionally built GPT-generated PoT data by leveraging the powerful command-following and code-generation capabilities of large language models. Specifically, we prompt <span id="S3.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib39" title="" class="ltx_ref">2023a</a>)</cite> to generate PoT answers similar to the template PoT format for questions annotated in ChartQA using in-context examples. As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1.1. Vision Transformer Encoder ‣ 3.1. Model Architecture ‣ 3. TinyChart ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, since <span id="S3.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span> does not accept image input, we also provide the data table corresponding to the chart as text input to <span id="S3.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span>. We screen the quality of the generated PoT answers by running them through a Python interpreter. If the annotated PoT answer can not run on the Python interpreter, or if the answer obtained is different from the annotated one in ChartQA, then the corresponding PoT Answer is deleted. In the end, we construct 21,303 (question, PoT Answer) pairs on 15,521 chart images.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Datasets used for training TinyChart. The benchmark datasets consist of basic chart understanding evaluations including QA, summary, and chart-to-table generation. Note that in ablation studies, we only use the benchmark datasets for training due to limited computational resources. </span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<td id="S3.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<td id="S3.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Benchmark</td>
<td id="S3.T1.4.1.1.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_tt">Samples</td>
</tr>
<tr id="S3.T1.4.2.2" class="ltx_tr">
<td id="S3.T1.4.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.4.2.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Chart question answer</span></td>
<td id="S3.T1.4.2.2.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.4.2.2.3" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S3.T1.4.3.3" class="ltx_tr">
<td id="S3.T1.4.3.3.1" class="ltx_td ltx_align_left">ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T1.4.3.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T1.4.3.3.3" class="ltx_td ltx_nopad_r ltx_align_right">28,299</td>
</tr>
<tr id="S3.T1.4.4.4" class="ltx_tr">
<td id="S3.T1.4.4.4.1" class="ltx_td ltx_align_left">ChartQA-PoT</td>
<td id="S3.T1.4.4.4.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T1.4.4.4.3" class="ltx_td ltx_nopad_r ltx_align_right">140,584</td>
</tr>
<tr id="S3.T1.4.5.5" class="ltx_tr">
<td id="S3.T1.4.5.5.1" class="ltx_td ltx_align_left">PlotQA <cite class="ltx_cite ltx_citemacro_citep">(Methani et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S3.T1.4.5.5.2" class="ltx_td"></td>
<td id="S3.T1.4.5.5.3" class="ltx_td ltx_nopad_r ltx_align_right">157,070</td>
</tr>
<tr id="S3.T1.4.6.6" class="ltx_tr">
<td id="S3.T1.4.6.6.1" class="ltx_td ltx_align_left">DVQA <cite class="ltx_cite ltx_citemacro_citep">(Kafle et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S3.T1.4.6.6.2" class="ltx_td"></td>
<td id="S3.T1.4.6.6.3" class="ltx_td ltx_nopad_r ltx_align_right">200,000</td>
</tr>
<tr id="S3.T1.4.7.7" class="ltx_tr">
<td id="S3.T1.4.7.7.1" class="ltx_td ltx_align_left">OpenCQA <cite class="ltx_cite ltx_citemacro_citep">(Kantharaj et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S3.T1.4.7.7.2" class="ltx_td"></td>
<td id="S3.T1.4.7.7.3" class="ltx_td ltx_nopad_r ltx_align_right">5,407</td>
</tr>
<tr id="S3.T1.4.8.8" class="ltx_tr">
<td id="S3.T1.4.8.8.1" class="ltx_td ltx_align_left ltx_border_t"><em id="S3.T1.4.8.8.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Chart-to-text generation</em></td>
<td id="S3.T1.4.8.8.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.4.8.8.3" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S3.T1.4.9.9" class="ltx_tr">
<td id="S3.T1.4.9.9.1" class="ltx_td ltx_align_left">Pew <cite class="ltx_cite ltx_citemacro_citep">(Kantharaj et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S3.T1.4.9.9.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T1.4.9.9.3" class="ltx_td ltx_nopad_r ltx_align_right">7,892</td>
</tr>
<tr id="S3.T1.4.10.10" class="ltx_tr">
<td id="S3.T1.4.10.10.1" class="ltx_td ltx_align_left">Statista <cite class="ltx_cite ltx_citemacro_citep">(Kantharaj et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S3.T1.4.10.10.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T1.4.10.10.3" class="ltx_td ltx_nopad_r ltx_align_right">29,589</td>
</tr>
<tr id="S3.T1.4.11.11" class="ltx_tr">
<td id="S3.T1.4.11.11.1" class="ltx_td ltx_align_left">OpenCQA <cite class="ltx_cite ltx_citemacro_citep">(Kantharaj et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S3.T1.4.11.11.2" class="ltx_td"></td>
<td id="S3.T1.4.11.11.3" class="ltx_td ltx_nopad_r ltx_align_right">5,407</td>
</tr>
<tr id="S3.T1.4.12.12" class="ltx_tr">
<td id="S3.T1.4.12.12.1" class="ltx_td ltx_align_left">Vistext <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.4.12.12.2" class="ltx_td"></td>
<td id="S3.T1.4.12.12.3" class="ltx_td ltx_nopad_r ltx_align_right">11,171</td>
</tr>
<tr id="S3.T1.4.13.13" class="ltx_tr">
<td id="S3.T1.4.13.13.1" class="ltx_td ltx_align_left">ChartSumm <cite class="ltx_cite ltx_citemacro_citep">(Rahman et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.4.13.13.2" class="ltx_td"></td>
<td id="S3.T1.4.13.13.3" class="ltx_td ltx_nopad_r ltx_align_right">75,255</td>
</tr>
<tr id="S3.T1.4.14.14" class="ltx_tr">
<td id="S3.T1.4.14.14.1" class="ltx_td ltx_align_left">Chart2Text-8k <cite class="ltx_cite ltx_citemacro_citep">(Obeid and Hoque, <a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S3.T1.4.14.14.2" class="ltx_td"></td>
<td id="S3.T1.4.14.14.3" class="ltx_td ltx_nopad_r ltx_align_right">7,862</td>
</tr>
<tr id="S3.T1.4.15.15" class="ltx_tr">
<td id="S3.T1.4.15.15.1" class="ltx_td ltx_align_left ltx_border_t"><em id="S3.T1.4.15.15.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Chart-to-table generation</em></td>
<td id="S3.T1.4.15.15.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.4.15.15.3" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S3.T1.4.16.16" class="ltx_tr">
<td id="S3.T1.4.16.16.1" class="ltx_td ltx_align_left">ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T1.4.16.16.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T1.4.16.16.3" class="ltx_td ltx_nopad_r ltx_align_right">19,373</td>
</tr>
<tr id="S3.T1.4.17.17" class="ltx_tr">
<td id="S3.T1.4.17.17.1" class="ltx_td ltx_align_left">PlotQA <cite class="ltx_cite ltx_citemacro_citep">(Methani et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S3.T1.4.17.17.2" class="ltx_td"></td>
<td id="S3.T1.4.17.17.3" class="ltx_td ltx_nopad_r ltx_align_right">190,720</td>
</tr>
<tr id="S3.T1.4.18.18" class="ltx_tr">
<td id="S3.T1.4.18.18.1" class="ltx_td ltx_align_left">Chart2Text-8k</td>
<td id="S3.T1.4.18.18.2" class="ltx_td"></td>
<td id="S3.T1.4.18.18.3" class="ltx_td ltx_nopad_r ltx_align_right">8,305</td>
</tr>
<tr id="S3.T1.4.19.19" class="ltx_tr">
<td id="S3.T1.4.19.19.1" class="ltx_td ltx_align_left">DVQA <cite class="ltx_cite ltx_citemacro_citep">(Kafle et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S3.T1.4.19.19.2" class="ltx_td"></td>
<td id="S3.T1.4.19.19.3" class="ltx_td ltx_nopad_r ltx_align_right">300,000</td>
</tr>
<tr id="S3.T1.4.20.20" class="ltx_tr">
<td id="S3.T1.4.20.20.1" class="ltx_td ltx_align_left">Statista <cite class="ltx_cite ltx_citemacro_citep">(Kantharaj et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S3.T1.4.20.20.2" class="ltx_td"></td>
<td id="S3.T1.4.20.20.3" class="ltx_td ltx_nopad_r ltx_align_right">29,589</td>
</tr>
<tr id="S3.T1.4.21.21" class="ltx_tr">
<td id="S3.T1.4.21.21.1" class="ltx_td ltx_align_left ltx_border_t"><em id="S3.T1.4.21.21.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Chart instruction following</em></td>
<td id="S3.T1.4.21.21.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.4.21.21.3" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S3.T1.4.22.22" class="ltx_tr">
<td id="S3.T1.4.22.22.1" class="ltx_td ltx_align_left">ChartLlama <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.4.22.22.2" class="ltx_td"></td>
<td id="S3.T1.4.22.22.3" class="ltx_td ltx_nopad_r ltx_align_right">148,398</td>
</tr>
<tr id="S3.T1.4.23.23" class="ltx_tr">
<td id="S3.T1.4.23.23.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T1.4.23.23.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="S3.T1.4.23.23.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S3.T1.4.23.23.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t"><span id="S3.T1.4.23.23.3.1" class="ltx_text ltx_font_bold">1,364,921</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Multitask Learning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We perform multitask learning to train our TinyChart model. We collect a chart understanding dataset that contains 1.36M samples for supervised fine-tuning. It covers various chart understanding tasks including chart question answering, chart-to-text generation, chart-to-table generation, and chart instruction following. Table <a href="#S3.T1" title="Table 1 ‣ 3.2.2. GPT-based PoT ‣ 3.2. Program-of-Thoughts Learning ‣ 3. TinyChart ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the collection of our training dataset.
We mix data in different tasks together to jointly train the model, and use task-specified instructions to enable the model to differentiate between them. The training objective is language modeling on response tokens as presented in Eq.<a href="#S3.E2" title="In 3.1.3. Large Language Model ‣ 3.1. Model Architecture ‣ 3. TinyChart ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Note that in ablation studies, we train solely with benchmark datasets due to limited computational resources.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.13.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>. </span><span id="S3.T2.14.2" class="ltx_text" style="font-size:90%;">Main results on chart-related benchmarks. The inference throughput is evaluated on the ChartQA test with a batch size of 1 on V100 32GB.</span></figcaption>
<table id="S3.T2.11" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.11.12.1" class="ltx_tr">
<th id="S3.T2.11.12.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S3.T2.11.12.1.1.1" class="ltx_text">Model</span></th>
<th id="S3.T2.11.12.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S3.T2.11.12.1.2.1" class="ltx_text">#Parameters</span></th>
<td id="S3.T2.11.12.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S3.T2.11.12.1.3.1" class="ltx_text">Resolution</span></td>
<td id="S3.T2.11.12.1.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S3.T2.11.12.1.4.1" class="ltx_text">
<span id="S3.T2.11.12.1.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.11.12.1.4.1.1.1" class="ltx_tr">
<span id="S3.T2.11.12.1.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Inference</span></span>
<span id="S3.T2.11.12.1.4.1.1.2" class="ltx_tr">
<span id="S3.T2.11.12.1.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Throughput</span></span>
</span></span></td>
<td id="S3.T2.11.12.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">ChartQA</td>
<td id="S3.T2.11.12.1.6" class="ltx_td ltx_align_center ltx_border_tt">Chart-to-Text</td>
<td id="S3.T2.11.12.1.7" class="ltx_td ltx_align_center ltx_border_tt">Chart-to-Table</td>
<td id="S3.T2.11.12.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">OpenCQA</td>
</tr>
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Aug.</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Hum.</td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="S3.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_t">BLEU4</td>
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t">RMS<sub id="S3.T2.1.1.1.1" class="ltx_sub"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">F1</span></sub>
</td>
<td id="S3.T2.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">BLEU4</td>
</tr>
<tr id="S3.T2.11.13.2" class="ltx_tr">
<th id="S3.T2.11.13.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><em id="S3.T2.11.13.2.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Close source models</em></th>
<td id="S3.T2.11.13.2.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.13.2.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.13.2.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.13.2.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.13.2.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.13.2.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.13.2.8" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.13.2.9" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T2.11.14.3" class="ltx_tr">
<th id="S3.T2.11.14.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4V <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib40" title="" class="ltx_ref">2023b</a>)</cite>
</th>
<th id="S3.T2.11.14.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">-</th>
<td id="S3.T2.11.14.3.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.14.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.14.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.14.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.14.3.7" class="ltx_td ltx_align_center">78.50</td>
<td id="S3.T2.11.14.3.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.14.3.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.14.3.10" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.11.15.4" class="ltx_tr">
<th id="S3.T2.11.15.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Gemini-Ultra <cite class="ltx_cite ltx_citemacro_citep">(Team et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>
</th>
<th id="S3.T2.11.15.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">-</th>
<td id="S3.T2.11.15.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.15.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.15.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.15.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.15.4.7" class="ltx_td ltx_align_center">80.80</td>
<td id="S3.T2.11.15.4.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.15.4.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.15.4.10" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.11.16.5" class="ltx_tr">
<th id="S3.T2.11.16.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen-VL-Max <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>
</th>
<th id="S3.T2.11.16.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">-</th>
<td id="S3.T2.11.16.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.16.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.16.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.16.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.16.5.7" class="ltx_td ltx_align_center">79.80</td>
<td id="S3.T2.11.16.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.16.5.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.16.5.10" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.11.17.6" class="ltx_tr">
<th id="S3.T2.11.17.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Deplot+Codex <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023a</a>)</cite>
</th>
<th id="S3.T2.11.17.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">1.3B+175B</th>
<td id="S3.T2.11.17.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.17.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.17.6.5" class="ltx_td ltx_align_center">91.00</td>
<td id="S3.T2.11.17.6.6" class="ltx_td ltx_align_center">67.60</td>
<td id="S3.T2.11.17.6.7" class="ltx_td ltx_align_center">79.30</td>
<td id="S3.T2.11.17.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.17.6.9" class="ltx_td ltx_align_center">87.22</td>
<td id="S3.T2.11.17.6.10" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.11.18.7" class="ltx_tr">
<th id="S3.T2.11.18.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><em id="S3.T2.11.18.7.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Open source models</em></th>
<td id="S3.T2.11.18.7.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.18.7.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.18.7.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.18.7.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.18.7.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.18.7.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.18.7.8" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.11.18.7.9" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<th id="S3.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Llava1.5 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023b</a>)</cite>
</th>
<th id="S3.T2.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">13B</th>
<td id="S3.T2.2.2.1" class="ltx_td ltx_align_center">336<math id="S3.T2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.2.2.1.m1.1a"><mo id="S3.T2.2.2.1.m1.1.1" xref="S3.T2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.m1.1b"><times id="S3.T2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.m1.1c">\times</annotation></semantics></math>336</td>
<td id="S3.T2.2.2.4" class="ltx_td ltx_align_center">1.94 it/s</td>
<td id="S3.T2.2.2.5" class="ltx_td ltx_align_center">72.96</td>
<td id="S3.T2.2.2.6" class="ltx_td ltx_align_center">37.68</td>
<td id="S3.T2.2.2.7" class="ltx_td ltx_align_center">55.32</td>
<td id="S3.T2.2.2.8" class="ltx_td ltx_align_center">7.16</td>
<td id="S3.T2.2.2.9" class="ltx_td ltx_align_center">48.95</td>
<td id="S3.T2.2.2.10" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<th id="S3.T2.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen-VL <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>
</th>
<th id="S3.T2.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">9.6B</th>
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_center">448<math id="S3.T2.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.3.3.1.m1.1a"><mo id="S3.T2.3.3.1.m1.1.1" xref="S3.T2.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.m1.1b"><times id="S3.T2.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.m1.1c">\times</annotation></semantics></math>448</td>
<td id="S3.T2.3.3.4" class="ltx_td ltx_align_center">1.65 it/s</td>
<td id="S3.T2.3.3.5" class="ltx_td ltx_align_center">78.90</td>
<td id="S3.T2.3.3.6" class="ltx_td ltx_align_center">44.30</td>
<td id="S3.T2.3.3.7" class="ltx_td ltx_align_center">61.60</td>
<td id="S3.T2.3.3.8" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.3.3.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.3.3.10" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.5.5" class="ltx_tr">
<th id="S3.T2.5.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">UReader <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2023a</a>)</cite>
</th>
<th id="S3.T2.5.5.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">7B</th>
<td id="S3.T2.5.5.2" class="ltx_td ltx_align_center">224<math id="S3.T2.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.4.4.1.m1.1a"><mo id="S3.T2.4.4.1.m1.1.1" xref="S3.T2.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.1.m1.1b"><times id="S3.T2.4.4.1.m1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.1.m1.1c">\times</annotation></semantics></math>224(<math id="S3.T2.5.5.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.5.5.2.m2.1a"><mo id="S3.T2.5.5.2.m2.1.1" xref="S3.T2.5.5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.2.m2.1b"><times id="S3.T2.5.5.2.m2.1.1.cmml" xref="S3.T2.5.5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.2.m2.1c">\times</annotation></semantics></math>20)</td>
<td id="S3.T2.5.5.5" class="ltx_td ltx_align_center">1.67 it/s</td>
<td id="S3.T2.5.5.6" class="ltx_td ltx_align_center">79.42</td>
<td id="S3.T2.5.5.7" class="ltx_td ltx_align_center">39.12</td>
<td id="S3.T2.5.5.8" class="ltx_td ltx_align_center">59.30</td>
<td id="S3.T2.5.5.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.5.5.10" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.5.5.11" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.7.7" class="ltx_tr">
<th id="S3.T2.7.7.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">DocOwl1.5 <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2024b</a>)</cite>
</th>
<th id="S3.T2.7.7.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">8B</th>
<td id="S3.T2.7.7.2" class="ltx_td ltx_align_center">448<math id="S3.T2.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.6.6.1.m1.1a"><mo id="S3.T2.6.6.1.m1.1.1" xref="S3.T2.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.1.m1.1b"><times id="S3.T2.6.6.1.m1.1.1.cmml" xref="S3.T2.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.1.m1.1c">\times</annotation></semantics></math>448(<math id="S3.T2.7.7.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.7.7.2.m2.1a"><mo id="S3.T2.7.7.2.m2.1.1" xref="S3.T2.7.7.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.2.m2.1b"><times id="S3.T2.7.7.2.m2.1.1.cmml" xref="S3.T2.7.7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.2.m2.1c">\times</annotation></semantics></math>9)</td>
<td id="S3.T2.7.7.5" class="ltx_td ltx_align_center">1.56 it/s</td>
<td id="S3.T2.7.7.6" class="ltx_td ltx_align_center">91.38</td>
<td id="S3.T2.7.7.7" class="ltx_td ltx_align_center">49.62</td>
<td id="S3.T2.7.7.8" class="ltx_td ltx_align_center">70.50</td>
<td id="S3.T2.7.7.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.7.7.10" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.7.7.11" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.11.19.8" class="ltx_tr">
<th id="S3.T2.11.19.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ChartInstruct <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>)</cite>
</th>
<th id="S3.T2.11.19.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">7B</th>
<td id="S3.T2.11.19.8.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.19.8.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.19.8.5" class="ltx_td ltx_align_center">87.76</td>
<td id="S3.T2.11.19.8.6" class="ltx_td ltx_align_center">45.52</td>
<td id="S3.T2.11.19.8.7" class="ltx_td ltx_align_center">66.64</td>
<td id="S3.T2.11.19.8.8" class="ltx_td ltx_align_center">13.83</td>
<td id="S3.T2.11.19.8.9" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.11.19.8.10" class="ltx_td ltx_nopad_r ltx_align_center">15.59</td>
</tr>
<tr id="S3.T2.8.8" class="ltx_tr">
<th id="S3.T2.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ChartLlama <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>
</th>
<th id="S3.T2.8.8.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">13B</th>
<td id="S3.T2.8.8.1" class="ltx_td ltx_align_center">336<math id="S3.T2.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.8.8.1.m1.1a"><mo id="S3.T2.8.8.1.m1.1.1" xref="S3.T2.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.1.m1.1b"><times id="S3.T2.8.8.1.m1.1.1.cmml" xref="S3.T2.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.1.m1.1c">\times</annotation></semantics></math>336</td>
<td id="S3.T2.8.8.4" class="ltx_td ltx_align_center">1.94 it/s</td>
<td id="S3.T2.8.8.5" class="ltx_td ltx_align_center">90.36</td>
<td id="S3.T2.8.8.6" class="ltx_td ltx_align_center">48.96</td>
<td id="S3.T2.8.8.7" class="ltx_td ltx_align_center">69.66</td>
<td id="S3.T2.8.8.8" class="ltx_td ltx_align_center">14.23</td>
<td id="S3.T2.8.8.9" class="ltx_td ltx_align_center">90.00</td>
<td id="S3.T2.8.8.10" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S3.T2.9.9" class="ltx_tr">
<th id="S3.T2.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">ChartAst <cite class="ltx_cite ltx_citemacro_citep">(Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>
</th>
<th id="S3.T2.9.9.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">13B</th>
<td id="S3.T2.9.9.1" class="ltx_td ltx_align_center">448<math id="S3.T2.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.9.9.1.m1.1a"><mo id="S3.T2.9.9.1.m1.1.1" xref="S3.T2.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.1.m1.1b"><times id="S3.T2.9.9.1.m1.1.1.cmml" xref="S3.T2.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.1.m1.1c">\times</annotation></semantics></math>448</td>
<td id="S3.T2.9.9.4" class="ltx_td ltx_align_center">1.47 it/s</td>
<td id="S3.T2.9.9.5" class="ltx_td ltx_align_center"><span id="S3.T2.9.9.5.1" class="ltx_text ltx_font_bold">93.90</span></td>
<td id="S3.T2.9.9.6" class="ltx_td ltx_align_center">65.90</td>
<td id="S3.T2.9.9.7" class="ltx_td ltx_align_center">79.90</td>
<td id="S3.T2.9.9.8" class="ltx_td ltx_align_center">15.50</td>
<td id="S3.T2.9.9.9" class="ltx_td ltx_align_center">91.60</td>
<td id="S3.T2.9.9.10" class="ltx_td ltx_nopad_r ltx_align_center">15.50</td>
</tr>
<tr id="S3.T2.10.10" class="ltx_tr">
<th id="S3.T2.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TinyChart@512</th>
<th id="S3.T2.10.10.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">3B</th>
<td id="S3.T2.10.10.1" class="ltx_td ltx_align_center ltx_border_t">512<math id="S3.T2.10.10.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.10.10.1.m1.1a"><mo id="S3.T2.10.10.1.m1.1.1" xref="S3.T2.10.10.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.10.10.1.m1.1b"><times id="S3.T2.10.10.1.m1.1.1.cmml" xref="S3.T2.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.10.1.m1.1c">\times</annotation></semantics></math>512</td>
<td id="S3.T2.10.10.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T2.10.10.4.1" class="ltx_text ltx_font_bold">3.65</span> it/s</td>
<td id="S3.T2.10.10.5" class="ltx_td ltx_align_center ltx_border_t">93.60</td>
<td id="S3.T2.10.10.6" class="ltx_td ltx_align_center ltx_border_t">72.16</td>
<td id="S3.T2.10.10.7" class="ltx_td ltx_align_center ltx_border_t">82.88</td>
<td id="S3.T2.10.10.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.10.10.8.1" class="ltx_text ltx_font_bold">17.93</span></td>
<td id="S3.T2.10.10.9" class="ltx_td ltx_align_center ltx_border_t">92.93</td>
<td id="S3.T2.10.10.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">19.62</td>
</tr>
<tr id="S3.T2.11.11" class="ltx_tr">
<th id="S3.T2.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">TinyChart@768</th>
<th id="S3.T2.11.11.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">3B</th>
<td id="S3.T2.11.11.1" class="ltx_td ltx_align_center ltx_border_bb">768<math id="S3.T2.11.11.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T2.11.11.1.m1.1a"><mo id="S3.T2.11.11.1.m1.1.1" xref="S3.T2.11.11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.11.11.1.m1.1b"><times id="S3.T2.11.11.1.m1.1.1.cmml" xref="S3.T2.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.11.1.m1.1c">\times</annotation></semantics></math>768</td>
<td id="S3.T2.11.11.4" class="ltx_td ltx_align_center ltx_border_bb">3.14 it/s</td>
<td id="S3.T2.11.11.5" class="ltx_td ltx_align_center ltx_border_bb">93.86</td>
<td id="S3.T2.11.11.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.11.11.6.1" class="ltx_text ltx_font_bold">73.34</span></td>
<td id="S3.T2.11.11.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.11.11.7.1" class="ltx_text ltx_font_bold">83.60</span></td>
<td id="S3.T2.11.11.8" class="ltx_td ltx_align_center ltx_border_bb">17.18</td>
<td id="S3.T2.11.11.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.11.11.9.1" class="ltx_text ltx_font_bold">93.78</span></td>
<td id="S3.T2.11.11.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S3.T2.11.11.10.1" class="ltx_text ltx_font_bold">20.39</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Implementation Details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.6" class="ltx_p">TinyChart is initialized from TinyLlava <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2024</a>)</cite>, which utilizes the SigLIP <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2023</a>)</cite> as the vision encoder and Phi-2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2023a</a>)</cite> as the large language model. The origin input resolution of the vision encoder is 384<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation></semantics></math>384. We extend the input resolution to 512<math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mo id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><times id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\times</annotation></semantics></math>512 and 768<math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mo id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><times id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\times</annotation></semantics></math>768 and apply visual token merging with <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="r=20" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">r</mi><mo id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><eq id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></eq><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">𝑟</ci><cn type="integer" id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">r=20</annotation></semantics></math> and <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="r=84" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mrow id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml"><mi id="S4.SS1.p1.5.m5.1.1.2" xref="S4.SS1.p1.5.m5.1.1.2.cmml">r</mi><mo id="S4.SS1.p1.5.m5.1.1.1" xref="S4.SS1.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.5.m5.1.1.3" xref="S4.SS1.p1.5.m5.1.1.3.cmml">84</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"><eq id="S4.SS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1.1"></eq><ci id="S4.SS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2">𝑟</ci><cn type="integer" id="S4.SS1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3">84</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">r=84</annotation></semantics></math> in each transformer layer respectively. We train the entire model for 3 epochs with a batch size of 512. The learning rate is set to <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mrow id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mrow id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml"><mn id="S4.SS1.p1.6.m6.1.1.2.2" xref="S4.SS1.p1.6.m6.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.6.m6.1.1.2.1" xref="S4.SS1.p1.6.m6.1.1.2.1.cmml">​</mo><mi id="S4.SS1.p1.6.m6.1.1.2.3" xref="S4.SS1.p1.6.m6.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS1.p1.6.m6.1.1.1" xref="S4.SS1.p1.6.m6.1.1.1.cmml">−</mo><mn id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><minus id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1.1"></minus><apply id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2"><times id="S4.SS1.p1.6.m6.1.1.2.1.cmml" xref="S4.SS1.p1.6.m6.1.1.2.1"></times><cn type="integer" id="S4.SS1.p1.6.m6.1.1.2.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2.2">1</cn><ci id="S4.SS1.p1.6.m6.1.1.2.3.cmml" xref="S4.SS1.p1.6.m6.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S4.SS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">1e-4</annotation></semantics></math>, with a warmup in the beginning 3% steps, and then decays to 0 at the end of training. The total training process costs 3 days on 32 Tesla V100 GPUs with 32 GB VRAMs.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Evaluation Benchmarks</h3>

<div id="S4.SS2.p1" class="ltx_para">
<span id="S4.SS2.p1.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS2.p1.2" class="ltx_p"><span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_bold">ChartQA</span> ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> aims to generate a short answer to the question based on the chart content. It includes a lot of questions that require numerical calculation.
We report the relaxed accuracy that allows numerical error within 5% as the metric following <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>; Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>. Note that our TinyChart with Program-of-Thoughts learning can perform ChartQA in the following four settings:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Direct</span>: the model produces short answers directly.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">PoT</span>: the model produces Python code. The answer is then calculated through the Python interpreter.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Combine</span>: the model produces Python code for questions that require calculation, and Direct answers for others. We determine whether a question requires calculation with a simple rule-based keyword detector. If the question contains one of the calculative keywords<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>sum, mean, average, ratio, mode, divide, dividing, differ, subtract, add, division, times, absolute, minus, exceed, below, less, fewer, bigger, biggest, greater, higher, longer, tallest, lowest, number, how many colors, what is the value</span></span></span>, the detector will treat it as a computational question and prompt the model to generate a PoT answer. Otherwise, the model is instructed to produce a Direct answer. Additionally, if the generated program of a calculative question encounters syntax errors, we let the model produce Direct answers for this question in the Combine setting.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Oracle</span>
We further introduce the Oracle setting for ChartQA evaluation. Under this setting, we always choose the correct one between the Direct and PoT answers after evaluating under both settings. It is the upper bound of the combination across the two answers.</p>
</div>
</li>
</ul>
<p id="S4.SS2.p1.3" class="ltx_p">We evaluate TinyChart under the Combine setting by default.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<span id="S4.SS2.p2.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS2.p2.2" class="ltx_p"><span id="S4.SS2.p2.2.1" class="ltx_text ltx_font_bold">Chart-to-Text</span> Chart-to-Text aims to generate a chart summarization based on chart content. We evaluate the model with the Pew benchmark <cite class="ltx_cite ltx_citemacro_citep">(Kantharaj et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022b</a>)</cite>, and report BLEU4 <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2002</a>)</cite> as the metric.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<span id="S4.SS2.p3.2" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Chart-to-Table</span> Chart-to-Table aims to extract the underlying data table presented by the chart. We evaluate the performance of Chart-to-Table with the data table annotation provided by ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> following <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>. We report RMS<sub id="S4.SS2.p3.1.2" class="ltx_sub"><span id="S4.SS2.p3.1.2.1" class="ltx_text ltx_font_italic">F1</span></sub> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2023a</a>)</cite> as the metric.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<span id="S4.SS2.p4.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS2.p4.2" class="ltx_p"><span id="S4.SS2.p4.2.1" class="ltx_text ltx_font_bold">OpenCQA</span> Different from ChartQA, OpenCQA <cite class="ltx_cite ltx_citemacro_citep">(Kantharaj et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022a</a>)</cite> evaluates the ability of models to generate free-form answers to the chart-related questions. We report BLEU4 <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2002</a>)</cite> as the metric following <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024</a>; Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<span id="S4.SS2.p5.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS2.p5.2" class="ltx_p"><span id="S4.SS2.p5.2.1" class="ltx_text ltx_font_bold">ChartX</span> ChartX <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>)</cite> is a recently proposed benchmark that contains more chart types. We evaluate the ChartX cognition tasks since they are more challenging. It covers Question Answering, Chart Description Generation, Chart Summary Generation, and Chart Redrawing. We report the GPT-Accuracy for QA and GPT-score for the remaining 3 tasks as the metrics following ChartX <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Main Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.3. Multitask Learning ‣ 3. TinyChart ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an extensive comparison between TinyChart and existing multimodal large language models on 4 chart understanding benchmarks. Our TinyChart model achieves state-of-the-art performance on ChartQA, Chart-to-Text, Chart-to-Table, and OpenCQA, while excels in larger inference throughput. Specifically, with the input resolution set at 768<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><times id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\times</annotation></semantics></math>768, TinyChart achieves an accuracy of 83.60 on ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>, surpassing several closed-source models including GPT-4V, Gemini-Ultra, and Qwen-VL-Max <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>. It also outperforms previous open-source SOTA ChartAst <cite class="ltx_cite ltx_citemacro_citep">(Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite> on chart understanding.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We find that previous models performed poorly on the ChartQA human subset, with none of them achieving over 70%. In contrast, the performance on the ChartQA-augmentation has approached 93.9%. This is because the questions posed by human annotators involve more computational problems <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> and are more challenging. By leveraging the Program-of-Thoughts learning, TinyChart achieves performance of 73.34% on ChartQA-human, which is an improvement of 7.44% over the previous state-of-the-art ChartAst <cite class="ltx_cite ltx_citemacro_citep">(Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>. This demonstrates the effectiveness of our proposed learning method based on the Program-of-Thoughts.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">We observed that models with higher input resolutions generally perform better on chart understanding tasks. However, encoding high-resolution charts leads to a decrease in inference speed (e.g., Qwen-VL vs. Llava1.5, DocOwl1.5 vs. UReader, ChartAst vs. ChartLlama). By leveraging visual token merging, TinyChart is able to accept higher-resolution input images with a limited increase in computing demands, thus achieving better performance. Due to the smaller model size and the efficient visual token merging strategy, TinyChart achieves significantly larger inference throughput compared to previous models. In summary, these results demonstrate that TinyChart can achieve efficient chart understanding with enhanced performance and faster inference.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<span id="S4.SS3.p4.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS3.p4.2" class="ltx_p"><span id="S4.SS3.p4.2.1" class="ltx_text ltx_font_bold">ChartQA performance under different settings.</span>Table <a href="#S4.T3" title="Table 3 ‣ 4.3. Main Results ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the performance comparison under different settings. Note that the performance of ChartAst under the Combine setting is from <cite class="ltx_cite ltx_citemacro_citet">Meng et al<span class="ltx_text">.</span> (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>, which leverages a combination of Direct answer and executive JSON to produce the final answer. The results indicate that our TinyChart model could achieve SOTA performance on the Direct answer. By combining with PoT answers, TinyChart could make further improvements. In addition, since the combination of Direct and PoT answers is very simple, the performance under the Combine setting falls behind the Oracle setting a lot. Further study can be conducted to better combine the two answers.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<span id="S4.SS3.p5.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS3.p5.2" class="ltx_p"><span id="S4.SS3.p5.2.1" class="ltx_text ltx_font_bold">Calculative and non-calculative questions.</span> We divide the questions in ChartQA test set <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> into two categories: calculative questions (761 of 2500) and non-calculative questions (1739 of 2500) by checking whether they contain calculative keywords mentioned above. Table <a href="#S4.T4" title="Table 4 ‣ 4.3. Main Results ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the performance of TinyChart@768 on these two types of questions under different settings. We observe that PoT significantly improves the performance on calculative questions compared to Direct settings (78.98 vs. 56.64) and thus it shows overall performance gains (80.84 vs. 76.36). And the simple combination of Direct and PoT strategies further makes improvements.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>. </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Performance on ChartQA under different settings.</span></figcaption>
<table id="S4.T3.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T3.4.1.1.1.1" class="ltx_text">Model</span></th>
<th id="S4.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">ChartQA</th>
</tr>
<tr id="S4.T3.4.2.2" class="ltx_tr">
<th id="S4.T3.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Direct</th>
<th id="S4.T3.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">PoT</th>
<th id="S4.T3.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Oracle</th>
<th id="S4.T3.4.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">Combine</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.3.1" class="ltx_tr">
<th id="S4.T3.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ChartLlama <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T3.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t">69.66</td>
<td id="S4.T3.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.4.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.4.3.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T3.4.4.2" class="ltx_tr">
<th id="S4.T3.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ChartAst <cite class="ltx_cite ltx_citemacro_citep">(Meng et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>
</th>
<td id="S4.T3.4.4.2.2" class="ltx_td ltx_align_center">75.10</td>
<td id="S4.T3.4.4.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.4.4.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.4.4.2.5" class="ltx_td ltx_nopad_r ltx_align_center">79.90</td>
</tr>
<tr id="S4.T3.4.5.3" class="ltx_tr">
<th id="S4.T3.4.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TinyChart@512</th>
<td id="S4.T3.4.5.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.5.3.2.1" class="ltx_text ltx_font_bold">76.92</span></td>
<td id="S4.T3.4.5.3.3" class="ltx_td ltx_align_center ltx_border_t">79.64</td>
<td id="S4.T3.4.5.3.4" class="ltx_td ltx_align_center ltx_border_t">88.76</td>
<td id="S4.T3.4.5.3.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">82.88</td>
</tr>
<tr id="S4.T3.4.6.4" class="ltx_tr">
<th id="S4.T3.4.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">TinyChart@768</th>
<td id="S4.T3.4.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">76.36</td>
<td id="S4.T3.4.6.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.6.4.3.1" class="ltx_text ltx_font_bold">80.84</span></td>
<td id="S4.T3.4.6.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.6.4.4.1" class="ltx_text ltx_font_bold">89.12</span></td>
<td id="S4.T3.4.6.4.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T3.4.6.4.5.1" class="ltx_text ltx_font_bold">83.60</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>. </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">ChartQA performance on Calculative (Cal.) and Non-calculative (Non-cal.) questions.</span></figcaption>
<div id="S4.T4.4" class="ltx_inline-block ltx_transformed_outer" style="width:300.4pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.7pt,4.5pt) scale(0.9,0.9) ;">
<table id="S4.T4.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.4.1.1.1" class="ltx_tr">
<th id="S4.T4.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T4.4.1.1.1.1.1" class="ltx_text">Model</span></th>
<th id="S4.T4.4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T4.4.1.1.1.2.1" class="ltx_text">Setting</span></th>
<th id="S4.T4.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">ChartQA</th>
</tr>
<tr id="S4.T4.4.1.2.2" class="ltx_tr">
<th id="S4.T4.4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Cal. (761)</th>
<th id="S4.T4.4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Non-cal. (1739)</th>
<th id="S4.T4.4.1.2.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">Total (2500)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.4.1.3.1" class="ltx_tr">
<th id="S4.T4.4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">TinyChart@768</th>
<th id="S4.T4.4.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Direct</th>
<td id="S4.T4.4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">56.64</td>
<td id="S4.T4.4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.3.1.4.1" class="ltx_text ltx_font_bold">84.99</span></td>
<td id="S4.T4.4.1.3.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">76.36</td>
</tr>
<tr id="S4.T4.4.1.4.2" class="ltx_tr">
<th id="S4.T4.4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TinyChart@768</th>
<th id="S4.T4.4.1.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">PoT</th>
<td id="S4.T4.4.1.4.2.3" class="ltx_td ltx_align_center">78.98</td>
<td id="S4.T4.4.1.4.2.4" class="ltx_td ltx_align_center">81.66</td>
<td id="S4.T4.4.1.4.2.5" class="ltx_td ltx_nopad_r ltx_align_center">80.84</td>
</tr>
<tr id="S4.T4.4.1.5.3" class="ltx_tr">
<th id="S4.T4.4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">TinyChart@768</th>
<th id="S4.T4.4.1.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Combine</th>
<td id="S4.T4.4.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.4.1.5.3.3.1" class="ltx_text ltx_font_bold">80.42</span></td>
<td id="S4.T4.4.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.4.1.5.3.4.1" class="ltx_text ltx_font_bold">84.99</span></td>
<td id="S4.T4.4.1.5.3.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T4.4.1.5.3.5.1" class="ltx_text ltx_font_bold">83.60</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS3.p6" class="ltx_para">
<span id="S4.SS3.p6.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS3.p6.2" class="ltx_p"><span id="S4.SS3.p6.2.1" class="ltx_text ltx_font_bold">Evaluation on ChartX.</span>To further assess the generalizability of TinyChart, we compare our model with end-to-end General MLLM and Chart MLLM on ChartX-Cognition benchmark <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>)</cite>, since it covers visually diverse chart types. We use TinyChart@768 to perform inference on ChartX without additional fine-tuning. As shown in Table <a href="#S4.T5" title="Table 5 ‣ 4.3. Main Results ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, benefiting from our Program-of-Thoughts learning method, TinyChart achieves a 33.35 GPT-Accuracy on the QA task, even surpassing the GPT-4V model. Though it falls behind GPT-4V in Summary, Description, and Redrawing tasks, TinyChart still performs better than open-source Chart MLLMs including ChartLlama and ChartAst. It indicates that TinyChart has a strong capability to generalize across various chart types.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>. </span><span id="S4.T5.3.2" class="ltx_text" style="font-size:90%;">Evaluation results on ChartX <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>)</cite>.</span></figcaption>
<table id="S4.T5.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.4.1.1" class="ltx_tr">
<th id="S4.T5.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T5.4.1.1.1.1" class="ltx_text">Model</span></th>
<td id="S4.T5.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">ChartX Cognition</td>
</tr>
<tr id="S4.T5.4.2.2" class="ltx_tr">
<td id="S4.T5.4.2.2.1" class="ltx_td ltx_align_center ltx_border_t">QA</td>
<td id="S4.T5.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Summary</td>
<td id="S4.T5.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Description</td>
<td id="S4.T5.4.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">Redrawing</td>
</tr>
<tr id="S4.T5.4.3.3" class="ltx_tr">
<th id="S4.T5.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><em id="S4.T5.4.3.3.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">General MLLM</em></th>
<td id="S4.T5.4.3.3.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.4.3.3.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.4.3.3.4" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S4.T5.4.4.4" class="ltx_tr">
<th id="S4.T5.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Llava1.5</th>
<td id="S4.T5.4.4.4.2" class="ltx_td ltx_align_center">17.19</td>
<td id="S4.T5.4.4.4.3" class="ltx_td ltx_align_center">1.48</td>
<td id="S4.T5.4.4.4.4" class="ltx_td ltx_align_center">1.29</td>
<td id="S4.T5.4.4.4.5" class="ltx_td ltx_nopad_r ltx_align_center">0.75</td>
</tr>
<tr id="S4.T5.4.5.5" class="ltx_tr">
<th id="S4.T5.4.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4V</th>
<td id="S4.T5.4.5.5.2" class="ltx_td ltx_align_center">33.04</td>
<td id="S4.T5.4.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T5.4.5.5.3.1" class="ltx_text ltx_font_bold">3.17</span></td>
<td id="S4.T5.4.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T5.4.5.5.4.1" class="ltx_text ltx_font_bold">3.12</span></td>
<td id="S4.T5.4.5.5.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.4.5.5.5.1" class="ltx_text ltx_font_bold">2.63</span></td>
</tr>
<tr id="S4.T5.4.6.6" class="ltx_tr">
<th id="S4.T5.4.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><em id="S4.T5.4.6.6.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Chart MLLM</em></th>
<td id="S4.T5.4.6.6.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.4.6.6.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.4.6.6.4" class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S4.T5.4.7.7" class="ltx_tr">
<th id="S4.T5.4.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ChartLlama</th>
<td id="S4.T5.4.7.7.2" class="ltx_td ltx_align_center">13.80</td>
<td id="S4.T5.4.7.7.3" class="ltx_td ltx_align_center">1.04</td>
<td id="S4.T5.4.7.7.4" class="ltx_td ltx_align_center">1.02</td>
<td id="S4.T5.4.7.7.5" class="ltx_td ltx_nopad_r ltx_align_center">0.94</td>
</tr>
<tr id="S4.T5.4.8.8" class="ltx_tr">
<th id="S4.T5.4.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ChartAst</th>
<td id="S4.T5.4.8.8.2" class="ltx_td ltx_align_center">30.99</td>
<td id="S4.T5.4.8.8.3" class="ltx_td ltx_align_center">0.33</td>
<td id="S4.T5.4.8.8.4" class="ltx_td ltx_align_center">1.03</td>
<td id="S4.T5.4.8.8.5" class="ltx_td ltx_nopad_r ltx_align_center">0.82</td>
</tr>
<tr id="S4.T5.4.9.9" class="ltx_tr">
<th id="S4.T5.4.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">TinyChart@768</th>
<td id="S4.T5.4.9.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.4.9.9.2.1" class="ltx_text ltx_font_bold">33.35</span></td>
<td id="S4.T5.4.9.9.3" class="ltx_td ltx_align_center ltx_border_bb">1.53</td>
<td id="S4.T5.4.9.9.4" class="ltx_td ltx_align_center ltx_border_bb">1.64</td>
<td id="S4.T5.4.9.9.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">1.89</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.24.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>. </span><span id="S4.T6.25.2" class="ltx_text" style="font-size:90%;">Ablation study. We train the models only using benchmark datasets in this experiment.</span></figcaption>
<div id="S4.T6.22" class="ltx_inline-block ltx_transformed_outer" style="width:639.0pt;height:199pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T6.22.22" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.22.22.23.1" class="ltx_tr">
<td id="S4.T6.22.22.23.1.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.22.22.23.1.1.1" class="ltx_text">Row</span></td>
<td id="S4.T6.22.22.23.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.22.22.23.1.2.1" class="ltx_text">Resolution</span></td>
<td id="S4.T6.22.22.23.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.22.22.23.1.3.1" class="ltx_text">
<span id="S4.T6.22.22.23.1.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.22.22.23.1.3.1.1.1" class="ltx_tr">
<span id="S4.T6.22.22.23.1.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">GPT</span></span>
<span id="S4.T6.22.22.23.1.3.1.1.2" class="ltx_tr">
<span id="S4.T6.22.22.23.1.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">PoT</span></span>
</span></span></td>
<td id="S4.T6.22.22.23.1.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.22.22.23.1.4.1" class="ltx_text">
<span id="S4.T6.22.22.23.1.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.22.22.23.1.4.1.1.1" class="ltx_tr">
<span id="S4.T6.22.22.23.1.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Template</span></span>
<span id="S4.T6.22.22.23.1.4.1.1.2" class="ltx_tr">
<span id="S4.T6.22.22.23.1.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">PoT</span></span>
</span></span></td>
<td id="S4.T6.22.22.23.1.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.22.22.23.1.5.1" class="ltx_text">
<span id="S4.T6.22.22.23.1.5.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.22.22.23.1.5.1.1.1" class="ltx_tr">
<span id="S4.T6.22.22.23.1.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Visual Patch</span></span>
<span id="S4.T6.22.22.23.1.5.1.1.2" class="ltx_tr">
<span id="S4.T6.22.22.23.1.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Merge</span></span>
</span></span></td>
<td id="S4.T6.22.22.23.1.6" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.22.22.23.1.6.1" class="ltx_text">
<span id="S4.T6.22.22.23.1.6.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.22.22.23.1.6.1.1.1" class="ltx_tr">
<span id="S4.T6.22.22.23.1.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Visual</span></span>
<span id="S4.T6.22.22.23.1.6.1.1.2" class="ltx_tr">
<span id="S4.T6.22.22.23.1.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Length</span></span>
</span></span></td>
<td id="S4.T6.22.22.23.1.7" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.22.22.23.1.7.1" class="ltx_text">
<span id="S4.T6.22.22.23.1.7.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.22.22.23.1.7.1.1.1" class="ltx_tr">
<span id="S4.T6.22.22.23.1.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Inference</span></span>
<span id="S4.T6.22.22.23.1.7.1.1.2" class="ltx_tr">
<span id="S4.T6.22.22.23.1.7.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Throughput</span></span>
</span></span></td>
<td id="S4.T6.22.22.23.1.8" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">ChartQA</td>
<td id="S4.T6.22.22.23.1.9" class="ltx_td ltx_align_center ltx_border_tt">Chart2Text</td>
<td id="S4.T6.22.22.23.1.10" class="ltx_td ltx_align_center ltx_border_tt">Chart2Table</td>
</tr>
<tr id="S4.T6.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Direct</td>
<td id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">PoT</td>
<td id="S4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Combine</td>
<td id="S4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">BLEU4</td>
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">RMS<sub id="S4.T6.1.1.1.1.1" class="ltx_sub"><span id="S4.T6.1.1.1.1.1.1" class="ltx_text ltx_font_italic">F1</span></sub>
</td>
</tr>
<tr id="S4.T6.5.5.5" class="ltx_tr">
<td id="S4.T6.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T6.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t">384<math id="S4.T6.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.2.2.2.1.m1.1a"><mo id="S4.T6.2.2.2.1.m1.1.1" xref="S4.T6.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.1.m1.1b"><times id="S4.T6.2.2.2.1.m1.1.1.cmml" xref="S4.T6.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.1.m1.1c">\times</annotation></semantics></math>384</td>
<td id="S4.T6.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T6.3.3.3.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.3.3.3.2.m1.1a"><mo id="S4.T6.3.3.3.2.m1.1.1" xref="S4.T6.3.3.3.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.2.m1.1b"><times id="S4.T6.3.3.3.2.m1.1.1.cmml" xref="S4.T6.3.3.3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T6.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T6.4.4.4.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.4.4.4.3.m1.1a"><mo id="S4.T6.4.4.4.3.m1.1.1" xref="S4.T6.4.4.4.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.3.m1.1b"><times id="S4.T6.4.4.4.3.m1.1.1.cmml" xref="S4.T6.4.4.4.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.3.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T6.5.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T6.5.5.5.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.5.5.5.4.m1.1a"><mo id="S4.T6.5.5.5.4.m1.1.1" xref="S4.T6.5.5.5.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.5.4.m1.1b"><times id="S4.T6.5.5.5.4.m1.1.1.cmml" xref="S4.T6.5.5.5.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.5.4.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T6.5.5.5.6" class="ltx_td ltx_align_center ltx_border_t">729</td>
<td id="S4.T6.5.5.5.7" class="ltx_td ltx_align_center ltx_border_t">3.73 it/s</td>
<td id="S4.T6.5.5.5.8" class="ltx_td ltx_align_center ltx_border_t">70.72</td>
<td id="S4.T6.5.5.5.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.5.5.5.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.5.5.5.11" class="ltx_td ltx_align_center ltx_border_t">17.10</td>
<td id="S4.T6.5.5.5.12" class="ltx_td ltx_align_center ltx_border_t">85.80</td>
</tr>
<tr id="S4.T6.8.8.8" class="ltx_tr">
<td id="S4.T6.8.8.8.4" class="ltx_td ltx_align_center">2</td>
<td id="S4.T6.6.6.6.1" class="ltx_td ltx_align_center">384<math id="S4.T6.6.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.6.6.6.1.m1.1a"><mo id="S4.T6.6.6.6.1.m1.1.1" xref="S4.T6.6.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.6.6.6.1.m1.1b"><times id="S4.T6.6.6.6.1.m1.1.1.cmml" xref="S4.T6.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.6.6.1.m1.1c">\times</annotation></semantics></math>384</td>
<td id="S4.T6.7.7.7.2" class="ltx_td ltx_align_center"><math id="S4.T6.7.7.7.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.7.7.7.2.m1.1a"><mo id="S4.T6.7.7.7.2.m1.1.1" xref="S4.T6.7.7.7.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.7.7.7.2.m1.1b"><times id="S4.T6.7.7.7.2.m1.1.1.cmml" xref="S4.T6.7.7.7.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.7.7.7.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T6.8.8.8.5" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.8.8.8.3" class="ltx_td ltx_align_center"><math id="S4.T6.8.8.8.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.8.8.8.3.m1.1a"><mo id="S4.T6.8.8.8.3.m1.1.1" xref="S4.T6.8.8.8.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.8.8.8.3.m1.1b"><times id="S4.T6.8.8.8.3.m1.1.1.cmml" xref="S4.T6.8.8.8.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.8.8.8.3.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T6.8.8.8.6" class="ltx_td ltx_align_center">729</td>
<td id="S4.T6.8.8.8.7" class="ltx_td ltx_align_center">3.73 it/s</td>
<td id="S4.T6.8.8.8.8" class="ltx_td ltx_align_center">71.12</td>
<td id="S4.T6.8.8.8.9" class="ltx_td ltx_align_center">55.44</td>
<td id="S4.T6.8.8.8.10" class="ltx_td ltx_align_center">73.00</td>
<td id="S4.T6.8.8.8.11" class="ltx_td ltx_align_center">17.04</td>
<td id="S4.T6.8.8.8.12" class="ltx_td ltx_align_center">87.68</td>
</tr>
<tr id="S4.T6.10.10.10" class="ltx_tr">
<td id="S4.T6.10.10.10.3" class="ltx_td ltx_align_center">3</td>
<td id="S4.T6.9.9.9.1" class="ltx_td ltx_align_center">384<math id="S4.T6.9.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.9.9.9.1.m1.1a"><mo id="S4.T6.9.9.9.1.m1.1.1" xref="S4.T6.9.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.9.9.9.1.m1.1b"><times id="S4.T6.9.9.9.1.m1.1.1.cmml" xref="S4.T6.9.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.9.9.9.1.m1.1c">\times</annotation></semantics></math>384</td>
<td id="S4.T6.10.10.10.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.10.10.10.5" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.10.10.10.2" class="ltx_td ltx_align_center"><math id="S4.T6.10.10.10.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.10.10.10.2.m1.1a"><mo id="S4.T6.10.10.10.2.m1.1.1" xref="S4.T6.10.10.10.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.10.10.10.2.m1.1b"><times id="S4.T6.10.10.10.2.m1.1.1.cmml" xref="S4.T6.10.10.10.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.10.10.10.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T6.10.10.10.6" class="ltx_td ltx_align_center">729</td>
<td id="S4.T6.10.10.10.7" class="ltx_td ltx_align_center">3.73 it/s</td>
<td id="S4.T6.10.10.10.8" class="ltx_td ltx_align_center">72.44</td>
<td id="S4.T6.10.10.10.9" class="ltx_td ltx_align_center">76.88</td>
<td id="S4.T6.10.10.10.10" class="ltx_td ltx_align_center">79.48</td>
<td id="S4.T6.10.10.10.11" class="ltx_td ltx_align_center">16.67</td>
<td id="S4.T6.10.10.10.12" class="ltx_td ltx_align_center">87.30</td>
</tr>
<tr id="S4.T6.12.12.12" class="ltx_tr">
<td id="S4.T6.12.12.12.3" class="ltx_td ltx_align_center ltx_border_t">4</td>
<td id="S4.T6.11.11.11.1" class="ltx_td ltx_align_center ltx_border_t">512<math id="S4.T6.11.11.11.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.11.11.11.1.m1.1a"><mo id="S4.T6.11.11.11.1.m1.1.1" xref="S4.T6.11.11.11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.11.11.11.1.m1.1b"><times id="S4.T6.11.11.11.1.m1.1.1.cmml" xref="S4.T6.11.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.11.11.11.1.m1.1c">\times</annotation></semantics></math>512</td>
<td id="S4.T6.12.12.12.4" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T6.12.12.12.5" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T6.12.12.12.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T6.12.12.12.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.12.12.12.2.m1.1a"><mo id="S4.T6.12.12.12.2.m1.1.1" xref="S4.T6.12.12.12.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.12.12.12.2.m1.1b"><times id="S4.T6.12.12.12.2.m1.1.1.cmml" xref="S4.T6.12.12.12.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.12.12.12.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T6.12.12.12.6" class="ltx_td ltx_align_center ltx_border_t">1,296</td>
<td id="S4.T6.12.12.12.7" class="ltx_td ltx_align_center ltx_border_t">2.38 it/s</td>
<td id="S4.T6.12.12.12.8" class="ltx_td ltx_align_center ltx_border_t">74.08</td>
<td id="S4.T6.12.12.12.9" class="ltx_td ltx_align_center ltx_border_t">79.64</td>
<td id="S4.T6.12.12.12.10" class="ltx_td ltx_align_center ltx_border_t">81.72</td>
<td id="S4.T6.12.12.12.11" class="ltx_td ltx_align_center ltx_border_t">17.32</td>
<td id="S4.T6.12.12.12.12" class="ltx_td ltx_align_center ltx_border_t">89.76</td>
</tr>
<tr id="S4.T6.14.14.14" class="ltx_tr">
<td id="S4.T6.14.14.14.3" class="ltx_td ltx_align_center">5</td>
<td id="S4.T6.13.13.13.1" class="ltx_td ltx_align_center">512<math id="S4.T6.13.13.13.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.13.13.13.1.m1.1a"><mo id="S4.T6.13.13.13.1.m1.1.1" xref="S4.T6.13.13.13.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.13.13.13.1.m1.1b"><times id="S4.T6.13.13.13.1.m1.1.1.cmml" xref="S4.T6.13.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.13.13.13.1.m1.1c">\times</annotation></semantics></math>512</td>
<td id="S4.T6.14.14.14.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.14.14.14.5" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.14.14.14.2" class="ltx_td ltx_align_center">
<math id="S4.T6.14.14.14.2.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.T6.14.14.14.2.m1.1a"><mi id="S4.T6.14.14.14.2.m1.1.1" xref="S4.T6.14.14.14.2.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.T6.14.14.14.2.m1.1b"><ci id="S4.T6.14.14.14.2.m1.1.1.cmml" xref="S4.T6.14.14.14.2.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.14.14.14.2.m1.1c">r</annotation></semantics></math>=12</td>
<td id="S4.T6.14.14.14.6" class="ltx_td ltx_align_center">984</td>
<td id="S4.T6.14.14.14.7" class="ltx_td ltx_align_center">2.84 it/s</td>
<td id="S4.T6.14.14.14.8" class="ltx_td ltx_align_center">73.24</td>
<td id="S4.T6.14.14.14.9" class="ltx_td ltx_align_center">77.72</td>
<td id="S4.T6.14.14.14.10" class="ltx_td ltx_align_center">80.52</td>
<td id="S4.T6.14.14.14.11" class="ltx_td ltx_align_center">16.54</td>
<td id="S4.T6.14.14.14.12" class="ltx_td ltx_align_center">88.26</td>
</tr>
<tr id="S4.T6.16.16.16" class="ltx_tr">
<td id="S4.T6.16.16.16.3" class="ltx_td ltx_align_center">6</td>
<td id="S4.T6.15.15.15.1" class="ltx_td ltx_align_center">512<math id="S4.T6.15.15.15.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.15.15.15.1.m1.1a"><mo id="S4.T6.15.15.15.1.m1.1.1" xref="S4.T6.15.15.15.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.15.15.15.1.m1.1b"><times id="S4.T6.15.15.15.1.m1.1.1.cmml" xref="S4.T6.15.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.15.15.15.1.m1.1c">\times</annotation></semantics></math>512</td>
<td id="S4.T6.16.16.16.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.16.16.16.5" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.16.16.16.2" class="ltx_td ltx_align_center">
<math id="S4.T6.16.16.16.2.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.T6.16.16.16.2.m1.1a"><mi id="S4.T6.16.16.16.2.m1.1.1" xref="S4.T6.16.16.16.2.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.T6.16.16.16.2.m1.1b"><ci id="S4.T6.16.16.16.2.m1.1.1.cmml" xref="S4.T6.16.16.16.2.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.16.16.16.2.m1.1c">r</annotation></semantics></math>=15</td>
<td id="S4.T6.16.16.16.6" class="ltx_td ltx_align_center">906</td>
<td id="S4.T6.16.16.16.7" class="ltx_td ltx_align_center">3.26 it/s</td>
<td id="S4.T6.16.16.16.8" class="ltx_td ltx_align_center">72.52</td>
<td id="S4.T6.16.16.16.9" class="ltx_td ltx_align_center">78.60</td>
<td id="S4.T6.16.16.16.10" class="ltx_td ltx_align_center">80.04</td>
<td id="S4.T6.16.16.16.11" class="ltx_td ltx_align_center">16.96</td>
<td id="S4.T6.16.16.16.12" class="ltx_td ltx_align_center">88.01</td>
</tr>
<tr id="S4.T6.18.18.18" class="ltx_tr">
<td id="S4.T6.18.18.18.3" class="ltx_td ltx_align_center">7</td>
<td id="S4.T6.17.17.17.1" class="ltx_td ltx_align_center">512<math id="S4.T6.17.17.17.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.17.17.17.1.m1.1a"><mo id="S4.T6.17.17.17.1.m1.1.1" xref="S4.T6.17.17.17.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.17.17.17.1.m1.1b"><times id="S4.T6.17.17.17.1.m1.1.1.cmml" xref="S4.T6.17.17.17.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.17.17.17.1.m1.1c">\times</annotation></semantics></math>512</td>
<td id="S4.T6.18.18.18.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.18.18.18.5" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T6.18.18.18.2" class="ltx_td ltx_align_center">
<math id="S4.T6.18.18.18.2.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.T6.18.18.18.2.m1.1a"><mi id="S4.T6.18.18.18.2.m1.1.1" xref="S4.T6.18.18.18.2.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.T6.18.18.18.2.m1.1b"><ci id="S4.T6.18.18.18.2.m1.1.1.cmml" xref="S4.T6.18.18.18.2.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.18.18.18.2.m1.1c">r</annotation></semantics></math>=20</td>
<td id="S4.T6.18.18.18.6" class="ltx_td ltx_align_center">776</td>
<td id="S4.T6.18.18.18.7" class="ltx_td ltx_align_center">3.65 it/s</td>
<td id="S4.T6.18.18.18.8" class="ltx_td ltx_align_center">73.36</td>
<td id="S4.T6.18.18.18.9" class="ltx_td ltx_align_center">78.84</td>
<td id="S4.T6.18.18.18.10" class="ltx_td ltx_align_center">80.76</td>
<td id="S4.T6.18.18.18.11" class="ltx_td ltx_align_center">16.57</td>
<td id="S4.T6.18.18.18.12" class="ltx_td ltx_align_center">87.81</td>
</tr>
<tr id="S4.T6.20.20.20" class="ltx_tr">
<td id="S4.T6.20.20.20.3" class="ltx_td ltx_align_center ltx_border_t">8</td>
<td id="S4.T6.19.19.19.1" class="ltx_td ltx_align_center ltx_border_t">768<math id="S4.T6.19.19.19.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.19.19.19.1.m1.1a"><mo id="S4.T6.19.19.19.1.m1.1.1" xref="S4.T6.19.19.19.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.19.19.19.1.m1.1b"><times id="S4.T6.19.19.19.1.m1.1.1.cmml" xref="S4.T6.19.19.19.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.19.19.19.1.m1.1c">\times</annotation></semantics></math>768</td>
<td id="S4.T6.20.20.20.4" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T6.20.20.20.5" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T6.20.20.20.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T6.20.20.20.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.20.20.20.2.m1.1a"><mo id="S4.T6.20.20.20.2.m1.1.1" xref="S4.T6.20.20.20.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.20.20.20.2.m1.1b"><times id="S4.T6.20.20.20.2.m1.1.1.cmml" xref="S4.T6.20.20.20.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.20.20.20.2.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T6.20.20.20.6" class="ltx_td ltx_align_center ltx_border_t">2,916</td>
<td id="S4.T6.20.20.20.7" class="ltx_td ltx_align_center ltx_border_t">OOM</td>
<td id="S4.T6.20.20.20.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.20.20.20.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.20.20.20.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.20.20.20.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.20.20.20.12" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T6.22.22.22" class="ltx_tr">
<td id="S4.T6.22.22.22.3" class="ltx_td ltx_align_center ltx_border_bb">9</td>
<td id="S4.T6.21.21.21.1" class="ltx_td ltx_align_center ltx_border_bb">768<math id="S4.T6.21.21.21.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T6.21.21.21.1.m1.1a"><mo id="S4.T6.21.21.21.1.m1.1.1" xref="S4.T6.21.21.21.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T6.21.21.21.1.m1.1b"><times id="S4.T6.21.21.21.1.m1.1.1.cmml" xref="S4.T6.21.21.21.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.21.21.21.1.m1.1c">\times</annotation></semantics></math>768</td>
<td id="S4.T6.22.22.22.4" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S4.T6.22.22.22.5" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S4.T6.22.22.22.2" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S4.T6.22.22.22.2.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.T6.22.22.22.2.m1.1a"><mi id="S4.T6.22.22.22.2.m1.1.1" xref="S4.T6.22.22.22.2.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.T6.22.22.22.2.m1.1b"><ci id="S4.T6.22.22.22.2.m1.1.1.cmml" xref="S4.T6.22.22.22.2.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.22.22.22.2.m1.1c">r</annotation></semantics></math>=84</td>
<td id="S4.T6.22.22.22.6" class="ltx_td ltx_align_center ltx_border_bb">732</td>
<td id="S4.T6.22.22.22.7" class="ltx_td ltx_align_center ltx_border_bb">3.14 it/s</td>
<td id="S4.T6.22.22.22.8" class="ltx_td ltx_align_center ltx_border_bb">73.24</td>
<td id="S4.T6.22.22.22.9" class="ltx_td ltx_align_center ltx_border_bb">77.72</td>
<td id="S4.T6.22.22.22.10" class="ltx_td ltx_align_center ltx_border_bb">81.04</td>
<td id="S4.T6.22.22.22.11" class="ltx_td ltx_align_center ltx_border_bb">16.43</td>
<td id="S4.T6.22.22.22.12" class="ltx_td ltx_align_center ltx_border_bb">88.90</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2404.16635/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Visual token merging visualization. Top 10 groups with the most merged tokens are outlined in different colors.</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To verify the effectiveness of visual token merging and program-of-thoughts learning, we conduct ablation studies in Table <a href="#S4.T6" title="Table 6 ‣ 4.3. Main Results ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<span id="S4.SS4.p2.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS4.p2.2" class="ltx_p"><span id="S4.SS4.p2.2.1" class="ltx_text ltx_font_bold">Ablation on PoT learning.</span>The upper block in Table <a href="#S4.T6" title="Table 6 ‣ 4.3. Main Results ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the performance of the model with and without the use of Program-of-Thoughts training data. Comparing Row 2 with Row 1, we observe that training solely with template-based PoT improves the model’s ability to generate direct answers (71.12 vs. 70.72). This improvement is attributed to PoT learning enhances the model’s reasoning abilities. At this point, the PoT answers produced by the model are less accurate than direct answers (55.44 vs. 71.12), which may be due to the inability of template-based PoT to cover all questions. However, when we ask the model to generate PoT answers for questions that require calculation and combine with direct answers, it outperforms solely direct answers (73.00 vs. 71.12). This indicates that PoT answers have advantages in computational problems. After incorporating GPT-based PoT into training, the performance of PoT answering surpasses direct answering (76.88 vs. 72.44), and both direct (72.44 vs. 71.12) and combined answering (79.48 vs. 73.00) show further improvements. These results confirm the effectiveness of our proposed Program-of-Thoughts learning method, suggesting that it not only strengthens the model’s computational capabilities but also enhances overall problem-solving capability.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<span id="S4.SS4.p3.4" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS4.p3.3" class="ltx_p"><span id="S4.SS4.p3.3.1" class="ltx_text ltx_font_bold">Ablation on Visual Token Merging.</span>
The middle block in Table <a href="#S4.T6" title="Table 6 ‣ 4.3. Main Results ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> compares the performance with and without using visual token merging when the input resolution is 512<math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mo id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><times id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\times</annotation></semantics></math>512, and with different numbers of tokens to merge in each layer. Comparing Row 4 and Row 3, increasing the input resolution from 384 to 512 significantly improves the model’s performance on three chart understanding benchmarks, demonstrating that high resolution is crucial for comprehending chart images. However, a direct increase in resolution leads to a substantial drop in the inference throughput (2.38 it/s vs. 3.73 it/s). The reason is that, given high-resolution images, the standard vision transformer produces a lengthy visual feature sequence that is then processed by the large language model. This brings considerable computational expenses. By adopting the visual token merging, we can control the length of the visual feature sequence by regulating the number of tokens to merge at each layer, and, thereby achieving efficient high-resolution encoding. When setting <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><mi id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><ci id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">r</annotation></semantics></math>=20, we attain an inference throughput nearly equal to that with an input resolution of 384<math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><mo id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><times id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">\times</annotation></semantics></math>384 (3.65 it/s vs. 3.73 it/s), while providing the performance benefits of higher resolutions.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<span id="S4.SS4.p4.2" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">Extending to higher resolution.</span>
To further highlight the advantages of visual token merging, we increase the input resolution to 768 in the bottom block of Table <a href="#S4.T6" title="Table 6 ‣ 4.3. Main Results ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. At this point, the length of the visual feature sequence is 2,916, which could not be trained using 32GB V100 due to insufficient VRAM. However, after employing the visual token merging module with <math id="S4.SS4.p4.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.SS4.p4.1.m1.1a"><mi id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><ci id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">r</annotation></semantics></math>=84, the input sequence length is reduced to 732 and we can perform training normally. In this setting, the model’s inference throughput is 3.14 it/s, and demonstrates a certain performance advantage in ChartQA (81.04 vs. 80.76) and Chart-to-Table (88.90 vs. 87.81). It illustrates that by utilizing visual token merging, we are able to leverage higher-resolution chart images under constrained resources, thereby improving performance.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2404.16635/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_img_landscape" width="450" height="314" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Case studies on ChartQA. We compare TinyChart@768 with ChartLlama.</span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2404.16635/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_img_landscape" width="450" height="205" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S4.F7.4.2" class="ltx_text" style="font-size:90%;">Examples of chart-to-table extraction of TinyChart@768. The wrong values produced by the model are marked <span id="S4.F7.4.2.1" class="ltx_text" style="color:#AF2519;">red</span>.</span></figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2404.16635/assets/x8.png" id="S4.F8.g1" class="ltx_graphics ltx_img_landscape" width="461" height="198" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.4.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span id="S4.F8.5.2" class="ltx_text" style="font-size:90%;">Cases of chart-to-text generation by TinyChart@768. Correct contents are shown in <span id="S4.F8.5.2.1" class="ltx_text" style="color:#5F8118;">green</span> and wrong contents are marked <span id="S4.F8.5.2.2" class="ltx_text" style="color:#AF2519;">red</span>.</span></figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2404.16635/assets/x9.png" id="S4.F9.g1" class="ltx_graphics ltx_img_landscape" width="461" height="210" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>. </span><span id="S4.F9.4.2" class="ltx_text" style="font-size:90%;">Examples of chart redrawing. We present the resulting image after executing the Python code produced by the model. The bad case is with the <span id="S4.F9.4.2.1" class="ltx_text" style="color:#AF2519;">red</span> bounding box.</span></figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Visualization</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">To investigate the effects of visual token merging, we visualized the token merging results at the final layer of the vision transformer. In Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3. Main Results ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we visualize the top ten groups with the largest numbers of tokens. Each group is outlined with a different color. The visualization reveals that these largest groups typically correspond to blank or colored areas. By compressing these areas down to a single token for encoding, our visual token merging module can thus reduce the length of the encoded sequence without losing much information, thereby achieving efficient visual encoding.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Case study</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">We conduct case studies with TinyChart when conducting chart question answering, chart-to-table, chart-to-text, and chart redrawing in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#S4.F7" title="Figure 7 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, <a href="#S4.F8" title="Figure 8 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, and <a href="#S4.F9" title="Figure 9 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<span id="S4.SS6.p2.1" class="ltx_ERROR undefined">\noindentparagraph</span>
<p id="S4.SS6.p2.2" class="ltx_p"><span id="S4.SS6.p2.2.1" class="ltx_text ltx_font_bold">Chart Question Answering.</span> In Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we present a case study on ChartQA. As shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (a-c), much key information within the chart is provided by visually situated texts within the image, which requires the model to have the ability to process high-resolution images. Since ChartLlama only supports 336 resolutions, it struggles to retrieve accurate information in these charts. In contrast, thanks to the visual token merging, our TinyChart can accept higher-resolution inputs without introducing excessive computations. Thus it can successfully find clues related to the questions. Meanwhile, ChartLlama suffers from numerical errors when faced with calculative questions in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (d-e), and our PoT (Program-of-Thoughts) learning method can accurately solve these problems. These examples further illustrate the advantages of our methods.
<span id="S4.SS6.p2.2.2" class="ltx_ERROR undefined">\noindentparagraph</span><span id="S4.SS6.p2.2.3" class="ltx_text ltx_font_bold">Chart-to-Table.</span>
For chart-to-table extraction, we find that our TinyChart model can successfully extractive values from several visually diverse charts in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> (a-c), thanks to its excellent text recognition ability with high-resolution input. However, as shown in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> (d), the model struggles to estimate the values of data points in the absence of OCR words. It seems that the model could make reasonable predictions based on surrounding points, but hardly provide accurate values based on the coordinate axis. This indicates that the model still lacks the ability to understand spatial relationships across large areas.
<span id="S4.SS6.p2.2.4" class="ltx_ERROR undefined">\noindentparagraph</span><span id="S4.SS6.p2.2.5" class="ltx_text ltx_font_bold">Chart-to-Text.</span>
From Figure <a href="#S4.F8" title="Figure 8 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we observe that the model can understand the data presented in the chart and generate descriptions and summaries in natural language. Though it can retrieve the data values correctly, we find it sometimes produces contents that do match the chart as shown in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (c-d). This may be due to the inherent limitations of hallucination in MLLMs <cite class="ltx_cite ltx_citemacro_citep">(Rohrbach et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2018</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2023b</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib50" title="" class="ltx_ref">a</a>)</cite>, and may be alleviated by addressing hallucinations <cite class="ltx_cite ltx_citemacro_citep">(Leng et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2023</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2024b</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2024</a>; Yue et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>.
<span id="S4.SS6.p2.2.6" class="ltx_ERROR undefined">\noindentparagraph</span><span id="S4.SS6.p2.2.7" class="ltx_text ltx_font_bold">Chart redrawing.</span>
We present four cases of chart redrawing in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. As shown in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (a-c), our TinyChart model can generate Python code to redraw visually diverse chart types including lines, heatmaps, and rings. However, it can be hard to draw unseen chart types such as 3D bar charts (Figure <a href="#S4.F9" title="Figure 9 ‣ 4.4. Ablation Studies ‣ 4. Experiment ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (d)). This may be mitigated by improving the coverage of different chart types in training data through automatic data construction techniques <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Xia et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper introduces TinyChart, a chart understanding Multimodal Large Language Model with 3 billion parameters. To address the inefficiency of lengthy visual token sequences with high-resolution images, TinyChart injects a visual token merging module that merges similar vision tokens together, thereby enabling efficient encoding of high-resolution images. To tackle the challenges of learning numerical computations, we propose a Program-of-Thoughts learning method that trains the model to generate Python programs to answer questions. Our TinyChart model achieves state-of-the-art (SOTA) performance on multiple chart understanding benchmarks, surpassing existing 13 billion parameter chart MLLMs, and outperforms closed-source models like GPT-4V on ChartQA. Extensive ablation studies confirm the effectiveness of our methods. Our code and model are released at <a target="_blank" href="https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart" title="" class="ltx_ref ltx_href">https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart</a>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.12966 [cs.CV]

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolya et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. 2023.

</span>
<span class="ltx_bibblock">Token Merging: Your ViT But Faster. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openreview.net/forum?id=JroZRaRw7Eu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=JroZRaRw7Eu</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Jinyue Chen, Lingyu Kong, Haoran Wei, Chenglong Liu, Zheng Ge, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. 2024.

</span>
<span class="ltx_bibblock">OneChart: Purify the Chart Structural Extraction via One Auxiliary Token.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.09987 [cs.CV]

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2023.

</span>
<span class="ltx_bibblock">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al<span id="bib.bib6.3.1" class="ltx_text">.</span> 2024a.

</span>
<span class="ltx_bibblock">InternLM-XComposer2: Mastering free-form text-image composition and comprehension in vision-language large model.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.16420</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al<span id="bib.bib7.3.1" class="ltx_text">.</span> 2024b.

</span>
<span class="ltx_bibblock">InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.06512</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020.

</span>
<span class="ltx_bibblock">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2010.11929 (2020).

</span>
<span class="ltx_bibblock">arXiv:2010.11929

<a target="_blank" href="https://arxiv.org/abs/2010.11929" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2010.11929</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang Li, and Can Huang. 2023.

</span>
<span class="ltx_bibblock">DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.11810</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiayun Fu, Bin B Zhu, Haidong Zhang, Yayi Zou, Song Ge, Weiwei Cui, Yun Wang, Dongmei Zhang, Xiaojing Ma, and Hai Jin. 2022.

</span>
<span class="ltx_bibblock">Chartstamp: Robust chart embedding for real-world applications. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</em>. 2786–2795.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023.

</span>
<span class="ltx_bibblock">Chartllama: A multimodal llm for chart understanding and generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16483</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks and Gimpel (2016)</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel. 2016.

</span>
<span class="ltx_bibblock">Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1606.08415 (2016).

</span>
<span class="ltx_bibblock">arXiv:1606.08415

<a target="_blank" href="http://arxiv.org/abs/1606.08415" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1606.08415</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al<span id="bib.bib13.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Cogagent: A visual language model for gui agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.08914</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Anwen Hu, Shizhe Chen, and Qin Jin. 2021.

</span>
<span class="ltx_bibblock">Question-controlled Text-aware Image Captioning. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on Multimedia</em> (Virtual Event, China) <em id="bib.bib14.4.2" class="ltx_emph ltx_font_italic">(MM ’21)</em>. Association for Computing Machinery, New York, NY, USA, 3097–3105.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3474085.3475452" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3474085.3475452</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang. 2024a.

</span>
<span class="ltx_bibblock">mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.18248 [cs.MM]

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. 2024b.

</span>
<span class="ltx_bibblock">mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.12895 [cs.CV]

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-Fu Chang, and Heng Ji. 2024a.

</span>
<span class="ltx_bibblock">From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.12027 [cs.CL]

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2024b.

</span>
<span class="ltx_bibblock">OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.17911 [cs.CV]

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. 2024.

</span>
<span class="ltx_bibblock">Hallucination Augmented Contrastive Learning for Multimodal Large Language Model.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.06968 [cs.CV]

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018.

</span>
<span class="ltx_bibblock">Dvqa: Understanding data visualizations via question answering. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 5648–5656.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kantharaj et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Leong, Jia Qing Tan, Enamul Hoque, and Shafiq Joty. 2022a.

</span>
<span class="ltx_bibblock">OpenCQA: Open-ended Question Answering with Charts. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 11817–11837.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.811" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2022.emnlp-main.811</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kantharaj et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. 2022b.

</span>
<span class="ltx_bibblock">Chart-to-Text: A Large-Scale Benchmark for Chart Summarization. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 4005–4023.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.277" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2022.acl-long.277</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leng et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.

</span>
<span class="ltx_bibblock">Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.16922 [cs.CV]

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023a.

</span>
<span class="ltx_bibblock">Textbooks Are All You Need II: phi-1.5 technical report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.05463 [cs.CL]

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023b.

</span>
<span class="ltx_bibblock">Evaluating Object Hallucination in Large Vision-Language Models. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 292–305.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. 2023.

</span>
<span class="ltx_bibblock">SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.07575 [cs.CV]

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Fangyu Liu, Julian Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. 2023a.

</span>
<span class="ltx_bibblock">DePlot: One-shot visual language reasoning by plot-to-table translation. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 10381–10399.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.660" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2023.findings-acl.660</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Eisenschlos. 2023c.

</span>
<span class="ltx_bibblock">MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering. In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 12756–12770.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.714" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2023.acl-long.714</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2023d)</span>
<span class="ltx_bibblock">
Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. 2023d.

</span>
<span class="ltx_bibblock">Mmc: Advancing multimodal chart understanding with large-scale instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.10774</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023b.

</span>
<span class="ltx_bibblock">Improved Baselines with Visual Instruction Tuning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.03744 [cs.CV]

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024a.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. 2024b.

</span>
<span class="ltx_bibblock">On the Hidden Mystery of OCR in Large Multimodal Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.07895 [cs.CV]

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022.

</span>
<span class="ltx_bibblock">ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2022</em>. 2263–2279.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. 2023.

</span>
<span class="ltx_bibblock">UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.14761 [cs.CL]

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. 2024.

</span>
<span class="ltx_bibblock">ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.09028 [cs.CL]

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024.

</span>
<span class="ltx_bibblock">ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.02384</em> (2024).

</span>
<span class="ltx_bibblock">arXiv:2401.02384

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Methani et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and Pratyush Kumar. 2020.

</span>
<span class="ltx_bibblock">Plotqa: Reasoning over scientific plots. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>. 1527–1536.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Obeid and Hoque (2020)</span>
<span class="ltx_bibblock">
Jason Obeid and Enamul Hoque. 2020.

</span>
<span class="ltx_bibblock">Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2010.09142 (2020).

</span>
<span class="ltx_bibblock">arXiv:2010.09142

<a target="_blank" href="https://arxiv.org/abs/2010.09142" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2010.09142</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI. 2023a.

</span>
<span class="ltx_bibblock">GPT-3.5-Turbo.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://platform.openai.com/docs/models/gpt-3-5-turbo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://platform.openai.com/docs/models/gpt-3-5-turbo</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI. 2023b.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al<span id="bib.bib41.3.1" class="ltx_text">.</span> 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.4.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 35 (2022), 27730–27744.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation. In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>. 311–318.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahman et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Raian Rahman, Rizvi Hasan, Abdullah Al Farhad, Md. Tahmid Rahman Laskar, Md. Hamjajul Ashmafee, and Abu Raihan Mostofa Kamal. 2023.

</span>
<span class="ltx_bibblock">ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Canadian Conference on Artificial Intelligence</em> (jun 5 2023).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">https://caiac.pubpub.org/pub/ujhjycsw.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohrbach et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.

</span>
<span class="ltx_bibblock">Object Hallucination in Image Captioning. In <em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>. 4035–4045.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019.

</span>
<span class="ltx_bibblock">Towards VQA Models That Can Read. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Benny Tang, Angie Boggust, and Arvind Satyanarayan. 2023.

</span>
<span class="ltx_bibblock">VisText: A Benchmark for Semantically Rich Chart Captioning. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 7268–7298.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.401" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2023.acl-long.401</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al<span id="bib.bib47.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Walt et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Stefan van der Walt, S. Chris Colbert, and Gael Varoquaux. 2011.

</span>
<span class="ltx_bibblock">The NumPy Array: A Structure for Efficient Numerical Computation.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Computing in Science &amp; Engineering</em> 13, 2 (2011), 22–30.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/MCSE.2011.37" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/MCSE.2011.37</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. 2023a.

</span>
<span class="ltx_bibblock">An llm-free multi-dimensional benchmark for mllms hallucination evaluation.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07397</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, and Haoyu Tang. 2023b.

</span>
<span class="ltx_bibblock">Evaluation and Analysis of Hallucination in Large Vision-Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.15126 [cs.LG]

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, and Yu Qiao. 2024.

</span>
<span class="ltx_bibblock">ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.12185 [cs.CV]

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, and Fei Huang. 2023a.

</span>
<span class="ltx_bibblock">UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model. In <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">EMNLP (Findings)</em>. Association for Computational Linguistics, 2841–2858.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2024.

</span>
<span class="ltx_bibblock">mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.14178 [cs.CL]

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Ming Yan, Chenlin Zhao, Junyang Wang, Xiaoshan Yang, Ji Zhang, Fei Huang, Jitao Sang, and Changsheng Xu. 2023b.

</span>
<span class="ltx_bibblock">mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM. In <em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>. 9365–9367.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023c.

</span>
<span class="ltx_bibblock">mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.04257 [cs.CL]

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Zihao Yue, Liang Zhang, and Qin Jin. 2024.

</span>
<span class="ltx_bibblock">Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.14545</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023.

</span>
<span class="ltx_bibblock">Sigmoid loss for language image pre-training. In <em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 11975–11986.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Liang Zhang, Anwen Hu, Jing Zhang, Shuo Hu, and Qin Jin. 2023a.

</span>
<span class="ltx_bibblock">MPMQA: multimodal question answering on product manuals. In <em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 37. 13958–13966.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al<span id="bib.bib60.3.1" class="ltx_text">.</span> 2023b.

</span>
<span class="ltx_bibblock">Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15112</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, and Fei Wu. 2020.

</span>
<span class="ltx_bibblock">TRIE: end-to-end text reading and information extraction for document understanding. In <em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on Multimedia</em>. 1413–1422.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. 2024.

</span>
<span class="ltx_bibblock">TinyLLaVA: A Framework of Small-scale Large Multimodal Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.14289 [cs.LG]

</span>
</li>
</ul>
</section>
<figure id="S5.F10" class="ltx_figure">
<div id="S5.F10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:440.3pt;height:453.7pt;vertical-align:-13.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><svg id="S5.F10.1.pic1" class="ltx_picture" height="609.79" overflow="visible" version="1.1" width="600"><g transform="translate(0,609.79) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 603.89 C 0 607.15 2.64 609.79 5.91 609.79 L 594.09 609.79 C 597.36 609.79 600 607.15 600 603.89 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F9F9F9" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 585.68 L 598.03 585.68 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 591.59)"><foreignObject width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S5.F10.1.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S5.F10.1.pic1.1.1.1.1.1.1" class="ltx_p">Instructions to <span id="S5.F10.1.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span></span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="560.09" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S5.F10.1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S5.F10.1.pic1.2.2.2.1.1.1" class="ltx_p">Please generate a list of <span id="S5.F10.1.pic1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">assignment statements</span> in Python to answer the question of a chart. You can only use the following operators in each statement: <span id="S5.F10.1.pic1.2.2.2.1.1.1.2" class="ltx_text ltx_font_typewriter">&lt;function_list&gt;<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_serif">2</span></span><span id="footnote2.5" class="ltx_text ltx_font_serif">function_list=[’len’, ’all’, ’any’, ’index’, ’np.sort’, ’np.abs’, ’np.add’, ’np.argmax’, ’np.argmin’, ’np.diff’, ’np.divide’,’np.greater’, ’np.greater_equal’, ’np.less’, ’np.max’, ’np.mean’, ’np.median’, ’np.min’, ’np.subtract’, ’np.sum’, ’np.count_nonzero’, ’np.where’, ’+’, ’-’, ’*’, ’/’, ’¿’, ’¡’, ’=’]</span></span></span></span></span>. Do not use any circulation or if-branch. Do not include any unnecessary statement that is not used. The chart is presented by a data table with color information. Note that the colors are estimated and may not match the description in the question. You can choose the most possible data if necessary. You must provide a one-line comment before each assignment statement. The last variable must be Answer.
Here are some examples:</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.2" class="ltx_p"><span id="S5.F10.1.pic1.2.2.2.1.1.2.1" class="ltx_text ltx_font_bold">Example Input #1:
<br class="ltx_break"></span>Chart title: Long-term price index in food commodities, 1850-2015, World, 1934</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.3" class="ltx_p">Chart type: Horizontal bar chart</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.4" class="ltx_p">Chart table:</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.5" class="ltx_p">— Food — Long-term price index in food commodities, 1850-2015, World, 1934 —</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.6" class="ltx_p">—:——————————————————:—</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.7" class="ltx_p">— Lamb (color: steelblue) — 103.7 —</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.8" class="ltx_p">— Corn (color: sienna) — 103.13 —</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.9" class="ltx_p">— Barley (color: mediumvioletred) — 102.46 —</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.10" class="ltx_p">— Rye (color: tomato) — 87.37 —</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.11" class="ltx_p">— Beef (color: sienna) — 85.27 —</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.12" class="ltx_p">— Wheat (color: slategray) — 83.73 —</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.13" class="ltx_p">Question: What is the sum of the price index that is greater than 100?</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.14" class="ltx_p">Answer: 309.29</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.15" class="ltx_p"><span id="S5.F10.1.pic1.2.2.2.1.1.15.1" class="ltx_text ltx_font_bold">Example Output #1:
<br class="ltx_break"></span># Get the values of all ’Long-term price index of each food’, set to Y</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.16" class="ltx_p">Y=[103.7, 103.13, 102.46, 87.37, 85.27, 83.73]</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.17" class="ltx_p"># Check whether Y is greater than 100, set to Greater</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.18" class="ltx_p">Greater=np.greater(Y,100)</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.19" class="ltx_p"># Find the indices where Greater is True, set to Indices</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.20" class="ltx_p">Indices=np.where(Greater)[0]</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.21" class="ltx_p"># Get the values at position Indices, set to Y</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.22" class="ltx_p">Y=np.array(Y)[Indices]</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.23" class="ltx_p"># Calculate the sum of all elements in Y, set to Answer</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.24" class="ltx_p">Answer=np.sum(Y)</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.25" class="ltx_p"><span id="S5.F10.1.pic1.2.2.2.1.1.25.1" class="ltx_text ltx_font_bold">Input:</span>
<span id="S5.F10.1.pic1.2.2.2.1.1.25.2" class="ltx_text ltx_font_typewriter">&lt;target_input&gt;
<br class="ltx_break"></span><span id="S5.F10.1.pic1.2.2.2.1.1.25.3" class="ltx_text ltx_font_bold">Output:</span></span>
</span></foreignObject></g></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>. </span><span id="S5.F10.4.2" class="ltx_text" style="font-size:90%;">Instructions used for generating GPT-based PoT.</span></figcaption>
</figure>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>ChartQA-PoT Details</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Dataset Statistic</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">We build ChartQA-PoT based on the images and questions in the training split of ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>. ChartQA-PoT consists of two subsets: Template-based PoT and GPT-based PoT. We present the statistics over ChartQA-PoT in Table <a href="#A1.T7" title="Table 7 ‣ A.1. Dataset Statistic ‣ Appendix A ChartQA-PoT Details ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We find that answers provided by <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span> are longer than template-based PoT, since they cover more diverse scenarios.</p>
</div>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>. </span><span id="A1.T7.3.2" class="ltx_text" style="font-size:90%;">Statistic over ChartQA-PoT</span></figcaption>
<table id="A1.T7.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T7.4.1.1" class="ltx_tr">
<th id="A1.T7.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Statistic</th>
<th id="A1.T7.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T7.4.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.2.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Template</td>
</tr>
<tr id="A1.T7.4.1.1.2.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">PoT</td>
</tr>
</table>
</th>
<th id="A1.T7.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T7.4.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.3.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">GPT</td>
</tr>
<tr id="A1.T7.4.1.1.3.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">PoT</td>
</tr>
</table>
</th>
<th id="A1.T7.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T7.4.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.4.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">ChartQA</td>
</tr>
<tr id="A1.T7.4.1.1.4.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">PoT</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T7.4.2.1" class="ltx_tr">
<th id="A1.T7.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Num. of samples</th>
<td id="A1.T7.4.2.1.2" class="ltx_td ltx_align_right ltx_border_t">119,281</td>
<td id="A1.T7.4.2.1.3" class="ltx_td ltx_align_right ltx_border_t">21,303</td>
<td id="A1.T7.4.2.1.4" class="ltx_td ltx_align_right ltx_border_t">140,584</td>
</tr>
<tr id="A1.T7.4.3.2" class="ltx_tr">
<th id="A1.T7.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Num. of images</th>
<td id="A1.T7.4.3.2.2" class="ltx_td ltx_align_right">17,498</td>
<td id="A1.T7.4.3.2.3" class="ltx_td ltx_align_right">15,521</td>
<td id="A1.T7.4.3.2.4" class="ltx_td ltx_align_right">18,133</td>
</tr>
<tr id="A1.T7.4.4.3" class="ltx_tr">
<th id="A1.T7.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Avg. answer characters</th>
<td id="A1.T7.4.4.3.2" class="ltx_td ltx_align_right">319.38</td>
<td id="A1.T7.4.4.3.3" class="ltx_td ltx_align_right">381.23</td>
<td id="A1.T7.4.4.3.4" class="ltx_td ltx_align_right">328.75</td>
</tr>
<tr id="A1.T7.4.5.4" class="ltx_tr">
<th id="A1.T7.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Avg. answer tokens</th>
<td id="A1.T7.4.5.4.2" class="ltx_td ltx_align_right ltx_border_bb">117.70</td>
<td id="A1.T7.4.5.4.3" class="ltx_td ltx_align_right ltx_border_bb">136.01</td>
<td id="A1.T7.4.5.4.4" class="ltx_td ltx_align_right ltx_border_bb">120.48</td>
</tr>
</tbody>
</table>
</figure>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">We further present the first 2-gram words of the questions after removing stop words in Template-based PoT and GPT-based PoT in Figure <a href="#A1.F11" title="Figure 11 ‣ A.1. Dataset Statistic ‣ Appendix A ChartQA-PoT Details ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. It is observed that GPT-PoT covers more diverse questions for ‘what’ type questions, and questions in Template-based PoT are more evenly distributed across all question types.</p>
</div>
<figure id="A1.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.16635/assets/x10.png" id="A1.F11.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.sf1.2.1.1" class="ltx_text" style="font-size:90%;">((a))</span> </span><span id="A1.F11.sf1.3.2" class="ltx_text" style="font-size:90%;">Template PoT.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.16635/assets/x11.png" id="A1.F11.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="462" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.sf2.2.1.1" class="ltx_text" style="font-size:90%;">((b))</span> </span><span id="A1.F11.sf2.3.2" class="ltx_text" style="font-size:90%;">GPT PoT.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>. </span><span id="A1.F11.3.2" class="ltx_text" style="font-size:90%;">First 2-gram of the questions in ChartQA-PoT after removing stop words.</span></figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Instructions for GPT-based PoT</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">Figure <a href="#S5.F10" title="Figure 10 ‣ TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the instructions for constructing GPT-based PoT answers. Note that we prompt <span id="A1.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span> to provide Python code consisting of assignment statements and avoid using loops or judgment statements.
This can simplify the program and reduce syntax errors. We also provide meta information including the chart title, type, and colors to <span id="A1.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span> since some questions rely on this information to answer.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.16634" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.16635" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.16635">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.16635" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.16636" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 22:06:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
