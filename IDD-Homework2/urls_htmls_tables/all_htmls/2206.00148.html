<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2206.00148] Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection</title><meta property="og:description" content="Over the past few years there has been major progress in the field of synthetic data generation using simulation based techniques. These methods use high-end graphics engines and physics-based ray-tracing rendering in …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2206.00148">

<!--Generated on Mon Mar 11 16:34:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul Yudkin
<br class="ltx_break">Datagen
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">paul.yudkin@datagen.tech</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eli Friedman
<br class="ltx_break">Datagen
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">eli.friedman@datagen.tech</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Orly Zvitia
<br class="ltx_break">Datagen
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">orly.zvitia@datagen.tech</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gil Elbaz
<br class="ltx_break">Datagen
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">gil@datagen.tech</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Over the past few years there has been major progress in the field of synthetic data generation using simulation based techniques. These methods use high-end graphics engines and physics-based ray-tracing rendering in order to represent the world in 3D and create highly realistic images. Datagen has specialized in the generation of high-quality 3D humans, realistic 3D environments and generation of realistic human motion. This technology has been developed into a data generation platform which we used for these experiments.
This work demonstrates the use of synthetic photo-realistic in-cabin data to train a Driver Monitoring System that uses a lightweight neural network to detect whether the driver’s hands are on the wheel. We demonstrate that when only a small amount of real data is available, synthetic data can be a simple way to boost performance. Moreover, we adopt the data-centric approach and show how performing error analysis and generating the missing edge-cases in our platform boosts performance. This showcases the ability of human-centric synthetic data to generalize well to the real world, and help train algorithms in computer vision settings where data from the target domain is scarce or hard to collect.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Currently, most vehicles on the road are driven by humans. People are prone to distractions while driving, which is the cause of 15% of the injury-causing accidents in the US <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. In the next few years the European regulations will require car manufacturers to gradually include new safety technologies, such as Driver Monitoring Systems (DMS) in vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
In addition, the European NCAP has started requiring driver monitoring features in order to qualify for a 5-star safety rating <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, raising the urgency of development of driver monitoring systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we develop a driver monitoring system to detect when a driver’s hands leave the wheel. Such a system could be useful in multiple DMS tasks, such as to raise an alert when the driver is distracted.
As with many deep learning projects, the challenge here is data. It is difficult to collect many images of drivers in a vehicle, and while some datasets do exist, they are limited in the number of drivers, vehicle types, behaviors, and camera models that they use. In addition, tagging tens of hours of videos in a consistent way is challenging. Synthetic data provides an alternative solution that, once developed, can be used to create significant variance with little manual effort.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We developed a synthetic data platform that renders highly realistic scenes of drivers in cars. Our synthetic data platform allows varying the camera position and type, scene lighting, and driver behavior (e.g., falling asleep, looking around, drinking, texting etc.,). It includes pixel perfect ground truth and 3D annotations so that no manual tagging is required.
Our two main contributions in this work are: 1) We demonstrate how using synthetic data along with a very small amount of real examples can boost performance relative to using the same amount of only-real data. 2) We show in practice a complete iteration of a data-centric approach using our platform to generate a specific edge case that we were lacking in the training dataset.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Increasingly, deep learning systems are being used to implement DMS systems. Kose et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> use a convolutional network to classify distracted driving, and Rangesh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> build a predictor to segment and localize the driver’s hands. The most important part of any AI based DMS system is collecting data. The Drive &amp; Act dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> contains 15 subjects, but the annotated behaviors focus on tasks a driver might perform in an autonomous vehicle instead of things a driver would do while driving.
The DMD dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is a comprehensive dataset, containing 37 drivers and 42 hours worth of video data. It contains videos of both real driving scenarios as well as driving in simulators, and includes annotated driving behaviors. It is, to our knowledge, the most comprehensive public dataset for research on DMS systems, which is why we chose it as our comparison dataset.
Synthetic data is becoming more prelavent as the need for data grows. Tremblay et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> train an object detection network using synthetic data and then finetune on real data. Sengupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> use a synthetic model to help regress the pose of a human, which could be used as part of a pipeline to first detect the pose of the driver’s hands and then whether the hands are on the wheel.
However we opt for a single stage pipeline that directly predicts whether the hands are on the wheel.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Real Data Preparation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We chose the DMD dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> as our real dataset because it contains a large number of drivers, driving scenarios, camera angles, and has a wide variety of tagged behaviors, including whether the hands are on the wheel.
We split the dataset into a train, validation, and test sets based on the identity of the drivers in the dataset. In total, the dataset contains 651k frames, of which we use 531k for training, 47k for validation, and the rest for test.
The drivers are recorded using three cameras–one facing the driver’s head, one facing the driver’s body, and one facing the driver’s hands. We use the camera facing the driver’s body because a side view of the wheel offers a clearer perspective whether the hands are on the wheel.
The dataset is not balanced between the hands. Drivers in countries that drive on the left side of the road will typically perform other actions with their right hand while the left hand remains on the wheel. This bias can be seen in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Real Data Preparation ‣ 3 Method ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_ll ltx_border_t" style="padding-bottom:2.15277pt;">Left</th>
<th id="S3.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">Right Hand</th>
<th id="S3.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">Synthetic</th>
<th id="S3.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Real</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.2.1" class="ltx_tr">
<td id="S3.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_tt">On</td>
<td id="S3.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">On</td>
<td id="S3.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5642 (50%)</td>
<td id="S3.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">214192 (32.8%)</td>
</tr>
<tr id="S3.T1.2.3.2" class="ltx_tr">
<td id="S3.T1.2.3.2.1" class="ltx_td ltx_align_center ltx_border_ll">On</td>
<td id="S3.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r">Off</td>
<td id="S3.T1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r">3546 (31.4%)</td>
<td id="S3.T1.2.3.2.4" class="ltx_td ltx_align_center ltx_border_rr">304102 (46.7%)</td>
</tr>
<tr id="S3.T1.2.4.3" class="ltx_tr">
<td id="S3.T1.2.4.3.1" class="ltx_td ltx_align_center ltx_border_ll">Off</td>
<td id="S3.T1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r">On</td>
<td id="S3.T1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_r">2014 (17.8%)</td>
<td id="S3.T1.2.4.3.4" class="ltx_td ltx_align_center ltx_border_rr">122416 (18.8%)</td>
</tr>
<tr id="S3.T1.2.5.4" class="ltx_tr">
<td id="S3.T1.2.5.4.1" class="ltx_td ltx_align_center ltx_border_ll">Off</td>
<td id="S3.T1.2.5.4.2" class="ltx_td ltx_align_center ltx_border_r">Off</td>
<td id="S3.T1.2.5.4.3" class="ltx_td ltx_align_center ltx_border_r">82 (0.7%)</td>
<td id="S3.T1.2.5.4.4" class="ltx_td ltx_align_center ltx_border_rr">10579 (1.6%)</td>
</tr>
<tr id="S3.T1.2.6.5" class="ltx_tr">
<td id="S3.T1.2.6.5.1" class="ltx_td ltx_border_bb ltx_border_ll ltx_border_t"></td>
<td id="S3.T1.2.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Total</td>
<td id="S3.T1.2.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">11,284</td>
<td id="S3.T1.2.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t">651,289</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.4.2" class="ltx_text" style="font-size:90%;"> Label distribution in real and synthetic datasets.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Synthetic Data Preparation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We use the Datagen synthetic data platform to generate a diverse video dataset composed of different drivers who perform various actions in different vehicles. Among multiple camera views available, we render the scene using a camera focused on the driver’s body, a similar viewpoint as the real data. Each scene is 10 seconds long and is rendered at 15 frames per second. Each image resolution is 256x256 and includes hand and body keypoints, and wheel keypoints. See Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 Synthetic Data Preparation ‣ 3 Method ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for some RGB examples from the synthetic dataset.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2206.00148/assets/imgs/synthetic_data_grid.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Sample images from our synthetic dataset</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To maximize variance in our dataset we generated diverse sequences with respect to the following aspects:
1) Environment - Our dataset includes various car types including large and medium SUVs and sedan type vehicles. The interior areas in the car differ to allow variance including seat types, wall colors and, especially important for our task, different wheel types. 2) Demographics - We used ten different drivers with different ethnicity, age and genders. 3) Behaviors - We generate multiple behaviors such as falling asleep, turning around, texting, one handed driving, and regular two handed driving. 4) Scene - We generate all sequences with a random background and lighting condition–daylight, evening light, or night. In total we generate 146 sequences.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For each frame we separately label each hand as being on or off the steering wheel. The availability of 3D key points from our platform makes the hands-on-wheel labeling almost a trivial task. We simply calculate the distance from the wheel to the closest point on each hand and consider the hand to be on the wheel if it is closer than 3 cm.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic Data Splits</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to balance the distribution of labels in our dataset, we undersampled the synthetic dataset and removed labels with both hands on wheel. In total, the synthetic dataset contains 11,284 unique images. We split our train, validation, and test sets based on the driver identity. Our training set contains 8,834 images. The validation set consists of 2,450 images following the same proportions as the train split.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2206.00148/assets/imgs/cropped.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="91" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Examples from the real and synthetic datasets after cropping around the wheel</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Pre-processing</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We wanted to eliminate background distractions from the model. Therefore, we manually crop both the real images and synthetic images around the wheel, so that only the wheel and hands are visible without any extraneous details. See Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 Synthetic Data Splits ‣ 3 Method ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for some examples from the real and synthetic datasets.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Model Architecture</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">We choose the lightweight MobileNetV3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> architecture as backbone for all our experiments considering the real-time nature of our task. We replaced the classification head with two binary classification heads each containing two fully connected layers activated with ReLU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and a final fully connected layer with a sigmoid activation. The two classification heads predict, respectively, whether the left or right hand is on the wheel.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We conduct two types of experiments to demonstrate the added value of easily reachable synthetic data.
1) We compare a model trained solely on the DMD real data with multiple models trained on synthetic data and fine-tuned on varying amounts of real data mixed with synthetic data. We assume tagging a small amount of real data is feasible in most cases and preferable over tagging hundreds of hours.
2) We show that one can boost performance by applying a data-centric iteration–searching the test errors for edge cases that are missing from the dataset and adding them.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We evaluate our performance with AUC scores for each of the hands.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training &amp; Fine-tuning</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">We refer to a model trained on DMD as our reference, and compare it to a model trained mainly on synthetic data and boosted with very limited amount of real data. We consider two methods for using the synthetic data. 1) Train on the synthetic data alone and 2) Train on the synthetic dataset followed by fine-tuning with a mix of real and synthetic data. We train all models using Adam optimizer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> with <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><msub id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2.2" xref="S4.SS1.p1.1.m1.1.1.2.2.cmml">β</mi><mn id="S4.SS1.p1.1.m1.1.1.2.3" xref="S4.SS1.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><eq id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></eq><apply id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS1.p1.1.m1.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\beta_{1}=0.9</annotation></semantics></math>, <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\beta_{2}=0.9999" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><msub id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2.2" xref="S4.SS1.p1.2.m2.1.1.2.2.cmml">β</mi><mn id="S4.SS1.p1.2.m2.1.1.2.3" xref="S4.SS1.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">0.9999</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><eq id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></eq><apply id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.2.1.cmml" xref="S4.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS1.p1.2.m2.1.1.2.3.cmml" xref="S4.SS1.p1.2.m2.1.1.2.3">2</cn></apply><cn type="float" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">0.9999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\beta_{2}=0.9999</annotation></semantics></math>, batch size of 128, weight decay of 0.1, and initial learning rate of 0.0001.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_ll ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">Experiment</th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;"><span id="S4.T2.2.1.1.2.1" class="ltx_text" style="font-size:90%;"># Train Images</span></th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;">Left <span id="S4.T2.2.1.1.3.1" class="ltx_text" style="font-size:80%;">AUC</span>
</th>
<th id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-bottom:2.15277pt;">Right <span id="S4.T2.2.1.1.4.1" class="ltx_text" style="font-size:80%;">AUC</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.1" class="ltx_tr">
<td id="S4.T2.2.2.1.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_r ltx_border_tt">Real Only</td>
<td id="S4.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">531k</td>
<td id="S4.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.9813</td>
<td id="S4.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">0.9941</td>
</tr>
<tr id="S4.T2.2.3.2" class="ltx_tr">
<td id="S4.T2.2.3.2.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_r">Synth Only</td>
<td id="S4.T2.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r">8,800</td>
<td id="S4.T2.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.7226</td>
<td id="S4.T2.2.3.2.4" class="ltx_td ltx_align_center ltx_border_rr">0.7581</td>
</tr>
<tr id="S4.T2.2.4.3" class="ltx_tr">
<td id="S4.T2.2.4.3.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_r">Synth+100 Real</td>
<td id="S4.T2.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r">8,900</td>
<td id="S4.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.8769</td>
<td id="S4.T2.2.4.3.4" class="ltx_td ltx_align_center ltx_border_rr">0.9045</td>
</tr>
<tr id="S4.T2.2.5.4" class="ltx_tr">
<td id="S4.T2.2.5.4.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_r">Synth+200 Real</td>
<td id="S4.T2.2.5.4.2" class="ltx_td ltx_align_center ltx_border_r">9,000</td>
<td id="S4.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.9139</td>
<td id="S4.T2.2.5.4.4" class="ltx_td ltx_align_center ltx_border_rr">0.9389</td>
</tr>
<tr id="S4.T2.2.6.5" class="ltx_tr">
<td id="S4.T2.2.6.5.1" class="ltx_td ltx_align_center ltx_border_ll ltx_border_r">Synth+300 Real</td>
<td id="S4.T2.2.6.5.2" class="ltx_td ltx_align_center ltx_border_r">9,100</td>
<td id="S4.T2.2.6.5.3" class="ltx_td ltx_align_center ltx_border_r">0.9251</td>
<td id="S4.T2.2.6.5.4" class="ltx_td ltx_align_center ltx_border_rr">0.9475</td>
</tr>
<tr id="S4.T2.2.7.6" class="ltx_tr">
<td id="S4.T2.2.7.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_ll ltx_border_r">Synth+400 Real</td>
<td id="S4.T2.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">9,200</td>
<td id="S4.T2.2.7.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.9369</td>
<td id="S4.T2.2.7.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr">0.9530</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">AUC scores for models trained on synthetic dataset, real dataset, and tested on the real dataset</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We would expect that due to the difference in dataset sizes, as well as because of the domain gap, it is to be expected that a model trained on synthetic dataset alone performs worse than one trained on the full dataset. This is exactly what we see in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Training &amp; Fine-tuning ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the real dataset performs significantly better than the model trained on the synthetic dataset.
We also sub-sample the DMD dataset to create four small datasets with only 100, 200, 300, and 400 frames in them, respectively. We create the datasets by choosing five drivers and sample 5, 10, 15, and 20 frames from each of the four videos that each driver appears in. We train the model on synthetic data and then fine-tune it for 2,500 batches using a mix of synthetic data and real data from the small datasets. When fine-tuning, we make sure that each batch contains an even mix of synthetic data and real data. Without this technique, the network forgets its initial training on the synthetic data.
We demonstrate the effectiveness of synthetic data by comparing to models trained solely on each of these small real datasets.
We ran the experiment 11 times on different sub-sampling splits of the data and show the AUC mean and standard deviation on the real test set for each small dataset in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 Training &amp; Fine-tuning ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The large error intervals are caused by the sensitivity to the small amount of data in each split.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Since the right hand is clearly visible in most frames, the network receives enough variations to learn. Therefore, the synthetic data does not improve the model as much over the real data. However, for the left hand, which is often occluded by the right hand, the lack of variations is noticeable, and the synthetic data provides a clear improvement. This is especially true when there are fewer than 200 images in the real training set and the results improve from 0.76 AUC to 0.91. In this case, the network did not have the opportunity to see all the different edge cases that are only present in the synthetic data.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.00148/assets/imgs/left_hand_finetune.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="638" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Left Hand</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.00148/assets/imgs/right_hand_finetune.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="685" height="638" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Right Hand</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">AUC comparison between baseline results (blue) and synthetic fine-tune results (orange) for the left hand (<a href="#S4.F3.sf1" title="Figure 3(a) ‣ Figure 3 ‣ 4.1 Training &amp; Fine-tuning ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>) and right hand (<a href="#S4.F3.sf2" title="Figure 3(b) ‣ Figure 3 ‣ 4.1 Training &amp; Fine-tuning ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>)</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data-Centric Iteration</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In addition to iterating on the model, we also iterate on the dataset. We use our base model, which was trained only on synthetic data (results in second row of Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Training &amp; Fine-tuning ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> ), and we visually analyze the errors in the DMD validation split.
The majority of misclassifications can be divided into several specific categories:
1) Occlusion - Hands overlap in the image, so the network has a hard time telling whether the left hand is on or off the wheel (See Figure <a href="#S4.F4.sf1" title="Figure 4(a) ‣ Figure 4 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>)
2) Both Off - This is an uncommon case, so the network has a harder time classifying this case correctly (See Figure <a href="#S4.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>)
3) Opposite Side - The right hand is on the left side of the wheel or vice versa. The network will classify the left hand as ”on” and the right hand as ”off” (See Figure <a href="#S4.F4.sf3" title="Figure 4(c) ‣ Figure 4 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(c)</span></a>)
4) Blur - The video is blurry when the hand is in motion and so it’s unclear if the hand on wheel or not (See Figure <a href="#S4.F4.sf4" title="Figure 4(d) ‣ Figure 4 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(d)</span></a>)
Based on our failure analysis we generated sequences with both hands off wheel (total of 450 images). Examples for the generated frames is shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. After retraining, performance on this specific scenario increased, with the recall and precision jumping from 0.77 and 0.98 to 0.85 and 0.99, respectively.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.00148/assets/imgs/errors/left_hand_on_pred_off.jpg" id="S4.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="651" height="766" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">Occlusion</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.00148/assets/imgs/errors/both_hands_off.jpg" id="S4.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="651" height="766" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">Both hands off</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.00148/assets/imgs/errors/right_hand_left_side.jpg" id="S4.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="651" height="766" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F4.sf3.3.2" class="ltx_text" style="font-size:90%;">Hand on opposite side</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.00148/assets/imgs/errors/blur.png" id="S4.F4.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="651" height="766" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F4.sf4.3.2" class="ltx_text" style="font-size:90%;">Blur</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Some examples of common types of errors. (<a href="#S4.F4.sf1" title="Figure 4(a) ‣ Figure 4 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>) (<a href="#S4.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>) Left hand classified as on (<a href="#S4.F4.sf3" title="Figure 4(c) ‣ Figure 4 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(c)</span></a>) Left hand classified as on, Right hand classified as off (<a href="#S4.F4.sf4" title="Figure 4(d) ‣ Figure 4 ‣ 4.2 Data-Centric Iteration ‣ 4 Experiments ‣ Hands-Up: Leveraging Synthetic Data for Hands-On-Wheel Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(d)</span></a>) Right hand classified as on</span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2206.00148/assets/imgs/new_images.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Some new frames with both hands off that were added to the synthetic dataset after the data-centric iteration.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we simulate a situation in which only a small amount of real data is available and demonstrate how synthetic data can compensate for the missing real data. We show that mixing real and synthetic data outperforms training on a small amount of real data alone. Introducing small amount or real data to a synthetic-first model boosts performance by compensating the domain gap reaching almost the same results as training on a large amount of real data. Furthermore, we followed the data-centric approach applying a single data improvement iteration leveraging our configurable platform that successfully improved our model emphasizing the great potential or incorporating synthetic data in real-life models.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Further research is required to improve results when training solely with synthetic data. We believe that adopting more iterations of the data centric approach will improve results. This involves iterations of failure analysis and updating the training dataset appropriately. Supplementing our model with additional information, such as depth maps or sequential frame information could also improve results. Another interesting experiment would be to compare results among different cameras and explore combinations of multiple cameras to help compensate for occluded areas. This could also support a pipeline that involves identifying the location of the hands individually rather than classifying the image directly. Additionally, we plan to utilize our 3D pixel perfect key-points to solve hands-on-wheel problem using pose-estimation of the hands. A final interesting direction involves the use of unsupervised pretraining to ensure that the feature distributions of the synthetic and real data are similar, thus overcoming the domain gap.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Abien Fred Agarap.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Deep learning using rectified linear units (relu).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1803.08375</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Jimmy Ba Diederik P. Kingma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Andrew G. Howard et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Mobilenets: Efficient convolutional neural networks for mobile vision
applications, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Juan Diego Ortega et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">DMD: A large-scale multi-modal driver monitoring dataset for
attention and alertness analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2020 Workshops</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages
387–405. Springer International Publishing, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Lucia Caudet et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Road safety: Commission welcomes agreement on new eu rules to help
save lives.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Manuel Martin et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Drive&amp;act: A multi-modal dataset for fine-grained driver behavior
recognition in autonomous vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">,
Oct 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Neslihan Kose et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Real-time driver state monitoring using a cnn based spatio-temporal
approach, 2019.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
EuroNCAP.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Occupant status monitoring, 2020.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
NHTSA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Traffic safety facts: Research notes, Apr 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Akshay Rangesh and Mohan M. Trivedi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Handynet: A one-stop solution to detect, segment, localize and
analyze driver hands, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Akash Sengupta, Ignas Budvytis, and Roberto Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Synthetic training for accurate 3d human pose and shape estimation in
the wild, 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2206.00147" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2206.00148" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2206.00148">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2206.00148" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2206.00149" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 16:34:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
