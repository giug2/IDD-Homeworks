<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1812.06876] Multi-task learning to improve natural language understanding</title><meta property="og:description" content="Recently advancements in sequence-to-sequence neural network architectures have led to an improved natural language understanding.
When building a neural network-based Natural Language Understanding component, one main‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-task learning to improve natural language understanding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-task learning to improve natural language understanding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1812.06876">

<!--Generated on Sat Mar 16 22:39:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Stefan Constantin</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Jan Niehues</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Alex Waibel</span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Karlsruhe Institute of Technology, Institute for Anthropomatics and Robotics, Karlsruhe, Germany
<br class="ltx_break"><span id="id4.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">email: </span><span id="id4.1.1" class="ltx_text ltx_font_typewriter">firstname.lastname@kit.edu</span></span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Multi-task learning to improve natural language understanding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefan Constantin
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Niehues
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alex Waibel
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Recently advancements in sequence-to-sequence neural network architectures have led to an improved natural language understanding.
When building a neural network-based Natural Language Understanding component, one main challenge is to collect enough training data.
The generation of a synthetic dataset is an inexpensive and quick way to collect data.
Since this data often has less variety than real natural language, neural networks often have problems to generalize to unseen utterances during testing.</p>
<p id="id3.id2" class="ltx_p">In this work, we address this challenge by using multi-task learning.
We train out-of-domain real data alongside in-domain synthetic data to improve natural language understanding.</p>
<p id="id1.1" class="ltx_p">We evaluate this approach in the domain of airline travel information with two synthetic datasets.
As out-of-domain real data, we test two datasets based on the subtitles of movies and series.
By using an attention-based encoder-decoder model, we were able to improve the <math id="id1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="id1.1.m1.1a"><msub id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">F</mi><mn id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="ambiguous" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1">subscript</csymbol><ci id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">F_{1}</annotation></semantics></math>-score over strong baselines from 80.76‚Äâ% to 84.98‚Äâ% in the smaller synthetic dataset.</p>
</div>
<section id="Ch0.S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="Ch0.S1.p1" class="ltx_para">
<p id="Ch0.S1.p1.1" class="ltx_p">One of the main challenges in building a Natural Language Understanding (NLU) component for a specific task is the necessary human effort to encode the task‚Äôs specific knowledge.
In traditional NLU components, this was done by creating hand-written rules.
In today‚Äôs state-of-the-art NLU components, significant amounts of human effort have to be used for collecting the training data.
For example, there are a lot of possibilities to express the situation that someone wants to book a flight from New York to Pittsburgh.
In order to get a good NLU component, we need to have seen many of them in the training data.
Although more and more data has been collected and datasets with this data have been published <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">SerbanLHCP18 </a></cite>, the datasets often consist of data from another domain, which is needed for a certain NLU component.</p>
</div>
<div id="Ch0.S1.p2" class="ltx_para">
<p id="Ch0.S1.p2.1" class="ltx_p">An inexpensive and quick way to collect data for a domain is to generate a synthetic dataset where templates are filled with various values.
A problem with such synthetic datasets is to encode enough variety of natural language to be able to generalize to unseen utterances during training.
To do this, an enormous amount of effort will be needed.
In this work, we address this challenge by combining task-specific synthetic data and real data from another domain.
The multi-task framework enables us to combine these two knowledge sources and therefore improve natural language understanding.</p>
</div>
<div id="Ch0.S1.p3" class="ltx_para">
<p id="Ch0.S1.p3.1" class="ltx_p">In this work, the NLU component is based on an attention-based encoder-decoder model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">BahdanauCB15 </a></cite>.
We evaluate the approach on the commonly used travel information task and used as an out-of-domain task the subtitles of movies and series.</p>
</div>
</section>
<section id="Ch0.S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="Ch0.S2.p1" class="ltx_para">
<p id="Ch0.S2.p1.1" class="ltx_p">There are many appropriate architectures for end-to-end trainable goal-oriented dialog systems <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">BahdanauCB15 </a>; <a href="#bib.bib4" title="" class="ltx_ref">ConstantinNW18 </a>; <a href="#bib.bib15" title="" class="ltx_ref">SerbanAHL2017 </a></cite> with different approaches for the NLU part; however, what they have in common is that they need a huge amount of training data.</p>
</div>
<div id="Ch0.S2.p2" class="ltx_para">
<p id="Ch0.S2.p2.1" class="ltx_p">Multi-task learning has been performed in many machine learning applications, e.‚Äâg., in facial landmark detection an application in the area of vision <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">ZhangLLT2014 </a></cite>.</p>
</div>
<div id="Ch0.S2.p3" class="ltx_para">
<p id="Ch0.S2.p3.1" class="ltx_p">Multi-task learning for sequence-to-sequence models in Natural Language Processing is described in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">LuongLSVK2016 </a>; <a href="#bib.bib9" title="" class="ltx_ref">NiehuesC17 </a>; <a href="#bib.bib10" title="" class="ltx_ref">PhamSSHNW2017 </a></cite>.
In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">LuongLSVK2016 </a></cite>, machine translation was trained together with either syntax parsing or image captioning on a not attention-based encoder-decoder model.
The encoder was shared between the tasks.
They improved the translation between English and German by up to 1.5 BLEU points.
In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">NiehuesC17 </a></cite>, the authors used an attention-based encoder-decoder model and were also able to improve on this model machine translation by up to 1.5 BLEU points by combining machine translation with part-of-speech tagging and named entity recognition (the encoder was shared).
In addition, they presented different architectures for multi-task learning, such as sharing in addition to the encoder, the attention layer, or decoder.
In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">PhamSSHNW2017 </a></cite>, the authors used multi-task learning to learn to translate 20 individual languages with one system.</p>
</div>
</section>
<section id="Ch0.S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Multi-task Learning</h2>

<div id="Ch0.S3.p1" class="ltx_para">
<p id="Ch0.S3.p1.1" class="ltx_p">In the multi-task learning approach of this work, in-domain synthetic data and out-of-domain real data are jointly trained.
In synthetic datasets, there are often missing expressions for situations.
However, in larger out-of-domain datasets, there are expressions for similar situations.
Through the joint training of the encoding for both tasks, we expect a better natural language understanding in the in-domain task because it can be learned to encode situations independent to their expression in natural language.</p>
</div>
<section id="Ch0.S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture</h3>

<div id="Ch0.S3.SS1.p1" class="ltx_para">
<p id="Ch0.S3.SS1.p1.1" class="ltx_p">We use an attention-based encoder-decoder model for multi-task learning.
We share between the tasks the embedding layer and the encoder.
The remaining components of the attention-based encoder-decoder model - the attention layer and the decoder with its final softmax layer - are not shared.
The intuition behind this is, that in our synthetic datasets, there are missing expressions for situations that are in the out-of-domain datasets.
With the training of the out-of-domain datasets, we want to learn to encode situations independent to their expression in natural language.
For improving encoding, we expect the best results by only sharing the encoder because knowledge from the out-of-domain dataset is transfered to the in-domain dataset.</p>
</div>
<div id="Ch0.S3.SS1.p2" class="ltx_para">
<p id="Ch0.S3.SS1.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">PhamSSHNW2017 </a></cite>, an attention-based encoder-decoder model that is able to share the weights of layers between tasks is described and its implementation was published.
We added to this implementation an option to train instances of the smallest dataset <math id="Ch0.S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="Ch0.S3.SS1.p2.1.m1.1a"><mi id="Ch0.S3.SS1.p2.1.m1.1.1" xref="Ch0.S3.SS1.p2.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Ch0.S3.SS1.p2.1.m1.1b"><ci id="Ch0.S3.SS1.p2.1.m1.1.1.cmml" xref="Ch0.S3.SS1.p2.1.m1.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S3.SS1.p2.1.m1.1c">m</annotation></semantics></math>-times and an option to accumulate gradients and published<span id="Ch0.footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/isl-mt/OpenNMT-py/tree/MultiTask</span></span></span></span> the additions under the MIT license.
The architecture is depicted in Figure <a href="#Ch0.F1" title="Figure 1 ‚Ä£ 3.1 Architecture ‚Ä£ 3 Multi-task Learning ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="Ch0.F1" class="ltx_figure"><img src="/html/1812.06876/assets/x1.png" id="Ch0.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>attention-based encoder-decoder</figcaption>
</figure>
</section>
<section id="Ch0.S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Schedule</h3>

<div id="Ch0.S3.SS2.p1" class="ltx_para">
<p id="Ch0.S3.SS2.p1.4" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">NiehuesC17 </a></cite>, only one task in each mini-batch is considered because this is more GPU-efficient given that not all weights are shared between the tasks.
Let <math id="Ch0.S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="Ch0.S3.SS2.p1.1.m1.1a"><mi id="Ch0.S3.SS2.p1.1.m1.1.1" xref="Ch0.S3.SS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch0.S3.SS2.p1.1.m1.1b"><ci id="Ch0.S3.SS2.p1.1.m1.1.1.cmml" xref="Ch0.S3.SS2.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S3.SS2.p1.1.m1.1c">n</annotation></semantics></math> be the number of instances that are trained simultaneously on the GPU.
The instances of one task are grouped into groups of size <math id="Ch0.S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="Ch0.S3.SS2.p1.2.m2.1a"><mi id="Ch0.S3.SS2.p1.2.m2.1.1" xref="Ch0.S3.SS2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch0.S3.SS2.p1.2.m2.1b"><ci id="Ch0.S3.SS2.p1.2.m2.1.1.cmml" xref="Ch0.S3.SS2.p1.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S3.SS2.p1.2.m2.1c">n</annotation></semantics></math>.
These groups are randomly shuffled before every epoch during training.
However, in our experiments, updating the weights after the training of a group of one task led to perplexity jumps.
To avoid these jumps, we accumulate the gradients and update our weights only after <math id="Ch0.S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="Ch0.S3.SS2.p1.3.m3.1a"><mi id="Ch0.S3.SS2.p1.3.m3.1.1" xref="Ch0.S3.SS2.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="Ch0.S3.SS2.p1.3.m3.1b"><ci id="Ch0.S3.SS2.p1.3.m3.1.1.cmml" xref="Ch0.S3.SS2.p1.3.m3.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S3.SS2.p1.3.m3.1c">t</annotation></semantics></math> groups.
This means that our mini-batch size is <math id="Ch0.S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="t\cdot n" display="inline"><semantics id="Ch0.S3.SS2.p1.4.m4.1a"><mrow id="Ch0.S3.SS2.p1.4.m4.1.1" xref="Ch0.S3.SS2.p1.4.m4.1.1.cmml"><mi id="Ch0.S3.SS2.p1.4.m4.1.1.2" xref="Ch0.S3.SS2.p1.4.m4.1.1.2.cmml">t</mi><mo lspace="0.222em" rspace="0.222em" id="Ch0.S3.SS2.p1.4.m4.1.1.1" xref="Ch0.S3.SS2.p1.4.m4.1.1.1.cmml">‚ãÖ</mo><mi id="Ch0.S3.SS2.p1.4.m4.1.1.3" xref="Ch0.S3.SS2.p1.4.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="Ch0.S3.SS2.p1.4.m4.1b"><apply id="Ch0.S3.SS2.p1.4.m4.1.1.cmml" xref="Ch0.S3.SS2.p1.4.m4.1.1"><ci id="Ch0.S3.SS2.p1.4.m4.1.1.1.cmml" xref="Ch0.S3.SS2.p1.4.m4.1.1.1">‚ãÖ</ci><ci id="Ch0.S3.SS2.p1.4.m4.1.1.2.cmml" xref="Ch0.S3.SS2.p1.4.m4.1.1.2">ùë°</ci><ci id="Ch0.S3.SS2.p1.4.m4.1.1.3.cmml" xref="Ch0.S3.SS2.p1.4.m4.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S3.SS2.p1.4.m4.1c">t\cdot n</annotation></semantics></math>.
We use the Adam optimization algorithm <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">KingmaB2015 </a></cite> for updating the weights.</p>
</div>
<div id="Ch0.S3.SS2.p2" class="ltx_para">
<p id="Ch0.S3.SS2.p2.1" class="ltx_p">After the multi-task learning, we fine-tune the model by retraining the model only with the synthetic dataset.
For this fine-tuning, we reset all the parameters of the Adam optimization algorithm.</p>
</div>
<div id="Ch0.S3.SS2.p3" class="ltx_para">
<p id="Ch0.S3.SS2.p3.1" class="ltx_p">The out-of-domain datasets have a huge size in comparison to the synthetic datasets.
To avoid instances of the synthetic datasets are not considered in the training of the model, instances of the synthetic dataset are trained <math id="Ch0.S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="Ch0.S3.SS2.p3.1.m1.1a"><mi id="Ch0.S3.SS2.p3.1.m1.1.1" xref="Ch0.S3.SS2.p3.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Ch0.S3.SS2.p3.1.m1.1b"><ci id="Ch0.S3.SS2.p3.1.m1.1.1.cmml" xref="Ch0.S3.SS2.p3.1.m1.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S3.SS2.p3.1.m1.1c">m</annotation></semantics></math>-times during one epoch.</p>
</div>
</section>
</section>
<section id="Ch0.S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="Ch0.S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data</h3>

<div id="Ch0.S4.SS1.p1" class="ltx_para">
<p id="Ch0.S4.SS1.p1.1" class="ltx_p">For the out-of-domain task, we use two subsets of the English OpenSubtitle corpus <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">Tiedemann2009 </a></cite><span id="Ch0.footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>based on http://www.opensubtitles.org/</span></span></span> in this work.
The OpenSubtitle corpus consists of the subtitles of movies and series.
The first subset was published by <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">Senellart2017 </a></cite><span id="Ch0.footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://s3.amazonaws.com/opennmt-trainingdata/opensub\_qa\_en.tgz</span></span></span></span> and consists of all the sentence pairs from the OpenSubtitle corpus that have the following properties: the first sentence ends with a question mark; the second sentence follows directly the first sentence and has no question mark; and the time difference between the sentences is less than 20 seconds.
In total, the subset has more than 14 million sentence pairs for training and 10‚Äâ000 sentence pairs for validation.
In the following sections, this dataset is called <span id="Ch0.S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">OpenSubtitles QA</span>.
We created the second subset in a similar manner as the SubTle dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">AmeixaC2013 </a></cite> was created.
It consists of sentence pairs with the following properties: the second sentence follows directly the first sentence; both sentences end with a point, exclamation point, or question mark; and between the two sentences, there is at maximum a pause of 1 second.
In the following sections, this dataset is called <span id="Ch0.S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">OpenSubtitles dialog</span>.
To be able to train the attention-based encoder-decoder model in a reasonable time, we only used the first 14 million sentence pairs for training.
The next 10‚Äâ000 sentence pairs were used for validation.
For both datasets we used the default English word tokenizer of the Natural Language Toolkit (NLTK) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">BirdKL2009 </a></cite><span id="Ch0.footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://www.nltk.org/</span></span></span> for tokenization.
As there is another tokenization approach in the OpenSubtitle corpus in comparison to the tokenizer in the NLTK, we had to merge the tokens ‚Äôs, ‚Äôre, ‚Äôt, ‚Äôll, and ‚Äôve to their previous token in the <span id="Ch0.S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">OpenSubtitles dialog</span> dataset to improve the compatibility with the tokenization of the NLTK.</p>
</div>
<div id="Ch0.S4.SS1.p2" class="ltx_para">
<p id="Ch0.S4.SS1.p2.1" class="ltx_p">We generated two synthetic datasets.
These two datasets are based on a subset of the ATIS (Airline Travel Information Systems) dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">Price1990 </a></cite> that was published by <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">HakkaniTCCGDW2016 </a></cite><span id="Ch0.footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>available at https://github.com/yvchen/JointSLU</span></span></span> and is called <span id="Ch0.S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">ATIS real</span> in the following sections.
In the ATIS corpus, every user utterance has one or multiple intents and every word of a user utterance is tagged in the IOB format.
The format is depicted in Figure <a href="#Ch0.F2" title="Figure 2 ‚Ä£ 4.1 Data ‚Ä£ 4 Experimental Setup ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
However, the out-of-domain dataset is no intent and slot filling task.
It is a sequence-to-sequence task.
To train both tasks together, we converted the intent and slot filling task to a sequence-to-sequence task.
The target sequence consists of the intents followed by the parameters.
A parameter consists of the slot name and the slot value.
An example conversion is depicted in Figure <a href="#Ch0.F2" title="Figure 2 ‚Ä£ 4.1 Data ‚Ä£ 4 Experimental Setup ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
With this conversion, the attention-based encoder-decoder can learn both tasks: user utterance to semantic representation and subtitle sentence A to subtitle sentence B.
In Figure <a href="#Ch0.F1" title="Figure 1 ‚Ä£ 3.1 Architecture ‚Ä£ 3 Multi-task Learning ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, it is depicted how the attention-based encoder-decoder handles both tasks.</p>
</div>
<figure id="Ch0.F2" class="ltx_figure">
<table id="Ch0.F2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Ch0.F2.1.1.1" class="ltx_tr">
<th id="Ch0.F2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch0.F2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.F2.1.1.1.1.1.1" class="ltx_p" style="width:61.2pt;">utterance (source sequence)</span>
</span>
</th>
<th id="Ch0.F2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">show</th>
<th id="Ch0.F2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">me</th>
<th id="Ch0.F2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">flights</th>
<th id="Ch0.F2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">between</th>
<th id="Ch0.F2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">new</th>
<th id="Ch0.F2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">york</th>
<th id="Ch0.F2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">city</th>
<th id="Ch0.F2.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">and</th>
<th id="Ch0.F2.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">pittsburgh</th>
</tr>
<tr id="Ch0.F2.1.2.2" class="ltx_tr">
<th id="Ch0.F2.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch0.F2.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.F2.1.2.2.1.1.1" class="ltx_p" style="width:61.2pt;">slots</span>
</span>
</th>
<th id="Ch0.F2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="Ch0.F2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="Ch0.F2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="Ch0.F2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="Ch0.F2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">B-fromloc</th>
<th id="Ch0.F2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">I-fromloc</th>
<th id="Ch0.F2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">I-fromloc</th>
<th id="Ch0.F2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">O</th>
<th id="Ch0.F2.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">B-toloc</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Ch0.F2.1.3.1" class="ltx_tr">
<th id="Ch0.F2.1.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch0.F2.1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.F2.1.3.1.1.1.1" class="ltx_p" style="width:61.2pt;">intents</span>
</span>
</th>
<td id="Ch0.F2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9">ATIS_flight</td>
</tr>
<tr id="Ch0.F2.1.4.2" class="ltx_tr">
<th id="Ch0.F2.1.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch0.F2.1.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.F2.1.4.2.1.1.1" class="ltx_p" style="width:61.2pt;">target sequence</span>
</span>
</th>
<td id="Ch0.F2.1.4.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="9">ATIS_flight fromloc new york city toloc pittsburgh</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>format of the ATIS corpus and the conversion to a sequence-to-sequence problem</figcaption>
</figure>
<div id="Ch0.S4.SS1.p3" class="ltx_para">
<p id="Ch0.S4.SS1.p3.1" class="ltx_p">In the <span id="Ch0.S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">ATIS real</span> dataset, there are 4478 tagged user utterances for training, 500 for validation and 893 for testing.</p>
</div>
<div id="Ch0.S4.SS1.p4" class="ltx_para">
<p id="Ch0.S4.SS1.p4.1" class="ltx_p">For training, the smaller synthetic dataset has 212 templates that form 17‚Äâ679 source target sequence pairs after filling the template placeholders and is called <span id="Ch0.S4.SS1.p4.1.1" class="ltx_text ltx_font_italic">ATIS small</span> in the following sections and the larger dataset has 832 templates that form 70‚Äâ040 source target sequence pairs and is called <span id="Ch0.S4.SS1.p4.1.2" class="ltx_text ltx_font_italic">ATIS medium</span> in the following sections.
The validation and test utterances are the same in all three datasets (<span id="Ch0.S4.SS1.p4.1.3" class="ltx_text ltx_font_italic">ATIS real</span>, <span id="Ch0.S4.SS1.p4.1.4" class="ltx_text ltx_font_italic">ATIS small</span>, and <span id="Ch0.S4.SS1.p4.1.5" class="ltx_text ltx_font_italic">ATIS medium</span>).
The templates for the training utterances of the <span id="Ch0.S4.SS1.p4.1.6" class="ltx_text ltx_font_italic">ATIS small</span> dataset were generated by extracting all the sequences that have a new parameter in the target sequence that was not included in any target sequence extracted before.
Extracting all the sequences that have a parameter combination that was not included in any target sequence extracted before, form the training templates of the <span id="Ch0.S4.SS1.p4.1.7" class="ltx_text ltx_font_italic">ATIS medium</span> dataset.
In the extracted sequences, the parameter values were replaced by placeholders to become templates.
For the placeholders, all the possible values were inserted.
When one template produced more than 1000 source target sequence pairs, then, instead of the Cartesian product, the random permutation algorithm <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">Hazwani2016 </a></cite> was used, which produces as many source target sequence pairs as the values of the placeholder with the greatest number of values.
For both datasets, we alphabetically sorted the parameters to ease the learning process.</p>
</div>
</section>
<section id="Ch0.S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation</h3>

<div id="Ch0.S4.SS2.p1" class="ltx_para">
<p id="Ch0.S4.SS2.p1.3" class="ltx_p">We evaluate the quality of the predicted intents and parameters with the metric <math id="Ch0.S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S4.SS2.p1.1.m1.1a"><msub id="Ch0.S4.SS2.p1.1.m1.1.1" xref="Ch0.S4.SS2.p1.1.m1.1.1.cmml"><mi id="Ch0.S4.SS2.p1.1.m1.1.1.2" xref="Ch0.S4.SS2.p1.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.S4.SS2.p1.1.m1.1.1.3" xref="Ch0.S4.SS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS2.p1.1.m1.1b"><apply id="Ch0.S4.SS2.p1.1.m1.1.1.cmml" xref="Ch0.S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.S4.SS2.p1.1.m1.1.1.1.cmml" xref="Ch0.S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="Ch0.S4.SS2.p1.1.m1.1.1.2.cmml" xref="Ch0.S4.SS2.p1.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S4.SS2.p1.1.m1.1.1.3.cmml" xref="Ch0.S4.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS2.p1.1.m1.1c">F_{1}</annotation></semantics></math>-score.
Every intent and parameter is considered individually.
For averaging the <math id="Ch0.S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S4.SS2.p1.2.m2.1a"><msub id="Ch0.S4.SS2.p1.2.m2.1.1" xref="Ch0.S4.SS2.p1.2.m2.1.1.cmml"><mi id="Ch0.S4.SS2.p1.2.m2.1.1.2" xref="Ch0.S4.SS2.p1.2.m2.1.1.2.cmml">F</mi><mn id="Ch0.S4.SS2.p1.2.m2.1.1.3" xref="Ch0.S4.SS2.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS2.p1.2.m2.1b"><apply id="Ch0.S4.SS2.p1.2.m2.1.1.cmml" xref="Ch0.S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Ch0.S4.SS2.p1.2.m2.1.1.1.cmml" xref="Ch0.S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="Ch0.S4.SS2.p1.2.m2.1.1.2.cmml" xref="Ch0.S4.SS2.p1.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S4.SS2.p1.2.m2.1.1.3.cmml" xref="Ch0.S4.SS2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS2.p1.2.m2.1c">F_{1}</annotation></semantics></math>-score over the target sequences, we use micro-averaging.
This means that we count the number of true positives, false positives, and false negatives for all the intents and parameters and calculate the recall and precision for the <math id="Ch0.S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S4.SS2.p1.3.m3.1a"><msub id="Ch0.S4.SS2.p1.3.m3.1.1" xref="Ch0.S4.SS2.p1.3.m3.1.1.cmml"><mi id="Ch0.S4.SS2.p1.3.m3.1.1.2" xref="Ch0.S4.SS2.p1.3.m3.1.1.2.cmml">F</mi><mn id="Ch0.S4.SS2.p1.3.m3.1.1.3" xref="Ch0.S4.SS2.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS2.p1.3.m3.1b"><apply id="Ch0.S4.SS2.p1.3.m3.1.1.cmml" xref="Ch0.S4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="Ch0.S4.SS2.p1.3.m3.1.1.1.cmml" xref="Ch0.S4.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="Ch0.S4.SS2.p1.3.m3.1.1.2.cmml" xref="Ch0.S4.SS2.p1.3.m3.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S4.SS2.p1.3.m3.1.1.3.cmml" xref="Ch0.S4.SS2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS2.p1.3.m3.1c">F_{1}</annotation></semantics></math>-score with these.</p>
</div>
<div id="Ch0.S4.SS2.p2" class="ltx_para">
<p id="Ch0.S4.SS2.p2.1" class="ltx_p">In addition, we provide the metric intent accuracy.
For the intent accuracy, the number of completely correct predicted intents (the intents of the reference and hypothesis must be the same) is divided by the number of target sequences.</p>
</div>
</section>
<section id="Ch0.S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>System Setup</h3>

<div id="Ch0.S4.SS3.p1" class="ltx_para">
<p id="Ch0.S4.SS3.p1.2" class="ltx_p">We optimized our single-task baseline to get a strong baseline in order to exclude better results in multi-task learning in comparison to single-task learning only because of these two following points: network parameters suit the multi-task learning approach better and a better randomness while training in the multi-task learning.
To exclude the first point, we tested different hyperparameters for the single-task baseline.
We tested all the combinations of the following hyperparameter values: 256, 512, or 1024 as the sizes for the hidden states of the LSTMs, 256, 512, or 1024 as word embedding sizes, and a dropout of 30‚Äâ%, 40‚Äâ%, or 50‚Äâ%.
We used subword units generated by byte-pair encoding (BPE) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">SennrichHB2016 </a></cite> as inputs for our model.
To avoid bad subword generation for the synthetic datasets, in addition to the training dataset, we considered the validation and test dataset for the generating of the BPE merge operations list.
We trained the configurations for 14 epochs and trained every configuration three times.
We chose the training with the best quality with regard to the validation <math id="Ch0.S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S4.SS3.p1.1.m1.1a"><msub id="Ch0.S4.SS3.p1.1.m1.1.1" xref="Ch0.S4.SS3.p1.1.m1.1.1.cmml"><mi id="Ch0.S4.SS3.p1.1.m1.1.1.2" xref="Ch0.S4.SS3.p1.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.S4.SS3.p1.1.m1.1.1.3" xref="Ch0.S4.SS3.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.p1.1.m1.1b"><apply id="Ch0.S4.SS3.p1.1.m1.1.1.cmml" xref="Ch0.S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.S4.SS3.p1.1.m1.1.1.1.cmml" xref="Ch0.S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="Ch0.S4.SS3.p1.1.m1.1.1.2.cmml" xref="Ch0.S4.SS3.p1.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S4.SS3.p1.1.m1.1.1.3.cmml" xref="Ch0.S4.SS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.p1.1.m1.1c">F_{1}</annotation></semantics></math>-score to exclude disadvantages of a bad randomness.
We got the best quality with regard to the <math id="Ch0.S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S4.SS3.p1.2.m2.1a"><msub id="Ch0.S4.SS3.p1.2.m2.1.1" xref="Ch0.S4.SS3.p1.2.m2.1.1.cmml"><mi id="Ch0.S4.SS3.p1.2.m2.1.1.2" xref="Ch0.S4.SS3.p1.2.m2.1.1.2.cmml">F</mi><mn id="Ch0.S4.SS3.p1.2.m2.1.1.3" xref="Ch0.S4.SS3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.p1.2.m2.1b"><apply id="Ch0.S4.SS3.p1.2.m2.1.1.cmml" xref="Ch0.S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Ch0.S4.SS3.p1.2.m2.1.1.1.cmml" xref="Ch0.S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="Ch0.S4.SS3.p1.2.m2.1.1.2.cmml" xref="Ch0.S4.SS3.p1.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S4.SS3.p1.2.m2.1.1.3.cmml" xref="Ch0.S4.SS3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.p1.2.m2.1c">F_{1}</annotation></semantics></math>-score with 256 as the size of the hidden states of the LSTMs, 1024 as word embedding size, and a dropout of 30‚Äâ%.
For the batch size, we used 64.</p>
</div>
<div id="Ch0.S4.SS3.p2" class="ltx_para">
<p id="Ch0.S4.SS3.p2.1" class="ltx_p">We optimized our single-task model trained on real data in the same manner as the single-task baseline, except that we used 64 epochs.</p>
</div>
<div id="Ch0.S4.SS3.p3" class="ltx_para">
<p id="Ch0.S4.SS3.p3.3" class="ltx_p">In the multi-task learning approach, we trained both tasks for 10 epochs.
We use for <math id="Ch0.S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="Ch0.S4.SS3.p3.1.m1.1a"><mi id="Ch0.S4.SS3.p3.1.m1.1.1" xref="Ch0.S4.SS3.p3.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.p3.1.m1.1b"><ci id="Ch0.S4.SS3.p3.1.m1.1.1.cmml" xref="Ch0.S4.SS3.p3.1.m1.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.p3.1.m1.1c">m</annotation></semantics></math> (the instance multiplicator of the synthetic dataset) such a value that the synthetic dataset has nearly the size of one-tenth of the out-of-domain dataset.
Because of long training times, we were not able to optimize the hyperparameters.
We chose 256 as the size of the hidden states of the LSTMs, 1024 as word embedding size, and 50‚Äâ% for the dropout and were not able to run multiple runs.
For <math id="Ch0.S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="Ch0.S4.SS3.p3.2.m2.1a"><mi id="Ch0.S4.SS3.p3.2.m2.1.1" xref="Ch0.S4.SS3.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.p3.2.m2.1b"><ci id="Ch0.S4.SS3.p3.2.m2.1.1.cmml" xref="Ch0.S4.SS3.p3.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.p3.2.m2.1c">n</annotation></semantics></math> (the number of instances that are trained simultaneously on the GPU), we chose 128 and for <math id="Ch0.S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="Ch0.S4.SS3.p3.3.m3.1a"><mi id="Ch0.S4.SS3.p3.3.m3.1.1" xref="Ch0.S4.SS3.p3.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.p3.3.m3.1b"><ci id="Ch0.S4.SS3.p3.3.m3.1.1.cmml" xref="Ch0.S4.SS3.p3.3.m3.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.p3.3.m3.1c">t</annotation></semantics></math> (number of groups after that the model weights are updated) we chose 11.
Other hyperparameters in the single-task and multi-task experiments were not changed from the default values of the published implementation.</p>
</div>
<div id="Ch0.S4.SS3.p4" class="ltx_para">
<p id="Ch0.S4.SS3.p4.3" class="ltx_p">We used the best epoch with regard to the validation <math id="Ch0.S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S4.SS3.p4.1.m1.1a"><msub id="Ch0.S4.SS3.p4.1.m1.1.1" xref="Ch0.S4.SS3.p4.1.m1.1.1.cmml"><mi id="Ch0.S4.SS3.p4.1.m1.1.1.2" xref="Ch0.S4.SS3.p4.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.S4.SS3.p4.1.m1.1.1.3" xref="Ch0.S4.SS3.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.p4.1.m1.1b"><apply id="Ch0.S4.SS3.p4.1.m1.1.1.cmml" xref="Ch0.S4.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.S4.SS3.p4.1.m1.1.1.1.cmml" xref="Ch0.S4.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="Ch0.S4.SS3.p4.1.m1.1.1.2.cmml" xref="Ch0.S4.SS3.p4.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S4.SS3.p4.1.m1.1.1.3.cmml" xref="Ch0.S4.SS3.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.p4.1.m1.1c">F_{1}</annotation></semantics></math>-score to fine-tune our model.
To exclude only better results because of good random initialization, we made three runs, used the epoch with the best validation <math id="Ch0.S4.SS3.p4.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S4.SS3.p4.2.m2.1a"><msub id="Ch0.S4.SS3.p4.2.m2.1.1" xref="Ch0.S4.SS3.p4.2.m2.1.1.cmml"><mi id="Ch0.S4.SS3.p4.2.m2.1.1.2" xref="Ch0.S4.SS3.p4.2.m2.1.1.2.cmml">F</mi><mn id="Ch0.S4.SS3.p4.2.m2.1.1.3" xref="Ch0.S4.SS3.p4.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.p4.2.m2.1b"><apply id="Ch0.S4.SS3.p4.2.m2.1.1.cmml" xref="Ch0.S4.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="Ch0.S4.SS3.p4.2.m2.1.1.1.cmml" xref="Ch0.S4.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="Ch0.S4.SS3.p4.2.m2.1.1.2.cmml" xref="Ch0.S4.SS3.p4.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S4.SS3.p4.2.m2.1.1.3.cmml" xref="Ch0.S4.SS3.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.p4.2.m2.1c">F_{1}</annotation></semantics></math>-score from every run, and chose the run with the worst validation <math id="Ch0.S4.SS3.p4.3.m3.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S4.SS3.p4.3.m3.1a"><msub id="Ch0.S4.SS3.p4.3.m3.1.1" xref="Ch0.S4.SS3.p4.3.m3.1.1.cmml"><mi id="Ch0.S4.SS3.p4.3.m3.1.1.2" xref="Ch0.S4.SS3.p4.3.m3.1.1.2.cmml">F</mi><mn id="Ch0.S4.SS3.p4.3.m3.1.1.3" xref="Ch0.S4.SS3.p4.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.p4.3.m3.1b"><apply id="Ch0.S4.SS3.p4.3.m3.1.1.cmml" xref="Ch0.S4.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="Ch0.S4.SS3.p4.3.m3.1.1.1.cmml" xref="Ch0.S4.SS3.p4.3.m3.1.1">subscript</csymbol><ci id="Ch0.S4.SS3.p4.3.m3.1.1.2.cmml" xref="Ch0.S4.SS3.p4.3.m3.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S4.SS3.p4.3.m3.1.1.3.cmml" xref="Ch0.S4.SS3.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.p4.3.m3.1c">F_{1}</annotation></semantics></math>-score for evaluation.
We used 64 as the batch size, 50‚Äâ% as dropout, and 14 as the number of epochs.</p>
</div>
<div id="Ch0.S4.SS3.p5" class="ltx_para">
<p id="Ch0.S4.SS3.p5.1" class="ltx_p">We used subword units generated by BPE for all approaches and used 40‚Äâ000 as the limit for the number of BPE merging operations as well as the vocabulary size.</p>
</div>
</section>
</section>
<section id="Ch0.S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="Ch0.S5.p1" class="ltx_para">
<p id="Ch0.S5.p1.1" class="ltx_p">For all validation and test results, the validation and test dataset of the <span id="Ch0.S5.p1.1.1" class="ltx_text ltx_font_italic">ATIS real</span> dataset is used.</p>
</div>
<div id="Ch0.S5.p2" class="ltx_para">
<p id="Ch0.S5.p2.4" class="ltx_p">In Figure <a href="#Ch0.F3" title="Figure 3 ‚Ä£ 5 Results ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the test <math id="Ch0.S5.p2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p2.1.m1.1a"><msub id="Ch0.S5.p2.1.m1.1.1" xref="Ch0.S5.p2.1.m1.1.1.cmml"><mi id="Ch0.S5.p2.1.m1.1.1.2" xref="Ch0.S5.p2.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.S5.p2.1.m1.1.1.3" xref="Ch0.S5.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p2.1.m1.1b"><apply id="Ch0.S5.p2.1.m1.1.1.cmml" xref="Ch0.S5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p2.1.m1.1.1.1.cmml" xref="Ch0.S5.p2.1.m1.1.1">subscript</csymbol><ci id="Ch0.S5.p2.1.m1.1.1.2.cmml" xref="Ch0.S5.p2.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p2.1.m1.1.1.3.cmml" xref="Ch0.S5.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p2.1.m1.1c">F_{1}</annotation></semantics></math>-score of the training run of the configuration with the best validation <math id="Ch0.S5.p2.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p2.2.m2.1a"><msub id="Ch0.S5.p2.2.m2.1.1" xref="Ch0.S5.p2.2.m2.1.1.cmml"><mi id="Ch0.S5.p2.2.m2.1.1.2" xref="Ch0.S5.p2.2.m2.1.1.2.cmml">F</mi><mn id="Ch0.S5.p2.2.m2.1.1.3" xref="Ch0.S5.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p2.2.m2.1b"><apply id="Ch0.S5.p2.2.m2.1.1.cmml" xref="Ch0.S5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p2.2.m2.1.1.1.cmml" xref="Ch0.S5.p2.2.m2.1.1">subscript</csymbol><ci id="Ch0.S5.p2.2.m2.1.1.2.cmml" xref="Ch0.S5.p2.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p2.2.m2.1.1.3.cmml" xref="Ch0.S5.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p2.2.m2.1c">F_{1}</annotation></semantics></math>-score is depicted with respect to the epoch for the <span id="Ch0.S5.p2.4.1" class="ltx_text ltx_font_italic">ATIS small</span> dataset and in Figure <a href="#Ch0.F4" title="Figure 4 ‚Ä£ 5 Results ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for the <span id="Ch0.S5.p2.4.2" class="ltx_text ltx_font_italic">ATIS medium</span> dataset.
The best result is achieved after epoch 11 or 7, respectively.
There is no trend for a further improvement after epoch 14.
The test <math id="Ch0.S5.p2.3.m3.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p2.3.m3.1a"><msub id="Ch0.S5.p2.3.m3.1.1" xref="Ch0.S5.p2.3.m3.1.1.cmml"><mi id="Ch0.S5.p2.3.m3.1.1.2" xref="Ch0.S5.p2.3.m3.1.1.2.cmml">F</mi><mn id="Ch0.S5.p2.3.m3.1.1.3" xref="Ch0.S5.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p2.3.m3.1b"><apply id="Ch0.S5.p2.3.m3.1.1.cmml" xref="Ch0.S5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p2.3.m3.1.1.1.cmml" xref="Ch0.S5.p2.3.m3.1.1">subscript</csymbol><ci id="Ch0.S5.p2.3.m3.1.1.2.cmml" xref="Ch0.S5.p2.3.m3.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p2.3.m3.1.1.3.cmml" xref="Ch0.S5.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p2.3.m3.1c">F_{1}</annotation></semantics></math>-score of the best epoch according to the validation <math id="Ch0.S5.p2.4.m4.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p2.4.m4.1a"><msub id="Ch0.S5.p2.4.m4.1.1" xref="Ch0.S5.p2.4.m4.1.1.cmml"><mi id="Ch0.S5.p2.4.m4.1.1.2" xref="Ch0.S5.p2.4.m4.1.1.2.cmml">F</mi><mn id="Ch0.S5.p2.4.m4.1.1.3" xref="Ch0.S5.p2.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p2.4.m4.1b"><apply id="Ch0.S5.p2.4.m4.1.1.cmml" xref="Ch0.S5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p2.4.m4.1.1.1.cmml" xref="Ch0.S5.p2.4.m4.1.1">subscript</csymbol><ci id="Ch0.S5.p2.4.m4.1.1.2.cmml" xref="Ch0.S5.p2.4.m4.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p2.4.m4.1.1.3.cmml" xref="Ch0.S5.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p2.4.m4.1c">F_{1}</annotation></semantics></math>-score is depicted in the Tables <a href="#Ch0.T1" title="Table 1 ‚Ä£ 5 Results ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#Ch0.T2" title="Table 2 ‚Ä£ 5 Results ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, respectively.</p>
</div>
<div id="Ch0.S5.p3" class="ltx_para">
<p id="Ch0.S5.p3.4" class="ltx_p">In Table <a href="#Ch0.T1" title="Table 1 ‚Ä£ 5 Results ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the validation and test <math id="Ch0.S5.p3.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p3.1.m1.1a"><msub id="Ch0.S5.p3.1.m1.1.1" xref="Ch0.S5.p3.1.m1.1.1.cmml"><mi id="Ch0.S5.p3.1.m1.1.1.2" xref="Ch0.S5.p3.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.S5.p3.1.m1.1.1.3" xref="Ch0.S5.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p3.1.m1.1b"><apply id="Ch0.S5.p3.1.m1.1.1.cmml" xref="Ch0.S5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p3.1.m1.1.1.1.cmml" xref="Ch0.S5.p3.1.m1.1.1">subscript</csymbol><ci id="Ch0.S5.p3.1.m1.1.1.2.cmml" xref="Ch0.S5.p3.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p3.1.m1.1.1.3.cmml" xref="Ch0.S5.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p3.1.m1.1c">F_{1}</annotation></semantics></math>-scores and intent accuracies with regard to the best validation <math id="Ch0.S5.p3.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p3.2.m2.1a"><msub id="Ch0.S5.p3.2.m2.1.1" xref="Ch0.S5.p3.2.m2.1.1.cmml"><mi id="Ch0.S5.p3.2.m2.1.1.2" xref="Ch0.S5.p3.2.m2.1.1.2.cmml">F</mi><mn id="Ch0.S5.p3.2.m2.1.1.3" xref="Ch0.S5.p3.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p3.2.m2.1b"><apply id="Ch0.S5.p3.2.m2.1.1.cmml" xref="Ch0.S5.p3.2.m2.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p3.2.m2.1.1.1.cmml" xref="Ch0.S5.p3.2.m2.1.1">subscript</csymbol><ci id="Ch0.S5.p3.2.m2.1.1.2.cmml" xref="Ch0.S5.p3.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p3.2.m2.1.1.3.cmml" xref="Ch0.S5.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p3.2.m2.1c">F_{1}</annotation></semantics></math>-score of the multi-task learning approach with the <span id="Ch0.S5.p3.4.1" class="ltx_text ltx_font_italic">ATIS small</span> dataset is depicted.
The test <math id="Ch0.S5.p3.3.m3.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p3.3.m3.1a"><msub id="Ch0.S5.p3.3.m3.1.1" xref="Ch0.S5.p3.3.m3.1.1.cmml"><mi id="Ch0.S5.p3.3.m3.1.1.2" xref="Ch0.S5.p3.3.m3.1.1.2.cmml">F</mi><mn id="Ch0.S5.p3.3.m3.1.1.3" xref="Ch0.S5.p3.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p3.3.m3.1b"><apply id="Ch0.S5.p3.3.m3.1.1.cmml" xref="Ch0.S5.p3.3.m3.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p3.3.m3.1.1.1.cmml" xref="Ch0.S5.p3.3.m3.1.1">subscript</csymbol><ci id="Ch0.S5.p3.3.m3.1.1.2.cmml" xref="Ch0.S5.p3.3.m3.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p3.3.m3.1.1.3.cmml" xref="Ch0.S5.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p3.3.m3.1c">F_{1}</annotation></semantics></math>-score could be improved 2.32 percentage points with not fine-tuned multi-task learning with the <span id="Ch0.S5.p3.4.2" class="ltx_text ltx_font_italic">OpenSubtitles QA</span> dataset and 4.22 percentage points to 84.98‚Äâ% with the <span id="Ch0.S5.p3.4.3" class="ltx_text ltx_font_italic">OpenSubtitles dialog</span> dataset.
The test intent accuracies could be improved with not fine-tuned multi-task learning 5.60 and 4.93 percentage points, respectively.
For both out-of-domain datasets, fine-tuning did change the <math id="Ch0.S5.p3.4.m4.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p3.4.m4.1a"><msub id="Ch0.S5.p3.4.m4.1.1" xref="Ch0.S5.p3.4.m4.1.1.cmml"><mi id="Ch0.S5.p3.4.m4.1.1.2" xref="Ch0.S5.p3.4.m4.1.1.2.cmml">F</mi><mn id="Ch0.S5.p3.4.m4.1.1.3" xref="Ch0.S5.p3.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p3.4.m4.1b"><apply id="Ch0.S5.p3.4.m4.1.1.cmml" xref="Ch0.S5.p3.4.m4.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p3.4.m4.1.1.1.cmml" xref="Ch0.S5.p3.4.m4.1.1">subscript</csymbol><ci id="Ch0.S5.p3.4.m4.1.1.2.cmml" xref="Ch0.S5.p3.4.m4.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p3.4.m4.1.1.3.cmml" xref="Ch0.S5.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p3.4.m4.1c">F_{1}</annotation></semantics></math>-score only neglectable.</p>
</div>
<div id="Ch0.S5.p4" class="ltx_para">
<p id="Ch0.S5.p4.4" class="ltx_p">In Table <a href="#Ch0.T2" title="Table 2 ‚Ä£ 5 Results ‚Ä£ Multi-task learning to improve natural language understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the validation and test <math id="Ch0.S5.p4.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p4.1.m1.1a"><msub id="Ch0.S5.p4.1.m1.1.1" xref="Ch0.S5.p4.1.m1.1.1.cmml"><mi id="Ch0.S5.p4.1.m1.1.1.2" xref="Ch0.S5.p4.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.S5.p4.1.m1.1.1.3" xref="Ch0.S5.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p4.1.m1.1b"><apply id="Ch0.S5.p4.1.m1.1.1.cmml" xref="Ch0.S5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p4.1.m1.1.1.1.cmml" xref="Ch0.S5.p4.1.m1.1.1">subscript</csymbol><ci id="Ch0.S5.p4.1.m1.1.1.2.cmml" xref="Ch0.S5.p4.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p4.1.m1.1.1.3.cmml" xref="Ch0.S5.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p4.1.m1.1c">F_{1}</annotation></semantics></math>-scores and intent accuracies with regard to the best validation <math id="Ch0.S5.p4.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p4.2.m2.1a"><msub id="Ch0.S5.p4.2.m2.1.1" xref="Ch0.S5.p4.2.m2.1.1.cmml"><mi id="Ch0.S5.p4.2.m2.1.1.2" xref="Ch0.S5.p4.2.m2.1.1.2.cmml">F</mi><mn id="Ch0.S5.p4.2.m2.1.1.3" xref="Ch0.S5.p4.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p4.2.m2.1b"><apply id="Ch0.S5.p4.2.m2.1.1.cmml" xref="Ch0.S5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p4.2.m2.1.1.1.cmml" xref="Ch0.S5.p4.2.m2.1.1">subscript</csymbol><ci id="Ch0.S5.p4.2.m2.1.1.2.cmml" xref="Ch0.S5.p4.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p4.2.m2.1.1.3.cmml" xref="Ch0.S5.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p4.2.m2.1c">F_{1}</annotation></semantics></math>-score of the multi-task learning approach with the <span id="Ch0.S5.p4.4.1" class="ltx_text ltx_font_italic">ATIS medium</span> dataset is depicted.
The test <math id="Ch0.S5.p4.3.m3.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p4.3.m3.1a"><msub id="Ch0.S5.p4.3.m3.1.1" xref="Ch0.S5.p4.3.m3.1.1.cmml"><mi id="Ch0.S5.p4.3.m3.1.1.2" xref="Ch0.S5.p4.3.m3.1.1.2.cmml">F</mi><mn id="Ch0.S5.p4.3.m3.1.1.3" xref="Ch0.S5.p4.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p4.3.m3.1b"><apply id="Ch0.S5.p4.3.m3.1.1.cmml" xref="Ch0.S5.p4.3.m3.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p4.3.m3.1.1.1.cmml" xref="Ch0.S5.p4.3.m3.1.1">subscript</csymbol><ci id="Ch0.S5.p4.3.m3.1.1.2.cmml" xref="Ch0.S5.p4.3.m3.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p4.3.m3.1.1.3.cmml" xref="Ch0.S5.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p4.3.m3.1c">F_{1}</annotation></semantics></math>-score could be improved 0.52 percentage points with not fine-tuned multi-task learning with the <span id="Ch0.S5.p4.4.2" class="ltx_text ltx_font_italic">OpenSubtitles QA</span> dataset and 0.30 percentage points with the <span id="Ch0.S5.p4.4.3" class="ltx_text ltx_font_italic">OpenSubtitles dialog</span> dataset.
The test intent accuracies could be improved with not fine-tuned multi-task learning by 0.34 and 1.79 percentage points, respectively.
These improvements are not big, but the <math id="Ch0.S5.p4.4.m4.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S5.p4.4.m4.1a"><msub id="Ch0.S5.p4.4.m4.1.1" xref="Ch0.S5.p4.4.m4.1.1.cmml"><mi id="Ch0.S5.p4.4.m4.1.1.2" xref="Ch0.S5.p4.4.m4.1.1.2.cmml">F</mi><mn id="Ch0.S5.p4.4.m4.1.1.3" xref="Ch0.S5.p4.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.p4.4.m4.1b"><apply id="Ch0.S5.p4.4.m4.1.1.cmml" xref="Ch0.S5.p4.4.m4.1.1"><csymbol cd="ambiguous" id="Ch0.S5.p4.4.m4.1.1.1.cmml" xref="Ch0.S5.p4.4.m4.1.1">subscript</csymbol><ci id="Ch0.S5.p4.4.m4.1.1.2.cmml" xref="Ch0.S5.p4.4.m4.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S5.p4.4.m4.1.1.3.cmml" xref="Ch0.S5.p4.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.p4.4.m4.1c">F_{1}</annotation></semantics></math>-score of the multi-task learning with the <span id="Ch0.S5.p4.4.4" class="ltx_text ltx_font_italic">OpenSubtitles QA</span> dataset is only 0.13 percentage points below the results of the model trained on the complete real training data of the <span id="Ch0.S5.p4.4.5" class="ltx_text ltx_font_italic">ATIS real</span> dataset.</p>
</div>
<figure id="Ch0.T1" class="ltx_table">
<table id="Ch0.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Ch0.T1.2.3.1" class="ltx_tr">
<th id="Ch0.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">training dataset(s)</th>
<th id="Ch0.T1.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">model</th>
<td id="Ch0.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">validation (<span id="Ch0.T1.2.3.1.3.1" class="ltx_text ltx_font_italic">ATIS real</span>)</td>
<td id="Ch0.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2">test (<span id="Ch0.T1.2.3.1.4.1" class="ltx_text ltx_font_italic">ATIS real</span>)</td>
</tr>
<tr id="Ch0.T1.2.2" class="ltx_tr">
<th id="Ch0.T1.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="Ch0.T1.2.2.4" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="Ch0.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="Ch0.T1.1.1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.T1.1.1.1.m1.1a"><msub id="Ch0.T1.1.1.1.m1.1.1" xref="Ch0.T1.1.1.1.m1.1.1.cmml"><mi id="Ch0.T1.1.1.1.m1.1.1.2" xref="Ch0.T1.1.1.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.T1.1.1.1.m1.1.1.3" xref="Ch0.T1.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.T1.1.1.1.m1.1b"><apply id="Ch0.T1.1.1.1.m1.1.1.cmml" xref="Ch0.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.T1.1.1.1.m1.1.1.1.cmml" xref="Ch0.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="Ch0.T1.1.1.1.m1.1.1.2.cmml" xref="Ch0.T1.1.1.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.T1.1.1.1.m1.1.1.3.cmml" xref="Ch0.T1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.T1.1.1.1.m1.1c">F_{1}</annotation></semantics></math></td>
<td id="Ch0.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">intent acc</td>
<td id="Ch0.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="Ch0.T1.2.2.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.T1.2.2.2.m1.1a"><msub id="Ch0.T1.2.2.2.m1.1.1" xref="Ch0.T1.2.2.2.m1.1.1.cmml"><mi id="Ch0.T1.2.2.2.m1.1.1.2" xref="Ch0.T1.2.2.2.m1.1.1.2.cmml">F</mi><mn id="Ch0.T1.2.2.2.m1.1.1.3" xref="Ch0.T1.2.2.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.T1.2.2.2.m1.1b"><apply id="Ch0.T1.2.2.2.m1.1.1.cmml" xref="Ch0.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="Ch0.T1.2.2.2.m1.1.1.1.cmml" xref="Ch0.T1.2.2.2.m1.1.1">subscript</csymbol><ci id="Ch0.T1.2.2.2.m1.1.1.2.cmml" xref="Ch0.T1.2.2.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.T1.2.2.2.m1.1.1.3.cmml" xref="Ch0.T1.2.2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.T1.2.2.2.m1.1c">F_{1}</annotation></semantics></math></td>
<td id="Ch0.T1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">intent acc</td>
</tr>
<tr id="Ch0.T1.2.4.2" class="ltx_tr">
<th id="Ch0.T1.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Ch0.T1.2.4.2.1.1" class="ltx_text ltx_font_italic">ATIS small</span></th>
<th id="Ch0.T1.2.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">single-task baseline</th>
<td id="Ch0.T1.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">80.79</td>
<td id="Ch0.T1.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.00</td>
<td id="Ch0.T1.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t">80.76</td>
<td id="Ch0.T1.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t">82.64</td>
</tr>
<tr id="Ch0.T1.2.5.3" class="ltx_tr">
<th id="Ch0.T1.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="Ch0.T1.2.5.3.1.1" class="ltx_text"><span id="Ch0.T1.2.5.3.1.1.1" class="ltx_text ltx_font_italic">ATIS small</span> + <span id="Ch0.T1.2.5.3.1.1.2" class="ltx_text ltx_font_italic">OpenSubtitles QA</span></span></th>
<th id="Ch0.T1.2.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">shared encoder</th>
<td id="Ch0.T1.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">82.21</td>
<td id="Ch0.T1.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Ch0.T1.2.5.3.4.1" class="ltx_text ltx_font_bold">87.60</span></td>
<td id="Ch0.T1.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">83.08</td>
<td id="Ch0.T1.2.5.3.6" class="ltx_td ltx_align_center ltx_border_t">88.24</td>
</tr>
<tr id="Ch0.T1.2.6.4" class="ltx_tr">
<th id="Ch0.T1.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">shared encoder fine-tuned</th>
<td id="Ch0.T1.2.6.4.2" class="ltx_td ltx_align_center">82.46</td>
<td id="Ch0.T1.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r">87.00</td>
<td id="Ch0.T1.2.6.4.4" class="ltx_td ltx_align_center">83.06</td>
<td id="Ch0.T1.2.6.4.5" class="ltx_td ltx_align_center">87.68</td>
</tr>
<tr id="Ch0.T1.2.7.5" class="ltx_tr">
<th id="Ch0.T1.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="Ch0.T1.2.7.5.1.1" class="ltx_text"><span id="Ch0.T1.2.7.5.1.1.1" class="ltx_text ltx_font_italic">ATIS small</span> + <span id="Ch0.T1.2.7.5.1.1.2" class="ltx_text ltx_font_italic">OpenSubtitles dialog</span></span></th>
<th id="Ch0.T1.2.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">shared encoder</th>
<td id="Ch0.T1.2.7.5.3" class="ltx_td ltx_align_center ltx_border_t">82.11</td>
<td id="Ch0.T1.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.00</td>
<td id="Ch0.T1.2.7.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T1.2.7.5.5.1" class="ltx_text ltx_font_bold">84.98</span></td>
<td id="Ch0.T1.2.7.5.6" class="ltx_td ltx_align_center ltx_border_t">87.57</td>
</tr>
<tr id="Ch0.T1.2.8.6" class="ltx_tr">
<th id="Ch0.T1.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">shared encoder fine-tuned</th>
<td id="Ch0.T1.2.8.6.2" class="ltx_td ltx_align_center ltx_border_b"><span id="Ch0.T1.2.8.6.2.1" class="ltx_text ltx_font_bold">82.65</span></td>
<td id="Ch0.T1.2.8.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">83.80</td>
<td id="Ch0.T1.2.8.6.4" class="ltx_td ltx_align_center ltx_border_b">84.55</td>
<td id="Ch0.T1.2.8.6.5" class="ltx_td ltx_align_center ltx_border_b"><span id="Ch0.T1.2.8.6.5.1" class="ltx_text ltx_font_bold">88.80</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>results on the <span id="Ch0.T1.5.1" class="ltx_text ltx_font_italic">ATIS real</span> dataset of the systems trained with the <span id="Ch0.T1.6.2" class="ltx_text ltx_font_italic">ATIS small</span> dataset</figcaption>
</figure>
<figure id="Ch0.T2" class="ltx_table">
<table id="Ch0.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Ch0.T2.2.3.1" class="ltx_tr">
<th id="Ch0.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">training dataset(s)</th>
<th id="Ch0.T2.2.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="Ch0.T2.2.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.T2.2.3.1.2.1.1" class="ltx_p" style="width:68.3pt;">model</span>
</span>
</th>
<td id="Ch0.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">validation (<span id="Ch0.T2.2.3.1.3.1" class="ltx_text ltx_font_italic">ATIS real</span>)</td>
<td id="Ch0.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2">test (<span id="Ch0.T2.2.3.1.4.1" class="ltx_text ltx_font_italic">ATIS real</span>)</td>
</tr>
<tr id="Ch0.T2.2.2" class="ltx_tr">
<th id="Ch0.T2.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="Ch0.T2.2.2.4" class="ltx_td ltx_align_top ltx_th ltx_th_row ltx_border_r"></th>
<td id="Ch0.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="Ch0.T2.1.1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.T2.1.1.1.m1.1a"><msub id="Ch0.T2.1.1.1.m1.1.1" xref="Ch0.T2.1.1.1.m1.1.1.cmml"><mi id="Ch0.T2.1.1.1.m1.1.1.2" xref="Ch0.T2.1.1.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.T2.1.1.1.m1.1.1.3" xref="Ch0.T2.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.T2.1.1.1.m1.1b"><apply id="Ch0.T2.1.1.1.m1.1.1.cmml" xref="Ch0.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.T2.1.1.1.m1.1.1.1.cmml" xref="Ch0.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="Ch0.T2.1.1.1.m1.1.1.2.cmml" xref="Ch0.T2.1.1.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.T2.1.1.1.m1.1.1.3.cmml" xref="Ch0.T2.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.T2.1.1.1.m1.1c">F_{1}</annotation></semantics></math></td>
<td id="Ch0.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">intent acc</td>
<td id="Ch0.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="Ch0.T2.2.2.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.T2.2.2.2.m1.1a"><msub id="Ch0.T2.2.2.2.m1.1.1" xref="Ch0.T2.2.2.2.m1.1.1.cmml"><mi id="Ch0.T2.2.2.2.m1.1.1.2" xref="Ch0.T2.2.2.2.m1.1.1.2.cmml">F</mi><mn id="Ch0.T2.2.2.2.m1.1.1.3" xref="Ch0.T2.2.2.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.T2.2.2.2.m1.1b"><apply id="Ch0.T2.2.2.2.m1.1.1.cmml" xref="Ch0.T2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="Ch0.T2.2.2.2.m1.1.1.1.cmml" xref="Ch0.T2.2.2.2.m1.1.1">subscript</csymbol><ci id="Ch0.T2.2.2.2.m1.1.1.2.cmml" xref="Ch0.T2.2.2.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.T2.2.2.2.m1.1.1.3.cmml" xref="Ch0.T2.2.2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.T2.2.2.2.m1.1c">F_{1}</annotation></semantics></math></td>
<td id="Ch0.T2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">intent acc</td>
</tr>
<tr id="Ch0.T2.2.4.2" class="ltx_tr">
<th id="Ch0.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Ch0.T2.2.4.2.1.1" class="ltx_text ltx_font_italic">ATIS medium</span></th>
<th id="Ch0.T2.2.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="Ch0.T2.2.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.T2.2.4.2.2.1.1" class="ltx_p" style="width:68.3pt;">single-task baseline</span>
</span>
</th>
<td id="Ch0.T2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">93.96</td>
<td id="Ch0.T2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.40</td>
<td id="Ch0.T2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t">92.97</td>
<td id="Ch0.T2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t">94.96</td>
</tr>
<tr id="Ch0.T2.2.5.3" class="ltx_tr">
<th id="Ch0.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="Ch0.T2.2.5.3.1.1" class="ltx_text"><span id="Ch0.T2.2.5.3.1.1.1" class="ltx_text ltx_font_italic">ATIS medium</span> + <span id="Ch0.T2.2.5.3.1.1.2" class="ltx_text ltx_font_italic">OpenSubtitles QA</span></span></th>
<th id="Ch0.T2.2.5.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="Ch0.T2.2.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.T2.2.5.3.2.1.1" class="ltx_p" style="width:68.3pt;">shared encoder</span>
</span>
</th>
<td id="Ch0.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">93.80</td>
<td id="Ch0.T2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.40</td>
<td id="Ch0.T2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.2.5.3.5.1" class="ltx_text ltx_font_bold">93.49</span></td>
<td id="Ch0.T2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_t">95.30</td>
</tr>
<tr id="Ch0.T2.2.6.4" class="ltx_tr">
<th id="Ch0.T2.2.6.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r">
<span id="Ch0.T2.2.6.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.T2.2.6.4.1.1.1" class="ltx_p" style="width:68.3pt;">shared encoder fine-tuned</span>
</span>
</th>
<td id="Ch0.T2.2.6.4.2" class="ltx_td ltx_align_center"><span id="Ch0.T2.2.6.4.2.1" class="ltx_text ltx_font_bold">94.00</span></td>
<td id="Ch0.T2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="Ch0.T2.2.6.4.3.1" class="ltx_text ltx_font_bold">97.20</span></td>
<td id="Ch0.T2.2.6.4.4" class="ltx_td ltx_align_center">92.81</td>
<td id="Ch0.T2.2.6.4.5" class="ltx_td ltx_align_center">94.96</td>
</tr>
<tr id="Ch0.T2.2.7.5" class="ltx_tr">
<th id="Ch0.T2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="Ch0.T2.2.7.5.1.1" class="ltx_text"><span id="Ch0.T2.2.7.5.1.1.1" class="ltx_text ltx_font_italic">ATIS medium</span> + <span id="Ch0.T2.2.7.5.1.1.2" class="ltx_text ltx_font_italic">OpenSubtitles dialog</span></span></th>
<th id="Ch0.T2.2.7.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="Ch0.T2.2.7.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.T2.2.7.5.2.1.1" class="ltx_p" style="width:68.3pt;">shared encoder</span>
</span>
</th>
<td id="Ch0.T2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_t">93.74</td>
<td id="Ch0.T2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.40</td>
<td id="Ch0.T2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_t">93.27</td>
<td id="Ch0.T2.2.7.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.2.7.5.6.1" class="ltx_text ltx_font_bold">96.75</span></td>
</tr>
<tr id="Ch0.T2.2.8.6" class="ltx_tr">
<th id="Ch0.T2.2.8.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r">
<span id="Ch0.T2.2.8.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.T2.2.8.6.1.1.1" class="ltx_p" style="width:68.3pt;">shared encoder fine-tuned</span>
</span>
</th>
<td id="Ch0.T2.2.8.6.2" class="ltx_td ltx_align_center">93.88</td>
<td id="Ch0.T2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r">97.00</td>
<td id="Ch0.T2.2.8.6.4" class="ltx_td ltx_align_center">92.88</td>
<td id="Ch0.T2.2.8.6.5" class="ltx_td ltx_align_center">96.42</td>
</tr>
<tr id="Ch0.T2.2.9.7" class="ltx_tr">
<th id="Ch0.T2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="Ch0.T2.2.9.7.1.1" class="ltx_text ltx_font_italic">ATIS real</span></th>
<th id="Ch0.T2.2.9.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">
<span id="Ch0.T2.2.9.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch0.T2.2.9.7.2.1.1" class="ltx_p" style="width:68.3pt;">single-task trained on real data</span>
</span>
</th>
<td id="Ch0.T2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="Ch0.T2.2.9.7.3.1" class="ltx_text ltx_font_bold">95.97</span></td>
<td id="Ch0.T2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">96.80</td>
<td id="Ch0.T2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="Ch0.T2.2.9.7.5.1" class="ltx_text ltx_font_bold">93.62</span></td>
<td id="Ch0.T2.2.9.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">94.74</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>results on the <span id="Ch0.T2.5.1" class="ltx_text ltx_font_italic">ATIS real</span> dataset of the systems trained with the <span id="Ch0.T2.6.2" class="ltx_text ltx_font_italic">ATIS medium</span> dataset</figcaption>
</figure>
<figure id="Ch0.F3" class="ltx_figure"><img src="/html/1812.06876/assets/x2.png" id="Ch0.F3.g1" class="ltx_graphics ltx_img_landscape" width="276" height="160" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>validation and test <math id="Ch0.F3.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.F3.2.m1.1b"><msub id="Ch0.F3.2.m1.1.1" xref="Ch0.F3.2.m1.1.1.cmml"><mi id="Ch0.F3.2.m1.1.1.2" xref="Ch0.F3.2.m1.1.1.2.cmml">F</mi><mn id="Ch0.F3.2.m1.1.1.3" xref="Ch0.F3.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.F3.2.m1.1c"><apply id="Ch0.F3.2.m1.1.1.cmml" xref="Ch0.F3.2.m1.1.1"><csymbol cd="ambiguous" id="Ch0.F3.2.m1.1.1.1.cmml" xref="Ch0.F3.2.m1.1.1">subscript</csymbol><ci id="Ch0.F3.2.m1.1.1.2.cmml" xref="Ch0.F3.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.F3.2.m1.1.1.3.cmml" xref="Ch0.F3.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.F3.2.m1.1d">F_{1}</annotation></semantics></math>-score of the single-task baseline trained with the <span id="Ch0.F3.4.1" class="ltx_text ltx_font_italic">ATIS small</span> dataset</figcaption>
</figure>
<figure id="Ch0.F4" class="ltx_figure"><img src="/html/1812.06876/assets/x3.png" id="Ch0.F4.g1" class="ltx_graphics ltx_img_landscape" width="278" height="164" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>validation and test <math id="Ch0.F4.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.F4.2.m1.1b"><msub id="Ch0.F4.2.m1.1.1" xref="Ch0.F4.2.m1.1.1.cmml"><mi id="Ch0.F4.2.m1.1.1.2" xref="Ch0.F4.2.m1.1.1.2.cmml">F</mi><mn id="Ch0.F4.2.m1.1.1.3" xref="Ch0.F4.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.F4.2.m1.1c"><apply id="Ch0.F4.2.m1.1.1.cmml" xref="Ch0.F4.2.m1.1.1"><csymbol cd="ambiguous" id="Ch0.F4.2.m1.1.1.1.cmml" xref="Ch0.F4.2.m1.1.1">subscript</csymbol><ci id="Ch0.F4.2.m1.1.1.2.cmml" xref="Ch0.F4.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.F4.2.m1.1.1.3.cmml" xref="Ch0.F4.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.F4.2.m1.1d">F_{1}</annotation></semantics></math>-score of the single-task baseline trained with the <span id="Ch0.F4.4.1" class="ltx_text ltx_font_italic">ATIS medium</span> dataset</figcaption>
</figure>
</section>
<section id="Ch0.S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Further Work</h2>

<div id="Ch0.S6.p1" class="ltx_para">
<p id="Ch0.S6.p1.1" class="ltx_p">In this work, we evaluated whether the training of a synthetic dataset alongside with an out-of-domain dataset can improve the quality in comparison to train only with the synthetic dataset.
Although we optimized the model of the single-task learning baseline and not the model of the multi-task learning approach, we were able to increase the <math id="Ch0.S6.p1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="Ch0.S6.p1.1.m1.1a"><msub id="Ch0.S6.p1.1.m1.1.1" xref="Ch0.S6.p1.1.m1.1.1.cmml"><mi id="Ch0.S6.p1.1.m1.1.1.2" xref="Ch0.S6.p1.1.m1.1.1.2.cmml">F</mi><mn id="Ch0.S6.p1.1.m1.1.1.3" xref="Ch0.S6.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S6.p1.1.m1.1b"><apply id="Ch0.S6.p1.1.m1.1.1.cmml" xref="Ch0.S6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.S6.p1.1.m1.1.1.1.cmml" xref="Ch0.S6.p1.1.m1.1.1">subscript</csymbol><ci id="Ch0.S6.p1.1.m1.1.1.2.cmml" xref="Ch0.S6.p1.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="Ch0.S6.p1.1.m1.1.1.3.cmml" xref="Ch0.S6.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S6.p1.1.m1.1c">F_{1}</annotation></semantics></math>-score 4.22 percentage points to 84.98‚Äâ% for the smaller synthetic dataset (<span id="Ch0.S6.p1.1.1" class="ltx_text ltx_font_italic">ATIS small</span>).
For the bigger dataset (<span id="Ch0.S6.p1.1.2" class="ltx_text ltx_font_italic">ATIS medium</span>), we could not significantly improve the results, but the results are already in the near of the results of the model trained on the real data.
To improve the quality of dialog systems for these exist only strong under-resourced synthetic datasets is especially helpful because the better a system is, the more it encourages users to use it.
This is often an inexpensive way to collect data to log real user usage.
However, by collecting real user data, it is necessary to account privacy laws.</p>
</div>
<div id="Ch0.S6.p2" class="ltx_para">
<p id="Ch0.S6.p2.1" class="ltx_p">The problem with the <span id="Ch0.S6.p2.1.1" class="ltx_text ltx_font_italic">OpenSubtitles QA</span> dataset is, that the form question as source sequence and answer as target sequence differs from the form of the ATIS datasets.
The problem with the <span id="Ch0.S6.p2.1.2" class="ltx_text ltx_font_italic">OpenSubtitles dialog</span> dataset is that it is very noisy.
Responses do not often refer to the previous utterance.
In future work, it would be interesting to test other datasets or a combination of datasets whose form is better fitting or are less noisy, respectively.</p>
</div>
<div id="Ch0.S6.p3" class="ltx_para">
<p id="Ch0.S6.p3.1" class="ltx_p">We expect a further improvement of the multi-task learning approach by optimizing the parameters of our model in the multi-task learning approach.
However, this is very computation time intensive because the out-of-domain datasets have 14 million instances, and therefore, we leave it open for future work.</p>
</div>
<div id="Ch0.S6.p4" class="ltx_para">
<p id="Ch0.S6.p4.1" class="ltx_p">We evaluated the multi-task learning approach with the attention-based encoder-decoder model, but we also expect an improvement by the multi-task learning approach for other architectures, such as the transformer model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">VaswaniSPUJGKP2017 </a></cite>, which could be researched in future work.</p>
</div>
</section>
<section id="Ch0.Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Ch0.Sx1.p1" class="ltx_para">
<p id="Ch0.Sx1.p1.1" class="ltx_p">This work has been conducted in the SecondHands project which has received funding from the European Union‚Äôs Horizon 2020 Research and Innovation programme (call:H2020- ICT-2014-1, RIA) under grant agreement No 643950.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Ameixa, D., Coheur, L.: From subtitles to human interactions : introducing the
subtle corpus.

</span>
<span class="ltx_bibblock">Tech. rep., INESC-ID (2013)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly
learning to align and translate.

</span>
<span class="ltx_bibblock">In: Proceedings of the Third International Conference on Learning
Representations (ICLR) (2015)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
Bird, S., Klein, E., Loper, E.: Natural Language Processing with Python, 1st
edn.

</span>
<span class="ltx_bibblock">O‚ÄôReilly Media, Inc. (2009)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
Constantin, S., Niehues, J., Waibel, A.: An end-to-end goal-oriented dialog
system with a generative natural language response generation.

</span>
<span class="ltx_bibblock">In: Proceedings of the Ninth International Workshop on Spoken
Dialogue Systems (IWSDS) (2018)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
Hakkani-Tur, D., Tur, G., Celikyilmaz, A., Chen, Y.N., Gao, J., Deng, L., Wang,
Y.Y.: Multi-domain joint semantic frame parsing using bi-directional
rnn-lstm.

</span>
<span class="ltx_bibblock">In: Proceedings of The 17th Annual Meeting of the International
Speech Communication Association (Interspeech) (2016)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
Hazwani, R.A., Wahida, N., Shafikah, S.I., Ellyza, P.N., et¬†al.: Automatic
artificial data generator: Framework and implementation.

</span>
<span class="ltx_bibblock">In: Proceedings of the First International Conference on Information
and Communication Technology (ICICTM) (2016)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam : A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In: Proceedings of the Third International Conference on Learning
Representations (ICLR) (2015)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
Luong, M.T., Le, Q.V., Sutskever, I., Vinyals, O., Kaiser, L.: Multi-task
sequence to sequence learning.

</span>
<span class="ltx_bibblock">In: Proceedings of the Fourth International Conference on Learning
Representations (ICLR) (2016)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
Niehues, J., Cho, E.: Exploiting linguistic resources for neural machine
translation using multi-task learning.

</span>
<span class="ltx_bibblock">In: Proceedings of the Second Conference on Machine Translation
(WMT). Association for Computational Linguistics (2017)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
Pham, N.Q., Sperber, M., Salesky, E., Ha, T.L., Niehues, J., Waibel, A.:
Kit‚Äôs multilingual neural machine translation systems for iwslt 2017.

</span>
<span class="ltx_bibblock">In: Proceedings of the 14th International Workshop on Spoken Language
Translation (IWSLT) (2017)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
Price, P.J.: Evaluation of spoken language systems: The atis domain.

</span>
<span class="ltx_bibblock">In: Proceedings of the Workshop on Speech and Natural Language, HLT
‚Äô90, pp. 91‚Äì95. Association for Computational Linguistics (1990)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
Senellart, J.: English chatbot model with opennmt (2009).

</span>
<span class="ltx_bibblock">URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://forum.opennmt.net/t/english-chatbot-model-with-opennmt/184</span>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words
with subword units.

</span>
<span class="ltx_bibblock">In: Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (ACL) (2016)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
Serban, I., Lowe, R., Henderson, P., Charlin, L., Pineau, J.: A survey of
available corpora for building data-driven dialogue systems.

</span>
<span class="ltx_bibblock">D&amp;D <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">9</span>, 1‚Äì49 (2018)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
Serban, I., Sordoni, A., Lowe, R., Charlin, L., Pineau, J., Courville, A.C.,
Bengio, Y.: A hierarchical latent variable encoder-decoder model for
generating dialogues.

</span>
<span class="ltx_bibblock">In: Proceedings of the 31st AAAI conference (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
Tiedemann, J.: News from OPUS - A collection of multilingual parallel
corpora with tools and interfaces.

</span>
<span class="ltx_bibblock">In: Recent Advances in Natural Language Processing, vol.¬†V, pp.
237‚Äì248. John Benjamins, Amsterdam/Philadelphia (2009)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, L.u., Polosukhin, I.: Attention is all you need.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems 30, pp.
5998‚Äì6008. Curran Associates, Inc. (2017)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
Zhang, Z., Luo, P., Loy, C.C., Tang, X.: Facial landmark detection by deep
multi-task learning.

</span>
<span class="ltx_bibblock">In: Proceedings of the 13th European Conference on Computer Vision
(ECCV) (2014)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1812.06875" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1812.06876" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1812.06876">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1812.06876" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1812.06877" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 22:39:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
