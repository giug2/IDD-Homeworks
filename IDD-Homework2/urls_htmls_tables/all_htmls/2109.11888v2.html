<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.11888] Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text</title><meta property="og:description" content="Understanding robustness and sensitivity of BERT models predicting Alzheimer’s disease from text is important for both developing better classification models and for understanding their capabilities and limitations. I…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.11888">

<!--Generated on Sun Mar  3 23:40:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Robustness and Sensitivity of BERT Models
<br class="ltx_break">Predicting Alzheimer’s Disease from Text</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jekaterina Novikova 
<br class="ltx_break">Winterlight Labs / Toronto, Canada 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">jekaterina@winterlightlabs.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Understanding robustness and sensitivity of BERT models predicting Alzheimer’s disease from text is important for both developing better classification models and for understanding their capabilities and limitations. In this paper, we analyze how a controlled amount of desired and undesired text alterations impacts performance of BERT. We show that BERT is robust to natural linguistic variations in text. On the other hand, we show that BERT is not sensitive to removing clinically important information from text.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Alzheimer’s disease (AD) is a prevalent neurodegerative condition that inhibits cognitive abilities and impacts one’s language abilities. For example, cognitively impaired people tend to use more pronouns instead of nouns, and pause more often between sentences in narrative speech <cite class="ltx_cite ltx_citemacro_cite">Roark et al. (<a href="#bib.bib26" title="" class="ltx_ref">2011</a>)</cite>. This insight makes automatic detection possible. Machine learning (ML) classifiers can detect cognitive impairments given descriptive linguistic features or using pre-trained large language models <cite class="ltx_cite ltx_citemacro_cite">Balagopalan et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>); Zhu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">BERT is a model that achieves promising performance on a variety of tasks, including AD prediction from speech and language <cite class="ltx_cite ltx_citemacro_cite">Searle et al. (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>); Yuan et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>.
However, this promising performance may be fallacious, i.e. deep neural language models may learn pseudo patterns from training data to attain high performance on test sets <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Gururangan et al., <a href="#bib.bib16" title="" class="ltx_ref">2018</a>; Glockner et al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>; Tsuchiya, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>; Geva et al., <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>. Therefore, in order to be confident in the outcomes of BERT models classifying AD it is important to assess whether these models are robust to some natural noise that may be introduced in language. It is also important to know if BERT models are sensitive to the aspects that are considered to be important for recognizing cognitive impairment from human language.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we analyze robustness and sensitivity of BERT models in their ability to classify AD from text by analysing the effect of noise, introduced from artificial text perturbations, on the performance of the model. Some previous research was conducted on the impact of ASR-related noise on dementia detection <cite class="ltx_cite ltx_citemacro_cite">Balagopalan et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020b</a>)</cite>, as well as the effect of artificial text alterations on AD classification <cite class="ltx_cite ltx_citemacro_cite">Novikova et al. (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>. However, these previous studies only focus on conventional classification models, such as Random Forest and SVM. To the best of our knowledge, we are the first to analyse how the noise introduced by texts perturbations impact BERT models, in the domain of AD classification.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.2" class="ltx_p">We use the ADReSS Challenge dataset <cite class="ltx_cite ltx_citemacro_cite">Luz et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>, which consists of 156 speech samples and associated transcripts from non-AD (<math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">N</annotation></semantics></math>=78) and AD (<math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">N</annotation></semantics></math>=78) English-speaking participants. Speech is elicited from participants through the Cookie Theft picture from the Boston Diagnostic Aphasia exam <cite class="ltx_cite ltx_citemacro_cite">Goodglass et al. (<a href="#bib.bib14" title="" class="ltx_ref">2001</a>)</cite>.
In contrast to other datasets for AD detection such as DementiaBank’s English Pitt Corpus <cite class="ltx_cite ltx_citemacro_cite">Becker et al. (<a href="#bib.bib6" title="" class="ltx_ref">1994</a>)</cite>, the ADReSS challenge dataset is well balanced in terms of age and gender (Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Data ‣ 2 Methodology ‣ Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Another benefit of this dataset is its division into standard train and test sets that makes it easy to directly compare to the previous research in the area.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.2" class="ltx_block ltx_pruned_first">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">Table 1: </span>Basic characteristics of the patients in each group in the ADReSS challenge dataset.</figcaption>
<div id="S2.T1.2.p2" class="ltx_para ltx_noindent ltx_align_center">
<p id="S2.T1.2.p2.1" class="ltx_p"><span id="S2.T1.2.p2.1.1" class="ltx_text">
<span id="S2.T1.2.p2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:207.4pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S2.T1.2.p2.1.1.1.1" class="ltx_p"><span id="S2.T1.2.p2.1.1.1.1.1" class="ltx_text">
<span id="S2.T1.2.p2.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S2.T1.2.p2.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S2.T1.2.p2.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="S2.T1.2.p2.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column"></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_r"></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_colspan ltx_colspan_2"><span id="S2.T1.2.p2.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Class</span></span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S2.T1.2.p2.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_th ltx_th_column"></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_th ltx_th_column ltx_border_r"></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S2.T1.2.p2.1.1.1.1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">AD</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T1.2.p2.1.1.1.1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Non-AD</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S2.T1.2.p2.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S2.T1.2.p2.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S2.T1.2.p2.1.1.1.1.1.1.3.1.1.1" class="ltx_text">ADReSS</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S2.T1.2.p2.1.1.1.1.1.1.3.1.2.1" class="ltx_text">Train</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Male</span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24</span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">24</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S2.T1.2.p2.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_border_r">Female</span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">30</span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">30</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S2.T1.2.p2.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S2.T1.2.p2.1.1.1.1.1.1.5.3.1.1" class="ltx_text">ADReSS</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S2.T1.2.p2.1.1.1.1.1.1.5.3.2.1" class="ltx_text">Test</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Male</span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11</span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t">11</span></span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.6.4" class="ltx_tr">
<span id="S2.T1.2.p2.1.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_border_r">Female</span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">13</span>
<span id="S2.T1.2.p2.1.1.1.1.1.1.6.4.3" class="ltx_td ltx_align_center">13</span></span>
</span>
</span></span></span>
</span></span></span></p>
</div>
</div>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Multiple recent studies showed that BERT is a promising model achieving strong enough performance in detecting Alzheimer’s disease from transcribed speech <cite class="ltx_cite ltx_citemacro_cite">Searle et al. (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>); Yuan et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>); Balagopalan et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020a</a>, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>. Motivated by these results, we use a fine-tuned BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> model in this work. To leverage the language information encoded by BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>,
we add a linear layer mapping representations from the final layer of a pre-trained 12-layer BERT base<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://huggingface.co/bert-base-uncased</span></span></span> for the AD vs non-AD binary classification task. The transcript-level input to the model consists of transcribed utterances with corresponding start and separator special tokens for each utterance, following Liu <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">Liu and Lapata (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. A pooled embedding
summarizing information across all tokens in the transcript is used as the aggregate transcript representation, and passed to the classification layer <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>); Wolf et al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>.
This model is then fine-tuned on training data for AD detection. For hyperparameter tuning, we optimize the number of epochs to 10 by varying it from 1 to 12 during cross-validation.
Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a href="#bib.bib17" title="" class="ltx_ref">2014</a>)</cite> and warmup linear learning rate scheduling <cite class="ltx_cite ltx_citemacro_cite">Paszke et al. (<a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> are used, based on prior work on fine-tuning BERT  <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>); Wolf et al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Perturbation Approaches</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We used a variety of word-based augmentation approaches with the help of the nlpaug<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/makcedward/nlpaug , the Python library for generating synthetic textual and speech data.</span></span></span> library to generate perturbed versions of the test set of the ADReSS dataset for the experiments.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">Back-translation: </span>this augmentation technique proposed by <cite class="ltx_cite ltx_citemacro_citet">Sennrich et al. (<a href="#bib.bib30" title="" class="ltx_ref">2016</a>)</cite> leverages two translation models, one translating the source text from English to German and the other translating it back to English. Back-translated texts should maintain the semantics and basic syntactic structure of original texts and as such, robust model’s performance should not decrease because of this augmentation.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">Word substitution with synonyms: </span>following <cite class="ltx_cite ltx_citemacro_citet">Niu and Bansal (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>, we substitute a controlled varying amount of words in the transcript (10-90%) with their synonyms in order to maintain semantic meaning of the utterances. Synonyms are extracted from the NLTK WordNet corpus<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.nltk.org/howto/wordnet.html</span></span></span>. Replacing words with their synonyms should not affect the ability of a robust model to accurately distinguish between healthy and AD classes.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_italic">Embedding-based word substitution: </span>following <cite class="ltx_cite ltx_citemacro_citet">Alzantot et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>); Wang and Yang (<a href="#bib.bib34" title="" class="ltx_ref">2015</a>)</cite>, we use pre-trained word2vec embeddings to perform a KNN with cosine similarity search to find the similar word for replacement. We then substitute a varying subsets (from 10 to 90%) of the original transcripts with these replacements. We hypothesize that model performance can be affected by such augmentation stronger than by synonym replacement, although this effect should not be significant for a robust AD prediction model.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_italic">Removal of filled pauses: </span>we remove all the filled pauses (transcribed as <span id="S2.SS3.p5.1.2" class="ltx_text ltx_font_italic">um</span> and <span id="S2.SS3.p5.1.3" class="ltx_text ltx_font_italic">uh</span>) from the original texts. Previous literature highlights the importance of pauses in Alzheimer’s disease detection from speech <cite class="ltx_cite ltx_citemacro_cite">Calley et al. (<a href="#bib.bib7" title="" class="ltx_ref">2010</a>); Mack et al. (<a href="#bib.bib21" title="" class="ltx_ref">2013</a>); Seifart et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite>. Several authors report increases in AD detection performance by extracting acoustic features such as filled pause counts <cite class="ltx_cite ltx_citemacro_cite">Eyre et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>); Tóth et al. (<a href="#bib.bib31" title="" class="ltx_ref">2015</a>, <a href="#bib.bib32" title="" class="ltx_ref">2018</a>); Pistono et al. (<a href="#bib.bib25" title="" class="ltx_ref">2016</a>)</cite>. Removal of such information should make it more difficult for a model to accurately detect AD-related samples of text.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_italic">Removal of information units: </span> multiple studies of AD narratives in picture description tasks have reported the importance of information units in detecting cognitive impairment <cite class="ltx_cite ltx_citemacro_cite">Fraser et al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>); Croisile et al. (<a href="#bib.bib8" title="" class="ltx_ref">1996</a>)</cite>. Following <cite class="ltx_cite ltx_citemacro_cite">Croisile et al. (<a href="#bib.bib8" title="" class="ltx_ref">1996</a>)</cite>, we define four key categories of information units - subjects, locations, objects, and actions - and delete them from the original transcripts to generate perturbed versions of the test set. Such a removal should make it more difficult for a model to distinguish between healthy and AD samples.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The results of testing the fine-tuned BERT model on the variety of perturbed versions of the ADReSS test set show that the performance changes differently depending on different types of text alterations (Table <a href="#S3.T2" title="Table 2 ‣ 3 Results ‣ Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Removing tokens of filled pauses does not change the performance at all. Removing information units, however, decreases the accuracy of the model by 4-8%, depending on the type of the information unit. Back translation and substitutions of words with their synonyms or otherwise similar words also negatively affect performance of the model, although have the opposite effect on recall vs specificity.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<p id="S3.T2.1" class="ltx_p"><span id="S3.T2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S3.T2.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:346.3pt;height:579.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.1.1.1.1" class="ltx_text">
<span id="S3.T2.1.1.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S3.T2.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.2.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Type of</span></span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.2.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">perturbation</span></span></span>
</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_left">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.3.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Level of</span></span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.3.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">perturbation</span></span></span>
</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Acc</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">F1</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Prec</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Rec</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.1.1.1.1.1.1.8.1" class="ltx_text ltx_font_bold">Spec</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="W_{1}" display="inline"><semantics id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">W</mi><mn id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">𝑊</ci><cn type="integer" id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.1.1.1.1.1.m1.1c">W_{1}</annotation></semantics></math></span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.1.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Original</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.1.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">transcript</span></span>
</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_tt">NA</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_tt">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_tt">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_tt">0.86</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_tt">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_tt">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_left ltx_border_tt">NA</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.1.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Deleting</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.1.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">filled pauses</span></span>
</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">All</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t">0.86</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_left ltx_border_t">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_left ltx_border_t">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.3.2.8" class="ltx_td ltx_align_left ltx_border_t">2.40</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_rowspan ltx_rowspan_5"><span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.2" class="ltx_text">
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.2.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Deleting</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.2.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">information</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.2.1.3" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">units</span></span>
</span></span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_left">All</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_left">0.84</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_left">0.74</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_left">0.96</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_left">0.67</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.4.3.8" class="ltx_td ltx_align_left">2.87</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.5.4" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_left">Action</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_left">0.77</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_left">0.80</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_left">0.71</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.5.4.5" class="ltx_td ltx_align_left">0.92</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.5.4.6" class="ltx_td ltx_align_left">0.63</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.5.4.7" class="ltx_td ltx_align_left">2.87</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.6.5" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.6.5.1" class="ltx_td ltx_align_left">Location</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.6.5.2" class="ltx_td ltx_align_left">0.77</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.6.5.3" class="ltx_td ltx_align_left">0.78</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.6.5.4" class="ltx_td ltx_align_left">0.74</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.6.5.5" class="ltx_td ltx_align_left">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.6.5.6" class="ltx_td ltx_align_left">0.71</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.6.5.7" class="ltx_td ltx_align_left">0.77</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.7.6" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.7.6.1" class="ltx_td ltx_align_left">Object</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.7.6.2" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.7.6.3" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.7.6.4" class="ltx_td ltx_align_left">0.69</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.7.6.5" class="ltx_td ltx_align_left">0.92</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.7.6.6" class="ltx_td ltx_align_left">0.58</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.7.6.7" class="ltx_td ltx_align_left">2.45</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.8.7" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.8.7.1" class="ltx_td ltx_align_left">Subject</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.8.7.2" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.8.7.3" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.8.7.4" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.8.7.5" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.8.7.6" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.8.7.7" class="ltx_td ltx_align_left">2.13</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.1.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Back</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.1.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">translation</span></span>
</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_t">Eng &lt;-&gt;DE</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_t">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_t">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.5" class="ltx_td ltx_align_left ltx_border_t">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.6" class="ltx_td ltx_align_left ltx_border_t">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.7" class="ltx_td ltx_align_left ltx_border_t">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.9.8.8" class="ltx_td ltx_align_left ltx_border_t">6.02</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_rowspan ltx_rowspan_9"><span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2" class="ltx_text">
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Substituting</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">with the most</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.3" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">similar word</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.4" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">(via word2vec</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.5" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.1.2.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left">embeddings)</span></span>
</span></span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.2" class="ltx_td ltx_align_left">10%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.3" class="ltx_td ltx_align_left">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.4" class="ltx_td ltx_align_left">0.84</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.5" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.6" class="ltx_td ltx_align_left">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.7" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.10.9.8" class="ltx_td ltx_align_left">5.63</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.11.10" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.11.10.1" class="ltx_td ltx_align_left">20%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.11.10.2" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.11.10.3" class="ltx_td ltx_align_left">0.82</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.11.10.4" class="ltx_td ltx_align_left">0.78</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.11.10.5" class="ltx_td ltx_align_left">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.11.10.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.11.10.7" class="ltx_td ltx_align_left">6.23</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.12.11" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.12.11.1" class="ltx_td ltx_align_left">30%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.12.11.2" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.12.11.3" class="ltx_td ltx_align_left">0.82</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.12.11.4" class="ltx_td ltx_align_left">0.80</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.12.11.5" class="ltx_td ltx_align_left">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.12.11.6" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.12.11.7" class="ltx_td ltx_align_left">6.23</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.13.12" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.13.12.1" class="ltx_td ltx_align_left">40%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.13.12.2" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.13.12.3" class="ltx_td ltx_align_left">0.82</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.13.12.4" class="ltx_td ltx_align_left">0.78</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.13.12.5" class="ltx_td ltx_align_left">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.13.12.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.13.12.7" class="ltx_td ltx_align_left">6.32</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.14.13" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.14.13.1" class="ltx_td ltx_align_left">50%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.14.13.2" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.14.13.3" class="ltx_td ltx_align_left">0.80</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.14.13.4" class="ltx_td ltx_align_left">0.86</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.14.13.5" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.14.13.6" class="ltx_td ltx_align_left">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.14.13.7" class="ltx_td ltx_align_left">6.20</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.15.14" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.15.14.1" class="ltx_td ltx_align_left">60%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.15.14.2" class="ltx_td ltx_align_left">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.15.14.3" class="ltx_td ltx_align_left">0.84</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.15.14.4" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.15.14.5" class="ltx_td ltx_align_left">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.15.14.6" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.15.14.7" class="ltx_td ltx_align_left">6.10</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.16.15" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.16.15.1" class="ltx_td ltx_align_left">70%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.16.15.2" class="ltx_td ltx_align_left">0.71</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.16.15.3" class="ltx_td ltx_align_left">0.70</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.16.15.4" class="ltx_td ltx_align_left">0.73</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.16.15.5" class="ltx_td ltx_align_left">0.67</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.16.15.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.16.15.7" class="ltx_td ltx_align_left">6.32</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.17.16" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.17.16.1" class="ltx_td ltx_align_left">80%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.17.16.2" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.17.16.3" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.17.16.4" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.17.16.5" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.17.16.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.17.16.7" class="ltx_td ltx_align_left">6.35</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.18.17" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.18.17.1" class="ltx_td ltx_align_left">90%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.18.17.2" class="ltx_td ltx_align_left">0.77</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.18.17.3" class="ltx_td ltx_align_left">0.78</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.18.17.4" class="ltx_td ltx_align_left">0.76</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.18.17.5" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.18.17.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.18.17.7" class="ltx_td ltx_align_left">6.26</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1" class="ltx_td ltx_align_left ltx_rowspan ltx_rowspan_9"><span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2" class="ltx_text">
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Substituting</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">synonyms</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1.3" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">(via</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1.4" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.1.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">WordNet)</span></span>
</span></span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.2" class="ltx_td ltx_align_left">10%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.3" class="ltx_td ltx_align_left">0.77</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.4" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.5" class="ltx_td ltx_align_left">0.72</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.6" class="ltx_td ltx_align_left">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.7" class="ltx_td ltx_align_left">0.67</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.19.18.8" class="ltx_td ltx_align_left">3.55</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.20.19" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.20.19.1" class="ltx_td ltx_align_left">20%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.20.19.2" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.20.19.3" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.20.19.4" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.20.19.5" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.20.19.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.20.19.7" class="ltx_td ltx_align_left">3.97</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.21.20" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.21.20.1" class="ltx_td ltx_align_left">30%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.21.20.2" class="ltx_td ltx_align_left">0.77</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.21.20.3" class="ltx_td ltx_align_left">0.78</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.21.20.4" class="ltx_td ltx_align_left">0.76</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.21.20.5" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.21.20.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.21.20.7" class="ltx_td ltx_align_left">4.12</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.22.21" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.22.21.1" class="ltx_td ltx_align_left">40%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.22.21.2" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.22.21.3" class="ltx_td ltx_align_left">0.77</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.22.21.4" class="ltx_td ltx_align_left">0.71</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.22.21.5" class="ltx_td ltx_align_left">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.22.21.6" class="ltx_td ltx_align_left">0.67</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.22.21.7" class="ltx_td ltx_align_left">4.01</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.23.22" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.23.22.1" class="ltx_td ltx_align_left">50%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.23.22.2" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.23.22.3" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.23.22.4" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.23.22.5" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.23.22.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.23.22.7" class="ltx_td ltx_align_left">3.86</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.24.23" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.24.23.1" class="ltx_td ltx_align_left">60%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.24.23.2" class="ltx_td ltx_align_left">0.71</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.24.23.3" class="ltx_td ltx_align_left">0.70</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.24.23.4" class="ltx_td ltx_align_left">0.73</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.24.23.5" class="ltx_td ltx_align_left">0.67</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.24.23.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.24.23.7" class="ltx_td ltx_align_left">4.33</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.25.24" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.25.24.1" class="ltx_td ltx_align_left">70%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.25.24.2" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.25.24.3" class="ltx_td ltx_align_left">0.82</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.25.24.4" class="ltx_td ltx_align_left">0.80</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.25.24.5" class="ltx_td ltx_align_left">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.25.24.6" class="ltx_td ltx_align_left">0.79</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.25.24.7" class="ltx_td ltx_align_left">3.98</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.26.25" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.26.25.1" class="ltx_td ltx_align_left">80%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.26.25.2" class="ltx_td ltx_align_left">0.81</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.26.25.3" class="ltx_td ltx_align_left">0.82</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.26.25.4" class="ltx_td ltx_align_left">0.78</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.26.25.5" class="ltx_td ltx_align_left">0.88</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.26.25.6" class="ltx_td ltx_align_left">0.75</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.26.25.7" class="ltx_td ltx_align_left">4.08</span></span>
<span id="S3.T2.1.1.1.1.1.1.1.1.27.26" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1.1.27.26.1" class="ltx_td ltx_align_left">90%</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.27.26.2" class="ltx_td ltx_align_left">0.77</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.27.26.3" class="ltx_td ltx_align_left">0.78</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.27.26.4" class="ltx_td ltx_align_left">0.74</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.27.26.5" class="ltx_td ltx_align_left">0.83</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.27.26.6" class="ltx_td ltx_align_left">0.71</span>
<span id="S3.T2.1.1.1.1.1.1.1.1.27.26.7" class="ltx_td ltx_align_left">4.16</span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of the fine-tuned BERT model and similarity between original and perturbed texts (Wasserstein distance <math id="S3.T2.3.m1.1" class="ltx_Math" alttext="W_{1}" display="inline"><semantics id="S3.T2.3.m1.1b"><msub id="S3.T2.3.m1.1.1" xref="S3.T2.3.m1.1.1.cmml"><mi id="S3.T2.3.m1.1.1.2" xref="S3.T2.3.m1.1.1.2.cmml">W</mi><mn id="S3.T2.3.m1.1.1.3" xref="S3.T2.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T2.3.m1.1c"><apply id="S3.T2.3.m1.1.1.cmml" xref="S3.T2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.3.m1.1.1.1.cmml" xref="S3.T2.3.m1.1.1">subscript</csymbol><ci id="S3.T2.3.m1.1.1.2.cmml" xref="S3.T2.3.m1.1.1.2">𝑊</ci><cn type="integer" id="S3.T2.3.m1.1.1.3.cmml" xref="S3.T2.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.m1.1d">W_{1}</annotation></semantics></math>).</figcaption>
</figure>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2109.11888/assets/images/synonyms_sex.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Differences in error rate between genders, by class. Here, M means ’male’ and F means ’female’.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Change in Classification Performance</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p"><span id="S4.SS1.p1.2.1" class="ltx_text ltx_font_bold">Undesired change:</span> As we have mentioned in Section <a href="#S2.SS3" title="2.3 Perturbation Approaches ‣ 2 Methodology ‣ Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, some types of text alterations, such as back translation, synonym substitution and embedding-based substitution, represent natural noise that can occur in user-generated texts. Changes in classification performance are not desired in this case because we want the model to be robust towards multiple paraphrases and use of synonyms. Our fine-tuned BERT model behaves in a robust way in terms of F1 and accuracy scores when up to 40% of words are substituted with similar words based on word2vec embeddings (the F1 score decreases by 1% and accuracy - by 2%, both changes not significant with McNemar’s test <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="p&gt;" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">&gt;</mo><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><gt id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></gt><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝑝</ci><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">p&gt;</annotation></semantics></math>0.65) or synonyms (a decrease in 4-8% in F1 and accuracy, both changes not significant with McNemar’s test <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="p&gt;" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">&gt;</mo><mi id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><gt id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></gt><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">𝑝</ci><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">p&gt;</annotation></semantics></math>0.15).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Interestingly, recall and specificity values seem to show the opposite results here - specificity decreases by up to 21%, while recall stays on the same level or even increases by 4-9%. Substituting words with their similar alternatives or synonyms may be understood as increasing the level of lexical complexity, i.e. the model is introduced with multiple, maybe less usual, ways to express the same meaning. It is known that lexical complexity is one of the prominent ways that allow detecting cognitive impairment from language. Thus such an implicit way to change the lexical complexity of texts seems to help the BERT model in reducing the amount of true positive errors while detecting AD. However, more than 40% of such substitutions may make the original texts less realistic, which, as we see from the results, substantially reduces model performance, including reducing recall level.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<p id="S4.T3.1" class="ltx_p"><span id="S4.T3.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T3.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:264.7pt;height:268.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T3.1.1.1.1.1.1" class="ltx_p"><span id="S4.T3.1.1.1.1.1.1.1" class="ltx_text">
<span id="S4.T3.1.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S4.T3.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_rowspan ltx_rowspan_2"><span id="S4.T3.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text">
<span id="S4.T3.1.1.1.1.1.1.1.1.1.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.1.1.1.1.1.1.1.2.1.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.1.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T3.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Type of</span></span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.1.2.1.1.2" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.1.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S4.T3.1.1.1.1.1.1.1.1.1.2.1.1.2.1.1" class="ltx_text ltx_font_bold">perturbation</span></span></span>
</span></span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_colspan ltx_colspan_5"><span id="S4.T3.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Correlation between <math id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="W_{1}" display="inline"><semantics id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">W</mi><mn id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">𝑊</ci><cn type="integer" id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.1.1.1.1.1.1.m1.1c">W_{1}</annotation></semantics></math> and</span></span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S4.T3.1.1.1.1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Acc</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S4.T3.1.1.1.1.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">F1</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S4.T3.1.1.1.1.1.1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Prec</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S4.T3.1.1.1.1.1.1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Rec</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S4.T3.1.1.1.1.1.1.1.1.2.1.5.1" class="ltx_text ltx_font_bold">Spec</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.1.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Deleting</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.1.1.2" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">informational</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.1.1.3" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">units</span></span>
</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">0.24</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_right ltx_border_tt">0.56</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_tt">-0.07</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_right ltx_border_tt">0.52</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_right ltx_border_tt">-0.21</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Substituting</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.2" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">with the most</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.3" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">similar word</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.4" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">(via word2vec</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.5" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.1.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left">embeddings)</span></span>
</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">0.77</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_right ltx_border_t">0.89</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_right ltx_border_t">0.89</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_right ltx_border_t">0.87</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_right ltx_border_t">-0.91</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Substituting</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1.2" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">synonyms</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1.3" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">(via</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1.4" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">WordNet)</span></span>
</span></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">-0.26</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_right ltx_border_t">-0.37</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_t">0.10</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_right ltx_border_t">-0.45</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_right ltx_border_t">0.41</span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Correlation between similarity and performance metrics.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Desired change:</span> Other types of text alterations, such as removal of information units or tokens representing filled pauses, are not considered to be natural noise. As these characteristics of language are clinically important in detecting cognitive impairment, the models should be sensitive to such changes in language. Our results show that the fine-tuned BERT model ignores completely removal of filled pauses. Performance of the model decreases by 3-5% of F1 as a reaction to deleting different types of information units but this change is not significant (McNemar’s test <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="p&gt;" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">&gt;</mo><mi id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><gt id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></gt><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝑝</ci><csymbol cd="latexml" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">p&gt;</annotation></semantics></math>0.18). This change in performance is similar to the change caused by synonym substitution and shows that the model is not sensitive enough to removal of clinically relevant information.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">This leads us to inspect how each type of alterations affects distributional shift from the original text and whether there is a relation between the shift and model’s performance.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Correlation with Distributional Shift</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Inspired by <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite> where hidden activations were used to detect out-of-distribution samples for images and by <cite class="ltx_cite ltx_citemacro_citet">Rychener et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> applied this method to text, we used sentence embeddings produced by BERT to quantify the distributional shift among the original test set and its perturbed versions. To understand the level of dissimilarity among the versions of test sets, we calculated the 1-Wasserstein distance (“earth mover distance", <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="W_{1}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">W</mi><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑊</ci><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">W_{1}</annotation></semantics></math>), since it measures the minimum cost to turn one probability distribution into another (Table <a href="#S3.T2" title="Table 2 ‣ 3 Results ‣ Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">W<sub id="S4.SS2.p2.2.1" class="ltx_sub">1</sub> values show that deletion of information units and filled pauses has the lowest effect on the original text, while word2vec-based substitution shifts the distribution further away from the original. Correlation between performance metrics and W<sub id="S4.SS2.p2.2.2" class="ltx_sub">1</sub> is not consistent across different types of text alterations (Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Change in Classification Performance ‣ 4 Discussion ‣ Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>): it is strongly positive between F1 and accuracy scores in case of embedding-based substitutions (0.77 and 0.89), positive but less strong (0.24 and 0.56) in case of deleting clinically relevant information, and negative in case of synonym substitution (-0.26 and -0.37). These inconsistencies imply that the lack of sensitivity in BERT models is caused by intrinsic model reasons rather than distributional shift of test data.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Differences Based on Gender</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In order to understand if BERT performance is biased towards any gender, we analyse the rate of error within each gender group and how the error rate is changing with additional amount of text alterations. The results of this analysis do not reveal any differences between males and females within the class of AD data samples. However when it comes to the non-AD class, BERT tends to misclassify the text samples produced by female subjects significantly more often than those produced by males, across all types of text alterations. The effect is pronounced the most in the case of synonym substitution (see Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Results ‣ Robustness and Sensitivity of BERT Models Predicting Alzheimer’s Disease from Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), where the error rate of classifying female-produced samples is 14% higher on average than that of male-produced samples<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Also significantly different based on t-test, <math id="footnote4.m1.1" class="ltx_Math" alttext="p&lt;" display="inline"><semantics id="footnote4.m1.1b"><mrow id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml"><mi id="footnote4.m1.1.1.2" xref="footnote4.m1.1.1.2.cmml">p</mi><mo id="footnote4.m1.1.1.1" xref="footnote4.m1.1.1.1.cmml">&lt;</mo><mi id="footnote4.m1.1.1.3" xref="footnote4.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><apply id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1"><lt id="footnote4.m1.1.1.1.cmml" xref="footnote4.m1.1.1.1"></lt><ci id="footnote4.m1.1.1.2.cmml" xref="footnote4.m1.1.1.2">𝑝</ci><csymbol cd="latexml" id="footnote4.m1.1.1.3.cmml" xref="footnote4.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">p&lt;</annotation></semantics></math>0.005.</span></span></span>. Given that both training and test sets of the dataset are well balanced, such a difference implies the pre-trained BERT model is gender-biased and this bias is not eliminated during fine-tuning.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations and Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we analysed how the controlled amount of desired and undesired text alterations impacts BERT classification performance in the domain of AD detection. We showed that BERT is robust enough to the natural linguistic noise, although the model is biased towards text samples of non-AD females. On the other hand, BERT is not sensitive enough to removal of clinically relevant information. This lack of sensitivity is not directly influenced by distributional shift.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">This work is a first step towards investigating BERT models’ robustness and sensitivity in the domain of AD detection from text, and we only report empirical results of one BERT model fine-tuned and tested on one dataset. More work should be done in this area to ensure the results are widely generalizable within the domain. Textual data used in our experiment represent transcribed conversational speech and as such, may be quite different from other types of texts, e.g. written text. Future work is necessary to see if the effect of text alterations remain the same with other types of text.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alzantot et al. (2018)</span>
<span class="ltx_bibblock">
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava,
and Kai-Wei Chang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1316" title="" class="ltx_ref ltx_href">Generating natural
language adversarial examples</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2890–2896, Brussels, Belgium.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balagopalan et al. (2021)</span>
<span class="ltx_bibblock">
Aparna Balagopalan, Benjamin Eyre, Jessica Robin, Frank Rudzicz, and Jekaterina
Novikova. 2021.

</span>
<span class="ltx_bibblock">Comparing pre-trained and feature-based models for prediction of
alzheimer’s disease based on speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Frontiers in aging neuroscience</em>, 13:189.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balagopalan et al. (2020a)</span>
<span class="ltx_bibblock">
Aparna Balagopalan, Benjamin Eyre, Frank Rudzicz, and Jekaterina Novikova.
2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2020-2557" title="" class="ltx_ref ltx_href">To BERT or
not to BERT: Comparing Speech and Language-Based Approaches for Alzheimer’s
Disease Detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, pages 2167–2171.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balagopalan et al. (2018)</span>
<span class="ltx_bibblock">
Aparna Balagopalan, Jekaterina Novikova, Frank Rudzicz, and Marzyeh Ghassemi.
2018.

</span>
<span class="ltx_bibblock">The effect of heterogeneous data for Alzheimer’s disease detection
from speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.12254</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balagopalan et al. (2020b)</span>
<span class="ltx_bibblock">
Aparna Balagopalan, Ksenia Shkaruta, and Jekaterina Novikova.
2020b.

</span>
<span class="ltx_bibblock">Impact of asr on alzheimer’s disease detection: All errors are
equal, but deletions are more equal than others.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Sixth Workshop on Noisy User-generated
Text (W-NUT 2020)</em>, pages 159–164.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Becker et al. (1994)</span>
<span class="ltx_bibblock">
James T Becker, François Boiler, Oscar L Lopez, Judith Saxton, and
Karen L McGonigle. 1994.

</span>
<span class="ltx_bibblock">The natural history of Alzheimer’s disease: description of study
cohort and accuracy of diagnosis.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Archives of Neurology</em>, 51(6):585–594.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calley et al. (2010)</span>
<span class="ltx_bibblock">
Clifford S Calley, Gail D Tillman, Kyle Womack, Patricia Moore, John Hart Jr,
and Michael A Kraut. 2010.

</span>
<span class="ltx_bibblock">Subjective report of word-finding and memory deficits in normal aging
and dementia.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Cognitive and behavioral neurology: official journal of the
Society for Behavioral and Cognitive Neurology</em>, 23(3):185.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Croisile et al. (1996)</span>
<span class="ltx_bibblock">
Bernard Croisile, Bernadette Ska, Marie-Josee Brabant, Annick Duchene, Yves
Lepage, Gilbert Aimard, and Marc Trillet. 1996.

</span>
<span class="ltx_bibblock">Comparative study of oral and written picture description in patients
with alzheimer’s disease.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Brain and language</em>, 53(1):1–19.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Volume 1 (Long and
Short Papers)</em>, pages 4171–4186.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eyre et al. (2020)</span>
<span class="ltx_bibblock">
Ben Eyre, Aparna Balagopalan, and Jekaterina Novikova. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.wnut-1.25" title="" class="ltx_ref ltx_href">Fantastic
features and where to find them: Detecting cognitive impairment with a
subsequence classification guided approach</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Sixth Workshop on Noisy User-generated
Text (W-NUT 2020)</em>, pages 193–199, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraser et al. (2016)</span>
<span class="ltx_bibblock">
Kathleen C Fraser, Jed A Meltzer, and Frank Rudzicz. 2016.

</span>
<span class="ltx_bibblock">Linguistic features identify alzheimer’s disease in narrative
speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Journal of Alzheimer’s Disease</em>, 49(2):407–422.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et al. (2019)</span>
<span class="ltx_bibblock">
Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1107" title="" class="ltx_ref ltx_href">Are we modeling the
task or the annotator? an investigation of annotator bias in natural language
understanding datasets</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1161–1166, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glockner et al. (2018)</span>
<span class="ltx_bibblock">
Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-2103" title="" class="ltx_ref ltx_href">Breaking NLI systems
with sentences that require simple lexical inferences</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers)</em>, pages 650–655,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodglass et al. (2001)</span>
<span class="ltx_bibblock">
Harold Goodglass, Edith Kaplan, and Barbara Barresi. 2001.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">BDAE-3: Boston Diagnostic Aphasia Examination–Third
Edition</em>.

</span>
<span class="ltx_bibblock">Lippincott Williams &amp; Wilkins Philadelphia, PA.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2019)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra,
and Devi Parikh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.670" title="" class="ltx_ref ltx_href">Making the V in VQA
Matter: Elevating the Role of Image Understanding in Visual Question
Answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 127(4):398–414.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et al. (2018)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman,
and Noah A. Smith. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-2017" title="" class="ltx_ref ltx_href">Annotation artifacts in
natural language inference data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</em>, pages 107–112, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba. 2014.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2018)</span>
<span class="ltx_bibblock">
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018.

</span>
<span class="ltx_bibblock">A simple unified framework for detecting out-of-distribution samples
and adversarial attacks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference on Neural
Information Processing Systems</em>, pages 7167–7177.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Lapata (2019)</span>
<span class="ltx_bibblock">
Yang Liu and Mirella Lapata. 2019.

</span>
<span class="ltx_bibblock">Text summarization with pretrained encoders.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3721–3731.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luz et al. (2020)</span>
<span class="ltx_bibblock">
Saturnino Luz, Fasih Haider, Sofia de la Fuente, Davida Fromm, and Brian
MacWhinney. 2020.

</span>
<span class="ltx_bibblock">Alzheimer’s Dementia Recognition through Spontaneous Speech: The
ADReSS Challenge.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mack et al. (2013)</span>
<span class="ltx_bibblock">
Jennifer Mack, Aya Meltzer-Asscher, Sarah Dove, Sandra Weintraub, Marsel
Mesulam, and Cynthia K Thompson. 2013.

</span>
<span class="ltx_bibblock">Word-finding pauses in primary progressive aphasia (ppa): Effects of
lexical category.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu and Bansal (2018)</span>
<span class="ltx_bibblock">
Tong Niu and Mohit Bansal. 2018.

</span>
<span class="ltx_bibblock">Adversarial over-sensitivity and over-stability strategies for
dialogue models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd Conference on Computational Natural
Language Learning</em>, pages 486–496.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Novikova et al. (2019)</span>
<span class="ltx_bibblock">
Jekaterina Novikova, Aparna Balagopalan, Ksenia Shkaruta, and Frank Rudzicz.
2019.

</span>
<span class="ltx_bibblock">Lexical features are more vulnerable, syntactic features have more
predictive power.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">W-NUT 2019</em>, page 431.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
2019.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
8024–8035.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pistono et al. (2016)</span>
<span class="ltx_bibblock">
Aurélie Pistono, Mélanie Jucla, Emmanuel J Barbeau, Laure Saint-Aubert,
Béatrice Lemesle, Benjamin Calvet, Barbara Köpke, Michèle Puel,
and Jérémie Pariente. 2016.

</span>
<span class="ltx_bibblock">Pauses during autobiographical discourse reflect episodic memory
processes in early alzheimer’s disease.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Journal of Alzheimer’s Disease</em>, 50(3):687–698.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roark et al. (2011)</span>
<span class="ltx_bibblock">
Brian Roark, Margaret Mitchell, John-Paul Hosom, Kristy Hollingshead, and
Jeffrey Kaye. 2011.

</span>
<span class="ltx_bibblock">Spoken language derived measures for detecting mild cognitive
impairment.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on audio, speech, and language processing</em>,
19(7):2081–2090.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rychener et al. (2020)</span>
<span class="ltx_bibblock">
Yves Rychener, Xavier Renard, Djamé Seddah, Pascal Frossard, and Marcin
Detyniecki. 2020.

</span>
<span class="ltx_bibblock">Sentence-based model agnostic nlp interpretability.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.13189</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Searle et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Searle, Zina Ibrahim, and Richard Dobson. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2020-2729" title="" class="ltx_ref ltx_href">Comparing
Natural Language Processing Techniques for Alzheimer’s Dementia Prediction
in Spontaneous Speech</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, pages 2192–2196.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seifart et al. (2018)</span>
<span class="ltx_bibblock">
Frank Seifart, Jan Strunk, Swintha Danielsen, Iren Hartmann, Brigitte
Pakendorf, Søren Wichmann, Alena Witzlack-Makarevich, Nivja H de Jong, and
Balthasar Bickel. 2018.

</span>
<span class="ltx_bibblock">Nouns slow down speech across structurally and culturally diverse
languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>,
115(22):5720–5725.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-1009" title="" class="ltx_ref ltx_href">Improving neural
machine translation models with monolingual data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, Berlin,
Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tóth et al. (2015)</span>
<span class="ltx_bibblock">
Laszló Tóth, Gábor Gosztolya, Veronika Vincze, Ildikó Hoffmann,
Gréta Szatlóczki, Edit Biró, Fruzsina Zsura, Magdolna
Pákáski, and János Kálmán. 2015.

</span>
<span class="ltx_bibblock">Automatic detection of mild cognitive impairment from spontaneous
speech using asr.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Sixteenth Annual Conference of the International Speech
Communication Association</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tóth et al. (2018)</span>
<span class="ltx_bibblock">
László Tóth, Ildikó Hoffmann, Gábor Gosztolya, Veronika
Vincze, Gréta Szatlóczki, Zoltán Bánréti, Magdolna
Pákáski, and János Kálmán. 2018.

</span>
<span class="ltx_bibblock">A speech recognition-based solution for the automatic detection of
mild cognitive impairment from spontaneous speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Current Alzheimer Research</em>, 15(2):130–138.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsuchiya (2018)</span>
<span class="ltx_bibblock">
Masatoshi Tsuchiya. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/L18-1239" title="" class="ltx_ref ltx_href">Performance impact
caused by hidden bias of training data for recognizing textual entailment</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018)</em>, Miyazaki, Japan. European
Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Yang (2015)</span>
<span class="ltx_bibblock">
William Yang Wang and Diyi Yang. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D15-1306" title="" class="ltx_ref ltx_href">That’s so
annoying!!!: A lexical and frame-semantic embedding based data augmentation
approach to automatic categorization of annoying behaviors using #petpeeve
tweets</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2557–2563, Lisbon, Portugal. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault,
R Louf, M Funtowicz, et al. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ArXiv, abs/1910.03771</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2020)</span>
<span class="ltx_bibblock">
Jiahong Yuan, Yuchen Bian, Xingyu Cai, Jiaji Huang, Zheng Ye, and Kenneth
Church. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2020-2516" title="" class="ltx_ref ltx_href">Disfluencies
and Fine-Tuning Pre-Trained Language Models for Detection of Alzheimer’s
Disease</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, pages 2162–2166.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2019)</span>
<span class="ltx_bibblock">
Zining Zhu, Jekaterina Novikova, and Frank Rudzicz. 2019.

</span>
<span class="ltx_bibblock">Detecting cognitive impairments by agreeing on interpretations of
linguistic features.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 1431–1441.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.11887" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.11888" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.11888">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.11888" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.11889" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar  3 23:40:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
