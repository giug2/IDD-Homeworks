<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2208.11512] FedOS: using open-set learning to stabilize training in federated learning</title><meta property="og:description" content="Federated Learning is a recent approach to train statistical models on distributed datasets without violating privacy constraints. The data locality principle is preserved by sharing the model instead of the data betweâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FedOS: using open-set learning to stabilize training in federated learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FedOS: using open-set learning to stabilize training in federated learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2208.11512">

<!--Generated on Wed Mar 13 17:24:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FedOS: using open-set learning to stabilize training in federated learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohamad Mohamad
<br class="ltx_break"><span class="ltx_ref ltx_nolink ltx_font_typewriter ltx_ref_self" style="font-size:90%;">mohamad_m.2@live.com</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julian Neubert
<br class="ltx_break"><span class="ltx_ref ltx_nolink ltx_font_typewriter ltx_ref_self" style="font-size:90%;">julian.neubert@gmx.de</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Juan Segundo Argayo
<br class="ltx_break"><span class="ltx_ref ltx_nolink ltx_font_typewriter ltx_ref_self" style="font-size:90%;">jargayo@gmail.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning is a recent approach to train statistical models on distributed datasets without violating privacy constraints. The data locality principle is preserved by sharing the model instead of the data between clients and the server. This brings many advantages but also poses new challenges. In this report, we explore this new research area and perform several experiments to deepen our understanding of what these challenges are and how different problem settings affect the performance of the final model. Finally, we present a novel approach to one of these challenges and compare it to other methods found in literature. The code is available on our github <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/mohamad-m2/federated-learning" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mohamad-m2/federated-learning</a></span></span></span>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Nowadays, phones are the primary and sometimes the only computing device for many people. As such, the interaction with them and their powerful sensors leave an unprecedented amount of rich data, much of it private in nature. Cameras, microphones, GPS, and tactile sensors are all examples of this. In the field of Big Data and Machine Learning applications, this data regains importance as models learned with it hold the promise of greatly improving usability by powering more intelligent applications, but the sensitive nature of the data means there are risks and responsibilities to storing it in a centralized location <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning (FL) is a privacy-preserving framework, originally introduced by McMahan <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, for training models from decentralized user data residing on devices at the edge, the one mentioned before. Models are trained iteratively across many federated rounds. For each round, every participating device (a.k.a. client), receives an initial model from a central server, performs stochastic gradient descent (SGD) on its local training data, and sends back the gradients. The server then aggregates all gradients from the participating clients and updates the starting model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In FL, raw client data is never shared with the server or other clients. This distinguishes FL from traditional distributed optimization and requires dealing with heterogeneous data</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">lr</th>
<th id="S2.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.2</th>
<th id="S2.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.15</th>
<th id="S2.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.1</th>
<th id="S2.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.08</th>
<th id="S2.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.06</th>
<th id="S2.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.05</th>
<th id="S2.T1.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.04</th>
<th id="S2.T1.2.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.02</th>
<th id="S2.T1.2.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.01</th>
<th id="S2.T1.2.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">0.005</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2.1" class="ltx_tr">
<th id="S2.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">raw</th>
<td id="S2.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.67</td>
<td id="S2.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.7</td>
<td id="S2.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.26</td>
<td id="S2.T1.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.75</td>
<td id="S2.T1.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.98</td>
<td id="S2.T1.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.61</td>
<td id="S2.T1.2.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.46</td>
<td id="S2.T1.2.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.13</td>
<td id="S2.T1.2.2.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">43.83</td>
<td id="S2.T1.2.2.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.92</td>
</tr>
<tr id="S2.T1.2.3.2" class="ltx_tr">
<th id="S2.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">scaled</th>
<td id="S2.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r">51.08</td>
<td id="S2.T1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r">49.88</td>
<td id="S2.T1.2.3.2.4" class="ltx_td ltx_align_center ltx_border_r">53.33</td>
<td id="S2.T1.2.3.2.5" class="ltx_td ltx_align_center ltx_border_r">52.29</td>
<td id="S2.T1.2.3.2.6" class="ltx_td ltx_align_center ltx_border_r">48.09</td>
<td id="S2.T1.2.3.2.7" class="ltx_td ltx_align_center ltx_border_r">50.77</td>
<td id="S2.T1.2.3.2.8" class="ltx_td ltx_align_center ltx_border_r">51.27</td>
<td id="S2.T1.2.3.2.9" class="ltx_td ltx_align_center ltx_border_r">43.80</td>
<td id="S2.T1.2.3.2.10" class="ltx_td ltx_align_center ltx_border_r">37.21</td>
<td id="S2.T1.2.3.2.11" class="ltx_td ltx_align_center ltx_border_r">28.71</td>
</tr>
<tr id="S2.T1.2.4.3" class="ltx_tr">
<th id="S2.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">standardized</th>
<td id="S2.T1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">53.42</td>
<td id="S2.T1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">54.07</td>
<td id="S2.T1.2.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">54.27</td>
<td id="S2.T1.2.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">53.53</td>
<td id="S2.T1.2.4.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">53.85</td>
<td id="S2.T1.2.4.3.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">52.32</td>
<td id="S2.T1.2.4.3.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">52.74</td>
<td id="S2.T1.2.4.3.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">47.54</td>
<td id="S2.T1.2.4.3.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">40.56</td>
<td id="S2.T1.2.4.3.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">34.81</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">Hyper-parameter tuning on centralized setting. Learning rate and pre-processing.</span></figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">FedProx.</span> There are two key challenges in Federated Learning that differentiate it from traditional distributed optimization: (1) significant variability in terms of the systemâ€™s characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). FedProx (Tian Li <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>) can be viewed as a generalization and re-parametrization of FedAvg. In this work, the authors have theoretically proven convergence guarantees when learning over data from non-identical distributions (satistical heterogeneity), while also adhering to the constraints each device-level system has by allowing each participating device to perform a variable amount of work (system heterogeneity). They demonstrate that in highly heterogeneous settings, FedProx has significantly more stable and accurate convergence behavior relative to FedAvg, improving absolute test accuracy by 22% on average.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.10" class="ltx_p"><span id="S2.p2.10.1" class="ltx_text ltx_font_bold">FedIR.</span> As well as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, the authors of this paper (Hsu <em id="S2.p2.10.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.10.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>) recognized that one of the challenges in terms of data diversity relies on the fact that data at the source is far from independent and identically distributed (IID). In addition, they also found that differing quantities of data are typically available at each device (imbalance). In their work, they characterized the effect these real-world data distributions have on distributed learning, using FedAvg as a benchmark. They proposed two new algorithms, FedVC and FedIR. The latter one addresses the issue of the non-identical class distribution shift present in the federated clients. If we consider a target distribution <math id="S2.p2.1.m1.2" class="ltx_Math" alttext="p(x,y)" display="inline"><semantics id="S2.p2.1.m1.2a"><mrow id="S2.p2.1.m1.2.3" xref="S2.p2.1.m1.2.3.cmml"><mi id="S2.p2.1.m1.2.3.2" xref="S2.p2.1.m1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.p2.1.m1.2.3.1" xref="S2.p2.1.m1.2.3.1.cmml">â€‹</mo><mrow id="S2.p2.1.m1.2.3.3.2" xref="S2.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.p2.1.m1.2.3.3.2.1" xref="S2.p2.1.m1.2.3.3.1.cmml">(</mo><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">x</mi><mo id="S2.p2.1.m1.2.3.3.2.2" xref="S2.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S2.p2.1.m1.2.2" xref="S2.p2.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S2.p2.1.m1.2.3.3.2.3" xref="S2.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.2b"><apply id="S2.p2.1.m1.2.3.cmml" xref="S2.p2.1.m1.2.3"><times id="S2.p2.1.m1.2.3.1.cmml" xref="S2.p2.1.m1.2.3.1"></times><ci id="S2.p2.1.m1.2.3.2.cmml" xref="S2.p2.1.m1.2.3.2">ğ‘</ci><interval closure="open" id="S2.p2.1.m1.2.3.3.1.cmml" xref="S2.p2.1.m1.2.3.3.2"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">ğ‘¥</ci><ci id="S2.p2.1.m1.2.2.cmml" xref="S2.p2.1.m1.2.2">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.2c">p(x,y)</annotation></semantics></math> of images <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">x</annotation></semantics></math> and class labels <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">y</annotation></semantics></math> on which a model is supposed to perform well, and a predefined loss function <math id="S2.p2.4.m4.2" class="ltx_Math" alttext="l(x,y)" display="inline"><semantics id="S2.p2.4.m4.2a"><mrow id="S2.p2.4.m4.2.3" xref="S2.p2.4.m4.2.3.cmml"><mi id="S2.p2.4.m4.2.3.2" xref="S2.p2.4.m4.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p2.4.m4.2.3.1" xref="S2.p2.4.m4.2.3.1.cmml">â€‹</mo><mrow id="S2.p2.4.m4.2.3.3.2" xref="S2.p2.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S2.p2.4.m4.2.3.3.2.1" xref="S2.p2.4.m4.2.3.3.1.cmml">(</mo><mi id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">x</mi><mo id="S2.p2.4.m4.2.3.3.2.2" xref="S2.p2.4.m4.2.3.3.1.cmml">,</mo><mi id="S2.p2.4.m4.2.2" xref="S2.p2.4.m4.2.2.cmml">y</mi><mo stretchy="false" id="S2.p2.4.m4.2.3.3.2.3" xref="S2.p2.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.2b"><apply id="S2.p2.4.m4.2.3.cmml" xref="S2.p2.4.m4.2.3"><times id="S2.p2.4.m4.2.3.1.cmml" xref="S2.p2.4.m4.2.3.1"></times><ci id="S2.p2.4.m4.2.3.2.cmml" xref="S2.p2.4.m4.2.3.2">ğ‘™</ci><interval closure="open" id="S2.p2.4.m4.2.3.3.1.cmml" xref="S2.p2.4.m4.2.3.3.2"><ci id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">ğ‘¥</ci><ci id="S2.p2.4.m4.2.2.cmml" xref="S2.p2.4.m4.2.2">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.2c">l(x,y)</annotation></semantics></math>, the objective of learning is to minimize the expected loss <math id="S2.p2.5.m5.3" class="ltx_Math" alttext="E_{p}[l(x,y)]" display="inline"><semantics id="S2.p2.5.m5.3a"><mrow id="S2.p2.5.m5.3.3" xref="S2.p2.5.m5.3.3.cmml"><msub id="S2.p2.5.m5.3.3.3" xref="S2.p2.5.m5.3.3.3.cmml"><mi id="S2.p2.5.m5.3.3.3.2" xref="S2.p2.5.m5.3.3.3.2.cmml">E</mi><mi id="S2.p2.5.m5.3.3.3.3" xref="S2.p2.5.m5.3.3.3.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S2.p2.5.m5.3.3.2" xref="S2.p2.5.m5.3.3.2.cmml">â€‹</mo><mrow id="S2.p2.5.m5.3.3.1.1" xref="S2.p2.5.m5.3.3.1.2.cmml"><mo stretchy="false" id="S2.p2.5.m5.3.3.1.1.2" xref="S2.p2.5.m5.3.3.1.2.1.cmml">[</mo><mrow id="S2.p2.5.m5.3.3.1.1.1" xref="S2.p2.5.m5.3.3.1.1.1.cmml"><mi id="S2.p2.5.m5.3.3.1.1.1.2" xref="S2.p2.5.m5.3.3.1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p2.5.m5.3.3.1.1.1.1" xref="S2.p2.5.m5.3.3.1.1.1.1.cmml">â€‹</mo><mrow id="S2.p2.5.m5.3.3.1.1.1.3.2" xref="S2.p2.5.m5.3.3.1.1.1.3.1.cmml"><mo stretchy="false" id="S2.p2.5.m5.3.3.1.1.1.3.2.1" xref="S2.p2.5.m5.3.3.1.1.1.3.1.cmml">(</mo><mi id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml">x</mi><mo id="S2.p2.5.m5.3.3.1.1.1.3.2.2" xref="S2.p2.5.m5.3.3.1.1.1.3.1.cmml">,</mo><mi id="S2.p2.5.m5.2.2" xref="S2.p2.5.m5.2.2.cmml">y</mi><mo stretchy="false" id="S2.p2.5.m5.3.3.1.1.1.3.2.3" xref="S2.p2.5.m5.3.3.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.p2.5.m5.3.3.1.1.3" xref="S2.p2.5.m5.3.3.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.3b"><apply id="S2.p2.5.m5.3.3.cmml" xref="S2.p2.5.m5.3.3"><times id="S2.p2.5.m5.3.3.2.cmml" xref="S2.p2.5.m5.3.3.2"></times><apply id="S2.p2.5.m5.3.3.3.cmml" xref="S2.p2.5.m5.3.3.3"><csymbol cd="ambiguous" id="S2.p2.5.m5.3.3.3.1.cmml" xref="S2.p2.5.m5.3.3.3">subscript</csymbol><ci id="S2.p2.5.m5.3.3.3.2.cmml" xref="S2.p2.5.m5.3.3.3.2">ğ¸</ci><ci id="S2.p2.5.m5.3.3.3.3.cmml" xref="S2.p2.5.m5.3.3.3.3">ğ‘</ci></apply><apply id="S2.p2.5.m5.3.3.1.2.cmml" xref="S2.p2.5.m5.3.3.1.1"><csymbol cd="latexml" id="S2.p2.5.m5.3.3.1.2.1.cmml" xref="S2.p2.5.m5.3.3.1.1.2">delimited-[]</csymbol><apply id="S2.p2.5.m5.3.3.1.1.1.cmml" xref="S2.p2.5.m5.3.3.1.1.1"><times id="S2.p2.5.m5.3.3.1.1.1.1.cmml" xref="S2.p2.5.m5.3.3.1.1.1.1"></times><ci id="S2.p2.5.m5.3.3.1.1.1.2.cmml" xref="S2.p2.5.m5.3.3.1.1.1.2">ğ‘™</ci><interval closure="open" id="S2.p2.5.m5.3.3.1.1.1.3.1.cmml" xref="S2.p2.5.m5.3.3.1.1.1.3.2"><ci id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1">ğ‘¥</ci><ci id="S2.p2.5.m5.2.2.cmml" xref="S2.p2.5.m5.2.2">ğ‘¦</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.3c">E_{p}[l(x,y)]</annotation></semantics></math> with respect to the target distribution <math id="S2.p2.6.m6.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S2.p2.6.m6.1a"><mi id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><ci id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">p</annotation></semantics></math>. Instead, training examples on a federated client <math id="S2.p2.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p2.7.m7.1a"><mi id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><ci id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">k</annotation></semantics></math> are sampled from a client-specific distribution <math id="S2.p2.8.m8.2" class="ltx_Math" alttext="q_{k}(x,y)" display="inline"><semantics id="S2.p2.8.m8.2a"><mrow id="S2.p2.8.m8.2.3" xref="S2.p2.8.m8.2.3.cmml"><msub id="S2.p2.8.m8.2.3.2" xref="S2.p2.8.m8.2.3.2.cmml"><mi id="S2.p2.8.m8.2.3.2.2" xref="S2.p2.8.m8.2.3.2.2.cmml">q</mi><mi id="S2.p2.8.m8.2.3.2.3" xref="S2.p2.8.m8.2.3.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.p2.8.m8.2.3.1" xref="S2.p2.8.m8.2.3.1.cmml">â€‹</mo><mrow id="S2.p2.8.m8.2.3.3.2" xref="S2.p2.8.m8.2.3.3.1.cmml"><mo stretchy="false" id="S2.p2.8.m8.2.3.3.2.1" xref="S2.p2.8.m8.2.3.3.1.cmml">(</mo><mi id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml">x</mi><mo id="S2.p2.8.m8.2.3.3.2.2" xref="S2.p2.8.m8.2.3.3.1.cmml">,</mo><mi id="S2.p2.8.m8.2.2" xref="S2.p2.8.m8.2.2.cmml">y</mi><mo stretchy="false" id="S2.p2.8.m8.2.3.3.2.3" xref="S2.p2.8.m8.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.2b"><apply id="S2.p2.8.m8.2.3.cmml" xref="S2.p2.8.m8.2.3"><times id="S2.p2.8.m8.2.3.1.cmml" xref="S2.p2.8.m8.2.3.1"></times><apply id="S2.p2.8.m8.2.3.2.cmml" xref="S2.p2.8.m8.2.3.2"><csymbol cd="ambiguous" id="S2.p2.8.m8.2.3.2.1.cmml" xref="S2.p2.8.m8.2.3.2">subscript</csymbol><ci id="S2.p2.8.m8.2.3.2.2.cmml" xref="S2.p2.8.m8.2.3.2.2">ğ‘</ci><ci id="S2.p2.8.m8.2.3.2.3.cmml" xref="S2.p2.8.m8.2.3.2.3">ğ‘˜</ci></apply><interval closure="open" id="S2.p2.8.m8.2.3.3.1.cmml" xref="S2.p2.8.m8.2.3.3.2"><ci id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1">ğ‘¥</ci><ci id="S2.p2.8.m8.2.2.cmml" xref="S2.p2.8.m8.2.2">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.2c">q_{k}(x,y)</annotation></semantics></math>. This implies that the empirical loss being optimized on every client is a <span id="S2.p2.10.4" class="ltx_text ltx_font_italic">biased</span> estimator of the loss with respect to the target distribution, since <math id="S2.p2.9.m9.6" class="ltx_Math" alttext="E_{q_{k}}[l(x,y)]\neq E_{p}[l(x,y)]" display="inline"><semantics id="S2.p2.9.m9.6a"><mrow id="S2.p2.9.m9.6.6" xref="S2.p2.9.m9.6.6.cmml"><mrow id="S2.p2.9.m9.5.5.1" xref="S2.p2.9.m9.5.5.1.cmml"><msub id="S2.p2.9.m9.5.5.1.3" xref="S2.p2.9.m9.5.5.1.3.cmml"><mi id="S2.p2.9.m9.5.5.1.3.2" xref="S2.p2.9.m9.5.5.1.3.2.cmml">E</mi><msub id="S2.p2.9.m9.5.5.1.3.3" xref="S2.p2.9.m9.5.5.1.3.3.cmml"><mi id="S2.p2.9.m9.5.5.1.3.3.2" xref="S2.p2.9.m9.5.5.1.3.3.2.cmml">q</mi><mi id="S2.p2.9.m9.5.5.1.3.3.3" xref="S2.p2.9.m9.5.5.1.3.3.3.cmml">k</mi></msub></msub><mo lspace="0em" rspace="0em" id="S2.p2.9.m9.5.5.1.2" xref="S2.p2.9.m9.5.5.1.2.cmml">â€‹</mo><mrow id="S2.p2.9.m9.5.5.1.1.1" xref="S2.p2.9.m9.5.5.1.1.2.cmml"><mo stretchy="false" id="S2.p2.9.m9.5.5.1.1.1.2" xref="S2.p2.9.m9.5.5.1.1.2.1.cmml">[</mo><mrow id="S2.p2.9.m9.5.5.1.1.1.1" xref="S2.p2.9.m9.5.5.1.1.1.1.cmml"><mi id="S2.p2.9.m9.5.5.1.1.1.1.2" xref="S2.p2.9.m9.5.5.1.1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p2.9.m9.5.5.1.1.1.1.1" xref="S2.p2.9.m9.5.5.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.p2.9.m9.5.5.1.1.1.1.3.2" xref="S2.p2.9.m9.5.5.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S2.p2.9.m9.5.5.1.1.1.1.3.2.1" xref="S2.p2.9.m9.5.5.1.1.1.1.3.1.cmml">(</mo><mi id="S2.p2.9.m9.1.1" xref="S2.p2.9.m9.1.1.cmml">x</mi><mo id="S2.p2.9.m9.5.5.1.1.1.1.3.2.2" xref="S2.p2.9.m9.5.5.1.1.1.1.3.1.cmml">,</mo><mi id="S2.p2.9.m9.2.2" xref="S2.p2.9.m9.2.2.cmml">y</mi><mo stretchy="false" id="S2.p2.9.m9.5.5.1.1.1.1.3.2.3" xref="S2.p2.9.m9.5.5.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.p2.9.m9.5.5.1.1.1.3" xref="S2.p2.9.m9.5.5.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S2.p2.9.m9.6.6.3" xref="S2.p2.9.m9.6.6.3.cmml">â‰ </mo><mrow id="S2.p2.9.m9.6.6.2" xref="S2.p2.9.m9.6.6.2.cmml"><msub id="S2.p2.9.m9.6.6.2.3" xref="S2.p2.9.m9.6.6.2.3.cmml"><mi id="S2.p2.9.m9.6.6.2.3.2" xref="S2.p2.9.m9.6.6.2.3.2.cmml">E</mi><mi id="S2.p2.9.m9.6.6.2.3.3" xref="S2.p2.9.m9.6.6.2.3.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S2.p2.9.m9.6.6.2.2" xref="S2.p2.9.m9.6.6.2.2.cmml">â€‹</mo><mrow id="S2.p2.9.m9.6.6.2.1.1" xref="S2.p2.9.m9.6.6.2.1.2.cmml"><mo stretchy="false" id="S2.p2.9.m9.6.6.2.1.1.2" xref="S2.p2.9.m9.6.6.2.1.2.1.cmml">[</mo><mrow id="S2.p2.9.m9.6.6.2.1.1.1" xref="S2.p2.9.m9.6.6.2.1.1.1.cmml"><mi id="S2.p2.9.m9.6.6.2.1.1.1.2" xref="S2.p2.9.m9.6.6.2.1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p2.9.m9.6.6.2.1.1.1.1" xref="S2.p2.9.m9.6.6.2.1.1.1.1.cmml">â€‹</mo><mrow id="S2.p2.9.m9.6.6.2.1.1.1.3.2" xref="S2.p2.9.m9.6.6.2.1.1.1.3.1.cmml"><mo stretchy="false" id="S2.p2.9.m9.6.6.2.1.1.1.3.2.1" xref="S2.p2.9.m9.6.6.2.1.1.1.3.1.cmml">(</mo><mi id="S2.p2.9.m9.3.3" xref="S2.p2.9.m9.3.3.cmml">x</mi><mo id="S2.p2.9.m9.6.6.2.1.1.1.3.2.2" xref="S2.p2.9.m9.6.6.2.1.1.1.3.1.cmml">,</mo><mi id="S2.p2.9.m9.4.4" xref="S2.p2.9.m9.4.4.cmml">y</mi><mo stretchy="false" id="S2.p2.9.m9.6.6.2.1.1.1.3.2.3" xref="S2.p2.9.m9.6.6.2.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.p2.9.m9.6.6.2.1.1.3" xref="S2.p2.9.m9.6.6.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.9.m9.6b"><apply id="S2.p2.9.m9.6.6.cmml" xref="S2.p2.9.m9.6.6"><neq id="S2.p2.9.m9.6.6.3.cmml" xref="S2.p2.9.m9.6.6.3"></neq><apply id="S2.p2.9.m9.5.5.1.cmml" xref="S2.p2.9.m9.5.5.1"><times id="S2.p2.9.m9.5.5.1.2.cmml" xref="S2.p2.9.m9.5.5.1.2"></times><apply id="S2.p2.9.m9.5.5.1.3.cmml" xref="S2.p2.9.m9.5.5.1.3"><csymbol cd="ambiguous" id="S2.p2.9.m9.5.5.1.3.1.cmml" xref="S2.p2.9.m9.5.5.1.3">subscript</csymbol><ci id="S2.p2.9.m9.5.5.1.3.2.cmml" xref="S2.p2.9.m9.5.5.1.3.2">ğ¸</ci><apply id="S2.p2.9.m9.5.5.1.3.3.cmml" xref="S2.p2.9.m9.5.5.1.3.3"><csymbol cd="ambiguous" id="S2.p2.9.m9.5.5.1.3.3.1.cmml" xref="S2.p2.9.m9.5.5.1.3.3">subscript</csymbol><ci id="S2.p2.9.m9.5.5.1.3.3.2.cmml" xref="S2.p2.9.m9.5.5.1.3.3.2">ğ‘</ci><ci id="S2.p2.9.m9.5.5.1.3.3.3.cmml" xref="S2.p2.9.m9.5.5.1.3.3.3">ğ‘˜</ci></apply></apply><apply id="S2.p2.9.m9.5.5.1.1.2.cmml" xref="S2.p2.9.m9.5.5.1.1.1"><csymbol cd="latexml" id="S2.p2.9.m9.5.5.1.1.2.1.cmml" xref="S2.p2.9.m9.5.5.1.1.1.2">delimited-[]</csymbol><apply id="S2.p2.9.m9.5.5.1.1.1.1.cmml" xref="S2.p2.9.m9.5.5.1.1.1.1"><times id="S2.p2.9.m9.5.5.1.1.1.1.1.cmml" xref="S2.p2.9.m9.5.5.1.1.1.1.1"></times><ci id="S2.p2.9.m9.5.5.1.1.1.1.2.cmml" xref="S2.p2.9.m9.5.5.1.1.1.1.2">ğ‘™</ci><interval closure="open" id="S2.p2.9.m9.5.5.1.1.1.1.3.1.cmml" xref="S2.p2.9.m9.5.5.1.1.1.1.3.2"><ci id="S2.p2.9.m9.1.1.cmml" xref="S2.p2.9.m9.1.1">ğ‘¥</ci><ci id="S2.p2.9.m9.2.2.cmml" xref="S2.p2.9.m9.2.2">ğ‘¦</ci></interval></apply></apply></apply><apply id="S2.p2.9.m9.6.6.2.cmml" xref="S2.p2.9.m9.6.6.2"><times id="S2.p2.9.m9.6.6.2.2.cmml" xref="S2.p2.9.m9.6.6.2.2"></times><apply id="S2.p2.9.m9.6.6.2.3.cmml" xref="S2.p2.9.m9.6.6.2.3"><csymbol cd="ambiguous" id="S2.p2.9.m9.6.6.2.3.1.cmml" xref="S2.p2.9.m9.6.6.2.3">subscript</csymbol><ci id="S2.p2.9.m9.6.6.2.3.2.cmml" xref="S2.p2.9.m9.6.6.2.3.2">ğ¸</ci><ci id="S2.p2.9.m9.6.6.2.3.3.cmml" xref="S2.p2.9.m9.6.6.2.3.3">ğ‘</ci></apply><apply id="S2.p2.9.m9.6.6.2.1.2.cmml" xref="S2.p2.9.m9.6.6.2.1.1"><csymbol cd="latexml" id="S2.p2.9.m9.6.6.2.1.2.1.cmml" xref="S2.p2.9.m9.6.6.2.1.1.2">delimited-[]</csymbol><apply id="S2.p2.9.m9.6.6.2.1.1.1.cmml" xref="S2.p2.9.m9.6.6.2.1.1.1"><times id="S2.p2.9.m9.6.6.2.1.1.1.1.cmml" xref="S2.p2.9.m9.6.6.2.1.1.1.1"></times><ci id="S2.p2.9.m9.6.6.2.1.1.1.2.cmml" xref="S2.p2.9.m9.6.6.2.1.1.1.2">ğ‘™</ci><interval closure="open" id="S2.p2.9.m9.6.6.2.1.1.1.3.1.cmml" xref="S2.p2.9.m9.6.6.2.1.1.1.3.2"><ci id="S2.p2.9.m9.3.3.cmml" xref="S2.p2.9.m9.3.3">ğ‘¥</ci><ci id="S2.p2.9.m9.4.4.cmml" xref="S2.p2.9.m9.4.4">ğ‘¦</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m9.6c">E_{q_{k}}[l(x,y)]\neq E_{p}[l(x,y)]</annotation></semantics></math>. Therefore, this algorithm presents an importance reweighting scheme that applies importance weights <math id="S2.p2.10.m10.2" class="ltx_Math" alttext="w_{k}(x,y)" display="inline"><semantics id="S2.p2.10.m10.2a"><mrow id="S2.p2.10.m10.2.3" xref="S2.p2.10.m10.2.3.cmml"><msub id="S2.p2.10.m10.2.3.2" xref="S2.p2.10.m10.2.3.2.cmml"><mi id="S2.p2.10.m10.2.3.2.2" xref="S2.p2.10.m10.2.3.2.2.cmml">w</mi><mi id="S2.p2.10.m10.2.3.2.3" xref="S2.p2.10.m10.2.3.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.p2.10.m10.2.3.1" xref="S2.p2.10.m10.2.3.1.cmml">â€‹</mo><mrow id="S2.p2.10.m10.2.3.3.2" xref="S2.p2.10.m10.2.3.3.1.cmml"><mo stretchy="false" id="S2.p2.10.m10.2.3.3.2.1" xref="S2.p2.10.m10.2.3.3.1.cmml">(</mo><mi id="S2.p2.10.m10.1.1" xref="S2.p2.10.m10.1.1.cmml">x</mi><mo id="S2.p2.10.m10.2.3.3.2.2" xref="S2.p2.10.m10.2.3.3.1.cmml">,</mo><mi id="S2.p2.10.m10.2.2" xref="S2.p2.10.m10.2.2.cmml">y</mi><mo stretchy="false" id="S2.p2.10.m10.2.3.3.2.3" xref="S2.p2.10.m10.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.10.m10.2b"><apply id="S2.p2.10.m10.2.3.cmml" xref="S2.p2.10.m10.2.3"><times id="S2.p2.10.m10.2.3.1.cmml" xref="S2.p2.10.m10.2.3.1"></times><apply id="S2.p2.10.m10.2.3.2.cmml" xref="S2.p2.10.m10.2.3.2"><csymbol cd="ambiguous" id="S2.p2.10.m10.2.3.2.1.cmml" xref="S2.p2.10.m10.2.3.2">subscript</csymbol><ci id="S2.p2.10.m10.2.3.2.2.cmml" xref="S2.p2.10.m10.2.3.2.2">ğ‘¤</ci><ci id="S2.p2.10.m10.2.3.2.3.cmml" xref="S2.p2.10.m10.2.3.2.3">ğ‘˜</ci></apply><interval closure="open" id="S2.p2.10.m10.2.3.3.1.cmml" xref="S2.p2.10.m10.2.3.3.2"><ci id="S2.p2.10.m10.1.1.cmml" xref="S2.p2.10.m10.1.1">ğ‘¥</ci><ci id="S2.p2.10.m10.2.2.cmml" xref="S2.p2.10.m10.2.2">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m10.2c">w_{k}(x,y)</annotation></semantics></math> to every clientâ€™s local objective. With those in place, an unbiased estimator of loss with respect to the target distribution can be obtained using training examples from the client distribution. The paper shows a consistent improvement versus the experiments run over the FedAvg baseline.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Baseline</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we will describe the baseline we compare our experiments to and give a brief justification for some of the initial hyperparameter choices.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Centralized Setting</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Following the project proposal, we choose LeNet-5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> as the architecture for our experiments, the only change is that our model takes the 3 input channels from RGB color images. This model is not the state of the art for the CIFAR-10 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, but it is sufficient to show the relative performance of our experiments. Additionally, using such a compact model in the federated setting brings several benefits due to the potentially limited performance of client devices and the need to share the model and its updates between the server and clients for each round.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2208.11512/assets/graphs/ablation/fedavg_ablation_study2.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.7.3.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">FedAvg ablation study<span id="S3.F1.4.2.2" class="ltx_text ltx_font_medium">. Figure (a) shows how classes are distributed for different values of <math id="S3.F1.3.1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F1.3.1.1.m1.1b"><mi id="S3.F1.3.1.1.m1.1.1" xref="S3.F1.3.1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.F1.3.1.1.m1.1c"><ci id="S3.F1.3.1.1.m1.1.1.cmml" xref="S3.F1.3.1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.3.1.1.m1.1d">\alpha</annotation></semantics></math> taking a random client as an example. Figure (c) illustrates the relative validation accuracy of FedAvg during the training process for different values of <math id="S3.F1.4.2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F1.4.2.2.m2.1b"><mi id="S3.F1.4.2.2.m2.1.1" xref="S3.F1.4.2.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.F1.4.2.2.m2.1c"><ci id="S3.F1.4.2.2.m2.1.1.cmml" xref="S3.F1.4.2.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.4.2.2.m2.1d">\alpha</annotation></semantics></math>. Figures (b) and (d) show the relative validation accuracy after 120 rounds of training with different hyperparameter settings hyperparameters by varying local epochs and reporting client fraction respectively.</span></span></figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p">We perform a small hyperparameter search over the learning rate and pre-processing to find an optimal setting to conduct our later experiments. The results of which can be seen in <a href="#S2.T1" title="In 2 Related Work â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>. We notice, that standardizing the data yields the best results. Interestingly, however, the model performed equally well over a wide range of learning rates. The decreased performance of models trained with lower learning rates is probably due to the short training process. They would likely converge after more epochs to similarly high values of accuracy, if not more. A learning rate schedule could help us circumvent this problem. However, for simplicity, we choose a learning rate of <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="float" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">0.1</annotation></semantics></math> for all following experiments. In this setting, we achieve a final validation accuracy of <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="54.03\%" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mn id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">54.03</mn><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="latexml" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">54.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">54.03\%</annotation></semantics></math> after 120 epochs of training.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Federated Setting</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.5" class="ltx_p">Similar to Hsu <em id="S3.SS2.p1.5.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS2.p1.5.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> we draw the data distributions for our clients from a Dirichlet distribution. This parametric distribution allows us to compare how different federated approaches handle different severities of unbalanced client data by varying <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\alpha</annotation></semantics></math>. For very large <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\alpha</annotation></semantics></math>, the clients will have an almost equal distribution among the classes. The smaller the value for <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\alpha</annotation></semantics></math>, the more extreme the difference between the clients. Having <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\alpha=0" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">Î±</mi><mo id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><eq id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></eq><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">ğ›¼</ci><cn type="integer" id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\alpha=0</annotation></semantics></math> will lead to each client only having samples from a single class. For now, we will set <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">Î±</mi><mo id="S3.SS2.p1.5.m5.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><eq id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1"></eq><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">ğ›¼</ci><cn type="integer" id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">\alpha=1</annotation></semantics></math> and further explore the effect of different client distributions in our ablation studies in <a href="#S4" title="4 Ablation study â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>. Having clients with diverse data is important for our experiments, as this difference in data availability on each client device is one of the major limiting factors for federated learning when compared to a centralized baseline. Additionally, it mirrors how data is available in the real world, with different users having different interests, therefore gathering data about different topics, in different quantities and qualities. As we can not emulate all of these parameters with the given dataset, we focus our experiments on the distribution of classes among clients.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.3" class="ltx_p">Furthermore, we keep our baseline simple by implementing a synchronous communication scheme and using federated averaging to aggregate the clientsâ€™ updates. <a href="#S7" title="7 Experiments â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">7</span></a> shows some experiments with more advanced aggregation schemes. By default, we distribute the dataset among <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">20</annotation></semantics></math> clients, all with equal number of samples (<math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="2000" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mn id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">2000</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><cn type="integer" id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">2000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">2000</annotation></semantics></math>). For each communication round, <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mn id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">20</mn><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">20\%</annotation></semantics></math> of them are chosen to train the network for one epoch on their local data and report their new weights back to the server. We evaluate our results by comparing the final modelâ€™s accuracy on a centralized validation set held by the server. This allows us to analyze how well the model was able to train on the distributed data and provides a more accurate comparison to its centralized counterpart. This is functionally equivalent to comparing the weighted accuracy between clients if the total test set distributed among the clients is equal to the central test set and all clients report their local accuracy.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">In this setting, we achieve a relative validation accuracy of <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="45.98\%" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mn id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">45.98</mn><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">45.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">45.98\%</annotation></semantics></math> after 120 communication rounds and <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">Î±</mi><mo id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><eq id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"></eq><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ›¼</ci><cn type="integer" id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\alpha=1</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Ablation study</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Since we choose the hyperparameters for our baseline without any factual basis, we will use this section to explore the effect each parameter has on the final result of the model.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data distribution</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Probably the most important factor affecting validation accuracy is the data distribution on each client device. Most machine learning methods assume the data they train on to be independent and identically distributed (IID). This means all data samples are drawn from the same distribution and independently from each other. Violating this assumption often leads to decreased performance of the model.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.5" class="ltx_p"><a href="#S3.F1" title="In 3.1 Centralized Setting â€£ 3 Baseline â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> (a) shows the number of samples a typical client has access to given different values of <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\alpha</annotation></semantics></math>. We observe that the smaller <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\alpha</annotation></semantics></math> is, the stronger the class unbalance on each client. For very small values such as <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\alpha=0.01" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">Î±</mi><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><eq id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></eq><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">ğ›¼</ci><cn type="float" id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\alpha=0.01</annotation></semantics></math>, each client typically only has access to samples of a single class. This violation of the IID assumption leads to strong performance decreases. <a href="#S3.F1" title="In 3.1 Centralized Setting â€£ 3 Baseline â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> (c) shows the relative validation accuracy during training over the number of communication rounds. For <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="\alpha=100" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mi id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">Î±</mi><mo id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><eq id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1"></eq><ci id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">ğ›¼</ci><cn type="integer" id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">\alpha=100</annotation></semantics></math> the data is more or less evenly distributed among the clients, so the model can achieve results very similar to those of the centralized case, albeit after a longer training process. For <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mrow id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><mi id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml">Î±</mi><mo id="S4.SS1.p2.5.m5.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.5.m5.1.1.3" xref="S4.SS1.p2.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><eq id="S4.SS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1"></eq><ci id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2">ğ›¼</ci><cn type="integer" id="S4.SS1.p2.5.m5.1.1.3.cmml" xref="S4.SS1.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">\alpha=1</annotation></semantics></math> and lower values, the model does not only produce much worse results, but we also observe that the training is highly unstable for these hard cases.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2208.11512/assets/graphs/normalization/normalization.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.5.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Normalization effect<span id="S4.F2.2.1.1" class="ltx_text ltx_font_medium">. we show in these graphs how different normalization layers perform compared to the raw model for various <math id="S4.F2.2.1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.F2.2.1.1.m1.1b"><mi id="S4.F2.2.1.1.m1.1.1" xref="S4.F2.2.1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.F2.2.1.1.m1.1c"><ci id="S4.F2.2.1.1.m1.1.1.cmml" xref="S4.F2.2.1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.1.1.m1.1d">\alpha</annotation></semantics></math> values. We present both mean and best validation accuracy.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Client settings</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Since the homogeneity of the data distribution on each client device is typically dictated by the task, it is worth analyzing which other parameters exist, that affect the predictive quality of the final model, and that can be influenced by the application. Here we analyze the effects of how many epochs the local modal is trained for on each selected client device during each round of communication and the fraction of clients chosen to participate in each round of communication relative to the total number of clients. In response to the inconsistent validation accuracy during the training process, that we discussed in the previous subsection, we will report both the average and the best validation accuracy each model achieved over the training process.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><a href="#S3.F1" title="In 3.1 Centralized Setting â€£ 3 Baseline â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> (b) shows what happens when we increase the number of local epochs. We observe that this helps for large values of <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\alpha</annotation></semantics></math>, while it seems to have no effect for smaller values. An intuitive explanation for this could be, that increasing the number of local epochs results in a longer effective training process, since the number of communication rounds does not change. However, this effect only helps in the case of more homogeneous data distributions on the clients, since each model is expected to have similar gradients. If the clientsâ€™ data grows more heterogeneous, each model will converge towards a different local optimum, rendering the weight updates, that are sent back to the server incompatible.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">Another parameter of the training process we experiment with is the relative amount of clients, that are chosen to train on their local data for each round. <a href="#S3.F1" title="In 3.1 Centralized Setting â€£ 3 Baseline â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> (d) shows the results of this study. While the results do not differ significantly from each other, we do see a slight improvement for larger client fractions. This remains true over all values of <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\alpha</annotation></semantics></math>, but more pronounced for larger values. Choosing a larger fraction of clients to train each round most likely stabilizes training, as the average update step will more likely point towards the true local minimum of the loss function. Again, this technique is more effective for larger values of <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\alpha</annotation></semantics></math>, as the weight updates of the clients are more compatible with each other.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Normalization</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Since normalization techniques proved their capabilities in helping deep networks train faster and attain better results, they have been widely used in many benchmark models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. While batch normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> helps to minimize internal covariate shift by batch statistics, it runs into problems with small batches. This issue is addressed with an alternative normalization method known as Group normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Group Normalization uses channel instead of batch statistics, which makes it more robust to the challenges and limitations that come with small batch sizes. Considering how widely both techniques used are, we discuss here how they perform in the federated scenario.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Batch normalization</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.3" class="ltx_p">As mentioned earlier, batch normalization uses batch statistics to compute a mean and a variance used for standardizing the batch, and then applying an affine transformation with learnable parameters. Usually, the shortcoming of this technique is its inaccurate estimation when having batches of insufficient sizes. In our study, we notice that Batch Normalization boosts the modelâ€™s learning capabilities while <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\alpha</annotation></semantics></math> is large but these improvements arenâ€™t achievable anymore when <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\alpha</annotation></semantics></math> decreases below a specific threshold. In fact for low <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mi id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><ci id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">\alpha</annotation></semantics></math> values Batch Normalization hurts the training of the model (Figure <a href="#S4.F2" title="Figure 2 â€£ 4.1 Data distribution â€£ 4 Ablation study â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and attains worse results than models with no normalization layers. This is mainly due to its biased estimation but in this case, the problem isnâ€™t the batch size, instead, it is the highly heterogeneous batches between clients. This results in highly biased statistics to be aggregated on the serverâ€™s side.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Group normalization</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.6" class="ltx_p">Group normalization was introduced as a simple alternative to Batch Normalization. Group Normalization normalizes layer outputs based on channel statistics. Specifically for a number of groups <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">G</annotation></semantics></math> (<math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">G</annotation></semantics></math> is a hyperparameter), it standardizes each group of channelsâ€™ features. Group Normalization shares a similar concept as other normalization methods (layer normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and instance normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>). Actually, in the extreme cases of <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="G=1" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">G</mi><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><eq id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1"></eq><ci id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">ğº</ci><cn type="integer" id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">G=1</annotation></semantics></math>, and <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="G=N" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mrow id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mi id="S5.SS2.p1.4.m4.1.1.2" xref="S5.SS2.p1.4.m4.1.1.2.cmml">G</mi><mo id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.cmml">=</mo><mi id="S5.SS2.p1.4.m4.1.1.3" xref="S5.SS2.p1.4.m4.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><eq id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1"></eq><ci id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2">ğº</ci><ci id="S5.SS2.p1.4.m4.1.1.3.cmml" xref="S5.SS2.p1.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">G=N</annotation></semantics></math> (where <math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><mi id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><ci id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">N</annotation></semantics></math> is the number of channels) it is equivalent to Layer Normalization and Instance Normalization respectively. We performed our analysis with (2 groups for the <span id="S5.SS2.p1.6.1" class="ltx_ERROR undefined">\nth</span>1 Convolutional layer with 6 channels, 4 groups for the <span id="S5.SS2.p1.6.2" class="ltx_ERROR undefined">\nth</span>2 one with 16 channels, and 30 groups for the last one with 120 channels), as shown in <a href="#S4.F2" title="Figure 2 â€£ 4.1 Data distribution â€£ 4 Ablation study â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, Group Normalization outperforms Batch Normalization for lower <math id="S5.SS2.p1.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS2.p1.6.m6.1a"><mi id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><ci id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">\alpha</annotation></semantics></math>. This shows that group norm isnâ€™t as limited as Batch Normalization with the batch samplesâ€™ distribution, still the model suffer from the non-identicalness of the clientâ€™s data issue.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.11512/assets/graphs/extension/extension_graph.png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="324" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.11512/assets/graphs/extension/extension_ablation_study.png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="685" height="439" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">FedOS<span id="S5.F3.4.2.1" class="ltx_text ltx_font_medium">. Figure (a) shows the structure of the model during training and some samples of the unknown class generated by the GAN. In (b) we show the results of the ablation study performed on our proposed FedOS method, in particular we show the max and mean relative accuracy over 60 rounds. The mean represents model learning stability while the max shows goodness. We have 4 hyperparameters in total with the default values (local epochs: 1, client fraction: 20%, unknown class loss weight: 1, unknown class fraction: 60%). For each study we change one parameter while fixing the others to their default values. </span></span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Our method</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">As we saw in our ablation studies in <a href="#S4" title="4 Ablation study â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>, one of the biggest challenges in federated learning is the heterogeneity of the client devices. During all of our experiments, the model struggled to achieve meaningful results in settings with highly unbalanced data distributions on client devices. We hypothesize that an important factor is, that most clients are only aware of a fraction of the classes the model is designed to recognize. In fact, in the case of <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="\alpha=0.01" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mi id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">Î±</mi><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">=</mo><mn id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><eq id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1"></eq><ci id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">ğ›¼</ci><cn type="float" id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\alpha=0.01</annotation></semantics></math>, many clients only have access to a single class and are not aware of features that could be necessary to recognize other classes. This does not only pose an issue for local training on the client devices. Since each client trains only on a small subset of classes, their weights will likely diverge from each other, making the aggregation of client updates even more challenging. In this section, we present a novel approach that is designed to address these shortcomings. During the local training step on each client device, we add artificial samples from an unknown class <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> that represent those classes that the client might not be aware of. Ideally, these artificial samples share the same feature space as the real images in the dataset. This allows the local model on each client to improve its feature extraction capabilities while simultaneously learning which features are important to discriminate those classes, which the local model has access to.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.16" class="ltx_p">Let <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S6.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><ci id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">\mathcal{D}</annotation></semantics></math> denote our full dataset, <math id="S6.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{D_{C}}" display="inline"><semantics id="S6.p2.2.m2.1a"><msub id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.2.m2.1.1.2" xref="S6.p2.2.m2.1.1.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.2.m2.1.1.3" xref="S6.p2.2.m2.1.1.3.cmml">ğ’</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><apply id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.p2.2.m2.1.1.1.cmml" xref="S6.p2.2.m2.1.1">subscript</csymbol><ci id="S6.p2.2.m2.1.1.2.cmml" xref="S6.p2.2.m2.1.1.2">ğ’Ÿ</ci><ci id="S6.p2.2.m2.1.1.3.cmml" xref="S6.p2.2.m2.1.1.3">ğ’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">\mathcal{D_{C}}</annotation></semantics></math> is the fraction of <math id="S6.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S6.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><ci id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">\mathcal{D}</annotation></semantics></math> belonging to Client <math id="S6.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S6.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S6.p2.4.m4.1.1" xref="S6.p2.4.m4.1.1.cmml">ğ’</mi><annotation-xml encoding="MathML-Content" id="S6.p2.4.m4.1b"><ci id="S6.p2.4.m4.1.1.cmml" xref="S6.p2.4.m4.1.1">ğ’</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.4.m4.1c">\mathcal{C}</annotation></semantics></math>, <math id="S6.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{\overline{D_{C}}}" display="inline"><semantics id="S6.p2.5.m5.1a"><mover accent="true" id="S6.p2.5.m5.1.1" xref="S6.p2.5.m5.1.1.cmml"><msub id="S6.p2.5.m5.1.1.2" xref="S6.p2.5.m5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.5.m5.1.1.2.2" xref="S6.p2.5.m5.1.1.2.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.5.m5.1.1.2.3" xref="S6.p2.5.m5.1.1.2.3.cmml">ğ’</mi></msub><mo id="S6.p2.5.m5.1.1.1" xref="S6.p2.5.m5.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S6.p2.5.m5.1b"><apply id="S6.p2.5.m5.1.1.cmml" xref="S6.p2.5.m5.1.1"><ci id="S6.p2.5.m5.1.1.1.cmml" xref="S6.p2.5.m5.1.1.1">Â¯</ci><apply id="S6.p2.5.m5.1.1.2.cmml" xref="S6.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S6.p2.5.m5.1.1.2.1.cmml" xref="S6.p2.5.m5.1.1.2">subscript</csymbol><ci id="S6.p2.5.m5.1.1.2.2.cmml" xref="S6.p2.5.m5.1.1.2.2">ğ’Ÿ</ci><ci id="S6.p2.5.m5.1.1.2.3.cmml" xref="S6.p2.5.m5.1.1.2.3">ğ’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.5.m5.1c">\mathcal{\overline{D_{C}}}</annotation></semantics></math> represents <math id="S6.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{D\setminus D_{C}}" display="inline"><semantics id="S6.p2.6.m6.1a"><mrow id="S6.p2.6.m6.1.1" xref="S6.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.6.m6.1.1.2" xref="S6.p2.6.m6.1.1.2.cmml">ğ’Ÿ</mi><mo id="S6.p2.6.m6.1.1.1" xref="S6.p2.6.m6.1.1.1.cmml">âˆ–</mo><msub id="S6.p2.6.m6.1.1.3" xref="S6.p2.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.6.m6.1.1.3.2" xref="S6.p2.6.m6.1.1.3.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.6.m6.1.1.3.3" xref="S6.p2.6.m6.1.1.3.3.cmml">ğ’</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.6.m6.1b"><apply id="S6.p2.6.m6.1.1.cmml" xref="S6.p2.6.m6.1.1"><setdiff id="S6.p2.6.m6.1.1.1.cmml" xref="S6.p2.6.m6.1.1.1"></setdiff><ci id="S6.p2.6.m6.1.1.2.cmml" xref="S6.p2.6.m6.1.1.2">ğ’Ÿ</ci><apply id="S6.p2.6.m6.1.1.3.cmml" xref="S6.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S6.p2.6.m6.1.1.3.1.cmml" xref="S6.p2.6.m6.1.1.3">subscript</csymbol><ci id="S6.p2.6.m6.1.1.3.2.cmml" xref="S6.p2.6.m6.1.1.3.2">ğ’Ÿ</ci><ci id="S6.p2.6.m6.1.1.3.3.cmml" xref="S6.p2.6.m6.1.1.3.3">ğ’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.6.m6.1c">\mathcal{D\setminus D_{C}}</annotation></semantics></math>, and <math id="S6.p2.7.m7.1" class="ltx_Math" alttext="\mathcal{M_{C}}" display="inline"><semantics id="S6.p2.7.m7.1a"><msub id="S6.p2.7.m7.1.1" xref="S6.p2.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.7.m7.1.1.2" xref="S6.p2.7.m7.1.1.2.cmml">â„³</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.7.m7.1.1.3" xref="S6.p2.7.m7.1.1.3.cmml">ğ’</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p2.7.m7.1b"><apply id="S6.p2.7.m7.1.1.cmml" xref="S6.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S6.p2.7.m7.1.1.1.cmml" xref="S6.p2.7.m7.1.1">subscript</csymbol><ci id="S6.p2.7.m7.1.1.2.cmml" xref="S6.p2.7.m7.1.1.2">â„³</ci><ci id="S6.p2.7.m7.1.1.3.cmml" xref="S6.p2.7.m7.1.1.3">ğ’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.7.m7.1c">\mathcal{M_{C}}</annotation></semantics></math> is the model currently training on <math id="S6.p2.8.m8.1" class="ltx_Math" alttext="\mathcal{D_{C}}" display="inline"><semantics id="S6.p2.8.m8.1a"><msub id="S6.p2.8.m8.1.1" xref="S6.p2.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.8.m8.1.1.2" xref="S6.p2.8.m8.1.1.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.8.m8.1.1.3" xref="S6.p2.8.m8.1.1.3.cmml">ğ’</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p2.8.m8.1b"><apply id="S6.p2.8.m8.1.1.cmml" xref="S6.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S6.p2.8.m8.1.1.1.cmml" xref="S6.p2.8.m8.1.1">subscript</csymbol><ci id="S6.p2.8.m8.1.1.2.cmml" xref="S6.p2.8.m8.1.1.2">ğ’Ÿ</ci><ci id="S6.p2.8.m8.1.1.3.cmml" xref="S6.p2.8.m8.1.1.3">ğ’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.8.m8.1c">\mathcal{D_{C}}</annotation></semantics></math>. Suppose a global dataset containing all classes as <math id="S6.p2.9.m9.1" class="ltx_Math" alttext="\mathcal{D_{G}}" display="inline"><semantics id="S6.p2.9.m9.1a"><msub id="S6.p2.9.m9.1.1" xref="S6.p2.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.9.m9.1.1.2" xref="S6.p2.9.m9.1.1.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.9.m9.1.1.3" xref="S6.p2.9.m9.1.1.3.cmml">ğ’¢</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p2.9.m9.1b"><apply id="S6.p2.9.m9.1.1.cmml" xref="S6.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S6.p2.9.m9.1.1.1.cmml" xref="S6.p2.9.m9.1.1">subscript</csymbol><ci id="S6.p2.9.m9.1.1.2.cmml" xref="S6.p2.9.m9.1.1.2">ğ’Ÿ</ci><ci id="S6.p2.9.m9.1.1.3.cmml" xref="S6.p2.9.m9.1.1.3">ğ’¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.9.m9.1c">\mathcal{D_{G}}</annotation></semantics></math>. by definition, a class belonging to <math id="S6.p2.10.m10.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S6.p2.10.m10.1a"><mi class="ltx_font_mathcaligraphic" id="S6.p2.10.m10.1.1" xref="S6.p2.10.m10.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S6.p2.10.m10.1b"><ci id="S6.p2.10.m10.1.1.cmml" xref="S6.p2.10.m10.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.10.m10.1c">\mathcal{D}</annotation></semantics></math> belongs to <math id="S6.p2.11.m11.1" class="ltx_Math" alttext="\mathcal{D_{G}}" display="inline"><semantics id="S6.p2.11.m11.1a"><msub id="S6.p2.11.m11.1.1" xref="S6.p2.11.m11.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.11.m11.1.1.2" xref="S6.p2.11.m11.1.1.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.11.m11.1.1.3" xref="S6.p2.11.m11.1.1.3.cmml">ğ’¢</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p2.11.m11.1b"><apply id="S6.p2.11.m11.1.1.cmml" xref="S6.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S6.p2.11.m11.1.1.1.cmml" xref="S6.p2.11.m11.1.1">subscript</csymbol><ci id="S6.p2.11.m11.1.1.2.cmml" xref="S6.p2.11.m11.1.1.2">ğ’Ÿ</ci><ci id="S6.p2.11.m11.1.1.3.cmml" xref="S6.p2.11.m11.1.1.3">ğ’¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.11.m11.1c">\mathcal{D_{G}}</annotation></semantics></math> which implies <math id="S6.p2.12.m12.1" class="ltx_Math" alttext="\mathcal{\overline{D_{C}}\subset D\subset D_{G}}" display="inline"><semantics id="S6.p2.12.m12.1a"><mrow id="S6.p2.12.m12.1.1" xref="S6.p2.12.m12.1.1.cmml"><mover accent="true" id="S6.p2.12.m12.1.1.2" xref="S6.p2.12.m12.1.1.2.cmml"><msub id="S6.p2.12.m12.1.1.2.2" xref="S6.p2.12.m12.1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.12.m12.1.1.2.2.2" xref="S6.p2.12.m12.1.1.2.2.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.12.m12.1.1.2.2.3" xref="S6.p2.12.m12.1.1.2.2.3.cmml">ğ’</mi></msub><mo id="S6.p2.12.m12.1.1.2.1" xref="S6.p2.12.m12.1.1.2.1.cmml">Â¯</mo></mover><mo id="S6.p2.12.m12.1.1.3" xref="S6.p2.12.m12.1.1.3.cmml">âŠ‚</mo><mi class="ltx_font_mathcaligraphic" id="S6.p2.12.m12.1.1.4" xref="S6.p2.12.m12.1.1.4.cmml">ğ’Ÿ</mi><mo id="S6.p2.12.m12.1.1.5" xref="S6.p2.12.m12.1.1.5.cmml">âŠ‚</mo><msub id="S6.p2.12.m12.1.1.6" xref="S6.p2.12.m12.1.1.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.12.m12.1.1.6.2" xref="S6.p2.12.m12.1.1.6.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.12.m12.1.1.6.3" xref="S6.p2.12.m12.1.1.6.3.cmml">ğ’¢</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.12.m12.1b"><apply id="S6.p2.12.m12.1.1.cmml" xref="S6.p2.12.m12.1.1"><and id="S6.p2.12.m12.1.1a.cmml" xref="S6.p2.12.m12.1.1"></and><apply id="S6.p2.12.m12.1.1b.cmml" xref="S6.p2.12.m12.1.1"><subset id="S6.p2.12.m12.1.1.3.cmml" xref="S6.p2.12.m12.1.1.3"></subset><apply id="S6.p2.12.m12.1.1.2.cmml" xref="S6.p2.12.m12.1.1.2"><ci id="S6.p2.12.m12.1.1.2.1.cmml" xref="S6.p2.12.m12.1.1.2.1">Â¯</ci><apply id="S6.p2.12.m12.1.1.2.2.cmml" xref="S6.p2.12.m12.1.1.2.2"><csymbol cd="ambiguous" id="S6.p2.12.m12.1.1.2.2.1.cmml" xref="S6.p2.12.m12.1.1.2.2">subscript</csymbol><ci id="S6.p2.12.m12.1.1.2.2.2.cmml" xref="S6.p2.12.m12.1.1.2.2.2">ğ’Ÿ</ci><ci id="S6.p2.12.m12.1.1.2.2.3.cmml" xref="S6.p2.12.m12.1.1.2.2.3">ğ’</ci></apply></apply><ci id="S6.p2.12.m12.1.1.4.cmml" xref="S6.p2.12.m12.1.1.4">ğ’Ÿ</ci></apply><apply id="S6.p2.12.m12.1.1c.cmml" xref="S6.p2.12.m12.1.1"><subset id="S6.p2.12.m12.1.1.5.cmml" xref="S6.p2.12.m12.1.1.5"></subset><share href="#S6.p2.12.m12.1.1.4.cmml" id="S6.p2.12.m12.1.1d.cmml" xref="S6.p2.12.m12.1.1"></share><apply id="S6.p2.12.m12.1.1.6.cmml" xref="S6.p2.12.m12.1.1.6"><csymbol cd="ambiguous" id="S6.p2.12.m12.1.1.6.1.cmml" xref="S6.p2.12.m12.1.1.6">subscript</csymbol><ci id="S6.p2.12.m12.1.1.6.2.cmml" xref="S6.p2.12.m12.1.1.6.2">ğ’Ÿ</ci><ci id="S6.p2.12.m12.1.1.6.3.cmml" xref="S6.p2.12.m12.1.1.6.3">ğ’¢</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.12.m12.1c">\mathcal{\overline{D_{C}}\subset D\subset D_{G}}</annotation></semantics></math>. What we want to achieve is <math id="S6.p2.13.m13.1" class="ltx_Math" alttext="\mathcal{M_{C}}" display="inline"><semantics id="S6.p2.13.m13.1a"><msub id="S6.p2.13.m13.1.1" xref="S6.p2.13.m13.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.13.m13.1.1.2" xref="S6.p2.13.m13.1.1.2.cmml">â„³</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.13.m13.1.1.3" xref="S6.p2.13.m13.1.1.3.cmml">ğ’</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p2.13.m13.1b"><apply id="S6.p2.13.m13.1.1.cmml" xref="S6.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S6.p2.13.m13.1.1.1.cmml" xref="S6.p2.13.m13.1.1">subscript</csymbol><ci id="S6.p2.13.m13.1.1.2.cmml" xref="S6.p2.13.m13.1.1.2">â„³</ci><ci id="S6.p2.13.m13.1.1.3.cmml" xref="S6.p2.13.m13.1.1.3">ğ’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.13.m13.1c">\mathcal{M_{C}}</annotation></semantics></math> predicting unknown for <math id="S6.p2.14.m14.1" class="ltx_Math" alttext="\mathcal{\overline{D_{C}}}" display="inline"><semantics id="S6.p2.14.m14.1a"><mover accent="true" id="S6.p2.14.m14.1.1" xref="S6.p2.14.m14.1.1.cmml"><msub id="S6.p2.14.m14.1.1.2" xref="S6.p2.14.m14.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.14.m14.1.1.2.2" xref="S6.p2.14.m14.1.1.2.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.14.m14.1.1.2.3" xref="S6.p2.14.m14.1.1.2.3.cmml">ğ’</mi></msub><mo id="S6.p2.14.m14.1.1.1" xref="S6.p2.14.m14.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S6.p2.14.m14.1b"><apply id="S6.p2.14.m14.1.1.cmml" xref="S6.p2.14.m14.1.1"><ci id="S6.p2.14.m14.1.1.1.cmml" xref="S6.p2.14.m14.1.1.1">Â¯</ci><apply id="S6.p2.14.m14.1.1.2.cmml" xref="S6.p2.14.m14.1.1.2"><csymbol cd="ambiguous" id="S6.p2.14.m14.1.1.2.1.cmml" xref="S6.p2.14.m14.1.1.2">subscript</csymbol><ci id="S6.p2.14.m14.1.1.2.2.cmml" xref="S6.p2.14.m14.1.1.2.2">ğ’Ÿ</ci><ci id="S6.p2.14.m14.1.1.2.3.cmml" xref="S6.p2.14.m14.1.1.2.3">ğ’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.14.m14.1c">\mathcal{\overline{D_{C}}}</annotation></semantics></math> for all clients so that at the aggregation step all models will distinguish their classes from those of other clients. We estimate <math id="S6.p2.15.m15.1" class="ltx_Math" alttext="\mathcal{D_{G}}" display="inline"><semantics id="S6.p2.15.m15.1a"><msub id="S6.p2.15.m15.1.1" xref="S6.p2.15.m15.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.15.m15.1.1.2" xref="S6.p2.15.m15.1.1.2.cmml">ğ’Ÿ</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.15.m15.1.1.3" xref="S6.p2.15.m15.1.1.3.cmml">ğ’¢</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p2.15.m15.1b"><apply id="S6.p2.15.m15.1.1.cmml" xref="S6.p2.15.m15.1.1"><csymbol cd="ambiguous" id="S6.p2.15.m15.1.1.1.cmml" xref="S6.p2.15.m15.1.1">subscript</csymbol><ci id="S6.p2.15.m15.1.1.2.cmml" xref="S6.p2.15.m15.1.1.2">ğ’Ÿ</ci><ci id="S6.p2.15.m15.1.1.3.cmml" xref="S6.p2.15.m15.1.1.3">ğ’¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.15.m15.1c">\mathcal{D_{G}}</annotation></semantics></math> using a relatively large dataset containing a fair amount of different classes and a GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> learning its general distribution. We also modify our model to accept one extra class â€unknownâ€ which introduces a new term to the loss function that symbolizes the loss of unknown samplesâ€™ misclassification, this term is weighted using a hyperparameter <math id="S6.p2.16.m16.1" class="ltx_Math" alttext="\mathcal{W_{U}}" display="inline"><semantics id="S6.p2.16.m16.1a"><msub id="S6.p2.16.m16.1.1" xref="S6.p2.16.m16.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p2.16.m16.1.1.2" xref="S6.p2.16.m16.1.1.2.cmml">ğ’²</mi><mi class="ltx_font_mathcaligraphic" id="S6.p2.16.m16.1.1.3" xref="S6.p2.16.m16.1.1.3.cmml">ğ’°</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p2.16.m16.1b"><apply id="S6.p2.16.m16.1.1.cmml" xref="S6.p2.16.m16.1.1"><csymbol cd="ambiguous" id="S6.p2.16.m16.1.1.1.cmml" xref="S6.p2.16.m16.1.1">subscript</csymbol><ci id="S6.p2.16.m16.1.1.2.cmml" xref="S6.p2.16.m16.1.1.2">ğ’²</ci><ci id="S6.p2.16.m16.1.1.3.cmml" xref="S6.p2.16.m16.1.1.3">ğ’°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.16.m16.1c">\mathcal{W_{U}}</annotation></semantics></math>.</p>
<table id="S6.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E1.m1.3" class="ltx_Math" alttext="L(C)=L(D_{C})+W_{U}*L(\text{unknown})" display="block"><semantics id="S6.E1.m1.3a"><mrow id="S6.E1.m1.3.3" xref="S6.E1.m1.3.3.cmml"><mrow id="S6.E1.m1.3.3.3" xref="S6.E1.m1.3.3.3.cmml"><mi id="S6.E1.m1.3.3.3.2" xref="S6.E1.m1.3.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.3.3.3.1" xref="S6.E1.m1.3.3.3.1.cmml">â€‹</mo><mrow id="S6.E1.m1.3.3.3.3.2" xref="S6.E1.m1.3.3.3.cmml"><mo stretchy="false" id="S6.E1.m1.3.3.3.3.2.1" xref="S6.E1.m1.3.3.3.cmml">(</mo><mi id="S6.E1.m1.1.1" xref="S6.E1.m1.1.1.cmml">C</mi><mo stretchy="false" id="S6.E1.m1.3.3.3.3.2.2" xref="S6.E1.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S6.E1.m1.3.3.2" xref="S6.E1.m1.3.3.2.cmml">=</mo><mrow id="S6.E1.m1.3.3.1" xref="S6.E1.m1.3.3.1.cmml"><mrow id="S6.E1.m1.3.3.1.1" xref="S6.E1.m1.3.3.1.1.cmml"><mi id="S6.E1.m1.3.3.1.1.3" xref="S6.E1.m1.3.3.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.3.3.1.1.2" xref="S6.E1.m1.3.3.1.1.2.cmml">â€‹</mo><mrow id="S6.E1.m1.3.3.1.1.1.1" xref="S6.E1.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.E1.m1.3.3.1.1.1.1.2" xref="S6.E1.m1.3.3.1.1.1.1.1.cmml">(</mo><msub id="S6.E1.m1.3.3.1.1.1.1.1" xref="S6.E1.m1.3.3.1.1.1.1.1.cmml"><mi id="S6.E1.m1.3.3.1.1.1.1.1.2" xref="S6.E1.m1.3.3.1.1.1.1.1.2.cmml">D</mi><mi id="S6.E1.m1.3.3.1.1.1.1.1.3" xref="S6.E1.m1.3.3.1.1.1.1.1.3.cmml">C</mi></msub><mo stretchy="false" id="S6.E1.m1.3.3.1.1.1.1.3" xref="S6.E1.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S6.E1.m1.3.3.1.2" xref="S6.E1.m1.3.3.1.2.cmml">+</mo><mrow id="S6.E1.m1.3.3.1.3" xref="S6.E1.m1.3.3.1.3.cmml"><mrow id="S6.E1.m1.3.3.1.3.2" xref="S6.E1.m1.3.3.1.3.2.cmml"><msub id="S6.E1.m1.3.3.1.3.2.2" xref="S6.E1.m1.3.3.1.3.2.2.cmml"><mi id="S6.E1.m1.3.3.1.3.2.2.2" xref="S6.E1.m1.3.3.1.3.2.2.2.cmml">W</mi><mi id="S6.E1.m1.3.3.1.3.2.2.3" xref="S6.E1.m1.3.3.1.3.2.2.3.cmml">U</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S6.E1.m1.3.3.1.3.2.1" xref="S6.E1.m1.3.3.1.3.2.1.cmml">âˆ—</mo><mi id="S6.E1.m1.3.3.1.3.2.3" xref="S6.E1.m1.3.3.1.3.2.3.cmml">L</mi></mrow><mo lspace="0em" rspace="0em" id="S6.E1.m1.3.3.1.3.1" xref="S6.E1.m1.3.3.1.3.1.cmml">â€‹</mo><mrow id="S6.E1.m1.3.3.1.3.3.2" xref="S6.E1.m1.2.2a.cmml"><mo stretchy="false" id="S6.E1.m1.3.3.1.3.3.2.1" xref="S6.E1.m1.2.2a.cmml">(</mo><mtext id="S6.E1.m1.2.2" xref="S6.E1.m1.2.2.cmml">unknown</mtext><mo stretchy="false" id="S6.E1.m1.3.3.1.3.3.2.2" xref="S6.E1.m1.2.2a.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.3b"><apply id="S6.E1.m1.3.3.cmml" xref="S6.E1.m1.3.3"><eq id="S6.E1.m1.3.3.2.cmml" xref="S6.E1.m1.3.3.2"></eq><apply id="S6.E1.m1.3.3.3.cmml" xref="S6.E1.m1.3.3.3"><times id="S6.E1.m1.3.3.3.1.cmml" xref="S6.E1.m1.3.3.3.1"></times><ci id="S6.E1.m1.3.3.3.2.cmml" xref="S6.E1.m1.3.3.3.2">ğ¿</ci><ci id="S6.E1.m1.1.1.cmml" xref="S6.E1.m1.1.1">ğ¶</ci></apply><apply id="S6.E1.m1.3.3.1.cmml" xref="S6.E1.m1.3.3.1"><plus id="S6.E1.m1.3.3.1.2.cmml" xref="S6.E1.m1.3.3.1.2"></plus><apply id="S6.E1.m1.3.3.1.1.cmml" xref="S6.E1.m1.3.3.1.1"><times id="S6.E1.m1.3.3.1.1.2.cmml" xref="S6.E1.m1.3.3.1.1.2"></times><ci id="S6.E1.m1.3.3.1.1.3.cmml" xref="S6.E1.m1.3.3.1.1.3">ğ¿</ci><apply id="S6.E1.m1.3.3.1.1.1.1.1.cmml" xref="S6.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S6.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S6.E1.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S6.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.2">ğ·</ci><ci id="S6.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S6.E1.m1.3.3.1.1.1.1.1.3">ğ¶</ci></apply></apply><apply id="S6.E1.m1.3.3.1.3.cmml" xref="S6.E1.m1.3.3.1.3"><times id="S6.E1.m1.3.3.1.3.1.cmml" xref="S6.E1.m1.3.3.1.3.1"></times><apply id="S6.E1.m1.3.3.1.3.2.cmml" xref="S6.E1.m1.3.3.1.3.2"><times id="S6.E1.m1.3.3.1.3.2.1.cmml" xref="S6.E1.m1.3.3.1.3.2.1"></times><apply id="S6.E1.m1.3.3.1.3.2.2.cmml" xref="S6.E1.m1.3.3.1.3.2.2"><csymbol cd="ambiguous" id="S6.E1.m1.3.3.1.3.2.2.1.cmml" xref="S6.E1.m1.3.3.1.3.2.2">subscript</csymbol><ci id="S6.E1.m1.3.3.1.3.2.2.2.cmml" xref="S6.E1.m1.3.3.1.3.2.2.2">ğ‘Š</ci><ci id="S6.E1.m1.3.3.1.3.2.2.3.cmml" xref="S6.E1.m1.3.3.1.3.2.2.3">ğ‘ˆ</ci></apply><ci id="S6.E1.m1.3.3.1.3.2.3.cmml" xref="S6.E1.m1.3.3.1.3.2.3">ğ¿</ci></apply><ci id="S6.E1.m1.2.2a.cmml" xref="S6.E1.m1.3.3.1.3.3.2"><mtext id="S6.E1.m1.2.2.cmml" xref="S6.E1.m1.2.2">unknown</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.3c">L(C)=L(D_{C})+W_{U}*L(\text{unknown})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Pre-training</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> we train our GAN for generating and modeling the unknown class. We differ from them in the final objective we have for training the GAN. In the context of open-set recognition, the model at the end should be able to recognize high-resolution unknown samples for which it has never been trained, so they train their adversarial network to model the out-of-distribution space using the classes at hand. On the other hand, our objective is to take into account the existence of numerous different classes that donâ€™t appear within the local data during training. We try to model the general space features by training the network on many classes to capture the most repeated features. We train our GAN on a random subset of the Tiny Imagenet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> containing samples from different classes. this step has a low cost since we donâ€™t need a specific class to train on, which makes the training almost unsupervised. The only requirement is to train on many classes.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Training</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.3" class="ltx_p">First, the trained GAN is sent to the client-side, this step accounts only for half a round since the adversarial network doesnâ€™t need to be sent back to the server-side and its size is comparable to the model used for classification. The clients generate a set of random samples proportional to the amount of data they have locally. This is controlled by our second hyper parameter <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{F_{U}}" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><msub id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.1.m1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.2.cmml">â„±</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.1.m1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.3.cmml">ğ’°</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><apply id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.1.m1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.p1.1.m1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.2">â„±</ci><ci id="S6.SS2.p1.1.m1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.3">ğ’°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">\mathcal{F_{U}}</annotation></semantics></math>. Suppose <math id="S6.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="S6.SS2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml">ğ’©</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><ci id="S6.SS2.p1.2.m2.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1">ğ’©</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">\mathcal{N}</annotation></semantics></math> is the total number of local samples, and the quantity of unknown samples generated is <math id="S6.SS2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{F_{U}*N}" display="inline"><semantics id="S6.SS2.p1.3.m3.1a"><mrow id="S6.SS2.p1.3.m3.1.1" xref="S6.SS2.p1.3.m3.1.1.cmml"><msub id="S6.SS2.p1.3.m3.1.1.2" xref="S6.SS2.p1.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.3.m3.1.1.2.2" xref="S6.SS2.p1.3.m3.1.1.2.2.cmml">â„±</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.3.m3.1.1.2.3" xref="S6.SS2.p1.3.m3.1.1.2.3.cmml">ğ’°</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S6.SS2.p1.3.m3.1.1.1" xref="S6.SS2.p1.3.m3.1.1.1.cmml">âˆ—</mo><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.3.m3.1.1.3" xref="S6.SS2.p1.3.m3.1.1.3.cmml">ğ’©</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.3.m3.1b"><apply id="S6.SS2.p1.3.m3.1.1.cmml" xref="S6.SS2.p1.3.m3.1.1"><times id="S6.SS2.p1.3.m3.1.1.1.cmml" xref="S6.SS2.p1.3.m3.1.1.1"></times><apply id="S6.SS2.p1.3.m3.1.1.2.cmml" xref="S6.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S6.SS2.p1.3.m3.1.1.2.1.cmml" xref="S6.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S6.SS2.p1.3.m3.1.1.2.2.cmml" xref="S6.SS2.p1.3.m3.1.1.2.2">â„±</ci><ci id="S6.SS2.p1.3.m3.1.1.2.3.cmml" xref="S6.SS2.p1.3.m3.1.1.2.3">ğ’°</ci></apply><ci id="S6.SS2.p1.3.m3.1.1.3.cmml" xref="S6.SS2.p1.3.m3.1.1.3">ğ’©</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.3.m3.1c">\mathcal{F_{U}*N}</annotation></semantics></math>.
We add an extra output class to classify â€unknownâ€ samples (see Figure <a href="#S5.F3.sf1" title="Figure 3(a) â€£ Figure 3 â€£ 5.2 Group normalization â€£ 5 Normalization â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>) and the training proceeds with the normal FedAvg scheme. At the end of training and before inference, we disable the extra output forcing the model to output one of the known classes since our final objective is a closed set classification task.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.2.1.1" class="ltx_tr">
<td id="S6.T2.2.1.1.1" class="ltx_td"></td>
<td id="S6.T2.2.1.1.2" class="ltx_td ltx_align_center" colspan="8"><span id="S6.T2.2.1.1.2.1" class="ltx_text ltx_font_bold">Relative accuracy @N rounds</span></td>
</tr>
<tr id="S6.T2.2.2.2" class="ltx_tr">
<td id="S6.T2.2.2.2.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S6.T2.2.2.2.2.1" class="ltx_text ltx_font_bold">Alpha=1</span></td>
<td id="S6.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S6.T2.2.2.2.3.1" class="ltx_text ltx_font_bold">Alpha=0.01</span></td>
</tr>
<tr id="S6.T2.2.3.3" class="ltx_tr">
<td id="S6.T2.2.3.3.1" class="ltx_td ltx_align_center"><span id="S6.T2.2.3.3.1.1" class="ltx_text ltx_font_bold">Methods</span></td>
<td id="S6.T2.2.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T2.2.3.3.2.1" class="ltx_text ltx_font_bold">@125</span></td>
<td id="S6.T2.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T2.2.3.3.3.1" class="ltx_text ltx_font_bold">@250</span></td>
<td id="S6.T2.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T2.2.3.3.4.1" class="ltx_text ltx_font_bold">@375</span></td>
<td id="S6.T2.2.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T2.2.3.3.5.1" class="ltx_text ltx_font_bold">@500</span></td>
<td id="S6.T2.2.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T2.2.3.3.6.1" class="ltx_text ltx_font_bold">@125</span></td>
<td id="S6.T2.2.3.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T2.2.3.3.7.1" class="ltx_text ltx_font_bold">@250</span></td>
<td id="S6.T2.2.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T2.2.3.3.8.1" class="ltx_text ltx_font_bold">@375</span></td>
<td id="S6.T2.2.3.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T2.2.3.3.9.1" class="ltx_text ltx_font_bold">@500</span></td>
</tr>
<tr id="S6.T2.2.4.4" class="ltx_tr">
<td id="S6.T2.2.4.4.1" class="ltx_td ltx_align_center ltx_border_t">FedAvg</td>
<td id="S6.T2.2.4.4.2" class="ltx_td ltx_align_center ltx_border_t">65.94</td>
<td id="S6.T2.2.4.4.3" class="ltx_td ltx_align_center ltx_border_t">82.17</td>
<td id="S6.T2.2.4.4.4" class="ltx_td ltx_align_center ltx_border_t">83.03</td>
<td id="S6.T2.2.4.4.5" class="ltx_td ltx_align_center ltx_border_t">88.34</td>
<td id="S6.T2.2.4.4.6" class="ltx_td ltx_align_center ltx_border_t">40.41</td>
<td id="S6.T2.2.4.4.7" class="ltx_td ltx_align_center ltx_border_t">41.92</td>
<td id="S6.T2.2.4.4.8" class="ltx_td ltx_align_center ltx_border_t">51.19</td>
<td id="S6.T2.2.4.4.9" class="ltx_td ltx_align_center ltx_border_t">60.75</td>
</tr>
<tr id="S6.T2.2.5.5" class="ltx_tr">
<td id="S6.T2.2.5.5.1" class="ltx_td ltx_align_center">FedIR</td>
<td id="S6.T2.2.5.5.2" class="ltx_td ltx_align_center">77.66</td>
<td id="S6.T2.2.5.5.3" class="ltx_td ltx_align_center">82.98</td>
<td id="S6.T2.2.5.5.4" class="ltx_td ltx_align_center">89.03</td>
<td id="S6.T2.2.5.5.5" class="ltx_td ltx_align_center">89.36</td>
<td id="S6.T2.2.5.5.6" class="ltx_td ltx_align_center"><span id="S6.T2.2.5.5.6.1" class="ltx_text ltx_font_bold">51.75</span></td>
<td id="S6.T2.2.5.5.7" class="ltx_td ltx_align_center"><span id="S6.T2.2.5.5.7.1" class="ltx_text ltx_font_bold">59.56</span></td>
<td id="S6.T2.2.5.5.8" class="ltx_td ltx_align_center">64</td>
<td id="S6.T2.2.5.5.9" class="ltx_td ltx_align_center">69.03</td>
</tr>
<tr id="S6.T2.2.6.6" class="ltx_tr">
<td id="S6.T2.2.6.6.1" class="ltx_td ltx_align_center">FedProx</td>
<td id="S6.T2.2.6.6.2" class="ltx_td ltx_align_center">63.85</td>
<td id="S6.T2.2.6.6.3" class="ltx_td ltx_align_center">82.07</td>
<td id="S6.T2.2.6.6.4" class="ltx_td ltx_align_center">86.51</td>
<td id="S6.T2.2.6.6.5" class="ltx_td ltx_align_center">91.4</td>
<td id="S6.T2.2.6.6.6" class="ltx_td ltx_align_center">28.81</td>
<td id="S6.T2.2.6.6.7" class="ltx_td ltx_align_center">37.64</td>
<td id="S6.T2.2.6.6.8" class="ltx_td ltx_align_center">48.58</td>
<td id="S6.T2.2.6.6.9" class="ltx_td ltx_align_center">48.58</td>
</tr>
<tr id="S6.T2.2.7.7" class="ltx_tr">
<td id="S6.T2.2.7.7.1" class="ltx_td ltx_align_center ltx_border_bb">FedOS</td>
<td id="S6.T2.2.7.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T2.2.7.7.2.1" class="ltx_text ltx_font_bold">77.92</span></td>
<td id="S6.T2.2.7.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T2.2.7.7.3.1" class="ltx_text ltx_font_bold">84.09</span></td>
<td id="S6.T2.2.7.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T2.2.7.7.4.1" class="ltx_text ltx_font_bold">94.98</span></td>
<td id="S6.T2.2.7.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T2.2.7.7.5.1" class="ltx_text ltx_font_bold">95.85</span></td>
<td id="S6.T2.2.7.7.6" class="ltx_td ltx_align_center ltx_border_bb">46.6</td>
<td id="S6.T2.2.7.7.7" class="ltx_td ltx_align_center ltx_border_bb">56.13</td>
<td id="S6.T2.2.7.7.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T2.2.7.7.8.1" class="ltx_text ltx_font_bold">64.7</span></td>
<td id="S6.T2.2.7.7.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T2.2.7.7.9.1" class="ltx_text ltx_font_bold">69.11</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S6.T2.4.2" class="ltx_text" style="font-size:90%;">Performance comparison of different methods</span></figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>FedOS ablation study</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.20" class="ltx_p">To study how the different hyperparameters affect our model behavior we conduct a detailed ablation study on all 4 parameters (local epochs <math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mi id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><ci id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">E</annotation></semantics></math>, reporting client fraction <math id="S6.SS3.p1.2.m2.1" class="ltx_Math" alttext="CL" display="inline"><semantics id="S6.SS3.p1.2.m2.1a"><mrow id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml"><mi id="S6.SS3.p1.2.m2.1.1.2" xref="S6.SS3.p1.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.2.m2.1.1.1" xref="S6.SS3.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S6.SS3.p1.2.m2.1.1.3" xref="S6.SS3.p1.2.m2.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><apply id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1"><times id="S6.SS3.p1.2.m2.1.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1.1"></times><ci id="S6.SS3.p1.2.m2.1.1.2.cmml" xref="S6.SS3.p1.2.m2.1.1.2">ğ¶</ci><ci id="S6.SS3.p1.2.m2.1.1.3.cmml" xref="S6.SS3.p1.2.m2.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">CL</annotation></semantics></math>, unknown class
weight <math id="S6.SS3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{W_{U}}" display="inline"><semantics id="S6.SS3.p1.3.m3.1a"><msub id="S6.SS3.p1.3.m3.1.1" xref="S6.SS3.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.3.m3.1.1.2" xref="S6.SS3.p1.3.m3.1.1.2.cmml">ğ’²</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.3.m3.1.1.3" xref="S6.SS3.p1.3.m3.1.1.3.cmml">ğ’°</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.3.m3.1b"><apply id="S6.SS3.p1.3.m3.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S6.SS3.p1.3.m3.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.2">ğ’²</ci><ci id="S6.SS3.p1.3.m3.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.3">ğ’°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.3.m3.1c">\mathcal{W_{U}}</annotation></semantics></math>, and unknown class fraction <math id="S6.SS3.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{F_{U}}" display="inline"><semantics id="S6.SS3.p1.4.m4.1a"><msub id="S6.SS3.p1.4.m4.1.1" xref="S6.SS3.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.4.m4.1.1.2" xref="S6.SS3.p1.4.m4.1.1.2.cmml">â„±</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.4.m4.1.1.3" xref="S6.SS3.p1.4.m4.1.1.3.cmml">ğ’°</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.4.m4.1b"><apply id="S6.SS3.p1.4.m4.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.4.m4.1.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S6.SS3.p1.4.m4.1.1.2.cmml" xref="S6.SS3.p1.4.m4.1.1.2">â„±</ci><ci id="S6.SS3.p1.4.m4.1.1.3.cmml" xref="S6.SS3.p1.4.m4.1.1.3">ğ’°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.4.m4.1c">\mathcal{F_{U}}</annotation></semantics></math>). For each of them, we perform tests varying only one value while keeping all other parameters constant with default settings (refer to <a href="#S5.F3" title="Figure 3 â€£ 5.2 Group normalization â€£ 5 Normalization â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for the default values). For high homogeneity levels (<math id="S6.SS3.p1.5.m5.1" class="ltx_Math" alttext="\alpha=100" display="inline"><semantics id="S6.SS3.p1.5.m5.1a"><mrow id="S6.SS3.p1.5.m5.1.1" xref="S6.SS3.p1.5.m5.1.1.cmml"><mi id="S6.SS3.p1.5.m5.1.1.2" xref="S6.SS3.p1.5.m5.1.1.2.cmml">Î±</mi><mo id="S6.SS3.p1.5.m5.1.1.1" xref="S6.SS3.p1.5.m5.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.5.m5.1.1.3" xref="S6.SS3.p1.5.m5.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.5.m5.1b"><apply id="S6.SS3.p1.5.m5.1.1.cmml" xref="S6.SS3.p1.5.m5.1.1"><eq id="S6.SS3.p1.5.m5.1.1.1.cmml" xref="S6.SS3.p1.5.m5.1.1.1"></eq><ci id="S6.SS3.p1.5.m5.1.1.2.cmml" xref="S6.SS3.p1.5.m5.1.1.2">ğ›¼</ci><cn type="integer" id="S6.SS3.p1.5.m5.1.1.3.cmml" xref="S6.SS3.p1.5.m5.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.5.m5.1c">\alpha=100</annotation></semantics></math>) the method behaves similarly to FedAvg <a href="#S3.F1" title="In 3.1 Centralized Setting â€£ 3 Baseline â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> where increasing local epochs or client fraction boosts performance. On the other side varying the parameters controlling the unknown class doesnâ€™t have the same effect simply since at high values of <math id="S6.SS3.p1.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S6.SS3.p1.6.m6.1a"><mi id="S6.SS3.p1.6.m6.1.1" xref="S6.SS3.p1.6.m6.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.6.m6.1b"><ci id="S6.SS3.p1.6.m6.1.1.cmml" xref="S6.SS3.p1.6.m6.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.6.m6.1c">\alpha</annotation></semantics></math> the new unknown class isnâ€™t strictly needed. It is still interesting to point out that for <math id="S6.SS3.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{W_{U}}=0.5" display="inline"><semantics id="S6.SS3.p1.7.m7.1a"><mrow id="S6.SS3.p1.7.m7.1.1" xref="S6.SS3.p1.7.m7.1.1.cmml"><msub id="S6.SS3.p1.7.m7.1.1.2" xref="S6.SS3.p1.7.m7.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.7.m7.1.1.2.2" xref="S6.SS3.p1.7.m7.1.1.2.2.cmml">ğ’²</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.7.m7.1.1.2.3" xref="S6.SS3.p1.7.m7.1.1.2.3.cmml">ğ’°</mi></msub><mo id="S6.SS3.p1.7.m7.1.1.1" xref="S6.SS3.p1.7.m7.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.7.m7.1.1.3" xref="S6.SS3.p1.7.m7.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.7.m7.1b"><apply id="S6.SS3.p1.7.m7.1.1.cmml" xref="S6.SS3.p1.7.m7.1.1"><eq id="S6.SS3.p1.7.m7.1.1.1.cmml" xref="S6.SS3.p1.7.m7.1.1.1"></eq><apply id="S6.SS3.p1.7.m7.1.1.2.cmml" xref="S6.SS3.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S6.SS3.p1.7.m7.1.1.2.1.cmml" xref="S6.SS3.p1.7.m7.1.1.2">subscript</csymbol><ci id="S6.SS3.p1.7.m7.1.1.2.2.cmml" xref="S6.SS3.p1.7.m7.1.1.2.2">ğ’²</ci><ci id="S6.SS3.p1.7.m7.1.1.2.3.cmml" xref="S6.SS3.p1.7.m7.1.1.2.3">ğ’°</ci></apply><cn type="float" id="S6.SS3.p1.7.m7.1.1.3.cmml" xref="S6.SS3.p1.7.m7.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.7.m7.1c">\mathcal{W_{U}}=0.5</annotation></semantics></math> the accuracy is higher than FedAvg with the same settings, which shows that the additional class could have a regularizing effect. Moving down to <math id="S6.SS3.p1.8.m8.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S6.SS3.p1.8.m8.1a"><mrow id="S6.SS3.p1.8.m8.1.1" xref="S6.SS3.p1.8.m8.1.1.cmml"><mi id="S6.SS3.p1.8.m8.1.1.2" xref="S6.SS3.p1.8.m8.1.1.2.cmml">Î±</mi><mo id="S6.SS3.p1.8.m8.1.1.1" xref="S6.SS3.p1.8.m8.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.8.m8.1.1.3" xref="S6.SS3.p1.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.8.m8.1b"><apply id="S6.SS3.p1.8.m8.1.1.cmml" xref="S6.SS3.p1.8.m8.1.1"><eq id="S6.SS3.p1.8.m8.1.1.1.cmml" xref="S6.SS3.p1.8.m8.1.1.1"></eq><ci id="S6.SS3.p1.8.m8.1.1.2.cmml" xref="S6.SS3.p1.8.m8.1.1.2">ğ›¼</ci><cn type="integer" id="S6.SS3.p1.8.m8.1.1.3.cmml" xref="S6.SS3.p1.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.8.m8.1c">\alpha=1</annotation></semantics></math> we still have the same effect for <math id="S6.SS3.p1.9.m9.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S6.SS3.p1.9.m9.1a"><mi id="S6.SS3.p1.9.m9.1.1" xref="S6.SS3.p1.9.m9.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.9.m9.1b"><ci id="S6.SS3.p1.9.m9.1.1.cmml" xref="S6.SS3.p1.9.m9.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.9.m9.1c">E</annotation></semantics></math> and <math id="S6.SS3.p1.10.m10.1" class="ltx_Math" alttext="CL" display="inline"><semantics id="S6.SS3.p1.10.m10.1a"><mrow id="S6.SS3.p1.10.m10.1.1" xref="S6.SS3.p1.10.m10.1.1.cmml"><mi id="S6.SS3.p1.10.m10.1.1.2" xref="S6.SS3.p1.10.m10.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.10.m10.1.1.1" xref="S6.SS3.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S6.SS3.p1.10.m10.1.1.3" xref="S6.SS3.p1.10.m10.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.10.m10.1b"><apply id="S6.SS3.p1.10.m10.1.1.cmml" xref="S6.SS3.p1.10.m10.1.1"><times id="S6.SS3.p1.10.m10.1.1.1.cmml" xref="S6.SS3.p1.10.m10.1.1.1"></times><ci id="S6.SS3.p1.10.m10.1.1.2.cmml" xref="S6.SS3.p1.10.m10.1.1.2">ğ¶</ci><ci id="S6.SS3.p1.10.m10.1.1.3.cmml" xref="S6.SS3.p1.10.m10.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.10.m10.1c">CL</annotation></semantics></math>, however, for <math id="S6.SS3.p1.11.m11.1" class="ltx_Math" alttext="\mathcal{W_{U}}=1.5" display="inline"><semantics id="S6.SS3.p1.11.m11.1a"><mrow id="S6.SS3.p1.11.m11.1.1" xref="S6.SS3.p1.11.m11.1.1.cmml"><msub id="S6.SS3.p1.11.m11.1.1.2" xref="S6.SS3.p1.11.m11.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.11.m11.1.1.2.2" xref="S6.SS3.p1.11.m11.1.1.2.2.cmml">ğ’²</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.11.m11.1.1.2.3" xref="S6.SS3.p1.11.m11.1.1.2.3.cmml">ğ’°</mi></msub><mo id="S6.SS3.p1.11.m11.1.1.1" xref="S6.SS3.p1.11.m11.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.11.m11.1.1.3" xref="S6.SS3.p1.11.m11.1.1.3.cmml">1.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.11.m11.1b"><apply id="S6.SS3.p1.11.m11.1.1.cmml" xref="S6.SS3.p1.11.m11.1.1"><eq id="S6.SS3.p1.11.m11.1.1.1.cmml" xref="S6.SS3.p1.11.m11.1.1.1"></eq><apply id="S6.SS3.p1.11.m11.1.1.2.cmml" xref="S6.SS3.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="S6.SS3.p1.11.m11.1.1.2.1.cmml" xref="S6.SS3.p1.11.m11.1.1.2">subscript</csymbol><ci id="S6.SS3.p1.11.m11.1.1.2.2.cmml" xref="S6.SS3.p1.11.m11.1.1.2.2">ğ’²</ci><ci id="S6.SS3.p1.11.m11.1.1.2.3.cmml" xref="S6.SS3.p1.11.m11.1.1.2.3">ğ’°</ci></apply><cn type="float" id="S6.SS3.p1.11.m11.1.1.3.cmml" xref="S6.SS3.p1.11.m11.1.1.3">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.11.m11.1c">\mathcal{W_{U}}=1.5</annotation></semantics></math> we have better performance and stability (max and mean accuracy). Similarly for increasing <math id="S6.SS3.p1.12.m12.1" class="ltx_Math" alttext="\mathcal{F_{U}}" display="inline"><semantics id="S6.SS3.p1.12.m12.1a"><msub id="S6.SS3.p1.12.m12.1.1" xref="S6.SS3.p1.12.m12.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.12.m12.1.1.2" xref="S6.SS3.p1.12.m12.1.1.2.cmml">â„±</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.12.m12.1.1.3" xref="S6.SS3.p1.12.m12.1.1.3.cmml">ğ’°</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.12.m12.1b"><apply id="S6.SS3.p1.12.m12.1.1.cmml" xref="S6.SS3.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.12.m12.1.1.1.cmml" xref="S6.SS3.p1.12.m12.1.1">subscript</csymbol><ci id="S6.SS3.p1.12.m12.1.1.2.cmml" xref="S6.SS3.p1.12.m12.1.1.2">â„±</ci><ci id="S6.SS3.p1.12.m12.1.1.3.cmml" xref="S6.SS3.p1.12.m12.1.1.3">ğ’°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.12.m12.1c">\mathcal{F_{U}}</annotation></semantics></math> we obtain better results. Setting <math id="S6.SS3.p1.13.m13.1" class="ltx_Math" alttext="\mathcal{F_{U}}=0.8" display="inline"><semantics id="S6.SS3.p1.13.m13.1a"><mrow id="S6.SS3.p1.13.m13.1.1" xref="S6.SS3.p1.13.m13.1.1.cmml"><msub id="S6.SS3.p1.13.m13.1.1.2" xref="S6.SS3.p1.13.m13.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.13.m13.1.1.2.2" xref="S6.SS3.p1.13.m13.1.1.2.2.cmml">â„±</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.13.m13.1.1.2.3" xref="S6.SS3.p1.13.m13.1.1.2.3.cmml">ğ’°</mi></msub><mo id="S6.SS3.p1.13.m13.1.1.1" xref="S6.SS3.p1.13.m13.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.13.m13.1.1.3" xref="S6.SS3.p1.13.m13.1.1.3.cmml">0.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.13.m13.1b"><apply id="S6.SS3.p1.13.m13.1.1.cmml" xref="S6.SS3.p1.13.m13.1.1"><eq id="S6.SS3.p1.13.m13.1.1.1.cmml" xref="S6.SS3.p1.13.m13.1.1.1"></eq><apply id="S6.SS3.p1.13.m13.1.1.2.cmml" xref="S6.SS3.p1.13.m13.1.1.2"><csymbol cd="ambiguous" id="S6.SS3.p1.13.m13.1.1.2.1.cmml" xref="S6.SS3.p1.13.m13.1.1.2">subscript</csymbol><ci id="S6.SS3.p1.13.m13.1.1.2.2.cmml" xref="S6.SS3.p1.13.m13.1.1.2.2">â„±</ci><ci id="S6.SS3.p1.13.m13.1.1.2.3.cmml" xref="S6.SS3.p1.13.m13.1.1.2.3">ğ’°</ci></apply><cn type="float" id="S6.SS3.p1.13.m13.1.1.3.cmml" xref="S6.SS3.p1.13.m13.1.1.3">0.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.13.m13.1c">\mathcal{F_{U}}=0.8</annotation></semantics></math> is overkill. Even if it has a slightly better performance than 0.4, it has decreased training stability. The most interesting result is at <math id="S6.SS3.p1.14.m14.1" class="ltx_Math" alttext="\alpha=0.01" display="inline"><semantics id="S6.SS3.p1.14.m14.1a"><mrow id="S6.SS3.p1.14.m14.1.1" xref="S6.SS3.p1.14.m14.1.1.cmml"><mi id="S6.SS3.p1.14.m14.1.1.2" xref="S6.SS3.p1.14.m14.1.1.2.cmml">Î±</mi><mo id="S6.SS3.p1.14.m14.1.1.1" xref="S6.SS3.p1.14.m14.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.14.m14.1.1.3" xref="S6.SS3.p1.14.m14.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.14.m14.1b"><apply id="S6.SS3.p1.14.m14.1.1.cmml" xref="S6.SS3.p1.14.m14.1.1"><eq id="S6.SS3.p1.14.m14.1.1.1.cmml" xref="S6.SS3.p1.14.m14.1.1.1"></eq><ci id="S6.SS3.p1.14.m14.1.1.2.cmml" xref="S6.SS3.p1.14.m14.1.1.2">ğ›¼</ci><cn type="float" id="S6.SS3.p1.14.m14.1.1.3.cmml" xref="S6.SS3.p1.14.m14.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.14.m14.1c">\alpha=0.01</annotation></semantics></math> where our method is at its full potential. The method outperforms FedAvg results for hyperparameter settings. Performing a few additional local epochs further enhances performance, it does, however, drop drastically at <math id="S6.SS3.p1.15.m15.1" class="ltx_Math" alttext="E=5" display="inline"><semantics id="S6.SS3.p1.15.m15.1a"><mrow id="S6.SS3.p1.15.m15.1.1" xref="S6.SS3.p1.15.m15.1.1.cmml"><mi id="S6.SS3.p1.15.m15.1.1.2" xref="S6.SS3.p1.15.m15.1.1.2.cmml">E</mi><mo id="S6.SS3.p1.15.m15.1.1.1" xref="S6.SS3.p1.15.m15.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.15.m15.1.1.3" xref="S6.SS3.p1.15.m15.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.15.m15.1b"><apply id="S6.SS3.p1.15.m15.1.1.cmml" xref="S6.SS3.p1.15.m15.1.1"><eq id="S6.SS3.p1.15.m15.1.1.1.cmml" xref="S6.SS3.p1.15.m15.1.1.1"></eq><ci id="S6.SS3.p1.15.m15.1.1.2.cmml" xref="S6.SS3.p1.15.m15.1.1.2">ğ¸</ci><cn type="integer" id="S6.SS3.p1.15.m15.1.1.3.cmml" xref="S6.SS3.p1.15.m15.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.15.m15.1c">E=5</annotation></semantics></math>. Similar behavior occurs with <math id="S6.SS3.p1.16.m16.1" class="ltx_Math" alttext="CL" display="inline"><semantics id="S6.SS3.p1.16.m16.1a"><mrow id="S6.SS3.p1.16.m16.1.1" xref="S6.SS3.p1.16.m16.1.1.cmml"><mi id="S6.SS3.p1.16.m16.1.1.2" xref="S6.SS3.p1.16.m16.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.16.m16.1.1.1" xref="S6.SS3.p1.16.m16.1.1.1.cmml">â€‹</mo><mi id="S6.SS3.p1.16.m16.1.1.3" xref="S6.SS3.p1.16.m16.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.16.m16.1b"><apply id="S6.SS3.p1.16.m16.1.1.cmml" xref="S6.SS3.p1.16.m16.1.1"><times id="S6.SS3.p1.16.m16.1.1.1.cmml" xref="S6.SS3.p1.16.m16.1.1.1"></times><ci id="S6.SS3.p1.16.m16.1.1.2.cmml" xref="S6.SS3.p1.16.m16.1.1.2">ğ¶</ci><ci id="S6.SS3.p1.16.m16.1.1.3.cmml" xref="S6.SS3.p1.16.m16.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.16.m16.1c">CL</annotation></semantics></math>, but contrary to <math id="S6.SS3.p1.17.m17.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S6.SS3.p1.17.m17.1a"><mi id="S6.SS3.p1.17.m17.1.1" xref="S6.SS3.p1.17.m17.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.17.m17.1b"><ci id="S6.SS3.p1.17.m17.1.1.cmml" xref="S6.SS3.p1.17.m17.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.17.m17.1c">E</annotation></semantics></math>, model stability keeps improving. Changing <math id="S6.SS3.p1.18.m18.1" class="ltx_Math" alttext="\mathcal{W_{U}}" display="inline"><semantics id="S6.SS3.p1.18.m18.1a"><msub id="S6.SS3.p1.18.m18.1.1" xref="S6.SS3.p1.18.m18.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.18.m18.1.1.2" xref="S6.SS3.p1.18.m18.1.1.2.cmml">ğ’²</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.18.m18.1.1.3" xref="S6.SS3.p1.18.m18.1.1.3.cmml">ğ’°</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.18.m18.1b"><apply id="S6.SS3.p1.18.m18.1.1.cmml" xref="S6.SS3.p1.18.m18.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.18.m18.1.1.1.cmml" xref="S6.SS3.p1.18.m18.1.1">subscript</csymbol><ci id="S6.SS3.p1.18.m18.1.1.2.cmml" xref="S6.SS3.p1.18.m18.1.1.2">ğ’²</ci><ci id="S6.SS3.p1.18.m18.1.1.3.cmml" xref="S6.SS3.p1.18.m18.1.1.3">ğ’°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.18.m18.1c">\mathcal{W_{U}}</annotation></semantics></math> seems to have minor effects for this <math id="S6.SS3.p1.19.m19.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S6.SS3.p1.19.m19.1a"><mi id="S6.SS3.p1.19.m19.1.1" xref="S6.SS3.p1.19.m19.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.19.m19.1b"><ci id="S6.SS3.p1.19.m19.1.1.cmml" xref="S6.SS3.p1.19.m19.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.19.m19.1c">\alpha</annotation></semantics></math>, however, choosing the highest <math id="S6.SS3.p1.20.m20.1" class="ltx_Math" alttext="\mathcal{F_{U}}" display="inline"><semantics id="S6.SS3.p1.20.m20.1a"><msub id="S6.SS3.p1.20.m20.1.1" xref="S6.SS3.p1.20.m20.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.20.m20.1.1.2" xref="S6.SS3.p1.20.m20.1.1.2.cmml">â„±</mi><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.20.m20.1.1.3" xref="S6.SS3.p1.20.m20.1.1.3.cmml">ğ’°</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.20.m20.1b"><apply id="S6.SS3.p1.20.m20.1.1.cmml" xref="S6.SS3.p1.20.m20.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.20.m20.1.1.1.cmml" xref="S6.SS3.p1.20.m20.1.1">subscript</csymbol><ci id="S6.SS3.p1.20.m20.1.1.2.cmml" xref="S6.SS3.p1.20.m20.1.1.2">â„±</ci><ci id="S6.SS3.p1.20.m20.1.1.3.cmml" xref="S6.SS3.p1.20.m20.1.1.3">ğ’°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.20.m20.1c">\mathcal{F_{U}}</annotation></semantics></math> boosts both stability and performance in agreement with our hypothesis, where we mentioned that those unknown classes help the model learn discriminative features better for very low alphas.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Experiments</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.20" class="ltx_p">Finally, we want to evaluate the modelâ€™s accuracy on the test set. We will compare the baseline FedAvg algorithm to our extension proposal, named FedOS in this context, and two alternative client update aggregation schemes from literature, FedIR, and FedProx. Our experiments are limited to the more challenging values of <math id="S7.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S7.p1.1.m1.1a"><mi id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><ci id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">\alpha</annotation></semantics></math>, namely <math id="S7.p1.2.m2.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S7.p1.2.m2.1a"><mrow id="S7.p1.2.m2.1.1" xref="S7.p1.2.m2.1.1.cmml"><mi id="S7.p1.2.m2.1.1.2" xref="S7.p1.2.m2.1.1.2.cmml">Î±</mi><mo id="S7.p1.2.m2.1.1.1" xref="S7.p1.2.m2.1.1.1.cmml">=</mo><mn id="S7.p1.2.m2.1.1.3" xref="S7.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.2.m2.1b"><apply id="S7.p1.2.m2.1.1.cmml" xref="S7.p1.2.m2.1.1"><eq id="S7.p1.2.m2.1.1.1.cmml" xref="S7.p1.2.m2.1.1.1"></eq><ci id="S7.p1.2.m2.1.1.2.cmml" xref="S7.p1.2.m2.1.1.2">ğ›¼</ci><cn type="integer" id="S7.p1.2.m2.1.1.3.cmml" xref="S7.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.2.m2.1c">\alpha=1</annotation></semantics></math> and <math id="S7.p1.3.m3.1" class="ltx_Math" alttext="\alpha=0.01" display="inline"><semantics id="S7.p1.3.m3.1a"><mrow id="S7.p1.3.m3.1.1" xref="S7.p1.3.m3.1.1.cmml"><mi id="S7.p1.3.m3.1.1.2" xref="S7.p1.3.m3.1.1.2.cmml">Î±</mi><mo id="S7.p1.3.m3.1.1.1" xref="S7.p1.3.m3.1.1.1.cmml">=</mo><mn id="S7.p1.3.m3.1.1.3" xref="S7.p1.3.m3.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.3.m3.1b"><apply id="S7.p1.3.m3.1.1.cmml" xref="S7.p1.3.m3.1.1"><eq id="S7.p1.3.m3.1.1.1.cmml" xref="S7.p1.3.m3.1.1.1"></eq><ci id="S7.p1.3.m3.1.1.2.cmml" xref="S7.p1.3.m3.1.1.2">ğ›¼</ci><cn type="float" id="S7.p1.3.m3.1.1.3.cmml" xref="S7.p1.3.m3.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.3.m3.1c">\alpha=0.01</annotation></semantics></math>. All other hyperparameters remain unchanged. We split the dataset among <math id="S7.p1.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S7.p1.4.m4.1a"><mn id="S7.p1.4.m4.1.1" xref="S7.p1.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S7.p1.4.m4.1b"><cn type="integer" id="S7.p1.4.m4.1.1.cmml" xref="S7.p1.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.4.m4.1c">20</annotation></semantics></math> clients according to the distribution defined by <math id="S7.p1.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S7.p1.5.m5.1a"><mi id="S7.p1.5.m5.1.1" xref="S7.p1.5.m5.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S7.p1.5.m5.1b"><ci id="S7.p1.5.m5.1.1.cmml" xref="S7.p1.5.m5.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.5.m5.1c">\alpha</annotation></semantics></math> and use this dataset for all <math id="S7.p1.6.m6.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S7.p1.6.m6.1a"><mn id="S7.p1.6.m6.1.1" xref="S7.p1.6.m6.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S7.p1.6.m6.1b"><cn type="integer" id="S7.p1.6.m6.1.1.cmml" xref="S7.p1.6.m6.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.6.m6.1c">4</annotation></semantics></math> algorithms. Training is done over <math id="S7.p1.7.m7.1" class="ltx_Math" alttext="125" display="inline"><semantics id="S7.p1.7.m7.1a"><mn id="S7.p1.7.m7.1.1" xref="S7.p1.7.m7.1.1.cmml">125</mn><annotation-xml encoding="MathML-Content" id="S7.p1.7.m7.1b"><cn type="integer" id="S7.p1.7.m7.1.1.cmml" xref="S7.p1.7.m7.1.1">125</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.7.m7.1c">125</annotation></semantics></math>, <math id="S7.p1.8.m8.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S7.p1.8.m8.1a"><mn id="S7.p1.8.m8.1.1" xref="S7.p1.8.m8.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S7.p1.8.m8.1b"><cn type="integer" id="S7.p1.8.m8.1.1.cmml" xref="S7.p1.8.m8.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.8.m8.1c">250</annotation></semantics></math>, <math id="S7.p1.9.m9.1" class="ltx_Math" alttext="375" display="inline"><semantics id="S7.p1.9.m9.1a"><mn id="S7.p1.9.m9.1.1" xref="S7.p1.9.m9.1.1.cmml">375</mn><annotation-xml encoding="MathML-Content" id="S7.p1.9.m9.1b"><cn type="integer" id="S7.p1.9.m9.1.1.cmml" xref="S7.p1.9.m9.1.1">375</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.9.m9.1c">375</annotation></semantics></math>, and <math id="S7.p1.10.m10.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S7.p1.10.m10.1a"><mn id="S7.p1.10.m10.1.1" xref="S7.p1.10.m10.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S7.p1.10.m10.1b"><cn type="integer" id="S7.p1.10.m10.1.1.cmml" xref="S7.p1.10.m10.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.10.m10.1c">500</annotation></semantics></math> rounds respectively, picking <math id="S7.p1.11.m11.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S7.p1.11.m11.1a"><mrow id="S7.p1.11.m11.1.1" xref="S7.p1.11.m11.1.1.cmml"><mn id="S7.p1.11.m11.1.1.2" xref="S7.p1.11.m11.1.1.2.cmml">20</mn><mo id="S7.p1.11.m11.1.1.1" xref="S7.p1.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.11.m11.1b"><apply id="S7.p1.11.m11.1.1.cmml" xref="S7.p1.11.m11.1.1"><csymbol cd="latexml" id="S7.p1.11.m11.1.1.1.cmml" xref="S7.p1.11.m11.1.1.1">percent</csymbol><cn type="integer" id="S7.p1.11.m11.1.1.2.cmml" xref="S7.p1.11.m11.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.11.m11.1c">20\%</annotation></semantics></math> of the clients each round to train for <math id="S7.p1.12.m12.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.p1.12.m12.1a"><mn id="S7.p1.12.m12.1.1" xref="S7.p1.12.m12.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.p1.12.m12.1b"><cn type="integer" id="S7.p1.12.m12.1.1.cmml" xref="S7.p1.12.m12.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.12.m12.1c">1</annotation></semantics></math> local epoch. The modelâ€™s weights used for evaluation are those that achieved the highest accuracy on a validation dataset during the training process. We choose this approach due to the aforementioned instability while training for low values of <math id="S7.p1.13.m13.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S7.p1.13.m13.1a"><mi id="S7.p1.13.m13.1.1" xref="S7.p1.13.m13.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S7.p1.13.m13.1b"><ci id="S7.p1.13.m13.1.1.cmml" xref="S7.p1.13.m13.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.13.m13.1c">\alpha</annotation></semantics></math>. Our extension is implemented with DCGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, down-scaling the GANâ€™s width so that it is close in size to our inference model, LeNet-5. The GAN network is trained for 90 epochs on 50% of randomly selected Tiny Imagenet samples. For our extension we set <math id="S7.p1.14.m14.1" class="ltx_Math" alttext="\mathcal{W_{U}}=1.5" display="inline"><semantics id="S7.p1.14.m14.1a"><mrow id="S7.p1.14.m14.1.1" xref="S7.p1.14.m14.1.1.cmml"><msub id="S7.p1.14.m14.1.1.2" xref="S7.p1.14.m14.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S7.p1.14.m14.1.1.2.2" xref="S7.p1.14.m14.1.1.2.2.cmml">ğ’²</mi><mi class="ltx_font_mathcaligraphic" id="S7.p1.14.m14.1.1.2.3" xref="S7.p1.14.m14.1.1.2.3.cmml">ğ’°</mi></msub><mo id="S7.p1.14.m14.1.1.1" xref="S7.p1.14.m14.1.1.1.cmml">=</mo><mn id="S7.p1.14.m14.1.1.3" xref="S7.p1.14.m14.1.1.3.cmml">1.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.14.m14.1b"><apply id="S7.p1.14.m14.1.1.cmml" xref="S7.p1.14.m14.1.1"><eq id="S7.p1.14.m14.1.1.1.cmml" xref="S7.p1.14.m14.1.1.1"></eq><apply id="S7.p1.14.m14.1.1.2.cmml" xref="S7.p1.14.m14.1.1.2"><csymbol cd="ambiguous" id="S7.p1.14.m14.1.1.2.1.cmml" xref="S7.p1.14.m14.1.1.2">subscript</csymbol><ci id="S7.p1.14.m14.1.1.2.2.cmml" xref="S7.p1.14.m14.1.1.2.2">ğ’²</ci><ci id="S7.p1.14.m14.1.1.2.3.cmml" xref="S7.p1.14.m14.1.1.2.3">ğ’°</ci></apply><cn type="float" id="S7.p1.14.m14.1.1.3.cmml" xref="S7.p1.14.m14.1.1.3">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.14.m14.1c">\mathcal{W_{U}}=1.5</annotation></semantics></math>, <math id="S7.p1.15.m15.1" class="ltx_Math" alttext="\mathcal{F_{U}}=0.4" display="inline"><semantics id="S7.p1.15.m15.1a"><mrow id="S7.p1.15.m15.1.1" xref="S7.p1.15.m15.1.1.cmml"><msub id="S7.p1.15.m15.1.1.2" xref="S7.p1.15.m15.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S7.p1.15.m15.1.1.2.2" xref="S7.p1.15.m15.1.1.2.2.cmml">â„±</mi><mi class="ltx_font_mathcaligraphic" id="S7.p1.15.m15.1.1.2.3" xref="S7.p1.15.m15.1.1.2.3.cmml">ğ’°</mi></msub><mo id="S7.p1.15.m15.1.1.1" xref="S7.p1.15.m15.1.1.1.cmml">=</mo><mn id="S7.p1.15.m15.1.1.3" xref="S7.p1.15.m15.1.1.3.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.15.m15.1b"><apply id="S7.p1.15.m15.1.1.cmml" xref="S7.p1.15.m15.1.1"><eq id="S7.p1.15.m15.1.1.1.cmml" xref="S7.p1.15.m15.1.1.1"></eq><apply id="S7.p1.15.m15.1.1.2.cmml" xref="S7.p1.15.m15.1.1.2"><csymbol cd="ambiguous" id="S7.p1.15.m15.1.1.2.1.cmml" xref="S7.p1.15.m15.1.1.2">subscript</csymbol><ci id="S7.p1.15.m15.1.1.2.2.cmml" xref="S7.p1.15.m15.1.1.2.2">â„±</ci><ci id="S7.p1.15.m15.1.1.2.3.cmml" xref="S7.p1.15.m15.1.1.2.3">ğ’°</ci></apply><cn type="float" id="S7.p1.15.m15.1.1.3.cmml" xref="S7.p1.15.m15.1.1.3">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.15.m15.1c">\mathcal{F_{U}}=0.4</annotation></semantics></math> for <math id="S7.p1.16.m16.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S7.p1.16.m16.1a"><mrow id="S7.p1.16.m16.1.1" xref="S7.p1.16.m16.1.1.cmml"><mi id="S7.p1.16.m16.1.1.2" xref="S7.p1.16.m16.1.1.2.cmml">Î±</mi><mo id="S7.p1.16.m16.1.1.1" xref="S7.p1.16.m16.1.1.1.cmml">=</mo><mn id="S7.p1.16.m16.1.1.3" xref="S7.p1.16.m16.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.16.m16.1b"><apply id="S7.p1.16.m16.1.1.cmml" xref="S7.p1.16.m16.1.1"><eq id="S7.p1.16.m16.1.1.1.cmml" xref="S7.p1.16.m16.1.1.1"></eq><ci id="S7.p1.16.m16.1.1.2.cmml" xref="S7.p1.16.m16.1.1.2">ğ›¼</ci><cn type="integer" id="S7.p1.16.m16.1.1.3.cmml" xref="S7.p1.16.m16.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.16.m16.1c">\alpha=1</annotation></semantics></math> and <math id="S7.p1.17.m17.1" class="ltx_Math" alttext="\mathcal{W_{U}}=1" display="inline"><semantics id="S7.p1.17.m17.1a"><mrow id="S7.p1.17.m17.1.1" xref="S7.p1.17.m17.1.1.cmml"><msub id="S7.p1.17.m17.1.1.2" xref="S7.p1.17.m17.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S7.p1.17.m17.1.1.2.2" xref="S7.p1.17.m17.1.1.2.2.cmml">ğ’²</mi><mi class="ltx_font_mathcaligraphic" id="S7.p1.17.m17.1.1.2.3" xref="S7.p1.17.m17.1.1.2.3.cmml">ğ’°</mi></msub><mo id="S7.p1.17.m17.1.1.1" xref="S7.p1.17.m17.1.1.1.cmml">=</mo><mn id="S7.p1.17.m17.1.1.3" xref="S7.p1.17.m17.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.17.m17.1b"><apply id="S7.p1.17.m17.1.1.cmml" xref="S7.p1.17.m17.1.1"><eq id="S7.p1.17.m17.1.1.1.cmml" xref="S7.p1.17.m17.1.1.1"></eq><apply id="S7.p1.17.m17.1.1.2.cmml" xref="S7.p1.17.m17.1.1.2"><csymbol cd="ambiguous" id="S7.p1.17.m17.1.1.2.1.cmml" xref="S7.p1.17.m17.1.1.2">subscript</csymbol><ci id="S7.p1.17.m17.1.1.2.2.cmml" xref="S7.p1.17.m17.1.1.2.2">ğ’²</ci><ci id="S7.p1.17.m17.1.1.2.3.cmml" xref="S7.p1.17.m17.1.1.2.3">ğ’°</ci></apply><cn type="integer" id="S7.p1.17.m17.1.1.3.cmml" xref="S7.p1.17.m17.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.17.m17.1c">\mathcal{W_{U}}=1</annotation></semantics></math>, <math id="S7.p1.18.m18.1" class="ltx_Math" alttext="\mathcal{F_{U}}=0.8" display="inline"><semantics id="S7.p1.18.m18.1a"><mrow id="S7.p1.18.m18.1.1" xref="S7.p1.18.m18.1.1.cmml"><msub id="S7.p1.18.m18.1.1.2" xref="S7.p1.18.m18.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S7.p1.18.m18.1.1.2.2" xref="S7.p1.18.m18.1.1.2.2.cmml">â„±</mi><mi class="ltx_font_mathcaligraphic" id="S7.p1.18.m18.1.1.2.3" xref="S7.p1.18.m18.1.1.2.3.cmml">ğ’°</mi></msub><mo id="S7.p1.18.m18.1.1.1" xref="S7.p1.18.m18.1.1.1.cmml">=</mo><mn id="S7.p1.18.m18.1.1.3" xref="S7.p1.18.m18.1.1.3.cmml">0.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.18.m18.1b"><apply id="S7.p1.18.m18.1.1.cmml" xref="S7.p1.18.m18.1.1"><eq id="S7.p1.18.m18.1.1.1.cmml" xref="S7.p1.18.m18.1.1.1"></eq><apply id="S7.p1.18.m18.1.1.2.cmml" xref="S7.p1.18.m18.1.1.2"><csymbol cd="ambiguous" id="S7.p1.18.m18.1.1.2.1.cmml" xref="S7.p1.18.m18.1.1.2">subscript</csymbol><ci id="S7.p1.18.m18.1.1.2.2.cmml" xref="S7.p1.18.m18.1.1.2.2">â„±</ci><ci id="S7.p1.18.m18.1.1.2.3.cmml" xref="S7.p1.18.m18.1.1.2.3">ğ’°</ci></apply><cn type="float" id="S7.p1.18.m18.1.1.3.cmml" xref="S7.p1.18.m18.1.1.3">0.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.18.m18.1c">\mathcal{F_{U}}=0.8</annotation></semantics></math> for <math id="S7.p1.19.m19.1" class="ltx_Math" alttext="\alpha=0.01" display="inline"><semantics id="S7.p1.19.m19.1a"><mrow id="S7.p1.19.m19.1.1" xref="S7.p1.19.m19.1.1.cmml"><mi id="S7.p1.19.m19.1.1.2" xref="S7.p1.19.m19.1.1.2.cmml">Î±</mi><mo id="S7.p1.19.m19.1.1.1" xref="S7.p1.19.m19.1.1.1.cmml">=</mo><mn id="S7.p1.19.m19.1.1.3" xref="S7.p1.19.m19.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.19.m19.1b"><apply id="S7.p1.19.m19.1.1.cmml" xref="S7.p1.19.m19.1.1"><eq id="S7.p1.19.m19.1.1.1.cmml" xref="S7.p1.19.m19.1.1.1"></eq><ci id="S7.p1.19.m19.1.1.2.cmml" xref="S7.p1.19.m19.1.1.2">ğ›¼</ci><cn type="float" id="S7.p1.19.m19.1.1.3.cmml" xref="S7.p1.19.m19.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.19.m19.1c">\alpha=0.01</annotation></semantics></math>. For FedProx we use the standard parameter setting of <math id="S7.p1.20.m20.1" class="ltx_Math" alttext="\mu=1" display="inline"><semantics id="S7.p1.20.m20.1a"><mrow id="S7.p1.20.m20.1.1" xref="S7.p1.20.m20.1.1.cmml"><mi id="S7.p1.20.m20.1.1.2" xref="S7.p1.20.m20.1.1.2.cmml">Î¼</mi><mo id="S7.p1.20.m20.1.1.1" xref="S7.p1.20.m20.1.1.1.cmml">=</mo><mn id="S7.p1.20.m20.1.1.3" xref="S7.p1.20.m20.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p1.20.m20.1b"><apply id="S7.p1.20.m20.1.1.cmml" xref="S7.p1.20.m20.1.1"><eq id="S7.p1.20.m20.1.1.1.cmml" xref="S7.p1.20.m20.1.1.1"></eq><ci id="S7.p1.20.m20.1.1.2.cmml" xref="S7.p1.20.m20.1.1.2">ğœ‡</ci><cn type="integer" id="S7.p1.20.m20.1.1.3.cmml" xref="S7.p1.20.m20.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.20.m20.1c">\mu=1</annotation></semantics></math>. The results of these experiments are reported in <a href="#S6.T2" title="In 6.2 Training â€£ 6 Our method â€£ FedOS: using open-set learning to stabilize training in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.4" class="ltx_p">FedOS Outperforms all methods for <math id="S7.p2.1.m1.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S7.p2.1.m1.1a"><mrow id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml"><mi id="S7.p2.1.m1.1.1.2" xref="S7.p2.1.m1.1.1.2.cmml">Î±</mi><mo id="S7.p2.1.m1.1.1.1" xref="S7.p2.1.m1.1.1.1.cmml">=</mo><mn id="S7.p2.1.m1.1.1.3" xref="S7.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><apply id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1"><eq id="S7.p2.1.m1.1.1.1.cmml" xref="S7.p2.1.m1.1.1.1"></eq><ci id="S7.p2.1.m1.1.1.2.cmml" xref="S7.p2.1.m1.1.1.2">ğ›¼</ci><cn type="integer" id="S7.p2.1.m1.1.1.3.cmml" xref="S7.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">\alpha=1</annotation></semantics></math>. Sometimes even using much fewer training rounds, as in the case of <math id="S7.p2.2.m2.1" class="ltx_Math" alttext="375" display="inline"><semantics id="S7.p2.2.m2.1a"><mn id="S7.p2.2.m2.1.1" xref="S7.p2.2.m2.1.1.cmml">375</mn><annotation-xml encoding="MathML-Content" id="S7.p2.2.m2.1b"><cn type="integer" id="S7.p2.2.m2.1.1.cmml" xref="S7.p2.2.m2.1.1">375</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.2.m2.1c">375</annotation></semantics></math> rounds for FedOS and <math id="S7.p2.3.m3.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S7.p2.3.m3.1a"><mn id="S7.p2.3.m3.1.1" xref="S7.p2.3.m3.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S7.p2.3.m3.1b"><cn type="integer" id="S7.p2.3.m3.1.1.cmml" xref="S7.p2.3.m3.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.3.m3.1c">500</annotation></semantics></math> rounds for all other methods. In the case of <math id="S7.p2.4.m4.1" class="ltx_Math" alttext="\alpha=0.01" display="inline"><semantics id="S7.p2.4.m4.1a"><mrow id="S7.p2.4.m4.1.1" xref="S7.p2.4.m4.1.1.cmml"><mi id="S7.p2.4.m4.1.1.2" xref="S7.p2.4.m4.1.1.2.cmml">Î±</mi><mo id="S7.p2.4.m4.1.1.1" xref="S7.p2.4.m4.1.1.1.cmml">=</mo><mn id="S7.p2.4.m4.1.1.3" xref="S7.p2.4.m4.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.p2.4.m4.1b"><apply id="S7.p2.4.m4.1.1.cmml" xref="S7.p2.4.m4.1.1"><eq id="S7.p2.4.m4.1.1.1.cmml" xref="S7.p2.4.m4.1.1.1"></eq><ci id="S7.p2.4.m4.1.1.2.cmml" xref="S7.p2.4.m4.1.1.2">ğ›¼</ci><cn type="float" id="S7.p2.4.m4.1.1.3.cmml" xref="S7.p2.4.m4.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.4.m4.1c">\alpha=0.01</annotation></semantics></math>, the FedOS takes more rounds to reach good results, however, it does surpass FedIR at 375 and 500 rounds. Note that since for each round we are training on only 20% out of a total of 20 clients, only 4 clients are chosen to report their weight updates. In the case where each client only has a single class, the entire model trains on 4 each round. Thus, an interesting alternative study would be to increase the total number of clients, with each client having fewer samples.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this work, we explored Federated Learning by having a look at some of the challenges that come with it, specifically training on non-IID client datasets. Furthermore, we discussed several techniques to tackle these challenges and compared their relative performance. Finally, we present our own idea, to add unknown class samples during the training process to help the local model focus on learning to distinguish features from those classes, that are present on the client device. We show that our approach performs well for highly heterogeneous clients, as well as slightly boosts performance in more homogeneous test cases. Since both the quantity and quality of our experiments were limited by the computational power provided by Colab, we would like to both further validate our hypotheses in the future, as well as compare our novel approach to more state of the art methods.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
JimmyÂ Lei Ba, JamieÂ Ryan Kiros, and GeoffreyÂ E. Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Layer normalization.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Abhijit Bendale and TerranceÂ E. Boult.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Towards open set deep networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, abs/1511.06233, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
ZongYuan Ge, Sergey Demyanov, Zetao Chen, and Rahil Garnavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Generative openmax for multi-class open set classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1707.07418</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
IanÂ J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Generative adversarial networks, 2014.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, abs/1512.03385, 2015.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Tzu-MingÂ Harry Hsu, Hang Qi, and Matthew Brown.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Measuring the effects of non-identical data distribution for
federated visual classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.06335</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Tzu-MingÂ Harry Hsu, Hang Qi, and Matthew Brown.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Federated visual classification with real-world data distribution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 76â€“92.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Sergey Ioffe and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, abs/1502.03167, 2015.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Learning multiple layers of features from tiny images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">Technical report, 2009.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Ya Le and XuanÂ S. Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Tiny imagenet visual recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">2015.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Gradient-based learning applied to document recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 86(11):2278â€“2324, 1998.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Tian Li, AnitÂ Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Federated optimization in heterogeneous networks.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and BlaiseÂ Aguera y
Arcas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Communication-efficient learning of deep networks from decentralized
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Artificial intelligence and statistics</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 1273â€“1282.
PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
H.Â Brendan McMahan, Eider Moore, Daniel Ramage, and BlaiseÂ AgÃ¼era y
Arcas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Federated learning of deep networks using model averaging.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, abs/1602.05629, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Open set learning with counterfactual images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, September 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Luke Metz, and Soumith Chintala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Unsupervised representation learning with deep convolutional
generative adversarial networks, 2015.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Dmitry Ulyanov, Andrea Vedaldi, and VictorÂ S. Lempitsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Instance normalization: The missing ingredient for fast stylization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, abs/1607.08022, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Yuxin Wu and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Group normalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, abs/1803.08494, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2208.11511" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2208.11512" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.11512">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2208.11512" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2208.11513" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 17:24:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
