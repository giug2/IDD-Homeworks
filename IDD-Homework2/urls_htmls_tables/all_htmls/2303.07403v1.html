<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.07403] Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region</title><meta property="og:description" content="For glaciologists, studying ice sheets from the polar regions is critical. With the advancement of deep learning techniques, we can now extract high-level information from the ice sheet data (e.g., estimating the ice l…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.07403">

<!--Generated on Thu Feb 29 20:25:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">For glaciologists, studying ice sheets from the polar regions is critical. With the advancement of deep learning techniques, we can now extract high-level information from the ice sheet data (e.g., estimating the ice layer thickness, predicting the ice accumulation for upcoming years, etc.). However, a vision-based conversational deep learning approach has not been explored yet, where scientists can get information by asking questions about images. In this paper, we have introduced the task of Visual Question Answering (VQA) on remote-sensed ice sheet imagery. To study, we have presented a unique VQA dataset, <span id="id1.id1.1" class="ltx_text ltx_font_italic">Polar-VQA</span>, in this study. All the images in this dataset were collected using four types of airborne radars. The main objective of this research is to highlight the importance of VQA in the context of ice sheet research and conduct a baseline study of existing VQA approaches on <span id="id1.id1.2" class="ltx_text ltx_font_italic">Polar-VQA</span> dataset.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Exploring polar regions and retrieving information from the ice sheets have great significance in glaciology studies. With this aim, a large amount of data has been collected from various airborne radars, such as <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Accumulation Radar</span>, <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">Ku-Band Radar</span>, <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">Radar Depth Sounder</span>, <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">Snow Radar</span>, over several years. All the data are primarily gathered from various geo-locations (e.g., <span id="S1.p1.1.5" class="ltx_text ltx_font_italic">Antarctica, Greenland</span>) and zones such as the <span id="S1.p1.1.6" class="ltx_text ltx_font_italic">dry and wet</span> of those locations. By definition, dry-zone imagery is collected from polar regions at a distance from the coastal area, whereas wet-zone imagery is collected from adjacent places in the coastal area. Data from these various sources and different locations can be utilized to study and monitor the ice sheets. With the advancement of many deep learning techniques, many challenges have been efficiently addressed in recent times regarding polar ice sheet research. Those researches mainly focused on estimating ice-layer thickness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, ice-layer tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, physics-driven Deep learning simulation for generating ice-imagery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, etc. However, more different types of information can be extracted from the ice sheet data that are very useful for scientists to have a better understanding. Identifying the radar type, the geo-locations, and recognizing the zone from images will assist the scientist from a different point of view. For instance, while estimating the ice accumulation over years, it is very important to understand the impact of geo-location and the zone (dry or wet) on that estimation. Additionally, dry-zone images naturally have more layers than wet-zone images. Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> represents both dry and wet zone images. Scientists often find it hard to visually differentiate between dry and wet zone images. Thus, they need an intelligent opinion that can extract high-level features and provide correct information. These are the reasons why it is necessary to identify locations and zones from images. Though all the information could be stored in the metadata (i.e., where all the information regarding data collection parameters is kept), it is very time-consuming to extract information from the metadata.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.07403/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="183" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Visual Question Answering (VQA) framework for remote-sensed ice imagery. In this approach, an image and question are fed into the VQA model, and finally we get the answer to that question from the VQA model</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Thus, an interactive approach is needed where a scientist can get this information by asking questions about the image. “Is the image taken from accumulation radar or snow radar? ”, “What kind of zone is this?” are some examples of questions that could be asked by an expert to get that information. To address this issue, we have presented a visual question answering framework based on ice sheet data. Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> represents the VQA framework.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Visual question answering (VQA) is an interactive approach where we can ask questions based on images and get query-based rational responses. Many researches have been conducted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> on several applications of the VQA approach. Most of the research is limited to natural <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, medical <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, remote-sensing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> images. In this work, we have presented a unique VQA dataset, <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Polar-VQA</span>, on ice sheet data for the first time. The main focus while developing this dataset is to identify three main components from the images: locations, radar-type and zone through asking questions. These details are not visible within the collected images and are also difficult for a human to visually differentiate. As VQA is able to provide high-level scene understanding, we take advantage of this deep learning technique in our ice sheet research. In addition, there is no other deep-learning techniques that can provide these three types of information per image simultaneously. Image classification, segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>  are hardly suitable for this application. To the best of our knowledge, this is the first VQA application for ice sheet research.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2303.07403/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="312" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>(1) Left figure shows the examples of images from dry and wet zone respectively, (2) right figure shows the images collected using (a) Accumulation Radar, (b) Ku Band Radar, (c) Radar Depth Sounder (d) Snow Radar</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Polar-VQA Dataset</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Collection Process</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">All ice sheet data are collected in image format using four different types of radars: accumulation radar, ku band radar, radar depth sounder, and snow radar. Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the outcome of four type of these radar images. These airborne sensors are installed in the Uninhabited Aircraft System (UAS), which provides an aerial platform for ice-penetration. All the data are available at the Center for Remote Sensing of Ice Sheets (CReSIS) <a target="_blank" href="https://data.cresis.ku.edu/data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://data.cresis.ku.edu/data</a>. For our experiment, we only considered the data collected from Antarctica and Greenland.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Type of Questions</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">All the questions in our dataset mainly focus on identifying the type of sensors, the geo-locations of the ice images, and recognizing the dry and wet zone images, respectively. Thus, all the questions are classified into three categories: sensor category, place category, and zone category. In addition, all these categories are subdivided into two types of questions</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Open-Ended (OE)</span>: Questions in this category start with the “WH” word. It allows the VQA system to give free-form responses. “What type of sensor is this?”, “From which zone was the image taken?” are examples of this question category.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Closed-Ended (CE)</span>: Close-ended questions usually offer a VQA system with a limited number of answers, such as ‘yes/no’ or any multiple-choice answer. “Is this data from Antarctica or Greenland?”, “Is the image taken from snow radar?” are some of the examples of this kind.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Baseline Studies</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this study, we conduct baseline comparative analysis by considering the five baseline VQA models on <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">Polar-VQA</span> dataset.</p>
</div>
<div id="S3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">In <span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Question-only</span> baseline model, the main purpose is to investigate the performance of predicting answer directly from questions without seeing any image.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">CNN+LSTM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is a coarse VQA approach where image features, extracted by CNN, and question features, are extracted from LSTM, are combined by element-wise multiplication and fed into the classifier to predict the answer.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">VIS+LSTM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is a simple VQA approach . Firstly, a image is fed into a convolution layers (CNN) and take the output. Then this output is considered for initializing the LSTM layers which then try to predict the answers.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Stacked Attention Network (SAN)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is a attention-based VQA model. In this work-frame, multi-step visual attention is considered to predict the answer.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">Multi-modal Factorized Bilinear Pooling with Co-Attention (MFB+CoAtt)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is another kind of attention-based VQA model. Considering attention for both image and question level is the main characteristic of this model. In the fusion stage, image and question features are combined with multi-modal factorized bilinear pooling block described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span> Accuracy Comparison between Baseline VQA Models on test data</figcaption>
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.1.1" class="ltx_tr">
<th id="S3.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">VQA Model</th>
<th id="S3.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Overall</th>
<th id="S3.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Radar Category</th>
<th id="S3.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Place Category</th>
<th id="S3.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Zone Category</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.2.1" class="ltx_tr">
<td id="S3.T1.3.2.1.1" class="ltx_td ltx_align_center ltx_border_tt">Q-Only</td>
<td id="S3.T1.3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">0.51</td>
<td id="S3.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.63</td>
<td id="S3.T1.3.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.54</td>
<td id="S3.T1.3.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">0.43</td>
</tr>
<tr id="S3.T1.3.3.2" class="ltx_tr">
<td id="S3.T1.3.3.2.1" class="ltx_td ltx_align_center">VIS-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S3.T1.3.3.2.2" class="ltx_td ltx_align_center">0.96</td>
<td id="S3.T1.3.3.2.3" class="ltx_td ltx_align_center">0.96</td>
<td id="S3.T1.3.3.2.4" class="ltx_td ltx_align_center">0.93</td>
<td id="S3.T1.3.3.2.5" class="ltx_td ltx_align_center">0.99</td>
</tr>
<tr id="S3.T1.3.4.3" class="ltx_tr">
<td id="S3.T1.3.4.3.1" class="ltx_td ltx_align_center">CNN+LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S3.T1.3.4.3.2" class="ltx_td ltx_align_center">0.96</td>
<td id="S3.T1.3.4.3.3" class="ltx_td ltx_align_center">0.95</td>
<td id="S3.T1.3.4.3.4" class="ltx_td ltx_align_center">0.94</td>
<td id="S3.T1.3.4.3.5" class="ltx_td ltx_align_center">0.99</td>
</tr>
<tr id="S3.T1.3.5.4" class="ltx_tr">
<td id="S3.T1.3.5.4.1" class="ltx_td ltx_align_center">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S3.T1.3.5.4.2" class="ltx_td ltx_align_center"><span id="S3.T1.3.5.4.2.1" class="ltx_text ltx_font_bold">0.97</span></td>
<td id="S3.T1.3.5.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.3.5.4.3.1" class="ltx_text ltx_font_bold">0.97</span></td>
<td id="S3.T1.3.5.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.3.5.4.4.1" class="ltx_text ltx_font_bold">0.94</span></td>
<td id="S3.T1.3.5.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.3.5.4.5.1" class="ltx_text ltx_font_bold">0.99</span></td>
</tr>
<tr id="S3.T1.3.6.5" class="ltx_tr">
<td id="S3.T1.3.6.5.1" class="ltx_td ltx_align_center ltx_border_b">MFB-CoAttention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T1.3.6.5.2" class="ltx_td ltx_align_center ltx_border_b">0.97</td>
<td id="S3.T1.3.6.5.3" class="ltx_td ltx_align_center ltx_border_b">0.96</td>
<td id="S3.T1.3.6.5.4" class="ltx_td ltx_align_center ltx_border_b">0.94</td>
<td id="S3.T1.3.6.5.5" class="ltx_td ltx_align_center ltx_border_b">0.99</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Question-wise Accuracy Comparison Between Baseline Models for Each Question Category</figcaption>
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.3.1.1" class="ltx_tr">
<th id="S3.T2.3.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">VQA Model</th>
<th id="S3.T2.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Open-Ended</th>
<th id="S3.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Close-Ended</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.3.2.1" class="ltx_tr">
<th id="S3.T2.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="4"><span id="S3.T2.3.2.1.1.1" class="ltx_text">Zone</span></th>
<td id="S3.T2.3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">VIS-LSTM</td>
<td id="S3.T2.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">1</td>
<td id="S3.T2.3.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.995</td>
</tr>
<tr id="S3.T2.3.3.2" class="ltx_tr">
<td id="S3.T2.3.3.2.1" class="ltx_td ltx_align_center">CNN+LSTM</td>
<td id="S3.T2.3.3.2.2" class="ltx_td ltx_align_center">1</td>
<td id="S3.T2.3.3.2.3" class="ltx_td ltx_align_center">0.998</td>
</tr>
<tr id="S3.T2.3.4.3" class="ltx_tr">
<td id="S3.T2.3.4.3.1" class="ltx_td ltx_align_center">SAN</td>
<td id="S3.T2.3.4.3.2" class="ltx_td ltx_align_center">1</td>
<td id="S3.T2.3.4.3.3" class="ltx_td ltx_align_center">0.995</td>
</tr>
<tr id="S3.T2.3.5.4" class="ltx_tr">
<td id="S3.T2.3.5.4.1" class="ltx_td ltx_align_center">MFB-CoAttention</td>
<td id="S3.T2.3.5.4.2" class="ltx_td ltx_align_center">1</td>
<td id="S3.T2.3.5.4.3" class="ltx_td ltx_align_center">0.996</td>
</tr>
<tr id="S3.T2.3.6.5" class="ltx_tr">
<th id="S3.T2.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S3.T2.3.6.5.1.1" class="ltx_text">Place</span></th>
<td id="S3.T2.3.6.5.2" class="ltx_td ltx_align_center ltx_border_t">VIS-LSTM</td>
<td id="S3.T2.3.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.93</td>
<td id="S3.T2.3.6.5.4" class="ltx_td ltx_align_center ltx_border_t">0.93</td>
</tr>
<tr id="S3.T2.3.7.6" class="ltx_tr">
<td id="S3.T2.3.7.6.1" class="ltx_td ltx_align_center">CNN+LSTM</td>
<td id="S3.T2.3.7.6.2" class="ltx_td ltx_align_center">0.94</td>
<td id="S3.T2.3.7.6.3" class="ltx_td ltx_align_center">0.94</td>
</tr>
<tr id="S3.T2.3.8.7" class="ltx_tr">
<td id="S3.T2.3.8.7.1" class="ltx_td ltx_align_center">SAN</td>
<td id="S3.T2.3.8.7.2" class="ltx_td ltx_align_center">0.93</td>
<td id="S3.T2.3.8.7.3" class="ltx_td ltx_align_center">0.94</td>
</tr>
<tr id="S3.T2.3.9.8" class="ltx_tr">
<td id="S3.T2.3.9.8.1" class="ltx_td ltx_align_center">MFB-CoAttention</td>
<td id="S3.T2.3.9.8.2" class="ltx_td ltx_align_center">0.93</td>
<td id="S3.T2.3.9.8.3" class="ltx_td ltx_align_center">0.94</td>
</tr>
<tr id="S3.T2.3.10.9" class="ltx_tr">
<th id="S3.T2.3.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="4"><span id="S3.T2.3.10.9.1.1" class="ltx_text">Radar</span></th>
<td id="S3.T2.3.10.9.2" class="ltx_td ltx_align_center ltx_border_t">VIS-LSTM</td>
<td id="S3.T2.3.10.9.3" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S3.T2.3.10.9.4" class="ltx_td ltx_align_center ltx_border_t">0.96</td>
</tr>
<tr id="S3.T2.3.11.10" class="ltx_tr">
<td id="S3.T2.3.11.10.1" class="ltx_td ltx_align_center">CNN+LSTM</td>
<td id="S3.T2.3.11.10.2" class="ltx_td ltx_align_center">0.99</td>
<td id="S3.T2.3.11.10.3" class="ltx_td ltx_align_center">0.94</td>
</tr>
<tr id="S3.T2.3.12.11" class="ltx_tr">
<td id="S3.T2.3.12.11.1" class="ltx_td ltx_align_center">SAN</td>
<td id="S3.T2.3.12.11.2" class="ltx_td ltx_align_center">0.99</td>
<td id="S3.T2.3.12.11.3" class="ltx_td ltx_align_center">0.96</td>
</tr>
<tr id="S3.T2.3.13.12" class="ltx_tr">
<td id="S3.T2.3.13.12.1" class="ltx_td ltx_align_center ltx_border_b">MFB-CoAttention</td>
<td id="S3.T2.3.13.12.2" class="ltx_td ltx_align_center ltx_border_b">0.99</td>
<td id="S3.T2.3.13.12.3" class="ltx_td ltx_align_center ltx_border_b">0.95</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">We have considered the batch size 16, <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">adam</span> optimizer is taken as a optimizer and <span id="S3.p3.1.2" class="ltx_text ltx_font_italic">early stopping</span> criteria with patience 10 is considered for all the experiments. <span id="S3.p3.1.3" class="ltx_text ltx_font_italic"> VGG-16</span> model, and a <span id="S3.p3.1.4" class="ltx_text ltx_font_italic">one-layer LSTM</span> model are chosen for image and question feature extraction purposes, respectively. The whole network is trained end-to-end by minimizing the categorical loss between the predicted and ground-truth answers.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.2" class="ltx_p">From Table <a href="#S3.T1" title="Table 1 ‣ 3 Baseline Studies ‣ Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can see that prediction directly from question <span id="S3.p4.2.1" class="ltx_text ltx_font_italic">Q-Only</span> is very poor compared to the other VQA approaches. This demonstrates that there is less language bias (i.e., the ability to predict an answer directly from a question) present in our dataset. The other four VQA models perform nearly the same on our <span id="S3.p4.2.2" class="ltx_text ltx_font_italic">Polar-VQA</span> dataset. We can identify that overall accuracy lies somewhere between <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="0.96-0.97" display="inline"><semantics id="S3.p4.1.m1.1a"><mrow id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mn id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">0.96</mn><mo id="S3.p4.1.m1.1.1.1" xref="S3.p4.1.m1.1.1.1.cmml">−</mo><mn id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3.cmml">0.97</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><minus id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1"></minus><cn type="float" id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2">0.96</cn><cn type="float" id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3">0.97</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">0.96-0.97</annotation></semantics></math> which is nearly perfect on test data. The accuracy for each category is also close to <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.p4.2.m2.1a"><mn id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><cn type="integer" id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">1</annotation></semantics></math>. These promising results highlight that the VQA framework is highly capable of identifying the types of radars, recognizing locations, and differentiating between dry and wet zone images very efficiently.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3 Baseline Studies ‣ Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> highlights the performance of two types of questions for each question category. We can see that for open-ended (OE) questions, the accuracy is 1 in the case of identifying the dry and wet zone images, regardless of the VQA models. However, the accuracy of the place category is lower than that of the other two categories. On the other hand, for close-ended (CE) questions, performance is better for the zone category among all the question categories. From both Table <a href="#S3.T1" title="Table 1 ‣ 3 Baseline Studies ‣ Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.T2" title="Table 2 ‣ 3 Baseline Studies ‣ Polar-VQA: Visual Question Answering on Remote Sensed Ice sheet Imagery from Polar Region" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we understand that performance among the VQA approaches given a question category is less deviated than the performance between question categories.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The visual question-answering task on ice sheet imagery collected using various airborne sensors is introduced in this study. To do so, we have presented a unique VQA dataset, namely <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">Polar-VQA</span>. From the comparative study among several VQA approaches, we can see that VQA has the potential to be incorporated into ice sheet research, where an expert can obtain vital information by asking questions. Developing a large-scale dataset that includes more types of questions targeted at extracting several types of information besides radars, locations, and zones is our further research direction.</p>
</div>
<section id="S4.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S4.SS0.SSSx1.p1" class="ltx_para">
<p id="S4.SS0.SSSx1.p1.1" class="ltx_p">This work was supported by NSF BIGDATA Awards (IIS-1838230, IIS-1947584), NSF HDR
Institute Award (OAC-2118285), IBM, and Amazon. We also acknowledge the support of the U.S. Army Grant No. W911NF2120076.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Debvrat Varshney, Maryam Rahnemoonfar, Masoud Yari, and John Paden,

</span>
<span class="ltx_bibblock">“Deep ice layer tracking and thickness estimation using fully
convolutional networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">2020 IEEE International Conference on Big Data (Big Data)</span>,
2020, pp. 3943–3952.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Debvrat Varshney, Maryam Rahnemoonfar, Masoud Yari, John Paden, Oluwanisola
Ibikunle, and Jilu Li,

</span>
<span class="ltx_bibblock">“Deep learning on airborne radar echograms for tracing snow
accumulation layers of the greenland ice sheet,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Remote Sensing</span>, vol. 13, no. 14, pp. 2707, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Masoud Yari, Oluwanisola Ibikunle, Debvrat Varshney, Tashnim Chowdhury, Argho
Sarkar, John Paden, Jilu Li, and Maryam Rahnemoonfar,

</span>
<span class="ltx_bibblock">“Airborne snow radar data simulation with deep learning and
physics-driven methods,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing</span>, vol. 14, pp. 12035–12047, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh,

</span>
<span class="ltx_bibblock">“Visual question answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola,

</span>
<span class="ltx_bibblock">“Stacked attention networks for image question answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2016, pp. 21–29.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao,

</span>
<span class="ltx_bibblock">“Multi-modal factorized bilinear pooling with co-attention learning
for visual question answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, 2017, pp. 1821–1830.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Asma Ben Abacha, Sadid A Hasan, Vivek V Datla, Joey Liu, Dina Demner-Fushman,
and Henning Müller,

</span>
<span class="ltx_bibblock">“Vqa-med: Overview of the medical visual question answering task at
imageclef 2019,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">CLEF2019 Working Notes</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Argho Sarkar and Maryam Rahnemoonfar,

</span>
<span class="ltx_bibblock">“Vqa-aid: Visual question answering for post-disaster damage
assessment and analysis,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.10548</span>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Argho Sarkar and Maryam Rahnemoonfar,

</span>
<span class="ltx_bibblock">“Grad-cam aware supervised attention for visual question answering
for post-disaster damage assessment,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">2022 IEEE International Conference on Image Processing
(ICIP)</span>, 2022, pp. 3783–3787.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jigar Doshi, Saikat Basu, and Guan Pang,

</span>
<span class="ltx_bibblock">“From satellite imagery to disaster insights,”

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.07033</span>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Sean Andrew Chen, Andrew Escay, Christopher Haberland, Tessa Schneider,
Valentina Staneva, and Youngjun Choe,

</span>
<span class="ltx_bibblock">“Benchmark dataset for automatic damaged building detection from
post-hurricane remotely sensed imagery,”

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.05581</span>, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jiasen Lu, Xiao Lin, Dhruv Batra, and Devi Parikh,

</span>
<span class="ltx_bibblock">“Deeper lstm and normalized cnn visual question answering model,”
<a target="_blank" href="https://github.com/VT-vision-lab/VQA_LSTM_CNN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/VT-vision-lab/VQA_LSTM_CNN</a>, 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel,

</span>
<span class="ltx_bibblock">“Image question answering: A visual semantic embedding model and a
new dataset,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proc. Advances in Neural Inf. Process. Syst</span>, vol. 1, no. 2, pp.
5, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.07402" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.07403" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.07403">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.07403" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.07404" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 20:25:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
