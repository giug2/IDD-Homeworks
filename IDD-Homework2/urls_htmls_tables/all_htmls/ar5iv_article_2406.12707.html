<article class="ltx_document">
 <h1 class="ltx_title ltx_title_document">
  Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Haoqiu Yan
    <sup class="ltx_sup" id="id12.12.id1">
     <span class="ltx_text ltx_font_italic" id="id12.12.id1.1">
      1,3
     </span>
    </sup>
    , Yongxin Zhu
    <span class="ltx_note ltx_role_footnotemark" id="footnotex1">
     <sup class="ltx_note_mark">
      0
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        0
       </sup>
       <span class="ltx_note_type">
        footnotemark:
       </span>
       <span class="ltx_tag ltx_tag_note">
        0
       </span>
      </span>
     </span>
    </span>
    <sup class="ltx_sup" id="id13.13.id2">
     <span class="ltx_text ltx_font_italic" id="id13.13.id2.1">
      2,3
     </span>
    </sup>
    , Kai Zheng
    <sup class="ltx_sup" id="id14.14.id3">
     <span class="ltx_text ltx_font_italic" id="id14.14.id3.1">
      1,3
     </span>
    </sup>
    ,
    <span class="ltx_ERROR undefined" id="id15.15.id4">
     \AND
    </span>
    Bing Liu
    <sup class="ltx_sup" id="id16.16.id5">
     <span class="ltx_text ltx_font_italic" id="id16.16.id5.1">
      4
     </span>
    </sup>
    , Haoyu Cao
    <sup class="ltx_sup" id="id17.17.id6">
     <span class="ltx_text ltx_font_italic" id="id17.17.id6.1">
      4
     </span>
    </sup>
    , Deqiang Jiang
    <sup class="ltx_sup" id="id18.18.id7">
     <span class="ltx_text ltx_font_italic" id="id18.18.id7.1">
      4
     </span>
    </sup>
    , Linli Xu
    <sup class="ltx_sup" id="id19.19.id8">
     <span class="ltx_text ltx_font_italic" id="id19.19.id8.1">
      1,3
     </span>
    </sup>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id20.20.id9">
     <span class="ltx_text ltx_font_italic" id="id20.20.id9.1">
      1
     </span>
    </sup>
    School of Computer Science and Technology, University of Science and Technology of China
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id21.21.id10">
     <span class="ltx_text ltx_font_italic" id="id21.21.id10.1">
      2
     </span>
    </sup>
    School of Data Science, University of Science and Technology of China
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id22.22.id11">
     <span class="ltx_text ltx_font_italic" id="id22.22.id11.1">
      3
     </span>
    </sup>
    State Key Laboratory of Cognitive Intelligence,
    <sup class="ltx_sup" id="id23.23.id12">
     <span class="ltx_text ltx_font_italic" id="id23.23.id12.1">
      4
     </span>
    </sup>
    Tencent Youtu Lab
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id24.24.id13">
     {yanhq,zyx2016,dthdzk}@mail.ustc.edu.cn
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id25.25.id14">
     {billbliu,rechycao,dqiangjiang}@tencent.com
    </span>
    <span class="ltx_text ltx_font_typewriter" id="id26.26.id15">
     linlixu@ustc.edu.cn
    </span>
   </span>
   <span class="ltx_author_notes">
    Equal contribution.Corresponding author.
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id27.id1">
   Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers’ intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers’ true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker’s true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at:
   <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Haoqiu-Yan/PerceptiveAgent" target="_blank" title="">
    https://github.com/Haoqiu-Yan/PerceptiveAgent
   </a>
   .
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <div class="ltx_block ltx_align_bottom" id="p1.11">
   <p class="ltx_p" id="p1.11.12">
    <span class="ltx_text ltx_font_bold" id="p1.11.12.1">
     Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
   <p class="ltx_p ltx_align_center" id="p1.3.3" style="width:433.6pt;">
    <span class="ltx_text ltx_inline-block" id="p1.3.3.3" style="width:0.0pt;">
     <span class="ltx_tabular ltx_align_top" id="p1.3.3.3.3">
      <span class="ltx_tbody">
       <span class="ltx_tr" id="p1.3.3.3.3.3">
        <span class="ltx_td ltx_align_center" id="p1.3.3.3.3.3.3">
         <span class="ltx_text ltx_font_bold" id="p1.3.3.3.3.3.3.3">
          Haoqiu Yan
          <span class="ltx_note ltx_role_thanks" id="p1.3.3.3.3.3.3.3.1">
           <sup class="ltx_note_mark">
            †
           </sup>
           <span class="ltx_note_outer">
            <span class="ltx_note_content">
             <sup class="ltx_note_mark">
              †
             </sup>
             <span class="ltx_note_type">
              thanks:
             </span>
             Equal contribution.
            </span>
           </span>
          </span>
          <sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.2">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.3.3.3.3.3.3.3.2.1">
            1,3
           </span>
          </sup>
          , Yongxin Zhu
          <span class="ltx_note ltx_role_footnotemark" id="footnotex2">
           <sup class="ltx_note_mark">
            0
           </sup>
           <span class="ltx_note_outer">
            <span class="ltx_note_content">
             <sup class="ltx_note_mark">
              0
             </sup>
             <span class="ltx_note_type">
              footnotemark:
             </span>
             <span class="ltx_tag ltx_tag_note">
              <span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">
               0
              </span>
             </span>
            </span>
           </span>
          </span>
          <sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.3">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.3.3.3.3.3.3.3.3.1">
            2,3
           </span>
          </sup>
          , Kai Zheng
          <sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.4">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.3.3.3.3.3.3.3.4.1">
            1,3
           </span>
          </sup>
          ,
         </span>
        </span>
       </span>
      </span>
     </span>
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
   <p class="ltx_p ltx_align_center" id="p1.11.11" style="width:433.6pt;">
    <span class="ltx_text ltx_inline-block" id="p1.11.11.8" style="width:0.0pt;">
     <span class="ltx_tabular ltx_align_top" id="p1.11.11.8.8">
      <span class="ltx_tbody">
       <span class="ltx_tr" id="p1.7.7.4.4.4">
        <span class="ltx_td ltx_align_center" id="p1.7.7.4.4.4.4">
         <span class="ltx_text ltx_font_bold" id="p1.7.7.4.4.4.4.4">
          Bing Liu
          <sup class="ltx_sup" id="p1.7.7.4.4.4.4.4.1">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.4.4.4.4.4.1.1">
            4
           </span>
          </sup>
          , Haoyu Cao
          <sup class="ltx_sup" id="p1.7.7.4.4.4.4.4.2">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.4.4.4.4.4.2.1">
            4
           </span>
          </sup>
          , Deqiang Jiang
          <sup class="ltx_sup" id="p1.7.7.4.4.4.4.4.3">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.4.4.4.4.4.3.1">
            4
           </span>
          </sup>
          , Linli Xu
          <span class="ltx_note ltx_role_thanks" id="p1.7.7.4.4.4.4.4.4">
           <sup class="ltx_note_mark">
            †
           </sup>
           <span class="ltx_note_outer">
            <span class="ltx_note_content">
             <sup class="ltx_note_mark">
              †
             </sup>
             <span class="ltx_note_type">
              thanks:
             </span>
             Corresponding author.
            </span>
           </span>
          </span>
          <sup class="ltx_sup" id="p1.7.7.4.4.4.4.4.5">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.7.4.4.4.4.4.5.1">
            1,3
           </span>
          </sup>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.8.8.5.5.5">
        <span class="ltx_td ltx_align_center" id="p1.8.8.5.5.5.1">
         <sup class="ltx_sup" id="p1.8.8.5.5.5.1.1">
          <span class="ltx_text ltx_font_italic" id="p1.8.8.5.5.5.1.1.1">
           1
          </span>
         </sup>
         School of Computer Science and Technology, University of Science and Technology of China
        </span>
       </span>
       <span class="ltx_tr" id="p1.9.9.6.6.6">
        <span class="ltx_td ltx_align_center" id="p1.9.9.6.6.6.1">
         <sup class="ltx_sup" id="p1.9.9.6.6.6.1.1">
          <span class="ltx_text ltx_font_italic" id="p1.9.9.6.6.6.1.1.1">
           2
          </span>
         </sup>
         School of Data Science, University of Science and Technology of China
        </span>
       </span>
       <span class="ltx_tr" id="p1.11.11.8.8.8">
        <span class="ltx_td ltx_align_center" id="p1.11.11.8.8.8.2">
         <sup class="ltx_sup" id="p1.11.11.8.8.8.2.1">
          <span class="ltx_text ltx_font_italic" id="p1.11.11.8.8.8.2.1.1">
           3
          </span>
         </sup>
         State Key Laboratory of Cognitive Intelligence,
         <sup class="ltx_sup" id="p1.11.11.8.8.8.2.2">
          <span class="ltx_text ltx_font_italic" id="p1.11.11.8.8.8.2.2.1">
           4
          </span>
         </sup>
         Tencent Youtu Lab
        </span>
       </span>
       <span class="ltx_tr" id="p1.11.11.8.8.9.1">
        <span class="ltx_td ltx_align_center" id="p1.11.11.8.8.9.1.1">
         <span class="ltx_text ltx_font_typewriter" id="p1.11.11.8.8.9.1.1.1">
          {yanhq,zyx2016,dthdzk}@mail.ustc.edu.cn
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.11.11.8.8.10.2">
        <span class="ltx_td ltx_align_center" id="p1.11.11.8.8.10.2.1">
         <span class="ltx_text ltx_font_typewriter" id="p1.11.11.8.8.10.2.1.1">
          {billbliu,rechycao,dqiangjiang}@tencent.com
         </span>
         <span class="ltx_text ltx_font_typewriter" id="p1.11.11.8.8.10.2.1.2">
          linlixu@ustc.edu.cn
         </span>
        </span>
       </span>
      </span>
     </span>
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
  </div>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Artificial Intelligence (AI) agents
    <cite class="ltx_cite ltx_citemacro_citep">
     (Russell and Norvig,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2010
     </a>
     ; Negnevitsky,
     <a class="ltx_ref" href="#bib.bib20" title="">
      2005
     </a>
     )
    </cite>
    are entities designed to replicate human-like intelligence and functionalities, serving as the essential building blocks of AI systems. An ideal agent should be capable of perceiving its environment with sensors, making informed decisions, and then taking actions in response to users or scenarios. Recently, Large Language Models (LLMs)
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wei et al.,
     <a class="ltx_ref" href="#bib.bib41" title="">
      2022
     </a>
     ; Shanahan,
     <a class="ltx_ref" href="#bib.bib33" title="">
      2024
     </a>
     ; Taylor et al.,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2022
     </a>
     )
    </cite>
    have exhibited remarkable capabilities in diverse tasks, offering opportunities for building general AI agents that engage in human-like interactions, such as virtual assistants and intelligent robots. However, current text-only dialogue systems
    <cite class="ltx_cite ltx_citemacro_citep">
     (Peng et al.,
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023
     </a>
     ; Touvron et al.,
     <a class="ltx_ref" href="#bib.bib38" title="">
      2023
     </a>
     )
    </cite>
    fall short in bridging the gap between experimental and realistic scenarios, where humans perceive and understand the world through diverse multi-modal information. Thus, the integration of acoustic information into dialogues has the potential to foster the development of more human-like agents, thereby enhancing the empathetic experience they offer.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="S1.F1.g1" src="/html/2406.12707/assets/x1.png" width="507"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Examples illustrating the definition of empathy within dialogues.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Empathetic responses involve two essential aspects: cognitive and affective empathy
    <cite class="ltx_cite ltx_citemacro_citep">
     (Cuff et al.,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2016
     </a>
     ; Kim et al.,
     <a class="ltx_ref" href="#bib.bib12" title="">
      2021
     </a>
     ; Reis et al.,
     <a class="ltx_ref" href="#bib.bib28" title="">
      2011
     </a>
     ; Smith,
     <a class="ltx_ref" href="#bib.bib36" title="">
      2006
     </a>
     )
    </cite>
    , which reflect an understanding of the human-talker’s thoughts and feelings respectively. Specifically, cognitive empathy involves understanding the human-talker’s thoughts, perspectives, and described events, enabling the agent to provide responses relevant to the dialogue topic
    <cite class="ltx_cite ltx_citemacro_citep">
     (Sabour et al.,
     <a class="ltx_ref" href="#bib.bib30" title="">
      2022
     </a>
     )
    </cite>
    . Conversely, affective empathy entails responding based on observed emotional expressions in the dialogue history, contributing to the naturalness of synthesized speech
    <cite class="ltx_cite ltx_citemacro_citep">
     (Cong et al.,
     <a class="ltx_ref" href="#bib.bib2" title="">
      2021
     </a>
     ; Guo et al.,
     <a class="ltx_ref" href="#bib.bib5" title="">
      2021
     </a>
     ; Nishimura et al.,
     <a class="ltx_ref" href="#bib.bib23" title="">
      2022
     </a>
     )
    </cite>
    . While recent works
    <cite class="ltx_cite ltx_citemacro_citep">
     (Saito et al.,
     <a class="ltx_ref" href="#bib.bib31" title="">
      2023
     </a>
     ; Nguyen et al.,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2022
     </a>
     ; Mitsui et al.,
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     )
    </cite>
    leverage LLM’s strong capabilities of contextual understanding and content generation to synthesize empathetic speeches, there remains a discrepancy between cognitive and affective empathy. This arises because cognitive content is preassigned before affective speech is deduced from latent representations of multi-modal dialogue history.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Recently, advancements in multi-modal content perception and generation have been achieved by various methods
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhang et al.,
     <a class="ltx_ref" href="#bib.bib46" title="">
      2023
     </a>
     ; Huang et al.,
     <a class="ltx_ref" href="#bib.bib8" title="">
      2024
     </a>
     ; Chen et al.,
     <a class="ltx_ref" href="#bib.bib1" title="">
      2023
     </a>
     ; Wu et al.,
     <a class="ltx_ref" href="#bib.bib42" title="">
      2023
     </a>
     )
    </cite>
    , where audio is represented as either recognized text with an automatic speech recognition model or discrete features with a speech encoder. However, while linguistic information in speech is predominantly captured by both discrete acoustic units and textual representations, acoustic features tend to be disregarded. This oversight can lead to misinterpretations of the speaker’s intentions, resulting in discrepant or even contradictory responses within the dialogue history. As illustrated in Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    , the left scenario fails to consider the perspective of the listener while the right one barely understands or empathizes with the speaker’s feelings.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    In this paper, we propose
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">
     PerceptiveAgent
    </span>
    , an empathetic multi-modal dialogue system that can discern deeper or more subtle meanings beyond the literal interpretations of words, based on speaking styles described in natural language. Specifically, PerceptiveAgent first comprehends the speaker’s intentions accurately by a perceptive captioner model that captures acoustic features from each speech within dialogues. Subsequently, an LLM module acts as the cognitive core, producing the relevant response content with a caption describing how to articulate the response. A Multi-Speaker and Multi-Attribute Synthesizer (MSMA-Synthesizer) is then developed to synthesize nuanced and expressive speech.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    Our contributions include the following:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.ix1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      <math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.ix1.1.1.m1.1">
       <semantics id="S1.I1.ix1.1.1.m1.1b">
        <mo id="S1.I1.ix1.1.1.m1.1.1" xref="S1.I1.ix1.1.1.m1.1.1.cmml">
         ∙
        </mo>
        <annotation-xml encoding="MathML-Content" id="S1.I1.ix1.1.1.m1.1c">
         <ci id="S1.I1.ix1.1.1.m1.1.1.cmml" xref="S1.I1.ix1.1.1.m1.1.1">
          ∙
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S1.I1.ix1.1.1.m1.1d">
         \bullet
        </annotation>
       </semantics>
      </math>
     </span>
     <div class="ltx_para" id="S1.I1.ix1.p1">
      <p class="ltx_p" id="S1.I1.ix1.p1.1">
       We pioneer the construction of a speech captioner model to perceive and express acoustic information through natural language.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.ix2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      <math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.ix2.1.1.m1.1">
       <semantics id="S1.I1.ix2.1.1.m1.1b">
        <mo id="S1.I1.ix2.1.1.m1.1.1" xref="S1.I1.ix2.1.1.m1.1.1.cmml">
         ∙
        </mo>
        <annotation-xml encoding="MathML-Content" id="S1.I1.ix2.1.1.m1.1c">
         <ci id="S1.I1.ix2.1.1.m1.1.1.cmml" xref="S1.I1.ix2.1.1.m1.1.1">
          ∙
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S1.I1.ix2.1.1.m1.1d">
         \bullet
        </annotation>
       </semantics>
      </math>
     </span>
     <div class="ltx_para" id="S1.I1.ix2.p1">
      <p class="ltx_p" id="S1.I1.ix2.p1.1">
       We develop an empathetic multi-modal dialogue system capable of identifying the speaker’s true intentions through audio modality perception and generating empathetic speech.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.ix3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      <math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.ix3.1.1.m1.1">
       <semantics id="S1.I1.ix3.1.1.m1.1b">
        <mo id="S1.I1.ix3.1.1.m1.1.1" xref="S1.I1.ix3.1.1.m1.1.1.cmml">
         ∙
        </mo>
        <annotation-xml encoding="MathML-Content" id="S1.I1.ix3.1.1.m1.1c">
         <ci id="S1.I1.ix3.1.1.m1.1.1.cmml" xref="S1.I1.ix3.1.1.m1.1.1">
          ∙
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S1.I1.ix3.1.1.m1.1d">
         \bullet
        </annotation>
       </semantics>
      </math>
     </span>
     <div class="ltx_para" id="S1.I1.ix3.p1">
      <p class="ltx_p" id="S1.I1.ix3.p1.1">
       Experiments demonstrate that PerceptiveAgent can accurately discern the true intentions in scenarios where the literal interpretations of words are either contrary to or inconsistent with the speaker’s true feelings.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Multi-modal Dialogue Systems
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     Recent advances in multi-modal dialogue systems have primarily focused on transforming speech into discrete latent representation. For instance,
     <cite class="ltx_cite ltx_citemacro_citet">
      Zhang et al. (
      <a class="ltx_ref" href="#bib.bib46" title="">
       2023
      </a>
      ); Chen et al. (
      <a class="ltx_ref" href="#bib.bib1" title="">
       2023
      </a>
      ); Wu et al. (
      <a class="ltx_ref" href="#bib.bib42" title="">
       2023
      </a>
      )
     </cite>
     utilize speech encoders to perceive speech and then synthesize responses according to discrete acoustic units derived from LLMs, showing intrinsic cross-modal conversational abilities. Besides, works including
     <cite class="ltx_cite ltx_citemacro_citep">
      (Nguyen et al.,
      <a class="ltx_ref" href="#bib.bib22" title="">
       2022
      </a>
      ; Mitsui et al.,
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
      )
     </cite>
     autonomously generate two-channel spoken dialogues, simulating realistic interactions between agents, including vocal interactions, laughter, and turn-taking. However, while discrete acoustic units capture linguistic information effectively, prosodic features are mostly ignored. To address this limitation and preserve prosodic information as much as possible, we develop a multi-modal dialog system that perceives prosody through speech captioning and responds empathetically using an LLM and a speech synthesizer.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Cross-Modal Text Generation
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     Cross-modal text generation involves generating text conditioned on other modalities such as audio and vision
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al.,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2022
      </a>
      ; Liu et al.,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2024
      </a>
      ; Zhang et al.,
      <a class="ltx_ref" href="#bib.bib47" title="">
       2024
      </a>
      )
     </cite>
     , where the key challenge is to align multi-modal features with the text latent space. Recent approaches
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhu et al.,
      <a class="ltx_ref" href="#bib.bib50" title="">
       2023
      </a>
      ; Chen et al.,
      <a class="ltx_ref" href="#bib.bib1" title="">
       2023
      </a>
      )
     </cite>
     address this challenge by aligning off-the-shelf pre-trained LLMs with learnable visual encoders
     <cite class="ltx_cite ltx_citemacro_citep">
      (Li et al.,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2023
      </a>
      ; Zhao et al.,
      <a class="ltx_ref" href="#bib.bib49" title="">
       2023
      </a>
      )
     </cite>
     , transforming multi-modal representations as learnable query embeddings while keeping both pre-trained LLMs and visual encoders frozen. Similarly, for audio caption tasks, audio embeddings are mapped to a sequence of prefix vectors and then taken as the context input for caption generation
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kim et al.,
      <a class="ltx_ref" href="#bib.bib13" title="">
       2023
      </a>
      ; Schaumlöffel et al.,
      <a class="ltx_ref" href="#bib.bib32" title="">
       2023
      </a>
      ; Xu et al.,
      <a class="ltx_ref" href="#bib.bib44" title="">
       2024
      </a>
      )
     </cite>
     . However, to the best of our knowledge, we are the first to construct a speech captioner capable of perceiving acoustic information in dialogues.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Expressive Text-to-Speech Synthesis
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     Given a transcript, text-to-speech (TTS) models achieve voice variability by conditioning on a zero-shot speech prompt or a text prompt of the desired style. For instance, zero-shot TTS systems reproduce the speaker characteristics and acoustic environments of a speech prompt through in-context learning
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wu et al.,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2022
      </a>
      ; Wang et al.,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2023
      </a>
      ; Shen et al.,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2023
      </a>
      ; Le et al.,
      <a class="ltx_ref" href="#bib.bib14" title="">
       2023
      </a>
      )
     </cite>
     . However, these systems lack independent control over speaking styles, including prosody, emotion, and acoustic environment. To address this, text prompts have been introduced for more natural and general speech synthesis. Approaches like
     <cite class="ltx_cite ltx_citemacro_citep">
      (Guo et al.,
      <a class="ltx_ref" href="#bib.bib6" title="">
       2023
      </a>
      ; Leng et al.,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2023
      </a>
      ; Shimizu et al.,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2023
      </a>
      ; Ji et al.,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2023
      </a>
      )
     </cite>
     express speaking styles in natural language, while methods such as
     <cite class="ltx_cite ltx_citemacro_citep">
      (Polyak et al.,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2021
      </a>
      ; Nguyen et al.,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023
      </a>
      )
     </cite>
     utilize explicit labels to generate diverse speech that matches the prompt. We follow the latter direction and construct a speech synthesis model with multiple speaking style labels.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Methods
  </h2>
  <figure class="ltx_figure" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="215" id="S3.F2.g1" src="/html/2406.12707/assets/x2.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    The overall architecture of
PerceptiveAgent. Three components are interconnected: the speech captioner, the LLM and the MSMA-Synthesizer. The speech captioner serves as a multi-modal sensory system, perceiving acoustic information from the dialogue history, which is crucial for discerning the speakers’ intentions. The LLM acts as the cognitive core, responsible for comprehending the speakers’ thoughts and emotions. Conditioned on the response contents and multiple attributes provided by the LLM, the MSMA-Synthesizer generates expressive speech outputs.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    As a multimodal dialog system, PerceptiveAgent is capable of audio modality perception and empathetic speech generation, which is achieved through the incorporation of prosodic information expressed in natural language. To capture prosodic features from speech inputs, we propose a novel speech caption model that aligns audio features with the latent space of a pre-trained language model. To enhance empathy and diversity of the simulated speech communication, a multi-speaker and multi-attribute vocoder is developed. This vocoder synthesizes speech by conditioning on both response contents and captions of speaking styles, resulting in more engaging and realistic dialogues.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Speech Captioner
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     The speech caption model is designed to capture prosodic information and transcribe it as textual descriptions. It operates by encoding speech inputs by the speech encoder in ImageBind
     <cite class="ltx_cite ltx_citemacro_citep">
      (Girdhar et al.,
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     , followed by description generation by the pre-trained GPT-2 decoder
     <cite class="ltx_cite ltx_citemacro_citep">
      (Radford et al.,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2019
      </a>
      )
     </cite>
     . To bridge the gap between the speech encoder and the text decoder, we introduce a Querying Transformer (Q-former) pre-trained in BuboGPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhao et al.,
      <a class="ltx_ref" href="#bib.bib49" title="">
       2023
      </a>
      )
     </cite>
     . This model is connected with a linear projection layer, which is subsequently followed by a text decoder. To effectively fine-tune this model, we integrate the following two fine-tuning strategies, while keeping the speech encoder frozen throughout the training procedure.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S3.SS1.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.1
     </span>
     Multi-modal Embedding Alignment
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS1.p1">
     <p class="ltx_p" id="S3.SS1.SSS1.p1.1">
      Prefix tuning is utilized to align the output of the Q-former with the latent space of the text decoder. A query vector with fixed dimensions is generated by the Q-former. These embeddings interact with each other through self-attention layers and with frozen audio features through cross-attention layers. To bridge the gap with the word embedding space, query embeddings are used as prefix vectors and attended to by the text decoder. This bottleneck architecture serves to compel the queries to extract the acoustic information that is most relevant to the textual descriptions.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS1.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.1.2
     </span>
     Instruction Tuning
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS2.p1">
     <p class="ltx_p" id="S3.SS1.SSS2.p1.1">
      To bridge the gap between the next-word prediction objective of the pre-trained decoder and the objective of acquiring multi-modal information conditioned on prefix sequences, instruction tuning is employed to train the speech captioner. We first construct an instructional dataset, where each instance comprises three elements: a query vector, an instruction, and a caption. The instruction is described as a natural language text sequence that specifies the task, serving to constrain the model’s outputs to align with desired response characteristics or domain knowledge. This provides a channel for humans to intervene with the model’s behaviors. Varied instructions are gathered using GPT-3.5-Turbo in this work. Additionally, the caption represents the desired output following the instruction, while the query vector is derived from acoustic representations. Throughout the training procedure, the parameters of the speech encoder are fixed, while the Q-former and text decoder remain trainable. During each inference process, instructions are randomly selected and incorporated into the generated sequence to enhance diversity and simulate human cognitive processes more effectively, thereby yielding more varied outputs.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    PerceptiveAgent
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     illustrates the overall framework of PerceptiveAgent, a multi-modal dialogue system comprising three interconnected stages: Intention Discerning by the speech captioner, Comprehension through Sensory Integration by the LLM and Expressive Speech Synthesis by the MSMA-Synthesizer. PerceptiveAgent exhibits two key characteristics: (1) It leverages natural language to perceive and express acoustic information, and (2) It employs an LLM as the cognitive core in the system, to comprehend multi-modal contextual history and deliver audio responses.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S3.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.1
     </span>
     Caption for Intention Discerning
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS1.p1">
     <p class="ltx_p" id="S3.SS2.SSS1.p1.1">
      In the initial stage, a speech caption model is employed to interpret acoustic information from audio inputs. Each speech within the dialogue history is encoded into latent features by a frozen speech encoder. These features are then compressed into a query vector with fixed dimensions, sharing the same latent space as the word embedding of a text decoder. Conditioned on this query sequence and instruction prompt, a textual caption describing the speaking styles for each speech is deduced by the text decoder.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.2
     </span>
     Comprehension through Sensory Integration
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS2.p1">
     <p class="ltx_p" id="S3.SS2.SSS2.p1.1">
      Subsequently, an LLM module acting as the cognitive core is integrated into the system, where GPT-3.5-Turbo is employed. The transcribed textual content for each audio is merged with the previously generated caption before being fed into the LLM. Prompts in Appendix
      <a class="ltx_ref" href="#A1" title="Appendix A Prompt for Dialogue Generation with Captions ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
       <span class="ltx_text ltx_ref_tag">
        A
       </span>
      </a>
      and
      <a class="ltx_ref" href="#A2" title="Appendix B Prompt for Dialogue Generation without Captions ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
       <span class="ltx_text ltx_ref_tag">
        B
       </span>
      </a>
      are designed to effectively leverage the LLM’s contextual understanding abilities. Upon recognizing speakers’ intentions by assimilating both the contextual caption and content, the LLM deduces the relevant dialogue content and generates a caption describing how to articulate the derived content.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.3
     </span>
     Expressive Speech Synthesis
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS3.p1">
     <p class="ltx_p" id="S3.SS2.SSS3.p1.1">
      Finally, empathetic audio responses are
synthesized by the MSMA-Synthesizer, a Multi-Speaker and Multi-Attribute vocoder that is conditioned on the generated dialogue contents and captions. This vocoder is a modification of
      <cite class="ltx_cite ltx_citemacro_citep">
       (Nguyen et al.,
       <a class="ltx_ref" href="#bib.bib21" title="">
        2023
       </a>
       )
      </cite>
      to facilitate fine control over speech expressiveness. In addition to taking discrete speech units, speaker and style (emotion) as inputs, our vocoder introduces multiple prosodic attributes, including pitch, speed and energy. To synthesize each inference, the LLM’s outputs of dialogue contents and captions are transformed into discrete units or attribute labels respectively, before being fed into the vocoder. Specifically, a text-to-unit (T2U) model is utilized to convert response contents into acoustic units with a Transformer machine translation structure
      <cite class="ltx_cite ltx_citemacro_citep">
       (Vaswani et al.,
       <a class="ltx_ref" href="#bib.bib39" title="">
        2017
       </a>
       )
      </cite>
      . Emotional and prosodic labels are recognized from response captions by sentence classifiers, accomplished with GPT-3.5-Turbo in this work, while the speaker label is randomly selected.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS3.p2">
     <p class="ltx_p" id="S3.SS2.SSS3.p2.1">
      The architecture of the vocoder comprises a speaker embedder, an attribute embedder and a HIFIGAN vocoder. The speaker embedder uses look-up tables to embed speaker identities, while a set of controllable attributes including speed, emotion, energy and pitch are embedded by the attribute embedder. To synthesize expressive speech, discrete units are initially embedded and up-sampled through a series of blocks consisting of transposed convolution and a residual block with dilated layers. Prior to duration prediction, this up-sampled sequence is concatenated with the speed embedding. The speaker embedding and style embedding are subsequently concatenated to each frame in the up-sampled sequence, which is transformed to a mel-spectrogram by the HiFiGAN generator.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Experimental Setup
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">
      Datasets.
     </span>
     We train our speech captioner on the TextrolSpeech
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ji et al.,
      <a class="ltx_ref" href="#bib.bib10" title="">
       2023
      </a>
      )
     </cite>
     dataset, which consists of 236,220 pairs of captions and the corresponding speech samples. The captions in this dataset describe speaking styles in terms of five factors: gender, emotion, pitch, speed and energy.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     For the MSMA-Synthesizer, we reproduce a vocoder proposed in
     <cite class="ltx_cite ltx_citemacro_citep">
      (Nguyen et al.,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023
      </a>
      )
     </cite>
     using the EXPRESSO, LJSpeech
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ito and Johnson,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2017
      </a>
      )
     </cite>
     and VCTK
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yamagishi et al.,
      <a class="ltx_ref" href="#bib.bib45" title="">
       2019
      </a>
      )
     </cite>
     datasets. The EXPRESSO dataset is subsequently labeled by the speech captioner and GPT-3.5-Turbo to recognize attributes of pitch, speed and energy for each speech. We then utilize this labeled EXPRESSO dataset and the reproduced vocoder to train the MSMA-Synthesizer. We refer to the reading and conversation sections of EXPRESSO as Exp-R and Exp-I respectively. Additionally, a T2U model is trained on the same datasets with the MSMA-Synthesizer to maintain consistency in unit distribution.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     To evaluate the overall performance of our system, we utilize a speech dialogue dataset from MELD
     <cite class="ltx_cite ltx_citemacro_citep">
      (Poria et al.,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2019
      </a>
      )
     </cite>
     . This dataset provides emotion labels for each sentence, which serve as ground truth labels for both response content and speech evaluation. The speeches in this dataset are recorded in realistic scenes with interruptions and environmental noise. In our evaluation, we only consider conversations with two speakers.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p4">
    <p class="ltx_p" id="S4.SS1.p4.1">
     We utilize English datasets throughout the entire training process. As a consequence, PerceptiveAgent currently supports only the English language. However, it is noteworthy that PerceptiveAgent can be readily expanded to accommodate multiple languages. Only the MSMA-Synthesizer module requires modification, as the language-agnostic nature of the speech captioner allows it to generate captions from various languages. Meanwhile, existing methods can recognize semantic contents and translate them into English.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p5">
    <p class="ltx_p" id="S4.SS1.p5.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">
      Configurations.
     </span>
     We utilize the speech encoder in ImageBind
     <cite class="ltx_cite ltx_citemacro_citep">
      (Girdhar et al.,
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     , the pre-trained Q-former in BuboGPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhao et al.,
      <a class="ltx_ref" href="#bib.bib49" title="">
       2023
      </a>
      )
     </cite>
     , and the pre-trained GPT-2
     <cite class="ltx_cite ltx_citemacro_citep">
      (Radford et al.,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2019
      </a>
      )
     </cite>
     to implement the speech captioner. Finetuning is conducted for 43,000 steps with a batch size of 16. For decoding, we use Top-k sampling with k=10 and set the minimum and maximum sequence lengths to
20 and 50, respectively. We reproduce the vocoder for 400,000 steps with a batch size of 32 and learning rate of 0.0004, and train the MSMA-Synthesizer for 200,000 steps with a batch size of 32 and learning rate of 0.0004. The T2U model is structured as a sequence-to-sequence transformer with 4 encoder layers, 4 decoder layers, and 4 attention heads, with a dropout of 0.1.
We utilize HuBERT
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hsu et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2021
      </a>
      )
     </cite>
     with 2000 clusters to acquire units as targets
     <span class="ltx_note ltx_role_footnote" id="footnote1">
      <sup class="ltx_note_mark">
       1
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         1
        </sup>
        <span class="ltx_tag ltx_tag_note">
         1
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="%7Bhttps://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt%7D" title="">
         {https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt}
        </a>
       </span>
      </span>
     </span>
     , provided by the textlesslib toolbox
     <cite class="ltx_cite ltx_citemacro_citep">
      (Kharitonov et al.,
      <a class="ltx_ref" href="#bib.bib11" title="">
       2022
      </a>
      )
     </cite>
     . Decoding is performed using Top-k sampling with k=10. All experiments are conducted on 4 NVIDIA GeForce RTX 4090 GPUs.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Evaluation
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">
      Speech-GPT3.5.
     </span>
     We implement Speech-GPT3.5, a dialogue system focusing solely on linguistic information as a baseline.
According to the textual history content recognized from the speech input, this system comprehends dialogue context with GPT-3.5-Turbo. After generating the response content, the audio response is synthesized by an off-the-shelf TTS (text-to-speech) model provided by OpenAI
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="%7Bhttps://platform.openai.com/docs/guides/text-to-speech%7D" title="">
         {https://platform.openai.com/docs/guides/text-to-speech}
        </a>
       </span>
      </span>
     </span>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">
      Metrics.
     </span>
     The performance of PerceptiveAgent is evaluated in terms of two fundamental aspects: 1)
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">
      cognitive empathy
     </span>
     demonstrates the ability to consider the perspective of speakers, reflected in the content of the response; and 2)
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.3">
      affective empathy
     </span>
     exhibits the ability to emotionally understand and share the speaker’s feelings, reflected in the prosody of the generated audio response. Cognitive and affective empathy are assessed by evaluating the quality of generated textual responses and audio responses, respectively.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     To evaluate the quality of dialogue text generation, we employ the BERTScore automatic evaluation metric proposed by
     <cite class="ltx_cite ltx_citemacro_citet">
      Zhang et al. (
      <a class="ltx_ref" href="#bib.bib48" title="">
       2020
      </a>
      )
     </cite>
     , which computes a similarity score for each token in the candidate sentence with each token in the reference sentence. To evaluate the expressiveness of audio generation, we employ an expressive style classifier proposed by
     <cite class="ltx_cite ltx_citemacro_citet">
      Nguyen et al. (
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023
      </a>
      )
     </cite>
     to recognize emotion labels for both generated and true speeches. Classification accuracy is used to measure the performance.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     Besides, we evaluate the perception ability of the speech captioner on the validation and test datasets, which are split from the TextrolSpeech dataset. We approach this model as a multi-attribute classification task. Upon generating captions from speeches, the predicted labels for attributes including gender, emotion, pitch, speed and energy are determined by a sentence classifier, GPT-3.5-Turbo, while the true labels are provided in the TextrolSpeech dataset. Weighted metrics including precision, recall and F1-score are used to quantify the disparity between the predicted and true labels.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p5">
    <p class="ltx_p" id="S4.SS2.p5.1">
     Moreover, the expressiveness of the speech synthesizer is assessed on the validation and test datasets split from the EXPRESSO dataset. We use the same expressive style classifier employed in affective empathy evaluation, to measure the preservation of emotion in the resynthesized speech. For evaluating the preservation of prosody, we compute the F0 Frame Error (FFE), which measures the percentage of frames with a deviation of more than 20
     <math alttext="\%" class="ltx_Math" display="inline" id="S4.SS2.p5.1.m1.1">
      <semantics id="S4.SS2.p5.1.m1.1a">
       <mo id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">
        %
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b">
        <csymbol cd="latexml" id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">
         percent
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">
        \%
       </annotation>
      </semantics>
     </math>
     in pitch value between the input and resynthesized output.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Result Analysis
   </h3>
   <section class="ltx_subsubsection" id="S4.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.1
     </span>
     PerceptiveAgent
    </h4>
    <figure class="ltx_table" id="S4.T1">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S4.T1.1.1.1">
        <th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1">
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">
         BERTScore
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">
         Accuracy
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S4.T1.1.2.1">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">
         Speech-GPT3.5
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">
         53.03±10.20
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">
         0.74
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.3.2">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.1.3.2.1">
         PerceptiveAgent
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.2.1">
          54.36±9.25
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">
         <span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.3.1">
          21.89
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T1.1.4.3">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.4.3.1">
         -w/o captions
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.2">
         -
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.3">
         16.53
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 1:
      </span>
      Performance evaluation of PerceptiveAgent. BERTScore (%) measures the quality of cognitive empathy in linguistic contents, while accuracy (%) assesses the quality of affective empathy in acoustic responses.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S4.SS3.SSS1.p1">
     <p class="ltx_p" id="S4.SS3.SSS1.p1.1">
      Table
      <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.3.1 PerceptiveAgent ‣ 4.3 Result Analysis ‣ 4 Experiments ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
       <span class="ltx_text ltx_ref_tag">
        1
       </span>
      </a>
      presents the overall performance of PerceptiveAgent on cognitive empathy and affective empathy, evaluated on the generated content and audio, respectively. BERTScore measures the semantic similarity between the generated and real response contents, while accuracy assesses the similarity and diversity of emotions between the generated and real speeches. Overall, compared to Speech-GPT3.5, PerceptiveAgent demonstrates a strong ability in generating empathetic responses with a closer alignment to the dialogue context in terms of linguistic content and a higher expressiveness in acoustic information. Specifically, PerceptiveAgent achieves a slightly higher BERTScore than Speech-GPT3.5, primarily because our model can generate content that more accurately captures the speaker’s intentions and contains more emotionally intense words. Additionally, PerceptiveAgent notably outperforms Speech-GPT3.5 in terms of accuracy, as the latter doesn’t incorporate any emotion prompts during speech generation, thus maintaining a limited variety of prosody. Despite this, the accuracy of PerceptiveAgent still remains at a relatively moderate level. This is because the generated responses, while contextually appropriate, may not entirely align with the real responses in terms of semantics and emotions.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S4.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.2
     </span>
     Speech Captioner
    </h4>
    <figure class="ltx_table" id="S4.T2">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S4.T2.1.1.1">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1" rowspan="2">
         <span class="ltx_text" id="S4.T2.1.1.1.1.1">
          Attribute
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T2.1.1.1.2">
         Validation
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T2.1.1.1.3">
         Test
        </th>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.2.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.1">
         Precision
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.2">
         Recall
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.2.2.3">
         F1-score
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.4">
         Precision
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.5">
         Recall
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.6">
         F1-score
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S4.T2.1.3.1">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.3.1.1">
         Gender
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.2">
         99.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.3">
         97.5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.1.4">
         98.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.5">
         99.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.6">
         98.6
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.7">
         99.0
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.4.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.4.2.1">
         Emotion
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.2">
         85.8
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.3">
         85.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.2.4">
         85.1
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.5">
         87.3
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.6">
         87.1
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.7">
         86.8
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.5.3">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.5.3.1">
         Pitch
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.5.3.2">
         85.6
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.5.3.3">
         76.8
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.5.3.4">
         80.4
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.5.3.5">
         79.6
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.5.3.6">
         72.1
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.5.3.7">
         75.3
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.6.4">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.6.4.1">
         Energy
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.6.4.2">
         72.4
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.6.4.3">
         57.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.6.4.4">
         63.1
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.6.4.5">
         77.7
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.6.4.6">
         65.3
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T2.1.6.4.7">
         69.9
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T2.1.7.5">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.1.7.5.1">
         Speed
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.5.2">
         47.2
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.5.3">
         36.7
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.1.7.5.4">
         41.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.5.5">
         48.5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.5.6">
         41.5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.5.7">
         44.7
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 2:
      </span>
      Performance evaluation of the speech captioner. Precision, recall and F1-score (%) are utilized to measure its generalization ability on both the validation and test sets. Predicted labels are obtained through semantic classification on the generated captions, while the true labels are derived from the TextroSpeech dataset.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S4.SS3.SSS2.p1">
     <p class="ltx_p" id="S4.SS3.SSS2.p1.1">
      Table
      <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.3.2 Speech Captioner ‣ 4.3 Result Analysis ‣ 4 Experiments ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      evaluates the speech captioner’s generalization performance on both the validation and test sets. Overall, it is evident that that the model achieves the highest F1-score for gender, followed by pitch and emotion. This underscores the model’s proficiency in accurately discerning these attributes from input speech. Besides, both gender and emotion exhibit closely aligned precision and recall metrics, affirming the model’s predictive prowess for these attributes. Meanwhile, there exists a notable disparity between precision and recall when predicting energy, indicating variable performance and a tendency towards confident predictions. Conversely, the model’s performance in predicting speed is unsatisfactory, which can be attributed to the imbalanced distribution of speed in the training dataset, with over 60% of samples labeled as “neutral”.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS2.p2">
     <p class="ltx_p" id="S4.SS3.SSS2.p2.1">
      We also discuss how errors in speech processing are affected by demographics of the speakers. Table
      <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.3.2 Speech Captioner ‣ 4.3 Result Analysis ‣ 4 Experiments ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      compares the performance of the speech captioner across genders, which represents the most prevalent factor. The F1-score on male speech surpasses that on female speech in terms of pitch, energy and speed, despite the comparable sample sizes for male and female groups (8634 VS. 8983). This demonstrates a variation in the model’s performance depending on the gender of the speakers.
     </p>
    </div>
    <figure class="ltx_table" id="S4.T3">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S4.T3.1.1.1">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.1" rowspan="2">
         <span class="ltx_text" id="S4.T3.1.1.1.1.1">
          Attribute
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T3.1.1.1.2">
         Male
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T3.1.1.1.3">
         Female
        </th>
       </tr>
       <tr class="ltx_tr" id="S4.T3.1.2.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.1">
         Precision
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.2">
         Recall
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.2.2.3">
         F1-score
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.4">
         Precision
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.5">
         Recall
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.6">
         F1-score
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S4.T3.1.3.1">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.3.1.1">
         Emotion
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.2">
         84.3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.3">
         85.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.4">
         84.2
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.5">
         87.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.6">
         85.5
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.7">
         86.0
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.1.4.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.4.2.1">
         Pitch
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.2">
         88.2
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.3">
         82.8
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.4">
         85.3
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.5">
         84.8
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.6">
         71.0
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.7">
         75.9
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.1.5.3">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.5.3.1">
         Energy
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.2">
         74.4
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.3">
         60.0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.4">
         65.0
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.5">
         71.2
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.6">
         54.9
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.7">
         60.9
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T3.1.6.4">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.1.6.4.1">
         Speed
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.2">
         46.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.3">
         43.1
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.6.4.4">
         44.6
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.5">
         48.0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.6">
         30.6
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.7">
         37.3
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 3:
      </span>
      Comparison of the speech captioner’s performance across genders.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S4.SS3.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.3
     </span>
     MSMA-Synthesizer
    </h4>
    <figure class="ltx_table" id="S4.T4">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S4.T4.1.1.1">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.1.1.1.1" rowspan="2">
         <span class="ltx_text" id="S4.T4.1.1.1.1.1">
          Method
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S4.T4.1.1.1.2">
         Accuracy
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3">
         FFE
        </th>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.2.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.1">
         Exp-R
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T4.1.2.2.2">
         Exp-I
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.2.2.3">
         Exp
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S4.T4.1.3.1">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.3.1.1">
         GT
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.2">
         91.9
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.1.3">
         75.1
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.4">
         -
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.4.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.4.2.1">
         EXPRESSO
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.2">
         <span class="ltx_text ltx_font_bold" id="S4.T4.1.4.2.2.1">
          87.9
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.2.3">
         67.0
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.4">
         <span class="ltx_text ltx_font_bold" id="S4.T4.1.4.2.4.1">
          0.17±0.12
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.5.3">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T4.1.5.3.1">
         MSMA
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.5.3.2">
         83.8
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.5.3.3">
         <span class="ltx_text ltx_font_bold" id="S4.T4.1.5.3.3.1">
          70.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.5.3.4">
         0.39±0.16
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 4:
      </span>
      Preservation evaluation of MSMA-Synthesizer. Accuracy (%) is evaluated on EXPRESSO read (Exp-R) and conversation (Exp-I) dataset. F0 Frame Error (FFE) is calculated on EXPRESSO (Exp). GT represents the results of automatic metrics calculated on real audio. EXPRESSO and MSMA refer to the synthesizers in EXPRESSO and PerceptiveAgent respectively.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S4.SS3.SSS3.p1">
     <p class="ltx_p" id="S4.SS3.SSS3.p1.1">
      Table
      <a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ 4.3.3 MSMA-Synthesizer ‣ 4.3 Result Analysis ‣ 4 Experiments ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      assesses the MSMA-Synthesizer’s ability to preserve emotion and prosody features on the test set, where the EXPRESSO Synthesizer is reproduced by us. The “GT” method represents the results of automatic metrics calculated on real audio. Clearly, the MSMA-Synthesizer achieves higher accuracy on the read dataset compared to EXPRESSO. This suggests that an integration of multiple attributes into speech synthesis can more effectively enable the model to synthesize emotionally expressive audio in dialogue scenarios, meeting the requirements of our system. However, there is a decrease in accuracy on the Exp-R dataset, which is relevant to the less apparent variation in prosody with emotional transitions. Additionally, in terms of FFE, it can be observed that incorporating multiple attributes into the MSMA-Synthesizer may lead to some degradation in speech synthesis quality. However this degradation remains within an acceptable range.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S4.SS3.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.4
     </span>
     Ablation Study
    </h4>
    <div class="ltx_para ltx_noindent" id="S4.SS3.SSS4.p1">
     <p class="ltx_p" id="S4.SS3.SSS4.p1.1">
      <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p1.1.1">
       Effectiveness of Captions.
      </span>
      The last line in Table
      <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.3.1 PerceptiveAgent ‣ 4.3 Result Analysis ‣ 4 Experiments ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
       <span class="ltx_text ltx_ref_tag">
        1
       </span>
      </a>
      demonstrates the effectiveness of captions in PerceptiveAgent. The system without captions synthesizes speech using randomly selected labels for all four speaking attributes (pitch, speed, energy, and emotion), while maintaining the same response contents as the PerceptiveAgent. It is evident that the PerceptiveAgent outperforms the system without captions, highlighting the effectiveness of captions in generating speech with affective empathy.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S4.SS3.SSS4.p2">
     <p class="ltx_p" id="S4.SS3.SSS4.p2.1">
      <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS4.p2.1.1">
       Effectiveness of Style Factors.
      </span>
      To discern the discrete impact of distinct speaking style factors, we conduct an ablation experiment by systematically varying each factor while maintaining the others at their default values. Table
      <a class="ltx_ref" href="#S4.T5" title="Table 5 ‣ 4.3.4 Ablation Study ‣ 4.3 Result Analysis ‣ 4 Experiments ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      presents that the model with style remained achieves the highest accuracy and the lowest FFE, while the models with the other factors exhibit similar performance. This underscores the predominant contribution of style to the effectiveness of expressive speech synthesis.
     </p>
    </div>
    <figure class="ltx_table" id="S4.T5">
     <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="S4.T5.1.1.1">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T5.1.1.1.1" rowspan="2">
         <span class="ltx_text" id="S4.T5.1.1.1.1.1">
          Method
         </span>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S4.T5.1.1.1.2">
         Accuracy
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.3">
         FFE
        </th>
       </tr>
       <tr class="ltx_tr" id="S4.T5.1.2.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.1.2.2.1">
         Exp-R
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T5.1.2.2.2">
         Exp-I
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.1.2.2.3">
         Exp
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="S4.T5.1.3.1">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.1.3.1.1">
         GT
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.3.1.2">
         91.9
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.1.3.1.3">
         75.1
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.3.1.4">
         -
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T5.1.4.2">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.4.2.1">
         EXPRESSO
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.4.2.2">
         <span class="ltx_text ltx_font_bold" id="S4.T5.1.4.2.2.1">
          87.9
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.4.2.3">
         67.0
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.4.2.4">
         <span class="ltx_text ltx_font_bold" id="S4.T5.1.4.2.4.1">
          0.17±0.12
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T5.1.5.3">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.5.3.1">
         MSMA
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.5.3.2">
         83.8
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.5.3.3">
         <span class="ltx_text ltx_font_bold" id="S4.T5.1.5.3.3.1">
          70.8
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.5.3.4">
         0.39±0.16
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T5.1.6.4">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.6.4.1">
         -style
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.6.4.2">
         82.2
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.6.4.3">
         69.0
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.6.4.4">
         0.40±0.16
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T5.1.7.5">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.7.5.1">
         -speed
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.7.5.2">
         31.8
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.7.5.3">
         9.2
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.7.5.4">
         0.44±0.13
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T5.1.8.6">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.8.6.1">
         -energy
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.8.6.2">
         31.0
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.8.6.3">
         9.1
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.8.6.4">
         0.44±0.13
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T5.1.9.7">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.9.7.1">
         -gender
        </th>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.9.7.2">
         30.8
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.9.7.3">
         8.7
        </td>
        <td class="ltx_td ltx_align_center" id="S4.T5.1.9.7.4">
         0.44±0.13
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T5.1.10.8">
        <th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T5.1.10.8.1">
         -pitch
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.10.8.2">
         30.7
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T5.1.10.8.3">
         7.4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.10.8.4">
         0.43±0.13
        </td>
       </tr>
      </tbody>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 5:
      </span>
      Performance of the MSMA-Synthesizer conditioned on single speaking style factors.
     </figcaption>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Case Study
  </h2>
  <figure class="ltx_figure" id="S5.F3">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F3.sf1">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="235" id="S5.F3.sf1.g1" src="/html/2406.12707/assets/x3.png" width="333"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        (a)
       </span>
       Contradictory Example
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F3.sf2">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="193" id="S5.F3.sf2.g1" src="/html/2406.12707/assets/x4.png" width="332"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        (b)
       </span>
       Consistent Example
      </figcaption>
     </figure>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 3:
    </span>
    Cases comparing the response quality between Speech-GPT3.5 and PerceptiveAgent.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    Figure
    <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5 Case Study ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    presents two cases comparing the response quality between Speech-GPT3.5 and PerceptiveAgent. It demonstrates that by explicitly incorporating acoustic information through captions, the LLM can more accurately comprehend the speaker’s intentions and generate more accurate and contextually appropriate responses. The first and second examples illustrate scenarios where the speaker’s intention either contradicts or aligns with the linguistic contents, respectively.
   </p>
  </div>
  <div class="ltx_para" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    The first example in Figure
    <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5 Case Study ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    (a) depicts an unplanned meeting conversation between two friends. Analyzing solely from the textual contents, it is suggested that the speaker B is extremely excited and delighted about this conversation. However, a closer examination of the key words of “lower vocal” and “subbed energy” in speaker B’s caption reveals an evasive attitude towards the situation. Consequently, when confronted with speaker A’s question, “Were you here waiting for me?”, it can be inferred that speaker B is not inclined to engage in extensive conversation. The absence of nuanced captions poses a challenge for Speech-GPT3.5, leading to a misinterpretation and generating a response that implies a strong desire to continue the conversation. In contrast, PerceptiveAgent provides a response in accordance with the underlying meaning. Therefore, despite potential inconsistencies between linguistic contents and speaker intentions disrupting the accuracy of dialogue context understanding, PerceptiveAgent, with the aid of captions, can effectively capture the speaker’s intent by correctly discerning the acoustic information of speech.
   </p>
  </div>
  <div class="ltx_para" id="S5.p3">
   <p class="ltx_p" id="S5.p3.1">
    In the second example in Figure
    <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5 Case Study ‣ Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    (b), the speaker A receives a paper from his mother and intends to share it with his friends. It can be inferred that he is highly excited at the moment, as evidenced by the key words “treble tone” and “energetically” in the caption. Recognizing speaker A’s excited mood, the response generated by PerceptiveAgent mirrors the same enthusiasm and curiosity, aligning well with the ground truth. However, Speech-GPT3.5 fails to perceive speaker A’s excitement and merely raises the question in a bland manner. Thus, in scenarios where the textual contents coincides with the speaker’s intent, our model can also provide responses that correspond to the context of the conversation.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system capable of accurately discerning the speaker’s intentions through the integration of perceptive speech captions and to respond with nuanced and expressive spoken dialogues. Specifically, PerceptiveAgent comprises three cascaded modules: a speech captioner for intention discernment, an LLM for comprehension through sensory integration, and an MSMA-Synthesizer for expressive speech synthesis. Initially, the system employs a perceptive captioner model to capture acoustic features from each speech within dialogues. Subsequently, an LLM module serves as the cognitive core, generating relevant response content with a caption conditioned on the comprehension of the speaker’s intentions. An MSMA-Synthesizer is then developed to synthesize expressive speech. Experimental results indicate PerceptiveAgent’s strong ability in empathetic response generation, closely aligning with the dialogue context in terms of linguistic contents and exhibiting high expressiveness in acoustic information. Additionally, a case study demonstrates PerceptiveAgent’s capability to accurately identify the speaker’s intentions in scenarios where the literal interpretations of words are either contrary to or inconsistent with the speaker’s true feelings.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Limitations
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    Although PerceptiveAgent excels at providing empathetic responses in terms of both linguistic and acoustic contents, several limitations can be observed in this system: 1)
    <span class="ltx_text ltx_font_bold" id="S7.p1.1.1">
     Dataset Limitation
    </span>
    : PerceptiveAgent’s perception ability is currently constrained by the comprehensiveness of the training dataset in describing speech information. Presently, it is unable to discern speaker identity and background noise from speech; 2)
    <span class="ltx_text ltx_font_bold" id="S7.p1.1.2">
     Time Delay Limitation
    </span>
    : PerceptiveAgent is a system cascaded by three interconnected components, which introduces accumulated delays to the response time, and 3)
    <span class="ltx_text ltx_font_bold" id="S7.p1.1.3">
     Length Limitation
    </span>
    : The maximum token length in LLMs may limit the multi-turn dialogue.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="Sx1">
  <h2 class="ltx_title ltx_title_section">
   Acknowledgements
  </h2>
  <div class="ltx_para" id="Sx1.p1">
   <p class="ltx_p" id="Sx1.p1.1">
    This research was supported by the National Natural Science Foundation of China (Grant No. 62276245).
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.04160" target="_blank" title="">
      X-LLM: bootstrapping advanced large language models by treating multi-modalities as foreign languages
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      CoRR
     </em>
     , abs/2305.04160.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cong et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Jian Cong, Shan Yang, Na Hu, Guangzhi Li, Lei Xie, and Dan Su. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.21437/INTERSPEECH.2021-412" target="_blank" title="">
      Controllable context-aware conversational speech synthesis
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      Interspeech
     </em>
     , pages 4658–4662. ISCA.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cuff et al. (2016)
    </span>
    <span class="ltx_bibblock">
     Benjamin MP Cuff, Sarah J Brown, Laura Taylor, and Douglas J Howat. 2016.
    </span>
    <span class="ltx_bibblock">
     Empathy: A review of the concept.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      Emotion review
     </em>
     , 8(2):144–153.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Girdhar et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR52729.2023.01457" target="_blank" title="">
      Imagebind one embedding space to bind them all
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     , pages 15180–15190. IEEE.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Haohan Guo, Shaofei Zhang, Frank K. Soong, Lei He, and Lei Xie. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/SLT48900.2021.9383460" target="_blank" title="">
      Conversational end-to-end TTS for voice agents
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      IEEE Spoken Language Technology Workshop
     </em>
     , pages 403–409. IEEE.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu Tan. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICASSP49357.2023.10096285" target="_blank" title="">
      Prompttts: Controllable text-to-speech with text descriptions
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
     </em>
     , pages 1–5. IEEE.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hsu et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TASLP.2021.3122291" target="_blank" title="">
      Hubert: Self-supervised speech representation learning by masked prediction of hidden units
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      IEEE/ACM Transactions on Audio, Speech, and Language Processing
     </em>
     , 29:3451–3460.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Yuexian Zou, Zhou Zhao, and Shinji Watanabe. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1609/AAAI.V38I21.30570" target="_blank" title="">
      Audiogpt: Understanding and generating speech, music, sound, and talking head
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      Proceedings of the AAAI Conference on Artificial Intelligence
     </em>
     , pages 23802–23804. AAAI Press.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ito and Johnson (2017)
    </span>
    <span class="ltx_bibblock">
     Keith Ito and Linda Johnson. 2017.
    </span>
    <span class="ltx_bibblock">
     The lj speech dataset.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://keithito.com/LJ-Speech-Dataset/" target="_blank" title="">
      https://keithito.com/LJ-Speech-Dataset/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ji et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shengpeng Ji, Jialong Zuo, Minghui Fang, Ziyue Jiang, Feiyang Chen, Xinyu Duan, Baoxing Huai, and Zhou Zhao. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2308.14430" target="_blank" title="">
      Textrolspeech: A text style control speech corpus with codec language text-to-speech models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      CoRR
     </em>
     , abs/2308.14430.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kharitonov et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, and Yossi Adi. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2202.07359" target="_blank" title="">
      textless-lib: a library for textless spoken language processing
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      CoRR
     </em>
     , abs/2202.07359.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Hyunwoo Kim, Byeongchang Kim, and Gunhee Kim. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.EMNLP-MAIN.170" target="_blank" title="">
      Perspective-taking and pragmatics for generating empathetic responses focused on emotion causes
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 2227–2240. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Minkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICASSP49357.2023.10096877" target="_blank" title="">
      Prefix tuning for automated audio captioning
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
     </em>
     , pages 1–5. IEEE.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Le et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, and Wei-Ning Hsu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2023/hash/2d8911db9ecedf866015091b28946e15-Abstract-Conference.html" target="_blank" title="">
      Voicebox: Text-guided multilingual universal speech generation at scale
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      Advances in Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Leng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, and Jiang Bian. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.02285" target="_blank" title="">
      Prompttts 2: Describing and generating voices with text prompt
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      CoRR
     </em>
     , abs/2309.02285.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/li23q.html" target="_blank" title="">
      BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      International Conference on Machine Learning
     </em>
     , volume 202 of
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">
      Proceedings of Machine Learning Research
     </em>
     , pages 19730–19742. PMLR.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v162/li22n.html" target="_blank" title="">
      BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      International Conference on Machine Learning
     </em>
     , volume 162 of
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">
      Proceedings of Machine Learning Research
     </em>
     , pages 12888–12900. PMLR.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Chaohu Liu, Kun Yin, Haoyu Cao, Xinghua Jiang, Xin Li, Yinsong Liu, Deqiang Jiang, Xing Sun, and Linli Xu. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2404.06918" target="_blank" title="">
      HRVDA: high-resolution visual document assistant
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      CoRR
     </em>
     , abs/2404.06918.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mitsui et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Kentaro Mitsui, Yukiya Hono, and Kei Sawada. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.01088" target="_blank" title="">
      Towards human-like spoken dialogue generation between AI agents from written dialogue
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      CoRR
     </em>
     , abs/2310.01088.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Negnevitsky (2005)
    </span>
    <span class="ltx_bibblock">
     Michael Negnevitsky. 2005.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Artificial intelligence: a guide to intelligent systems
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Pearson education.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nguyen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Tu Anh Nguyen, Wei-Ning Hsu, Antony D’Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarandi, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, and Emmanuel Dupoux. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2308.05725" target="_blank" title="">
      EXPRESSO: A benchmark and analysis of discrete expressive speech resynthesis
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      CoRR
     </em>
     , abs/2308.05725.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nguyen et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoît Sagot, Abdelrahman Mohamed, and Emmanuel Dupoux. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2203.16502" target="_blank" title="">
      Generative spoken dialogue language modeling
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      CoRR
     </em>
     , abs/2203.16502.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nishimura et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Yuto Nishimura, Yuki Saito, Shinnosuke Takamichi, Kentaro Tachibana, and Hiroshi Saruwatari. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.21437/INTERSPEECH.2022-403" target="_blank" title="">
      Acoustic modeling for end-to-end empathetic dialogue speech synthesis using linguistic and prosodic contexts of dialogue history
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      Interspeech
     </em>
     , pages 3373–3377. ISCA.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Peng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2304.03277" target="_blank" title="">
      Instruction tuning with GPT-4
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      CoRR
     </em>
     , abs/2304.03277.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Polyak et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.21437/INTERSPEECH.2021-475" target="_blank" title="">
      Speech resynthesis from discrete disentangled self-supervised representations
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      Interspeech
     </em>
     , pages 3615–3619. ISCA.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Poria et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/P19-1050" target="_blank" title="">
      MELD: A multimodal multi-party dataset for emotion recognition in conversations
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      Proceedings of the 57th Conference of the Association for Computational Linguistics
     </em>
     , pages 527–536. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.
    </span>
    <span class="ltx_bibblock">
     Language models are unsupervised multitask learners.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Reis et al. (2011)
    </span>
    <span class="ltx_bibblock">
     Harry T Reis, Michael R Maniaci, Peter A Caprariello, Paul W Eastwick, and Eli J Finkel. 2011.
    </span>
    <span class="ltx_bibblock">
     Familiarity does indeed promote attraction in live interaction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      Journal of personality and social psychology
     </em>
     , 101(3):557.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Russell and Norvig (2010)
    </span>
    <span class="ltx_bibblock">
     Stuart J Russell and Peter Norvig. 2010.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      Artificial intelligence a modern approach
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     London.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sabour et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Sahand Sabour, Chujie Zheng, and Minlie Huang. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1609/AAAI.V36I10.21373" target="_blank" title="">
      CEM: commonsense-aware empathetic response generation
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      Proceedings of the AAAI Conference on Artificial Intelligence
     </em>
     , pages 11229–11237. AAAI Press.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Saito et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yuki Saito, Shinnosuke Takamichi, Eiji Iimori, Kentaro Tachibana, and Hiroshi Saruwatari. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.13724" target="_blank" title="">
      Chatgpt-edss: Empathetic dialogue speech synthesis trained from chatgpt-derived context word embeddings
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      CoRR
     </em>
     , abs/2305.13724.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schaumlöffel et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Timothy Schaumlöffel, Martina G Vilas, and Gemma Roig. 2023.
    </span>
    <span class="ltx_bibblock">
     Peacs: Prefix encoding for auditory caption synthesis.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      Proceedings of the Detection and Classification of Acoustic. Scenes Events Challenge
     </em>
     , pages 1–3.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shanahan (2024)
    </span>
    <span class="ltx_bibblock">
     Murray Shanahan. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3624724" target="_blank" title="">
      Talking about large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      Commun. ACM
     </em>
     , 67(2):68–79.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2304.09116" target="_blank" title="">
      Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      CoRR
     </em>
     , abs/2304.09116.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shimizu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Reo Shimizu, Ryuichi Yamamoto, Masaya Kawamura, Yuma Shirahata, Hironori Doi, Tatsuya Komatsu, and Kentaro Tachibana. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.08140" target="_blank" title="">
      Prompttts++: Controlling speaker identity in prompt-based text-to-speech using natural language descriptions
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      CoRR
     </em>
     , abs/2309.08140.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Smith (2006)
    </span>
    <span class="ltx_bibblock">
     Adam Smith. 2006.
    </span>
    <span class="ltx_bibblock">
     Cognitive empathy and emotional empathy in human behavior and evolution.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      The Psychological Record
     </em>
     , 56(1):3–21.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Taylor et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2211.09085" target="_blank" title="">
      Galactica: A large language model for science
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      CoRR
     </em>
     , abs/2211.09085.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.09288" target="_blank" title="">
      Llama 2: Open foundation and fine-tuned chat models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      CoRR
     </em>
     , abs/2307.09288.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vaswani et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" target="_blank" title="">
      Attention is all you need
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , pages 5998–6008.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2301.02111" target="_blank" title="">
      Neural codec language models are zero-shot text to speech synthesizers
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      CoRR
     </em>
     , abs/2301.02111.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" target="_blank" title="">
      Chain-of-thought prompting elicits reasoning in large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , pages 24824–24837.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.05519" target="_blank" title="">
      Next-gpt: Any-to-any multimodal LLM
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      CoRR
     </em>
     , abs/2309.05519.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Yihan Wu, Xu Tan, Bohan Li, Lei He, Sheng Zhao, Ruihua Song, Tao Qin, and Tie-Yan Liu. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.21437/INTERSPEECH.2022-901" target="_blank" title="">
      Adaspeech 4: Adaptive text to speech in zero-shot scenarios
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      Interspeech
     </em>
     , pages 2568–2572. ISCA.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Yaoxun Xu, Hangting Chen, Jianwei Yu, Qiaochu Huang, Zhiyong Wu, Shi-Xiong Zhang, Guangzhi Li, Yi Luo, and Rongzhi Gu. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1609/AAAI.V38I17.29902" target="_blank" title="">
      Secap: Speech emotion captioning with large language model
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      Proceedings of the AAAI Conference on Artificial Intelligence
     </em>
     , pages 19323–19331. AAAI Press.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yamagishi et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. 2019.
    </span>
    <span class="ltx_bibblock">
     Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92).
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.7488/ds/2645" target="_blank" title="">
      https://doi.org/10.7488/ds/2645
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.1055" target="_blank" title="">
      Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 15757–15773. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Fang Zhang, Yongxin Zhu, Xiangxiang Wang, Huang Chen, Xing Sun, and Linli Xu. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1609/AAAI.V38I17.29926" target="_blank" title="">
      Visual hallucination elevates speech recognition
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      Proceedings of the AAAI Conference on Artificial Intelligence
     </em>
     , pages 19542–19550. AAAI Press.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SkeHuCVFDr" target="_blank" title="">
      Bertscore: Evaluating text generation with BERT
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      International Conference on Learning Representations
     </em>
     . OpenReview.net.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.08581" target="_blank" title="">
      Bubogpt: Enabling visual grounding in multi-modal llms
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      CoRR
     </em>
     , abs/2307.08581.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2304.10592" target="_blank" title="">
      Minigpt-4: Enhancing vision-language understanding with advanced large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      CoRR
     </em>
     , abs/2304.10592.
    </span>
   </li>
  </ul>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Prompt for Dialogue Generation with Captions
  </h2>
  <figure class="ltx_figure" id="A1.1">
   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="516" id="A1.1.g1" src="/html/2406.12707/assets/x5.png" width="415"/>
  </figure>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Prompt for Dialogue Generation without Captions
  </h2>
  <figure class="ltx_figure" id="A2.1">
   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="393" id="A2.1.g1" src="/html/2406.12707/assets/x6.png" width="415"/>
  </figure>
 </section>
</article>
