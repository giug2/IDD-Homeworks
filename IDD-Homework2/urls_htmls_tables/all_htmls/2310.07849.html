<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.07849] Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations</title><meta property="og:description" content="The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment.
Researche…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.07849">

<!--Generated on Wed Feb 28 01:25:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zhuoyan Li<sup id="id1.1.id1" class="ltx_sup">1</sup>, Hangxiao Zhu<sup id="id2.2.id2" class="ltx_sup">2</sup>, Zhuoran Lu<sup id="id3.3.id3" class="ltx_sup">1</sup>, Ming Yin<sup id="id4.4.id4" class="ltx_sup">1</sup> 
<br class="ltx_break"><sup id="id5.5.id5" class="ltx_sup">1</sup>Purdue University 
<br class="ltx_break"><sup id="id6.6.id6" class="ltx_sup">2</sup>Washington University in St. Louis 
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter">{li4178, lu800, mingyin}@purdue.edu</span>, <span id="id8.8.id8" class="ltx_text ltx_font_typewriter">hangxiao@wustl.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment.
Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks.
To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the <span id="id9.id1.1" class="ltx_text ltx_font_italic">subjectivity</span> of classification.
Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data.
We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The collected human annotations are available at 
<br class="ltx_break">huggingface.co/datasets/xfleezy/human_annotation_emnlp23.</span></span></span>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Today, machine-learning-powered text classification models have been widely applied in diverse applications such as detecting biased or toxic language on online platforms <cite class="ltx_cite ltx_citemacro_cite">Wiegand et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite> and filtering spam emails <cite class="ltx_cite ltx_citemacro_cite">Jindal and Liu (<a href="#bib.bib25" title="" class="ltx_ref">2007</a>)</cite>. However, the performance of these models largely depends on the quality of the training data. This poses a substantial challenge in practice, especially when models need to be built for a novel task domain or to incorporate new classification categories, as the training data collection and curation process is often costly, time-consuming, and complex.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Meanwhile, with the recent advancements in large language models (LLMs), researchers have started to explore the potential of utilizing LLMs for generating synthetic data tailored to specific tasks and augmenting the training data in low-resourced data settings <cite class="ltx_cite ltx_citemacro_cite">Kumar et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>); Yoo et al. (<a href="#bib.bib53" title="" class="ltx_ref">2021</a>); Hartvigsen et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>); Sahu et al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>.
Most recently, a few studies also investigate into the feasibility of generating a synthetic dataset from scratch using LLMs to support zero-shot learning <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib47" title="" class="ltx_ref">2021</a>); Tang et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>); Gao et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>.
While LLM-based data augmentation is often found to outperform other data augmentation methods in boosting the model performance, mixed results are reported regarding whether the LLM-generated synthetic data can effectively support model training to enable a level of model performance that is comparable to models trained on the data collected in the real world and carefully annotated. This leaves uncertainty for researchers and practitioners in deciding whether to rely on LLMs for synthetic data generation or to proceed with the traditional data collection and curation pipeline when they need to construct a text classification model for a new task. Naturally, one may wonder <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">what factors might moderate the effectiveness of LLM-generated synthetic data in facilitating successful model training</span>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We conjecture that one such factor could be the <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">subjectivity</span> of classification tasks. Indeed, language is inherently subjective and interpretive <cite class="ltx_cite ltx_citemacro_cite">Benveniste (<a href="#bib.bib5" title="" class="ltx_ref">1971</a>); Wiebe et al. (<a href="#bib.bib49" title="" class="ltx_ref">2004</a>)</cite>. Previous research has showed that people often perceive the same text in different ways because of their personal biases and perspectives <cite class="ltx_cite ltx_citemacro_cite">Sap et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>); Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>); Gordon et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>. Thus, achieving high model performance for classification tasks with high subjectivity seems to impose a greater demand on the training data in reflecting the richness and nuances present in human language, and the extent to which LLM-generated synthetic data can acompolish this objective is unclear.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Thus, in this paper, we formally evaluate the effectiveness of LLM (i.e., the cutting-edge GPT-3.5-Turbo model) in generating synthetic data to support model training for different text classification tasks. We adopt two approaches for synthetic data generation—a <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">zero-shot</span> setting in which the LLM is directly prompted to generate text instances with different labels of interests, and a <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">few-shot</span> setting in which a few real-world data instances are provided as examples to guide the LLM in generating the synthetic data. We conduct two evaluation studies, each corresponding to one dimension of subjectivity—the first study examines the effectiveness of the synthetic data on 10 types of classification tasks and explores how it varies with the <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">task-level subjectivity</span> (i.e., whether this type of classification task is subjective); the second study concerns that given a specific classification task, how the performance of a model trained on synthetic data changes with the <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">instance-level subjectivity</span> (i.e., whether people tend to disagree with each other on the label of this task instance). Our findings suggest that across the 10 types of classification tasks that we have considered in this study, models trained on the LLM-generated synthetic data generally perform worse than those trained on the real-world data, yet guiding LLM’s synthetic data generation process with a small amount of real-world data (i.e., as done in the few-shot data generation setting) can improve the effectiveness of the data generated.
Moreover, we find that the performance of models trained on the LLM-generated synthetic data is very close to those trained on the real-world data for tasks with low subjectivity (e.g., news topic classification, spam email detection), while the performance decrease is much bigger on tasks with high subjectivity (e.g., humor or sarcasm detection). Finally, even within the same type of classification task, models trained on the LLM-generated synthetic data tend to exhibit a higher level of performance on those task instances with lower subjectivity, for which human annotators exhibit a higher level of agreement in their annotation.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Together, our study provides important experimental evidence regarding the potential and limitations of using LLMs to generate synthetic data for text classification tasks. We conclude by discussing the implications, limitations, and future work of our study.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Generative AI in synthetic data generation.</span> Recent advancements in generative AI have motivated numerous studies to explore the potential of leveraging generative models to create synthetic data for training machine learning models, especially for computer vision (CV) and natural language processing (NLP) tasks. In the realm of CV, several works have utilized GAN-based models <cite class="ltx_cite ltx_citemacro_cite">Karras et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> or diffusion models <cite class="ltx_cite ltx_citemacro_cite">Nichol et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite> to generate synthetic data for image recognition <cite class="ltx_cite ltx_citemacro_cite">Besnier et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>); He et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> or object segmentation <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>. Similarly, in the NLP field, researchers have also probed into the capacity of language models in generating synthetic data for various text classification tasks <cite class="ltx_cite ltx_citemacro_cite">Kumar et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>); Chung et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>); Sahu et al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>); Yoo et al. (<a href="#bib.bib53" title="" class="ltx_ref">2021</a>); Ye et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib47" title="" class="ltx_ref">2021</a>); Hartvigsen et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>); Meng et al. (<a href="#bib.bib33" title="" class="ltx_ref">2022</a>); Gao et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>); Aggarwal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); Chen et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>, with mixed results reported regarding the effectiveness of the synthetic data generated.
In this study, we aim to obtain a better understanding of <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">when</span> the synthetic data generated by language models can lead to effective model training, and we focus on exploring the role of task subjectivity in moderating the effectiveness of the synthetic data.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Large language models.</span>
Based on the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib45" title="" class="ltx_ref">2017</a>)</cite>, large language models (LLMs) have facilitated remarkable progress in the field of natural language processing. The utilization of bidirectional contexts in the BERT model <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> has resulted in superior performance across a wide range of tasks. Building on this, OpenAI’s GPT series, comprising of models like GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>, the colossal GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> with an impressive 175 billion parameters and the most recent GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite>, pushed the boundaries of possibilities of LLMs. These models exhibit remarkable proficiency in generating high-quality human-like text <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>); Dou et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>); Zhou et al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>, showcasing capabilities in rudimentary reasoning <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite>, translation <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, scientific synthetic data generation <cite class="ltx_cite ltx_citemacro_cite">Hämäläinen et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, and code generation <cite class="ltx_cite ltx_citemacro_cite">Mcnutt et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>.
In this study, we focus on leveraging the cutting-edge GPT-3.5-Turbo model<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We used GPT-3.5-Turbo as the foundational model to generate synthetic data because at the time of this study, an official API for the more advanced GPT-4 model was not yet available from OpenAI.</span></span></span> to explore its capabilities and limitations in synthesizing data for text classification tasks with different subjectivity levels.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodolgy</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we outline the procedure we have followed when leveraging the large language model to generate the synthetic training data for text classification. We consider two data generation settings in this study, i.e., the <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">zero-shot</span> setting and the <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">few-shot</span> setting.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Zero-shot Synthetic Data Generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Under the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">zero-shot</span> synthetic data generation setting, given a text classification task, we assume that the real-world data in the form of “text-label pairs” do not exist.
Thus, in order to obtain synthetic training data for the text classification task, two sequential prompts are constructed and supplied to the pretrained large language model (i.e., the GPT-3.5-Turbo model). First, a customized “context prompt” relevant to the targeted domain of interest is used to set the context. For example, in the case of the IMDB movie review classification task <cite class="ltx_cite ltx_citemacro_cite">Maas et al. (<a href="#bib.bib30" title="" class="ltx_ref">2011</a>)</cite>, the customized context prompt used is “Imagine you are a movie reviewer on the IMDB platform”. This prompt aims to encourage the LLM to generate synthetic data that resemble the real texts produced in the targeted domain.
After the context is set, a second prompt, i.e., the “data generation prompt”, is provided to the LLM, instructing the model to generate texts with a specific style, label (with respect to the classification task of interest), and word limit.
For example, for the IMDB movie review classification task, the style of the text is a movie review, and the label is a targeted sentiment conveyed by the review (i.e., “positive” or “negative”).
To further enhance the diversity of the generated data, after the generation of every <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">n</annotation></semantics></math> data points (i.e., texts of targeted styles, labels, and word limits)<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>To increase data diversity while maintaining a reasonable data generation speed, <math id="footnote3.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="footnote3.m1.1b"><mi id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><ci id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">n</annotation></semantics></math> is set to 10 for generating short texts (i.e., texts with a maximum length of 30 words), and 1 for generating longer paragraphs.
</span></span></span>, we provide a “diversity prompt” to the LLM—“Can you provide something more diverse compared to the previously generated data?”—aiming to increase the diversity of the synthetic data generated.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Few-shot Synthetic Data Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Under the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">few-shot</span> synthetic data generation setting, we assume that a small amount of real-world data
are available for the text classification task. These data points can then serve as the examples for the large language model in the data generation process, which can potentially provide LLM with insights of the patterns exhibited in the real-world data. We again start the data generation process by using a context prompt to set the context. However, different from that in the zero-shot setting, here, each time before we instruct the LLM to generate a piece of text, we first provide the model with a few randomly sampled real-world data instances (including both the text and the label) as the examples.
To keep the LLM from merely rephrasing the provided examples, an additional prompt is used to impose a constraint on the LLM in generating the synthetic data (i.e., “You should imitate the example I have provided, but you cannot simply modify or rewrite the example I have given.”).
</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For more details about prompts used for generating data for each type of text classification task, please refer to the App. <a href="#A4" title="Appendix D Additional Details on the Generation of Synthetic Data ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation I: Comparison Across Different Types of Tasks</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In our first evaluation study, we investigate into how well the synthetic data generated by LLM under both zero-shot and few-shot settings can support effective model training for different types of text classification tasks.
We are especially interested in comparing the model performance between those trained on the real-world data and on the LLM-generated synthetic data, and in understanding how the performance of those models trained on the LLM-generated synthetic data varies with the subjectivity of the text classification task.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and Tasks</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We experiment with 10 representative datasets covering a variety of text classification tasks: AG’s news <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib54" title="" class="ltx_ref">2015</a>)</cite>, IMDB reviews <cite class="ltx_cite ltx_citemacro_citep">(Maas et al., <a href="#bib.bib30" title="" class="ltx_ref">2011</a>)</cite>,
SMS spam <cite class="ltx_cite ltx_citemacro_citep">(Almeida et al., <a href="#bib.bib3" title="" class="ltx_ref">2011</a>)</cite>, Financial phrase bank <cite class="ltx_cite ltx_citemacro_citep">(Malo et al., <a href="#bib.bib31" title="" class="ltx_ref">2014</a>)</cite>,
Reddit emotion <cite class="ltx_cite ltx_citemacro_citep">(Demszky et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, Relation classification <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>, Tweet irony speech <cite class="ltx_cite ltx_citemacro_citep">(Van Hee et al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>,
Tweet emotions <cite class="ltx_cite ltx_citemacro_citep">(Mohammad et al., <a href="#bib.bib36" title="" class="ltx_ref">2018</a>)</cite>,
Sarcasm news (<cite class="ltx_cite ltx_citemacro_citep">Misra and Arora, <a href="#bib.bib34" title="" class="ltx_ref">2023</a></cite>, <cite class="ltx_cite ltx_citemacro_citep">Misra and Grover, <a href="#bib.bib35" title="" class="ltx_ref">2021</a></cite>), and
Humor speech <cite class="ltx_cite ltx_citemacro_citep">(Annamoradnejad and Zoghi, <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>.
See App. <a href="#A1.SS1" title="A.1 Dataset and Task Descriptions ‣ Appendix A Appendices ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> for detailed descriptions of datasets and the corresponding text classification tasks.
These datasets are selected with the goal of spanning a wide range of task subjectivity in mind. For example, we conjecture that classifying the news topic category (e.g., as that in the AG’s news dataset) is relatively objective, while determining whether texts are humorous (e.g., as that in the Humor speech dataset) is quite subjective <cite class="ltx_cite ltx_citemacro_cite">Veatch (<a href="#bib.bib46" title="" class="ltx_ref">1998</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Task-level Subjectivity Determination</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To formally determine the subjectivity levels of different text classification tasks, we first conduct a crowdsourced study to collect subjectivity judgements from the crowd.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Study procedure.</span>
We adopt a comparative approach to collect crowdsourced subjectivity judgements in this study.
Specifically,
we recruited crowd workers from Amazon Mechanical Turk (MTurk), and each worker was asked to complete a sequence of 10 subjectivity judgement tasks.
In each task, we randomly sampled a pair of text classification tasks from the 10 tasks that we considered in this evaluation, and we presented to the worker the task description, label description, and task examples for each task in the pair.
Then, the worker was asked to determine which text classification task in the pair was more objective, with “objectivity” of a task defined as “the classification of a piece of text is based on clear, identifiable features in the text (e.g., keywords or phrases), and can be done without being affected by any personal interpretation of the text resulted from personal biases, emotions or beliefs.” The study was restricted to U.S. workers. Each worker was allowed to participate only once and received a $1.2 payment. An attention check question was included in the study to validate the worker’s engagement, and only the data from workers who successfully passed the attention check were considered valid.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Ranking task subjectivity.</span>
After excluding responses from inattentive workers, a total of 540 pairwise subjectivity comparisons for the 10 tasks were obtained from 54 workers. For each pair of tasks, we aggregated relative subjectivity judgments made on this pair to determine which task was perceived as more subjective (i.e., less objective). To produce a ranking of the subjectivity of the 10 tasks, we constructed a directed graph based on the pairwise subjectivity comparisons—each task was a node in this graph, and directed edges were added between each pair of tasks, pointing from the one that was deemed as more subjective (on the aggregate level) to the one deemed as less subjective.
The topological sort algorithm <cite class="ltx_cite ltx_citemacro_cite">Cormen et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite> was then applied to this directed graph to obtain a linear ordering of the nodes. If a cycle was detected within the graph, the corresponding tasks were considered to have the same level of subjectivity
and were merged into a single meta-node before re-runing the algorithm.
Our final task subjectivity ranking results are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Model Training ‣ 4 Evaluation I: Comparison Across Different Types of Tasks ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Training</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Given a text classification task, following the procedures outlined in Sections <a href="#S3.SS1" title="3.1 Zero-shot Synthetic Data Generation ‣ 3 Methodolgy ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and <a href="#S3.SS2" title="3.2 Few-shot Synthetic Data Generation ‣ 3 Methodolgy ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, 3,000 synthetic data points were generated for each candidate label under both zero-shot and few-shot settings.
We then trained classification models using the real-world training data provided by the original dataset, the synthetic data generated under the zero-shot settings, and the synthetic data generated under the few-shot settings<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Under the few-shot setting, we randomly sampled <math id="footnote4.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="footnote4.m1.1b"><mrow id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml"><mn id="footnote4.m1.1.1.2" xref="footnote4.m1.1.1.2.cmml">10</mn><mo id="footnote4.m1.1.1.1" xref="footnote4.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><apply id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1"><csymbol cd="latexml" id="footnote4.m1.1.1.1.cmml" xref="footnote4.m1.1.1.1">percent</csymbol><cn type="integer" id="footnote4.m1.1.1.2.cmml" xref="footnote4.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">10\%</annotation></semantics></math> of the data points from the real-world training data provided in the original dataset as the example pool to guide the LLM’s synthetic data generation process, but only the sythetic data generated were used to train the models.</span></span></span>, respectively.
Specifically, we utilized the pre-trained BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> and RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite> models from Huggingface’s transformers library <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib51" title="" class="ltx_ref">2020</a>)</cite> as the encoders, and used the representation embeddings from the last layer of these models as the input to our classification models. The classification model itself comprised a hidden layer of 768 units and an output layer, and it was fine-tuned with a learning rate of <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="5e-5" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mrow id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2.2" xref="S4.SS3.p1.1.m1.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.2.1" xref="S4.SS3.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.SS3.p1.1.m1.1.1.2.3" xref="S4.SS3.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><minus id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></minus><apply id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><times id="S4.SS3.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1.2.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2.2">5</cn><ci id="S4.SS3.p1.1.m1.1.1.2.3.cmml" xref="S4.SS3.p1.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">5e-5</annotation></semantics></math> and a batch size of 64. For datasets that provided official partitions for training and test sets, we directly evaluated the classification model’s performance on the test sets. Otherwise, we randomly divided the dataset into training (70%), validation (5%), and test (25%) sets<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>To ensure a fair comparison, we maintained an equal size for both the real-world and synthetic training data by downsampling the dataset with a larger size.
</span></span></span>.
Models’ performance was evaluated via Macro-F1 and Accuracy scores, and they were computed by comparing the model’s predictions with the gold labels provided in the test sets.
To ensure the robustness of our results, all experiments were repeated three times, and the average performance across these repetitions was reported.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.40" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:90.3pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-347.5pt,72.1pt) scale(0.384187306798407,0.384187306798407) ;">
<table id="S4.T1.40.40" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.40.40.41.1" class="ltx_tr">
<td id="S4.T1.40.40.41.1.1" class="ltx_td ltx_align_right"><span id="S4.T1.40.40.41.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S4.T1.40.40.41.1.2" class="ltx_td ltx_align_center"><span id="S4.T1.40.40.41.1.2.1" class="ltx_text ltx_font_bold">Subjectivity</span></td>
<td id="S4.T1.40.40.41.1.3" class="ltx_td ltx_align_right"><span id="S4.T1.40.40.41.1.3.1" class="ltx_text ltx_font_bold">BERT</span></td>
<td id="S4.T1.40.40.41.1.4" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.5" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.6" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.7" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.8" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.9" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.41.1.9.1" class="ltx_text ltx_font_bold">RoBERTa</span></td>
<td id="S4.T1.40.40.41.1.10" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.11" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.12" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.13" class="ltx_td"></td>
<td id="S4.T1.40.40.41.1.14" class="ltx_td"></td>
</tr>
<tr id="S4.T1.40.40.42.2" class="ltx_tr">
<td id="S4.T1.40.40.42.2.1" class="ltx_td"></td>
<td id="S4.T1.40.40.42.2.2" class="ltx_td"></td>
<td id="S4.T1.40.40.42.2.3" class="ltx_td ltx_align_right"><span id="S4.T1.40.40.42.2.3.1" class="ltx_text ltx_font_bold">Real-world data</span></td>
<td id="S4.T1.40.40.42.2.4" class="ltx_td"></td>
<td id="S4.T1.40.40.42.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.40.40.42.2.5.1" class="ltx_text ltx_font_bold">Zero-shot setting</span></td>
<td id="S4.T1.40.40.42.2.6" class="ltx_td"></td>
<td id="S4.T1.40.40.42.2.7" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.42.2.7.1" class="ltx_text ltx_font_bold">Few-shot setting</span></td>
<td id="S4.T1.40.40.42.2.8" class="ltx_td"></td>
<td id="S4.T1.40.40.42.2.9" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.42.2.9.1" class="ltx_text ltx_font_bold">Real-world data</span></td>
<td id="S4.T1.40.40.42.2.10" class="ltx_td"></td>
<td id="S4.T1.40.40.42.2.11" class="ltx_td ltx_align_right"><span id="S4.T1.40.40.42.2.11.1" class="ltx_text ltx_font_bold">Zero-shot setting</span></td>
<td id="S4.T1.40.40.42.2.12" class="ltx_td"></td>
<td id="S4.T1.40.40.42.2.13" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.42.2.13.1" class="ltx_text ltx_font_bold">Few-shot setting</span></td>
<td id="S4.T1.40.40.42.2.14" class="ltx_td"></td>
</tr>
<tr id="S4.T1.40.40.43.3" class="ltx_tr">
<td id="S4.T1.40.40.43.3.1" class="ltx_td"></td>
<td id="S4.T1.40.40.43.3.2" class="ltx_td"></td>
<td id="S4.T1.40.40.43.3.3" class="ltx_td ltx_align_right"><span id="S4.T1.40.40.43.3.3.1" class="ltx_text ltx_font_bold">Macro-F1</span></td>
<td id="S4.T1.40.40.43.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.40.40.43.3.4.1" class="ltx_text ltx_font_bold">Accuracy Score</span></td>
<td id="S4.T1.40.40.43.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.40.40.43.3.5.1" class="ltx_text ltx_font_bold">Macro-F1</span></td>
<td id="S4.T1.40.40.43.3.6" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.43.3.6.1" class="ltx_text ltx_font_bold">Accuracy Score</span></td>
<td id="S4.T1.40.40.43.3.7" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.43.3.7.1" class="ltx_text ltx_font_bold">Macro-F1</span></td>
<td id="S4.T1.40.40.43.3.8" class="ltx_td ltx_align_center"><span id="S4.T1.40.40.43.3.8.1" class="ltx_text ltx_font_bold">Accuracy Score</span></td>
<td id="S4.T1.40.40.43.3.9" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.43.3.9.1" class="ltx_text ltx_font_bold">Macro-F1</span></td>
<td id="S4.T1.40.40.43.3.10" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.43.3.10.1" class="ltx_text ltx_font_bold">Accuracy Score</span></td>
<td id="S4.T1.40.40.43.3.11" class="ltx_td ltx_align_right"><span id="S4.T1.40.40.43.3.11.1" class="ltx_text ltx_font_bold">Macro-F1</span></td>
<td id="S4.T1.40.40.43.3.12" class="ltx_td ltx_align_center"><span id="S4.T1.40.40.43.3.12.1" class="ltx_text ltx_font_bold">Accuracy Score</span></td>
<td id="S4.T1.40.40.43.3.13" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.43.3.13.1" class="ltx_text ltx_font_bold">Macro-F1</span></td>
<td id="S4.T1.40.40.43.3.14" class="ltx_td ltx_align_left"><span id="S4.T1.40.40.43.3.14.1" class="ltx_text ltx_font_bold">Accuracy Score</span></td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_right">AG</td>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center"><math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\star</annotation></semantics></math></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_right">95.3%</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center">95.3%</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center">89.3% <span id="S4.T1.1.1.1.5.1" class="ltx_text" style="color:#ED2B2A;">(-6.0%)</span>
</td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_left">89.3% <span id="S4.T1.1.1.1.6.1" class="ltx_text" style="color:#ED2B2A;">(-6.0%)</span>
</td>
<td id="S4.T1.1.1.1.7" class="ltx_td ltx_align_left">91.5% <span id="S4.T1.1.1.1.7.1" class="ltx_text" style="color:#ED2B2A;">(-3.8%)</span>
</td>
<td id="S4.T1.1.1.1.8" class="ltx_td ltx_align_center">91.6% <span id="S4.T1.1.1.1.8.1" class="ltx_text" style="color:#ED2B2A;">(-3.7%)</span>
</td>
<td id="S4.T1.1.1.1.9" class="ltx_td ltx_align_left">94.6%</td>
<td id="S4.T1.1.1.1.10" class="ltx_td ltx_align_left">94.6%</td>
<td id="S4.T1.1.1.1.11" class="ltx_td ltx_align_right">88.6% <span id="S4.T1.1.1.1.11.1" class="ltx_text" style="color:#ED2B2A;">(-6.0%)</span>
</td>
<td id="S4.T1.1.1.1.12" class="ltx_td ltx_align_center">88.6% <span id="S4.T1.1.1.1.12.1" class="ltx_text" style="color:#ED2B2A;">(-6.0%)</span>
</td>
<td id="S4.T1.1.1.1.13" class="ltx_td ltx_align_left">92.9% <span id="S4.T1.1.1.1.13.1" class="ltx_text" style="color:#ED2B2A;">(-1.7%)</span>
</td>
<td id="S4.T1.1.1.1.14" class="ltx_td ltx_align_left">92.9% <span id="S4.T1.1.1.1.14.1" class="ltx_text" style="color:#ED2B2A;">(-1.7%)</span>
</td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_right">Relation</td>
<td id="S4.T1.3.3.3.2" class="ltx_td ltx_align_center">
<math id="S4.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.2.2.2.1.m1.1a"><mo id="S4.T1.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.3.3.3.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.3.3.3.2.m2.1a"><mo id="S4.T1.3.3.3.2.m2.1.1" xref="S4.T1.3.3.3.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.2.m2.1b"><ci id="S4.T1.3.3.3.2.m2.1.1.cmml" xref="S4.T1.3.3.3.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.2.m2.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_right">98.6%</td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center">98.6%</td>
<td id="S4.T1.3.3.3.6" class="ltx_td ltx_align_center">92.4% <span id="S4.T1.3.3.3.6.1" class="ltx_text" style="color:#ED2B2A;">(-6.2%)</span>
</td>
<td id="S4.T1.3.3.3.7" class="ltx_td ltx_align_left">92.7% <span id="S4.T1.3.3.3.7.1" class="ltx_text" style="color:#ED2B2A;">(-5.9%)</span>
</td>
<td id="S4.T1.3.3.3.8" class="ltx_td ltx_align_left">96.4% <span id="S4.T1.3.3.3.8.1" class="ltx_text" style="color:#ED2B2A;">(-2.2%)</span>
</td>
<td id="S4.T1.3.3.3.9" class="ltx_td ltx_align_center">96.4% <span id="S4.T1.3.3.3.9.1" class="ltx_text" style="color:#ED2B2A;">(-2.2%)</span>
</td>
<td id="S4.T1.3.3.3.10" class="ltx_td ltx_align_left">97.0%</td>
<td id="S4.T1.3.3.3.11" class="ltx_td ltx_align_left">96.9%</td>
<td id="S4.T1.3.3.3.12" class="ltx_td ltx_align_right">91.4% <span id="S4.T1.3.3.3.12.1" class="ltx_text" style="color:#ED2B2A;">(-5.6%)</span>
</td>
<td id="S4.T1.3.3.3.13" class="ltx_td ltx_align_center">91.6% <span id="S4.T1.3.3.3.13.1" class="ltx_text" style="color:#ED2B2A;">(-5.3%)</span>
</td>
<td id="S4.T1.3.3.3.14" class="ltx_td ltx_align_left">94.1% <span id="S4.T1.3.3.3.14.1" class="ltx_text" style="color:#ED2B2A;">(-2.9%)</span>
</td>
<td id="S4.T1.3.3.3.15" class="ltx_td ltx_align_left">94.1% <span id="S4.T1.3.3.3.15.1" class="ltx_text" style="color:#ED2B2A;">(-2.8%)</span>
</td>
</tr>
<tr id="S4.T1.6.6.6" class="ltx_tr">
<td id="S4.T1.6.6.6.4" class="ltx_td ltx_align_right">IMDB</td>
<td id="S4.T1.6.6.6.3" class="ltx_td ltx_align_center">
<math id="S4.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.4.4.4.1.m1.1a"><mo id="S4.T1.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.m1.1b"><ci id="S4.T1.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.5.5.5.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.5.5.5.2.m2.1a"><mo id="S4.T1.5.5.5.2.m2.1.1" xref="S4.T1.5.5.5.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.2.m2.1b"><ci id="S4.T1.5.5.5.2.m2.1.1.cmml" xref="S4.T1.5.5.5.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.2.m2.1c">\star</annotation></semantics></math><math id="S4.T1.6.6.6.3.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.6.6.6.3.m3.1a"><mo id="S4.T1.6.6.6.3.m3.1.1" xref="S4.T1.6.6.6.3.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.3.m3.1b"><ci id="S4.T1.6.6.6.3.m3.1.1.cmml" xref="S4.T1.6.6.6.3.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.3.m3.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.6.6.6.5" class="ltx_td ltx_align_right">87.6%</td>
<td id="S4.T1.6.6.6.6" class="ltx_td ltx_align_center">87.6%</td>
<td id="S4.T1.6.6.6.7" class="ltx_td ltx_align_center">81.2% <span id="S4.T1.6.6.6.7.1" class="ltx_text" style="color:#ED2B2A;">(-6.4%)</span>
</td>
<td id="S4.T1.6.6.6.8" class="ltx_td ltx_align_left">81.5% <span id="S4.T1.6.6.6.8.1" class="ltx_text" style="color:#ED2B2A;">(-6.1%)</span>
</td>
<td id="S4.T1.6.6.6.9" class="ltx_td ltx_align_left">81.1% <span id="S4.T1.6.6.6.9.1" class="ltx_text" style="color:#ED2B2A;">(-6.5%)</span>
</td>
<td id="S4.T1.6.6.6.10" class="ltx_td ltx_align_center">81.2% <span id="S4.T1.6.6.6.10.1" class="ltx_text" style="color:#ED2B2A;">(-6.4%)</span>
</td>
<td id="S4.T1.6.6.6.11" class="ltx_td ltx_align_left">89.0%</td>
<td id="S4.T1.6.6.6.12" class="ltx_td ltx_align_left">89.0%</td>
<td id="S4.T1.6.6.6.13" class="ltx_td ltx_align_right">81.2% <span id="S4.T1.6.6.6.13.1" class="ltx_text" style="color:#ED2B2A;">(-7.8%)</span>
</td>
<td id="S4.T1.6.6.6.14" class="ltx_td ltx_align_center">81.3% <span id="S4.T1.6.6.6.14.1" class="ltx_text" style="color:#ED2B2A;">(-7.7%)</span>
</td>
<td id="S4.T1.6.6.6.15" class="ltx_td ltx_align_left">82.4% <span id="S4.T1.6.6.6.15.1" class="ltx_text" style="color:#ED2B2A;">(-1.6%)</span>
</td>
<td id="S4.T1.6.6.6.16" class="ltx_td ltx_align_left">82.4% <span id="S4.T1.6.6.6.16.1" class="ltx_text" style="color:#ED2B2A;">(-1.6%)</span>
</td>
</tr>
<tr id="S4.T1.10.10.10" class="ltx_tr">
<td id="S4.T1.10.10.10.5" class="ltx_td ltx_align_right">SMS spam</td>
<td id="S4.T1.10.10.10.4" class="ltx_td ltx_align_center">
<math id="S4.T1.7.7.7.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.7.7.7.1.m1.1a"><mo id="S4.T1.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.1b"><ci id="S4.T1.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.8.8.8.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.8.8.8.2.m2.1a"><mo id="S4.T1.8.8.8.2.m2.1.1" xref="S4.T1.8.8.8.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.2.m2.1b"><ci id="S4.T1.8.8.8.2.m2.1.1.cmml" xref="S4.T1.8.8.8.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.2.m2.1c">\star</annotation></semantics></math><math id="S4.T1.9.9.9.3.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.9.9.9.3.m3.1a"><mo id="S4.T1.9.9.9.3.m3.1.1" xref="S4.T1.9.9.9.3.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.3.m3.1b"><ci id="S4.T1.9.9.9.3.m3.1.1.cmml" xref="S4.T1.9.9.9.3.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.3.m3.1c">\star</annotation></semantics></math><math id="S4.T1.10.10.10.4.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.10.10.10.4.m4.1a"><mo id="S4.T1.10.10.10.4.m4.1.1" xref="S4.T1.10.10.10.4.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.4.m4.1b"><ci id="S4.T1.10.10.10.4.m4.1.1.cmml" xref="S4.T1.10.10.10.4.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.4.m4.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.10.10.10.6" class="ltx_td ltx_align_right">97.2%</td>
<td id="S4.T1.10.10.10.7" class="ltx_td ltx_align_center">98.8%</td>
<td id="S4.T1.10.10.10.8" class="ltx_td ltx_align_center">93.8% <span id="S4.T1.10.10.10.8.1" class="ltx_text" style="color:#ED2B2A;">(-3.4%)</span>
</td>
<td id="S4.T1.10.10.10.9" class="ltx_td ltx_align_left">95.1% <span id="S4.T1.10.10.10.9.1" class="ltx_text" style="color:#ED2B2A;">(-3.7%)</span>
</td>
<td id="S4.T1.10.10.10.10" class="ltx_td ltx_align_left">94.3% <span id="S4.T1.10.10.10.10.1" class="ltx_text" style="color:#ED2B2A;">(-2.9%)</span>
</td>
<td id="S4.T1.10.10.10.11" class="ltx_td ltx_align_center">94.8% <span id="S4.T1.10.10.10.11.1" class="ltx_text" style="color:#ED2B2A;">(-4.0%)</span>
</td>
<td id="S4.T1.10.10.10.12" class="ltx_td ltx_align_left">97.3%</td>
<td id="S4.T1.10.10.10.13" class="ltx_td ltx_align_left">98.8%</td>
<td id="S4.T1.10.10.10.14" class="ltx_td ltx_align_right">93.5% <span id="S4.T1.10.10.10.14.1" class="ltx_text" style="color:#ED2B2A;">(-3.8%)</span>
</td>
<td id="S4.T1.10.10.10.15" class="ltx_td ltx_align_center">95.9% <span id="S4.T1.10.10.10.15.1" class="ltx_text" style="color:#ED2B2A;">(-2.9%)</span>
</td>
<td id="S4.T1.10.10.10.16" class="ltx_td ltx_align_left">94.0% <span id="S4.T1.10.10.10.16.1" class="ltx_text" style="color:#ED2B2A;">(-3.3%)</span>
</td>
<td id="S4.T1.10.10.10.17" class="ltx_td ltx_align_left">95.7% <span id="S4.T1.10.10.10.17.1" class="ltx_text" style="color:#ED2B2A;">(-3.1%)</span>
</td>
</tr>
<tr id="S4.T1.15.15.15" class="ltx_tr">
<td id="S4.T1.15.15.15.6" class="ltx_td ltx_align_right">Reddit emotion</td>
<td id="S4.T1.15.15.15.5" class="ltx_td ltx_align_center">
<math id="S4.T1.11.11.11.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.11.11.11.1.m1.1a"><mo id="S4.T1.11.11.11.1.m1.1.1" xref="S4.T1.11.11.11.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.1.m1.1b"><ci id="S4.T1.11.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.11.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.12.12.12.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.12.12.12.2.m2.1a"><mo id="S4.T1.12.12.12.2.m2.1.1" xref="S4.T1.12.12.12.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.2.m2.1b"><ci id="S4.T1.12.12.12.2.m2.1.1.cmml" xref="S4.T1.12.12.12.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.2.m2.1c">\star</annotation></semantics></math><math id="S4.T1.13.13.13.3.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.13.13.13.3.m3.1a"><mo id="S4.T1.13.13.13.3.m3.1.1" xref="S4.T1.13.13.13.3.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.13.3.m3.1b"><ci id="S4.T1.13.13.13.3.m3.1.1.cmml" xref="S4.T1.13.13.13.3.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.13.3.m3.1c">\star</annotation></semantics></math><math id="S4.T1.14.14.14.4.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.14.14.14.4.m4.1a"><mo id="S4.T1.14.14.14.4.m4.1.1" xref="S4.T1.14.14.14.4.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.14.4.m4.1b"><ci id="S4.T1.14.14.14.4.m4.1.1.cmml" xref="S4.T1.14.14.14.4.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.14.4.m4.1c">\star</annotation></semantics></math><math id="S4.T1.15.15.15.5.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.15.15.15.5.m5.1a"><mo id="S4.T1.15.15.15.5.m5.1.1" xref="S4.T1.15.15.15.5.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.15.5.m5.1b"><ci id="S4.T1.15.15.15.5.m5.1.1.cmml" xref="S4.T1.15.15.15.5.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.15.5.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.15.15.15.7" class="ltx_td ltx_align_right">93.7%</td>
<td id="S4.T1.15.15.15.8" class="ltx_td ltx_align_center">94.6%</td>
<td id="S4.T1.15.15.15.9" class="ltx_td ltx_align_center">72.7% <span id="S4.T1.15.15.15.9.1" class="ltx_text" style="color:#ED2B2A;">(-21.0%)</span>
</td>
<td id="S4.T1.15.15.15.10" class="ltx_td ltx_align_left">74.4% <span id="S4.T1.15.15.15.10.1" class="ltx_text" style="color:#ED2B2A;">(-20.2%)</span>
</td>
<td id="S4.T1.15.15.15.11" class="ltx_td ltx_align_left">81.9% <span id="S4.T1.15.15.15.11.1" class="ltx_text" style="color:#ED2B2A;">(-11.8%)</span>
</td>
<td id="S4.T1.15.15.15.12" class="ltx_td ltx_align_center">82.0% <span id="S4.T1.15.15.15.12.1" class="ltx_text" style="color:#ED2B2A;">(-12.6%)</span>
</td>
<td id="S4.T1.15.15.15.13" class="ltx_td ltx_align_left">91.3%</td>
<td id="S4.T1.15.15.15.14" class="ltx_td ltx_align_left">92.1%</td>
<td id="S4.T1.15.15.15.15" class="ltx_td ltx_align_right">77.9% <span id="S4.T1.15.15.15.15.1" class="ltx_text" style="color:#ED2B2A;">(-13.4%)</span>
</td>
<td id="S4.T1.15.15.15.16" class="ltx_td ltx_align_center">78.1% <span id="S4.T1.15.15.15.16.1" class="ltx_text" style="color:#ED2B2A;">(-14.0%)</span>
</td>
<td id="S4.T1.15.15.15.17" class="ltx_td ltx_align_left">87.5% <span id="S4.T1.15.15.15.17.1" class="ltx_text" style="color:#ED2B2A;">(-3.8%)</span>
</td>
<td id="S4.T1.15.15.15.18" class="ltx_td ltx_align_left">87.7% <span id="S4.T1.15.15.15.18.1" class="ltx_text" style="color:#ED2B2A;">(-4.4%)</span>
</td>
</tr>
<tr id="S4.T1.20.20.20" class="ltx_tr">
<td id="S4.T1.20.20.20.6" class="ltx_td ltx_align_right">Tweet irony</td>
<td id="S4.T1.20.20.20.5" class="ltx_td ltx_align_center">
<math id="S4.T1.16.16.16.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.16.16.16.1.m1.1a"><mo id="S4.T1.16.16.16.1.m1.1.1" xref="S4.T1.16.16.16.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.16.1.m1.1b"><ci id="S4.T1.16.16.16.1.m1.1.1.cmml" xref="S4.T1.16.16.16.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.16.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.17.17.17.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.17.17.17.2.m2.1a"><mo id="S4.T1.17.17.17.2.m2.1.1" xref="S4.T1.17.17.17.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.17.17.2.m2.1b"><ci id="S4.T1.17.17.17.2.m2.1.1.cmml" xref="S4.T1.17.17.17.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.17.17.2.m2.1c">\star</annotation></semantics></math><math id="S4.T1.18.18.18.3.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.18.18.18.3.m3.1a"><mo id="S4.T1.18.18.18.3.m3.1.1" xref="S4.T1.18.18.18.3.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.18.18.3.m3.1b"><ci id="S4.T1.18.18.18.3.m3.1.1.cmml" xref="S4.T1.18.18.18.3.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.18.18.3.m3.1c">\star</annotation></semantics></math><math id="S4.T1.19.19.19.4.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.19.19.19.4.m4.1a"><mo id="S4.T1.19.19.19.4.m4.1.1" xref="S4.T1.19.19.19.4.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.19.19.19.4.m4.1b"><ci id="S4.T1.19.19.19.4.m4.1.1.cmml" xref="S4.T1.19.19.19.4.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.19.19.4.m4.1c">\star</annotation></semantics></math><math id="S4.T1.20.20.20.5.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.20.20.20.5.m5.1a"><mo id="S4.T1.20.20.20.5.m5.1.1" xref="S4.T1.20.20.20.5.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.20.20.20.5.m5.1b"><ci id="S4.T1.20.20.20.5.m5.1.1.cmml" xref="S4.T1.20.20.20.5.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.20.20.5.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.20.20.20.7" class="ltx_td ltx_align_right">72.2%</td>
<td id="S4.T1.20.20.20.8" class="ltx_td ltx_align_center">73.9%</td>
<td id="S4.T1.20.20.20.9" class="ltx_td ltx_align_center">63.4% <span id="S4.T1.20.20.20.9.1" class="ltx_text" style="color:#ED2B2A;">(-8.8%)</span>
</td>
<td id="S4.T1.20.20.20.10" class="ltx_td ltx_align_left">63.6% <span id="S4.T1.20.20.20.10.1" class="ltx_text" style="color:#ED2B2A;">(-10.3%)</span>
</td>
<td id="S4.T1.20.20.20.11" class="ltx_td ltx_align_left">81.5% <span id="S4.T1.20.20.20.11.1" class="ltx_text" style="color:#03C988;">(+9.3%)</span>
</td>
<td id="S4.T1.20.20.20.12" class="ltx_td ltx_align_center">81.9% <span id="S4.T1.20.20.20.12.1" class="ltx_text" style="color:#03C988;">(+8.0%)</span>
</td>
<td id="S4.T1.20.20.20.13" class="ltx_td ltx_align_left">74.0%</td>
<td id="S4.T1.20.20.20.14" class="ltx_td ltx_align_left">75.5%</td>
<td id="S4.T1.20.20.20.15" class="ltx_td ltx_align_right">57.8% <span id="S4.T1.20.20.20.15.1" class="ltx_text" style="color:#ED2B2A;">(-16.2%)</span>
</td>
<td id="S4.T1.20.20.20.16" class="ltx_td ltx_align_center">59.1% <span id="S4.T1.20.20.20.16.1" class="ltx_text" style="color:#ED2B2A;">(-16.4%)</span>
</td>
<td id="S4.T1.20.20.20.17" class="ltx_td ltx_align_left">83.3% <span id="S4.T1.20.20.20.17.1" class="ltx_text" style="color:#03C988;">(+9.3%)</span>
</td>
<td id="S4.T1.20.20.20.18" class="ltx_td ltx_align_left">83.7% <span id="S4.T1.20.20.20.18.1" class="ltx_text" style="color:#03C988;">(+8.2%)</span>
</td>
</tr>
<tr id="S4.T1.25.25.25" class="ltx_tr">
<td id="S4.T1.25.25.25.6" class="ltx_td ltx_align_right">Tweet emotions</td>
<td id="S4.T1.25.25.25.5" class="ltx_td ltx_align_center">
<math id="S4.T1.21.21.21.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.21.21.21.1.m1.1a"><mo id="S4.T1.21.21.21.1.m1.1.1" xref="S4.T1.21.21.21.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.21.21.21.1.m1.1b"><ci id="S4.T1.21.21.21.1.m1.1.1.cmml" xref="S4.T1.21.21.21.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.21.21.21.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.22.22.22.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.22.22.22.2.m2.1a"><mo id="S4.T1.22.22.22.2.m2.1.1" xref="S4.T1.22.22.22.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.22.22.22.2.m2.1b"><ci id="S4.T1.22.22.22.2.m2.1.1.cmml" xref="S4.T1.22.22.22.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.22.22.22.2.m2.1c">\star</annotation></semantics></math><math id="S4.T1.23.23.23.3.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.23.23.23.3.m3.1a"><mo id="S4.T1.23.23.23.3.m3.1.1" xref="S4.T1.23.23.23.3.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.23.23.23.3.m3.1b"><ci id="S4.T1.23.23.23.3.m3.1.1.cmml" xref="S4.T1.23.23.23.3.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.23.23.23.3.m3.1c">\star</annotation></semantics></math><math id="S4.T1.24.24.24.4.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.24.24.24.4.m4.1a"><mo id="S4.T1.24.24.24.4.m4.1.1" xref="S4.T1.24.24.24.4.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.24.24.24.4.m4.1b"><ci id="S4.T1.24.24.24.4.m4.1.1.cmml" xref="S4.T1.24.24.24.4.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.24.24.24.4.m4.1c">\star</annotation></semantics></math><math id="S4.T1.25.25.25.5.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.25.25.25.5.m5.1a"><mo id="S4.T1.25.25.25.5.m5.1.1" xref="S4.T1.25.25.25.5.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.25.25.25.5.m5.1b"><ci id="S4.T1.25.25.25.5.m5.1.1.cmml" xref="S4.T1.25.25.25.5.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.25.25.25.5.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.25.25.25.7" class="ltx_td ltx_align_right">77.7%</td>
<td id="S4.T1.25.25.25.8" class="ltx_td ltx_align_center">81.1%</td>
<td id="S4.T1.25.25.25.9" class="ltx_td ltx_align_center">58.1% <span id="S4.T1.25.25.25.9.1" class="ltx_text" style="color:#ED2B2A;">(-19.6%)</span>
</td>
<td id="S4.T1.25.25.25.10" class="ltx_td ltx_align_left">64.5% <span id="S4.T1.25.25.25.10.1" class="ltx_text" style="color:#ED2B2A;">(-16.6%)</span>
</td>
<td id="S4.T1.25.25.25.11" class="ltx_td ltx_align_left">64.6% <span id="S4.T1.25.25.25.11.1" class="ltx_text" style="color:#ED2B2A;">(-13.1%)</span>
</td>
<td id="S4.T1.25.25.25.12" class="ltx_td ltx_align_center">69.1% <span id="S4.T1.25.25.25.12.1" class="ltx_text" style="color:#ED2B2A;">(-12.0%)</span>
</td>
<td id="S4.T1.25.25.25.13" class="ltx_td ltx_align_left">75.8%</td>
<td id="S4.T1.25.25.25.14" class="ltx_td ltx_align_left">78.9%</td>
<td id="S4.T1.25.25.25.15" class="ltx_td ltx_align_right">64.6% <span id="S4.T1.25.25.25.15.1" class="ltx_text" style="color:#ED2B2A;">(-11.2%)</span>
</td>
<td id="S4.T1.25.25.25.16" class="ltx_td ltx_align_center">71.5% <span id="S4.T1.25.25.25.16.1" class="ltx_text" style="color:#ED2B2A;">(-7.4%)</span>
</td>
<td id="S4.T1.25.25.25.17" class="ltx_td ltx_align_left">66.3% <span id="S4.T1.25.25.25.17.1" class="ltx_text" style="color:#ED2B2A;">(-9.5%)</span>
</td>
<td id="S4.T1.25.25.25.18" class="ltx_td ltx_align_left">72.7% <span id="S4.T1.25.25.25.18.1" class="ltx_text" style="color:#ED2B2A;">(-6.2%)</span>
</td>
</tr>
<tr id="S4.T1.30.30.30" class="ltx_tr">
<td id="S4.T1.30.30.30.6" class="ltx_td ltx_align_right">Sarcasm</td>
<td id="S4.T1.30.30.30.5" class="ltx_td ltx_align_center">
<math id="S4.T1.26.26.26.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.26.26.26.1.m1.1a"><mo id="S4.T1.26.26.26.1.m1.1.1" xref="S4.T1.26.26.26.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.26.26.26.1.m1.1b"><ci id="S4.T1.26.26.26.1.m1.1.1.cmml" xref="S4.T1.26.26.26.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.26.26.26.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.27.27.27.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.27.27.27.2.m2.1a"><mo id="S4.T1.27.27.27.2.m2.1.1" xref="S4.T1.27.27.27.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.27.27.27.2.m2.1b"><ci id="S4.T1.27.27.27.2.m2.1.1.cmml" xref="S4.T1.27.27.27.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.27.27.27.2.m2.1c">\star</annotation></semantics></math><math id="S4.T1.28.28.28.3.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.28.28.28.3.m3.1a"><mo id="S4.T1.28.28.28.3.m3.1.1" xref="S4.T1.28.28.28.3.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.28.28.28.3.m3.1b"><ci id="S4.T1.28.28.28.3.m3.1.1.cmml" xref="S4.T1.28.28.28.3.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.28.28.28.3.m3.1c">\star</annotation></semantics></math><math id="S4.T1.29.29.29.4.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.29.29.29.4.m4.1a"><mo id="S4.T1.29.29.29.4.m4.1.1" xref="S4.T1.29.29.29.4.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.29.29.29.4.m4.1b"><ci id="S4.T1.29.29.29.4.m4.1.1.cmml" xref="S4.T1.29.29.29.4.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.29.29.29.4.m4.1c">\star</annotation></semantics></math><math id="S4.T1.30.30.30.5.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.30.30.30.5.m5.1a"><mo id="S4.T1.30.30.30.5.m5.1.1" xref="S4.T1.30.30.30.5.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.30.30.30.5.m5.1b"><ci id="S4.T1.30.30.30.5.m5.1.1.cmml" xref="S4.T1.30.30.30.5.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.30.30.30.5.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.30.30.30.7" class="ltx_td ltx_align_right">89.9%</td>
<td id="S4.T1.30.30.30.8" class="ltx_td ltx_align_center">90.3%</td>
<td id="S4.T1.30.30.30.9" class="ltx_td ltx_align_center">51.1% <span id="S4.T1.30.30.30.9.1" class="ltx_text" style="color:#ED2B2A;">(-38.8%)</span>
</td>
<td id="S4.T1.30.30.30.10" class="ltx_td ltx_align_left">51.2% <span id="S4.T1.30.30.30.10.1" class="ltx_text" style="color:#ED2B2A;">(-39.1%)</span>
</td>
<td id="S4.T1.30.30.30.11" class="ltx_td ltx_align_left">63.6% <span id="S4.T1.30.30.30.11.1" class="ltx_text" style="color:#ED2B2A;">(-26.3%)</span>
</td>
<td id="S4.T1.30.30.30.12" class="ltx_td ltx_align_center">64.8% <span id="S4.T1.30.30.30.12.1" class="ltx_text" style="color:#ED2B2A;">(-25.5%)</span>
</td>
<td id="S4.T1.30.30.30.13" class="ltx_td ltx_align_left">91.8%</td>
<td id="S4.T1.30.30.30.14" class="ltx_td ltx_align_left">92.0%</td>
<td id="S4.T1.30.30.30.15" class="ltx_td ltx_align_right">54.3% <span id="S4.T1.30.30.30.15.1" class="ltx_text" style="color:#ED2B2A;">(-37.5%)</span>
</td>
<td id="S4.T1.30.30.30.16" class="ltx_td ltx_align_center">54.3% <span id="S4.T1.30.30.30.16.1" class="ltx_text" style="color:#ED2B2A;">(-37.7%)</span>
</td>
<td id="S4.T1.30.30.30.17" class="ltx_td ltx_align_left">61.5% <span id="S4.T1.30.30.30.17.1" class="ltx_text" style="color:#ED2B2A;">(-30.3%)</span>
</td>
<td id="S4.T1.30.30.30.18" class="ltx_td ltx_align_left">63.6% <span id="S4.T1.30.30.30.18.1" class="ltx_text" style="color:#ED2B2A;">(-28.4%)</span>
</td>
</tr>
<tr id="S4.T1.35.35.35" class="ltx_tr">
<td id="S4.T1.35.35.35.6" class="ltx_td ltx_align_right">Financial</td>
<td id="S4.T1.35.35.35.5" class="ltx_td ltx_align_center">
<math id="S4.T1.31.31.31.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.31.31.31.1.m1.1a"><mo id="S4.T1.31.31.31.1.m1.1.1" xref="S4.T1.31.31.31.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.31.31.31.1.m1.1b"><ci id="S4.T1.31.31.31.1.m1.1.1.cmml" xref="S4.T1.31.31.31.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.31.31.31.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.32.32.32.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.32.32.32.2.m2.1a"><mo id="S4.T1.32.32.32.2.m2.1.1" xref="S4.T1.32.32.32.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.32.32.32.2.m2.1b"><ci id="S4.T1.32.32.32.2.m2.1.1.cmml" xref="S4.T1.32.32.32.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.32.32.32.2.m2.1c">\star</annotation></semantics></math><math id="S4.T1.33.33.33.3.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.33.33.33.3.m3.1a"><mo id="S4.T1.33.33.33.3.m3.1.1" xref="S4.T1.33.33.33.3.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.33.33.33.3.m3.1b"><ci id="S4.T1.33.33.33.3.m3.1.1.cmml" xref="S4.T1.33.33.33.3.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.33.33.33.3.m3.1c">\star</annotation></semantics></math><math id="S4.T1.34.34.34.4.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.34.34.34.4.m4.1a"><mo id="S4.T1.34.34.34.4.m4.1.1" xref="S4.T1.34.34.34.4.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.34.34.34.4.m4.1b"><ci id="S4.T1.34.34.34.4.m4.1.1.cmml" xref="S4.T1.34.34.34.4.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.34.34.34.4.m4.1c">\star</annotation></semantics></math><math id="S4.T1.35.35.35.5.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.35.35.35.5.m5.1a"><mo id="S4.T1.35.35.35.5.m5.1.1" xref="S4.T1.35.35.35.5.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.35.35.35.5.m5.1b"><ci id="S4.T1.35.35.35.5.m5.1.1.cmml" xref="S4.T1.35.35.35.5.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.35.35.35.5.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.35.35.35.7" class="ltx_td ltx_align_right">83.2%</td>
<td id="S4.T1.35.35.35.8" class="ltx_td ltx_align_center">84.6%</td>
<td id="S4.T1.35.35.35.9" class="ltx_td ltx_align_center">48.2% <span id="S4.T1.35.35.35.9.1" class="ltx_text" style="color:#ED2B2A;">(-35.0%)</span>
</td>
<td id="S4.T1.35.35.35.10" class="ltx_td ltx_align_left">60.7% <span id="S4.T1.35.35.35.10.1" class="ltx_text" style="color:#ED2B2A;">(-23.9%)</span>
</td>
<td id="S4.T1.35.35.35.11" class="ltx_td ltx_align_left">70.6% <span id="S4.T1.35.35.35.11.1" class="ltx_text" style="color:#ED2B2A;">(-12.6%)</span>
</td>
<td id="S4.T1.35.35.35.12" class="ltx_td ltx_align_center">74.2% <span id="S4.T1.35.35.35.12.1" class="ltx_text" style="color:#ED2B2A;">(-10.4%)</span>
</td>
<td id="S4.T1.35.35.35.13" class="ltx_td ltx_align_left">85.0%</td>
<td id="S4.T1.35.35.35.14" class="ltx_td ltx_align_left">86.6%</td>
<td id="S4.T1.35.35.35.15" class="ltx_td ltx_align_right">58.5% <span id="S4.T1.35.35.35.15.1" class="ltx_text" style="color:#ED2B2A;">(-26.5%)</span>
</td>
<td id="S4.T1.35.35.35.16" class="ltx_td ltx_align_center">70.3% <span id="S4.T1.35.35.35.16.1" class="ltx_text" style="color:#ED2B2A;">(-16.3%)</span>
</td>
<td id="S4.T1.35.35.35.17" class="ltx_td ltx_align_left">75.0% <span id="S4.T1.35.35.35.17.1" class="ltx_text" style="color:#ED2B2A;">(-10.0%)</span>
</td>
<td id="S4.T1.35.35.35.18" class="ltx_td ltx_align_left">78.9% <span id="S4.T1.35.35.35.18.1" class="ltx_text" style="color:#ED2B2A;">(-7.7%)</span>
</td>
</tr>
<tr id="S4.T1.40.40.40" class="ltx_tr">
<td id="S4.T1.40.40.40.6" class="ltx_td ltx_align_right">Humor speech</td>
<td id="S4.T1.40.40.40.5" class="ltx_td ltx_align_center">
<math id="S4.T1.36.36.36.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.36.36.36.1.m1.1a"><mo id="S4.T1.36.36.36.1.m1.1.1" xref="S4.T1.36.36.36.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.36.36.36.1.m1.1b"><ci id="S4.T1.36.36.36.1.m1.1.1.cmml" xref="S4.T1.36.36.36.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.36.36.36.1.m1.1c">\star</annotation></semantics></math><math id="S4.T1.37.37.37.2.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.37.37.37.2.m2.1a"><mo id="S4.T1.37.37.37.2.m2.1.1" xref="S4.T1.37.37.37.2.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.37.37.37.2.m2.1b"><ci id="S4.T1.37.37.37.2.m2.1.1.cmml" xref="S4.T1.37.37.37.2.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.37.37.37.2.m2.1c">\star</annotation></semantics></math><math id="S4.T1.38.38.38.3.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.38.38.38.3.m3.1a"><mo id="S4.T1.38.38.38.3.m3.1.1" xref="S4.T1.38.38.38.3.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.38.38.38.3.m3.1b"><ci id="S4.T1.38.38.38.3.m3.1.1.cmml" xref="S4.T1.38.38.38.3.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.38.38.38.3.m3.1c">\star</annotation></semantics></math><math id="S4.T1.39.39.39.4.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.39.39.39.4.m4.1a"><mo id="S4.T1.39.39.39.4.m4.1.1" xref="S4.T1.39.39.39.4.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.39.39.39.4.m4.1b"><ci id="S4.T1.39.39.39.4.m4.1.1.cmml" xref="S4.T1.39.39.39.4.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.39.39.39.4.m4.1c">\star</annotation></semantics></math><math id="S4.T1.40.40.40.5.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.40.40.40.5.m5.1a"><mo id="S4.T1.40.40.40.5.m5.1.1" xref="S4.T1.40.40.40.5.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.40.40.40.5.m5.1b"><ci id="S4.T1.40.40.40.5.m5.1.1.cmml" xref="S4.T1.40.40.40.5.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.40.40.40.5.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S4.T1.40.40.40.7" class="ltx_td ltx_align_right">97.0%</td>
<td id="S4.T1.40.40.40.8" class="ltx_td ltx_align_center">97.0%</td>
<td id="S4.T1.40.40.40.9" class="ltx_td ltx_align_center">56.0% <span id="S4.T1.40.40.40.9.1" class="ltx_text" style="color:#ED2B2A;">(-41.0%)</span>
</td>
<td id="S4.T1.40.40.40.10" class="ltx_td ltx_align_left">61.7% <span id="S4.T1.40.40.40.10.1" class="ltx_text" style="color:#ED2B2A;">(-35.3%)</span>
</td>
<td id="S4.T1.40.40.40.11" class="ltx_td ltx_align_left">86.9% <span id="S4.T1.40.40.40.11.1" class="ltx_text" style="color:#ED2B2A;">(-10.1%)</span>
</td>
<td id="S4.T1.40.40.40.12" class="ltx_td ltx_align_center">87.0% <span id="S4.T1.40.40.40.12.1" class="ltx_text" style="color:#ED2B2A;">(-10.0%)</span>
</td>
<td id="S4.T1.40.40.40.13" class="ltx_td ltx_align_left">96.7%</td>
<td id="S4.T1.40.40.40.14" class="ltx_td ltx_align_left">96.7%</td>
<td id="S4.T1.40.40.40.15" class="ltx_td ltx_align_right">54.9% <span id="S4.T1.40.40.40.15.1" class="ltx_text" style="color:#ED2B2A;">(-41.8%)</span>
</td>
<td id="S4.T1.40.40.40.16" class="ltx_td ltx_align_center">60.9% <span id="S4.T1.40.40.40.16.1" class="ltx_text" style="color:#ED2B2A;">(-35.8%)</span>
</td>
<td id="S4.T1.40.40.40.17" class="ltx_td ltx_align_left">84.0% <span id="S4.T1.40.40.40.17.1" class="ltx_text" style="color:#ED2B2A;">(-12.7%)</span>
</td>
<td id="S4.T1.40.40.40.18" class="ltx_td ltx_align_left">84.0% <span id="S4.T1.40.40.40.18.1" class="ltx_text" style="color:#ED2B2A;">(-12.7%)</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparing the performance of classification models trained on the LLM-generated synthetic data under the zero-shot or few-shot settings, with those trained with the original real-world data, in terms of Macro-F1 (%) and Accuracy Score (%). In the “Subjectivity” column, more "<math id="S4.T1.42.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S4.T1.42.m1.1b"><mo id="S4.T1.42.m1.1.1" xref="S4.T1.42.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S4.T1.42.m1.1c"><ci id="S4.T1.42.m1.1.1.cmml" xref="S4.T1.42.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.42.m1.1d">\star</annotation></semantics></math>" symbols indicate a higher level of task subjectivity. </figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation Results</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Model Training ‣ 4 Evaluation I: Comparison Across Different Types of Tasks ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the comparative performance of classification models trained with different data. Below, we highlight a few key observations we get from this comparison.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.4" class="ltx_p"><span id="S4.SS4.p2.4.1" class="ltx_text ltx_font_bold">Models trained on the real-world data consistently outperform those trained on the synthetic data.</span> Our results indicate that models trained on the original real-world data consistently outperform their counterparts trained on the synthetic data generated under either zero-shot or few-shot settings, almost for every task. In particular, with the RoBERTa model, we observe that the average improvements of the model trained on the real-world data
over
the models trained on zero-shot synthetic data and few-shot synthetic data
are <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="16.9\%" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mn id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">16.9</mn><mo id="S4.SS4.p2.1.m1.1.1.1" xref="S4.SS4.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">16.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">16.9\%</annotation></semantics></math> and <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="6.7\%" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mrow id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mn id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">6.7</mn><mo id="S4.SS4.p2.2.m2.1.1.1" xref="S4.SS4.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">6.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">6.7\%</annotation></semantics></math> in terms of Macro-F1, and <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="14.9\%" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><mrow id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mn id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">14.9</mn><mo id="S4.SS4.p2.3.m3.1.1.1" xref="S4.SS4.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">14.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">14.9\%</annotation></semantics></math> and <math id="S4.SS4.p2.4.m4.1" class="ltx_Math" alttext="6.1\%" display="inline"><semantics id="S4.SS4.p2.4.m4.1a"><mrow id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml"><mn id="S4.SS4.p2.4.m4.1.1.2" xref="S4.SS4.p2.4.m4.1.1.2.cmml">6.1</mn><mo id="S4.SS4.p2.4.m4.1.1.1" xref="S4.SS4.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><apply id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS4.p2.4.m4.1.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.4.m4.1.1.2.cmml" xref="S4.SS4.p2.4.m4.1.1.2">6.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">6.1\%</annotation></semantics></math> in terms of accuracy.
Similar trends are observed with the BERT model as well.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.4" class="ltx_p"><span id="S4.SS4.p3.4.1" class="ltx_text ltx_font_bold">Guiding LLM with real-world data examples can boost the effectiveness of the synthetic data.</span>
We also observe that models trained on those synthetic data generated under the few-shot settings almost always outperform those trained on the synthetic data generated under the zero-shot settings. For instance, for the BERT model, we see an average increase of <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="10.6\%" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mrow id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mn id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">10.6</mn><mo id="S4.SS4.p3.1.m1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">10.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">10.6\%</annotation></semantics></math> and <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="8.8\%" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><mrow id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mn id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">8.8</mn><mo id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">8.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">8.8\%</annotation></semantics></math> in Macro-F1 and accuracy scores, respectively, across the 10 tasks in the few-shot setting, as compared to the zero-shot setting. Similarly, with the RoBERTa model, there is an average increase of <math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="10.3\%" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><mrow id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mn id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">10.3</mn><mo id="S4.SS4.p3.3.m3.1.1.1" xref="S4.SS4.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">10.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">10.3\%</annotation></semantics></math> in Macro-F1 and <math id="S4.SS4.p3.4.m4.1" class="ltx_Math" alttext="8.9\%" display="inline"><semantics id="S4.SS4.p3.4.m4.1a"><mrow id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml"><mn id="S4.SS4.p3.4.m4.1.1.2" xref="S4.SS4.p3.4.m4.1.1.2.cmml">8.9</mn><mo id="S4.SS4.p3.4.m4.1.1.1" xref="S4.SS4.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><apply id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS4.p3.4.m4.1.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p3.4.m4.1.1.2.cmml" xref="S4.SS4.p3.4.m4.1.1.2">8.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">8.9\%</annotation></semantics></math> in accuracy scores across the 10 tasks when the real-world data are used as examples for LLM to mimic in the synthetic data generation process. For more analysis of the few-shot synthetic data, please see App. <a href="#A2.SS2" title="B.2 Potential of Few-shot Synthetic Data for Data Augmentation ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a> and <a href="#A2.SS3" title="B.3 Similarity between the Synthetic Data and the Real Data ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para ltx_noindent">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">Synthetic data support more effective model training for tasks that are less subjective.</span> Finally, we notice that for classification tasks with relatively low levels of subjectivity (e.g., those in the AG’s news, Relation classification, IMDB reviews, and SMS spam datasets), the performance difference between models trained on the synthetic data and those trained on the real-world data is remarkably small.
However, for tasks with high subjectivity, the performance decrease resulted from the usage of the synthetic data is more significant—for instance, across the cluster of 6 tasks with the highest level of subjectivity in our evaluation, there is an average decrease of 27.4% and 24.2% in Macro-F1 and accuracy, respectively, comparing the BERT models trained on the zero-shot synthetic data with those trained on the real-world data. In other words, for text classification tasks that are highly objective, there is great potential in training high-performing models simply based on synthetic data generated by LLMs, but the same method falls short in generating synthetic data that can effectively support model training for highly subjective classifications.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Exploratory Analysis: Data Diversity</h3>

<figure id="S4.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/remote_clique.png" id="S4.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="144" height="111" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Remote Clique</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/chamer_distance.png" id="S4.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="144" height="111" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Chamfer Distance</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparing the diversity of the real-world data and the synthetic data.</figcaption>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">To explore the potential reasons underlying the model performance difference, we conducted an exploratory analysis on the diversity of the training data. Following <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib40" title="" class="ltx_ref">Rhys Cox et al.</a></cite> <cite class="ltx_cite ltx_citemacro_cite">Rhys Cox et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>, we used the <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_italic">Remote Clique Score</span> (i.e., the average mean distance of a data instance to other instances) and the <span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_italic">Chamfer Distance Score</span> (i.e., the average minimum distance of a data instance to other instances) to quantify the diversity of a set of data.
For both metrics, higher values indicate greater data diversity.
As shown in Figure <a href="#S4.F1" title="Figure 1 ‣ 4.5 Exploratory Analysis: Data Diversity ‣ 4 Evaluation I: Comparison Across Different Types of Tasks ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we find that in general, the real-world data appear to be more diverse than the synthetic data generated under the few-shot settings, which in turn seem to be more diverse than the zero-shot synthetic data. This might partially explain why models trained on the real-world data and the few-shot synthetic data tend to outperform those trained on the zero-shot synthetic data.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">In addition, we also notice that compared to that on the low subjectivity tasks (i.e., AG, Relation, IMDB, Spam), the differences in data diversity between the real-world data and the synthetic data seem to be more salient on the high subjectivity tasks (i.e., the other 6 tasks), especially in terms of the Chamfer Distance Score. In fact, a t-test shows that the decrease of the Chamfer Distance Score in the zero-shot synthetic data compared to the real data is significantly larger for the high subjectivity tasks than for the low subjectivity tasks (<math id="S4.SS5.p2.1.m1.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S4.SS5.p2.1.m1.1a"><mrow id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml"><mi id="S4.SS5.p2.1.m1.1.1.2" xref="S4.SS5.p2.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS5.p2.1.m1.1.1.1" xref="S4.SS5.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS5.p2.1.m1.1.1.3" xref="S4.SS5.p2.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><apply id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1"><lt id="S4.SS5.p2.1.m1.1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1.1"></lt><ci id="S4.SS5.p2.1.m1.1.1.2.cmml" xref="S4.SS5.p2.1.m1.1.1.2">𝑝</ci><cn type="float" id="S4.SS5.p2.1.m1.1.1.3.cmml" xref="S4.SS5.p2.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">p&lt;0.01</annotation></semantics></math>).
This suggests that for tasks with high subjectivity, such as interpreting humor or sarcasm in language, LLMs may not be able to
generate data instances that can cover the full spectrum of real-life scenarios, which may limit the performance of models trained on the synthetic data.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation II: Comparison Across Different Task Instances</h2>

<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.42" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:44.6pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-137.9pt,14.0pt) scale(0.611281325438238,0.611281325438238) ;">
<table id="S5.T2.42.42" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.42.42.43.1" class="ltx_tr">
<td id="S5.T2.42.42.43.1.1" class="ltx_td ltx_align_center"><span id="S5.T2.42.42.43.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T2.42.42.43.1.2" class="ltx_td ltx_align_left"><span id="S5.T2.42.42.43.1.2.1" class="ltx_text ltx_font_bold">AG</span></td>
<td id="S5.T2.42.42.43.1.3" class="ltx_td ltx_align_left"><span id="S5.T2.42.42.43.1.3.1" class="ltx_text ltx_font_bold">Relation</span></td>
<td id="S5.T2.42.42.43.1.4" class="ltx_td ltx_align_center"><span id="S5.T2.42.42.43.1.4.1" class="ltx_text ltx_font_bold">IMDB</span></td>
<td id="S5.T2.42.42.43.1.5" class="ltx_td ltx_align_left"><span id="S5.T2.42.42.43.1.5.1" class="ltx_text ltx_font_bold">SMS Spam</span></td>
<td id="S5.T2.42.42.43.1.6" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.42.42.43.1.6.1" class="ltx_text ltx_font_bold">Reddit Emotion</span></td>
<td id="S5.T2.42.42.43.1.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.42.42.43.1.7.1" class="ltx_text ltx_font_bold">Humor Speech</span></td>
<td id="S5.T2.42.42.43.1.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.42.42.43.1.8.1" class="ltx_text ltx_font_bold">Tweet Irony</span></td>
<td id="S5.T2.42.42.43.1.9" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.42.42.43.1.9.1" class="ltx_text ltx_font_bold">Sarcasm</span></td>
<td id="S5.T2.42.42.43.1.10" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.42.42.43.1.10.1" class="ltx_text ltx_font_bold">Tweet Emotions</span></td>
<td id="S5.T2.42.42.43.1.11" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.42.42.43.1.11.1" class="ltx_text ltx_font_bold">Finanical</span></td>
</tr>
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_center">Average Agreement <math id="S5.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\overline{a}" display="inline"><semantics id="S5.T2.1.1.1.1.m1.1a"><mover accent="true" id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml"><mi id="S5.T2.1.1.1.1.m1.1.1.2" xref="S5.T2.1.1.1.1.m1.1.1.2.cmml">a</mi><mo id="S5.T2.1.1.1.1.m1.1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1"><ci id="S5.T2.1.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1.1">¯</ci><ci id="S5.T2.1.1.1.1.m1.1.1.2.cmml" xref="S5.T2.1.1.1.1.m1.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\overline{a}</annotation></semantics></math>
</td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_left">0.80 (4.2)</td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_left">0.78 (4.5)</td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center">0.76 (7.3)</td>
<td id="S5.T2.1.1.1.5" class="ltx_td ltx_align_left">0.73 (8.5)</td>
<td id="S5.T2.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center">0.69 (6.6)</td>
<td id="S5.T2.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center">0.68 (7.1)</td>
<td id="S5.T2.1.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center">0.68 (6.7)</td>
<td id="S5.T2.1.1.1.9" class="ltx_td ltx_nopad_r ltx_align_center">0.64 (7.7)</td>
<td id="S5.T2.1.1.1.10" class="ltx_td ltx_nopad_r ltx_align_center">0.64 (4.6)</td>
<td id="S5.T2.1.1.1.11" class="ltx_td ltx_nopad_r ltx_align_center">0.57 (7.6)</td>
</tr>
<tr id="S5.T2.2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.2.1" class="ltx_td ltx_align_center">Krippendorff’s <math id="S5.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.T2.2.2.2.1.m1.1a"><mi id="S5.T2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.m1.1c">\alpha</annotation></semantics></math>
</td>
<td id="S5.T2.2.2.2.2" class="ltx_td ltx_align_left">0.51</td>
<td id="S5.T2.2.2.2.3" class="ltx_td ltx_align_left">0.43</td>
<td id="S5.T2.2.2.2.4" class="ltx_td ltx_align_center">0.19</td>
<td id="S5.T2.2.2.2.5" class="ltx_td ltx_align_left">0.27</td>
<td id="S5.T2.2.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center">0.30</td>
<td id="S5.T2.2.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center">0.06</td>
<td id="S5.T2.2.2.2.8" class="ltx_td ltx_nopad_r ltx_align_center">0.03</td>
<td id="S5.T2.2.2.2.9" class="ltx_td ltx_nopad_r ltx_align_center">0.01</td>
<td id="S5.T2.2.2.2.10" class="ltx_td ltx_nopad_r ltx_align_center">0.17</td>
<td id="S5.T2.2.2.2.11" class="ltx_td ltx_nopad_r ltx_align_center">-0.03</td>
</tr>
<tr id="S5.T2.42.42.42" class="ltx_tr">
<td id="S5.T2.42.42.42.41" class="ltx_td ltx_align_center">Subjectivity Level</td>
<td id="S5.T2.3.3.3.1" class="ltx_td ltx_align_left"><math id="S5.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.3.3.3.1.m1.1a"><mo id="S5.T2.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.1.m1.1b"><ci id="S5.T2.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.1.m1.1c">\star</annotation></semantics></math></td>
<td id="S5.T2.5.5.5.3" class="ltx_td ltx_align_left">
<math id="S5.T2.4.4.4.2.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.4.4.4.2.m1.1a"><mo id="S5.T2.4.4.4.2.m1.1.1" xref="S5.T2.4.4.4.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.2.m1.1b"><ci id="S5.T2.4.4.4.2.m1.1.1.cmml" xref="S5.T2.4.4.4.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.2.m1.1c">\star</annotation></semantics></math><math id="S5.T2.5.5.5.3.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.5.5.5.3.m2.1a"><mo id="S5.T2.5.5.5.3.m2.1.1" xref="S5.T2.5.5.5.3.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.3.m2.1b"><ci id="S5.T2.5.5.5.3.m2.1.1.cmml" xref="S5.T2.5.5.5.3.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.3.m2.1c">\star</annotation></semantics></math>
</td>
<td id="S5.T2.8.8.8.6" class="ltx_td ltx_align_center">
<math id="S5.T2.6.6.6.4.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.6.6.6.4.m1.1a"><mo id="S5.T2.6.6.6.4.m1.1.1" xref="S5.T2.6.6.6.4.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.4.m1.1b"><ci id="S5.T2.6.6.6.4.m1.1.1.cmml" xref="S5.T2.6.6.6.4.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.4.m1.1c">\star</annotation></semantics></math><math id="S5.T2.7.7.7.5.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.7.7.7.5.m2.1a"><mo id="S5.T2.7.7.7.5.m2.1.1" xref="S5.T2.7.7.7.5.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.5.m2.1b"><ci id="S5.T2.7.7.7.5.m2.1.1.cmml" xref="S5.T2.7.7.7.5.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.5.m2.1c">\star</annotation></semantics></math><math id="S5.T2.8.8.8.6.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.8.8.8.6.m3.1a"><mo id="S5.T2.8.8.8.6.m3.1.1" xref="S5.T2.8.8.8.6.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.8.6.m3.1b"><ci id="S5.T2.8.8.8.6.m3.1.1.cmml" xref="S5.T2.8.8.8.6.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.8.6.m3.1c">\star</annotation></semantics></math>
</td>
<td id="S5.T2.12.12.12.10" class="ltx_td ltx_align_left">
<math id="S5.T2.9.9.9.7.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.9.9.9.7.m1.1a"><mo id="S5.T2.9.9.9.7.m1.1.1" xref="S5.T2.9.9.9.7.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.9.9.9.7.m1.1b"><ci id="S5.T2.9.9.9.7.m1.1.1.cmml" xref="S5.T2.9.9.9.7.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.9.9.7.m1.1c">\star</annotation></semantics></math><math id="S5.T2.10.10.10.8.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.10.10.10.8.m2.1a"><mo id="S5.T2.10.10.10.8.m2.1.1" xref="S5.T2.10.10.10.8.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.10.10.10.8.m2.1b"><ci id="S5.T2.10.10.10.8.m2.1.1.cmml" xref="S5.T2.10.10.10.8.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.10.10.8.m2.1c">\star</annotation></semantics></math><math id="S5.T2.11.11.11.9.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.11.11.11.9.m3.1a"><mo id="S5.T2.11.11.11.9.m3.1.1" xref="S5.T2.11.11.11.9.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.11.11.11.9.m3.1b"><ci id="S5.T2.11.11.11.9.m3.1.1.cmml" xref="S5.T2.11.11.11.9.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.11.11.11.9.m3.1c">\star</annotation></semantics></math><math id="S5.T2.12.12.12.10.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.12.12.12.10.m4.1a"><mo id="S5.T2.12.12.12.10.m4.1.1" xref="S5.T2.12.12.12.10.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.12.12.12.10.m4.1b"><ci id="S5.T2.12.12.12.10.m4.1.1.cmml" xref="S5.T2.12.12.12.10.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.12.12.12.10.m4.1c">\star</annotation></semantics></math>
</td>
<td id="S5.T2.17.17.17.15" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S5.T2.13.13.13.11.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.13.13.13.11.m1.1a"><mo id="S5.T2.13.13.13.11.m1.1.1" xref="S5.T2.13.13.13.11.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.13.13.13.11.m1.1b"><ci id="S5.T2.13.13.13.11.m1.1.1.cmml" xref="S5.T2.13.13.13.11.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.13.13.13.11.m1.1c">\star</annotation></semantics></math><math id="S5.T2.14.14.14.12.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.14.14.14.12.m2.1a"><mo id="S5.T2.14.14.14.12.m2.1.1" xref="S5.T2.14.14.14.12.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.14.14.14.12.m2.1b"><ci id="S5.T2.14.14.14.12.m2.1.1.cmml" xref="S5.T2.14.14.14.12.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.14.14.14.12.m2.1c">\star</annotation></semantics></math><math id="S5.T2.15.15.15.13.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.15.15.15.13.m3.1a"><mo id="S5.T2.15.15.15.13.m3.1.1" xref="S5.T2.15.15.15.13.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.15.15.15.13.m3.1b"><ci id="S5.T2.15.15.15.13.m3.1.1.cmml" xref="S5.T2.15.15.15.13.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.15.15.15.13.m3.1c">\star</annotation></semantics></math><math id="S5.T2.16.16.16.14.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.16.16.16.14.m4.1a"><mo id="S5.T2.16.16.16.14.m4.1.1" xref="S5.T2.16.16.16.14.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.16.16.16.14.m4.1b"><ci id="S5.T2.16.16.16.14.m4.1.1.cmml" xref="S5.T2.16.16.16.14.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.16.16.16.14.m4.1c">\star</annotation></semantics></math><math id="S5.T2.17.17.17.15.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.17.17.17.15.m5.1a"><mo id="S5.T2.17.17.17.15.m5.1.1" xref="S5.T2.17.17.17.15.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.17.17.17.15.m5.1b"><ci id="S5.T2.17.17.17.15.m5.1.1.cmml" xref="S5.T2.17.17.17.15.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.17.17.17.15.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S5.T2.22.22.22.20" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S5.T2.18.18.18.16.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.18.18.18.16.m1.1a"><mo id="S5.T2.18.18.18.16.m1.1.1" xref="S5.T2.18.18.18.16.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.18.18.18.16.m1.1b"><ci id="S5.T2.18.18.18.16.m1.1.1.cmml" xref="S5.T2.18.18.18.16.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.18.18.18.16.m1.1c">\star</annotation></semantics></math><math id="S5.T2.19.19.19.17.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.19.19.19.17.m2.1a"><mo id="S5.T2.19.19.19.17.m2.1.1" xref="S5.T2.19.19.19.17.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.19.19.19.17.m2.1b"><ci id="S5.T2.19.19.19.17.m2.1.1.cmml" xref="S5.T2.19.19.19.17.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.19.19.19.17.m2.1c">\star</annotation></semantics></math><math id="S5.T2.20.20.20.18.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.20.20.20.18.m3.1a"><mo id="S5.T2.20.20.20.18.m3.1.1" xref="S5.T2.20.20.20.18.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.20.20.20.18.m3.1b"><ci id="S5.T2.20.20.20.18.m3.1.1.cmml" xref="S5.T2.20.20.20.18.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.20.20.20.18.m3.1c">\star</annotation></semantics></math><math id="S5.T2.21.21.21.19.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.21.21.21.19.m4.1a"><mo id="S5.T2.21.21.21.19.m4.1.1" xref="S5.T2.21.21.21.19.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.21.21.21.19.m4.1b"><ci id="S5.T2.21.21.21.19.m4.1.1.cmml" xref="S5.T2.21.21.21.19.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.21.21.21.19.m4.1c">\star</annotation></semantics></math><math id="S5.T2.22.22.22.20.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.22.22.22.20.m5.1a"><mo id="S5.T2.22.22.22.20.m5.1.1" xref="S5.T2.22.22.22.20.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.22.22.22.20.m5.1b"><ci id="S5.T2.22.22.22.20.m5.1.1.cmml" xref="S5.T2.22.22.22.20.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.22.22.22.20.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S5.T2.27.27.27.25" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S5.T2.23.23.23.21.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.23.23.23.21.m1.1a"><mo id="S5.T2.23.23.23.21.m1.1.1" xref="S5.T2.23.23.23.21.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.23.23.23.21.m1.1b"><ci id="S5.T2.23.23.23.21.m1.1.1.cmml" xref="S5.T2.23.23.23.21.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.23.23.23.21.m1.1c">\star</annotation></semantics></math><math id="S5.T2.24.24.24.22.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.24.24.24.22.m2.1a"><mo id="S5.T2.24.24.24.22.m2.1.1" xref="S5.T2.24.24.24.22.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.24.24.24.22.m2.1b"><ci id="S5.T2.24.24.24.22.m2.1.1.cmml" xref="S5.T2.24.24.24.22.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.24.24.24.22.m2.1c">\star</annotation></semantics></math><math id="S5.T2.25.25.25.23.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.25.25.25.23.m3.1a"><mo id="S5.T2.25.25.25.23.m3.1.1" xref="S5.T2.25.25.25.23.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.25.25.25.23.m3.1b"><ci id="S5.T2.25.25.25.23.m3.1.1.cmml" xref="S5.T2.25.25.25.23.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.25.25.25.23.m3.1c">\star</annotation></semantics></math><math id="S5.T2.26.26.26.24.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.26.26.26.24.m4.1a"><mo id="S5.T2.26.26.26.24.m4.1.1" xref="S5.T2.26.26.26.24.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.26.26.26.24.m4.1b"><ci id="S5.T2.26.26.26.24.m4.1.1.cmml" xref="S5.T2.26.26.26.24.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.26.26.26.24.m4.1c">\star</annotation></semantics></math><math id="S5.T2.27.27.27.25.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.27.27.27.25.m5.1a"><mo id="S5.T2.27.27.27.25.m5.1.1" xref="S5.T2.27.27.27.25.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.27.27.27.25.m5.1b"><ci id="S5.T2.27.27.27.25.m5.1.1.cmml" xref="S5.T2.27.27.27.25.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.27.27.27.25.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S5.T2.32.32.32.30" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S5.T2.28.28.28.26.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.28.28.28.26.m1.1a"><mo id="S5.T2.28.28.28.26.m1.1.1" xref="S5.T2.28.28.28.26.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.28.28.28.26.m1.1b"><ci id="S5.T2.28.28.28.26.m1.1.1.cmml" xref="S5.T2.28.28.28.26.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.28.28.28.26.m1.1c">\star</annotation></semantics></math><math id="S5.T2.29.29.29.27.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.29.29.29.27.m2.1a"><mo id="S5.T2.29.29.29.27.m2.1.1" xref="S5.T2.29.29.29.27.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.29.29.29.27.m2.1b"><ci id="S5.T2.29.29.29.27.m2.1.1.cmml" xref="S5.T2.29.29.29.27.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.29.29.29.27.m2.1c">\star</annotation></semantics></math><math id="S5.T2.30.30.30.28.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.30.30.30.28.m3.1a"><mo id="S5.T2.30.30.30.28.m3.1.1" xref="S5.T2.30.30.30.28.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.30.30.30.28.m3.1b"><ci id="S5.T2.30.30.30.28.m3.1.1.cmml" xref="S5.T2.30.30.30.28.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.30.30.30.28.m3.1c">\star</annotation></semantics></math><math id="S5.T2.31.31.31.29.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.31.31.31.29.m4.1a"><mo id="S5.T2.31.31.31.29.m4.1.1" xref="S5.T2.31.31.31.29.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.31.31.31.29.m4.1b"><ci id="S5.T2.31.31.31.29.m4.1.1.cmml" xref="S5.T2.31.31.31.29.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.31.31.31.29.m4.1c">\star</annotation></semantics></math><math id="S5.T2.32.32.32.30.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.32.32.32.30.m5.1a"><mo id="S5.T2.32.32.32.30.m5.1.1" xref="S5.T2.32.32.32.30.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.32.32.32.30.m5.1b"><ci id="S5.T2.32.32.32.30.m5.1.1.cmml" xref="S5.T2.32.32.32.30.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.32.32.32.30.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S5.T2.37.37.37.35" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S5.T2.33.33.33.31.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.33.33.33.31.m1.1a"><mo id="S5.T2.33.33.33.31.m1.1.1" xref="S5.T2.33.33.33.31.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.33.33.33.31.m1.1b"><ci id="S5.T2.33.33.33.31.m1.1.1.cmml" xref="S5.T2.33.33.33.31.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.33.33.33.31.m1.1c">\star</annotation></semantics></math><math id="S5.T2.34.34.34.32.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.34.34.34.32.m2.1a"><mo id="S5.T2.34.34.34.32.m2.1.1" xref="S5.T2.34.34.34.32.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.34.34.34.32.m2.1b"><ci id="S5.T2.34.34.34.32.m2.1.1.cmml" xref="S5.T2.34.34.34.32.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.34.34.34.32.m2.1c">\star</annotation></semantics></math><math id="S5.T2.35.35.35.33.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.35.35.35.33.m3.1a"><mo id="S5.T2.35.35.35.33.m3.1.1" xref="S5.T2.35.35.35.33.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.35.35.35.33.m3.1b"><ci id="S5.T2.35.35.35.33.m3.1.1.cmml" xref="S5.T2.35.35.35.33.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.35.35.35.33.m3.1c">\star</annotation></semantics></math><math id="S5.T2.36.36.36.34.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.36.36.36.34.m4.1a"><mo id="S5.T2.36.36.36.34.m4.1.1" xref="S5.T2.36.36.36.34.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.36.36.36.34.m4.1b"><ci id="S5.T2.36.36.36.34.m4.1.1.cmml" xref="S5.T2.36.36.36.34.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.36.36.36.34.m4.1c">\star</annotation></semantics></math><math id="S5.T2.37.37.37.35.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.37.37.37.35.m5.1a"><mo id="S5.T2.37.37.37.35.m5.1.1" xref="S5.T2.37.37.37.35.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.37.37.37.35.m5.1b"><ci id="S5.T2.37.37.37.35.m5.1.1.cmml" xref="S5.T2.37.37.37.35.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.37.37.37.35.m5.1c">\star</annotation></semantics></math>
</td>
<td id="S5.T2.42.42.42.40" class="ltx_td ltx_nopad_r ltx_align_center">
<math id="S5.T2.38.38.38.36.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.38.38.38.36.m1.1a"><mo id="S5.T2.38.38.38.36.m1.1.1" xref="S5.T2.38.38.38.36.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.38.38.38.36.m1.1b"><ci id="S5.T2.38.38.38.36.m1.1.1.cmml" xref="S5.T2.38.38.38.36.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.38.38.38.36.m1.1c">\star</annotation></semantics></math><math id="S5.T2.39.39.39.37.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.39.39.39.37.m2.1a"><mo id="S5.T2.39.39.39.37.m2.1.1" xref="S5.T2.39.39.39.37.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.39.39.39.37.m2.1b"><ci id="S5.T2.39.39.39.37.m2.1.1.cmml" xref="S5.T2.39.39.39.37.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.39.39.39.37.m2.1c">\star</annotation></semantics></math><math id="S5.T2.40.40.40.38.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.40.40.40.38.m3.1a"><mo id="S5.T2.40.40.40.38.m3.1.1" xref="S5.T2.40.40.40.38.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.40.40.40.38.m3.1b"><ci id="S5.T2.40.40.40.38.m3.1.1.cmml" xref="S5.T2.40.40.40.38.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.40.40.40.38.m3.1c">\star</annotation></semantics></math><math id="S5.T2.41.41.41.39.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.41.41.41.39.m4.1a"><mo id="S5.T2.41.41.41.39.m4.1.1" xref="S5.T2.41.41.41.39.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.41.41.41.39.m4.1b"><ci id="S5.T2.41.41.41.39.m4.1.1.cmml" xref="S5.T2.41.41.41.39.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.41.41.41.39.m4.1c">\star</annotation></semantics></math><math id="S5.T2.42.42.42.40.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="S5.T2.42.42.42.40.m5.1a"><mo id="S5.T2.42.42.42.40.m5.1.1" xref="S5.T2.42.42.42.40.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.42.42.42.40.m5.1b"><ci id="S5.T2.42.42.42.40.m5.1.1.cmml" xref="S5.T2.42.42.42.40.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.42.42.42.40.m5.1c">\star</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The average instance-level annotation agreement for different types of tasks,
alongside the corresponding task-level subjectivity. Numbers in parentheses in the first row
represent the average number of annotations received per task instance. Higher values for both the average agreement <math id="S5.T2.45.m1.1" class="ltx_Math" alttext="\overline{a}" display="inline"><semantics id="S5.T2.45.m1.1b"><mover accent="true" id="S5.T2.45.m1.1.1" xref="S5.T2.45.m1.1.1.cmml"><mi id="S5.T2.45.m1.1.1.2" xref="S5.T2.45.m1.1.1.2.cmml">a</mi><mo id="S5.T2.45.m1.1.1.1" xref="S5.T2.45.m1.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S5.T2.45.m1.1c"><apply id="S5.T2.45.m1.1.1.cmml" xref="S5.T2.45.m1.1.1"><ci id="S5.T2.45.m1.1.1.1.cmml" xref="S5.T2.45.m1.1.1.1">¯</ci><ci id="S5.T2.45.m1.1.1.2.cmml" xref="S5.T2.45.m1.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.45.m1.1d">\overline{a}</annotation></semantics></math> and Krippendorff’s <math id="S5.T2.46.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.T2.46.m2.1b"><mi id="S5.T2.46.m2.1.1" xref="S5.T2.46.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T2.46.m2.1c"><ci id="S5.T2.46.m2.1.1.cmml" xref="S5.T2.46.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.46.m2.1d">\alpha</annotation></semantics></math> indicate a higher degree inter-annotator agreement.
</figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the previous section, we have discovered that the subjectivity of a task can adversely affect the performance of classification models trained on the LLM-generated synthetic data. However, even for the same type of task, the classification for each individual task instance may exhibits different levels of subjectivity as well. Naturally, one may wonder whether models trained on the LLM-generated synthetic data may show different performance on task instances of different subjectivity. We aim to explore the answers to this question in this section.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Instance-level Subjectivity Determination</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Given a text classification task and a specific text instance, we consider the degree of <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">agreement among annotators</span> on the label of this text as a proxy for the subjectivity of this instance—a lower level of agreement means that annotators hold more divergent views, hence the task may have a higher level of subjectivity. Thus, to formally quantify the subjectivity of different instances for different tasks, we again conduct a crowdsourced study to collect instance-level annotations.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Study procedure.</span> We again considered the 10 types of text classification tasks as that in the first evaluation study. For each type of task, we randomly sampled 50 text instances per category from the test set to compose our “evaluation dataset” for that task.
We then recruited U.S. workers from MTurk to complete annotation tasks for those instances in our evaluation dataset. Specifically, each worker was randomly assigned to one type of text classification tasks. After going through a brief
instruction of the assigned task,
the worker was asked to complete 20 classification tasks of the assigned type to get a payment of $1.2, where the texts presented in these 20 tasks were randomly sampled from the evaluation dataset for the assigned type of task.
Again, we included two attention check questions in our study to filter out inattentive workers. We ensured that each task instance received at least three annotations from unique MTurk workers.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.17" class="ltx_p"><span id="S5.SS1.p3.17.1" class="ltx_text ltx_font_bold">Computing instance subjectivity.</span>
Based on annotations we obtained from attentive workers,
we quantify the subjectivity level of each task instance using the fraction of annotators who agree with the majority label for the task instance, that is:</p>
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E1.m1.1" class="ltx_Math" alttext="a_{i}=\frac{{\max}_{y\in\mathcal{Y}}\sum_{k=1}^{K_{i}}\mathds{1}{(r_{i}^{k}=y)}}{K_{i}}" display="block"><semantics id="S5.E1.m1.1a"><mrow id="S5.E1.m1.1.2" xref="S5.E1.m1.1.2.cmml"><msub id="S5.E1.m1.1.2.2" xref="S5.E1.m1.1.2.2.cmml"><mi id="S5.E1.m1.1.2.2.2" xref="S5.E1.m1.1.2.2.2.cmml">a</mi><mi id="S5.E1.m1.1.2.2.3" xref="S5.E1.m1.1.2.2.3.cmml">i</mi></msub><mo id="S5.E1.m1.1.2.1" xref="S5.E1.m1.1.2.1.cmml">=</mo><mfrac id="S5.E1.m1.1.1" xref="S5.E1.m1.1.1.cmml"><mrow id="S5.E1.m1.1.1.1" xref="S5.E1.m1.1.1.1.cmml"><msub id="S5.E1.m1.1.1.1.3" xref="S5.E1.m1.1.1.1.3.cmml"><mi id="S5.E1.m1.1.1.1.3.2" xref="S5.E1.m1.1.1.1.3.2.cmml">max</mi><mrow id="S5.E1.m1.1.1.1.3.3" xref="S5.E1.m1.1.1.1.3.3.cmml"><mi id="S5.E1.m1.1.1.1.3.3.2" xref="S5.E1.m1.1.1.1.3.3.2.cmml">y</mi><mo id="S5.E1.m1.1.1.1.3.3.1" xref="S5.E1.m1.1.1.1.3.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.1.1.1.3.3.3" xref="S5.E1.m1.1.1.1.3.3.3.cmml">𝒴</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.2" xref="S5.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S5.E1.m1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.cmml"><msubsup id="S5.E1.m1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.2.cmml"><mo id="S5.E1.m1.1.1.1.1.2.2.2" xref="S5.E1.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.E1.m1.1.1.1.1.2.2.3" xref="S5.E1.m1.1.1.1.1.2.2.3.cmml"><mi id="S5.E1.m1.1.1.1.1.2.2.3.2" xref="S5.E1.m1.1.1.1.1.2.2.3.2.cmml">k</mi><mo id="S5.E1.m1.1.1.1.1.2.2.3.1" xref="S5.E1.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E1.m1.1.1.1.1.2.2.3.3" xref="S5.E1.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><msub id="S5.E1.m1.1.1.1.1.2.3" xref="S5.E1.m1.1.1.1.1.2.3.cmml"><mi id="S5.E1.m1.1.1.1.1.2.3.2" xref="S5.E1.m1.1.1.1.1.2.3.2.cmml">K</mi><mi id="S5.E1.m1.1.1.1.1.2.3.3" xref="S5.E1.m1.1.1.1.1.2.3.3.cmml">i</mi></msub></msubsup><mrow id="S5.E1.m1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.cmml"><mn id="S5.E1.m1.1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.1.3.cmml">𝟙</mn><mo lspace="0em" rspace="0em" id="S5.E1.m1.1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S5.E1.m1.1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E1.m1.1.1.1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E1.m1.1.1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S5.E1.m1.1.1.1.1.1.1.1.1.2" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">r</mi><mi id="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S5.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S5.E1.m1.1.1.1.1.1.1.1.1.1" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1.cmml">=</mo><mi id="S5.E1.m1.1.1.1.1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.1.1.1.1.3.cmml">y</mi></mrow><mo stretchy="false" id="S5.E1.m1.1.1.1.1.1.1.1.3" xref="S5.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><msub id="S5.E1.m1.1.1.3" xref="S5.E1.m1.1.1.3.cmml"><mi id="S5.E1.m1.1.1.3.2" xref="S5.E1.m1.1.1.3.2.cmml">K</mi><mi id="S5.E1.m1.1.1.3.3" xref="S5.E1.m1.1.1.3.3.cmml">i</mi></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m1.1b"><apply id="S5.E1.m1.1.2.cmml" xref="S5.E1.m1.1.2"><eq id="S5.E1.m1.1.2.1.cmml" xref="S5.E1.m1.1.2.1"></eq><apply id="S5.E1.m1.1.2.2.cmml" xref="S5.E1.m1.1.2.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.2.2.1.cmml" xref="S5.E1.m1.1.2.2">subscript</csymbol><ci id="S5.E1.m1.1.2.2.2.cmml" xref="S5.E1.m1.1.2.2.2">𝑎</ci><ci id="S5.E1.m1.1.2.2.3.cmml" xref="S5.E1.m1.1.2.2.3">𝑖</ci></apply><apply id="S5.E1.m1.1.1.cmml" xref="S5.E1.m1.1.1"><divide id="S5.E1.m1.1.1.2.cmml" xref="S5.E1.m1.1.1"></divide><apply id="S5.E1.m1.1.1.1.cmml" xref="S5.E1.m1.1.1.1"><times id="S5.E1.m1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.2"></times><apply id="S5.E1.m1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.1.3">subscript</csymbol><max id="S5.E1.m1.1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.1.3.2"></max><apply id="S5.E1.m1.1.1.1.3.3.cmml" xref="S5.E1.m1.1.1.1.3.3"><in id="S5.E1.m1.1.1.1.3.3.1.cmml" xref="S5.E1.m1.1.1.1.3.3.1"></in><ci id="S5.E1.m1.1.1.1.3.3.2.cmml" xref="S5.E1.m1.1.1.1.3.3.2">𝑦</ci><ci id="S5.E1.m1.1.1.1.3.3.3.cmml" xref="S5.E1.m1.1.1.1.3.3.3">𝒴</ci></apply></apply><apply id="S5.E1.m1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1"><apply id="S5.E1.m1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.2.1.cmml" xref="S5.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S5.E1.m1.1.1.1.1.2.2.cmml" xref="S5.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.2.2.1.cmml" xref="S5.E1.m1.1.1.1.1.2">subscript</csymbol><sum id="S5.E1.m1.1.1.1.1.2.2.2.cmml" xref="S5.E1.m1.1.1.1.1.2.2.2"></sum><apply id="S5.E1.m1.1.1.1.1.2.2.3.cmml" xref="S5.E1.m1.1.1.1.1.2.2.3"><eq id="S5.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S5.E1.m1.1.1.1.1.2.2.3.1"></eq><ci id="S5.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S5.E1.m1.1.1.1.1.2.2.3.2">𝑘</ci><cn type="integer" id="S5.E1.m1.1.1.1.1.2.2.3.3.cmml" xref="S5.E1.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S5.E1.m1.1.1.1.1.2.3.cmml" xref="S5.E1.m1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.2.3.1.cmml" xref="S5.E1.m1.1.1.1.1.2.3">subscript</csymbol><ci id="S5.E1.m1.1.1.1.1.2.3.2.cmml" xref="S5.E1.m1.1.1.1.1.2.3.2">𝐾</ci><ci id="S5.E1.m1.1.1.1.1.2.3.3.cmml" xref="S5.E1.m1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S5.E1.m1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1"><times id="S5.E1.m1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.1.2"></times><cn type="integer" id="S5.E1.m1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.3">1</cn><apply id="S5.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1"><eq id="S5.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.1"></eq><apply id="S5.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.2">𝑟</ci><ci id="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S5.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><ci id="S5.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E1.m1.1.1.1.1.1.1.1.1.3">𝑦</ci></apply></apply></apply></apply><apply id="S5.E1.m1.1.1.3.cmml" xref="S5.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.E1.m1.1.1.3.1.cmml" xref="S5.E1.m1.1.1.3">subscript</csymbol><ci id="S5.E1.m1.1.1.3.2.cmml" xref="S5.E1.m1.1.1.3.2">𝐾</ci><ci id="S5.E1.m1.1.1.3.3.cmml" xref="S5.E1.m1.1.1.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.1c">a_{i}=\frac{{\max}_{y\in\mathcal{Y}}\sum_{k=1}^{K_{i}}\mathds{1}{(r_{i}^{k}=y)}}{K_{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S5.SS1.p3.16" class="ltx_p">where <math id="S5.SS1.p3.1.m1.3" class="ltx_Math" alttext="\mathcal{Y}=\{1,\cdot\cdot\cdot,Y\}" display="inline"><semantics id="S5.SS1.p3.1.m1.3a"><mrow id="S5.SS1.p3.1.m1.3.4" xref="S5.SS1.p3.1.m1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p3.1.m1.3.4.2" xref="S5.SS1.p3.1.m1.3.4.2.cmml">𝒴</mi><mo id="S5.SS1.p3.1.m1.3.4.1" xref="S5.SS1.p3.1.m1.3.4.1.cmml">=</mo><mrow id="S5.SS1.p3.1.m1.3.4.3.2" xref="S5.SS1.p3.1.m1.3.4.3.1.cmml"><mo stretchy="false" id="S5.SS1.p3.1.m1.3.4.3.2.1" xref="S5.SS1.p3.1.m1.3.4.3.1.cmml">{</mo><mn id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">1</mn><mo id="S5.SS1.p3.1.m1.3.4.3.2.2" xref="S5.SS1.p3.1.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S5.SS1.p3.1.m1.2.2" xref="S5.SS1.p3.1.m1.2.2.cmml">⋯</mi><mo id="S5.SS1.p3.1.m1.3.4.3.2.3" xref="S5.SS1.p3.1.m1.3.4.3.1.cmml">,</mo><mi id="S5.SS1.p3.1.m1.3.3" xref="S5.SS1.p3.1.m1.3.3.cmml">Y</mi><mo stretchy="false" id="S5.SS1.p3.1.m1.3.4.3.2.4" xref="S5.SS1.p3.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.3b"><apply id="S5.SS1.p3.1.m1.3.4.cmml" xref="S5.SS1.p3.1.m1.3.4"><eq id="S5.SS1.p3.1.m1.3.4.1.cmml" xref="S5.SS1.p3.1.m1.3.4.1"></eq><ci id="S5.SS1.p3.1.m1.3.4.2.cmml" xref="S5.SS1.p3.1.m1.3.4.2">𝒴</ci><set id="S5.SS1.p3.1.m1.3.4.3.1.cmml" xref="S5.SS1.p3.1.m1.3.4.3.2"><cn type="integer" id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">1</cn><ci id="S5.SS1.p3.1.m1.2.2.cmml" xref="S5.SS1.p3.1.m1.2.2">⋯</ci><ci id="S5.SS1.p3.1.m1.3.3.cmml" xref="S5.SS1.p3.1.m1.3.3">𝑌</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.3c">\mathcal{Y}=\{1,\cdot\cdot\cdot,Y\}</annotation></semantics></math> is the set of all possible labels, <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="K_{i}" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><msub id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">K</mi><mi id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">𝐾</ci><ci id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">K_{i}</annotation></semantics></math> is the total number of annotators who labeled instance <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mi id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><ci id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">i</annotation></semantics></math>, and <math id="S5.SS1.p3.4.m4.1" class="ltx_Math" alttext="r_{i}^{k}" display="inline"><semantics id="S5.SS1.p3.4.m4.1a"><msubsup id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml"><mi id="S5.SS1.p3.4.m4.1.1.2.2" xref="S5.SS1.p3.4.m4.1.1.2.2.cmml">r</mi><mi id="S5.SS1.p3.4.m4.1.1.2.3" xref="S5.SS1.p3.4.m4.1.1.2.3.cmml">i</mi><mi id="S5.SS1.p3.4.m4.1.1.3" xref="S5.SS1.p3.4.m4.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><apply id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.4.m4.1.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">superscript</csymbol><apply id="S5.SS1.p3.4.m4.1.1.2.cmml" xref="S5.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.4.m4.1.1.2.1.cmml" xref="S5.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.p3.4.m4.1.1.2.2.cmml" xref="S5.SS1.p3.4.m4.1.1.2.2">𝑟</ci><ci id="S5.SS1.p3.4.m4.1.1.2.3.cmml" xref="S5.SS1.p3.4.m4.1.1.2.3">𝑖</ci></apply><ci id="S5.SS1.p3.4.m4.1.1.3.cmml" xref="S5.SS1.p3.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">r_{i}^{k}</annotation></semantics></math> is the <math id="S5.SS1.p3.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.p3.5.m5.1a"><mi id="S5.SS1.p3.5.m5.1.1" xref="S5.SS1.p3.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.5.m5.1b"><ci id="S5.SS1.p3.5.m5.1.1.cmml" xref="S5.SS1.p3.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.5.m5.1c">k</annotation></semantics></math>-th annotator’s annotation on instance <math id="S5.SS1.p3.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p3.6.m6.1a"><mi id="S5.SS1.p3.6.m6.1.1" xref="S5.SS1.p3.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.6.m6.1b"><ci id="S5.SS1.p3.6.m6.1.1.cmml" xref="S5.SS1.p3.6.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.6.m6.1c">i</annotation></semantics></math>. Intuitively, a lower value of <math id="S5.SS1.p3.7.m7.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S5.SS1.p3.7.m7.1a"><msub id="S5.SS1.p3.7.m7.1.1" xref="S5.SS1.p3.7.m7.1.1.cmml"><mi id="S5.SS1.p3.7.m7.1.1.2" xref="S5.SS1.p3.7.m7.1.1.2.cmml">a</mi><mi id="S5.SS1.p3.7.m7.1.1.3" xref="S5.SS1.p3.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.7.m7.1b"><apply id="S5.SS1.p3.7.m7.1.1.cmml" xref="S5.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.7.m7.1.1.1.cmml" xref="S5.SS1.p3.7.m7.1.1">subscript</csymbol><ci id="S5.SS1.p3.7.m7.1.1.2.cmml" xref="S5.SS1.p3.7.m7.1.1.2">𝑎</ci><ci id="S5.SS1.p3.7.m7.1.1.3.cmml" xref="S5.SS1.p3.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.7.m7.1c">a_{i}</annotation></semantics></math> suggests that consensus is less likely to be reached among annotators on instance <math id="S5.SS1.p3.8.m8.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p3.8.m8.1a"><mi id="S5.SS1.p3.8.m8.1.1" xref="S5.SS1.p3.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.8.m8.1b"><ci id="S5.SS1.p3.8.m8.1.1.cmml" xref="S5.SS1.p3.8.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.8.m8.1c">i</annotation></semantics></math>, thus instance <math id="S5.SS1.p3.9.m9.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p3.9.m9.1a"><mi id="S5.SS1.p3.9.m9.1.1" xref="S5.SS1.p3.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.9.m9.1b"><ci id="S5.SS1.p3.9.m9.1.1.cmml" xref="S5.SS1.p3.9.m9.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.9.m9.1c">i</annotation></semantics></math> may have a higher level of subjectivity. In Table <a href="#S5.T2" title="Table 2 ‣ 5 Evaluation II: Comparison Across Different Task Instances ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we report the average values of
<math id="S5.SS1.p3.10.m10.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S5.SS1.p3.10.m10.1a"><msub id="S5.SS1.p3.10.m10.1.1" xref="S5.SS1.p3.10.m10.1.1.cmml"><mi id="S5.SS1.p3.10.m10.1.1.2" xref="S5.SS1.p3.10.m10.1.1.2.cmml">a</mi><mi id="S5.SS1.p3.10.m10.1.1.3" xref="S5.SS1.p3.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.10.m10.1b"><apply id="S5.SS1.p3.10.m10.1.1.cmml" xref="S5.SS1.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.10.m10.1.1.1.cmml" xref="S5.SS1.p3.10.m10.1.1">subscript</csymbol><ci id="S5.SS1.p3.10.m10.1.1.2.cmml" xref="S5.SS1.p3.10.m10.1.1.2">𝑎</ci><ci id="S5.SS1.p3.10.m10.1.1.3.cmml" xref="S5.SS1.p3.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.10.m10.1c">a_{i}</annotation></semantics></math>
(i.e., <math id="S5.SS1.p3.11.m11.1" class="ltx_Math" alttext="\overline{a}" display="inline"><semantics id="S5.SS1.p3.11.m11.1a"><mover accent="true" id="S5.SS1.p3.11.m11.1.1" xref="S5.SS1.p3.11.m11.1.1.cmml"><mi id="S5.SS1.p3.11.m11.1.1.2" xref="S5.SS1.p3.11.m11.1.1.2.cmml">a</mi><mo id="S5.SS1.p3.11.m11.1.1.1" xref="S5.SS1.p3.11.m11.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.11.m11.1b"><apply id="S5.SS1.p3.11.m11.1.1.cmml" xref="S5.SS1.p3.11.m11.1.1"><ci id="S5.SS1.p3.11.m11.1.1.1.cmml" xref="S5.SS1.p3.11.m11.1.1.1">¯</ci><ci id="S5.SS1.p3.11.m11.1.1.2.cmml" xref="S5.SS1.p3.11.m11.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.11.m11.1c">\overline{a}</annotation></semantics></math>) for instances in the evaluation datasets of different types of tasks, along with the average inter-annotator agreement on each task instance (as measured by the Krippendorff’s <math id="S5.SS1.p3.12.m12.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS1.p3.12.m12.1a"><mi id="S5.SS1.p3.12.m12.1.1" xref="S5.SS1.p3.12.m12.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.12.m12.1b"><ci id="S5.SS1.p3.12.m12.1.1.cmml" xref="S5.SS1.p3.12.m12.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.12.m12.1c">\alpha</annotation></semantics></math>) as well as the task-level subjectivity level for different types of tasks. We can see that
<math id="S5.SS1.p3.13.m13.1" class="ltx_Math" alttext="\overline{a}" display="inline"><semantics id="S5.SS1.p3.13.m13.1a"><mover accent="true" id="S5.SS1.p3.13.m13.1.1" xref="S5.SS1.p3.13.m13.1.1.cmml"><mi id="S5.SS1.p3.13.m13.1.1.2" xref="S5.SS1.p3.13.m13.1.1.2.cmml">a</mi><mo id="S5.SS1.p3.13.m13.1.1.1" xref="S5.SS1.p3.13.m13.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.13.m13.1b"><apply id="S5.SS1.p3.13.m13.1.1.cmml" xref="S5.SS1.p3.13.m13.1.1"><ci id="S5.SS1.p3.13.m13.1.1.1.cmml" xref="S5.SS1.p3.13.m13.1.1.1">¯</ci><ci id="S5.SS1.p3.13.m13.1.1.2.cmml" xref="S5.SS1.p3.13.m13.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.13.m13.1c">\overline{a}</annotation></semantics></math> closely aligns with the Krippendorff’s <math id="S5.SS1.p3.14.m14.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS1.p3.14.m14.1a"><mi id="S5.SS1.p3.14.m14.1.1" xref="S5.SS1.p3.14.m14.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.14.m14.1b"><ci id="S5.SS1.p3.14.m14.1.1.cmml" xref="S5.SS1.p3.14.m14.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.14.m14.1c">\alpha</annotation></semantics></math>, and tasks with higher levels of subjectivity also exhibit a higher value of <math id="S5.SS1.p3.15.m15.1" class="ltx_Math" alttext="\overline{a}" display="inline"><semantics id="S5.SS1.p3.15.m15.1a"><mover accent="true" id="S5.SS1.p3.15.m15.1.1" xref="S5.SS1.p3.15.m15.1.1.cmml"><mi id="S5.SS1.p3.15.m15.1.1.2" xref="S5.SS1.p3.15.m15.1.1.2.cmml">a</mi><mo id="S5.SS1.p3.15.m15.1.1.1" xref="S5.SS1.p3.15.m15.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.15.m15.1b"><apply id="S5.SS1.p3.15.m15.1.1.cmml" xref="S5.SS1.p3.15.m15.1.1"><ci id="S5.SS1.p3.15.m15.1.1.1.cmml" xref="S5.SS1.p3.15.m15.1.1.1">¯</ci><ci id="S5.SS1.p3.15.m15.1.1.2.cmml" xref="S5.SS1.p3.15.m15.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.15.m15.1c">\overline{a}</annotation></semantics></math> in general, indicating that <math id="S5.SS1.p3.16.m16.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S5.SS1.p3.16.m16.1a"><msub id="S5.SS1.p3.16.m16.1.1" xref="S5.SS1.p3.16.m16.1.1.cmml"><mi id="S5.SS1.p3.16.m16.1.1.2" xref="S5.SS1.p3.16.m16.1.1.2.cmml">a</mi><mi id="S5.SS1.p3.16.m16.1.1.3" xref="S5.SS1.p3.16.m16.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.16.m16.1b"><apply id="S5.SS1.p3.16.m16.1.1.cmml" xref="S5.SS1.p3.16.m16.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.16.m16.1.1.1.cmml" xref="S5.SS1.p3.16.m16.1.1">subscript</csymbol><ci id="S5.SS1.p3.16.m16.1.1.2.cmml" xref="S5.SS1.p3.16.m16.1.1.2">𝑎</ci><ci id="S5.SS1.p3.16.m16.1.1.3.cmml" xref="S5.SS1.p3.16.m16.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.16.m16.1c">a_{i}</annotation></semantics></math> can potentially serve as a reasonable proxy for the subjectivity of each task instance.</p>
</div>
<figure id="S5.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/ag_line.png" id="S5.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="114" height="98" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>AG</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/relations_line.png" id="S5.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="114" height="98" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Relation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/imdb_line.png" id="S5.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>IMDB Reviews</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/spam_line.png" id="S5.F2.sf4.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>SMS Spam</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/go_emotions_line.png" id="S5.F2.sf5.g1" class="ltx_graphics ltx_img_square" width="114" height="100" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Reddit Emotion</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/sarcasm_line.png" id="S5.F2.sf6.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Sarcasm News</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/humor_line.png" id="S5.F2.sf7.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>Humor Detection</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/tweet_emotion_line.png" id="S5.F2.sf8.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>Tweet Emotions</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/tweet_irony_line.png" id="S5.F2.sf9.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(i) </span>Tweet Irony Speech</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf10" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/financial_line.png" id="S5.F2.sf10.g1" class="ltx_graphics ltx_img_square" width="114" height="100" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(j) </span>Financial Phrasebank</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Changes in the accuracy of the BERT model trained on zero-shot synthetic data as the instance-level annotation agreement threshold varies.
The solid blue line in each plot is the linear regression fitted on the data, and the <math id="S5.F2.6.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S5.F2.6.m1.1b"><mi id="S5.F2.6.m1.1.1" xref="S5.F2.6.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S5.F2.6.m1.1c"><ci id="S5.F2.6.m1.1.1.cmml" xref="S5.F2.6.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.6.m1.1d">R</annotation></semantics></math>-squared score quantifies the goodness of fit. The Spearman’s <math id="S5.F2.7.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.F2.7.m2.1b"><mi id="S5.F2.7.m2.1.1" xref="S5.F2.7.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.F2.7.m2.1c"><ci id="S5.F2.7.m2.1.1.cmml" xref="S5.F2.7.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.7.m2.1d">\rho</annotation></semantics></math> assesses the strength of rank correlation between the instance-level agreement threshold and the model accuracy for each task. Higher values for both <math id="S5.F2.8.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S5.F2.8.m3.1b"><mi id="S5.F2.8.m3.1.1" xref="S5.F2.8.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S5.F2.8.m3.1c"><ci id="S5.F2.8.m3.1.1.cmml" xref="S5.F2.8.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.8.m3.1d">R</annotation></semantics></math>-squared and Spearman’s <math id="S5.F2.9.m4.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.F2.9.m4.1b"><mi id="S5.F2.9.m4.1.1" xref="S5.F2.9.m4.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.F2.9.m4.1c"><ci id="S5.F2.9.m4.1.1.cmml" xref="S5.F2.9.m4.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.9.m4.1d">\rho</annotation></semantics></math>, ideally close to <math id="S5.F2.10.m5.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.F2.10.m5.1b"><mn id="S5.F2.10.m5.1.1" xref="S5.F2.10.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.F2.10.m5.1c"><cn type="integer" id="S5.F2.10.m5.1.1.cmml" xref="S5.F2.10.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.10.m5.1d">1</annotation></semantics></math>, indicate a stronger monotonic relationship between the instance-level subjectivity and the model accuracy. </figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Results</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.3" class="ltx_p">We now look into whether models trained on the LLM-generated synthetic data exhibit different performance on instances with different levels of subjectivity, and we focus on the models trained on zero-shot synthetic data in this evaluation. Specifically, given a classification task, we trained a BERT model using the zero-shot synthetic data and computed its accuracy on the subset of task instances in the evaluation dataset whose instance-level annotation agreement (i.e., <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">a</mi><mi id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑎</ci><ci id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">a_{i}</annotation></semantics></math>) exceeds a threshold <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\gamma</annotation></semantics></math>, and we repeated this computation for many times as we varied the value of <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\gamma</annotation></semantics></math>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.5" class="ltx_p">Figure <a href="#S5.F2" title="Figure 2 ‣ 5.1 Instance-level Subjectivity Determination ‣ 5 Evaluation II: Comparison Across Different Task Instances ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates how the model accuracy varies with the instance-level annotation agreement threshold <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\gamma</annotation></semantics></math> for different types of tasks. For most tasks (except for the tasks in the Scarcasm News and Finanical Phrasebank datasets), we observe a strong <span id="S5.SS2.p2.5.1" class="ltx_text ltx_font_italic">monotonically increasing</span> relationship between <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">\gamma</annotation></semantics></math> and the model accuracy, with correlations between them (i.e., <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">\beta</annotation></semantics></math>) being positive and values of the Spearman’s rank correlation coefficient <math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">\rho</annotation></semantics></math> often exceeding 0.85. Since increasing the instance-level annotation agreement threshold <math id="S5.SS2.p2.5.m5.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S5.SS2.p2.5.m5.1a"><mi id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><ci id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">\gamma</annotation></semantics></math> effectively filters out task instances with high subjectivity, this observation suggests that models trained on synthetic data indeed tend to have varying performance on different instances—even within the same type of tasks, these models still perform better on those task instances with low subjectivity.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.2" class="ltx_p">As a comparison, we also investigate into whether models trained on the real-world data exhibit similar behaviors. The detailed results are reported in App. <a href="#A3" title="Appendix C Evaluation II: Comparison Across Different Task Instances (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>. On the high level, while we also observe the trend that these models’ performance appears to increase as the instance-level task subjectivity decreases, such relationship is usually weaker than that illustrated in the models trained on the synthetic data (e.g., <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mi id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">\beta</annotation></semantics></math> and <math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mi id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><ci id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">\rho</annotation></semantics></math> are smaller).</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Discussions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we present an initial exploration into factors that moderate the effectiveness of LLM-generated synthetic data for facilitating the training of text classification models.
Our results show that the performance of the models trained on synthetic data decreases both for classification tasks with higher levels of subjectivity and on task instances with higher subjectivity. In this section, we provide some potential explanations for the observations of our study, and discuss the implications, limitations, and future directions of our work.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Why subjectivity adversely impacts the effectiveness of the synthetic data?</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We provide a few explanations for why task subjectivity is found to be negatively associated with the performance of models trained on the LLM-generated synthetic data.
First, highly subjective tasks often require a deep understanding of nuanced human emotions and contextual subtleties, as well as the ability to discern and accurately interpret different perspectives. As such, LLMs may encounter limitations in generating data that can capture the extensive range and complexity of real-life use of language. Indeed, as shown in our exploratory analysis in Section <a href="#S4.SS5" title="4.5 Exploratory Analysis: Data Diversity ‣ 4 Evaluation I: Comparison Across Different Types of Tasks ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>, the diversity of the LLM-generated synthetic data appears to be particularly limited on tasks with high subjectivity, when compared to the real-world data. This implies that one potential way to improve the effectiveness of synthetic data on high subjectivity tasks is to increase the data diversity and ensure the synthetic data can better reflect real-world data distributions.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Second, specific to the relationship between the instance-level subjectivity and model performance, we note that the “gold label” of a task instance is usually decided by a majority vote within a group of annotators. This means that the gold label may not represent the perspective of each individual <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, and they are sometimes “biased” themselves depending on the annotator decomposition <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>. Thus, it may be challenging for LLMs to generate synthetic data to recover such potentially biased “majority view,” especially if the LLMs are trained to maintain neutrality. Alternatively, one may ask for subjective task instances that humans can hardly reach any consensus on, whether the “gold label” is really the only “correct” label? If not, a rethinking of how to develop and evaluate models for these task instances is urgently needed.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Explaining a few exceptions</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Model Training ‣ 4 Evaluation I: Comparison Across Different Types of Tasks ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we surprisingly find that on the Tweet irony detection tasks, models trained on the few-shot synthetic data even outperform models trained on the real-world data. One plausible explanation is that the nature of generating irony texts for social media involves a creative writing task with few language formality constraints, and recent research suggests that LLMs have
the potential to exhibit comparable creativity with human writers in such task <cite class="ltx_cite ltx_citemacro_cite">Franceschelli and Musolesi (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. Another exception we find is in Section <a href="#S5.SS2" title="5.2 Evaluation Results ‣ 5 Evaluation II: Comparison Across Different Task Instances ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>—for the Financial Phrasebank and Scarcasm datasets, unlike other tasks, the effectiveness of the models trained on the synthetic data do not vary much with the instance-level task subjectivity.
We conjecture that this can be caused by some
task-specific properties.
On the Financial Phasebank dataset, accurate sentiment analysis
requires the understanding of specialized terminology related to finance. Similarly, the Sarcasm detection task aims at identifying sarcasm in news headlines from selected sources and requires the comprehension on political topics. Thus, on these tasks, LLMs might not be fully equipped with the necessary domain knowledge to create effective synthetic data under the zero-shot setting. In fact, as shown in Figure <a href="#S5.F2" title="Figure 2 ‣ 5.1 Instance-level Subjectivity Determination ‣ 5 Evaluation II: Comparison Across Different Task Instances ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, models trained on the zero-shot synthetic data have very low performance on these two datasets, regardless of the subjectivity levels of task instances.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Limitations and future work</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">We acknowledge that task subjectivity may not be the only factor that moderates the effectiveness of the LLM-generated synthetic data. Future studies can look into the potential moderating role of other factors, such as language formality and the requirement for domain-specific knowledge. Our reliance on crowd workers in determining task subjectivity may introduce some variability due to their lack of linguistic expertise. Our evaluation is also based on the GPT-3.5-Turbo model only. It is important to note that the conclusions we get here may not generalize to other LLMs (e.g., the more advanced GPT-4), considering the continuous improvements of LLMs in generating human-like texts.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">Our findings suggest that incorporating real-world data examples into the synthetic data generation process can increase the data diversity and boost the performance of the resulting models. Thus, future work can explore strategies that leverage human intelligence, such as feedback or direct intervention in the generation process, to further enrich the diversity of synthetic data <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite> and to identify the most “informative” type of data instance to generate. Finally, the significant correlation between the subjectivity of tasks or instances and the performance of models trained on synthetic data also suggests the potential to utilize the performance of such models as a proxy for approximating task or instance subjectivity, or to estimate the reliability of gold labels.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aggarwal et al. (2022)</span>
<span class="ltx_bibblock">
Karan Aggarwal, Henry Jin, and Aitzaz Ahmad. 2022.

</span>
<span class="ltx_bibblock">Entity-controlled synthetic text generation using contextual question
and answering with pre-trained language models.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">all MiniLM-L6-v2 (2023)</span>
<span class="ltx_bibblock">
all MiniLM-L6-v2. 2023.

</span>
<span class="ltx_bibblock">sentence-transformers/all-minilm-l6-v2.

</span>
<span class="ltx_bibblock">Accessed on Hugging Face Model Hub.

</span>
<span class="ltx_bibblock">Available from:
<a target="_blank" href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almeida et al. (2011)</span>
<span class="ltx_bibblock">
Tiago A. Almeida, Jose Maria Gomez Hidalgo, and Akebo Yamakami. 2011.

</span>
<span class="ltx_bibblock">Contributions to the study of sms spam filtering: New collection and
results.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2011 ACM Symposium on Document
Engineering (DOCENG’11)</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Annamoradnejad and Zoghi (2020)</span>
<span class="ltx_bibblock">
Issa Annamoradnejad and Gohar Zoghi. 2020.

</span>
<span class="ltx_bibblock">Colbert: Using bert sentence embedding for humor detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.12765</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benveniste (1971)</span>
<span class="ltx_bibblock">
Emile Benveniste. 1971.

</span>
<span class="ltx_bibblock">Subjectivity in language.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Problems in general linguistics</em>, 1:223–30.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besnier et al. (2020)</span>
<span class="ltx_bibblock">
Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick
Pérez. 2020.

</span>
<span class="ltx_bibblock">This dataset does not exist: training models from generated images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 1–5. IEEE.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Andy Rosenbaum, Seokhwan
Kim, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2022.

</span>
<span class="ltx_bibblock">Weakly supervised data augmentation through prompting for dialogue
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.14169</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2023)</span>
<span class="ltx_bibblock">
John Joon Young Chung, Ece Kamar, and Saleema Amershi. 2023.

</span>
<span class="ltx_bibblock">Increasing diversity while maintaining accuracy: Text data generation
with large language models and human interventions.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.04140</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2021)</span>
<span class="ltx_bibblock">
Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan,
and Noah A Smith. 2021.

</span>
<span class="ltx_bibblock">All that’s’ human’is not gold: Evaluating human evaluation of
generated text.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.00061</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cormen et al. (2022)</span>
<span class="ltx_bibblock">
Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein.
2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Introduction to algorithms</em>.

</span>
<span class="ltx_bibblock">MIT press.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demszky et al. (2020)</span>
<span class="ltx_bibblock">
Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav
Nemade, and Sujith Ravi. 2020.

</span>
<span class="ltx_bibblock">GoEmotions: A Dataset of Fine-Grained Emotions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">58th Annual Meeting of the Association for Computational
Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dou et al. (2021)</span>
<span class="ltx_bibblock">
Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A Smith, and Yejin Choi.
2021.

</span>
<span class="ltx_bibblock">Is gpt-3 text indistinguishable from human text? scarecrow: A
framework for scrutinizing machine text.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.01294</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekman et al. (1999)</span>
<span class="ltx_bibblock">
Paul Ekman et al. 1999.

</span>
<span class="ltx_bibblock">Basic emotions.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Handbook of cognition and emotion</em>, 98(45-60):16.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Franceschelli and Musolesi (2023)</span>
<span class="ltx_bibblock">
Giorgio Franceschelli and Mirco Musolesi. 2023.

</span>
<span class="ltx_bibblock">On the creativity of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.00008</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2022)</span>
<span class="ltx_bibblock">
Jiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng Ye, Zhiyong Wu, WEIZHONG
ZHANG, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. 2022.

</span>
<span class="ltx_bibblock">Self-guided noise-free data generation for efficient zero-shot
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Jiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng Ye, Zhiyong Wu, WEIZHONG
ZHANG, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. 2023.

</span>
<span class="ltx_bibblock">Self-guided noise-free data generation for efficient zero-shot
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2019)</span>
<span class="ltx_bibblock">
Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1649" title="" class="ltx_ref ltx_href">FewRel 2.0: Towards
more challenging few-shot relation classification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 6251–6256, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon et al. (2022)</span>
<span class="ltx_bibblock">
Mitchell L Gordon, Michelle S Lam, Joon Sung Park, Kayur Patel, Jeff Hancock,
Tatsunori Hashimoto, and Michael S Bernstein. 2022.

</span>
<span class="ltx_bibblock">Jury learning: Integrating dissenting voices into machine learning
models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 CHI Conference on Human Factors in
Computing Systems</em>, pages 1–19.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2022)</span>
<span class="ltx_bibblock">
Nitesh Goyal, Ian D Kivlichan, Rachel Rosen, and Lucy Vasserman. 2022.

</span>
<span class="ltx_bibblock">Is your toxicity my toxicity? exploring the impact of rater identity
on toxicity annotation.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Human-Computer Interaction</em>,
6(CSCW2):1–28.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hämäläinen et al. (2023)</span>
<span class="ltx_bibblock">
Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3544548.3580688" title="" class="ltx_ref ltx_href">Evaluating large
language models in generating synthetic hci research data: A case study</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems</em>, CHI ’23, New York, NY, USA. Association for Computing
Machinery.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et al. (2022)</span>
<span class="ltx_bibblock">
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
and Ece Kamar. 2022.

</span>
<span class="ltx_bibblock">Toxigen: A large-scale machine-generated dataset for adversarial and
implicit hate speech detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.09509</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2022)</span>
<span class="ltx_bibblock">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song
Bai, and Xiaojuan Qi. 2022.

</span>
<span class="ltx_bibblock">Is synthetic data from generative models ready for image recognition?

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.07574</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jindal and Liu (2007)</span>
<span class="ltx_bibblock">
Nitin Jindal and Bing Liu. 2007.

</span>
<span class="ltx_bibblock">Review spam detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th international conference on World
Wide Web</em>, pages 1189–1190.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al. (2019)</span>
<span class="ltx_bibblock">
Tero Karras, Samuli Laine, and Timo Aila. 2019.

</span>
<span class="ltx_bibblock">A style-based generator architecture for generative adversarial
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 4401–4410.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al. (2020)</span>
<span class="ltx_bibblock">
Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020.

</span>
<span class="ltx_bibblock">Data augmentation using pre-trained transformer models.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.02245</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Zhuoyan Li, Zhuoran Lu, and Ming Yin. 2022.

</span>
<span class="ltx_bibblock">Towards better detection of biased language with scarce, noisy, and
biased annotations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics,
and Society</em>, pages 411–423.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maas et al. (2011)</span>
<span class="ltx_bibblock">
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and
Christopher Potts. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.aclweb.org/anthology/P11-1015" title="" class="ltx_ref ltx_href">Learning word
vectors for sentiment analysis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies</em>, pages 142–150,
Portland, Oregon, USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malo et al. (2014)</span>
<span class="ltx_bibblock">
P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. 2014.

</span>
<span class="ltx_bibblock">Good debt or bad debt: Detecting semantic orientations in economic
texts.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Journal of the Association for Information Science and
Technology</em>, 65.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mcnutt et al. (2023)</span>
<span class="ltx_bibblock">
Andrew M Mcnutt, Chenglong Wang, Robert A Deline, and Steven M. Drucker. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3544548.3580940" title="" class="ltx_ref ltx_href">On the design of
ai-powered code assistants for notebooks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems</em>, CHI ’23, New York, NY, USA. Association for Computing
Machinery.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. (2022)</span>
<span class="ltx_bibblock">
Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022.

</span>
<span class="ltx_bibblock">Generating training data with language models: Towards zero-shot
language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:462–477.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Misra and Arora (2023)</span>
<span class="ltx_bibblock">
Rishabh Misra and Prahal Arora. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.aiopen.2023.01.001" title="" class="ltx_ref ltx_href">Sarcasm detection using news headlines dataset</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">AI Open</em>, 4:13–18.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Misra and Grover (2021)</span>
<span class="ltx_bibblock">
Rishabh Misra and Jigyasa Grover. 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Sculpting Data for ML: The first act of Machine Learning</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohammad et al. (2018)</span>
<span class="ltx_bibblock">
Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana
Kiritchenko. 2018.

</span>
<span class="ltx_bibblock">Semeval-2018 task 1: Affect in tweets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th international workshop on semantic
evaluation</em>, pages 1–17.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nichol et al. (2021)</span>
<span class="ltx_bibblock">
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
Bob McGrew, Ilya Sutskever, and Mark Chen. 2021.

</span>
<span class="ltx_bibblock">Glide: Towards photorealistic image generation and editing with
text-guided diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.10741</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">Gpt-4 technical report</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhys Cox et al. (2021)</span>
<span class="ltx_bibblock">
Samuel Rhys Cox, Yunlong Wang, Ashraf Abdul, Christian von der Weth, and Brian
Y. Lim. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3411764.3445782" title="" class="ltx_ref ltx_href">Directed diversity:
Leveraging language embedding distances for collective creativity in crowd
ideation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems</em>, CHI ’21, New York, NY, USA. Association for Computing
Machinery.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu et al. (2022)</span>
<span class="ltx_bibblock">
Gaurav Sahu, Pau Rodriguez, Issam H. Laradji, Parmida Atighehchian, David
Vazquez, and Dzmitry Bahdanau. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2204.01959" title="" class="ltx_ref ltx_href">Data augmentation for intent
classification with off-the-shelf large language models</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al. (2021)</span>
<span class="ltx_bibblock">
Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and
Noah A Smith. 2021.

</span>
<span class="ltx_bibblock">Annotators with attitudes: How annotator beliefs and identities bias
toxic language detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.07997</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023.

</span>
<span class="ltx_bibblock">Does synthetic data generation of llms help clinical text mining?

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04360</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Hee et al. (2018)</span>
<span class="ltx_bibblock">
Cynthia Van Hee, Els Lefever, and Véronique Hoste. 2018.

</span>
<span class="ltx_bibblock">Semeval-2018 task 3: Irony detection in english tweets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 12th International Workshop on Semantic
Evaluation</em>, pages 39–50.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Veatch (1998)</span>
<span class="ltx_bibblock">
Thomas C Veatch. 1998.

</span>
<span class="ltx_bibblock">A theory of humor.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Zirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao. 2021.

</span>
<span class="ltx_bibblock">Towards zero-label language learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.09193</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.01652</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiebe et al. (2004)</span>
<span class="ltx_bibblock">
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin.
2004.

</span>
<span class="ltx_bibblock">Learning subjective language.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Computational linguistics</em>, 30(3):277–308.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiegand et al. (2019)</span>
<span class="ltx_bibblock">
Michael Wiegand, Josef Ruppenhofer, and Thomas Kleinbauer. 2019.

</span>
<span class="ltx_bibblock">Detection of abusive language: the problem of biased datasets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 conference of the North American
Chapter of the Association for Computational Linguistics: human language
technologies, volume 1 (long and short papers)</em>, pages 602–608.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="" class="ltx_ref ltx_href">Transformers:
State-of-the-art natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 38–45, Online.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2022)</span>
<span class="ltx_bibblock">
Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao
Yu, and Lingpeng Kong. 2022.

</span>
<span class="ltx_bibblock">Zerogen: Efficient zero-shot learning via dataset generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.07922</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoo et al. (2021)</span>
<span class="ltx_bibblock">
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyeong Park.
2021.

</span>
<span class="ltx_bibblock">Gpt3mix: Leveraging large-scale language models for text
augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.08826</em>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2015)</span>
<span class="ltx_bibblock">
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.

</span>
<span class="ltx_bibblock">Character-level convolutional networks for text classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">NIPS</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela
Barriuso, Antonio Torralba, and Sanja Fidler. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2104.06490" title="" class="ltx_ref ltx_href">Datasetgan: Efficient
labeled data factory with minimal human effort</a>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and Munmun
De Choudhury. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3544548.3581318" title="" class="ltx_ref ltx_href">Synthetic lies:
Understanding ai-generated misinformation and evaluating algorithmic and
human solutions</a>.

</span>
<span class="ltx_bibblock">CHI ’23, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendices</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Dataset and Task Descriptions</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p"><span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_bold">AG’s News:</span> This task involves classifying news articles from the subset of AG’s News Topic Classification dataset into one of thee categories: World, Sports and Sci/Tech. The AG’s News Topic Classification dataset, collected from over 2,000 news sources by the academic news search engine, ComeToMyHead, consists of a training set of 120,000 instances and a test set of 7,600 instances.
<br class="ltx_break"><span id="A1.SS1.p1.1.2" class="ltx_text ltx_font_bold">Relation Classification:</span> This task requires the identification of the relationships between two entities within a given sentence. In this study, we focus on four relations: ‘country’, ‘league’, ‘screenwriter’, and ‘tributary’. The dataset comprises English text sourced from Wikipedia and supplemented with crowdsourced English annotations. Each relation has 700 instances. As the dataset does not provide an official division into train, validation, and test sets, we randomly allocated the dataset into train (70%), validation (5%), and test (25%) sets. In our evaluation, this process was repeated three times, with the average performance reported. 
<br class="ltx_break"><span id="A1.SS1.p1.1.3" class="ltx_text ltx_font_bold">IMDB Reviews:</span> This task requires classifying the sentiment of movie reviews from the IMDB platform into one of two categories: positive (pos) or negative (neg). The dataset comprises 50,000 movie reviews evenly split, with 25,000 designated for training and 25,000 for testing. 
<br class="ltx_break"><span id="A1.SS1.p1.1.4" class="ltx_text ltx_font_bold">SMS Message Spam:</span> This task involves the classification of SMS messages from the SMS Spam Collection v.1 dataset into either ‘ham’ (legitimate) or ‘spam’ categories. The training dataset contains 5,574 English messages, each labeled according to its legitimacy. As the dataset does not provide an official division into train, validation, and test sets, we randomly divided the dataset into train (70%), validation (5%), and test (25%) sets. In our evaluation, this process was repeated three times, with the average performance reported.
<br class="ltx_break"><span id="A1.SS1.p1.1.5" class="ltx_text ltx_font_bold">Financial Phrasebank:</span> This task entails the classification of finance-related sentences into one of three categories—positive, negative, or neutral—based on the sentiment expressed by the sentence. The dataset comprises 4,840 English sentences sourced from financial news articles. As the dataset does not provide an official division into train, validation, and test sets, we randomly allocated the dataset into train (70%), validation (5%), and test (25%) sets. In our evaluation, this process was repeated three times, with the average performance reported. 
<br class="ltx_break"><span id="A1.SS1.p1.1.6" class="ltx_text ltx_font_bold">Reddit Emotion:</span> The Reddit Emotion is the subset of the Go Emotions dataset. The
Go Emotions dataset is comprised of 58,009 comments collected from Reddit, and each comment has been annotated with respect to 28
emotion categories. In this task, we focus on three basic emotions <cite class="ltx_cite ltx_citemacro_cite">Ekman et al. (<a href="#bib.bib15" title="" class="ltx_ref">1999</a>)</cite>: joy,
sadness, and surprise. 
<br class="ltx_break"><span id="A1.SS1.p1.1.7" class="ltx_text ltx_font_bold">Tweet Irony Speech:</span> The task involves classifying tweets into two categories:
irony, non-irony. The dataset, which is composed of English-language tweets, has been manually annotated for these specific categories. The distribution of the data includes a training set of 2,862 instances and a test set of 784 instances. 
<br class="ltx_break"><span id="A1.SS1.p1.1.8" class="ltx_text ltx_font_bold">Tweet Emotion:</span> The task involves classifying tweets into four emotion
categories: anger, joy, optimism, sadness. Each tweet in this English-language dataset has been annotated by human reviewers with respect to these emotional categories. The dataset is partitioned into a training set of 3,257 instances and a test set of 1,421 instances. 
<br class="ltx_break"><span id="A1.SS1.p1.1.9" class="ltx_text ltx_font_bold">Sarcasm News Headlines:</span> This task requires distinguishing between sarcastic and non-sarcastic news headlines. The dataset comprises 26,709 headlines from two news sources: TheOnion, representing sarcasm, and HuffPost, representing non-sarcasm. As the dataset does not provide an official division into train, validation, and test sets, we randomly allocated the dataset into train (70%), validation (5%), and test (25%) sets. In our evaluation, this process was repeated three times, with the average performance reported. 
<br class="ltx_break"><span id="A1.SS1.p1.1.10" class="ltx_text ltx_font_bold">Humor Speech Detection:</span> This task involves discerning humorous from non-humorous content for short texts. The dataset, specifically curated for humor detection, is composed of 200,000 instances, balanced between humorous and non-humorous data. It is divided into a training set of 160,000 instances and a test set of 40,000 instances.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Evaluation I: Comparison Across Different Types of Tasks (Additional Results)</h2>

<figure id="A2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/agnews.png" id="A2.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="120" height="83" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>AG’s News</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/fewrel.png" id="A2.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="120" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Relation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/imdb.png" id="A2.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="120" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>IMDB Reviews</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/spam.png" id="A2.F1.sf4.g1" class="ltx_graphics ltx_img_landscape" width="120" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>SMS Spam</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/financial.png" id="A2.F1.sf5.g1" class="ltx_graphics ltx_img_landscape" width="120" height="83" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Financial Phrasebank</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/goemotions.png" id="A2.F1.sf6.g1" class="ltx_graphics ltx_img_landscape" width="120" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Reddit Emotion</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/sarcasm.png" id="A2.F1.sf7.g1" class="ltx_graphics ltx_img_landscape" width="120" height="86" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>Sarcasm News</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/humor.png" id="A2.F1.sf8.g1" class="ltx_graphics ltx_img_landscape" width="120" height="86" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>Humor Detection</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/tweetemotion.png" id="A2.F1.sf9.g1" class="ltx_graphics ltx_img_landscape" width="120" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(i) </span>Tweet Emotions</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A2.F1.sf10" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/tweet_irony.png" id="A2.F1.sf10.g1" class="ltx_graphics ltx_img_landscape" width="120" height="86" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(j) </span>Tweet Irony Speech</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure B.1: </span>The training curves for classification models trained with the real-world data, the zero-shot synthetic data, and the few-shot synthetic data. </figcaption>
</figure>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Convergence Analysis</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">Figure <a href="#A2.F1" title="Figure B.1 ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a> illustrates the training curves of classification models across the 10 types of tasks. We find that compared to the training curves derived from the real-world data, models trained on the synthetic data exhibit a faster convergence rate and a greater propensity to overfit.
This indicates that under both zero-shot and few-shot settings, the synthetic data generated by the LLM may lack a degree of diversity and falls short in fully capturing the complex patterns found in the real world language contexts.</p>
</div>
<figure id="A2.T1" class="ltx_table">
<div id="A2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:99.9pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-293.0pt,67.2pt) scale(0.425278893546846,0.425278893546846) ;">
<table id="A2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T1.1.1.1.1" class="ltx_tr">
<td id="A2.T1.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="A2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="A2.T1.1.1.1.1.2" class="ltx_td ltx_align_left"><span id="A2.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">BERT</span></td>
<td id="A2.T1.1.1.1.1.3" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.4" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.5" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.6" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.7" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.8" class="ltx_td ltx_align_right"><span id="A2.T1.1.1.1.1.8.1" class="ltx_text ltx_font_bold">RoBERTa</span></td>
<td id="A2.T1.1.1.1.1.9" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.10" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.11" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.12" class="ltx_td"></td>
<td id="A2.T1.1.1.1.1.13" class="ltx_td"></td>
</tr>
<tr id="A2.T1.1.1.2.2" class="ltx_tr">
<td id="A2.T1.1.1.2.2.1" class="ltx_td"></td>
<td id="A2.T1.1.1.2.2.2" class="ltx_td ltx_align_left">real</td>
<td id="A2.T1.1.1.2.2.3" class="ltx_td"></td>
<td id="A2.T1.1.1.2.2.4" class="ltx_td ltx_align_center">synthetic</td>
<td id="A2.T1.1.1.2.2.5" class="ltx_td"></td>
<td id="A2.T1.1.1.2.2.6" class="ltx_td ltx_align_left">real + synthetic</td>
<td id="A2.T1.1.1.2.2.7" class="ltx_td"></td>
<td id="A2.T1.1.1.2.2.8" class="ltx_td ltx_align_right">real</td>
<td id="A2.T1.1.1.2.2.9" class="ltx_td"></td>
<td id="A2.T1.1.1.2.2.10" class="ltx_td ltx_align_left">synthetic</td>
<td id="A2.T1.1.1.2.2.11" class="ltx_td"></td>
<td id="A2.T1.1.1.2.2.12" class="ltx_td ltx_align_center">real+ synthetic</td>
<td id="A2.T1.1.1.2.2.13" class="ltx_td"></td>
</tr>
<tr id="A2.T1.1.1.3.3" class="ltx_tr">
<td id="A2.T1.1.1.3.3.1" class="ltx_td"></td>
<td id="A2.T1.1.1.3.3.2" class="ltx_td ltx_align_left">Macro-F1</td>
<td id="A2.T1.1.1.3.3.3" class="ltx_td ltx_align_left">Accuracy Score</td>
<td id="A2.T1.1.1.3.3.4" class="ltx_td ltx_align_center">Macro-F1</td>
<td id="A2.T1.1.1.3.3.5" class="ltx_td ltx_align_center">Accuracy Score</td>
<td id="A2.T1.1.1.3.3.6" class="ltx_td ltx_align_left">Macro-F1</td>
<td id="A2.T1.1.1.3.3.7" class="ltx_td ltx_align_left">Accuracy Score</td>
<td id="A2.T1.1.1.3.3.8" class="ltx_td ltx_align_right">Macro-F1</td>
<td id="A2.T1.1.1.3.3.9" class="ltx_td ltx_align_center">Accuracy Score</td>
<td id="A2.T1.1.1.3.3.10" class="ltx_td ltx_align_left">Macro-F1</td>
<td id="A2.T1.1.1.3.3.11" class="ltx_td ltx_align_left">Accuracy Score</td>
<td id="A2.T1.1.1.3.3.12" class="ltx_td ltx_align_center">Macro-F1</td>
<td id="A2.T1.1.1.3.3.13" class="ltx_td ltx_align_center">Accuracy Score</td>
</tr>
<tr id="A2.T1.1.1.4.4" class="ltx_tr">
<td id="A2.T1.1.1.4.4.1" class="ltx_td ltx_align_center">AG</td>
<td id="A2.T1.1.1.4.4.2" class="ltx_td ltx_align_left">93.1%</td>
<td id="A2.T1.1.1.4.4.3" class="ltx_td ltx_align_left">93.2%</td>
<td id="A2.T1.1.1.4.4.4" class="ltx_td ltx_align_center">91.5% <span id="A2.T1.1.1.4.4.4.1" class="ltx_text" style="color:#ED2B2A;">(-1.6%)</span>
</td>
<td id="A2.T1.1.1.4.4.5" class="ltx_td ltx_align_center">91.6% <span id="A2.T1.1.1.4.4.5.1" class="ltx_text" style="color:#ED2B2A;">(-1.6%)</span>
</td>
<td id="A2.T1.1.1.4.4.6" class="ltx_td ltx_align_left">93.1% <span id="A2.T1.1.1.4.4.6.1" class="ltx_text" style="color:#03C988;">(+0.0%)</span>
</td>
<td id="A2.T1.1.1.4.4.7" class="ltx_td ltx_align_left">93.1% <span id="A2.T1.1.1.4.4.7.1" class="ltx_text" style="color:#ED2B2A;">(-0.1%)</span>
</td>
<td id="A2.T1.1.1.4.4.8" class="ltx_td ltx_align_right">93.6%</td>
<td id="A2.T1.1.1.4.4.9" class="ltx_td ltx_align_center">93.6%</td>
<td id="A2.T1.1.1.4.4.10" class="ltx_td ltx_align_left">92.9% <span id="A2.T1.1.1.4.4.10.1" class="ltx_text" style="color:#ED2B2A;">(-0.7%)</span>
</td>
<td id="A2.T1.1.1.4.4.11" class="ltx_td ltx_align_left">92.9% <span id="A2.T1.1.1.4.4.11.1" class="ltx_text" style="color:#ED2B2A;">(-0.7%)</span>
</td>
<td id="A2.T1.1.1.4.4.12" class="ltx_td ltx_align_center">93.4% <span id="A2.T1.1.1.4.4.12.1" class="ltx_text" style="color:#ED2B2A;">(-0.2%)</span>
</td>
<td id="A2.T1.1.1.4.4.13" class="ltx_td ltx_align_center">93.5% <span id="A2.T1.1.1.4.4.13.1" class="ltx_text" style="color:#ED2B2A;">(-0.1%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.5.5" class="ltx_tr">
<td id="A2.T1.1.1.5.5.1" class="ltx_td ltx_align_center">Relation</td>
<td id="A2.T1.1.1.5.5.2" class="ltx_td ltx_align_left">96.8%</td>
<td id="A2.T1.1.1.5.5.3" class="ltx_td ltx_align_left">96.8%</td>
<td id="A2.T1.1.1.5.5.4" class="ltx_td ltx_align_center">96.4% <span id="A2.T1.1.1.5.5.4.1" class="ltx_text" style="color:#ED2B2A;">(-0.4%)</span>
</td>
<td id="A2.T1.1.1.5.5.5" class="ltx_td ltx_align_center">96.4% <span id="A2.T1.1.1.5.5.5.1" class="ltx_text" style="color:#ED2B2A;">(-0.4%)</span>
</td>
<td id="A2.T1.1.1.5.5.6" class="ltx_td ltx_align_left">96.7% <span id="A2.T1.1.1.5.5.6.1" class="ltx_text" style="color:#ED2B2A;">(-0.1%)</span>
</td>
<td id="A2.T1.1.1.5.5.7" class="ltx_td ltx_align_left">96.8% <span id="A2.T1.1.1.5.5.7.1" class="ltx_text" style="color:#03C988;">(+0.0%)</span>
</td>
<td id="A2.T1.1.1.5.5.8" class="ltx_td ltx_align_right">97.6%</td>
<td id="A2.T1.1.1.5.5.9" class="ltx_td ltx_align_center">97.6%</td>
<td id="A2.T1.1.1.5.5.10" class="ltx_td ltx_align_left">94.1% <span id="A2.T1.1.1.5.5.10.1" class="ltx_text" style="color:#ED2B2A;">(-3.5%)</span>
</td>
<td id="A2.T1.1.1.5.5.11" class="ltx_td ltx_align_left">94.1% <span id="A2.T1.1.1.5.5.11.1" class="ltx_text" style="color:#ED2B2A;">(-3.5%)</span>
</td>
<td id="A2.T1.1.1.5.5.12" class="ltx_td ltx_align_center">97.1% <span id="A2.T1.1.1.5.5.12.1" class="ltx_text" style="color:#ED2B2A;">(-0.5%)</span>
</td>
<td id="A2.T1.1.1.5.5.13" class="ltx_td ltx_align_center">97.3% <span id="A2.T1.1.1.5.5.13.1" class="ltx_text" style="color:#ED2B2A;">(-0.3%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.6.6" class="ltx_tr">
<td id="A2.T1.1.1.6.6.1" class="ltx_td ltx_align_center">IMDB</td>
<td id="A2.T1.1.1.6.6.2" class="ltx_td ltx_align_left">77.4%</td>
<td id="A2.T1.1.1.6.6.3" class="ltx_td ltx_align_left">78.6%</td>
<td id="A2.T1.1.1.6.6.4" class="ltx_td ltx_align_center">81.1% <span id="A2.T1.1.1.6.6.4.1" class="ltx_text" style="color:#03C988;">(+3.7%)</span>
</td>
<td id="A2.T1.1.1.6.6.5" class="ltx_td ltx_align_center">81.2% <span id="A2.T1.1.1.6.6.5.1" class="ltx_text" style="color:#03C988;">(+2.6%)</span>
</td>
<td id="A2.T1.1.1.6.6.6" class="ltx_td ltx_align_left">80.2% <span id="A2.T1.1.1.6.6.6.1" class="ltx_text" style="color:#03C988;">(+2.8%)</span>
</td>
<td id="A2.T1.1.1.6.6.7" class="ltx_td ltx_align_left">80.1% <span id="A2.T1.1.1.6.6.7.1" class="ltx_text" style="color:#03C988;">(+1.5%)</span>
</td>
<td id="A2.T1.1.1.6.6.8" class="ltx_td ltx_align_right">75.7%</td>
<td id="A2.T1.1.1.6.6.9" class="ltx_td ltx_align_center">76.1%</td>
<td id="A2.T1.1.1.6.6.10" class="ltx_td ltx_align_left">82.4% <span id="A2.T1.1.1.6.6.10.1" class="ltx_text" style="color:#03C988;">(+6.7%)</span>
</td>
<td id="A2.T1.1.1.6.6.11" class="ltx_td ltx_align_left">82.4% <span id="A2.T1.1.1.6.6.11.1" class="ltx_text" style="color:#03C988;">(+6.3%)</span>
</td>
<td id="A2.T1.1.1.6.6.12" class="ltx_td ltx_align_center">81.0% <span id="A2.T1.1.1.6.6.12.1" class="ltx_text" style="color:#03C988;">(+5.3%)</span>
</td>
<td id="A2.T1.1.1.6.6.13" class="ltx_td ltx_align_center">81.1% <span id="A2.T1.1.1.6.6.13.1" class="ltx_text" style="color:#03C988;">(+5.0%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.7.7" class="ltx_tr">
<td id="A2.T1.1.1.7.7.1" class="ltx_td ltx_align_center">SMS Spam</td>
<td id="A2.T1.1.1.7.7.2" class="ltx_td ltx_align_left">98.2%</td>
<td id="A2.T1.1.1.7.7.3" class="ltx_td ltx_align_left">98.2%</td>
<td id="A2.T1.1.1.7.7.4" class="ltx_td ltx_align_center">94.3% <span id="A2.T1.1.1.7.7.4.1" class="ltx_text" style="color:#ED2B2A;">(-3.9%)</span>
</td>
<td id="A2.T1.1.1.7.7.5" class="ltx_td ltx_align_center">94.8% <span id="A2.T1.1.1.7.7.5.1" class="ltx_text" style="color:#ED2B2A;">(-3.4%)</span>
</td>
<td id="A2.T1.1.1.7.7.6" class="ltx_td ltx_align_left">98.1% <span id="A2.T1.1.1.7.7.6.1" class="ltx_text" style="color:#ED2B2A;">(-0.1%)</span>
</td>
<td id="A2.T1.1.1.7.7.7" class="ltx_td ltx_align_left">98.2% <span id="A2.T1.1.1.7.7.7.1" class="ltx_text" style="color:#03C988;">(+0.0%)</span>
</td>
<td id="A2.T1.1.1.7.7.8" class="ltx_td ltx_align_right">98.1%</td>
<td id="A2.T1.1.1.7.7.9" class="ltx_td ltx_align_center">98.1%</td>
<td id="A2.T1.1.1.7.7.10" class="ltx_td ltx_align_left">94.0% <span id="A2.T1.1.1.7.7.10.1" class="ltx_text" style="color:#ED2B2A;">(-4.1%)</span>
</td>
<td id="A2.T1.1.1.7.7.11" class="ltx_td ltx_align_left">95.7% <span id="A2.T1.1.1.7.7.11.1" class="ltx_text" style="color:#ED2B2A;">(-2.4%)</span>
</td>
<td id="A2.T1.1.1.7.7.12" class="ltx_td ltx_align_center">98.1% <span id="A2.T1.1.1.7.7.12.1" class="ltx_text" style="color:#03C988;">(+0.0%)</span>
</td>
<td id="A2.T1.1.1.7.7.13" class="ltx_td ltx_align_center">98.1% <span id="A2.T1.1.1.7.7.13.1" class="ltx_text" style="color:#03C988;">(+0.0%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.8.8" class="ltx_tr">
<td id="A2.T1.1.1.8.8.1" class="ltx_td ltx_align_center">Reddit Emotion</td>
<td id="A2.T1.1.1.8.8.2" class="ltx_td ltx_align_left">92.5%</td>
<td id="A2.T1.1.1.8.8.3" class="ltx_td ltx_align_left">92.5%</td>
<td id="A2.T1.1.1.8.8.4" class="ltx_td ltx_align_center">81.9% <span id="A2.T1.1.1.8.8.4.1" class="ltx_text" style="color:#ED2B2A;">(-10.6%)</span>
</td>
<td id="A2.T1.1.1.8.8.5" class="ltx_td ltx_align_center">82.0% <span id="A2.T1.1.1.8.8.5.1" class="ltx_text" style="color:#ED2B2A;">(-10.5%)</span>
</td>
<td id="A2.T1.1.1.8.8.6" class="ltx_td ltx_align_left">91.8% <span id="A2.T1.1.1.8.8.6.1" class="ltx_text" style="color:#ED2B2A;">(-0.7%)</span>
</td>
<td id="A2.T1.1.1.8.8.7" class="ltx_td ltx_align_left">91.8% <span id="A2.T1.1.1.8.8.7.1" class="ltx_text" style="color:#ED2B2A;">(-0.7%)</span>
</td>
<td id="A2.T1.1.1.8.8.8" class="ltx_td ltx_align_right">91.7%</td>
<td id="A2.T1.1.1.8.8.9" class="ltx_td ltx_align_center">91.8%</td>
<td id="A2.T1.1.1.8.8.10" class="ltx_td ltx_align_left">87.5% <span id="A2.T1.1.1.8.8.10.1" class="ltx_text" style="color:#ED2B2A;">(-4.2%)</span>
</td>
<td id="A2.T1.1.1.8.8.11" class="ltx_td ltx_align_left">87.7% <span id="A2.T1.1.1.8.8.11.1" class="ltx_text" style="color:#ED2B2A;">(-4.1%)</span>
</td>
<td id="A2.T1.1.1.8.8.12" class="ltx_td ltx_align_center">90.4% <span id="A2.T1.1.1.8.8.12.1" class="ltx_text" style="color:#ED2B2A;">(-1.3%)</span>
</td>
<td id="A2.T1.1.1.8.8.13" class="ltx_td ltx_align_center">90.8% <span id="A2.T1.1.1.8.8.13.1" class="ltx_text" style="color:#ED2B2A;">(-1.0%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.9.9" class="ltx_tr">
<td id="A2.T1.1.1.9.9.1" class="ltx_td ltx_align_center">Tweet Irony</td>
<td id="A2.T1.1.1.9.9.2" class="ltx_td ltx_align_left">67.3%</td>
<td id="A2.T1.1.1.9.9.3" class="ltx_td ltx_align_left">68.2%</td>
<td id="A2.T1.1.1.9.9.4" class="ltx_td ltx_align_center">81.5% <span id="A2.T1.1.1.9.9.4.1" class="ltx_text" style="color:#03C988;">(+14.2%)</span>
</td>
<td id="A2.T1.1.1.9.9.5" class="ltx_td ltx_align_center">81.9% <span id="A2.T1.1.1.9.9.5.1" class="ltx_text" style="color:#03C988;">(+13.7%)</span>
</td>
<td id="A2.T1.1.1.9.9.6" class="ltx_td ltx_align_left">81.2% <span id="A2.T1.1.1.9.9.6.1" class="ltx_text" style="color:#03C988;">(+13.9%)</span>
</td>
<td id="A2.T1.1.1.9.9.7" class="ltx_td ltx_align_left">81.5% <span id="A2.T1.1.1.9.9.7.1" class="ltx_text" style="color:#03C988;">(+13.3%)</span>
</td>
<td id="A2.T1.1.1.9.9.8" class="ltx_td ltx_align_right">66.4%</td>
<td id="A2.T1.1.1.9.9.9" class="ltx_td ltx_align_center">67.2%</td>
<td id="A2.T1.1.1.9.9.10" class="ltx_td ltx_align_left">83.3% <span id="A2.T1.1.1.9.9.10.1" class="ltx_text" style="color:#03C988;">(+16.9%)</span>
</td>
<td id="A2.T1.1.1.9.9.11" class="ltx_td ltx_align_left">83.7% <span id="A2.T1.1.1.9.9.11.1" class="ltx_text" style="color:#03C988;">(+16.5%)</span>
</td>
<td id="A2.T1.1.1.9.9.12" class="ltx_td ltx_align_center">80.8% <span id="A2.T1.1.1.9.9.12.1" class="ltx_text" style="color:#03C988;">(+14.4%)</span>
</td>
<td id="A2.T1.1.1.9.9.13" class="ltx_td ltx_align_center">81.3% <span id="A2.T1.1.1.9.9.13.1" class="ltx_text" style="color:#03C988;">(+14.1%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.10.10" class="ltx_tr">
<td id="A2.T1.1.1.10.10.1" class="ltx_td ltx_align_center">Tweet Emotion</td>
<td id="A2.T1.1.1.10.10.2" class="ltx_td ltx_align_left">64.5%</td>
<td id="A2.T1.1.1.10.10.3" class="ltx_td ltx_align_left">64.5%</td>
<td id="A2.T1.1.1.10.10.4" class="ltx_td ltx_align_center">64.6% <span id="A2.T1.1.1.10.10.4.1" class="ltx_text" style="color:#03C988;">(+0.1%)</span>
</td>
<td id="A2.T1.1.1.10.10.5" class="ltx_td ltx_align_center">69.1% <span id="A2.T1.1.1.10.10.5.1" class="ltx_text" style="color:#03C988;">(+4.6%)</span>
</td>
<td id="A2.T1.1.1.10.10.6" class="ltx_td ltx_align_left">70.4% <span id="A2.T1.1.1.10.10.6.1" class="ltx_text" style="color:#03C988;">(+5.9%)</span>
</td>
<td id="A2.T1.1.1.10.10.7" class="ltx_td ltx_align_left">70.5% <span id="A2.T1.1.1.10.10.7.1" class="ltx_text" style="color:#03C988;">(+6.0%)</span>
</td>
<td id="A2.T1.1.1.10.10.8" class="ltx_td ltx_align_right">72.2%</td>
<td id="A2.T1.1.1.10.10.9" class="ltx_td ltx_align_center">72.5%</td>
<td id="A2.T1.1.1.10.10.10" class="ltx_td ltx_align_left">66.3% <span id="A2.T1.1.1.10.10.10.1" class="ltx_text" style="color:#ED2B2A;">(-5.9%)</span>
</td>
<td id="A2.T1.1.1.10.10.11" class="ltx_td ltx_align_left">72.7% <span id="A2.T1.1.1.10.10.11.1" class="ltx_text" style="color:#03C988;">(+0.2%)</span>
</td>
<td id="A2.T1.1.1.10.10.12" class="ltx_td ltx_align_center">73.4% <span id="A2.T1.1.1.10.10.12.1" class="ltx_text" style="color:#03C988;">(+1.2%)</span>
</td>
<td id="A2.T1.1.1.10.10.13" class="ltx_td ltx_align_center">73.5% <span id="A2.T1.1.1.10.10.13.1" class="ltx_text" style="color:#03C988;">(+1.0%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.11.11" class="ltx_tr">
<td id="A2.T1.1.1.11.11.1" class="ltx_td ltx_align_center">Sarcasm</td>
<td id="A2.T1.1.1.11.11.2" class="ltx_td ltx_align_left">76.1%</td>
<td id="A2.T1.1.1.11.11.3" class="ltx_td ltx_align_left">78.3%</td>
<td id="A2.T1.1.1.11.11.4" class="ltx_td ltx_align_center">63.6% <span id="A2.T1.1.1.11.11.4.1" class="ltx_text" style="color:#ED2B2A;">(-12.5%)</span>
</td>
<td id="A2.T1.1.1.11.11.5" class="ltx_td ltx_align_center">64.8% <span id="A2.T1.1.1.11.11.5.1" class="ltx_text" style="color:#ED2B2A;">(-13.5%)</span>
</td>
<td id="A2.T1.1.1.11.11.6" class="ltx_td ltx_align_left">77.5% <span id="A2.T1.1.1.11.11.6.1" class="ltx_text" style="color:#03C988;">(+1.4%)</span>
</td>
<td id="A2.T1.1.1.11.11.7" class="ltx_td ltx_align_left">76.4% <span id="A2.T1.1.1.11.11.7.1" class="ltx_text" style="color:#ED2B2A;">(-1.9%)</span>
</td>
<td id="A2.T1.1.1.11.11.8" class="ltx_td ltx_align_right">72.4%</td>
<td id="A2.T1.1.1.11.11.9" class="ltx_td ltx_align_center">72.5%</td>
<td id="A2.T1.1.1.11.11.10" class="ltx_td ltx_align_left">61.5% <span id="A2.T1.1.1.11.11.10.1" class="ltx_text" style="color:#ED2B2A;">(-10.9%)</span>
</td>
<td id="A2.T1.1.1.11.11.11" class="ltx_td ltx_align_left">63.6% <span id="A2.T1.1.1.11.11.11.1" class="ltx_text" style="color:#ED2B2A;">(-8.9%)</span>
</td>
<td id="A2.T1.1.1.11.11.12" class="ltx_td ltx_align_center">72.9% <span id="A2.T1.1.1.11.11.12.1" class="ltx_text" style="color:#03C988;">(+0.5%)</span>
</td>
<td id="A2.T1.1.1.11.11.13" class="ltx_td ltx_align_center">73.2% <span id="A2.T1.1.1.11.11.13.1" class="ltx_text" style="color:#03C988;">(+0.7%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.12.12" class="ltx_tr">
<td id="A2.T1.1.1.12.12.1" class="ltx_td ltx_align_center">Financial</td>
<td id="A2.T1.1.1.12.12.2" class="ltx_td ltx_align_left">72.5%</td>
<td id="A2.T1.1.1.12.12.3" class="ltx_td ltx_align_left">75.1%</td>
<td id="A2.T1.1.1.12.12.4" class="ltx_td ltx_align_center">70.6% <span id="A2.T1.1.1.12.12.4.1" class="ltx_text" style="color:#ED2B2A;">(-1.9%)</span>
</td>
<td id="A2.T1.1.1.12.12.5" class="ltx_td ltx_align_center">74.2% <span id="A2.T1.1.1.12.12.5.1" class="ltx_text" style="color:#ED2B2A;">(-0.9%)</span>
</td>
<td id="A2.T1.1.1.12.12.6" class="ltx_td ltx_align_left">74.6% <span id="A2.T1.1.1.12.12.6.1" class="ltx_text" style="color:#03C988;">(+2.1%)</span>
</td>
<td id="A2.T1.1.1.12.12.7" class="ltx_td ltx_align_left">76.3% <span id="A2.T1.1.1.12.12.7.1" class="ltx_text" style="color:#03C988;">(+1.2%)</span>
</td>
<td id="A2.T1.1.1.12.12.8" class="ltx_td ltx_align_right">76.9%</td>
<td id="A2.T1.1.1.12.12.9" class="ltx_td ltx_align_center">78.2%</td>
<td id="A2.T1.1.1.12.12.10" class="ltx_td ltx_align_left">75.0% <span id="A2.T1.1.1.12.12.10.1" class="ltx_text" style="color:#ED2B2A;">(-1.9%)</span>
</td>
<td id="A2.T1.1.1.12.12.11" class="ltx_td ltx_align_left">78.9% <span id="A2.T1.1.1.12.12.11.1" class="ltx_text" style="color:#03C988;">(+0.7%)</span>
</td>
<td id="A2.T1.1.1.12.12.12" class="ltx_td ltx_align_center">78.4% <span id="A2.T1.1.1.12.12.12.1" class="ltx_text" style="color:#03C988;">(+1.5%)</span>
</td>
<td id="A2.T1.1.1.12.12.13" class="ltx_td ltx_align_center">80.1% <span id="A2.T1.1.1.12.12.13.1" class="ltx_text" style="color:#03C988;">(+1.9%)</span>
</td>
</tr>
<tr id="A2.T1.1.1.13.13" class="ltx_tr">
<td id="A2.T1.1.1.13.13.1" class="ltx_td ltx_align_center">Humor Speech</td>
<td id="A2.T1.1.1.13.13.2" class="ltx_td ltx_align_left">94.8%</td>
<td id="A2.T1.1.1.13.13.3" class="ltx_td ltx_align_left">94.7%</td>
<td id="A2.T1.1.1.13.13.4" class="ltx_td ltx_align_center">86.9% <span id="A2.T1.1.1.13.13.4.1" class="ltx_text" style="color:#ED2B2A;">(-7.9%)</span>
</td>
<td id="A2.T1.1.1.13.13.5" class="ltx_td ltx_align_center">87.0% <span id="A2.T1.1.1.13.13.5.1" class="ltx_text" style="color:#ED2B2A;">(-7.7%)</span>
</td>
<td id="A2.T1.1.1.13.13.6" class="ltx_td ltx_align_left">93.3% <span id="A2.T1.1.1.13.13.6.1" class="ltx_text" style="color:#ED2B2A;">(-1.5%)</span>
</td>
<td id="A2.T1.1.1.13.13.7" class="ltx_td ltx_align_left">93.3% <span id="A2.T1.1.1.13.13.7.1" class="ltx_text" style="color:#ED2B2A;">(-1.4%)</span>
</td>
<td id="A2.T1.1.1.13.13.8" class="ltx_td ltx_align_right">95.3%</td>
<td id="A2.T1.1.1.13.13.9" class="ltx_td ltx_align_center">95.3%</td>
<td id="A2.T1.1.1.13.13.10" class="ltx_td ltx_align_left">84.0% <span id="A2.T1.1.1.13.13.10.1" class="ltx_text" style="color:#ED2B2A;">(-11.3%)</span>
</td>
<td id="A2.T1.1.1.13.13.11" class="ltx_td ltx_align_left">84.0% <span id="A2.T1.1.1.13.13.11.1" class="ltx_text" style="color:#ED2B2A;">(-11.3%)</span>
</td>
<td id="A2.T1.1.1.13.13.12" class="ltx_td ltx_align_center">94.6% <span id="A2.T1.1.1.13.13.12.1" class="ltx_text" style="color:#ED2B2A;">(-0.7%)</span>
</td>
<td id="A2.T1.1.1.13.13.13" class="ltx_td ltx_align_center">94.6% <span id="A2.T1.1.1.13.13.13.1" class="ltx_text" style="color:#ED2B2A;">(-0.7%)</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table B.1: </span>Comparing the performance of classification models trained using three types of data: a small amount of the real-world data used as the examples for guiding LLM in synthetic data generation (i.e., “real”), few-shot synthetic data generated by the LLM (i.e., “synthetic”), and a combination of both (“real+synthetic”). The performance is measured in terms of Macro-F1 (%) and Accuracy Score (%).</figcaption>
</figure>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Potential of Few-shot Synthetic Data for Data Augmentation</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">In the main text, the model performance we report for the “few-shot synthetic data” are based on models that are trained only on the synthetic data. As we assume that a small amount of real-world data are available under the few-shot data generation setting, a natural question to ask is whether the few-shot synthetic data can be used to augment the real-world data (which are used as the examples in the synthetic data generation process) and improve the model performance.
Answering this question, Table <a href="#A2.T1" title="Table B.1 ‣ B.1 Convergence Analysis ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a> compares the performance of classification models trained only on the limited set of real-world data (i.e., those used as example to guide LLM in generating synthetic data), only on the few-shot synthetic data generated, and on the combination of both data.
We find that the comparison between the performance of models trained exclusively on the limited real-world data and models trained exclusively on few-shot synthetic data is task-dependent.
However, when the few-shot synthetic data is combined with the small set of real-world data, the resulting model can outperform the model trained only on the real-world data for many tasks. This highlights the potential of the few-shot synthetic data for data augmentation.</p>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Similarity between the Synthetic Data and the Real Data</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p">In the few-shot setting, we utilized real-world data examples to guide the generation of synthetic data. To quantify the similarity between the real-world data examples and the few-shot synthetic data generated,
we employed a pre-trained Sentence Transformer model <cite class="ltx_cite ltx_citemacro_cite">all MiniLM-L6-v2 (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite> to convert texts into vector embeddings. We then computed the cosine similarity between the embeddings of real-world examples and the embeddings of the the synthetic texts.
The consine similarity metric ranges from -1 to 1, and we rescaled it to the interval of [0, 1], with 1 representing the highest level of similarity.
Then, for each real-world example, we obtained its mean similarity with the top 5 most similar synthetic texts in the synthetic data and then computed the average mean similarity scores across all real-world examples within each type of classification tasks. As a reference, we also conducted the same computation between the real-world examples and the synthetic data generated under the zero-shot settings,
and results of the similarity comparisons are shown in Figure <a href="#A2.F2" title="Figure B.2 ‣ B.3 Similarity between the Synthetic Data and the Real Data ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>.</p>
</div>
<figure id="A2.F2" class="ltx_figure"><img src="/html/2310.07849/assets/figures/sim.png" id="A2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="287" height="243" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure B.2: </span>Average top 5 cosine similarity between the real and synthetic data</figcaption>
</figure>
<div id="A2.SS3.p2" class="ltx_para">
<p id="A2.SS3.p2.1" class="ltx_p">Visually, we find a consistent trend that the few-shot synthetic data has a higher level of similarity with the real-world examples compared to the zero-shot synthetic data. We then performed t-tests on each classification task to determine whether the difference of the average cosine similarity scores for the zero-shot and few-shot
synthetic data is significant. The results are shown in Table <a href="#A2.T2" title="Table B.2 ‣ B.3 Similarity between the Synthetic Data and the Real Data ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>, which indicates that the difference is statistically significant for all but the IMDB review classification task.
In other words, the few-shot synthetic data is more similar to the real-world data than the zero-shot synthetic data, which may partly explains why models trained on the few-shot synthetic data tend to outperform models trained on the zero-shot synthetic data.</p>
</div>
<figure id="A2.T2" class="ltx_table">
<table id="A2.T2.10" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T2.10.11.1" class="ltx_tr">
<th id="A2.T2.10.11.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="A2.T2.10.11.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="A2.T2.10.11.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="A2.T2.10.11.1.2.1" class="ltx_text ltx_font_bold">p-value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T2.1.1" class="ltx_tr">
<th id="A2.T2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">AG News</th>
<td id="A2.T2.1.1.1" class="ltx_td ltx_align_left"><math id="A2.T2.1.1.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.1.1.1.m1.1a"><mrow id="A2.T2.1.1.1.m1.1.1" xref="A2.T2.1.1.1.m1.1.1.cmml"><mi id="A2.T2.1.1.1.m1.1.1.2" xref="A2.T2.1.1.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.1.1.1.m1.1.1.1" xref="A2.T2.1.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.1.1.1.m1.1.1.3" xref="A2.T2.1.1.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.1.1.1.m1.1b"><apply id="A2.T2.1.1.1.m1.1.1.cmml" xref="A2.T2.1.1.1.m1.1.1"><lt id="A2.T2.1.1.1.m1.1.1.1.cmml" xref="A2.T2.1.1.1.m1.1.1.1"></lt><ci id="A2.T2.1.1.1.m1.1.1.2.cmml" xref="A2.T2.1.1.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.1.1.1.m1.1.1.3.cmml" xref="A2.T2.1.1.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.1.1.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.2.2" class="ltx_tr">
<th id="A2.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Relation</th>
<td id="A2.T2.2.2.1" class="ltx_td ltx_align_left"><math id="A2.T2.2.2.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.2.2.1.m1.1a"><mrow id="A2.T2.2.2.1.m1.1.1" xref="A2.T2.2.2.1.m1.1.1.cmml"><mi id="A2.T2.2.2.1.m1.1.1.2" xref="A2.T2.2.2.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.2.2.1.m1.1.1.1" xref="A2.T2.2.2.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.2.2.1.m1.1.1.3" xref="A2.T2.2.2.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.2.2.1.m1.1b"><apply id="A2.T2.2.2.1.m1.1.1.cmml" xref="A2.T2.2.2.1.m1.1.1"><lt id="A2.T2.2.2.1.m1.1.1.1.cmml" xref="A2.T2.2.2.1.m1.1.1.1"></lt><ci id="A2.T2.2.2.1.m1.1.1.2.cmml" xref="A2.T2.2.2.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.2.2.1.m1.1.1.3.cmml" xref="A2.T2.2.2.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.2.2.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.3.3" class="ltx_tr">
<th id="A2.T2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">IMDB</th>
<td id="A2.T2.3.3.1" class="ltx_td ltx_align_left"><math id="A2.T2.3.3.1.m1.1" class="ltx_Math" alttext="p&lt;0.1" display="inline"><semantics id="A2.T2.3.3.1.m1.1a"><mrow id="A2.T2.3.3.1.m1.1.1" xref="A2.T2.3.3.1.m1.1.1.cmml"><mi id="A2.T2.3.3.1.m1.1.1.2" xref="A2.T2.3.3.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.3.3.1.m1.1.1.1" xref="A2.T2.3.3.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.3.3.1.m1.1.1.3" xref="A2.T2.3.3.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.3.3.1.m1.1b"><apply id="A2.T2.3.3.1.m1.1.1.cmml" xref="A2.T2.3.3.1.m1.1.1"><lt id="A2.T2.3.3.1.m1.1.1.1.cmml" xref="A2.T2.3.3.1.m1.1.1.1"></lt><ci id="A2.T2.3.3.1.m1.1.1.2.cmml" xref="A2.T2.3.3.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.3.3.1.m1.1.1.3.cmml" xref="A2.T2.3.3.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.3.3.1.m1.1c">p&lt;0.1</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.4.4" class="ltx_tr">
<th id="A2.T2.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Spam</th>
<td id="A2.T2.4.4.1" class="ltx_td ltx_align_left"><math id="A2.T2.4.4.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.4.4.1.m1.1a"><mrow id="A2.T2.4.4.1.m1.1.1" xref="A2.T2.4.4.1.m1.1.1.cmml"><mi id="A2.T2.4.4.1.m1.1.1.2" xref="A2.T2.4.4.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.4.4.1.m1.1.1.1" xref="A2.T2.4.4.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.4.4.1.m1.1.1.3" xref="A2.T2.4.4.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.4.4.1.m1.1b"><apply id="A2.T2.4.4.1.m1.1.1.cmml" xref="A2.T2.4.4.1.m1.1.1"><lt id="A2.T2.4.4.1.m1.1.1.1.cmml" xref="A2.T2.4.4.1.m1.1.1.1"></lt><ci id="A2.T2.4.4.1.m1.1.1.2.cmml" xref="A2.T2.4.4.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.4.4.1.m1.1.1.3.cmml" xref="A2.T2.4.4.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.4.4.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.5.5" class="ltx_tr">
<th id="A2.T2.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Financial</th>
<td id="A2.T2.5.5.1" class="ltx_td ltx_align_left"><math id="A2.T2.5.5.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.5.5.1.m1.1a"><mrow id="A2.T2.5.5.1.m1.1.1" xref="A2.T2.5.5.1.m1.1.1.cmml"><mi id="A2.T2.5.5.1.m1.1.1.2" xref="A2.T2.5.5.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.5.5.1.m1.1.1.1" xref="A2.T2.5.5.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.5.5.1.m1.1.1.3" xref="A2.T2.5.5.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.5.5.1.m1.1b"><apply id="A2.T2.5.5.1.m1.1.1.cmml" xref="A2.T2.5.5.1.m1.1.1"><lt id="A2.T2.5.5.1.m1.1.1.1.cmml" xref="A2.T2.5.5.1.m1.1.1.1"></lt><ci id="A2.T2.5.5.1.m1.1.1.2.cmml" xref="A2.T2.5.5.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.5.5.1.m1.1.1.3.cmml" xref="A2.T2.5.5.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.5.5.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.6.6" class="ltx_tr">
<th id="A2.T2.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Reddit Emotion</th>
<td id="A2.T2.6.6.1" class="ltx_td ltx_align_left"><math id="A2.T2.6.6.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.6.6.1.m1.1a"><mrow id="A2.T2.6.6.1.m1.1.1" xref="A2.T2.6.6.1.m1.1.1.cmml"><mi id="A2.T2.6.6.1.m1.1.1.2" xref="A2.T2.6.6.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.6.6.1.m1.1.1.1" xref="A2.T2.6.6.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.6.6.1.m1.1.1.3" xref="A2.T2.6.6.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.6.6.1.m1.1b"><apply id="A2.T2.6.6.1.m1.1.1.cmml" xref="A2.T2.6.6.1.m1.1.1"><lt id="A2.T2.6.6.1.m1.1.1.1.cmml" xref="A2.T2.6.6.1.m1.1.1.1"></lt><ci id="A2.T2.6.6.1.m1.1.1.2.cmml" xref="A2.T2.6.6.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.6.6.1.m1.1.1.3.cmml" xref="A2.T2.6.6.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.6.6.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.7.7" class="ltx_tr">
<th id="A2.T2.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Sarcasm</th>
<td id="A2.T2.7.7.1" class="ltx_td ltx_align_left"><math id="A2.T2.7.7.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.7.7.1.m1.1a"><mrow id="A2.T2.7.7.1.m1.1.1" xref="A2.T2.7.7.1.m1.1.1.cmml"><mi id="A2.T2.7.7.1.m1.1.1.2" xref="A2.T2.7.7.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.7.7.1.m1.1.1.1" xref="A2.T2.7.7.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.7.7.1.m1.1.1.3" xref="A2.T2.7.7.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.7.7.1.m1.1b"><apply id="A2.T2.7.7.1.m1.1.1.cmml" xref="A2.T2.7.7.1.m1.1.1"><lt id="A2.T2.7.7.1.m1.1.1.1.cmml" xref="A2.T2.7.7.1.m1.1.1.1"></lt><ci id="A2.T2.7.7.1.m1.1.1.2.cmml" xref="A2.T2.7.7.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.7.7.1.m1.1.1.3.cmml" xref="A2.T2.7.7.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.7.7.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.8.8" class="ltx_tr">
<th id="A2.T2.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Humor</th>
<td id="A2.T2.8.8.1" class="ltx_td ltx_align_left"><math id="A2.T2.8.8.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.8.8.1.m1.1a"><mrow id="A2.T2.8.8.1.m1.1.1" xref="A2.T2.8.8.1.m1.1.1.cmml"><mi id="A2.T2.8.8.1.m1.1.1.2" xref="A2.T2.8.8.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.8.8.1.m1.1.1.1" xref="A2.T2.8.8.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.8.8.1.m1.1.1.3" xref="A2.T2.8.8.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.8.8.1.m1.1b"><apply id="A2.T2.8.8.1.m1.1.1.cmml" xref="A2.T2.8.8.1.m1.1.1"><lt id="A2.T2.8.8.1.m1.1.1.1.cmml" xref="A2.T2.8.8.1.m1.1.1.1"></lt><ci id="A2.T2.8.8.1.m1.1.1.2.cmml" xref="A2.T2.8.8.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.8.8.1.m1.1.1.3.cmml" xref="A2.T2.8.8.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.8.8.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.9.9" class="ltx_tr">
<th id="A2.T2.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Tweet Emotion</th>
<td id="A2.T2.9.9.1" class="ltx_td ltx_align_left"><math id="A2.T2.9.9.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.9.9.1.m1.1a"><mrow id="A2.T2.9.9.1.m1.1.1" xref="A2.T2.9.9.1.m1.1.1.cmml"><mi id="A2.T2.9.9.1.m1.1.1.2" xref="A2.T2.9.9.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.9.9.1.m1.1.1.1" xref="A2.T2.9.9.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.9.9.1.m1.1.1.3" xref="A2.T2.9.9.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.9.9.1.m1.1b"><apply id="A2.T2.9.9.1.m1.1.1.cmml" xref="A2.T2.9.9.1.m1.1.1"><lt id="A2.T2.9.9.1.m1.1.1.1.cmml" xref="A2.T2.9.9.1.m1.1.1.1"></lt><ci id="A2.T2.9.9.1.m1.1.1.2.cmml" xref="A2.T2.9.9.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.9.9.1.m1.1.1.3.cmml" xref="A2.T2.9.9.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.9.9.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
<tr id="A2.T2.10.10" class="ltx_tr">
<th id="A2.T2.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Tweet Irony</th>
<td id="A2.T2.10.10.1" class="ltx_td ltx_align_left"><math id="A2.T2.10.10.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="A2.T2.10.10.1.m1.1a"><mrow id="A2.T2.10.10.1.m1.1.1" xref="A2.T2.10.10.1.m1.1.1.cmml"><mi id="A2.T2.10.10.1.m1.1.1.2" xref="A2.T2.10.10.1.m1.1.1.2.cmml">p</mi><mo id="A2.T2.10.10.1.m1.1.1.1" xref="A2.T2.10.10.1.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T2.10.10.1.m1.1.1.3" xref="A2.T2.10.10.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.10.10.1.m1.1b"><apply id="A2.T2.10.10.1.m1.1.1.cmml" xref="A2.T2.10.10.1.m1.1.1"><lt id="A2.T2.10.10.1.m1.1.1.1.cmml" xref="A2.T2.10.10.1.m1.1.1.1"></lt><ci id="A2.T2.10.10.1.m1.1.1.2.cmml" xref="A2.T2.10.10.1.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T2.10.10.1.m1.1.1.3.cmml" xref="A2.T2.10.10.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.10.10.1.m1.1c">p&lt;0.001</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table B.2: </span>T-test results for the similarity comparison.</figcaption>
</figure>
<figure id="A2.T3" class="ltx_table">
<div id="A2.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:216.8pt;height:40.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.5pt,6.8pt) scale(0.747906260298682,0.747906260298682) ;">
<table id="A2.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T3.1.1.1.1" class="ltx_tr">
<th id="A2.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Dataset</th>
<th id="A2.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">BBC news</th>
<th id="A2.T3.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Amazon review</th>
<th id="A2.T3.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">SST-2</th>
<th id="A2.T3.1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Yelp</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T3.1.1.2.1" class="ltx_tr">
<td id="A2.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Real data</td>
<td id="A2.T3.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">93.6</td>
<td id="A2.T3.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">91.8</td>
<td id="A2.T3.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">89.2</td>
<td id="A2.T3.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">94.3</td>
</tr>
<tr id="A2.T3.1.1.3.2" class="ltx_tr">
<td id="A2.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_b">Zero-shot data</td>
<td id="A2.T3.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_b">91.2</td>
<td id="A2.T3.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_b">87.7</td>
<td id="A2.T3.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_b">86.4</td>
<td id="A2.T3.1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_b">87.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table B.3: </span>Comparing the performance of classification models trained on the LLM-generated synthetic data under the zero-shot with those trained with the original real-world data, in terms of Macro-F1 (%)</figcaption>
</figure>
</section>
<section id="A2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Additional Results of Zero-shot Synthetic Data for a few More “less subjective” Tasks</h3>

<div id="A2.SS4.p1" class="ltx_para">
<p id="A2.SS4.p1.1" class="ltx_p">To validate our observations regarding “subjectivity” in the data, we conducted additional experiments on a few more datasets which represent less subjective text classification tasks: the BBC News dataset, SST-2 movie review, Amazon US review, and Yelp review. We compared the performance of BERT models trained on real data with those trained on zero-shot synthetic data. As indicated in Table <a href="#A2.T3" title="Table B.3 ‣ B.3 Similarity between the Synthetic Data and the Real Data ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>, the average performance difference between real-world data and zero-shot synthetic data is only 4.2%. This gap is notably smaller than what is observed in tasks with greater subjectivity, reinforcing the finding that the subjectivity of a task can indeed diminish the effectiveness of synthetic data.</p>
</div>
<figure id="A2.T4" class="ltx_table">
<table id="A2.T4.23" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T4.23.24.1" class="ltx_tr">
<th id="A2.T4.23.24.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="A2.T4.23.24.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="A2.T4.23.24.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T4.23.24.1.2.1" class="ltx_text ltx_font_bold">AG</span></th>
<th id="A2.T4.23.24.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T4.23.24.1.3.1" class="ltx_text ltx_font_bold">IMDB</span></th>
<th id="A2.T4.23.24.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T4.23.24.1.4.1" class="ltx_text ltx_font_bold">SMS</span></th>
<th id="A2.T4.23.24.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T4.23.24.1.5.1" class="ltx_text ltx_font_bold">Tweet Emotion</span></th>
<th id="A2.T4.23.24.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T4.23.24.1.6.1" class="ltx_text ltx_font_bold">Humor Speech</span></th>
<th id="A2.T4.23.24.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T4.23.24.1.7.1" class="ltx_text ltx_font_bold">Tweet Irony</span></th>
</tr>
<tr id="A2.T4.23.23" class="ltx_tr">
<th id="A2.T4.23.23.24" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Subjectivity Level</th>
<th id="A2.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="A2.T4.1.1.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.1.1.1.m1.1a"><mo id="A2.T4.1.1.1.m1.1.1" xref="A2.T4.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.1.1.1.m1.1b"><ci id="A2.T4.1.1.1.m1.1.1.cmml" xref="A2.T4.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.1.1.1.m1.1c">\star</annotation></semantics></math></th>
<th id="A2.T4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T4.2.2.2.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.2.2.2.m1.1a"><mo id="A2.T4.2.2.2.m1.1.1" xref="A2.T4.2.2.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.2.2.2.m1.1b"><ci id="A2.T4.2.2.2.m1.1.1.cmml" xref="A2.T4.2.2.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.2.2.2.m1.1c">\star</annotation></semantics></math><math id="A2.T4.3.3.3.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.3.3.3.m2.1a"><mo id="A2.T4.3.3.3.m2.1.1" xref="A2.T4.3.3.3.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.3.3.3.m2.1b"><ci id="A2.T4.3.3.3.m2.1.1.cmml" xref="A2.T4.3.3.3.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.3.3.3.m2.1c">\star</annotation></semantics></math><math id="A2.T4.4.4.4.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.4.4.4.m3.1a"><mo id="A2.T4.4.4.4.m3.1.1" xref="A2.T4.4.4.4.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.4.4.4.m3.1b"><ci id="A2.T4.4.4.4.m3.1.1.cmml" xref="A2.T4.4.4.4.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.4.4.4.m3.1c">\star</annotation></semantics></math>
</th>
<th id="A2.T4.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T4.5.5.5.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.5.5.5.m1.1a"><mo id="A2.T4.5.5.5.m1.1.1" xref="A2.T4.5.5.5.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.5.5.5.m1.1b"><ci id="A2.T4.5.5.5.m1.1.1.cmml" xref="A2.T4.5.5.5.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.5.5.5.m1.1c">\star</annotation></semantics></math><math id="A2.T4.6.6.6.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.6.6.6.m2.1a"><mo id="A2.T4.6.6.6.m2.1.1" xref="A2.T4.6.6.6.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.6.6.6.m2.1b"><ci id="A2.T4.6.6.6.m2.1.1.cmml" xref="A2.T4.6.6.6.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.6.6.6.m2.1c">\star</annotation></semantics></math><math id="A2.T4.7.7.7.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.7.7.7.m3.1a"><mo id="A2.T4.7.7.7.m3.1.1" xref="A2.T4.7.7.7.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.7.7.7.m3.1b"><ci id="A2.T4.7.7.7.m3.1.1.cmml" xref="A2.T4.7.7.7.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.7.7.7.m3.1c">\star</annotation></semantics></math><math id="A2.T4.8.8.8.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.8.8.8.m4.1a"><mo id="A2.T4.8.8.8.m4.1.1" xref="A2.T4.8.8.8.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.8.8.8.m4.1b"><ci id="A2.T4.8.8.8.m4.1.1.cmml" xref="A2.T4.8.8.8.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.8.8.8.m4.1c">\star</annotation></semantics></math>
</th>
<th id="A2.T4.13.13.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T4.9.9.9.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.9.9.9.m1.1a"><mo id="A2.T4.9.9.9.m1.1.1" xref="A2.T4.9.9.9.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.9.9.9.m1.1b"><ci id="A2.T4.9.9.9.m1.1.1.cmml" xref="A2.T4.9.9.9.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.9.9.9.m1.1c">\star</annotation></semantics></math><math id="A2.T4.10.10.10.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.10.10.10.m2.1a"><mo id="A2.T4.10.10.10.m2.1.1" xref="A2.T4.10.10.10.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.10.10.10.m2.1b"><ci id="A2.T4.10.10.10.m2.1.1.cmml" xref="A2.T4.10.10.10.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.10.10.10.m2.1c">\star</annotation></semantics></math><math id="A2.T4.11.11.11.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.11.11.11.m3.1a"><mo id="A2.T4.11.11.11.m3.1.1" xref="A2.T4.11.11.11.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.11.11.11.m3.1b"><ci id="A2.T4.11.11.11.m3.1.1.cmml" xref="A2.T4.11.11.11.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.11.11.11.m3.1c">\star</annotation></semantics></math><math id="A2.T4.12.12.12.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.12.12.12.m4.1a"><mo id="A2.T4.12.12.12.m4.1.1" xref="A2.T4.12.12.12.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.12.12.12.m4.1b"><ci id="A2.T4.12.12.12.m4.1.1.cmml" xref="A2.T4.12.12.12.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.12.12.12.m4.1c">\star</annotation></semantics></math><math id="A2.T4.13.13.13.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.13.13.13.m5.1a"><mo id="A2.T4.13.13.13.m5.1.1" xref="A2.T4.13.13.13.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.13.13.13.m5.1b"><ci id="A2.T4.13.13.13.m5.1.1.cmml" xref="A2.T4.13.13.13.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.13.13.13.m5.1c">\star</annotation></semantics></math>
</th>
<th id="A2.T4.18.18.18" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T4.14.14.14.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.14.14.14.m1.1a"><mo id="A2.T4.14.14.14.m1.1.1" xref="A2.T4.14.14.14.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.14.14.14.m1.1b"><ci id="A2.T4.14.14.14.m1.1.1.cmml" xref="A2.T4.14.14.14.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.14.14.14.m1.1c">\star</annotation></semantics></math><math id="A2.T4.15.15.15.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.15.15.15.m2.1a"><mo id="A2.T4.15.15.15.m2.1.1" xref="A2.T4.15.15.15.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.15.15.15.m2.1b"><ci id="A2.T4.15.15.15.m2.1.1.cmml" xref="A2.T4.15.15.15.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.15.15.15.m2.1c">\star</annotation></semantics></math><math id="A2.T4.16.16.16.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.16.16.16.m3.1a"><mo id="A2.T4.16.16.16.m3.1.1" xref="A2.T4.16.16.16.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.16.16.16.m3.1b"><ci id="A2.T4.16.16.16.m3.1.1.cmml" xref="A2.T4.16.16.16.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.16.16.16.m3.1c">\star</annotation></semantics></math><math id="A2.T4.17.17.17.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.17.17.17.m4.1a"><mo id="A2.T4.17.17.17.m4.1.1" xref="A2.T4.17.17.17.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.17.17.17.m4.1b"><ci id="A2.T4.17.17.17.m4.1.1.cmml" xref="A2.T4.17.17.17.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.17.17.17.m4.1c">\star</annotation></semantics></math><math id="A2.T4.18.18.18.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.18.18.18.m5.1a"><mo id="A2.T4.18.18.18.m5.1.1" xref="A2.T4.18.18.18.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.18.18.18.m5.1b"><ci id="A2.T4.18.18.18.m5.1.1.cmml" xref="A2.T4.18.18.18.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.18.18.18.m5.1c">\star</annotation></semantics></math>
</th>
<th id="A2.T4.23.23.23" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T4.19.19.19.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.19.19.19.m1.1a"><mo id="A2.T4.19.19.19.m1.1.1" xref="A2.T4.19.19.19.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.19.19.19.m1.1b"><ci id="A2.T4.19.19.19.m1.1.1.cmml" xref="A2.T4.19.19.19.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.19.19.19.m1.1c">\star</annotation></semantics></math><math id="A2.T4.20.20.20.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.20.20.20.m2.1a"><mo id="A2.T4.20.20.20.m2.1.1" xref="A2.T4.20.20.20.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.20.20.20.m2.1b"><ci id="A2.T4.20.20.20.m2.1.1.cmml" xref="A2.T4.20.20.20.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.20.20.20.m2.1c">\star</annotation></semantics></math><math id="A2.T4.21.21.21.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.21.21.21.m3.1a"><mo id="A2.T4.21.21.21.m3.1.1" xref="A2.T4.21.21.21.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.21.21.21.m3.1b"><ci id="A2.T4.21.21.21.m3.1.1.cmml" xref="A2.T4.21.21.21.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.21.21.21.m3.1c">\star</annotation></semantics></math><math id="A2.T4.22.22.22.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.22.22.22.m4.1a"><mo id="A2.T4.22.22.22.m4.1.1" xref="A2.T4.22.22.22.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.22.22.22.m4.1b"><ci id="A2.T4.22.22.22.m4.1.1.cmml" xref="A2.T4.22.22.22.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.22.22.22.m4.1c">\star</annotation></semantics></math><math id="A2.T4.23.23.23.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T4.23.23.23.m5.1a"><mo id="A2.T4.23.23.23.m5.1.1" xref="A2.T4.23.23.23.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T4.23.23.23.m5.1b"><ci id="A2.T4.23.23.23.m5.1.1.cmml" xref="A2.T4.23.23.23.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.23.23.23.m5.1c">\star</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T4.23.25.1" class="ltx_tr">
<th id="A2.T4.23.25.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Real data</th>
<td id="A2.T4.23.25.1.2" class="ltx_td ltx_align_center">95.3</td>
<td id="A2.T4.23.25.1.3" class="ltx_td ltx_align_center">87.6</td>
<td id="A2.T4.23.25.1.4" class="ltx_td ltx_align_center">97.2</td>
<td id="A2.T4.23.25.1.5" class="ltx_td ltx_align_center">77.7</td>
<td id="A2.T4.23.25.1.6" class="ltx_td ltx_align_center">97.0</td>
<td id="A2.T4.23.25.1.7" class="ltx_td ltx_align_center">72.2</td>
</tr>
<tr id="A2.T4.23.26.2" class="ltx_tr">
<th id="A2.T4.23.26.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">GPT2-Large</th>
<td id="A2.T4.23.26.2.2" class="ltx_td ltx_align_center">86.5</td>
<td id="A2.T4.23.26.2.3" class="ltx_td ltx_align_center">80.9</td>
<td id="A2.T4.23.26.2.4" class="ltx_td ltx_align_center">86.4</td>
<td id="A2.T4.23.26.2.5" class="ltx_td ltx_align_center">52.2</td>
<td id="A2.T4.23.26.2.6" class="ltx_td ltx_align_center">51.5</td>
<td id="A2.T4.23.26.2.7" class="ltx_td ltx_align_center">60.8</td>
</tr>
<tr id="A2.T4.23.27.3" class="ltx_tr">
<th id="A2.T4.23.27.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Llama 2</th>
<td id="A2.T4.23.27.3.2" class="ltx_td ltx_align_center">88.7</td>
<td id="A2.T4.23.27.3.3" class="ltx_td ltx_align_center">82.4</td>
<td id="A2.T4.23.27.3.4" class="ltx_td ltx_align_center">88.5</td>
<td id="A2.T4.23.27.3.5" class="ltx_td ltx_align_center">59.1</td>
<td id="A2.T4.23.27.3.6" class="ltx_td ltx_align_center">57.2</td>
<td id="A2.T4.23.27.3.7" class="ltx_td ltx_align_center">63.1</td>
</tr>
<tr id="A2.T4.23.28.4" class="ltx_tr">
<th id="A2.T4.23.28.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">GPT-3.5 turbo</th>
<td id="A2.T4.23.28.4.2" class="ltx_td ltx_align_center ltx_border_b">89.3</td>
<td id="A2.T4.23.28.4.3" class="ltx_td ltx_align_center ltx_border_b">81.2</td>
<td id="A2.T4.23.28.4.4" class="ltx_td ltx_align_center ltx_border_b">93.8</td>
<td id="A2.T4.23.28.4.5" class="ltx_td ltx_align_center ltx_border_b">58.5</td>
<td id="A2.T4.23.28.4.6" class="ltx_td ltx_align_center ltx_border_b">56.0</td>
<td id="A2.T4.23.28.4.7" class="ltx_td ltx_align_center ltx_border_b">63.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table B.4: </span>Comparing the performance of Bert classification models trained on synthetic data generated by various LLMs within a zero-shot setting using Macro-F1 (%) as the metric.</figcaption>
</figure>
</section>
<section id="A2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.5 </span>Additional Results of More LLMs</h3>

<div id="A2.SS5.p1" class="ltx_para">
<p id="A2.SS5.p1.1" class="ltx_p">To examine whether our findings hold true for decoder-based models as well as models that are reasonably large, we conducted the same evaluation studies using the GPT2-large (774M) and Llama2 (7B) models. We conducted this evaluation on 6 selected datasets from the entire set of 10 datasets which covered different levels of subjectivity. As indicated in Table <a href="#A2.T4" title="Table B.4 ‣ B.4 Additional Results of Zero-shot Synthetic Data for a few More “less subjective” Tasks ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.4</span></a>, we observed that models trained on the LLM-generated synthetic data only exhibits slight variations among different LLMs for each respective task. The overall trend remains consistent: the effectiveness of synthetic data tends to be higher for tasks with lower subjectivity.</p>
</div>
<figure id="A2.T5" class="ltx_table">
<table id="A2.T5.23" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T5.23.24.1" class="ltx_tr">
<th id="A2.T5.23.24.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="A2.T5.23.24.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="A2.T5.23.24.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T5.23.24.1.2.1" class="ltx_text ltx_font_bold">AG</span></th>
<th id="A2.T5.23.24.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T5.23.24.1.3.1" class="ltx_text ltx_font_bold">IMDB</span></th>
<th id="A2.T5.23.24.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T5.23.24.1.4.1" class="ltx_text ltx_font_bold">SMS</span></th>
<th id="A2.T5.23.24.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T5.23.24.1.5.1" class="ltx_text ltx_font_bold">Tweet Emotion</span></th>
<th id="A2.T5.23.24.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T5.23.24.1.6.1" class="ltx_text ltx_font_bold">Humor Speech</span></th>
<th id="A2.T5.23.24.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.T5.23.24.1.7.1" class="ltx_text ltx_font_bold">Tweet Irony</span></th>
</tr>
<tr id="A2.T5.23.23" class="ltx_tr">
<th id="A2.T5.23.23.24" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Subjectivity Level</th>
<th id="A2.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="A2.T5.1.1.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.1.1.1.m1.1a"><mo id="A2.T5.1.1.1.m1.1.1" xref="A2.T5.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.1.1.1.m1.1b"><ci id="A2.T5.1.1.1.m1.1.1.cmml" xref="A2.T5.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.1.1.1.m1.1c">\star</annotation></semantics></math></th>
<th id="A2.T5.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T5.2.2.2.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.2.2.2.m1.1a"><mo id="A2.T5.2.2.2.m1.1.1" xref="A2.T5.2.2.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.2.2.2.m1.1b"><ci id="A2.T5.2.2.2.m1.1.1.cmml" xref="A2.T5.2.2.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.2.2.2.m1.1c">\star</annotation></semantics></math><math id="A2.T5.3.3.3.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.3.3.3.m2.1a"><mo id="A2.T5.3.3.3.m2.1.1" xref="A2.T5.3.3.3.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.3.3.3.m2.1b"><ci id="A2.T5.3.3.3.m2.1.1.cmml" xref="A2.T5.3.3.3.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.3.3.3.m2.1c">\star</annotation></semantics></math><math id="A2.T5.4.4.4.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.4.4.4.m3.1a"><mo id="A2.T5.4.4.4.m3.1.1" xref="A2.T5.4.4.4.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.4.4.4.m3.1b"><ci id="A2.T5.4.4.4.m3.1.1.cmml" xref="A2.T5.4.4.4.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.4.4.4.m3.1c">\star</annotation></semantics></math>
</th>
<th id="A2.T5.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T5.5.5.5.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.5.5.5.m1.1a"><mo id="A2.T5.5.5.5.m1.1.1" xref="A2.T5.5.5.5.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.5.5.5.m1.1b"><ci id="A2.T5.5.5.5.m1.1.1.cmml" xref="A2.T5.5.5.5.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.5.5.5.m1.1c">\star</annotation></semantics></math><math id="A2.T5.6.6.6.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.6.6.6.m2.1a"><mo id="A2.T5.6.6.6.m2.1.1" xref="A2.T5.6.6.6.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.6.6.6.m2.1b"><ci id="A2.T5.6.6.6.m2.1.1.cmml" xref="A2.T5.6.6.6.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.6.6.6.m2.1c">\star</annotation></semantics></math><math id="A2.T5.7.7.7.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.7.7.7.m3.1a"><mo id="A2.T5.7.7.7.m3.1.1" xref="A2.T5.7.7.7.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.7.7.7.m3.1b"><ci id="A2.T5.7.7.7.m3.1.1.cmml" xref="A2.T5.7.7.7.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.7.7.7.m3.1c">\star</annotation></semantics></math><math id="A2.T5.8.8.8.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.8.8.8.m4.1a"><mo id="A2.T5.8.8.8.m4.1.1" xref="A2.T5.8.8.8.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.8.8.8.m4.1b"><ci id="A2.T5.8.8.8.m4.1.1.cmml" xref="A2.T5.8.8.8.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.8.8.8.m4.1c">\star</annotation></semantics></math>
</th>
<th id="A2.T5.13.13.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T5.9.9.9.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.9.9.9.m1.1a"><mo id="A2.T5.9.9.9.m1.1.1" xref="A2.T5.9.9.9.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.9.9.9.m1.1b"><ci id="A2.T5.9.9.9.m1.1.1.cmml" xref="A2.T5.9.9.9.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.9.9.9.m1.1c">\star</annotation></semantics></math><math id="A2.T5.10.10.10.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.10.10.10.m2.1a"><mo id="A2.T5.10.10.10.m2.1.1" xref="A2.T5.10.10.10.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.10.10.10.m2.1b"><ci id="A2.T5.10.10.10.m2.1.1.cmml" xref="A2.T5.10.10.10.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.10.10.10.m2.1c">\star</annotation></semantics></math><math id="A2.T5.11.11.11.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.11.11.11.m3.1a"><mo id="A2.T5.11.11.11.m3.1.1" xref="A2.T5.11.11.11.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.11.11.11.m3.1b"><ci id="A2.T5.11.11.11.m3.1.1.cmml" xref="A2.T5.11.11.11.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.11.11.11.m3.1c">\star</annotation></semantics></math><math id="A2.T5.12.12.12.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.12.12.12.m4.1a"><mo id="A2.T5.12.12.12.m4.1.1" xref="A2.T5.12.12.12.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.12.12.12.m4.1b"><ci id="A2.T5.12.12.12.m4.1.1.cmml" xref="A2.T5.12.12.12.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.12.12.12.m4.1c">\star</annotation></semantics></math><math id="A2.T5.13.13.13.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.13.13.13.m5.1a"><mo id="A2.T5.13.13.13.m5.1.1" xref="A2.T5.13.13.13.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.13.13.13.m5.1b"><ci id="A2.T5.13.13.13.m5.1.1.cmml" xref="A2.T5.13.13.13.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.13.13.13.m5.1c">\star</annotation></semantics></math>
</th>
<th id="A2.T5.18.18.18" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T5.14.14.14.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.14.14.14.m1.1a"><mo id="A2.T5.14.14.14.m1.1.1" xref="A2.T5.14.14.14.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.14.14.14.m1.1b"><ci id="A2.T5.14.14.14.m1.1.1.cmml" xref="A2.T5.14.14.14.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.14.14.14.m1.1c">\star</annotation></semantics></math><math id="A2.T5.15.15.15.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.15.15.15.m2.1a"><mo id="A2.T5.15.15.15.m2.1.1" xref="A2.T5.15.15.15.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.15.15.15.m2.1b"><ci id="A2.T5.15.15.15.m2.1.1.cmml" xref="A2.T5.15.15.15.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.15.15.15.m2.1c">\star</annotation></semantics></math><math id="A2.T5.16.16.16.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.16.16.16.m3.1a"><mo id="A2.T5.16.16.16.m3.1.1" xref="A2.T5.16.16.16.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.16.16.16.m3.1b"><ci id="A2.T5.16.16.16.m3.1.1.cmml" xref="A2.T5.16.16.16.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.16.16.16.m3.1c">\star</annotation></semantics></math><math id="A2.T5.17.17.17.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.17.17.17.m4.1a"><mo id="A2.T5.17.17.17.m4.1.1" xref="A2.T5.17.17.17.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.17.17.17.m4.1b"><ci id="A2.T5.17.17.17.m4.1.1.cmml" xref="A2.T5.17.17.17.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.17.17.17.m4.1c">\star</annotation></semantics></math><math id="A2.T5.18.18.18.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.18.18.18.m5.1a"><mo id="A2.T5.18.18.18.m5.1.1" xref="A2.T5.18.18.18.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.18.18.18.m5.1b"><ci id="A2.T5.18.18.18.m5.1.1.cmml" xref="A2.T5.18.18.18.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.18.18.18.m5.1c">\star</annotation></semantics></math>
</th>
<th id="A2.T5.23.23.23" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="A2.T5.19.19.19.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.19.19.19.m1.1a"><mo id="A2.T5.19.19.19.m1.1.1" xref="A2.T5.19.19.19.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.19.19.19.m1.1b"><ci id="A2.T5.19.19.19.m1.1.1.cmml" xref="A2.T5.19.19.19.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.19.19.19.m1.1c">\star</annotation></semantics></math><math id="A2.T5.20.20.20.m2.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.20.20.20.m2.1a"><mo id="A2.T5.20.20.20.m2.1.1" xref="A2.T5.20.20.20.m2.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.20.20.20.m2.1b"><ci id="A2.T5.20.20.20.m2.1.1.cmml" xref="A2.T5.20.20.20.m2.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.20.20.20.m2.1c">\star</annotation></semantics></math><math id="A2.T5.21.21.21.m3.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.21.21.21.m3.1a"><mo id="A2.T5.21.21.21.m3.1.1" xref="A2.T5.21.21.21.m3.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.21.21.21.m3.1b"><ci id="A2.T5.21.21.21.m3.1.1.cmml" xref="A2.T5.21.21.21.m3.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.21.21.21.m3.1c">\star</annotation></semantics></math><math id="A2.T5.22.22.22.m4.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.22.22.22.m4.1a"><mo id="A2.T5.22.22.22.m4.1.1" xref="A2.T5.22.22.22.m4.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.22.22.22.m4.1b"><ci id="A2.T5.22.22.22.m4.1.1.cmml" xref="A2.T5.22.22.22.m4.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.22.22.22.m4.1c">\star</annotation></semantics></math><math id="A2.T5.23.23.23.m5.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="A2.T5.23.23.23.m5.1a"><mo id="A2.T5.23.23.23.m5.1.1" xref="A2.T5.23.23.23.m5.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T5.23.23.23.m5.1b"><ci id="A2.T5.23.23.23.m5.1.1.cmml" xref="A2.T5.23.23.23.m5.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.23.23.23.m5.1c">\star</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T5.23.25.1" class="ltx_tr">
<th id="A2.T5.23.25.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Real data</th>
<td id="A2.T5.23.25.1.2" class="ltx_td ltx_align_center">95.3</td>
<td id="A2.T5.23.25.1.3" class="ltx_td ltx_align_center">87.6</td>
<td id="A2.T5.23.25.1.4" class="ltx_td ltx_align_center">97.2</td>
<td id="A2.T5.23.25.1.5" class="ltx_td ltx_align_center">77.7</td>
<td id="A2.T5.23.25.1.6" class="ltx_td ltx_align_center">97.0</td>
<td id="A2.T5.23.25.1.7" class="ltx_td ltx_align_center">72.2</td>
</tr>
<tr id="A2.T5.23.26.2" class="ltx_tr">
<th id="A2.T5.23.26.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Direct Prompt</th>
<td id="A2.T5.23.26.2.2" class="ltx_td ltx_align_center">86.5</td>
<td id="A2.T5.23.26.2.3" class="ltx_td ltx_align_center">82.8</td>
<td id="A2.T5.23.26.2.4" class="ltx_td ltx_align_center">89.4</td>
<td id="A2.T5.23.26.2.5" class="ltx_td ltx_align_center">54.3</td>
<td id="A2.T5.23.26.2.6" class="ltx_td ltx_align_center">59.2</td>
<td id="A2.T5.23.26.2.7" class="ltx_td ltx_align_center">61.1</td>
</tr>
<tr id="A2.T5.23.27.3" class="ltx_tr">
<th id="A2.T5.23.27.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">Zero-shot</th>
<td id="A2.T5.23.27.3.2" class="ltx_td ltx_align_center ltx_border_b">89.3</td>
<td id="A2.T5.23.27.3.3" class="ltx_td ltx_align_center ltx_border_b">81.2</td>
<td id="A2.T5.23.27.3.4" class="ltx_td ltx_align_center ltx_border_b">93.8</td>
<td id="A2.T5.23.27.3.5" class="ltx_td ltx_align_center ltx_border_b">58.5</td>
<td id="A2.T5.23.27.3.6" class="ltx_td ltx_align_center ltx_border_b">56.0</td>
<td id="A2.T5.23.27.3.7" class="ltx_td ltx_align_center ltx_border_b">63.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table B.5: </span>Performance comparisons in terms of Macro-F1 (%) between “direct prompt” and “zero-shot data generation” using GPT-3.5 turbo. For the zero-shot synthetica data and real data, we adopted the Bert model as the base for classification.</figcaption>
</figure>
</section>
<section id="A2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.6 </span>Additional Results of Direct Prompt by LLMs</h3>

<div id="A2.SS6.p1" class="ltx_para">
<p id="A2.SS6.p1.1" class="ltx_p">While LLMs are capable of generating high-quality synthetic data through prompting, their direct classification performance can sometimes lag behind that of smaller models trained on this synthetic data. As shown in Table <a href="#A2.T5" title="Table B.5 ‣ B.5 Additional Results of More LLMs ‣ Appendix B Evaluation I: Comparison Across Different Types of Tasks (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.5</span></a>, for many tasks, directly prompting GPT-3.5 turbo model for classification often yields poorer results compared to a smaller model trained on the synthetic data. This discrepancy might arise because the prompt constraints defining the label space for the LLM can sometimes be too lax, making accurate classification challenging.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evaluation II: Comparison Across
Different Task Instances (Additional Results)</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.2" class="ltx_p">In order to investigate how models trained on the real-world data perform across task instances of varying subjectivity, we used BERT as the foundational model for training a classification model with the real-world data. As depicted in Figure <a href="#A3.F1" title="Figure C.1 ‣ Appendix C Evaluation II: Comparison Across Different Task Instances (Additional Results) ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1</span></a>, we observed that compared to models trained on zero-shot synthetic data, the performance of models trained on the real-world data is less affected by the subjectivity of the task instance (i.e., <math id="A3.p1.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="A3.p1.1.m1.1a"><mi id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><ci id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">\beta</annotation></semantics></math> and <math id="A3.p1.2.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A3.p1.2.m2.1a"><mi id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><ci id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">\rho</annotation></semantics></math> are smaller), except for that on the Scarcasm News and Financial Phrasebank datasets.</p>
</div>
<figure id="A3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/ag_line_real.png" id="A3.F1.sf1.g1" class="ltx_graphics ltx_img_square" width="114" height="98" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>AG</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/relations_line_real.png" id="A3.F1.sf2.g1" class="ltx_graphics ltx_img_square" width="114" height="98" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Relation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/imdb_line_real.png" id="A3.F1.sf3.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>IMDB Reviews</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/spam_line_real.png" id="A3.F1.sf4.g1" class="ltx_graphics ltx_img_square" width="114" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>SMS Spam</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/go_emotions_line_real.png" id="A3.F1.sf5.g1" class="ltx_graphics ltx_img_square" width="114" height="98" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Reddit Emotion</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/sarcasm_line_real.png" id="A3.F1.sf6.g1" class="ltx_graphics ltx_img_square" width="114" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Sarcasm News</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/humor_line_real.png" id="A3.F1.sf7.g1" class="ltx_graphics ltx_img_square" width="114" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>Humor Detection</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/tweet_emotion_line_real.png" id="A3.F1.sf8.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>Tweet Emotions</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/tweet_irony_line_real.png" id="A3.F1.sf9.g1" class="ltx_graphics ltx_img_square" width="114" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(i) </span>Tweet Irony Speech</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A3.F1.sf10" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2310.07849/assets/figures/financial_line_real.png" id="A3.F1.sf10.g1" class="ltx_graphics ltx_img_square" width="114" height="100" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(j) </span>Financial Phrasebank</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C.1: </span>Changes in the accuracy of the BERT model trained on real-world data as the instance-level annotation agreement threshold varies. The solid blue line in each plot is the linear regression fitted on the data, and the <math id="A3.F1.6.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="A3.F1.6.m1.1b"><mi id="A3.F1.6.m1.1.1" xref="A3.F1.6.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A3.F1.6.m1.1c"><ci id="A3.F1.6.m1.1.1.cmml" xref="A3.F1.6.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F1.6.m1.1d">R</annotation></semantics></math>-squared score quantifies the goodness of fit. The Spearman’s <math id="A3.F1.7.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A3.F1.7.m2.1b"><mi id="A3.F1.7.m2.1.1" xref="A3.F1.7.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A3.F1.7.m2.1c"><ci id="A3.F1.7.m2.1.1.cmml" xref="A3.F1.7.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F1.7.m2.1d">\rho</annotation></semantics></math> assesses the strength of rank correlation between the instance-level agreement threshold and the model accuracy for each task. Higher values for both <math id="A3.F1.8.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="A3.F1.8.m3.1b"><mi id="A3.F1.8.m3.1.1" xref="A3.F1.8.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="A3.F1.8.m3.1c"><ci id="A3.F1.8.m3.1.1.cmml" xref="A3.F1.8.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F1.8.m3.1d">R</annotation></semantics></math>-squared and Spearman’s <math id="A3.F1.9.m4.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A3.F1.9.m4.1b"><mi id="A3.F1.9.m4.1.1" xref="A3.F1.9.m4.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A3.F1.9.m4.1c"><ci id="A3.F1.9.m4.1.1.cmml" xref="A3.F1.9.m4.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F1.9.m4.1d">\rho</annotation></semantics></math>, ideally close to <math id="A3.F1.10.m5.1" class="ltx_Math" alttext="1" display="inline"><semantics id="A3.F1.10.m5.1b"><mn id="A3.F1.10.m5.1.1" xref="A3.F1.10.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A3.F1.10.m5.1c"><cn type="integer" id="A3.F1.10.m5.1.1.cmml" xref="A3.F1.10.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.F1.10.m5.1d">1</annotation></semantics></math>, indicate a stronger monotonic relationship between the instance-level subjectivity and the model accuracy.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Details on the Generation of Synthetic Data</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">The prompts we used to generate synthetic data under both the zero-shot setting and the few-shot setting are shown in the Table <a href="#A4.T1" title="Table D.1 ‣ Appendix D Additional Details on the Generation of Synthetic Data ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.1</span></a> and the Table <a href="#A4.T2" title="Table D.2 ‣ Appendix D Additional Details on the Generation of Synthetic Data ‣ Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.2</span></a>.</p>
</div>
<figure id="A4.T1" class="ltx_table">
<table id="A4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T1.1.1.1" class="ltx_tr">
<th id="A4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></th>
<td id="A4.T1.1.1.1.2" class="ltx_td ltx_align_left"><span id="A4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Zero-shot/Few-shot</span></td>
</tr>
<tr id="A4.T1.1.2.2" class="ltx_tr">
<th id="A4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">AG</th>
<td id="A4.T1.1.2.2.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> Now you are a journalist writing news articles. You are given a topic and must write a corresponding news article for it. You are also given a length requirement. You must ensure your news meets the length requirement.</td>
</tr>
<tr id="A4.T1.1.3.3" class="ltx_tr">
<th id="A4.T1.1.3.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A4.T1.1.3.3.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.3.3.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Can you write a news report with the topic {label}? The length requirement is: {num_words} words. Please be creative and write unique news articles.</td>
</tr>
<tr id="A4.T1.1.4.4" class="ltx_tr">
<th id="A4.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Relation</th>
<td id="A4.T1.1.4.4.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.4.4.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> Now you are a Wikipedia editor. You need to generate new records for describing the relation between entities. You are given a relation type, as well as a sentence describing the relationship. You must write a sentence to describe the specified relationship between the two entities that you came up with.</td>
</tr>
<tr id="A4.T1.1.5.5" class="ltx_tr">
<th id="A4.T1.1.5.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A4.T1.1.5.5.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.5.5.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Give me one pair of entities, which have the relation: {label}, and generate a sentence which contains the pair of entities that have the relation: {label}. The description of the relation is: {label_description}.</td>
</tr>
<tr id="A4.T1.1.6.6" class="ltx_tr">
<th id="A4.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">IMDB</th>
<td id="A4.T1.1.6.6.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.6.6.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> Now you are a movie critic. You need to have delicate emotions, unique perspectives, and a distinctive style. You are going to write a highly polar review for a movie and post it on IMDB. You are given a movie genre/style and a length requirement. You must come up with a movie that corresponds to the genre/style and write a review that meets the length requirement.</td>
</tr>
<tr id="A4.T1.1.7.7" class="ltx_tr">
<th id="A4.T1.1.7.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A4.T1.1.7.7.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.7.7.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Write a film review for a {genre} movie to express {pos_or_neg} feedback. Each review should have {num_of_words} words. Be sure to express your personal insights and feelings. Please be creative and write unique movie reviews.</td>
</tr>
<tr id="A4.T1.1.8.8" class="ltx_tr">
<th id="A4.T1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">SMS spam</th>
<td id="A4.T1.1.8.8.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.8.8.2.1" class="ltx_text ltx_font_bold">Context Prompt (Spam):</span> Now you are a person who is planning to send a spam SMS message. You must be as creative as possible to diversify your messages. Ensure your language is conversational and colloquial. Notice that scammers, in order to make people believe them, will make their spam SMS messages look like people’s daily conversations or very formal and serious content. You also need to imitate these contents. <span id="A4.T1.1.8.8.2.2" class="ltx_text ltx_font_bold">Context Prompt (Ham):</span> Now you are a person who is planning to send a SMS message. You must be as creative as possible to diversify your messages. Ensure your language is conversational and colloquial. Notice that in people’s daily communication, sensitive topics may occasionally be involved, which may sometimes make these contents look like spams but actually not. You also need to imitate these contents.</td>
</tr>
<tr id="A4.T1.1.9.9" class="ltx_tr">
<th id="A4.T1.1.9.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A4.T1.1.9.9.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.9.9.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Now write SMS messages as I required. Be creative and write unique SMS messages.</td>
</tr>
<tr id="A4.T1.1.10.10" class="ltx_tr">
<th id="A4.T1.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Reddit emotion</th>
<td id="A4.T1.1.10.10.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.10.10.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> Now you are a Reddit user and you are going to write a comment to express your emotions. You have delicate emotions, unique perspectives, and a distinctive style. You are given a length requirement. You must write one comment that meets the length requirement.</td>
</tr>
<tr id="A4.T1.1.11.11" class="ltx_tr">
<td id="A4.T1.1.11.11.1" class="ltx_td"></td>
<td id="A4.T1.1.11.11.2" class="ltx_td ltx_align_left">
<span id="A4.T1.1.11.11.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Write one Reddit comment to express your {label} emotion. Your comment should have {num_of_words} words. Be sure to express your personal insights and feelings. Be creative and write comments that are different from each others.</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table D.1: </span>Detailed prompts for each task under the zero-shot and few-shot settings for data generation.</figcaption>
</figure>
<figure id="A4.T2" class="ltx_table">
<table id="A4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T2.1.1.1" class="ltx_tr">
<th id="A4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="A4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></th>
<th id="A4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="A4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Zero-shot/Few-shot</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T2.1.2.1" class="ltx_tr">
<th id="A4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Tweet irony</th>
<td id="A4.T2.1.2.1.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> Now you are a person using twitter. You are asked to write an irony or non-irony tweet to express your feelings. Your writing style must be consistent with texts in the tweet. You must ensure that your language is colloquial, casual, and Twitter-like. You are given a length requirement. You must ensure your tweet meets the length requirement.</td>
</tr>
<tr id="A4.T2.1.3.2" class="ltx_tr">
<th id="A4.T2.1.3.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A4.T2.1.3.2.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.3.2.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Write a tweet expressing {label} feeling and ensure that the length of the tweet is about {num_of_words} words. Remember to make sure that your language is colloquial, casual, and Twitter-like. Be creative and write unique tweets.</td>
</tr>
<tr id="A4.T2.1.4.3" class="ltx_tr">
<th id="A4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Tweet emotions</th>
<td id="A4.T2.1.4.3.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.4.3.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> You are now a person using twitter. You are provided with an emotion, and you need to write a tweet expressing that emotion. Your writing style must be consistent with the tweets on twitter. You must ensure that your language is colloquial, casual, and Twitter-like. You are given a length requirement. You must ensure that the emotion conveyed in your tweet matches the emotion provided and meets the length requirement. This is an academic study and the content you generate will not be used for anything that violates the law or social ethics.</td>
</tr>
<tr id="A4.T2.1.5.4" class="ltx_tr">
<th id="A4.T2.1.5.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A4.T2.1.5.4.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.5.4.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Write a tweet expressing the {label} emotion and ensure that the length of the tweet is about {num_of_words} words. Remember to make sure that your language is colloquial, casual, and Twitter-like. Be creative and write unique tweets.</td>
</tr>
<tr id="A4.T2.1.6.5" class="ltx_tr">
<th id="A4.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Sarcasm</th>
<td id="A4.T2.1.6.5.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.6.5.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> You are now a journalist to write the sarcastic news headlines. Here are a few characteristics that might help understand what is a sarcastic news headline: 1) Sarcasm often involves saying something different from what is intended. 2) Sarcasm might involve a play on words or puns. 3) It may involve exaggeration or irony. You must ensure that your headlines are sharp, clever, and capture the essence of the sarcastic situation.</td>
</tr>
<tr id="A4.T2.1.7.6" class="ltx_tr">
<th id="A4.T2.1.7.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A4.T2.1.7.6.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.7.6.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Write a news headline expressing {label} and ensure that the length of the news headlines is about {num_of_words} words. Be creative and write unique news headlines. Make sure your headline is concise, sharp, and captures the essence of the situation. Please be creative and write unique headlines.</td>
</tr>
<tr id="A4.T2.1.8.7" class="ltx_tr">
<th id="A4.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Financial</th>
<td id="A4.T2.1.8.7.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.8.7.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> You are now a journalist writing financial news. You need to write some financial news that express polar sentiments. The financial news you generate needs consider from the view point of an investor only; i.e. whether the news may have positive, negative or neutral influence on the stock price. As a result, sentences which have a sentiment that is not relevant from an economic or financial perspective are considered neutral. You are given one of the polar sentiments and a length requirement. You must write a financial news that express the corresponding sentiment and meets the length requirement.</td>
</tr>
<tr id="A4.T2.1.9.8" class="ltx_tr">
<th id="A4.T2.1.9.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A4.T2.1.9.8.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.9.8.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Write a financial news with {label} sentiment and ensure that the length of the financial news is about {num_of_words} words. Be creative and write unique financial news.</td>
</tr>
<tr id="A4.T2.1.10.9" class="ltx_tr">
<th id="A4.T2.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Humor speech</th>
<td id="A4.T2.1.10.9.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.10.9.2.1" class="ltx_text ltx_font_bold">Context Prompt:</span> You are now creating a dataset containing humor and non-humor texts. Here are a few characteristics that might help understand what is humorous text: 1) Sarcasm and Irony: Sarcasm and irony involve stating one thing and meaning another, often the opposite. 2) Double Entendre: A double entendre is a figure of speech or a particular way of wording that is devised to have a double meaning, of which one is typically obvious, while the other often carries a risqué or ironic connotation. 3) Parody and Satire: Both involve imitating and exaggerating the features of a particular language style, genre, or piece of content to humorous effect. 4) Absurdity and Nonsense: Language that describes absurd or nonsensical scenarios can often be funny. This includes non-sequiturs, in which conclusions do not follow from their premises, and other forms of illogical statements.</td>
</tr>
<tr id="A4.T2.1.11.10" class="ltx_tr">
<td id="A4.T2.1.11.10.1" class="ltx_td"></td>
<td id="A4.T2.1.11.10.2" class="ltx_td ltx_align_left">
<span id="A4.T2.1.11.10.2.1" class="ltx_text ltx_font_bold">Data Generation Prompt:</span> Write a {label} short text and ensure that the length of the short text is about {num_of_words} words. Be creative and write unique short text.</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table D.2: </span>Detailed prompts for each task under the zero-shot and few-shot settings for data generation (Continued).</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.07847" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.07849" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.07849">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.07849" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.07850" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 01:25:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
