<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention</title>
<!--Generated on Mon Oct 14 17:44:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.10774v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S1" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S2" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S2.SS1" title="In 2 Related Works â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Camera Controllable Video Diffusion Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S2.SS2" title="In 2 Related Works â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multi-view Image Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S2.SS3" title="In 2 Related Works â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>4D Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS1" title="In 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS2" title="In 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Camera Controllable Video Diffusion Model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS2.SSS0.Px1" title="In 3.2 Camera Controllable Video Diffusion Model â€£ 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS2.SSS0.Px2" title="In 3.2 Camera Controllable Video Diffusion Model â€£ 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Camera Conditioning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS2.SSS0.Px3" title="In 3.2 Camera Controllable Video Diffusion Model â€£ 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Cross-frame Attention for Temporal Consistency</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS3" title="In 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Consistent Multi-view Video Diffusion Model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.SS3.SSS0.Px1" title="In 3.3 Consistent Multi-view Video Diffusion Model â€£ 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Cross-view Attention for Multi-view consistency</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Joint Training Strategy on Curated Data Mixtures</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.SS1" title="In 4 Joint Training Strategy on Curated Data Mixtures â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Joint Training on Data Mixtures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.SS2" title="In 4 Joint Training Strategy on Curated Data Mixtures â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Data Curation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS1" title="In 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Quantitative Comparisons</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS1.SSS0.Px1" title="In 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">3D Consistency of Frames</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS1.SSS0.Px2" title="In 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Multi-view Consistency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS1.SSS0.Px3" title="In 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Visual Quality</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS2" title="In 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Qualtitative Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.SS3" title="In 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Studies and Applications</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S6" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A1" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Data Curation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.SS0.SSS0.Px1" title="In Appendix B Additional Data Curation Details â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Static 3D Objects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.SS0.SSS0.Px2" title="In Appendix B Additional Data Curation Details â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Static 3D Scenes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.SS0.SSS0.Px3" title="In Appendix B Additional Data Curation Details â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Dynamic 3D Objects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.SS0.SSS0.Px4" title="In Appendix B Additional Data Curation Details â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title">Monocular Videos</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A3" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Evaluation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A4" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Ablation Studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A5" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Applications</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A5.SS1" title="In Appendix E Applications â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Advancing to Four Views at Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A5.SS2" title="In Appendix E Applications â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>3D Reconstruction of Generated Frames</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A6" title="In Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Limitations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">
Dejia Xu<sup class="ltx_sup" id="id1.1.id1.1">1</sup>â€‰â€‰,Yifan Jiang<sup class="ltx_sup" id="id1.1.id1.2">2</sup>, Chen Huang
<sup class="ltx_sup" id="id1.1.id1.3">2</sup>,
Liangchen Song<sup class="ltx_sup" id="id1.1.id1.4">2</sup>, Thorsten Gernoth<sup class="ltx_sup" id="id1.1.id1.5">2</sup>
<br class="ltx_break"/></span>â€ƒâ€ƒ<span class="ltx_text ltx_font_bold" id="id2.2.id2">Liangliang Cao<sup class="ltx_sup" id="id2.2.id2.1">3</sup>â€ , Zhangyang Wang<sup class="ltx_sup" id="id2.2.id2.2">1</sup>, Hao Tang<sup class="ltx_sup" id="id2.2.id2.3">2</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id2.2.id2.4"><span class="ltx_text ltx_font_medium" id="id2.2.id2.4.1">1</span></sup></span>University of Texas at Austin, <sup class="ltx_sup" id="id3.3.id3">2</sup>Apple, <sup class="ltx_sup" id="id4.4.id4">3</sup>Google
</span><span class="ltx_author_notes"><span class="ltx_text ltx_font_bold" id="id5.5.id1">This work was performed while Dejia Xu interned at Apple.</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">In recent years there have been remarkable breakthroughs in image-to-video generation.
However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce <span class="ltx_text ltx_font_bold" id="id6.id1.1">Cavia</span>, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion.
Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ir1d.github.io/Cavia/" title="">https://ir1d.github.io/Cavia/</a></p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotetext: </span>This work was performed while Liangliang Cao worked at Apple.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid development of diffusion models has enabled significant advancements in video generative models. Early efforts have explored various approaches, either training a video model from scratch or by fine-tuning pre-trained image generation models with additional temporal layersÂ <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib64" title="">2023a</a>; Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib17" title="">2022b</a>; Singer etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib55" title="">2022</a>; Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib16" title="">2022a</a>; Nan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>. The training data of these video models typically consist of a curated mixture of imageÂ <cite class="ltx_cite ltx_citemacro_citep">(Schuhmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib52" title="">2022</a>)</cite> and video datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Bain etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib3" title="">2021</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib64" title="">a</a>; Nan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>.
While substantial progress has been made in improving model architectures and refining training data, relatively little research has been conducted on the 3D consistency and camera controllability of generated videos.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To tackle this issue, several recent worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib66" title="">2023c</a>; He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>; Bahmani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib2" title="">2024</a>; Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>; Hou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib18" title="">2024</a>)</cite> have attempted to introduce camera controllability in video generation, aiming to ensure that generated frames adhere to viewpoint instructions, thereby improving 3D consistency.
These works either enhance viewpoint control through better conditioning signalsÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib66" title="">2023c</a>; He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>; Bahmani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib2" title="">2024</a>)</cite> or by utilizing geometric priors, such as epipolar constraintsÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> or explicit 3D representationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Hou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib18" title="">2024</a>)</cite>.
However, despite these efforts, the generated videos often lack precise 3D consistency or are restricted to static scenes with little to no object motion. Moreover, it remains challenging for monocular video generators to produce multi-view consistent videos of the same scene from different camera trajectories.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Since independently sampling multiple sequences often results in significantly inconsistent scenes, generating multiple video sequences simultaneously is desirable. However, this remains extremely challenging due to the scarcity of multi-view video data in the wild, leading to multi-view generations limited to inconsistent near-static scenes or synthetic objects.
A concurrent work, CVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite>, builds on multi-view static videosÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite> and warping-augmented monocular videosÂ <cite class="ltx_cite ltx_citemacro_citep">(Bain etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib3" title="">2021</a>)</cite>, but it can only generate videos with limited baselines, yielding inconsistent results when object motion is present. Another concurrent work, Vivid-ZOOÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib29" title="">2024a</a>)</cite>, leverages dynamic objects from ObjaverseÂ <cite class="ltx_cite ltx_citemacro_citep">(Deitke etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>)</cite> dataset and renders multi-view videos to train a video generator.
However, due to limited data sources, their results are primarily object-centric frames from fixed viewpoints, lacking realistic backgrounds.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address these challenges, we propose <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Cavia</span>, a novel framework that extends a monocular video generatorÂ <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite> to generate multi-view consistent videos with precise camera control. We enhance the spatial and temporal attention modules to cross-view and cross-frame 3D attentions respectively, improving consistency across both viewpoints and frames.
Our model architecture enables a novel joint training strategy that fully utilizes static, monocular, and multi-view dynamic videos. Static videosÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib76" title="">2023</a>; Xia etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib69" title="">2024</a>; Reizenstein etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>; Deitke etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib9" title="">a</a>)</cite> are converted to multi-view formats to ensure the geometric consistency in the generated frames. We then incorporate rendered synthetic multi-view videos of dynamic 3D objectsÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>; Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib23" title="">2024</a>; Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib32" title="">2024c</a>)</cite> to teach the model to generate reasonable object motion. To prevent overfitting on synthetic data, we finetune the model on pose-annotated monocular videosÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>; Nan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite> to enhance performance on complex scenes.
Our framework synthesizes cross-view and cross-frame consistent videos, and extensive evaluations on real and text-to-image generated images show its applicability across challenging indoor, outdoor, object-centric, and large-scene cases. We systematically measure the quality of the generated videos in terms of per-video and cross-view geometric consistency and perceptual quality. Our experiments demonstrate our superiority compared to previous works both qualitatively and quantitatively. Our experiments demonstrate superior performance compared to previous methods, both qualitatively and quantitatively. Additionally, we show that our method can extrapolate to generate four views during inference and enable 3D reconstruction of the generated frames.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our main contributions can be summarized as follows,</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a novel framework, <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Cavia</span>, for generating multi-view videos with camera controllability. We introduce view-integrated attentions, namely cross-view and cross-frame 3D attentions, to enhance consistency across viewpoints and frames.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We introduce an effective joint training strategy that leverages a curated mixture of static, monocular dynamic, and multi-view dynamic videos, ensuring geometric consistency, high-quality object motion, and background preservation in the generated results.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our experiments demonstrate superior geometric and perceptual quality in both monocular video generation and cross-video consistency compared to baseline methods. Additionally, our flexible framework can operate on four views at inference, offering improved view consistency and enabling 3D reconstruction of the generated frames.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Camera Controllable Video Diffusion Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Recent advancements in video diffusion models have significantly benefited from scaling model architectures and leveraging extensive datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Bain etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib3" title="">2021</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib64" title="">a</a>)</cite>, leading to impressive capabilities in generating high-quality videosÂ <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>; Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib17" title="">2022b</a>; Singer etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib55" title="">2022</a>; Ho etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib16" title="">2022a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib43" title="">OpenAI, </a>)</cite>.
While large foundational video diffusion models exist, our work focuses on enhancing camera control over video diffusion processes, a rapidly growing area of research. AnimateDiffÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib13" title="">2023</a>)</cite> and Stable Video Diffusion (SVD)Â <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite> employ individual camera LoRAÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib19" title="">2021</a>)</cite> models for specific camera motions. MotionCtrlÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib66" title="">2023c</a>)</cite> improves flexibility by introducing camera matrices, while CameraCtrlÂ <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>)</cite>, CamCoÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite>, and VD3DÂ <cite class="ltx_cite ltx_citemacro_citep">(Bahmani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib2" title="">2024</a>)</cite> enhance the camera control accuracy by introducing PlÃ¼cker coordinates to the video models via controlnetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Agrawala, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib79" title="">2023</a>)</cite>. To further improve the geometric consistency, CamCoÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> applies epipolar constraints and CamTrolÂ <cite class="ltx_cite ltx_citemacro_citep">(Hou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib18" title="">2024</a>)</cite> incorporates 3D GaussiansÂ <cite class="ltx_cite ltx_citemacro_citep">(Kerbl etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib26" title="">2023</a>)</cite>. However, these methods focus on monocular video generation, limiting their ability to sample multiple consistent video sequences of the same scene from distinct camera paths. CVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite> extends CameraCtrlÂ <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>)</cite> for multi-view video generation, but their results are constrained to simple camera and object motion. ViVid-ZooÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib29" title="">2024a</a>)</cite> extends MVDreamÂ <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib54" title="">2023b</a>)</cite> for multi-view purposes but is limited to object-centric results with fixed viewpoints. In contrast, our work explores view-integrated attentions for more precise camera control over arbitrary viewpoints and introduces a joint training strategy leveraging data mixtures to improve novel-view performance in complex scenes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multi-view Image Generation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Early approaches such as MVDiffusion <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib59" title="">2023</a>)</cite> focused on generating multiview images in parallel by employing correspondence-aware attention mechanisms, enabling effective cross-view information interaction, particularly for textured scene meshes. Recent approaches like Zero123++ <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib53" title="">2023a</a>)</cite>, Direct2.5 <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib40" title="">2024</a>)</cite>, Instant3D <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib30" title="">2023</a>)</cite>, MVDream <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib54" title="">2023b</a>)</cite>, MVDiffusion++ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib60" title="">2024</a>)</cite>, CAT3D <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib12" title="">2024</a>)</cite>, and Wonder3D <cite class="ltx_cite ltx_citemacro_citep">(Long etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib39" title="">2024</a>)</cite> have introduced single-pass frameworks for multiview generation, utilizing multiview self-attention to improve viewpoint consistency. Other works, such as SyncDreamer <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib38" title="">2023b</a>)</cite>, One-2-3-45 <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib36" title="">2024</a>)</cite>, Cascade-Zero123 <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib7" title="">2023</a>)</cite> and ConsistNet <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib73" title="">2024a</a>)</cite>, incorporate multiview features into 3D volumes to facilitate 3D-aware diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib37" title="">2023a</a>; Watson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib67" title="">2022</a>)</cite>.
Meanwhile, techniques such as Pose-Guided Diffusion <cite class="ltx_cite ltx_citemacro_citep">(Tseng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib61" title="">2023</a>)</cite>, Era3D <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib31" title="">2024b</a>)</cite>, Epidiff <cite class="ltx_cite ltx_citemacro_citep">(Huang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib20" title="">2024</a>)</cite>, and SPAD <cite class="ltx_cite ltx_citemacro_citep">(Kant etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib24" title="">2024</a>)</cite> have integrated epipolar-based features to facilitate enhanced viewpoint fusion within diffusion models. Finally, approaches like V3D <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib8" title="">2024b</a>)</cite>, IM-3D <cite class="ltx_cite ltx_citemacro_citep">(Melas-Kyriazi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib41" title="">2024</a>)</cite>, SV3D <cite class="ltx_cite ltx_citemacro_citep">(Voleti etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib63" title="">2024</a>)</cite> and Vivid-1-to-3 <cite class="ltx_cite ltx_citemacro_citep">(Kwak etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib28" title="">2024</a>)</cite> leverage priors from video diffusion models to achieve multiview generation with improved consistency.
However, these methods focus on generating static 3D objects or scenes, while our work introduces vivid object motion into multiview dynamic video generation in complex scenes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>4D Generation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Recent efforts in 4D generation have explored various methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Singer etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib56" title="">2023</a>; Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib82" title="">2023</a>; Bahmani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib1" title="">2023</a>; Zheng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib83" title="">2023</a>; Ling etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib34" title="">2023a</a>)</cite> that use score distillation from video diffusion models to optimize dynamic NeRFs or 3D Gaussians for text- or image-conditioned scenes.
Follow-up worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib22" title="">2023</a>; Ren etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib46" title="">2023</a>; Yin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib75" title="">2023</a>; Ren etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib47" title="">2024</a>; Zeng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib77" title="">2024</a>; Pan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib44" title="">2024</a>)</cite> investigate video-to-4D generation, enabling controllable 4D scene generation from monocular videos.
More recent methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>; Xie etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib70" title="">2024</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib78" title="">2024</a>)</cite> utilize video diffusion models to address the spatial-temporal consistency required for efficient 4D generation.
However, these approaches primarily focus on object-centric generation and face challenges in producing realistic results with complex backgrounds. In contrast, our work emphasizes generating multi-view, 3D-consistent videos for complex scenes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.4">Image-to-video generation takes a single image <math alttext="I_{0}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">I</mi><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ¼</ci><cn id="S3.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">I_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> as input and outputs a video sequence <math alttext="O_{1},\cdots,O_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.3"><semantics id="S3.SS1.p1.2.m2.3a"><mrow id="S3.SS1.p1.2.m2.3.3.2" xref="S3.SS1.p1.2.m2.3.3.3.cmml"><msub id="S3.SS1.p1.2.m2.2.2.1.1" xref="S3.SS1.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.2.2.1.1.2" xref="S3.SS1.p1.2.m2.2.2.1.1.2.cmml">O</mi><mn id="S3.SS1.p1.2.m2.2.2.1.1.3" xref="S3.SS1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.2.m2.3.3.2.3" xref="S3.SS1.p1.2.m2.3.3.3.cmml">,</mo><mi id="S3.SS1.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS1.p1.2.m2.1.1.cmml">â‹¯</mi><mo id="S3.SS1.p1.2.m2.3.3.2.4" xref="S3.SS1.p1.2.m2.3.3.3.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.p1.2.m2.3.3.2.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.2.cmml">O</mi><mi id="S3.SS1.p1.2.m2.3.3.2.2.3" xref="S3.SS1.p1.2.m2.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.3b"><list id="S3.SS1.p1.2.m2.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.2"><apply id="S3.SS1.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.2">ğ‘‚</ci><cn id="S3.SS1.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">â‹¯</ci><apply id="S3.SS1.p1.2.m2.3.3.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.2">ğ‘‚</ci><ci id="S3.SS1.p1.2.m2.3.3.2.2.3.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.3">ğ‘›</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.3c">O_{1},\cdots,O_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.3d">italic_O start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â‹¯ , italic_O start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. By introducing camera control, the model additionally takes in a sequence of camera information <math alttext="C_{1},\cdots,C_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.3"><semantics id="S3.SS1.p1.3.m3.3a"><mrow id="S3.SS1.p1.3.m3.3.3.2" xref="S3.SS1.p1.3.m3.3.3.3.cmml"><msub id="S3.SS1.p1.3.m3.2.2.1.1" xref="S3.SS1.p1.3.m3.2.2.1.1.cmml"><mi id="S3.SS1.p1.3.m3.2.2.1.1.2" xref="S3.SS1.p1.3.m3.2.2.1.1.2.cmml">C</mi><mn id="S3.SS1.p1.3.m3.2.2.1.1.3" xref="S3.SS1.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.3.m3.3.3.2.3" xref="S3.SS1.p1.3.m3.3.3.3.cmml">,</mo><mi id="S3.SS1.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p1.3.m3.1.1.cmml">â‹¯</mi><mo id="S3.SS1.p1.3.m3.3.3.2.4" xref="S3.SS1.p1.3.m3.3.3.3.cmml">,</mo><msub id="S3.SS1.p1.3.m3.3.3.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.cmml"><mi id="S3.SS1.p1.3.m3.3.3.2.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.2.cmml">C</mi><mi id="S3.SS1.p1.3.m3.3.3.2.2.3" xref="S3.SS1.p1.3.m3.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.3b"><list id="S3.SS1.p1.3.m3.3.3.3.cmml" xref="S3.SS1.p1.3.m3.3.3.2"><apply id="S3.SS1.p1.3.m3.2.2.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.2">ğ¶</ci><cn id="S3.SS1.p1.3.m3.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">â‹¯</ci><apply id="S3.SS1.p1.3.m3.3.3.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.3.3.2.2.1.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.2">ğ¶</ci><ci id="S3.SS1.p1.3.m3.3.3.2.2.3.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.3">ğ‘›</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.3c">C_{1},\cdots,C_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.3d">italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â‹¯ , italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, which dictates the desired viewpoint changes for the output sequence. In the multi-view scenario, we extend each batch of the camera control signal and output video sequence to <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_V</annotation></semantics></math> sequences.
In the following paragraphs, we present our proposed <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.4.1">Cavia</span> framework in detail.
First, we outline the preliminaries of image-to-video diffusion and describe how camera controllability is introduced in monocular video generation. Then, we elaborate on the model design for multi-view consistent video generation.
An overview of our framework is provided in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.F1" title="Figure 1 â€£ 3.1 Overview â€£ 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S3.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of Cavia is shown in (a). We introduce view-integrated attention modules, namely cross-view attentions and cross-frame attentions, which enforce viewpoint and temporal consistency of the generated frames, respectively. As illustrated in (b) and (c), our view-integrated attention incorporates additional feature dimensions into the attention mechanism, enhancing consistency across views and frames.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Camera Controllable Video Diffusion Model</h3>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Preliminaries</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.4">Our model builds on pre-trained Stable Video Diffusion (SVD)Â <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite>.
SVD extends Stable Diffusion 2.1<cite class="ltx_cite ltx_citemacro_citep">(Rombach etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib48" title="">2022</a>)</cite> by adding temporal convolution and attention layers, following the VideoLDM architectureÂ <cite class="ltx_cite ltx_citemacro_citep">(Blattmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib4" title="">2023</a>)</cite>.
SVD is trained with a continuous-time noise schedulerÂ <cite class="ltx_cite ltx_citemacro_citep">(Karras etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib25" title="">2022</a>)</cite>. In each iteration, the training data is perturbed by Gaussian noise <math alttext="\mathbf{n}(t)\sim\mathcal{N}(0,\sigma^{2}(t)\mathbf{I})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.4"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.4a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.2.cmml">ğ§</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.1.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml"><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">âˆ¼</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.3.cmml">ğ’©</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.2.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml"><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">(</mo><mn id="S3.SS2.SSS0.Px1.p1.1.m1.3.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.3.cmml">0</mn><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">,</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml"><msup id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.2.cmml">Ïƒ</mi><mn id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.3.cmml">2</mn></msup><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1a" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.4.cmml">ğˆ</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.4" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.4b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.2">similar-to</csymbol><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3"><times id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.1"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.3.2">ğ§</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">ğ‘¡</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.2"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.3">ğ’©</ci><interval closure="open" id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1"><cn id="S3.SS2.SSS0.Px1.p1.1.m1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.1.m1.3.3">0</cn><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1"><times id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.1"></times><apply id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.2">ğœ</ci><cn id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.2.3">2</cn></apply><ci id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2">ğ‘¡</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.1.4">ğˆ</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.4c">\mathbf{n}(t)\sim\mathcal{N}(0,\sigma^{2}(t)\mathbf{I})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.4d">bold_n ( italic_t ) âˆ¼ caligraphic_N ( 0 , italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_t ) bold_I )</annotation></semantics></math> and the diffusion model is tasked with estimating the clean data <math alttext="x_{0}\sim p_{0}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">x</mi><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">âˆ¼</mo><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">p</mi><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1">similar-to</csymbol><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2">ğ‘¥</ci><cn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3">0</cn></apply><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2">ğ‘</ci><cn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">x_{0}\sim p_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT âˆ¼ italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. Let <math alttext="p(\mathbf{x};\sigma(t))" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.3.m3.3"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.3a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3.cmml">p</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml"><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.cmml">ğ±</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml">;</mo><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.2.cmml">Ïƒ</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.1.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.4" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.3b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3"><times id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2"></times><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3">ğ‘</ci><list id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1"><ci id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2">ğ±</ci><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1"><times id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.1.1.1.2">ğœ</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">ğ‘¡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.3c">p(\mathbf{x};\sigma(t))</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.3.m3.3d">italic_p ( bold_x ; italic_Ïƒ ( italic_t ) )</annotation></semantics></math> denote the marginal probability of noisy data <math alttext="\mathbf{x}_{t}=\mathbf{x}_{0}+\mathbf{n}(t)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.cmml"><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.2.cmml">ğ±</mi><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.3.cmml">t</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.1.cmml">=</mo><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.cmml"><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.2.cmml">ğ±</mi><mn id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.3.cmml">0</mn></msub><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.1.cmml">+</mo><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.2.cmml">ğ§</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.1.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml"><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2"><eq id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.1"></eq><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.2">ğ±</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.2.3">ğ‘¡</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3"><plus id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.1"></plus><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.2">ğ±</ci><cn id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.2.3">0</cn></apply><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3"><times id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.1"></times><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.2.3.3.2">ğ§</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">\mathbf{x}_{t}=\mathbf{x}_{0}+\mathbf{n}(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.4.m4.1d">bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + bold_n ( italic_t )</annotation></semantics></math>, the iterative refinement process of diffusion model corresponds to the probability flow ordinary differential equation (ODE):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d\mathbf{x}=-\dot{\sigma}(t)\sigma(t)\nabla_{\mathbf{x}}\log p(\mathbf{x};%
\sigma(t))dt." class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2" xref="S3.E1.m1.5.5.1.1.3.2.cmml">d</mi><mo id="S3.E1.m1.5.5.1.1.3.1" xref="S3.E1.m1.5.5.1.1.3.1.cmml">â¢</mo><mi id="S3.E1.m1.5.5.1.1.3.3" xref="S3.E1.m1.5.5.1.1.3.3.cmml">ğ±</mi></mrow><mo id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1a" xref="S3.E1.m1.5.5.1.1.1.cmml">âˆ’</mo><mrow id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.5.5.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.3.2.cmml">Ïƒ</mi><mo id="S3.E1.m1.5.5.1.1.1.1.3.1" xref="S3.E1.m1.5.5.1.1.1.1.3.1.cmml">Ë™</mo></mover><mo id="S3.E1.m1.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.4.2" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.4.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">t</mi><mo id="S3.E1.m1.5.5.1.1.1.1.4.2.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.2a" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">â¢</mo><mi id="S3.E1.m1.5.5.1.1.1.1.5" xref="S3.E1.m1.5.5.1.1.1.1.5.cmml">Ïƒ</mi><mo id="S3.E1.m1.5.5.1.1.1.1.2b" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.6.2" xref="S3.E1.m1.5.5.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.6.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">t</mi><mo id="S3.E1.m1.5.5.1.1.1.1.6.2.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.2c" lspace="0.167em" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.7" xref="S3.E1.m1.5.5.1.1.1.1.7.cmml"><mrow id="S3.E1.m1.5.5.1.1.1.1.7.1" xref="S3.E1.m1.5.5.1.1.1.1.7.1.cmml"><msub id="S3.E1.m1.5.5.1.1.1.1.7.1.1" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.7.1.1.2" rspace="0.167em" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.2.cmml">âˆ‡</mo><mi id="S3.E1.m1.5.5.1.1.1.1.7.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.3.cmml">ğ±</mi></msub><mi id="S3.E1.m1.5.5.1.1.1.1.7.1.2" xref="S3.E1.m1.5.5.1.1.1.1.7.1.2.cmml">log</mi></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.7a" lspace="0.167em" xref="S3.E1.m1.5.5.1.1.1.1.7.cmml">â¡</mo><mi id="S3.E1.m1.5.5.1.1.1.1.7.2" xref="S3.E1.m1.5.5.1.1.1.1.7.2.cmml">p</mi></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.2d" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">ğ±</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">;</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml">Ïƒ</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml">â¢</mo><mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">t</mi><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.1.1.4" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.1.1.1.1.2e" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">â¢</mo><mi id="S3.E1.m1.5.5.1.1.1.1.8" xref="S3.E1.m1.5.5.1.1.1.1.8.cmml">d</mi><mo id="S3.E1.m1.5.5.1.1.1.1.2f" xref="S3.E1.m1.5.5.1.1.1.1.2.cmml">â¢</mo><mi id="S3.E1.m1.5.5.1.1.1.1.9" xref="S3.E1.m1.5.5.1.1.1.1.9.cmml">t</mi></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" lspace="0em" xref="S3.E1.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"></eq><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><times id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.1"></times><ci id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2">ğ‘‘</ci><ci id="S3.E1.m1.5.5.1.1.3.3.cmml" xref="S3.E1.m1.5.5.1.1.3.3">ğ±</ci></apply><apply id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"><minus id="S3.E1.m1.5.5.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1"></minus><apply id="S3.E1.m1.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.2"></times><apply id="S3.E1.m1.5.5.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3"><ci id="S3.E1.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.1">Ë™</ci><ci id="S3.E1.m1.5.5.1.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.3.2">ğœ</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ‘¡</ci><ci id="S3.E1.m1.5.5.1.1.1.1.5.cmml" xref="S3.E1.m1.5.5.1.1.1.1.5">ğœ</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğ‘¡</ci><apply id="S3.E1.m1.5.5.1.1.1.1.7.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7"><apply id="S3.E1.m1.5.5.1.1.1.1.7.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1"><apply id="S3.E1.m1.5.5.1.1.1.1.7.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.7.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.7.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.2">âˆ‡</ci><ci id="S3.E1.m1.5.5.1.1.1.1.7.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.1.3">ğ±</ci></apply><log id="S3.E1.m1.5.5.1.1.1.1.7.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.1.2"></log></apply><ci id="S3.E1.m1.5.5.1.1.1.1.7.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.7.2">ğ‘</ci></apply><list id="S3.E1.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1"><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ğ±</ci><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1"><times id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2">ğœ</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ğ‘¡</ci></apply></list><ci id="S3.E1.m1.5.5.1.1.1.1.8.cmml" xref="S3.E1.m1.5.5.1.1.1.1.8">ğ‘‘</ci><ci id="S3.E1.m1.5.5.1.1.1.1.9.cmml" xref="S3.E1.m1.5.5.1.1.1.1.9">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">d\mathbf{x}=-\dot{\sigma}(t)\sigma(t)\nabla_{\mathbf{x}}\log p(\mathbf{x};%
\sigma(t))dt.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_d bold_x = - overË™ start_ARG italic_Ïƒ end_ARG ( italic_t ) italic_Ïƒ ( italic_t ) âˆ‡ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_p ( bold_x ; italic_Ïƒ ( italic_t ) ) italic_d italic_t .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.9"><math alttext="\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma(t))" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.5.m1.3"><semantics id="S3.SS2.SSS0.Px1.p1.5.m1.3a"><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.2.cmml">âˆ‡</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.3.cmml">ğ±</mi></msub><mi id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.2.cmml">log</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3a" lspace="0.167em" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.cmml">â¡</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.2.cmml">p</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.2.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml"><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m1.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.2.2.cmml">ğ±</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml">;</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.2.cmml">Ïƒ</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.1.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m1.1.1.cmml">t</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.4" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m1.3b"><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3"><times id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.2"></times><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3"><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1"><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.2">âˆ‡</ci><ci id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.1.3">ğ±</ci></apply><log id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.1.2"></log></apply><ci id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.3.2">ğ‘</ci></apply><list id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1"><ci id="S3.SS2.SSS0.Px1.p1.5.m1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.2.2">ğ±</ci><apply id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1"><times id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.3.3.1.1.1.2">ğœ</ci><ci id="S3.SS2.SSS0.Px1.p1.5.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m1.1.1">ğ‘¡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m1.3c">\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma(t))</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.5.m1.3d">âˆ‡ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_p ( bold_x ; italic_Ïƒ ( italic_t ) )</annotation></semantics></math> refers to the score function, which is parameterized by a denoiser <math alttext="D_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.6.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.6.m2.1a"><msub id="S3.SS2.SSS0.Px1.p1.6.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.3.cmml">ğœ½</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.6.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.2">ğ·</ci><ci id="S3.SS2.SSS0.Px1.p1.6.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m2.1.1.3">ğœ½</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.6.m2.1c">D_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.6.m2.1d">italic_D start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> through
<math alttext="\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma)\approx\left(D_{\bm{\theta}}(%
\mathbf{x};\sigma)-\mathbf{x}\right)/\sigma^{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.7.m3.5"><semantics id="S3.SS2.SSS0.Px1.p1.7.m3.5a"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.cmml"><msub id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.2.cmml">âˆ‡</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.3.cmml">ğ±</mi></msub><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.2.cmml">log</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2a" lspace="0.167em" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.cmml">â¡</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.2.cmml">p</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.1.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.1.1.cmml">ğ±</mi><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml">;</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.2.2.cmml">Ïƒ</mi><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.2.cmml">â‰ˆ</mo><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.cmml"><msub id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.3.cmml">ğœ½</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.1.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.3.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.3.3.cmml">ğ±</mi><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml">;</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.4.4" xref="S3.SS2.SSS0.Px1.p1.7.m3.4.4.cmml">Ïƒ</mi><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.3.cmml">ğ±</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.2.cmml">/</mo><msup id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.2" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.2.cmml">Ïƒ</mi><mn id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.3" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.7.m3.5b"><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5"><approx id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.2"></approx><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3"><times id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.1"></times><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2"><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1"><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.2">âˆ‡</ci><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.1.3">ğ±</ci></apply><log id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.1.2"></log></apply><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.2.2">ğ‘</ci></apply><list id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.3.3.2"><ci id="S3.SS2.SSS0.Px1.p1.7.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.1.1">ğ±</ci><ci id="S3.SS2.SSS0.Px1.p1.7.m3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.2.2">ğœ</ci></list></apply><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1"><divide id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.2"></divide><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1"><minus id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.1"></minus><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2"><times id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.1"></times><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.2">ğ·</ci><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.2.3">ğœ½</ci></apply><list id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.2.3.2"><ci id="S3.SS2.SSS0.Px1.p1.7.m3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.3.3">ğ±</ci><ci id="S3.SS2.SSS0.Px1.p1.7.m3.4.4.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.4.4">ğœ</ci></list></apply><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.1.1.1.3">ğ±</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.2">ğœ</ci><cn id="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.7.m3.5.5.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.7.m3.5c">\nabla_{\mathbf{x}}\log p(\mathbf{x};\sigma)\approx\left(D_{\bm{\theta}}(%
\mathbf{x};\sigma)-\mathbf{x}\right)/\sigma^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.7.m3.5d">âˆ‡ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_p ( bold_x ; italic_Ïƒ ) â‰ˆ ( italic_D start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT ( bold_x ; italic_Ïƒ ) - bold_x ) / italic_Ïƒ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>. We follow the EDM-preconditioning frameworkÂ <cite class="ltx_cite ltx_citemacro_citep">(Karras etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib25" title="">2022</a>; Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite> and parameterize <math alttext="D_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.8.m4.1"><semantics id="S3.SS2.SSS0.Px1.p1.8.m4.1a"><msub id="S3.SS2.SSS0.Px1.p1.8.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.3.cmml">ğœ½</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.8.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.2">ğ·</ci><ci id="S3.SS2.SSS0.Px1.p1.8.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m4.1.1.3">ğœ½</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.8.m4.1c">D_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.8.m4.1d">italic_D start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> with a neural network <math alttext="F_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.9.m5.1"><semantics id="S3.SS2.SSS0.Px1.p1.9.m5.1a"><msub id="S3.SS2.SSS0.Px1.p1.9.m5.1.1" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.2" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.2.cmml">F</mi><mi id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.3" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.3.cmml">ğœ½</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.9.m5.1b"><apply id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.2">ğ¹</ci><ci id="S3.SS2.SSS0.Px1.p1.9.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m5.1.1.3">ğœ½</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.9.m5.1c">F_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.9.m5.1d">italic_F start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> as follows,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="D_{\bm{\theta}}=c_{\text{skip}}\mathbf{x}+c_{\text{out}}F_{\bm{\theta}}(c_{%
\text{in}}\mathbf{x};c_{\text{noise}})." class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml"><mi id="S3.E2.m1.1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.1.4.2.cmml">D</mi><mi id="S3.E2.m1.1.1.1.1.4.3" xref="S3.E2.m1.1.1.1.1.4.3.cmml">ğœ½</mi></msub><mo id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.2.4.cmml"><msub id="S3.E2.m1.1.1.1.1.2.4.2" xref="S3.E2.m1.1.1.1.1.2.4.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.4.2.2" xref="S3.E2.m1.1.1.1.1.2.4.2.2.cmml">c</mi><mtext id="S3.E2.m1.1.1.1.1.2.4.2.3" xref="S3.E2.m1.1.1.1.1.2.4.2.3a.cmml">skip</mtext></msub><mo id="S3.E2.m1.1.1.1.1.2.4.1" xref="S3.E2.m1.1.1.1.1.2.4.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.1.1.2.4.3" xref="S3.E2.m1.1.1.1.1.2.4.3.cmml">ğ±</mi></mrow><mo id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.cmml"><msub id="S3.E2.m1.1.1.1.1.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.4.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.4.2" xref="S3.E2.m1.1.1.1.1.2.2.4.2.cmml">c</mi><mtext id="S3.E2.m1.1.1.1.1.2.2.4.3" xref="S3.E2.m1.1.1.1.1.2.2.4.3a.cmml">out</mtext></msub><mo id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">â¢</mo><msub id="S3.E2.m1.1.1.1.1.2.2.5" xref="S3.E2.m1.1.1.1.1.2.2.5.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.5.2" xref="S3.E2.m1.1.1.1.1.2.2.5.2.cmml">F</mi><mi id="S3.E2.m1.1.1.1.1.2.2.5.3" xref="S3.E2.m1.1.1.1.1.2.2.5.3.cmml">ğœ½</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.2.3a" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">â¢</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml"><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">c</mi><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3a.cmml">in</mtext></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">ğ±</mi></mrow><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">;</mo><msub id="S3.E2.m1.1.1.1.1.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml">c</mi><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3a.cmml">noise</mtext></msub><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.5" stretchy="false" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" lspace="0em" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"></eq><apply id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2">ğ·</ci><ci id="S3.E2.m1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.4.3">ğœ½</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><plus id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3"></plus><apply id="S3.E2.m1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.4"><times id="S3.E2.m1.1.1.1.1.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.4.1"></times><apply id="S3.E2.m1.1.1.1.1.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.4.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.4.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.2.4.2.3a.cmml" xref="S3.E2.m1.1.1.1.1.2.4.2.3"><mtext id="S3.E2.m1.1.1.1.1.2.4.2.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.2.4.2.3">skip</mtext></ci></apply><ci id="S3.E2.m1.1.1.1.1.2.4.3.cmml" xref="S3.E2.m1.1.1.1.1.2.4.3">ğ±</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2"><times id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3"></times><apply id="S3.E2.m1.1.1.1.1.2.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.4.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.4.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.2.2.4.3a.cmml" xref="S3.E2.m1.1.1.1.1.2.2.4.3"><mtext id="S3.E2.m1.1.1.1.1.2.2.4.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.2.2.4.3">out</mtext></ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.5.cmml" xref="S3.E2.m1.1.1.1.1.2.2.5"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.5.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.5">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.5.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.5.2">ğ¹</ci><ci id="S3.E2.m1.1.1.1.1.2.2.5.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.5.3">ğœ½</ci></apply><list id="S3.E2.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3">in</mtext></ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">ğ±</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3a.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3"><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3">noise</mtext></ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">D_{\bm{\theta}}=c_{\text{skip}}\mathbf{x}+c_{\text{out}}F_{\bm{\theta}}(c_{%
\text{in}}\mathbf{x};c_{\text{noise}}).</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_D start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT = italic_c start_POSTSUBSCRIPT skip end_POSTSUBSCRIPT bold_x + italic_c start_POSTSUBSCRIPT out end_POSTSUBSCRIPT italic_F start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT ( italic_c start_POSTSUBSCRIPT in end_POSTSUBSCRIPT bold_x ; italic_c start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.11">During training, the network <math alttext="F_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.10.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.10.m1.1a"><msub id="S3.SS2.SSS0.Px1.p1.10.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.2.cmml">F</mi><mi id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.3.cmml">ğœ½</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.10.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.2">ğ¹</ci><ci id="S3.SS2.SSS0.Px1.p1.10.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m1.1.1.3">ğœ½</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.10.m1.1c">F_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.10.m1.1d">italic_F start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> is optimized using denoising score matching for <math alttext="D_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.11.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.11.m2.1a"><msub id="S3.SS2.SSS0.Px1.p1.11.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.3.cmml">ğœ½</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.11.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.2">ğ·</ci><ci id="S3.SS2.SSS0.Px1.p1.11.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m2.1.1.3">ğœ½</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.11.m2.1c">D_{\bm{\theta}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.11.m2.1d">italic_D start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}\left[\left\|D_{\bm{\theta}}\left(\mathbf{x}_{0}+\mathbf{n};\sigma,%
\text{cond}\right)-\mathbf{x}_{0}\right\|_{2}^{2}\right]." class="ltx_Math" display="block" id="S3.E3.m1.3"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml">ğ”¼</mi><mo id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml">â¢</mo><mrow id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.2.cmml"><mo id="S3.E3.m1.3.3.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.2.1.cmml">[</mo><msubsup id="S3.E3.m1.3.3.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml">D</mi><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">ğœ½</mi></msub><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ±</mi><mn id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">0</mn></msub><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ§</mi></mrow><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">Ïƒ</mi><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mtext id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2a.cmml">cond</mtext><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.5" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">âˆ’</mo><msub id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">ğ±</mi><mn id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E3.m1.3.3.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E3.m1.3.3.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.3.cmml">2</mn></msubsup><mo id="S3.E3.m1.3.3.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.2" lspace="0em" xref="S3.E3.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><times id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2"></times><ci id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3">ğ”¼</ci><apply id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.2"></minus><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">ğ·</ci><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">ğœ½</ci></apply><list id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1"><plus id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ±</ci><cn id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">0</cn></apply><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ§</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğœ</ci><ci id="S3.E3.m1.2.2a.cmml" xref="S3.E3.m1.2.2"><mtext id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">cond</mtext></ci></list></apply><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">ğ±</ci><cn id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply><cn id="S3.E3.m1.3.3.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.1.1.1.3">2</cn></apply><cn id="S3.E3.m1.3.3.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\mathbb{E}\left[\left\|D_{\bm{\theta}}\left(\mathbf{x}_{0}+\mathbf{n};\sigma,%
\text{cond}\right)-\mathbf{x}_{0}\right\|_{2}^{2}\right].</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.3d">blackboard_E [ âˆ¥ italic_D start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + bold_n ; italic_Ïƒ , cond ) - bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Camera Conditioning</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.10">Although SVD is pre-trained on various high-quality video and image data, it does not natively support precise camera control instructions directly.
To address this, we introduce camera conditioning to the model via PlÃ¼cker coordinates Â <cite class="ltx_cite ltx_citemacro_citep">(Jia, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib21" title="">2020</a>)</cite>, which is widely adopted as position
embeddings in 360<sup class="ltx_sup" id="S3.SS2.SSS0.Px2.p1.10.1">âˆ˜</sup> unbounded light fields<cite class="ltx_cite ltx_citemacro_citep">(Sitzmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib57" title="">2021</a>)</cite>. PlÃ¼cker coordinates are defined as <math alttext="P=(d^{\prime},o\times d^{\prime})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.2.m2.2"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.2a"><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.4.cmml">P</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml"><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.3" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml">(</mo><msup id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.3.cmml">â€²</mo></msup><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml">,</mo><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2.cmml">o</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.1.cmml">Ã—</mo><msup id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.3.cmml">â€²</mo></msup></mrow><mo id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.5" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.2b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2"><eq id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.3"></eq><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.4.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.4">ğ‘ƒ</ci><interval closure="open" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.2">ğ‘‘</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.3">â€²</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2"><times id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.1"></times><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2">ğ‘œ</ci><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.2">ğ‘‘</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.3">â€²</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.2c">P=(d^{\prime},o\times d^{\prime})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.2.m2.2d">italic_P = ( italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT , italic_o Ã— italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT )</annotation></semantics></math>, where <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mo id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><times id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.3.m3.1d">Ã—</annotation></semantics></math> is the cross product and <math alttext="d^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><msup id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2">ğ‘‘</ci><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">d^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.4.m4.1d">italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math> refers to the normalized ray direction <math alttext="d^{\prime}=\frac{d}{||d||}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.cmml"><msup id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.3.cmml">â€²</mo></msup><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1.cmml">=</mo><mfrac id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml">d</mi><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.cmml"><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3.1" maxsize="142%" minsize="142%" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.1.cmml">â€–</mo><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.1.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3.2" maxsize="142%" minsize="142%" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.1.cmml">â€–</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2"><eq id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1"></eq><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.2">ğ‘‘</ci><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.3">â€²</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1"><divide id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1"></divide><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3">ğ‘‘</ci><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.3.1">norm</csymbol><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.1">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">d^{\prime}=\frac{d}{||d||}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.5.m5.1d">italic_d start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = divide start_ARG italic_d end_ARG start_ARG | | italic_d | | end_ARG</annotation></semantics></math>. Let camera extrinsic matrix be <math alttext="E=[\bf{R}|\bf{T}]" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml">E</mi><mo id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.cmml"><mo id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.1.cmml">[</mo><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.2.cmml">ğ‘</mi><mo fence="false" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.3.cmml">ğ“</mi></mrow><mo id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"><eq id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2"></eq><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3">ğ¸</ci><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.1.1.3">ğ“</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">E=[\bf{R}|\bf{T}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.6.m6.1d">italic_E = [ bold_R | bold_T ]</annotation></semantics></math> and intrinsic matrix be <math alttext="\bf{K}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.7.m7.1"><semantics id="S3.SS2.SSS0.Px2.p1.7.m7.1a"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml">ğŠ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m7.1b"><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1">ğŠ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m7.1c">\bf{K}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.7.m7.1d">bold_K</annotation></semantics></math>,
the ray direction <math alttext="d_{x,y}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.8.m8.2"><semantics id="S3.SS2.SSS0.Px2.p1.8.m8.2a"><msub id="S3.SS2.SSS0.Px2.p1.8.m8.2.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.2.3.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3.2.cmml">d</mi><mrow id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1.1.1.cmml">x</mi><mo id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.4.1" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.2.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.8.m8.2b"><apply id="S3.SS2.SSS0.Px2.p1.8.m8.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.8.m8.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.8.m8.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.3.2">ğ‘‘</ci><list id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.4"><ci id="S3.SS2.SSS0.Px2.p1.8.m8.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1.1.1">ğ‘¥</ci><ci id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.2.2">ğ‘¦</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.8.m8.2c">d_{x,y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.8.m8.2d">italic_d start_POSTSUBSCRIPT italic_x , italic_y end_POSTSUBSCRIPT</annotation></semantics></math> for 2D pixel located at <math alttext="(x,y)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.9.m9.2"><semantics id="S3.SS2.SSS0.Px2.p1.9.m9.2a"><mrow id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml">x</mi><mo id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2.2" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.9.m9.2.2" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.2.cmml">y</mi><mo id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.9.m9.2b"><interval closure="open" id="S3.SS2.SSS0.Px2.p1.9.m9.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.3.2"><ci id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1">ğ‘¥</ci><ci id="S3.SS2.SSS0.Px2.p1.9.m9.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.9.m9.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.9.m9.2d">( italic_x , italic_y )</annotation></semantics></math> is formulated as <math alttext="d=\bf{R}\bf{K}^{-1}(\begin{smallmatrix}x\\
y\\
1\end{smallmatrix})+\bf{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.10.m10.1"><semantics id="S3.SS2.SSS0.Px2.p1.10.m10.1a"><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.2.cmml">d</mi><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.1.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.cmml"><msup id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.2.cmml">ğ‘ğŠ</mi><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.cmml"><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3a" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.cmml">âˆ’</mo><mn id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.2.cmml">ğŸ</mn></mrow></msup><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.1.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml">(</mo><mtable id="S3.SS2.SSS0.Px2.p1.10.m10.1.1" rowspacing="0pt" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mtr id="S3.SS2.SSS0.Px2.p1.10.m10.1.1a" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mtd id="S3.SS2.SSS0.Px2.p1.10.m10.1.1b" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.1.1.cmml">ğ±</mi></mtd></mtr><mtr id="S3.SS2.SSS0.Px2.p1.10.m10.1.1c" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mtd id="S3.SS2.SSS0.Px2.p1.10.m10.1.1d" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.1.1.cmml">ğ²</mi></mtd></mtr><mtr id="S3.SS2.SSS0.Px2.p1.10.m10.1.1e" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mtd id="S3.SS2.SSS0.Px2.p1.10.m10.1.1f" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.1.1.cmml">ğŸ</mn></mtd></mtr></mtable><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2.2" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.1.cmml">+</mo><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.3.cmml">ğ“</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.10.m10.1b"><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2"><eq id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.1"></eq><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.2">ğ‘‘</ci><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3"><plus id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.1"></plus><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2"><times id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.1"></times><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.2">ğ‘ğŠ</ci><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3"><minus id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3"></minus><cn id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.2.3.2">1</cn></apply></apply><matrix id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2"><matrixrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.1a.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2"><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.1.1">ğ±</ci></matrixrow><matrixrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.1b.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2"><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.1.1">ğ²</ci></matrixrow><matrixrow id="S3.SS2.SSS0.Px2.p1.10.m10.1.1c.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.2.3.2"><cn id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.1.1.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.1.1">1</cn></matrixrow></matrix></apply><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.2.3.3">ğ“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.10.m10.1c">d=\bf{R}\bf{K}^{-1}(\begin{smallmatrix}x\\
y\\
1\end{smallmatrix})+\bf{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.10.m10.1d">italic_d = bold_RK start_POSTSUPERSCRIPT - bold_1 end_POSTSUPERSCRIPT ( start_ROW start_CELL bold_x end_CELL end_ROW start_ROW start_CELL bold_y end_CELL end_ROW start_ROW start_CELL bold_1 end_CELL end_ROW ) + bold_T</annotation></semantics></math>.
These spatial PlÃ¼cker coordinates are concatenated channel-wise with the original latent inputs of SVD. We enlarge the convolution kernel of the first layer accordingly. The newly introduced matrices are zero-initialized to ensure training stability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1">We utilize a relative camera coordinate system, where the first frame is positioned at the world origin with an identity matrix for rotation. The following frames are rotated accordingly. To stabilize training, we normalize the scale of the training sequences to a unit scale. This is implemented by resizing the maximum distance-to-origin in the multi-view camera sequence to 1.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Cross-frame Attention for Temporal Consistency</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">Vanilla 1D temporal attention in the SVD backbone is insufficient for modeling large pixel displacements when the viewpoint changesÂ <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib54" title="">2023b</a>; Yang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib74" title="">2024b</a>)</cite>. In vanilla 1D temporal attention, attention matrices are calculated over the frame number dimension, and latent features only interact with features from the same spatial location across frames. This limits information flow between different spatial-temporal locations. While this might not be a big issue for video generation with limited motion, viewpoint changes typically cause significant pixel displacements, which calls for better architecture for more efficient information propagation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p2.5">To overcome this issue, we inflate the original 1D temporal attention modules in the SVD network into 3D cross-frame temporal attention modules, allowing for joint modeling of spatial-temporal feature coherence.
The inflation operation can be achieved by rearranging the latent features before the attention matrix calculations.
Consider the latent features of shape <math alttext="(B\hskip 2.5ptV\hskip 2.5ptF\hskip 2.5ptC\hskip 2.5ptH\hskip 2.5ptW)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.1.m1.1"><semantics id="S3.SS2.SSS0.Px3.p2.1.m1.1a"><mrow id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.2.cmml">B</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.3.cmml">V</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.4" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.4.cmml">F</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1b" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.5" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.5.cmml">C</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1c" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.6" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.6.cmml">H</mi><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1d" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.7" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.7.cmml">W</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.2">ğµ</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.3">ğ‘‰</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.4">ğ¹</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.5.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.5">ğ¶</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.6.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.6">ğ»</ci><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.7.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.1.1.7">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.1.m1.1c">(B\hskip 2.5ptV\hskip 2.5ptF\hskip 2.5ptC\hskip 2.5ptH\hskip 2.5ptW)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.1.m1.1d">( italic_B italic_V italic_F italic_C italic_H italic_W )</annotation></semantics></math> where <math alttext="F" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.2.m2.1"><semantics id="S3.SS2.SSS0.Px3.p2.2.m2.1a"><mi id="S3.SS2.SSS0.Px3.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p2.2.m2.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.2.m2.1b"><ci id="S3.SS2.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.2.m2.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.2.m2.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.2.m2.1d">italic_F</annotation></semantics></math> refers to the length of frames and <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.3.m3.1"><semantics id="S3.SS2.SSS0.Px3.p2.3.m3.1a"><mi id="S3.SS2.SSS0.Px3.p2.3.m3.1.1" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.3.m3.1b"><ci id="S3.SS2.SSS0.Px3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.3.m3.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.3.m3.1d">italic_V</annotation></semantics></math> is the number of views, instead of employing 1D attention mechanism on rearranged shape of <math alttext="((B\hskip 2.5ptV\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptF\hskip 2.5ptC)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.4.m4.1"><semantics id="S3.SS2.SSS0.Px3.p2.4.m4.1a"><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.2.cmml">B</mi><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.3.cmml">V</mi><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.4" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.4.cmml">H</mi><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1b" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.5" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.5.cmml">W</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.3.cmml">F</mi><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.4" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.4.cmml">C</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.4.m4.1b"><apply id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.2"></times><apply id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.2">ğµ</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.3">ğ‘‰</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.4">ğ»</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.5.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.1.1.1.5">ğ‘Š</ci></apply><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.3">ğ¹</ci><ci id="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.4.m4.1.1.1.1.4">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.4.m4.1c">((B\hskip 2.5ptV\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptF\hskip 2.5ptC)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.4.m4.1d">( ( italic_B italic_V italic_H italic_W ) italic_F italic_C )</annotation></semantics></math>, our inflated attention operates on the rearranged shape of <math alttext="((B\hskip 2.5ptV)\hskip 2.5pt(F\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptC)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p2.5.m5.1"><semantics id="S3.SS2.SSS0.Px3.p2.5.m5.1a"><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.2.cmml">B</mi><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.1" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.3.cmml">V</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3.cmml">â¢</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml"><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.2" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.2" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.2.cmml">F</mi><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.3" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.3.cmml">H</mi><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.4" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.4.cmml">W</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml">)</mo></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3a" lspace="0.250em" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3.cmml">â¢</mo><mi id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.4" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.4.cmml">C</mi></mrow><mo id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.3" stretchy="false" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.5.m5.1b"><apply id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.3"></times><apply id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1"><times id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.2">ğµ</ci><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.1.1.1.3">ğ‘‰</ci></apply><apply id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1"><times id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.1"></times><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.2">ğ¹</ci><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.3">ğ»</ci><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.2.1.1.4">ğ‘Š</ci></apply><ci id="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.4.cmml" xref="S3.SS2.SSS0.Px3.p2.5.m5.1.1.1.1.4">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.5.m5.1c">((B\hskip 2.5ptV)\hskip 2.5pt(F\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptC)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p2.5.m5.1d">( ( italic_B italic_V ) ( italic_F italic_H italic_W ) italic_C )</annotation></semantics></math>, integrating spatial features into the attention matrices.
A visualization is provided in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.F1" title="Figure 1 â€£ 3.1 Overview â€£ 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a>(c).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p3.1">Since our rearrange operation only alters the sequence length of the attention inputs without modifying the feature dimensions, we can seamlessly inherit the pre-trained weights from the SVD backbone for our purpose. Thanks to this rearrange operation, our inflated temporal attention now calculates the similarity of spatial-temporal features simultaneously, accommodating larger pixel displacements while maintaining temporal consistency.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Consistent Multi-view Video Diffusion Model</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Adding PlÃ¼cker coordinates for camera control and introducing improved temporal attention allows the video diffusion model to generate reasonably consistent monocular videos. However, for multi-view generation, a monocular video diffusion model that generates samples independently cannot ensure view consistency across multiple sequences. To address this, we introduce novel design mechanisms and training strategies to extend the monocular video diffusion model to the multi-view generation task.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Cross-view Attention for Multi-view consistency</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.3">To improve cross-view consistency in multi-view videos, we aim to encourage information exchange during the generation process.
Since our temporal cross-frame attention modules already handle intra-view feature connections within each video sequence, we focus on exchanging inter-view signals through the spatial cross-view modules.
Inspired by MVDreamÂ <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib54" title="">2023b</a>)</cite>, we introduce 3D cross-view attention modules, inflated from the spatial attention blocks of SVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite>.
Specifically, we rearrange the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px1.p1.3.1">V</span> views such that frames at each corresponding timesteps are concatenated before being sent into the attention modules.
In detail, we rearrange the latent features from shape <math alttext="(B\hskip 2.5ptV\hskip 2.5ptF\hskip 2.5ptC\hskip 2.5ptH\hskip 2.5ptW)" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml"><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.2" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml">B</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml">V</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1a" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.4" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.4.cmml">F</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1b" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.5" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.5.cmml">C</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1c" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.6" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.6.cmml">H</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1d" lspace="0.250em" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml">â¢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.7" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.7.cmml">W</mi></mrow><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.3" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1"><times id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.1"></times><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.2">ğµ</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.3">ğ‘‰</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.4">ğ¹</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.5.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.5">ğ¶</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.6.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.6">ğ»</ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.7.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.1.7">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">(B\hskip 2.5ptV\hskip 2.5ptF\hskip 2.5ptC\hskip 2.5ptH\hskip 2.5ptW)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">( italic_B italic_V italic_F italic_C italic_H italic_W )</annotation></semantics></math> to <math alttext="(((B\hskip 2.5ptF)(V\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptC)" class="ltx_math_unparsed" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" stretchy="false">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.1.2"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.1" stretchy="false">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2.1" stretchy="false">(</mo><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2.2">B</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2.3">F</mi><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.2.4" stretchy="false">)</mo></mrow><mrow id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.1" stretchy="false">(</mo><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.2">V</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.3">H</mi><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.4">W</mi><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.3.5" rspace="0.250em" stretchy="false">)</mo></mrow><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.4">C</mi><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">(((B\hskip 2.5ptF)(V\hskip 2.5ptH\hskip 2.5ptW)\hskip 2.5ptC)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.1d">( ( ( italic_B italic_F ) ( italic_V italic_H italic_W ) italic_C )</annotation></semantics></math> instead of <math alttext="(((B\hskip 2.5ptV\hskip 2.5ptF)(H\hskip 2.5ptW)\hskip 2.5ptC)" class="ltx_math_unparsed" display="inline" id="S3.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" stretchy="false">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1.2"><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.1" stretchy="false">(</mo><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2"><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.1" stretchy="false">(</mo><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.2">B</mi><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.3">V</mi><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.4">F</mi><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.2.5" stretchy="false">)</mo></mrow><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3"><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3.1" stretchy="false">(</mo><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3.2">H</mi><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3.3">W</mi><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.3.4" rspace="0.250em" stretchy="false">)</mo></mrow><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.4">C</mi><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">(((B\hskip 2.5ptV\hskip 2.5ptF)(H\hskip 2.5ptW)\hskip 2.5ptC)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.3.m3.1d">( ( ( italic_B italic_V italic_F ) ( italic_H italic_W ) italic_C )</annotation></semantics></math>. A visualization is provided in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S3.F1" title="Figure 1 â€£ 3.1 Overview â€£ 3 Method â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a>(b).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p2.1">Since only the second-to-last dimension, representing token length, is extended while other dimensions remain unchanged, our inflated spatial attention can inherit the model weights from the monocular setting. This flexibility allows our model to leverage training data with varying numbers of views and facilitates extrapolation to additional views at inference.
To handle multi-view generation, we introduce an additional view dimension to the input data.
To maintain workflow simplicity, we absorb the view dimension into the batch dimension during processing of other blocks, ensuring flexibility in handling different numbers of views.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Joint Training Strategy on Curated Data Mixtures</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Thanks to the view-integrated attention mechanism, which allows for inheriting the module weights, our framework can leverage various data sources, including static, multi-view dynamic, and monocular videos. This is hard to achieve in previous methods. In this section, we first illustrate our joint training strategy, followed by details on the curated data mixtures.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Joint Training on Data Mixtures</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.4">For videos capturing static scenesÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib76" title="">2023</a>; Xia etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib69" title="">2024</a>; Reizenstein etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>; Deitke etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib9" title="">a</a>)</cite>, we consider all frames to be temporally synchronized.
An arbitrary subsequence of length <math alttext="(F-1)\times V+1" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.SS1.p1.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p1.1.m1.1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml">F</mi><mo id="S4.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="S4.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.SS1.p1.1.m1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.SS1.p1.1.m1.1.1.1.2" rspace="0.222em" xref="S4.SS1.p1.1.m1.1.1.1.2.cmml">Ã—</mo><mi id="S4.SS1.p1.1.m1.1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.1.3.cmml">V</mi></mrow><mo id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">+</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><plus id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"></plus><apply id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.1.2"></times><apply id="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1"><minus id="S4.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.1"></minus><ci id="S4.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.2">ğ¹</ci><cn id="S4.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.3">1</cn></apply><ci id="S4.SS1.p1.1.m1.1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.1.3">ğ‘‰</ci></apply><cn id="S4.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">(F-1)\times V+1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">( italic_F - 1 ) Ã— italic_V + 1</annotation></semantics></math> from the original video can be reformatted into a <math alttext="V" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_V</annotation></semantics></math>-view sequence with a shared starting frame and <math alttext="F" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">F</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_F</annotation></semantics></math> total frames per view. Static scenes also allow frame order reversal, providing additional augmentation opportunities.
We further prepare multi-view dynamic videos by rendering animatable objects from ObjaverseÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>; Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib23" title="">2024</a>)</cite>. We design random smooth trajectories with diverse elevation and azimuth changes to avoid overfitting on simple camera movements. These trajectories start from a shared random forward-facing starting point and result in <math alttext="n\times v" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">n</mi><mo id="S4.SS1.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.4.m4.1.1.1.cmml">Ã—</mo><mi id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><times id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></times><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">ğ‘›</ci><ci id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">n\times v</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_n Ã— italic_v</annotation></semantics></math> frames in total.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To avoid the model overfitting on synthetic images with simple backgrounds, we include a portion of data from monocular in-the-wild videosÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>; Nan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>. Training multi-view camera control from monocular videos is extremely challenging. Although CamCoÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> and 4DiMÂ <cite class="ltx_cite ltx_citemacro_citep">(Watson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib68" title="">2024</a>)</cite> have explored joint training for monocular video generation, these approaches are unsuitable for multi-view scenarios. The concurrent work CVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite> explored homography warping to augment the monocular videos into pseudo-multi-view videos, but the limited realism of these augmentations restricts their ability to generate complex camera and object motion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.2">To overcome these issues, we choose to jointly train our model on monocular and multi-view videos to effectively utilize the abundant object motion information from all data sources. We annotate the monocular videos with camera poses using Particle-SfMÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib81" title="">2022</a>)</cite>.
Since in-the-wild monocular videos often contain noisy or unnatural content, we apply a rigorous filtering pipeline to remove unsuitable clips.
These curated video clips, sourced from InternVidÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>)</cite> and OpenVidÂ <cite class="ltx_cite ltx_citemacro_citep">(Nan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite> datasets, provide rich object motion as well as complex backgrounds that mitigate the gap between scene-level static data and object-level dynamic data.
We rearrange monocular videos as <math alttext="V=1" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">V</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">ğ‘‰</ci><cn id="S4.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">V=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_V = 1</annotation></semantics></math> samples so that all data items can be processed uniformly without bells and whistles. Thanks to our view-integrated attention modules, which accommodate varying token lengths, the varying view numbers <math alttext="V" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_V</annotation></semantics></math> do not affect the training process.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S4.F2.g1" src="x2.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Statistics of the (a) point cloud size, (b) aesthetic score, and (c) camera motion classification result for our monocular video dataset.</figcaption>
</figure>
<figure class="ltx_figure ltx_align_floatright" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="154" id="S4.F3.g1" src="extracted/5924086/figures/data.png" width="220"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Sources of our training videos.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Curation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We begin by training our model extensively on static video data sourced from various publicly available datasets. Wild-RGBDÂ <cite class="ltx_cite ltx_citemacro_citep">(Xia etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib69" title="">2024</a>)</cite> includes nearly 20,000 RGB-D videos across 46 common object categories.
MVImgNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib76" title="">2023</a>)</cite> comprises 219,188 videos featuring objects from 238 classes. DL3DV-10KÂ <cite class="ltx_cite ltx_citemacro_citep">(Ling etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib35" title="">2023b</a>)</cite> provides 7,000 long-duration videos captured in both indoor and outdoor environments. CO3Dv2Â <cite class="ltx_cite ltx_citemacro_citep">(Reizenstein etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>)</cite> contains 34,000 turntable-like videos of rigid objects, crowd-sourced by nonexperts using cellphone cameras. ObjaverseÂ <cite class="ltx_cite ltx_citemacro_citep">(Deitke etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>)</cite> and Objaverse-XLÂ <cite class="ltx_cite ltx_citemacro_citep">(Deitke etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib9" title="">2023a</a>)</cite> exhaustively crawl 10 million publicly available 3D assets. From these, we filtered out low-quality assets, such as those with incorrect textures or overly simplistic geometry, yielding a high-quality subset of 400,000 assets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Similar to Diffusion4DÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>)</cite> and Animate3DÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib23" title="">2024</a>)</cite>, we filter the animatable objects from Objaverseâ€™s Sketchfab subset. We exclude objects with excessive motion, which might result in partial observations, as well as nearly static objects with minimal motion. This curation process helps us obtain 19,000 high-quality dynamic assets that can be rendered from arbitrary viewpoints and timesteps, facilitating multi-view video generation. During each training iteration, we augment the frames with randomly selected background colors.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">To improve the modelâ€™s ability to generate object motion in the presence of complex backgrounds, we prepare monocular videos with camera pose annotations similar to CamCoÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite>. First, we use Particle-SfMÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib81" title="">2022</a>)</cite> to estimate the camera poses for randomly sampled frames from videos from InternVidÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>)</cite> and OpenVidÂ <cite class="ltx_cite ltx_citemacro_citep">(Nan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>. Inspired by CO3DÂ <cite class="ltx_cite ltx_citemacro_citep">(Reizenstein etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>)</cite> and CamCoÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite>, we remove the videos where SfM fails to register all available frames or produces a point cloud with too few points or too many points.
Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F2" title="Figure 2 â€£ 4.1 Joint Training on Data Mixtures â€£ 4 Joint Training Strategy on Curated Data Mixtures â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">2</span></a>(a) shows the point count statistics. A point cloud with too few points indicates poor frame registration to a shared 3D representation, while too many points suggest a mostly static scene, which is undesirable as we focus on object motion. Additionally, non-registered frames may indicate potential scene changes.
We then apply a rigorous filtering pipeline to ensure the quality of the video samples used for training.
This includes filtering based on aesthetic scores, optical character recognition (OCR), and camera motion classification using optical flow. Videos containing detected character regions are aggressively removed. Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F2" title="Figure 2 â€£ 4.1 Joint Training on Data Mixtures â€£ 4 Joint Training Strategy on Curated Data Mixtures â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">2</span></a>(b) and (c) present statistics on aesthetic score and camera motion classification results. Videos with low aesthetic scores or those classified as having static camera motion are excluded from the training set. Ultimately, we construct a dataset of 393,000 monocular videos annotated with camera poses. We provide a summary of the data sources used in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F3" title="Figure 3 â€£ 4.1 Joint Training on Data Mixtures â€£ 4 Joint Training Strategy on Curated Data Mixtures â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">3</span></a>. More details and analysis are provided in the appendix.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1051" id="S4.F4.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Per-video qualitative comparisons. The first frame in each reference set is the input image. Neither the image nor the camera trajectories were seen during model training. Video results are provided in supplementary for clearer qualitative comparisons.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present experimental results and analysis. Video comparisons are included in the supplementary material for optimal visual evaluation.
It is important to note that for all qualitative and quantitative evaluations, neither the input images nor the camera trajectories were used during model training.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Quantitative Comparisons</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">3D Consistency of Frames</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.3">We evaluate the 3D consistency of the generated videos using COLMAPÂ <cite class="ltx_cite ltx_citemacro_citep">(SchÃ¶nberger &amp; Frahm, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib50" title="">2016</a>; SchÃ¶nberger etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib51" title="">2016</a>)</cite>. COLMAP is widely adopted for 3D reconstruction methods where camera pose estimation is required for in-the-wild images. We configure the COLMAP following previous methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Deng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib11" title="">2022</a>; Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> for best few-view performance.
A higher COLMAP error rate indicates poorer 3D consistency in the input images. Motivated by this, we report COLMAP errors as a measure of the 3D consistency of the frames. Each video is retried up to five times to reduce randomness.
We randomly sample 1,000 video sequences from RealEstate10KÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite> test set for evaluation.
Since we have ground truth 3D scenes, we use the ground truth camera pose sequences as the viewpoint instruction of the video model and compare the generated frames against the ground truth images.
Similar to prior worksÂ <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>; Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite>, we extract the estimated camera poses and calculate the relative translation and rotation differences.
Specifically, given two camera pose sequences, we convert them to relative poses and align the first frames to world origin.
We then measure the angular errors in translation and rotation. Unlike previous worksÂ <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib14" title="">2024</a>; Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> that calculate the Euclidean distance of translation vectors, we use angular error measurements to ensure the camera pose scales are normalized, addressing scale ambiguity. As shown in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.T1" title="Table 1 â€£ 3D Consistency of Frames â€£ 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a>, we calculate the area under the cumulative error curve (AUC) of frames whose rotation and translations are below certain thresholds (<math alttext="5^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><msup id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">5</mn><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2">5</cn><compose id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.1.m1.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="10^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px1.p1.2.m2.1a"><msup id="S5.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">10</mn><mo id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2">10</cn><compose id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.2.m2.1c">10^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.2.m2.1d">10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="20^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S5.SS1.SSS0.Px1.p1.3.m3.1a"><msup id="S5.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml">20</mn><mo id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.2">20</cn><compose id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.3.m3.1c">20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.3.m3.1d">20 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>). Our method significantly outperforms existing baselines.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1064" id="S5.F5.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative comparisons for 2-view video generations. Each generation consists of two rows, where each row represents a sequence of generated frames, with columns showing frames at the same timestep. Neither the image nor the camera trajectories were used during model training. <span class="ltx_text" id="S5.F5.2.1" style="color:#FF0000;">Red</span> dotted lines are annotated to highlight object motion. Video results are included in the supplementary material for clearer comparisons.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Quantitative comparison for monocular geometry consistency on RealEstate10K test set.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.7" style="width:318.0pt;height:85.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.9pt,11.7pt) scale(0.783705904146909,0.783705904146909) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.7.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.5.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.5.5.5.6" rowspan="2"><span class="ltx_text" id="S5.T1.5.5.5.6.1">Methods</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T1.1.1.1.1.1">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.2.2.2.2" rowspan="2"><span class="ltx_text" id="S5.T1.2.2.2.2.1">FVD <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.2.2.2.2.1.m1.1"><semantics id="S5.T1.2.2.2.2.1.m1.1a"><mo id="S5.T1.2.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T1.2.2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.1.m1.1b"><ci id="S5.T1.2.2.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.2.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.3.3.3.3" rowspan="2"><span class="ltx_text" id="S5.T1.3.3.3.3.1">COLMAP error<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.3.3.3.3.1.m1.1"><semantics id="S5.T1.3.3.3.3.1.m1.1a"><mo id="S5.T1.3.3.3.3.1.m1.1.1" stretchy="false" xref="S5.T1.3.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.1.m1.1b"><ci id="S5.T1.3.3.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.3.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.4.4.4.4">Rot. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.4.4.4.4.m1.1"><semantics id="S5.T1.4.4.4.4.m1.1a"><mo id="S5.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S5.T1.4.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.m1.1b"><ci id="S5.T1.4.4.4.4.m1.1.1.cmml" xref="S5.T1.4.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.4.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.5.5.5.5">Trans. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.5.5.5.5.m1.1"><semantics id="S5.T1.5.5.5.5.m1.1a"><mo id="S5.T1.5.5.5.5.m1.1.1" stretchy="false" xref="S5.T1.5.5.5.5.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.5.m1.1b"><ci id="S5.T1.5.5.5.5.m1.1.1.cmml" xref="S5.T1.5.5.5.5.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.5.5.m1.1d">â†‘</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.6.6.6.1">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="S5.T1.6.6.6.1.m1.1"><semantics id="S5.T1.6.6.6.1.m1.1a"><mrow id="S5.T1.6.6.6.1.m1.1.1" xref="S5.T1.6.6.6.1.m1.1.1.cmml"><msup id="S5.T1.6.6.6.1.m1.1.1.2" xref="S5.T1.6.6.6.1.m1.1.1.2.cmml"><mn id="S5.T1.6.6.6.1.m1.1.1.2.2" xref="S5.T1.6.6.6.1.m1.1.1.2.2.cmml">5</mn><mo id="S5.T1.6.6.6.1.m1.1.1.2.3" xref="S5.T1.6.6.6.1.m1.1.1.2.3.cmml">âˆ˜</mo></msup><mo id="S5.T1.6.6.6.1.m1.1.1.1" xref="S5.T1.6.6.6.1.m1.1.1.1.cmml">/</mo><msup id="S5.T1.6.6.6.1.m1.1.1.3" xref="S5.T1.6.6.6.1.m1.1.1.3.cmml"><mn id="S5.T1.6.6.6.1.m1.1.1.3.2" xref="S5.T1.6.6.6.1.m1.1.1.3.2.cmml">10</mn><mo id="S5.T1.6.6.6.1.m1.1.1.3.3" xref="S5.T1.6.6.6.1.m1.1.1.3.3.cmml">âˆ˜</mo></msup><mo id="S5.T1.6.6.6.1.m1.1.1.1a" xref="S5.T1.6.6.6.1.m1.1.1.1.cmml">/</mo><msup id="S5.T1.6.6.6.1.m1.1.1.4" xref="S5.T1.6.6.6.1.m1.1.1.4.cmml"><mn id="S5.T1.6.6.6.1.m1.1.1.4.2" xref="S5.T1.6.6.6.1.m1.1.1.4.2.cmml">20</mn><mo id="S5.T1.6.6.6.1.m1.1.1.4.3" xref="S5.T1.6.6.6.1.m1.1.1.4.3.cmml">âˆ˜</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.6.1.m1.1b"><apply id="S5.T1.6.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1"><divide id="S5.T1.6.6.6.1.m1.1.1.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1.1"></divide><apply id="S5.T1.6.6.6.1.m1.1.1.2.cmml" xref="S5.T1.6.6.6.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.6.6.6.1.m1.1.1.2.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1.2">superscript</csymbol><cn id="S5.T1.6.6.6.1.m1.1.1.2.2.cmml" type="integer" xref="S5.T1.6.6.6.1.m1.1.1.2.2">5</cn><compose id="S5.T1.6.6.6.1.m1.1.1.2.3.cmml" xref="S5.T1.6.6.6.1.m1.1.1.2.3"></compose></apply><apply id="S5.T1.6.6.6.1.m1.1.1.3.cmml" xref="S5.T1.6.6.6.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T1.6.6.6.1.m1.1.1.3.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1.3">superscript</csymbol><cn id="S5.T1.6.6.6.1.m1.1.1.3.2.cmml" type="integer" xref="S5.T1.6.6.6.1.m1.1.1.3.2">10</cn><compose id="S5.T1.6.6.6.1.m1.1.1.3.3.cmml" xref="S5.T1.6.6.6.1.m1.1.1.3.3"></compose></apply><apply id="S5.T1.6.6.6.1.m1.1.1.4.cmml" xref="S5.T1.6.6.6.1.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T1.6.6.6.1.m1.1.1.4.1.cmml" xref="S5.T1.6.6.6.1.m1.1.1.4">superscript</csymbol><cn id="S5.T1.6.6.6.1.m1.1.1.4.2.cmml" type="integer" xref="S5.T1.6.6.6.1.m1.1.1.4.2">20</cn><compose id="S5.T1.6.6.6.1.m1.1.1.4.3.cmml" xref="S5.T1.6.6.6.1.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.6.1.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.6.1.m1.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.7.2">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="S5.T1.7.7.7.2.m1.1"><semantics id="S5.T1.7.7.7.2.m1.1a"><mrow id="S5.T1.7.7.7.2.m1.1.1" xref="S5.T1.7.7.7.2.m1.1.1.cmml"><msup id="S5.T1.7.7.7.2.m1.1.1.2" xref="S5.T1.7.7.7.2.m1.1.1.2.cmml"><mn id="S5.T1.7.7.7.2.m1.1.1.2.2" xref="S5.T1.7.7.7.2.m1.1.1.2.2.cmml">5</mn><mo id="S5.T1.7.7.7.2.m1.1.1.2.3" xref="S5.T1.7.7.7.2.m1.1.1.2.3.cmml">âˆ˜</mo></msup><mo id="S5.T1.7.7.7.2.m1.1.1.1" xref="S5.T1.7.7.7.2.m1.1.1.1.cmml">/</mo><msup id="S5.T1.7.7.7.2.m1.1.1.3" xref="S5.T1.7.7.7.2.m1.1.1.3.cmml"><mn id="S5.T1.7.7.7.2.m1.1.1.3.2" xref="S5.T1.7.7.7.2.m1.1.1.3.2.cmml">10</mn><mo id="S5.T1.7.7.7.2.m1.1.1.3.3" xref="S5.T1.7.7.7.2.m1.1.1.3.3.cmml">âˆ˜</mo></msup><mo id="S5.T1.7.7.7.2.m1.1.1.1a" xref="S5.T1.7.7.7.2.m1.1.1.1.cmml">/</mo><msup id="S5.T1.7.7.7.2.m1.1.1.4" xref="S5.T1.7.7.7.2.m1.1.1.4.cmml"><mn id="S5.T1.7.7.7.2.m1.1.1.4.2" xref="S5.T1.7.7.7.2.m1.1.1.4.2.cmml">20</mn><mo id="S5.T1.7.7.7.2.m1.1.1.4.3" xref="S5.T1.7.7.7.2.m1.1.1.4.3.cmml">âˆ˜</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.7.2.m1.1b"><apply id="S5.T1.7.7.7.2.m1.1.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1"><divide id="S5.T1.7.7.7.2.m1.1.1.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1.1"></divide><apply id="S5.T1.7.7.7.2.m1.1.1.2.cmml" xref="S5.T1.7.7.7.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.7.7.7.2.m1.1.1.2.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1.2">superscript</csymbol><cn id="S5.T1.7.7.7.2.m1.1.1.2.2.cmml" type="integer" xref="S5.T1.7.7.7.2.m1.1.1.2.2">5</cn><compose id="S5.T1.7.7.7.2.m1.1.1.2.3.cmml" xref="S5.T1.7.7.7.2.m1.1.1.2.3"></compose></apply><apply id="S5.T1.7.7.7.2.m1.1.1.3.cmml" xref="S5.T1.7.7.7.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T1.7.7.7.2.m1.1.1.3.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1.3">superscript</csymbol><cn id="S5.T1.7.7.7.2.m1.1.1.3.2.cmml" type="integer" xref="S5.T1.7.7.7.2.m1.1.1.3.2">10</cn><compose id="S5.T1.7.7.7.2.m1.1.1.3.3.cmml" xref="S5.T1.7.7.7.2.m1.1.1.3.3"></compose></apply><apply id="S5.T1.7.7.7.2.m1.1.1.4.cmml" xref="S5.T1.7.7.7.2.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T1.7.7.7.2.m1.1.1.4.1.cmml" xref="S5.T1.7.7.7.2.m1.1.1.4">superscript</csymbol><cn id="S5.T1.7.7.7.2.m1.1.1.4.2.cmml" type="integer" xref="S5.T1.7.7.7.2.m1.1.1.4.2">20</cn><compose id="S5.T1.7.7.7.2.m1.1.1.4.3.cmml" xref="S5.T1.7.7.7.2.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.7.2.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.7.2.m1.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.7.7.8.1.1">SVD</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.7.7.8.1.2">16.89</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.8.1.3">139.64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.8.1.4">30.3%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.8.1.5">14.4 / 22.8 / 35.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.7.7.8.1.6">0.2 / 1.0 / 3.2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.7.7.9.2.1">MotionCtrl</th>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.9.2.2">21.09</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.9.2.3">119.06</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.9.2.4">55.0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.9.2.5">8.6 / 13.9 / 22.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.9.2.6">0.6 / 2.1 / 5.7</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.7.7.10.3.1">CameraCtrl</th>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.10.3.2">14.69</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.10.3.3">105.41</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.10.3.4">19.3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.7.7.10.3.5">21.4 / 32.9 / 48.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.10.3.6">0.3 / 1.3 / 4.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T1.7.7.11.4.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.7.7.11.4.2"><span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.2.1">11.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.7.7.11.4.3"><span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.3.1">55.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.7.7.11.4.4"><span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.4.1">14.4%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.7.7.11.4.5">
<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.5.1">22.9 </span>/ <span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.5.2">34.5</span> /<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.5.3"> 50.1</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.7.7.11.4.6">
<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.6.1">5.1</span> /<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.6.2"> 12.7</span> /<span class="ltx_text ltx_font_bold" id="S5.T1.7.7.11.4.6.3"> 24.6</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Quantitative comparison for 2-view video generation.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.8" style="width:368.6pt;height:133.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-40.4pt,14.6pt) scale(0.820127696935088,0.820127696935088) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.6.7" rowspan="2"><span class="ltx_text" id="S5.T2.6.6.6.7.1">Scenes</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.6.8" rowspan="2"><span class="ltx_text" id="S5.T2.6.6.6.8.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T2.1.1.1.1.1">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.2.2.2.2" rowspan="2"><span class="ltx_text" id="S5.T2.2.2.2.2.1">FVD <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.2.1.m1.1a"><mo id="S5.T2.2.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T2.2.2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.3.3.3.3">Rot. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.3.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T2.3.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.3.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.4.4.4.4">Trans. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.4.4.4.4.m1.1"><semantics id="S5.T2.4.4.4.4.m1.1a"><mo id="S5.T2.4.4.4.4.m1.1.1" stretchy="false" xref="S5.T2.4.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.4.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.5.5.5" rowspan="2"><span class="ltx_text" id="S5.T2.5.5.5.5.1">Prec. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.5.5.5.5.1.m1.1"><semantics id="S5.T2.5.5.5.5.1.m1.1a"><mo id="S5.T2.5.5.5.5.1.m1.1.1" stretchy="false" xref="S5.T2.5.5.5.5.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.5.1.m1.1b"><ci id="S5.T2.5.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.5.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.5.5.1.m1.1d">â†‘</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.6.6" rowspan="2"><span class="ltx_text" id="S5.T2.6.6.6.6.1">MS. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.6.6.6.6.1.m1.1"><semantics id="S5.T2.6.6.6.6.1.m1.1a"><mo id="S5.T2.6.6.6.6.1.m1.1.1" stretchy="false" xref="S5.T2.6.6.6.6.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.6.1.m1.1b"><ci id="S5.T2.6.6.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.6.6.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.6.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.6.6.1.m1.1d">â†‘</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.7.7.7.1">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="S5.T2.7.7.7.1.m1.1"><semantics id="S5.T2.7.7.7.1.m1.1a"><mrow id="S5.T2.7.7.7.1.m1.1.1" xref="S5.T2.7.7.7.1.m1.1.1.cmml"><msup id="S5.T2.7.7.7.1.m1.1.1.2" xref="S5.T2.7.7.7.1.m1.1.1.2.cmml"><mn id="S5.T2.7.7.7.1.m1.1.1.2.2" xref="S5.T2.7.7.7.1.m1.1.1.2.2.cmml">5</mn><mo id="S5.T2.7.7.7.1.m1.1.1.2.3" xref="S5.T2.7.7.7.1.m1.1.1.2.3.cmml">âˆ˜</mo></msup><mo id="S5.T2.7.7.7.1.m1.1.1.1" xref="S5.T2.7.7.7.1.m1.1.1.1.cmml">/</mo><msup id="S5.T2.7.7.7.1.m1.1.1.3" xref="S5.T2.7.7.7.1.m1.1.1.3.cmml"><mn id="S5.T2.7.7.7.1.m1.1.1.3.2" xref="S5.T2.7.7.7.1.m1.1.1.3.2.cmml">10</mn><mo id="S5.T2.7.7.7.1.m1.1.1.3.3" xref="S5.T2.7.7.7.1.m1.1.1.3.3.cmml">âˆ˜</mo></msup><mo id="S5.T2.7.7.7.1.m1.1.1.1a" xref="S5.T2.7.7.7.1.m1.1.1.1.cmml">/</mo><msup id="S5.T2.7.7.7.1.m1.1.1.4" xref="S5.T2.7.7.7.1.m1.1.1.4.cmml"><mn id="S5.T2.7.7.7.1.m1.1.1.4.2" xref="S5.T2.7.7.7.1.m1.1.1.4.2.cmml">20</mn><mo id="S5.T2.7.7.7.1.m1.1.1.4.3" xref="S5.T2.7.7.7.1.m1.1.1.4.3.cmml">âˆ˜</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.1.m1.1b"><apply id="S5.T2.7.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1"><divide id="S5.T2.7.7.7.1.m1.1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.1"></divide><apply id="S5.T2.7.7.7.1.m1.1.1.2.cmml" xref="S5.T2.7.7.7.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T2.7.7.7.1.m1.1.1.2.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.2">superscript</csymbol><cn id="S5.T2.7.7.7.1.m1.1.1.2.2.cmml" type="integer" xref="S5.T2.7.7.7.1.m1.1.1.2.2">5</cn><compose id="S5.T2.7.7.7.1.m1.1.1.2.3.cmml" xref="S5.T2.7.7.7.1.m1.1.1.2.3"></compose></apply><apply id="S5.T2.7.7.7.1.m1.1.1.3.cmml" xref="S5.T2.7.7.7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T2.7.7.7.1.m1.1.1.3.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.3">superscript</csymbol><cn id="S5.T2.7.7.7.1.m1.1.1.3.2.cmml" type="integer" xref="S5.T2.7.7.7.1.m1.1.1.3.2">10</cn><compose id="S5.T2.7.7.7.1.m1.1.1.3.3.cmml" xref="S5.T2.7.7.7.1.m1.1.1.3.3"></compose></apply><apply id="S5.T2.7.7.7.1.m1.1.1.4.cmml" xref="S5.T2.7.7.7.1.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.7.7.7.1.m1.1.1.4.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.4">superscript</csymbol><cn id="S5.T2.7.7.7.1.m1.1.1.4.2.cmml" type="integer" xref="S5.T2.7.7.7.1.m1.1.1.4.2">20</cn><compose id="S5.T2.7.7.7.1.m1.1.1.4.3.cmml" xref="S5.T2.7.7.7.1.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.1.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.7.7.7.1.m1.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.8.2">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="S5.T2.8.8.8.2.m1.1"><semantics id="S5.T2.8.8.8.2.m1.1a"><mrow id="S5.T2.8.8.8.2.m1.1.1" xref="S5.T2.8.8.8.2.m1.1.1.cmml"><msup id="S5.T2.8.8.8.2.m1.1.1.2" xref="S5.T2.8.8.8.2.m1.1.1.2.cmml"><mn id="S5.T2.8.8.8.2.m1.1.1.2.2" xref="S5.T2.8.8.8.2.m1.1.1.2.2.cmml">5</mn><mo id="S5.T2.8.8.8.2.m1.1.1.2.3" xref="S5.T2.8.8.8.2.m1.1.1.2.3.cmml">âˆ˜</mo></msup><mo id="S5.T2.8.8.8.2.m1.1.1.1" xref="S5.T2.8.8.8.2.m1.1.1.1.cmml">/</mo><msup id="S5.T2.8.8.8.2.m1.1.1.3" xref="S5.T2.8.8.8.2.m1.1.1.3.cmml"><mn id="S5.T2.8.8.8.2.m1.1.1.3.2" xref="S5.T2.8.8.8.2.m1.1.1.3.2.cmml">10</mn><mo id="S5.T2.8.8.8.2.m1.1.1.3.3" xref="S5.T2.8.8.8.2.m1.1.1.3.3.cmml">âˆ˜</mo></msup><mo id="S5.T2.8.8.8.2.m1.1.1.1a" xref="S5.T2.8.8.8.2.m1.1.1.1.cmml">/</mo><msup id="S5.T2.8.8.8.2.m1.1.1.4" xref="S5.T2.8.8.8.2.m1.1.1.4.cmml"><mn id="S5.T2.8.8.8.2.m1.1.1.4.2" xref="S5.T2.8.8.8.2.m1.1.1.4.2.cmml">20</mn><mo id="S5.T2.8.8.8.2.m1.1.1.4.3" xref="S5.T2.8.8.8.2.m1.1.1.4.3.cmml">âˆ˜</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.8.2.m1.1b"><apply id="S5.T2.8.8.8.2.m1.1.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1"><divide id="S5.T2.8.8.8.2.m1.1.1.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.1"></divide><apply id="S5.T2.8.8.8.2.m1.1.1.2.cmml" xref="S5.T2.8.8.8.2.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T2.8.8.8.2.m1.1.1.2.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.2">superscript</csymbol><cn id="S5.T2.8.8.8.2.m1.1.1.2.2.cmml" type="integer" xref="S5.T2.8.8.8.2.m1.1.1.2.2">5</cn><compose id="S5.T2.8.8.8.2.m1.1.1.2.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.2.3"></compose></apply><apply id="S5.T2.8.8.8.2.m1.1.1.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T2.8.8.8.2.m1.1.1.3.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.3">superscript</csymbol><cn id="S5.T2.8.8.8.2.m1.1.1.3.2.cmml" type="integer" xref="S5.T2.8.8.8.2.m1.1.1.3.2">10</cn><compose id="S5.T2.8.8.8.2.m1.1.1.3.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.3.3"></compose></apply><apply id="S5.T2.8.8.8.2.m1.1.1.4.cmml" xref="S5.T2.8.8.8.2.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.8.8.8.2.m1.1.1.4.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.4">superscript</csymbol><cn id="S5.T2.8.8.8.2.m1.1.1.4.2.cmml" type="integer" xref="S5.T2.8.8.8.2.m1.1.1.4.2">20</cn><compose id="S5.T2.8.8.8.2.m1.1.1.4.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.8.2.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.8.8.8.2.m1.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.9.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.1" rowspan="4"><span class="ltx_text" id="S5.T2.8.8.9.1.1.1">Real10K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.2">SVD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.9.1.3">37.99</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.4">296.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.5">7.9 / 13.5 / 28.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.9.1.6">0.2 / 0.7 / 2.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.9.1.7">6.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.9.1.8">4.17</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.10.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.10.2.1">MotionCtrl</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.10.2.2">29.23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.10.2.3">277.05</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.10.2.4">8.1 / 16.5 / 29.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.10.2.5">1.5 / 5.3 / 16.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.10.2.6">11.45</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.10.2.7">5.90</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.11.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.11.3.1">CameraCtrl</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.11.3.2">12.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.11.3.3">131.32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.11.3.4">22.4 / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.11.3.4.1">38.5</span> / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.11.3.4.2">56.2</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.11.3.5">0.6 / 2.5 / 8.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.11.3.6">19.49</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.11.3.7">11.25</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.12.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.12.4.1">Ours</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.12.4.2"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.2.1">8.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.12.4.3"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.3.1">94.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.12.4.4">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.4.1">23.9</span> / 37.4 / 52.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.12.4.5">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.5.1">3.3</span> / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.5.2">10.2</span> / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.5.3">23.5</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.12.4.6"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.6.1">29.39</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.12.4.7"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.12.4.7.1">15.22</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.13.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.1" rowspan="3"><span class="ltx_text" id="S5.T2.8.8.13.5.1.1">General</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.2">MotionCtrl</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.13.5.3">47.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.4">313.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.5">4.9 / 11.3 / 21.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.8.8.13.5.6">0.7 / 2.4 / 8.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.13.5.7">8.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.8.8.13.5.8">3.93</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.14.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.14.6.1">CameraCtrl</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.14.6.2">26.71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.14.6.3">221.23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.14.6.4">14.1 / 26.9 / 43.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.8.8.14.6.5">0.5 / 1.7 / 5.7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.14.6.6">15.13</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.14.6.7">7.35</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8.15.7">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.8.8.15.7.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.8.8.15.7.2"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.2.1">26.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.8.8.15.7.3"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.3.1">173.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.8.8.15.7.4">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.4.1">19.7</span> / <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.4.2">32.7 </span>/ <span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.4.3">48.4</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.8.8.15.7.5">
<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.5.1">0.8 </span>/<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.5.2"> 2.8</span> /<span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.5.3"> 8.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.8.8.15.7.6"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.6.1">33.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.8.8.15.7.7"><span class="ltx_text ltx_font_bold" id="S5.T2.8.8.15.7.7.1">19.96</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Multi-view Consistency</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.3">Alongside evaluating the individual monocular frame pose accuracy using COLMAP-based metrics, we further assess the cross-video consistency of the corresponding frames from generated multi-view videos.
We randomly sample 1,000 videos, each with 27 frames, from RealEstate10kÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite> test set and convert each video into a two-view sequence with 14 frames per view.
The new camera pose sequences are generated by setting the 14th frame as the world origin and positioning the remaining frames relative to it. The scales of the scenes are normalized so that the maximum distance from the origin is 1.
Following CVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite>, we adopt SuperGlueÂ <cite class="ltx_cite ltx_citemacro_citep">(Sarlin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib49" title="">2020</a>)</cite> to find correspondences and estimate the camera poses between each time-aligned set of frames.
SuperGlue not only measures angular errors in the rotation and translation but also computes the epipolar error of the matched correspondences.
We similarly collect the AUC for frame pairs with rotation and translation errors below specific thresholds (<math alttext="5^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px2.p1.1.m1.1a"><msup id="S5.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">5</mn><mo id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.2">5</cn><compose id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.1.m1.1c">5^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.1.m1.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="10^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px2.p1.2.m2.1a"><msup id="S5.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">10</mn><mo id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.2">10</cn><compose id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.2.m2.1c">10^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.2.m2.1d">10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="20^{\circ}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S5.SS1.SSS0.Px2.p1.3.m3.1a"><msup id="S5.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mn id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">20</mn><mo id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1">superscript</csymbol><cn id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" type="integer" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.2">20</cn><compose id="S5.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S5.SS1.SSS0.Px2.p1.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.3.m3.1c">20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.3.m3.1d">20 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>). The epipolar errors for the estimated correspondences are summarized to the precision (P) and matching score (MS). As shown in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.T2" title="Table 2 â€£ 3D Consistency of Frames â€£ 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">2</span></a>, our method outperforms baselines greatly. The â€œReal10Kâ€ category means that the input images are taken from the corresponding RealEstate10K test sequence, while the â€œGeneralâ€ means that the input images are taken from 1,000 randomly sampled images in the test split of our monocular video dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Visual Quality</h4>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">To assess the frame perceptual quality, we evaluate visual quality using FIDÂ <cite class="ltx_cite ltx_citemacro_citep">(Heusel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib15" title="">2017</a>)</cite> and FVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Unterthiner etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib62" title="">2018</a>)</cite>. FID and FVD measure the feature-space similarity of two sets of images and videos, respectively. In our case, they quantify the distribution distance between the generated frame sequences and the ground-truth frames. We provide monocular evaluations in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.T1" title="Table 1 â€£ 3D Consistency of Frames â€£ 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">1</span></a> and multi-view evaluations in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.T2" title="Table 2 â€£ 3D Consistency of Frames â€£ 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">2</span></a>.
As shown in these tables, our proposed framework enjoys the best visual quality. For both the â€œReal10Kâ€ and â€œGeneralâ€ categories, the ground-truth videos used to calculate these metrics are the video sequences corresponding to the input frames. These video sequences are from the test set split of the datasets and are not seen during training.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Qualtitative Comparison</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We provide qualitative comparisons on RealEstate10kÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite> scenes in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F4" title="Figure 4 â€£ 4.2 Data Curation â€£ 4 Joint Training Strategy on Curated Data Mixtures â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">4</span></a> and text-to-image generated images in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.F5" title="Figure 5 â€£ 3D Consistency of Frames â€£ 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">5</span></a>. As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.F4" title="Figure 4 â€£ 4.2 Data Curation â€£ 4 Joint Training Strategy on Curated Data Mixtures â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">4</span></a>, our method produces videos with precise camera control, whereas MotionCtrl tends to generate overly smooth trajectories that simplify the viewpoint instructions, and CameraCtrl suffers from severe distortions at novel viewpoints. For example, in the first case, the camera instruction involves multiple panning operations, first panning left and then panning right. Still, MotionCtrl only pans left, ignoring the rest of the instructions. CameraCtrlâ€™s outputs, particularly in the first two cases, exhibit noticeable distortion, with the walls bending in the later frames.
Additionally, in the third and fourth cases, where the camera trajectories cover a long distance, both MotionCtrl and CameraCtrl produce unrealistic hallucinations, introducing artifacts such as merging indoor and outdoor pixels or distorting input pixels to compensate for a lack of generation ability.
In Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.F5" title="Figure 5 â€£ 3D Consistency of Frames â€£ 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">5</span></a>, we observe that MotionCtrl and CameraCtrl tend to generate static scenes without any object motion. Although their methods produce realistic novel views, the synthesized objects remain static. In contrast, our method generates vivid object motion while maintaining accurate camera control. We highlight the object motion in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S5.F5" title="Figure 5 â€£ 3D Consistency of Frames â€£ 5.1 Quantitative Comparisons â€£ 5 Experiments â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">5</span></a> using auxiliary red lines. We encourage readers to view the supplementary videos for optimal visual comparisons.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Studies and Applications</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Due to the space limit, we refer readers to the Appendix for ablation studies and applications of our framework. We provide detailed ablation studies in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A4" title="Appendix D Ablation Studies â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">D</span></a> on our proposed framework. Additionally, we explore the 3D reconstruction of our generated frames and four-view generation capabilities in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A5" title="Appendix E Applications â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">E</span></a>. Videos are included in the supplementary material for optimal qualitative comparison.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we propose Cavia, a novel framework for consistent multi-view camera-controllable video generation. Our framework incorporates cross-frame and cross-view attentions for effective camera controllability and view consistency. Our model benefits from joint training using static 3D scenes and objects, animatable objects, and in-the-wild monocular videos.
Extensive experiments demonstrate the superiority of our method over previous works in terms of geometric consistency and perceptual quality.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahmani etÂ al. (2023)</span>
<span class="ltx_bibblock">
Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, JeongÂ Joon Park, Andrea Tagliasacchi, and DavidÂ B Lindell.

</span>
<span class="ltx_bibblock">4d-fy: Text-to-4d generation using hybrid score distillation sampling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2311.17984</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahmani etÂ al. (2024)</span>
<span class="ltx_bibblock">
Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, etÂ al.

</span>
<span class="ltx_bibblock">Vd3d: Taming large video diffusion transformers for 3d camera control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2407.12781</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain etÂ al. (2021)</span>
<span class="ltx_bibblock">
Max Bain, Arsha Nagrani, GÃ¼l Varol, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Frozen in time: A joint video and image encoder for end-to-end retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.Â  1728â€“1738, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattmann etÂ al. (2023)</span>
<span class="ltx_bibblock">
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, SeungÂ Wook Kim, Sanja Fidler, and Karsten Kreis.

</span>
<span class="ltx_bibblock">Align your latents: High-resolution video synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  22563â€“22575, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charatan etÂ al. (2023)</span>
<span class="ltx_bibblock">
David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann.

</span>
<span class="ltx_bibblock">pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2312.12337</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, ByungÂ Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, etÂ al.

</span>
<span class="ltx_bibblock">Panda-70m: Captioning 70m videos with multiple cross-modality teachers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  13320â€“13331, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yabo Chen, Jiemin Fang, Yuyang Huang, Taoran Yi, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, and QiÂ Tian.

</span>
<span class="ltx_bibblock">Cascade-zero123: One image to highly consistent 3d with self-prompted nearby views.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2312.04424</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu.

</span>
<span class="ltx_bibblock">V3d: Video diffusion models are effective 3d generators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2403.06738</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deitke etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, SamirÂ Yitzhak Gadre, etÂ al.

</span>
<span class="ltx_bibblock">Objaverse-xl: A universe of 10m+ 3d objects.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2307.05663</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deitke etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Objaverse: A universe of annotated 3d objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  13142â€“13153, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan.

</span>
<span class="ltx_bibblock">Depth-supervised nerf: Fewer views and faster training for free.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  12882â€“12891, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, JonathanÂ T Barron, and Ben Poole.

</span>
<span class="ltx_bibblock">Cat3d: Create anything in 3d with multi-view diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2405.10314</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, YuÂ Qiao, Dahua Lin, and BoÂ Dai.

</span>
<span class="ltx_bibblock">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2307.04725</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2024)</span>
<span class="ltx_bibblock">
Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, BoÂ Dai, Hongsheng Li, and Ceyuan Yang.

</span>
<span class="ltx_bibblock">Cameractrl: Enabling camera control for text-to-video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2404.02101</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel etÂ al. (2017)</span>
<span class="ltx_bibblock">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash equilibrium.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, DiederikÂ P Kingma, Ben Poole, Mohammad Norouzi, DavidÂ J Fleet, etÂ al.

</span>
<span class="ltx_bibblock">Imagen video: High definition video generation with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2210.02303</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and DavidÂ J Fleet.

</span>
<span class="ltx_bibblock">Video diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</em>, 35:8633â€“8646, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou etÂ al. (2024)</span>
<span class="ltx_bibblock">
Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen.

</span>
<span class="ltx_bibblock">Training-free camera control for video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2406.10126</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2021)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, YuÂ Qiao, BoÂ Dai, etÂ al.

</span>
<span class="ltx_bibblock">Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  9784â€“9794, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia (2020)</span>
<span class="ltx_bibblock">
Yan-Bin Jia.

</span>
<span class="ltx_bibblock">PlÃ¼cker coordinates for lines in the space.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Problem Solver Techniques for Applied Computer Science, Com-S-477/577 Course Handout</em>, 3, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yanqin Jiang, LiÂ Zhang, Jin Gao, Weimin Hu, and Yao Yao.

</span>
<span class="ltx_bibblock">Consistent4d: Consistent 360 <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib22.1.m1.1"><semantics id="bib.bib22.1.m1.1a"><mo id="bib.bib22.1.m1.1.1" stretchy="false" xref="bib.bib22.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.1.m1.1b"><ci id="bib.bib22.1.m1.1.1.cmml" xref="bib.bib22.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.1.m1.1d">{</annotation></semantics></math><math alttext="\backslash" class="ltx_Math" display="inline" id="bib.bib22.2.m2.1"><semantics id="bib.bib22.2.m2.1a"><mo id="bib.bib22.2.m2.1.1" xref="bib.bib22.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.2.m2.1b"><ci id="bib.bib22.2.m2.1.1.cmml" xref="bib.bib22.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.2.m2.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.2.m2.1d">\</annotation></semantics></math>deg<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib22.3.m3.1"><semantics id="bib.bib22.3.m3.1a"><mo id="bib.bib22.3.m3.1.1" stretchy="false" xref="bib.bib22.3.m3.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.3.m3.1b"><ci id="bib.bib22.3.m3.1.1.cmml" xref="bib.bib22.3.m3.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.3.m3.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.3.m3.1d">}</annotation></semantics></math> dynamic object generation from monocular video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.4.1">arXiv preprint arXiv:2311.02848</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao.

</span>
<span class="ltx_bibblock">Animate3d: Animating any 3d model with multi-view video diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2407.11398</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kant etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, RizaÂ Alp Guler, Bernard Ghanem, Sergey Tulyakov, and Igor Gilitschenski.

</span>
<span class="ltx_bibblock">Spad: Spatially aware multi-view diffusers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  10026â€“10038, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.

</span>
<span class="ltx_bibblock">Elucidating the design space of diffusion-based generative models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Advances in Neural Information Processing Systems</em>, 35:26565â€“26577, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kerbl etÂ al. (2023)</span>
<span class="ltx_bibblock">
Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis.

</span>
<span class="ltx_bibblock">3d gaussian splatting for real-time radiance field rendering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ACM Transactions on Graphics (ToG)</em>, 42(4):1â€“14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein.

</span>
<span class="ltx_bibblock">Collaborative video diffusion: Consistent multi-video generation with camera control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2405.17414</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwak etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and KwangÂ Moo Yi.

</span>
<span class="ltx_bibblock">Vivid-1-to-3: Novel view synthesis with video diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  6775â€“6785, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem.

</span>
<span class="ltx_bibblock">Vivid-zoo: Multi-view video generation with diffusion model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2406.08659</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi.

</span>
<span class="ltx_bibblock">Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">https://arxiv.org/abs/2311.06214</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, etÂ al.

</span>
<span class="ltx_bibblock">Era3d: High-resolution multiview diffusion using efficient row-wise attention.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2405.11616</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024c)</span>
<span class="ltx_bibblock">
Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Puppet-master: Scaling interactive video generation as a motion prior for part-level dynamics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2408.04631</em>, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, KonstantinosÂ N Plataniotis, Yao Zhao, and Yunchao Wei.

</span>
<span class="ltx_bibblock">Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2405.16645</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Huan Ling, SeungÂ Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis.

</span>
<span class="ltx_bibblock">Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2312.13763</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling etÂ al. (2023b)</span>
<span class="ltx_bibblock">
LuÂ Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, etÂ al.

</span>
<span class="ltx_bibblock">Dl3dv-10k: A large-scale scene dataset for deep learning-based 3d vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2312.16256</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund VarmaÂ T, Zexiang Xu, and Hao Su.

</span>
<span class="ltx_bibblock">One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Ruoshi Liu, Rundi Wu, Basile VanÂ Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.

</span>
<span class="ltx_bibblock">Zero-1-to-3: Zero-shot one image to 3d object.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2303.11328</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.

</span>
<span class="ltx_bibblock">Syncdreamer: Generating multiview-consistent images from a single-view image.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2309.03453</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, etÂ al.

</span>
<span class="ltx_bibblock">Wonder3d: Single image to 3d using cross-domain diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  9970â€“9980, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yuanxun Lu, Jingyang Zhang, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Xun Cao, and Yao Yao.

</span>
<span class="ltx_bibblock">Direct2. 5: Diverse text-to-3d generation via multi-view 2.5 d diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  8744â€“8753, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melas-Kyriazi etÂ al. (2024)</span>
<span class="ltx_bibblock">
Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos.

</span>
<span class="ltx_bibblock">Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">International Conference on Machine Learning, 2024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai.

</span>
<span class="ltx_bibblock">Openvid-1m: A large-scale high-quality dataset for text-to-video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2407.02371</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Sora â€” openai.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/sora/" title="">https://openai.com/index/sora/</a>.

</span>
<span class="ltx_bibblock">(Accessed on 09/22/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zijie Pan, Zeyu Yang, Xiatian Zhu, and LiÂ Zhang.

</span>
<span class="ltx_bibblock">Fast dynamic 3d object generation from a single-view video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2401.08742</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reizenstein etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny.

</span>
<span class="ltx_bibblock">Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.Â  10901â€“10911, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Dreamgaussian4d: Generative 4d gaussian splatting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2312.17142</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, SeungÂ Wook Kim, etÂ al.

</span>
<span class="ltx_bibblock">L4gm: Large 4d gaussian reconstruction model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2406.10324</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach etÂ al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  10684â€“10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarlin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Superglue: Learning feature matching with graph neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  4938â€“4947, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SchÃ¶nberger &amp; Frahm (2016)</span>
<span class="ltx_bibblock">
JohannesÂ Lutz SchÃ¶nberger and Jan-Michael Frahm.

</span>
<span class="ltx_bibblock">Structure-from-motion revisited.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SchÃ¶nberger etÂ al. (2016)</span>
<span class="ltx_bibblock">
JohannesÂ Lutz SchÃ¶nberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm.

</span>
<span class="ltx_bibblock">Pixelwise view selection for unstructured multi-view stereo.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">European Conference on Computer Vision (ECCV)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann etÂ al. (2022)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, etÂ al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2210.08402</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su.

</span>
<span class="ltx_bibblock">Zero123++: a single image to consistent multi-view diffusion base model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2310.15110</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang.

</span>
<span class="ltx_bibblock">Mvdream: Multi-view diffusion for 3d generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2308.16512</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singer etÂ al. (2022)</span>
<span class="ltx_bibblock">
Uriel Singer, Adam Polyak, Thomas Hayes, XiÂ Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, etÂ al.

</span>
<span class="ltx_bibblock">Make-a-video: Text-to-video generation without text-video data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2209.14792</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singer etÂ al. (2023)</span>
<span class="ltx_bibblock">
Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, etÂ al.

</span>
<span class="ltx_bibblock">Text-to-4d dynamic scene generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2301.11280</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sitzmann etÂ al. (2021)</span>
<span class="ltx_bibblock">
Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand.

</span>
<span class="ltx_bibblock">Light field networks: Neural scene representations with single-evaluation rendering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in Neural Information Processing Systems</em>, 34:19313â€“19325, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stability (2023)</span>
<span class="ltx_bibblock">
Stability.

</span>
<span class="ltx_bibblock">Stable video diffusion: Scaling latent video diffusion models to large datasets.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets" title="">https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa.

</span>
<span class="ltx_bibblock">Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, and Rakesh Ranjan.

</span>
<span class="ltx_bibblock">Mvdiffusion++: A dense high-resolution multi-view diffusion model for single or sparse-view 3d object reconstruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2402.12712</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tseng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, and Johannes Kopf.

</span>
<span class="ltx_bibblock">Consistent view synthesis with pose-guided diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">CVPR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unterthiner etÂ al. (2018)</span>
<span class="ltx_bibblock">
Thomas Unterthiner, Sjoerd VanÂ Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly.

</span>
<span class="ltx_bibblock">Towards accurate generative models of video: A new metric &amp; challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:1812.01717</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voleti etÂ al. (2024)</span>
<span class="ltx_bibblock">
Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani.

</span>
<span class="ltx_bibblock">Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2403.12008</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu.

</span>
<span class="ltx_bibblock">Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation.

</span>
<span class="ltx_bibblock">2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
YiÂ Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, etÂ al.

</span>
<span class="ltx_bibblock">Internvid: A large-scale video-text dataset for multimodal understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2307.06942</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan.

</span>
<span class="ltx_bibblock">Motionctrl: A unified and flexible motion controller for video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2312.03641</em>, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watson etÂ al. (2022)</span>
<span class="ltx_bibblock">
Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.

</span>
<span class="ltx_bibblock">Novel view synthesis with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2210.04628</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watson etÂ al. (2024)</span>
<span class="ltx_bibblock">
Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and DavidÂ J Fleet.

</span>
<span class="ltx_bibblock">Controlling space and time with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2407.07860</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia etÂ al. (2024)</span>
<span class="ltx_bibblock">
Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang.

</span>
<span class="ltx_bibblock">Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2401.12592</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani.

</span>
<span class="ltx_bibblock">Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2407.17470</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat.

</span>
<span class="ltx_bibblock">Camco: Camera-controllable 3d-consistent image-to-video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2406.02509</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas.

</span>
<span class="ltx_bibblock">Videogpt: Video generation using vq-vae and transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2104.10157</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li.

</span>
<span class="ltx_bibblock">Consistnet: Enforcing 3d consistency for multi-view images diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  7079â€“7088, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, etÂ al.

</span>
<span class="ltx_bibblock">Cogvideox: Text-to-video diffusion models with an expert transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2408.06072</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao, and Yunchao Wei.

</span>
<span class="ltx_bibblock">4dgen: Grounded 4d content generation with spatial-temporal consistency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">arXiv preprint arXiv:2312.17225</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, etÂ al.

</span>
<span class="ltx_bibblock">Mvimgnet: A large-scale dataset of multi-view images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  9150â€“9161, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao.

</span>
<span class="ltx_bibblock">Stag4d: Spatial-temporal anchored generative 4d gaussians.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2403.14939</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and YuÂ Qiao.

</span>
<span class="ltx_bibblock">4diffusion: Multi-view video diffusion model for 4d generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2405.20674</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Agrawala (2023)</span>
<span class="ltx_bibblock">
Lvmin Zhang and Maneesh Agrawala.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2302.05543</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, AlexeiÂ A Efros, Eli Shechtman, and Oliver Wang.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as a perceptual metric.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">CVPR</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu.

</span>
<span class="ltx_bibblock">Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">European Conference on Computer Vision</em>, pp.Â  523â€“542. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and GimÂ Hee Lee.

</span>
<span class="ltx_bibblock">Animate124: Animating one image to 4d dynamic scene.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">arXiv preprint arXiv:2311.14603</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini DeÂ Mello.

</span>
<span class="ltx_bibblock">A unified approach for text-and image-guided 4d scene generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2311.16854</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2018)</span>
<span class="ltx_bibblock">
Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely.

</span>
<span class="ltx_bibblock">Stereo magnification: Learning view synthesis using multiplane images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:1805.09817</em>, 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Implementation Details</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.2">Our training is divided into static stage and dynamic stage.
Our static stage is trained for around 500k iterations and our dynamic stage is trained for roughly 300k iterations.
The effective batch size is 128 and the learning rate is 1e-4. Our video length is 14 frames for each view with the first frame shared across views. Our model is fine-tuned at <math alttext="256\times 256" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mn id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">256</mn><mo id="A1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><times id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></times><cn id="A1.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.p1.1.m1.1.1.2">256</cn><cn id="A1.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">256 Ã— 256</annotation></semantics></math> spatial resolution from the SVD 1.0 checkpoint. The training data are prepared by first center-cropping the original videos and then resizing each frame to the shape of <math alttext="256\times 256" class="ltx_Math" display="inline" id="A1.p1.2.m2.1"><semantics id="A1.p1.2.m2.1a"><mrow id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mn id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml">256</mn><mo id="A1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><times id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1.1"></times><cn id="A1.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.p1.2.m2.1.1.2">256</cn><cn id="A1.p1.2.m2.1.1.3.cmml" type="integer" xref="A1.p1.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="A1.p1.2.m2.1d">256 Ã— 256</annotation></semantics></math>. In the dynamic stage, 30% of iterations are used to train on monocular videos. During static training, the strides of frames are randomly sampled in the range of <span class="ltx_text ltx_font_typewriter" id="A1.p1.2.1">[1, 8]</span>. For monocular videos, the strides are sampled in the range of <span class="ltx_text ltx_font_typewriter" id="A1.p1.2.2">[1, 2]</span>. For dynamic multi-view object renderings, the strides are fixed to 1 to use all rendered frames since we already introduced randomness in the frame rate during rendering.
At inference time, the decoding chunk is set to 14 so all frames are decoded altogether. We sample 25 steps to obtain all our results.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Data Curation Details</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In this section, we provide additional details on our data processing and curation pipelines.</p>
</div>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Static 3D Objects</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1">Our static objects data comprises multi-view images rendered from the ObjaverseÂ <cite class="ltx_cite ltx_citemacro_citep">(Deitke etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib10" title="">2023b</a>)</cite> and Objaverse-XL<cite class="ltx_cite ltx_citemacro_citep">(Deitke etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib9" title="">2023a</a>)</cite> dataset. Similar to InstantMesh, we use a filtered high-quality subset of the original dataset to train our model. The filtering goal is to remove objects that satisfy any of the following criteria: (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.1">\romannum</span>1) objects without texture maps, (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.2">\romannum</span>2) objects with rendered images occupying less than 10% of the view from any angle, (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.3">\romannum</span>3) including multiple separate objects, (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.4">\romannum</span>4) objects with no caption information provided by the Cap3D dataset, and (<span class="ltx_ERROR undefined" id="A2.SS0.SSS0.Px1.p1.1.5">\romannum</span>5) low-quality objects. The classification of â€œlow-qualityâ€ objects is determined based on the presence of tags such as â€œlowpolyâ€ and its variants (e.g., â€œlow polyâ€) in the metadata. By applying our filtering criteria, we curated approximately 400k high-quality instances from the initial pool of 800k objects in the Objaverse dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p2.1">For each 3D object, we use Blenderâ€™s EEVEE renderer to render an 84-frame RGBA orbit at <math alttext="512\times 512" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p2.1.m1.1"><semantics id="A2.SS0.SSS0.Px1.p2.1.m1.1a"><mrow id="A2.SS0.SSS0.Px1.p2.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml"><mn id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.2" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.2.cmml">512</mn><mo id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.3" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p2.1.m1.1b"><apply id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1"><times id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.1"></times><cn id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.2.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.2">512</cn><cn id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.3.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p2.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p2.1.m1.1d">512 Ã— 512</annotation></semantics></math> resolution. we adaptively position the camera to a distance sufficient to ensure that the rendered object content makes good and consistent use of the image extents without being clipped in any view. For each frame, the azimuths can be irregularly spaced, and the elevation can vary per view. Specifically, the sequence of camera elevations for each orbit is obtained from a random weighted combination of sinusoids with different frequencies. The azimuth angles are sampled regularly, and then a small amount of noise is added to make them irregular. The elevation values are smoothed using a simple convolution kernel and then clamped to a maximum elevation of 89 degrees.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Static 3D Scenes</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">Our static scenes data are sourced from RealEstate10kÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib84" title="">2018</a>)</cite>, WildRGBDÂ <cite class="ltx_cite ltx_citemacro_citep">(Xia etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib69" title="">2024</a>)</cite>, MVImgNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib76" title="">2023</a>)</cite>, CO3Dv2Â <cite class="ltx_cite ltx_citemacro_citep">(Reizenstein etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib45" title="">2021</a>)</cite>, and DL3DV-10KÂ <cite class="ltx_cite ltx_citemacro_citep">(Ling etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib35" title="">2023b</a>)</cite>.
For RealEstate10k, we use the train/test split released by PixelSplatÂ <cite class="ltx_cite ltx_citemacro_citep">(Charatan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib5" title="">2023</a>)</cite>. During training, we sample every 8 original frames to construct the training sequences.
For DL3DV-10K, we construct training sequences from the publicly available 7k subset. Since each video is very long for the DL3DV-10k dataset, we offline randomly sample multiple sequences from a single ground truth video to obtain multiple training data items.
For CO3Dv2, we remove the video sequences that contain whole-black images to avoid temporally inconsistent frames.
For WildRGBD and MVImgNet we use all classes available and removed sequences whose lengths are not enough for two-view training (shorter than 27 frames).</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Dynamic 3D Objects</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1">Our dynamic 3D objects are similarly rendered as the static 3D objects. The filtering pipelines remain mostly the same as the static objects, except that we introduce additional workflows to consider object motion. Inspired by previous worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib33" title="">2024</a>; Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib23" title="">2024</a>; Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib29" title="">2024a</a>)</cite> that employ animatable objects from Objaverse. We render multiple fixed-view videos to examine the motion quality of the objects. We utilize lpipsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib80" title="">2018</a>)</cite> to measure the similarity of nearby frames and consider an object to be static if lpips similarity is above a certain threshold. Additionally, we render the alpha masks of the object and use this as an indicator of whether the object has moved out of the visible regions. Consequently, we remove objects with too large or sudden movements as well as objects with little-to-no motion. These filterings result in 19,000 objects. Our rendering strategy is also very similar to that of static 3D objects, introducing random elevation and azimuth changes to complicate the trajectories, except that we additionally introduce a random frame stride at rendering to augment the object motion. The stride is sampled individually for each object from the range <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px3.p1.1.1">[1, 3]</span>. A larger the stride leads to renderings with faster object motion.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Monocular Videos</h4>
<div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1">Our monocular video filtering pipeline involves filtering according to Particle-SfM output, OCR, aesthetic score, and camera motion. As mentioned in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#S4.SS2" title="4.2 Data Curation â€£ 4 Joint Training Strategy on Curated Data Mixtures â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we first attempt to annotate the camera poses for the video frames using Particle-SfMÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib81" title="">2022</a>)</cite>. Take InternVidÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib65" title="">2023b</a>)</cite> as an example, roughly 10 million video clips are processed and around 3 million samples are successfully processed by Particle-SfM. For each video, we start from the first frame and randomly select a frame stride of 1 or 2. The total number of images sent to Particle-SfM is 32 images. Our point count filtering is empirically implemented as a cut-off at 1,000 points and 40,000 points. Point clouds with too few points are removed due to the concern that the frames are poorly registered. Point clouds with too many points are avoided because their limited object motion. This aggressive filtering results in around 2 million samples for further processing. We then evaluate all the video clips using OCR detection algorithms and remove the samples whose detected text regions are larger then <math alttext="{10^{-4}}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="A2.SS0.SSS0.Px4.p1.1.m1.1a"><msup id="A2.SS0.SSS0.Px4.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">10</mn><mrow id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml"><mo id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3a" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml">âˆ’</mo><mn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.2" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1">superscript</csymbol><cn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2">10</cn><apply id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3"><minus id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.1.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3"></minus><cn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px4.p1.1.m1.1c">{10^{-4}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px4.p1.1.m1.1d">10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> of the image resolution (<span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px4.p1.1.1">i.e.</span> 6 pixels). This process results in 604,000 samples. The next step is filtering with aesthetic scores and videos with aesthetic score annotations smaller than 4 are removed. 467,000 videos are left after these filtering process. Finally, we employ a camera motion classifier extended from the Open-Sora pipeline<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hpcaitech/Open-Sora/tree/main/tools/caption/camera_motion" title="">https://github.com/hpcaitech/Open-Sora/tree/main/tools/caption/camera_motion</a></span></span></span>. The main motivation is that optical-flow on consecutive frames can be summarized to a global motion vector, assuming the most parts of the scene is moving in a uniform direction. Optical flow is first obtained using <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px4.p1.1.2">cv2.calcOpticalFlowFarneback</span> for each consecutive frame pairs. Then, the magnitudes and directions are calculated via <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px4.p1.1.3">cv2.cartToPolar</span>. These magnitudes and directions are classified into 8 categories: static, zoom out, zoom in, pan left, tilt up, pan right, tilt down, and unknown. The results of the frame pairs are summarized to obtain the final result of each video clip. When a certain type appears more than 50%, the type for the whole video clip is determined directly. We aggressively classify a video clip as static if any of its frame pairs is categorized into static or unknown. Finally, we obtain 355,000 clips that satisfy our needs. The process is similarly applied to OpenVidÂ <cite class="ltx_cite ltx_citemacro_citep">(Nan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib42" title="">2024</a>)</cite>â€™s Panda-70M subsetÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib6" title="">2024a</a>)</cite> and we obtained 38,000 clips. In summary, our monocular video dataset consists of 393,000 clips.</p>
</div>
<figure class="ltx_table" id="A2.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation Studies on each of our introduced modules. â€œw/o Pluckerâ€ refers to replacing the Plucker coordinate conditioning with one-dimensional conditioning as in MotionCtrl. â€œw/o Cross-frameâ€ refers to replacing the Cross-frame attention with vanilla 1D temporal attention. â€œw/o Cross-viewâ€ refers to replacing the Cross-view attention with vanilla spatial attention. â€œOurs (Static)â€ means the model is only trained on static video datasets. â€œOurs (w/o Mono)â€ means that the model is fine-tuned on synthetic multi-view datasets, but is not trained with monocular video datasets. â€œOurs (Full)â€ means that the model is trained on all available data sources.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T3.8" style="width:433.6pt;height:183.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.1pt,7.6pt) scale(0.923139723404239,0.923139723404239) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T3.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.6.6.6.7" rowspan="2"><span class="ltx_text" id="A2.T3.6.6.6.7.1">Scenes</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.6.6.6.8" rowspan="2"><span class="ltx_text" id="A2.T3.6.6.6.8.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.1.1.1.1" rowspan="2"><span class="ltx_text" id="A2.T3.1.1.1.1.1">FID<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T3.1.1.1.1.1.m1.1"><semantics id="A2.T3.1.1.1.1.1.m1.1a"><mo id="A2.T3.1.1.1.1.1.m1.1.1" stretchy="false" xref="A2.T3.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.1.1.m1.1b"><ci id="A2.T3.1.1.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.1.1.1.1.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.2.2.2.2" rowspan="2"><span class="ltx_text" id="A2.T3.2.2.2.2.1">FVD<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T3.2.2.2.2.1.m1.1"><semantics id="A2.T3.2.2.2.2.1.m1.1a"><mo id="A2.T3.2.2.2.2.1.m1.1.1" stretchy="false" xref="A2.T3.2.2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.2.2.1.m1.1b"><ci id="A2.T3.2.2.2.2.1.m1.1.1.cmml" xref="A2.T3.2.2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.2.2.2.2.1.m1.1d">â†“</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.3.3.3.3">Rot. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T3.3.3.3.3.m1.1"><semantics id="A2.T3.3.3.3.3.m1.1a"><mo id="A2.T3.3.3.3.3.m1.1.1" stretchy="false" xref="A2.T3.3.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A2.T3.3.3.3.3.m1.1b"><ci id="A2.T3.3.3.3.3.m1.1.1.cmml" xref="A2.T3.3.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.3.3.3.3.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.4.4.4.4">Trans. AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T3.4.4.4.4.m1.1"><semantics id="A2.T3.4.4.4.4.m1.1a"><mo id="A2.T3.4.4.4.4.m1.1.1" stretchy="false" xref="A2.T3.4.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A2.T3.4.4.4.4.m1.1b"><ci id="A2.T3.4.4.4.4.m1.1.1.cmml" xref="A2.T3.4.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.4.4.4.4.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.5.5.5.5" rowspan="2"><span class="ltx_text" id="A2.T3.5.5.5.5.1">Prec. <math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T3.5.5.5.5.1.m1.1"><semantics id="A2.T3.5.5.5.5.1.m1.1a"><mo id="A2.T3.5.5.5.5.1.m1.1.1" stretchy="false" xref="A2.T3.5.5.5.5.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A2.T3.5.5.5.5.1.m1.1b"><ci id="A2.T3.5.5.5.5.1.m1.1.1.cmml" xref="A2.T3.5.5.5.5.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.5.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.5.5.5.5.1.m1.1d">â†‘</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.6.6.6.6" rowspan="2"><span class="ltx_text" id="A2.T3.6.6.6.6.1">MS. <math alttext="\uparrow" class="ltx_Math" display="inline" id="A2.T3.6.6.6.6.1.m1.1"><semantics id="A2.T3.6.6.6.6.1.m1.1a"><mo id="A2.T3.6.6.6.6.1.m1.1.1" stretchy="false" xref="A2.T3.6.6.6.6.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A2.T3.6.6.6.6.1.m1.1b"><ci id="A2.T3.6.6.6.6.1.m1.1.1.cmml" xref="A2.T3.6.6.6.6.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.6.6.6.6.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T3.6.6.6.6.1.m1.1d">â†‘</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.7.7.7.1">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="A2.T3.7.7.7.1.m1.1"><semantics id="A2.T3.7.7.7.1.m1.1a"><mrow id="A2.T3.7.7.7.1.m1.1.1" xref="A2.T3.7.7.7.1.m1.1.1.cmml"><msup id="A2.T3.7.7.7.1.m1.1.1.2" xref="A2.T3.7.7.7.1.m1.1.1.2.cmml"><mn id="A2.T3.7.7.7.1.m1.1.1.2.2" xref="A2.T3.7.7.7.1.m1.1.1.2.2.cmml">5</mn><mo id="A2.T3.7.7.7.1.m1.1.1.2.3" xref="A2.T3.7.7.7.1.m1.1.1.2.3.cmml">âˆ˜</mo></msup><mo id="A2.T3.7.7.7.1.m1.1.1.1" xref="A2.T3.7.7.7.1.m1.1.1.1.cmml">/</mo><msup id="A2.T3.7.7.7.1.m1.1.1.3" xref="A2.T3.7.7.7.1.m1.1.1.3.cmml"><mn id="A2.T3.7.7.7.1.m1.1.1.3.2" xref="A2.T3.7.7.7.1.m1.1.1.3.2.cmml">10</mn><mo id="A2.T3.7.7.7.1.m1.1.1.3.3" xref="A2.T3.7.7.7.1.m1.1.1.3.3.cmml">âˆ˜</mo></msup><mo id="A2.T3.7.7.7.1.m1.1.1.1a" xref="A2.T3.7.7.7.1.m1.1.1.1.cmml">/</mo><msup id="A2.T3.7.7.7.1.m1.1.1.4" xref="A2.T3.7.7.7.1.m1.1.1.4.cmml"><mn id="A2.T3.7.7.7.1.m1.1.1.4.2" xref="A2.T3.7.7.7.1.m1.1.1.4.2.cmml">20</mn><mo id="A2.T3.7.7.7.1.m1.1.1.4.3" xref="A2.T3.7.7.7.1.m1.1.1.4.3.cmml">âˆ˜</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.7.7.7.1.m1.1b"><apply id="A2.T3.7.7.7.1.m1.1.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1"><divide id="A2.T3.7.7.7.1.m1.1.1.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1.1"></divide><apply id="A2.T3.7.7.7.1.m1.1.1.2.cmml" xref="A2.T3.7.7.7.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T3.7.7.7.1.m1.1.1.2.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1.2">superscript</csymbol><cn id="A2.T3.7.7.7.1.m1.1.1.2.2.cmml" type="integer" xref="A2.T3.7.7.7.1.m1.1.1.2.2">5</cn><compose id="A2.T3.7.7.7.1.m1.1.1.2.3.cmml" xref="A2.T3.7.7.7.1.m1.1.1.2.3"></compose></apply><apply id="A2.T3.7.7.7.1.m1.1.1.3.cmml" xref="A2.T3.7.7.7.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.T3.7.7.7.1.m1.1.1.3.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1.3">superscript</csymbol><cn id="A2.T3.7.7.7.1.m1.1.1.3.2.cmml" type="integer" xref="A2.T3.7.7.7.1.m1.1.1.3.2">10</cn><compose id="A2.T3.7.7.7.1.m1.1.1.3.3.cmml" xref="A2.T3.7.7.7.1.m1.1.1.3.3"></compose></apply><apply id="A2.T3.7.7.7.1.m1.1.1.4.cmml" xref="A2.T3.7.7.7.1.m1.1.1.4"><csymbol cd="ambiguous" id="A2.T3.7.7.7.1.m1.1.1.4.1.cmml" xref="A2.T3.7.7.7.1.m1.1.1.4">superscript</csymbol><cn id="A2.T3.7.7.7.1.m1.1.1.4.2.cmml" type="integer" xref="A2.T3.7.7.7.1.m1.1.1.4.2">20</cn><compose id="A2.T3.7.7.7.1.m1.1.1.4.3.cmml" xref="A2.T3.7.7.7.1.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.7.7.7.1.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.7.7.7.1.m1.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.8.2">(@<math alttext="5^{\circ}/10^{\circ}/20^{\circ}" class="ltx_Math" display="inline" id="A2.T3.8.8.8.2.m1.1"><semantics id="A2.T3.8.8.8.2.m1.1a"><mrow id="A2.T3.8.8.8.2.m1.1.1" xref="A2.T3.8.8.8.2.m1.1.1.cmml"><msup id="A2.T3.8.8.8.2.m1.1.1.2" xref="A2.T3.8.8.8.2.m1.1.1.2.cmml"><mn id="A2.T3.8.8.8.2.m1.1.1.2.2" xref="A2.T3.8.8.8.2.m1.1.1.2.2.cmml">5</mn><mo id="A2.T3.8.8.8.2.m1.1.1.2.3" xref="A2.T3.8.8.8.2.m1.1.1.2.3.cmml">âˆ˜</mo></msup><mo id="A2.T3.8.8.8.2.m1.1.1.1" xref="A2.T3.8.8.8.2.m1.1.1.1.cmml">/</mo><msup id="A2.T3.8.8.8.2.m1.1.1.3" xref="A2.T3.8.8.8.2.m1.1.1.3.cmml"><mn id="A2.T3.8.8.8.2.m1.1.1.3.2" xref="A2.T3.8.8.8.2.m1.1.1.3.2.cmml">10</mn><mo id="A2.T3.8.8.8.2.m1.1.1.3.3" xref="A2.T3.8.8.8.2.m1.1.1.3.3.cmml">âˆ˜</mo></msup><mo id="A2.T3.8.8.8.2.m1.1.1.1a" xref="A2.T3.8.8.8.2.m1.1.1.1.cmml">/</mo><msup id="A2.T3.8.8.8.2.m1.1.1.4" xref="A2.T3.8.8.8.2.m1.1.1.4.cmml"><mn id="A2.T3.8.8.8.2.m1.1.1.4.2" xref="A2.T3.8.8.8.2.m1.1.1.4.2.cmml">20</mn><mo id="A2.T3.8.8.8.2.m1.1.1.4.3" xref="A2.T3.8.8.8.2.m1.1.1.4.3.cmml">âˆ˜</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.8.8.8.2.m1.1b"><apply id="A2.T3.8.8.8.2.m1.1.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1"><divide id="A2.T3.8.8.8.2.m1.1.1.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1.1"></divide><apply id="A2.T3.8.8.8.2.m1.1.1.2.cmml" xref="A2.T3.8.8.8.2.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T3.8.8.8.2.m1.1.1.2.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1.2">superscript</csymbol><cn id="A2.T3.8.8.8.2.m1.1.1.2.2.cmml" type="integer" xref="A2.T3.8.8.8.2.m1.1.1.2.2">5</cn><compose id="A2.T3.8.8.8.2.m1.1.1.2.3.cmml" xref="A2.T3.8.8.8.2.m1.1.1.2.3"></compose></apply><apply id="A2.T3.8.8.8.2.m1.1.1.3.cmml" xref="A2.T3.8.8.8.2.m1.1.1.3"><csymbol cd="ambiguous" id="A2.T3.8.8.8.2.m1.1.1.3.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1.3">superscript</csymbol><cn id="A2.T3.8.8.8.2.m1.1.1.3.2.cmml" type="integer" xref="A2.T3.8.8.8.2.m1.1.1.3.2">10</cn><compose id="A2.T3.8.8.8.2.m1.1.1.3.3.cmml" xref="A2.T3.8.8.8.2.m1.1.1.3.3"></compose></apply><apply id="A2.T3.8.8.8.2.m1.1.1.4.cmml" xref="A2.T3.8.8.8.2.m1.1.1.4"><csymbol cd="ambiguous" id="A2.T3.8.8.8.2.m1.1.1.4.1.cmml" xref="A2.T3.8.8.8.2.m1.1.1.4">superscript</csymbol><cn id="A2.T3.8.8.8.2.m1.1.1.4.2.cmml" type="integer" xref="A2.T3.8.8.8.2.m1.1.1.4.2">20</cn><compose id="A2.T3.8.8.8.2.m1.1.1.4.3.cmml" xref="A2.T3.8.8.8.2.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.8.8.8.2.m1.1c">5^{\circ}/10^{\circ}/20^{\circ}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.8.8.8.2.m1.1d">5 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 10 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT / 20 start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.9.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.1" rowspan="4"><span class="ltx_text" id="A2.T3.8.8.9.1.1.1">Real10K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.2">w/o PlÃ¼cker</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.9.1.3">12.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.4">195.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.5">12.1 / 21.9 / 35.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.9.1.6">1.6 / 5.8 / 16.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.9.1.7">14.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.9.1.8">10.02</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.10.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.10.2.1">w/o Cross-frame</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.10.2.2">17.04</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.10.2.3">154.54</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.10.2.4">21.4 / 34.8 / 50.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.10.2.5">3.8 / 11.1 / 24.2</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.10.2.6">25.67</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.10.2.7">12.70</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.11.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.11.3.1">w/o Cross-view</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.11.3.2">9.45</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.11.3.3">106.82</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.11.3.4">22.8 / 36.7 / 52.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.11.3.5">2.7 / 8.7 / 22.1</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.11.3.6">27.57</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.11.3.7">14.65</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.12.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.12.4.1">Ours</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.12.4.2"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.2.1">8.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.12.4.3"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.3.1">94.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.12.4.4">
<span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.4.1">23.9 </span>/ <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.4.2">37.4</span> / <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.4.3">52.9</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.12.4.5">
<span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.5.1">3.3</span> / <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.5.2">10.2</span> / <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.5.3">23.5</span>
</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.12.4.6"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.6.1">29.39</span></td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.12.4.7"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.12.4.7.1">15.22</span></td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.13.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.1" rowspan="5"><span class="ltx_text" id="A2.T3.8.8.13.5.1.1">General</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.2">w/o Cross-frame</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.13.5.3">71.39</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.4">249.02</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.5">9.8 / 19.1 / 32.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T3.8.8.13.5.6">0.5 / 1.9 / 6.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.13.5.7">13.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.8.8.13.5.8">8.97</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.14.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.14.6.1">w/o Cross-view</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.14.6.2">30.89</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.14.6.3">246.68</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.14.6.4">14.9 / 27.4 / 42.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.14.6.5">1.2 / 4.3 / 12.2</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.14.6.6">17.58</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.14.6.7">9.59</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.15.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.15.7.1">Ours (Static)</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.15.7.2">27.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.15.7.3">185.58</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.15.7.4">15.9 / 28.7 / 44.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.15.7.5">1.4 / 4.6 / 12.9</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.15.7.6">21.75</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.15.7.7">12.04</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.16.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.16.8.1">Ours (w/o Mono)</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.16.8.2">35.79</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.16.8.3">243.05</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.16.8.4">15.0 / 27.1 / 42.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T3.8.8.16.8.5">0.3 / 1.3 / 4.2</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.16.8.6">18.55</td>
<td class="ltx_td ltx_align_center" id="A2.T3.8.8.16.8.7">10.78</td>
</tr>
<tr class="ltx_tr" id="A2.T3.8.8.17.9">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T3.8.8.17.9.1">Ours (Full)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T3.8.8.17.9.2"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.2.1">26.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T3.8.8.17.9.3"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.3.1">173.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T3.8.8.17.9.4">
<span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.4.1">19.7</span> / <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.4.2">32.7 </span>/ <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.4.3">48.4</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T3.8.8.17.9.5">
<span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.5.1">0.8 </span>/ <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.5.2">2.8 </span>/ <span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.5.3">8.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T3.8.8.17.9.6"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.6.1">33.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T3.8.8.17.9.7"><span class="ltx_text ltx_font_bold" id="A2.T3.8.8.17.9.7.1">19.96</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="A2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="942" id="A2.F6.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Ablation studies on PlÃ¼cker coordinates and Cross-frame Attention. Video results are provided in supplementary for clearer qualitative comparisons.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="463" id="A2.F7.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Ablation studies on Cross-view Attention. Video results are provided in supplementary for clearer qualitative comparisons.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="462" id="A2.F8.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Ablation studies on the joint training strategy on monocular videos. Video results are provided in supplementary for clearer qualitative comparisons.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evaluation Details</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">For MotionCtrl and CameraCtrl, we use the open-source checkpoints trained from SVD released by the authors. These checkpoints are designed for image-to-video tasks so we can have fair comparisons. We use â€œclean-fidâ€<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/GaParmar/clean-fid" title="">https://github.com/GaParmar/clean-fid</a></span></span></span> and â€œcommon-metrics-on-video-qualityâ€<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/JunyaoHu/common_metrics_on_video_quality" title="">https://github.com/JunyaoHu/common_metrics_on_video_quality</a></span></span></span> for obtaining FID and FVD, respectively. Our FVD results are reported in VideoGPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Yan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib72" title="">2021</a>)</cite> format. Our COLMAP is configured following DSNeRFÂ <cite class="ltx_cite ltx_citemacro_citep">(Deng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib11" title="">2022</a>)</cite> and CamCoÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib71" title="">2024</a>)</cite> to improve the few-view reconstruction performance. Concretely speaking, we enable <span class="ltx_text ltx_font_typewriter" id="A3.p1.1.1">--SiftMatching.max_num_matches 65536</span> to support robust feature matching. To ensure that the SfM results best align with our videos, we set <span class="ltx_text ltx_font_typewriter" id="A3.p1.1.2">--ImageReader.single_camera 1</span> since most videos in our datasets consist of frames captured from a single camera.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Ablation Studies</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">In this section, we conduct extensive evaluations for ablation studies. We provide video comparisons in the supplementary.
We provide thorough quantitative comparisons in Tab.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.T3" title="Table 3 â€£ Monocular Videos â€£ Appendix B Additional Data Curation Details â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">3</span></a> to illustrate the importance of our proposed components. The models are evaluated using RealEstate10K camera trajectories. For the â€œReal10Kâ€ and â€œGeneralâ€ categories, the testing images are from our test set split of RealEstate10K and InternVid, respectively. Our full model enjoys the best perceptual quality and geometric consistency.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">We first examine the importance of Plucker coordinates conditioning and the cross-frame attention modules. As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.F6" title="Figure 6 â€£ Monocular Videos â€£ Appendix B Additional Data Curation Details â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">6</span></a>, model variants without cross-frame attention contains severe distortion artifacts, such as the bent walls. The model variant without Plucker coordinates results in simplified camera motion that ignores the complex camera viewpoint instructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p3">
<p class="ltx_p" id="A4.p3.1">We then evaluate the model variant without cross-view attention. As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.F7" title="Figure 7 â€£ Monocular Videos â€£ Appendix B Additional Data Curation Details â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">7</span></a>, we observe that removing the cross-view attention module results in multiple individual video samples that contain different object motions. For example, the penguin moves differently in the first case, and the wood sticks in the fire appear differently in the second case. This behavior is not desirable because our goal is to obtain multiple videos from different camera paths of the same scene.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p4">
<p class="ltx_p" id="A4.p4.1">Finally, we examine the importance of our monocular video joint training strategy. As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A2.F8" title="Figure 8 â€£ Monocular Videos â€£ Appendix B Additional Data Curation Details â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">8</span></a>, we observe that when overfitting on dynamic objects from Objaverse, the generated results tend to contain frames with simplified backgrounds. This is mainly because, during the training, all data samples from Objaverse are implemented with single random color backgrounds. Our model benefits from joint training on monocular videos and preserves the ability to generate complex backgrounds when object motion is present.</p>
</div>
<figure class="ltx_figure" id="A4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="710" id="A4.F9.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Four-view video comparison. The result of CVD is taken from their website. CVD tends to generate black border pixels, potentially due to its homography warping augmentations during training. In comparison, our method produces frames with better geometric consistency and perceptual quality.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="864" id="A4.F10.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>3D Reconstruction comparison. We render the reconstructed 3D Gaussians from an elliptical trajectory consisting of 16 novel views. The result of CVD is taken from their website. CVDâ€™s reconstruction results suffer from floaters and blurry artifacts due to the inconsistency in their generated frames. In comparison, our method produces sharper results with clearer visual quality.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Applications</h2>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">In this section, we provide additional results on four-view inference and 3D reconstruction of our generated frames.</p>
</div>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Advancing to Four Views at Inference</h3>
<div class="ltx_para ltx_noindent" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.1">Our cross-view attention design enables us to extrapolate to more views straightforwardly at inference time. This design is more efficient compared with the concurrent work CVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite> which requires enumeration of viewpoint pairs at inference time. We conduct a side-by-side comparison for 4-view generation in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A4.F9" title="Figure 9 â€£ Appendix D Ablation Studies â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">9</span></a>. Our method enjoys better consistency and shows more realistic results than CVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite>. In comparison, CVD tends to produce artifacts at border regions. For example, the structure of the wall (first case) and the window (second case) change when the viewpoint changes. The results from CVD are taken from their authorâ€™s website. We provide video comparisons in the supplementary. We also provide more 4-view generation results from Cavia in our supplementary.</p>
</div>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>3D Reconstruction of Generated Frames</h3>
<div class="ltx_para ltx_noindent" id="A5.SS2.p1">
<p class="ltx_p" id="A5.SS2.p1.1">We further perform 3D reconstruction on our generated frames. We render our reconstructed 3D Gaussians from an elliptical trajectory consisting of 16 novel views. We provide a side-by-side comparison with the concurrent work CVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib27" title="">2024</a>)</cite> in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#A4.F10" title="Figure 10 â€£ Appendix D Ablation Studies â€£ Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention"><span class="ltx_text ltx_ref_tag">10</span></a>.
Compared with the results of CVD, our frames are more geometrically consistent and result in clearer 3D reconstruction and fewer floaters. For example, the results from CVD produce floaters on the cupboard regions and generate blurry artifacts for the wall and the TV due to inconsistencies. We provide video comparisons in the supplementary for clearer comparisons. We also provide additional 3D reconstruction results of Caviaâ€™s generated frames in the supplementary.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Limitations</h2>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">Our framework has limited ability to generate large object motion, mainly due to the limitation of the base video generator SVDÂ <cite class="ltx_cite ltx_citemacro_citep">(Stability, <a class="ltx_ref" href="https://arxiv.org/html/2410.10774v1#bib.bib58" title="">2023</a>)</cite>. We will explore better base models in future works.
Moreover, our data curation pipelines assume a simple camera model using shared camera intrinsic across frames. While enabling easier data preparation, this limits our model from generalizing to complex camera intrinsic changes at inference time, which is widely adopted in cinematography.
Additionally, for simplicity, our framework is trained with normalized scales of scenes, which can be further improved if potentially calibrated with metric scale. We will explore calibration techniques for better quality if a well-generalizable metric depth estimator becomes publicly available.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct 14 17:44:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
