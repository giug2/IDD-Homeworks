<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.17914] 3D-Aware Visual Question Answering about Parts, Poses and Occlusions</title><meta property="og:description" content="Despite rapid progress in Visual question answering (VQA), existing datasets and models mainly focus on testing reasoning in 2D.
However, it is important that VQA models also understand the 3D structure of visual scene…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3D-Aware Visual Question Answering about Parts, Poses and Occlusions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="3D-Aware Visual Question Answering about Parts, Poses and Occlusions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.17914">

<!--Generated on Wed Feb 28 00:18:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">3D-Aware Visual Question Answering 
<br class="ltx_break">about Parts, Poses and Occlusions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xingrui Wang<sup id="id1.1.1" class="ltx_sup"><math id="id1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="id1.1.1.m1.1a"><mn id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><cn type="integer" id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">1</annotation></semantics></math></sup>
 Wufei Ma<sup id="id2.2.2" class="ltx_sup"><math id="id2.2.2.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="id2.2.2.m1.1a"><mn id="id2.2.2.m1.1.1" xref="id2.2.2.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><cn type="integer" id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">1</annotation></semantics></math></sup>  Zhuowan Li<sup id="id3.3.3" class="ltx_sup"><math id="id3.3.3.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="id3.3.3.m1.1a"><mn id="id3.3.3.m1.1.1" xref="id3.3.3.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="id3.3.3.m1.1b"><cn type="integer" id="id3.3.3.m1.1.1.cmml" xref="id3.3.3.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m1.1c">1</annotation></semantics></math></sup>
 <span id="id4.4.4" class="ltx_text ltx_font_bold">Adam Kortylewski<sup id="id4.4.4.1" class="ltx_sup"><math id="id4.4.4.1.m1.2" class="ltx_Math" alttext="2,3" display="inline"><semantics id="id4.4.4.1.m1.2a"><mrow id="id4.4.4.1.m1.2.3.2" xref="id4.4.4.1.m1.2.3.1.cmml"><mn id="id4.4.4.1.m1.1.1" xref="id4.4.4.1.m1.1.1.cmml">2</mn><mo id="id4.4.4.1.m1.2.3.2.1" xref="id4.4.4.1.m1.2.3.1.cmml">,</mo><mn id="id4.4.4.1.m1.2.2" xref="id4.4.4.1.m1.2.2.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="id4.4.4.1.m1.2b"><list id="id4.4.4.1.m1.2.3.1.cmml" xref="id4.4.4.1.m1.2.3.2"><cn type="integer" id="id4.4.4.1.m1.1.1.cmml" xref="id4.4.4.1.m1.1.1">2</cn><cn type="integer" id="id4.4.4.1.m1.2.2.cmml" xref="id4.4.4.1.m1.2.2">3</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id4.4.4.1.m1.2c">2,3</annotation></semantics></math></sup></span>  <span id="id5.5.5" class="ltx_text ltx_font_bold">Alan Yuille<sup id="id5.5.5.1" class="ltx_sup"><math id="id5.5.5.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="id5.5.5.1.m1.1a"><mn id="id5.5.5.1.m1.1.1" xref="id5.5.5.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="id5.5.5.1.m1.1b"><cn type="integer" id="id5.5.5.1.m1.1.1.cmml" xref="id5.5.5.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="id5.5.5.1.m1.1c">1</annotation></semantics></math></sup></span> 
<br class="ltx_break"><sup id="id6.6.6" class="ltx_sup"><math id="id6.6.6.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="id6.6.6.m1.1a"><mn id="id6.6.6.m1.1.1" xref="id6.6.6.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="id6.6.6.m1.1b"><cn type="integer" id="id6.6.6.m1.1.1.cmml" xref="id6.6.6.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="id6.6.6.m1.1c">1</annotation></semantics></math></sup> Johns Hopkins University   <sup id="id7.7.7" class="ltx_sup"><math id="id7.7.7.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="id7.7.7.m1.1a"><mn id="id7.7.7.m1.1.1" xref="id7.7.7.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="id7.7.7.m1.1b"><cn type="integer" id="id7.7.7.m1.1.1.cmml" xref="id7.7.7.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="id7.7.7.m1.1c">2</annotation></semantics></math></sup> Max Planck Institute for Informatics  <sup id="id8.8.8" class="ltx_sup"><math id="id8.8.8.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="id8.8.8.m1.1a"><mn id="id8.8.8.m1.1.1" xref="id8.8.8.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="id8.8.8.m1.1b"><cn type="integer" id="id8.8.8.m1.1.1.cmml" xref="id8.8.8.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="id8.8.8.m1.1c">3</annotation></semantics></math></sup> University of Freiburg  
<br class="ltx_break"><span id="id9.9.id1" class="ltx_text ltx_font_typewriter">{xwang378, wma27, zli110, ayuille1}@jhu.edu</span>   <span id="id10.10.id2" class="ltx_text ltx_font_typewriter">akortyle@mpi-inf.mpg.de</span>
</span><span class="ltx_author_notes">Wufei contributed to develop the 3D algorithm for multi-objects pose estimation.Zhuowan contributed to dataset construction, manuscript writing and conceptualizing the framework.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Despite rapid progress in Visual question answering (<span id="id11.id1.1" class="ltx_text ltx_font_italic">VQA</span>), existing datasets and models mainly focus on testing reasoning in 2D.
However, it is important that VQA models also understand the 3D structure of visual scenes, for example to support tasks like navigation or manipulation.
This includes an understanding of the 3D object pose, their parts and occlusions.
In this work, we introduce the task of 3D-aware VQA, which focuses on challenging questions that require a compositional reasoning over the 3D structure of visual scenes.
We address 3D-aware VQA from both the dataset and the model perspective.
First, we introduce Super-CLEVR-3D, a compositional reasoning dataset that contains questions about object parts, their 3D poses, and occlusions.
Second, we propose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas: probabilistic neural symbolic program execution for reasoning and deep neural networks with 3D generative representations of objects for robust visual recognition.
Our experimental results show our model PO3D-VQA outperforms existing methods significantly, but we still observe a significant performance gap compared to 2D VQA benchmarks, indicating that 3D-aware VQA remains an important open research area.
The code is available at <a target="_blank" href="https://github.com/XingruiWang/3D-Aware-VQA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/XingruiWang/3D-Aware-VQA</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (<span id="S1.p1.1.1" class="ltx_text ltx_font_italic">VQA</span>) is a challenging task that requires an in-depth understanding of vision and language, as well as multi-modal reasoning. Various benchmarks and models have been proposed to tackle this challenging task, but they mainly focus on 2D questions about objects, attributes, or 2D spatial relationships. However, it is important that VQA models understand the 3D structure of scenes, in order to support tasks like autonomous navigation and manipulation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">An inherent property of human vision is that we can naturally answer questions that require a comprehensive understanding of the 3D structure in images. For example, humans can answer the questions shown in <a href="#S1.F1" title="In 1 Introduction ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, which ask about the object parts, their 3D poses, and occlusions. However, current VQA models, which often rely on 2D bounding boxes to encode a visual scene <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">anderson2018bottom, </a>; <a href="#bib.bib59" title="" class="ltx_ref">zhang2021vinvl, </a>; <a href="#bib.bib25" title="" class="ltx_ref">kamath2021mdetr, </a>)</cite> struggle to answer such questions reliably (as can be seen from our experiments). We hypothesize this is caused by the lack of understanding of the 3D structure images.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.17914/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples from Super-CLEVR-3D. We introduce the task of 3D-aware VQA, which requires 3D understanding of the image, including the parts, 3D poses, and occlusions.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we introduce the task of 3D-aware VQA, where answering the questions requires compositional reasoning over the 3D structure of the visual scenes. More specifically, we focus on challenging questions that require multi-step reasoning about the object-part hierarchy, the 3D poses of the objects, and the occlusion relationships between objects or parts.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We address the challenging 3D-aware VQA task from both the dataset and the model perspective.
From the dataset perspective, we introduce Super-CLEVR-3D, which extends the Super-CLEVR dataset <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite> with 3D-aware questions. Given the visual scenes from Super-CLEVR that contain randomly placed vehicles of various categories, we define a set of 3D-aware reasoning operations and automatically generate 3D questions based on these operations. <a href="#S1.F1" title="In 1 Introduction ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows examples of the images, questions and the underlying 3D operations for the questions.
From the model perspective, we introduce PO3D-VQA, a VQA model that marries two powerful ideas: probabilistic neural symbolic program execution for reasoning and a deep neural network with 3D generative representations of objects for robust visual scene parsing. Our model first recovers a 3D scene representation from the image and a program from the question, and subsequently executes the program on the 3D scene representation to obtain an answer using a probabilistic reasoning process that takes into account the confidence of predictions from the neural network.
We refer to our system as PO3D-VQA, which stands for <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">P</span>arts, Poses, and <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">O</span>cclusions in <span id="S1.p4.1.3" class="ltx_text ltx_font_bold">3D</span> <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">V</span>isual <span id="S1.p4.1.5" class="ltx_text ltx_font_bold">Q</span>uestion <span id="S1.p4.1.6" class="ltx_text ltx_font_bold">A</span>nswering.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">On Super-CLEVR-3D, we experiment with existing representative models, their variants, and our model PO3D-VQA. The results show that our model outperforms existing methods significantly, leading to an improvement in accuracy of more than 11%, which shows the advantage of the generative 3D scene parser and the probabilistic neural symbolic reasoning process. Moreover, further analysis on questions with different difficulty levels reveals that the improvements of our model are even greater on harder questions with heavy occlusions and small part sizes. Our results indicate that a reliable 3D understanding, together with the modular reasoning procedure, produces a desirable 3D-aware VQA model.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, our contributions are as follows. (1) We introduce the challenging task of 3D-aware VQA and propose the Super-CLEVR-3D dataset, where 3D visual understanding about parts, 3D poses, and occlusions are required. (2) We propose a 3D-aware neural modular model PO3D-VQA that conducts probabilistic reasoning in a step-wise modular procedure based on robust 3D scene parsing. (3) With experiments, we show that 3D-aware knowledge and modular reasoning are crucial for 3D-aware VQA, and suggest future VQA methods take 3D understanding into account.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Question Answering (VQA).</span>
Rapid progress has been made in VQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib4" title="" class="ltx_ref">antol2015vqa, </a>)</cite> in both the datasets and the models. To solve the challenging VQA datasets <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib15" title="" class="ltx_ref">balanced_vqa_v2, </a>; <a href="#bib.bib61" title="" class="ltx_ref">zhu2016visual7w, </a>; <a href="#bib.bib17" title="" class="ltx_ref">gurari2018vizwiz, </a>; <a href="#bib.bib45" title="" class="ltx_ref">ren2015exploring, </a>)</cite> with real images, multiple models are developed including two-stream feature fusion <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">anderson2018bottom, </a>; <a href="#bib.bib14" title="" class="ltx_ref">fukui2016multimodal, </a>; <a href="#bib.bib28" title="" class="ltx_ref">kim2018bilinear, </a>; <a href="#bib.bib55" title="" class="ltx_ref">yu2019mcan, </a>; <a href="#bib.bib23" title="" class="ltx_ref">hudson2018compositional, </a>; <a href="#bib.bib44" title="" class="ltx_ref">perez2018film, </a>; <a href="#bib.bib30" title="" class="ltx_ref">li2019relation, </a>)</cite> or transformer-based pretraining <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib48" title="" class="ltx_ref">tan2019lxmert, </a>; <a href="#bib.bib36" title="" class="ltx_ref">lu2019vilbert, </a>; <a href="#bib.bib31" title="" class="ltx_ref">li2020oscar, </a>; <a href="#bib.bib59" title="" class="ltx_ref">zhang2021vinvl, </a>; <a href="#bib.bib25" title="" class="ltx_ref">kamath2021mdetr, </a>)</cite>. However, the real datasets are shown to suffer from spurious correlations and biases <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib42" title="" class="ltx_ref">Niu2021CounterfactualVA, </a>; <a href="#bib.bib16" title="" class="ltx_ref">gupta2022swapmix, </a>; <a href="#bib.bib41" title="" class="ltx_ref">Niu_2021_CVPR, </a>; <a href="#bib.bib1" title="" class="ltx_ref">agrawal2016analyzing, </a>; <a href="#bib.bib15" title="" class="ltx_ref">balanced_vqa_v2, </a>; <a href="#bib.bib26" title="" class="ltx_ref">GQA-OOD, </a>; <a href="#bib.bib27" title="" class="ltx_ref">Kervadec2021HowTA, </a>)</cite>. Alternatively, synthetic datasets like CLEVR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib24" title="" class="ltx_ref">johnson2017clevr, </a>)</cite> and Super-CLEVR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite>, are developed to study the compositional reasoning ability of VQA systems, which are also extended to study other vision-and-language tasks <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib34" title="" class="ltx_ref">liu2019clevr, </a>; <a href="#bib.bib29" title="" class="ltx_ref">kottur2019clevr, </a>; <a href="#bib.bib53" title="" class="ltx_ref">yi2019clevrer, </a>; <a href="#bib.bib58" title="" class="ltx_ref">zhang2019raven, </a>; <a href="#bib.bib6" title="" class="ltx_ref">bahdanau2019closure, </a>; <a href="#bib.bib47" title="" class="ltx_ref">salewski2022clevr, </a>; <a href="#bib.bib20" title="" class="ltx_ref">hong2021ptr, </a>)</cite>. The synthetic datasets promote the development of neural modular methods <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">andreas2016neural, </a>; <a href="#bib.bib54" title="" class="ltx_ref">nsvqa, </a>; <a href="#bib.bib40" title="" class="ltx_ref">Mao2019NeuroSymbolic, </a>; <a href="#bib.bib22" title="" class="ltx_ref">hu2018explainable, </a>)</cite>, where the reasoning is done in a modular step-by-step manner. It is shown that the modular methods have nice properties including interpretability, data efficiency <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib54" title="" class="ltx_ref">nsvqa, </a>; <a href="#bib.bib40" title="" class="ltx_ref">Mao2019NeuroSymbolic, </a>)</cite>, better robustness <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite> and strong performance on synthetic images <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib54" title="" class="ltx_ref">nsvqa, </a>)</cite>. However, most existing methods rely on region features <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">anderson2018bottom, </a>; <a href="#bib.bib59" title="" class="ltx_ref">zhang2021vinvl, </a>)</cite> extracted using 2D object detectors <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib46" title="" class="ltx_ref">ren2015faster, </a>)</cite> for image encoding, which is not 3D-aware. We follow the works on the synthetic dataset and enhance the modular methods with 3D understanding.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">VQA in 3D.</span>
Multiple existing works study VQA under the 3D setting, such as SimVQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib8" title="" class="ltx_ref">cadene2019rubi, </a>)</cite>, SQA3D <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib39" title="" class="ltx_ref">ma2022sqa3d, </a>)</cite>, 3DMV-VQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib19" title="" class="ltx_ref">hong20233d, </a>)</cite>, CLEVR-3D <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">yan2021clevr3d, </a>)</cite>, ScanQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib52" title="" class="ltx_ref">ye20223d, </a>)</cite>, 3DQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib52" title="" class="ltx_ref">ye20223d, </a>)</cite>, and EmbodiedQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">das2018embodied, </a>)</cite>, which focus on question answering on the 3D visual scenes like real 3D scans <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib39" title="" class="ltx_ref">ma2022sqa3d, </a>; <a href="#bib.bib51" title="" class="ltx_ref">yan2021clevr3d, </a>; <a href="#bib.bib5" title="" class="ltx_ref">azuma2022scanqa, </a>; <a href="#bib.bib52" title="" class="ltx_ref">ye20223d, </a>)</cite>, simulated 3D environments <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib9" title="" class="ltx_ref">cascante2022simvqa, </a>; <a href="#bib.bib13" title="" class="ltx_ref">das2018embodied, </a>)</cite>, or multi-view images <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib19" title="" class="ltx_ref">hong20233d, </a>)</cite>.
PTR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">hong2021ptr, </a>)</cite> is a synthetic VQA dataset that requires part-based reasoning about physics, analogy and geometry. Our setting differs from these works because we focus on 3D in the <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">questions</span> instead of 3D in the <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">visual scenes</span>, since our 3D-aware questions explicitly query the 3D information that can be inferred from the 2D input images.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">3D scene understanding.</span>
One popular approach for scene understanding is to use the CLIP features pretrained on large-scale text-image pairs and segment the 2D scene into semantic regions <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib10" title="" class="ltx_ref">chen2023clip2scene, </a>; <a href="#bib.bib43" title="" class="ltx_ref">Peng2023OpenScene, </a>)</cite>. However, these methods lack a 3D understanding of the scene and cannot be used to answer 3D-related questions. Another approach is to adopt category-level 6D pose estimation methods that can locate objects in the image and estimate their 3D formulations. Previous approaches include classification-based methods that extend a Faster R-CNN model for 6D pose estimation <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib60" title="" class="ltx_ref">zhou2018starmap, </a>; <a href="#bib.bib38" title="" class="ltx_ref">ma2022robust, </a>)</cite> and compositional models that predicts 6D poses with analysis-by-synthesis <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib38" title="" class="ltx_ref">ma2022robust, </a>)</cite>.
We also notice the huge progress of 3D vision language foundation models, which excel in multiple 3D vision-language understanding tasks <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib19" title="" class="ltx_ref">hong20233d, </a>; <a href="#bib.bib37" title="" class="ltx_ref">luo2023scalable, </a>; <a href="#bib.bib21" title="" class="ltx_ref">hong20233dllm, </a>)</cite>. Still, we focus on the reasoning with compositional reasoning that brings more interpretability and robustness <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Super-CLEVR-3D Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To study 3D-aware VQA, we propose the Super-CLEVR-3D dataset, which contains questions explicitly asking about the 3D object configurations of the image. The images are rendered using scenes from the Super-CLEVR dataset <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite>, which is a VQA dataset containing synthetic scenes of randomly placed vehicles from 5 categories (car, plane, bicycle, motorbike, bus) with various of sub-types (<em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p1.1.2" class="ltx_text"></span> different types of cars) and attributes (color, material, size). The questions are generated by instantiating the question templates based on the image scenes, using a pipeline similar to Super-CLEVR. In Super-CLEVR-3D, three types of 3D-aware questions are introduced: part questions, 3D pose questions, and occlusion questions. In the following, we will describe these three types of questions, and show the new operations we introduced for our 3D-aware questions about object parts, 3D poses, and occlusions. Examples of the dataset are shown in <a href="#S1.F1" title="In 1 Introduction ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.2" class="ltx_p"><span id="S3.p2.2.3" class="ltx_text ltx_font_bold">Part questions.</span>
While in the original Super-CLEVR dataset refers to objects using their holistic names or attributes, objects are complex and have hierarchical parts, as studied in recent works <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib33" title="" class="ltx_ref">liu2022learning, </a>; <a href="#bib.bib11" title="" class="ltx_ref">chen2014detect, </a>; <a href="#bib.bib20" title="" class="ltx_ref">hong2021ptr, </a>)</cite>. Therefore, we introduce part-based questions, which use parts to identify objects (<em id="S3.p2.2.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p2.2.5" class="ltx_text"></span> “which vehicle has red door”) or query about object parts (<em id="S3.p2.2.6" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p2.2.7" class="ltx_text"></span> “what color is the door of the car”).
To enable the generation of part-based questions, we introduce two new operations into the reasoning programs: <span id="S3.p2.1.1" class="ltx_text ltx_font_typewriter">part_to_object(<math id="S3.p2.1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S3.p2.1.1.m1.1a"><mo id="S3.p2.1.1.m1.1.1" xref="S3.p2.1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S3.p2.1.1.m1.1b"><ci id="S3.p2.1.1.m1.1.1.cmml" xref="S3.p2.1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.1.m1.1c">\cdot</annotation></semantics></math>)</span>, which find the objects containing the given part, and <span id="S3.p2.2.2" class="ltx_text ltx_font_typewriter">object_to_part(<math id="S3.p2.2.2.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S3.p2.2.2.m1.1a"><mo id="S3.p2.2.2.m1.1.1" xref="S3.p2.2.2.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S3.p2.2.2.m1.1b"><ci id="S3.p2.2.2.m1.1.1.cmml" xref="S3.p2.2.2.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.2.m1.1c">\cdot</annotation></semantics></math>)</span>, which select all the parts of the given object. We also modify some existing operations (<em id="S3.p2.2.8" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.p2.2.9" class="ltx_text"></span> <span id="S3.p2.2.10" class="ltx_text ltx_font_typewriter">filter</span>, <span id="S3.p2.2.11" class="ltx_text ltx_font_typewriter">query</span> and <span id="S3.p2.2.12" class="ltx_text ltx_font_typewriter">unique</span>), enabling them to operate on both object-level and part-level. With those reasoning operations, we collect 9 part-based templates and instantiate them with the image scene graph to generate questions.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.2" class="ltx_text ltx_font_bold">3D pose questions.</span>
Super-CLEVR-3D asks questions about the 3D poses of objects (<em id="S3.p3.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p3.1.4" class="ltx_text"></span> “which direction is the car facing in”), or the pair-wise pose relationships between objects (<em id="S3.p3.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p3.1.6" class="ltx_text"></span> “which object has vertical direction with the red car”). The pose for an individual object (<em id="S3.p3.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p3.1.8" class="ltx_text"></span> “facing left”) can be processed in a similar way as attributes like colors, so we extend the existing attribute-related operations like <span id="S3.p3.1.9" class="ltx_text ltx_font_typewriter">filter</span> and <span id="S3.p3.1.10" class="ltx_text ltx_font_typewriter">query</span> to have them include pose as well. For pair-wise pose relationship between objects, we add three operations, <em id="S3.p3.1.11" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.p3.1.12" class="ltx_text"></span> <span id="S3.p3.1.13" class="ltx_text ltx_font_typewriter">same_pose</span>, <span id="S3.p3.1.14" class="ltx_text ltx_font_typewriter">opposite_pose</span> and <span id="S3.p3.1.15" class="ltx_text ltx_font_typewriter">vertical_pose</span>, to deal with the three types of pose relationships between objects. For example, <span id="S3.p3.1.1" class="ltx_text ltx_font_typewriter">opposite_pose(<math id="S3.p3.1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S3.p3.1.1.m1.1a"><mo id="S3.p3.1.1.m1.1.1" xref="S3.p3.1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S3.p3.1.1.m1.1b"><ci id="S3.p3.1.1.m1.1.1.cmml" xref="S3.p3.1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.1.m1.1c">\cdot</annotation></semantics></math>)</span> returns the objects that are in the opposite pose direction with the given object. 17 templates are collected to generate 3D pose questions.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Occlusion questions.</span>
Occlusion questions ask about the occlusion between entities (<em id="S3.p4.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.p4.1.3" class="ltx_text"></span> objects or parts). Similar to 3D poses, occlusion can also be regarded as either an attributes for an entity (<em id="S3.p4.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p4.1.5" class="ltx_text"></span> “which object is occluded”), or as a relationship between entities (<em id="S3.p4.1.6" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p4.1.7" class="ltx_text"></span> “which object occludes the car door“). We extend the attribute-related operations, and introduce new operations to handle the pair-wise occlusion relationships: <span id="S3.p4.1.8" class="ltx_text ltx_font_typewriter">filter_occludee</span> which filters the entities that are being occluded, <span id="S3.p4.1.9" class="ltx_text ltx_font_typewriter">relate_occluding</span> which finds the entities that are occluded by the given entity, and <span id="S3.p4.1.10" class="ltx_text ltx_font_typewriter">relate_occluded</span> which finds the entities that are occluding the given entity. Using these operations, 35 templates are collected to generate the occlusion questions.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2310.17914/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="433" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of our model PO3D-VQA. The image is parsed into 3D-aware scene representations (blue box) using our proposed scene parser based on the idea of render-and-compare (green box). The question is parsed into a program composed of reasoning operations (orange box). Then the operations are executed on the 3D-aware scene representations to predict the answer. </figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we introduce PO3D-VQA, which is a parse-then-execute modular model for 3D-aware VQA. The overview of our system is shown in <a href="#S4.F2" title="In 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. We first parse the image into a scene graph representation that is aware of 3D information like object parts, 3D poses and occlusion relations, then we parse the question into a reasoning program and execute the program on the derived scene representations in a probabilistic manner. In <a href="#S4.SS1" title="4.1 3D-aware scene representation ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>, we define the scene representation required; in <a href="#S4.SS2" title="4.2 Multi-class 6D Scene Parsing ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>, we describe how we parse the image into the scene representation based on a multi-class 6D pose estimation model with non-trivial extensions; in <a href="#S4.SS3" title="4.3 Program execution ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>, we describe how the question is executed on the derived scene representation to predict the answer.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>3D-aware scene representation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.9" class="ltx_p">Given an input image <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">I</annotation></semantics></math>, we parse it into a 3D-aware scene representation <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">R</annotation></semantics></math> that contains the <span id="S4.SS1.p1.9.1" class="ltx_text ltx_font_bold">objects</span> (<math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">O</annotation></semantics></math>) with attributes (<math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="A^{o}" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><msup id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">A</mi><mi id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml">o</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">superscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">𝐴</ci><ci id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">A^{o}</annotation></semantics></math>), the <span id="S4.SS1.p1.9.2" class="ltx_text ltx_font_bold">parts</span> (<math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">P</annotation></semantics></math>) with attributes (<math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="A^{p}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><msup id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml">A</mi><mi id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml">p</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">superscript</csymbol><ci id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2">𝐴</ci><ci id="S4.SS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">A^{p}</annotation></semantics></math>), the <span id="S4.SS1.p1.9.3" class="ltx_text ltx_font_bold">hierarchical relationships</span> between objects and parts (<math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><mi id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><ci id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">H</annotation></semantics></math>), and the <span id="S4.SS1.p1.9.4" class="ltx_text ltx_font_bold">occlusion relationships</span> between them (<math id="S4.SS1.p1.8.m8.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS1.p1.8.m8.1a"><mi id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><ci id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">S</annotation></semantics></math>). The attributes include the 3D poses and locations of objects or parts, as well as their colors, materials, and sizes.
The scene representation <math id="S4.SS1.p1.9.m9.6" class="ltx_Math" alttext="R=\{O,P,A^{o},A^{p},H,S\}" display="inline"><semantics id="S4.SS1.p1.9.m9.6a"><mrow id="S4.SS1.p1.9.m9.6.6" xref="S4.SS1.p1.9.m9.6.6.cmml"><mi id="S4.SS1.p1.9.m9.6.6.4" xref="S4.SS1.p1.9.m9.6.6.4.cmml">R</mi><mo id="S4.SS1.p1.9.m9.6.6.3" xref="S4.SS1.p1.9.m9.6.6.3.cmml">=</mo><mrow id="S4.SS1.p1.9.m9.6.6.2.2" xref="S4.SS1.p1.9.m9.6.6.2.3.cmml"><mo stretchy="false" id="S4.SS1.p1.9.m9.6.6.2.2.3" xref="S4.SS1.p1.9.m9.6.6.2.3.cmml">{</mo><mi id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml">O</mi><mo id="S4.SS1.p1.9.m9.6.6.2.2.4" xref="S4.SS1.p1.9.m9.6.6.2.3.cmml">,</mo><mi id="S4.SS1.p1.9.m9.2.2" xref="S4.SS1.p1.9.m9.2.2.cmml">P</mi><mo id="S4.SS1.p1.9.m9.6.6.2.2.5" xref="S4.SS1.p1.9.m9.6.6.2.3.cmml">,</mo><msup id="S4.SS1.p1.9.m9.5.5.1.1.1" xref="S4.SS1.p1.9.m9.5.5.1.1.1.cmml"><mi id="S4.SS1.p1.9.m9.5.5.1.1.1.2" xref="S4.SS1.p1.9.m9.5.5.1.1.1.2.cmml">A</mi><mi id="S4.SS1.p1.9.m9.5.5.1.1.1.3" xref="S4.SS1.p1.9.m9.5.5.1.1.1.3.cmml">o</mi></msup><mo id="S4.SS1.p1.9.m9.6.6.2.2.6" xref="S4.SS1.p1.9.m9.6.6.2.3.cmml">,</mo><msup id="S4.SS1.p1.9.m9.6.6.2.2.2" xref="S4.SS1.p1.9.m9.6.6.2.2.2.cmml"><mi id="S4.SS1.p1.9.m9.6.6.2.2.2.2" xref="S4.SS1.p1.9.m9.6.6.2.2.2.2.cmml">A</mi><mi id="S4.SS1.p1.9.m9.6.6.2.2.2.3" xref="S4.SS1.p1.9.m9.6.6.2.2.2.3.cmml">p</mi></msup><mo id="S4.SS1.p1.9.m9.6.6.2.2.7" xref="S4.SS1.p1.9.m9.6.6.2.3.cmml">,</mo><mi id="S4.SS1.p1.9.m9.3.3" xref="S4.SS1.p1.9.m9.3.3.cmml">H</mi><mo id="S4.SS1.p1.9.m9.6.6.2.2.8" xref="S4.SS1.p1.9.m9.6.6.2.3.cmml">,</mo><mi id="S4.SS1.p1.9.m9.4.4" xref="S4.SS1.p1.9.m9.4.4.cmml">S</mi><mo stretchy="false" id="S4.SS1.p1.9.m9.6.6.2.2.9" xref="S4.SS1.p1.9.m9.6.6.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.6b"><apply id="S4.SS1.p1.9.m9.6.6.cmml" xref="S4.SS1.p1.9.m9.6.6"><eq id="S4.SS1.p1.9.m9.6.6.3.cmml" xref="S4.SS1.p1.9.m9.6.6.3"></eq><ci id="S4.SS1.p1.9.m9.6.6.4.cmml" xref="S4.SS1.p1.9.m9.6.6.4">𝑅</ci><set id="S4.SS1.p1.9.m9.6.6.2.3.cmml" xref="S4.SS1.p1.9.m9.6.6.2.2"><ci id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1">𝑂</ci><ci id="S4.SS1.p1.9.m9.2.2.cmml" xref="S4.SS1.p1.9.m9.2.2">𝑃</ci><apply id="S4.SS1.p1.9.m9.5.5.1.1.1.cmml" xref="S4.SS1.p1.9.m9.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.9.m9.5.5.1.1.1.1.cmml" xref="S4.SS1.p1.9.m9.5.5.1.1.1">superscript</csymbol><ci id="S4.SS1.p1.9.m9.5.5.1.1.1.2.cmml" xref="S4.SS1.p1.9.m9.5.5.1.1.1.2">𝐴</ci><ci id="S4.SS1.p1.9.m9.5.5.1.1.1.3.cmml" xref="S4.SS1.p1.9.m9.5.5.1.1.1.3">𝑜</ci></apply><apply id="S4.SS1.p1.9.m9.6.6.2.2.2.cmml" xref="S4.SS1.p1.9.m9.6.6.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.9.m9.6.6.2.2.2.1.cmml" xref="S4.SS1.p1.9.m9.6.6.2.2.2">superscript</csymbol><ci id="S4.SS1.p1.9.m9.6.6.2.2.2.2.cmml" xref="S4.SS1.p1.9.m9.6.6.2.2.2.2">𝐴</ci><ci id="S4.SS1.p1.9.m9.6.6.2.2.2.3.cmml" xref="S4.SS1.p1.9.m9.6.6.2.2.2.3">𝑝</ci></apply><ci id="S4.SS1.p1.9.m9.3.3.cmml" xref="S4.SS1.p1.9.m9.3.3">𝐻</ci><ci id="S4.SS1.p1.9.m9.4.4.cmml" xref="S4.SS1.p1.9.m9.4.4">𝑆</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.6c">R=\{O,P,A^{o},A^{p},H,S\}</annotation></semantics></math> is comprehensive and therefore we can directly execute the symbolic reasoning module on this representation without taking into account the image any further.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.18" class="ltx_p">In more detail, <span id="S4.SS1.p2.18.1" class="ltx_text ltx_font_bold">objects</span> are represented as a matrix <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="O\in\mathbb{R}^{n\times N_{obj}}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">O</mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.3.2" xref="S4.SS1.p2.1.m1.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.1.m1.1.1.3.3.1" xref="S4.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S4.SS1.p2.1.m1.1.1.3.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.3.3.2" xref="S4.SS1.p2.1.m1.1.1.3.3.3.2.cmml">N</mi><mrow id="S4.SS1.p2.1.m1.1.1.3.3.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.cmml"><mi id="S4.SS1.p2.1.m1.1.1.3.3.3.3.2" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.3.3.3.3.1" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.1.m1.1.1.3.3.3.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.3.3.3.3.1a" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.1.m1.1.1.3.3.3.3.4" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.4.cmml">j</mi></mrow></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><in id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></in><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝑂</ci><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"><times id="S4.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.1"></times><ci id="S4.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.2">𝑛</ci><apply id="S4.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.3.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.3.3.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3.2">𝑁</ci><apply id="S4.SS1.p2.1.m1.1.1.3.3.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3"><times id="S4.SS1.p2.1.m1.1.1.3.3.3.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.1"></times><ci id="S4.SS1.p2.1.m1.1.1.3.3.3.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.2">𝑜</ci><ci id="S4.SS1.p2.1.m1.1.1.3.3.3.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.3">𝑏</ci><ci id="S4.SS1.p2.1.m1.1.1.3.3.3.3.4.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3.3.3.4">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">O\in\mathbb{R}^{n\times N_{obj}}</annotation></semantics></math> containing the probability scores of each object being a certain instance, where <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">n</annotation></semantics></math> is the number of objects in the given image and <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="N_{obj}" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><msub id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">N</mi><mrow id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml"><mi id="S4.SS1.p2.3.m3.1.1.3.2" xref="S4.SS1.p2.3.m3.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.3.1" xref="S4.SS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p2.3.m3.1.1.3.3" xref="S4.SS1.p2.3.m3.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.3.1a" xref="S4.SS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p2.3.m3.1.1.3.4" xref="S4.SS1.p2.3.m3.1.1.3.4.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">𝑁</ci><apply id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3"><times id="S4.SS1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.3.1"></times><ci id="S4.SS1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.3.2">𝑜</ci><ci id="S4.SS1.p2.3.m3.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3.3">𝑏</ci><ci id="S4.SS1.p2.3.m3.1.1.3.4.cmml" xref="S4.SS1.p2.3.m3.1.1.3.4">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">N_{obj}</annotation></semantics></math> is the number of all possible object categories in the dataset (<em id="S4.SS1.p2.18.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p2.18.3" class="ltx_text"></span> vocabulary size of the objects). Similarly, <span id="S4.SS1.p2.18.4" class="ltx_text ltx_font_bold">parts</span> are represented as <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="P\in\mathbb{R}^{p\times N_{prt}}" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mi id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">P</mi><mo id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml"><mi id="S4.SS1.p2.4.m4.1.1.3.2" xref="S4.SS1.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.p2.4.m4.1.1.3.3" xref="S4.SS1.p2.4.m4.1.1.3.3.cmml"><mi id="S4.SS1.p2.4.m4.1.1.3.3.2" xref="S4.SS1.p2.4.m4.1.1.3.3.2.cmml">p</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.4.m4.1.1.3.3.1" xref="S4.SS1.p2.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S4.SS1.p2.4.m4.1.1.3.3.3" xref="S4.SS1.p2.4.m4.1.1.3.3.3.cmml"><mi id="S4.SS1.p2.4.m4.1.1.3.3.3.2" xref="S4.SS1.p2.4.m4.1.1.3.3.3.2.cmml">N</mi><mrow id="S4.SS1.p2.4.m4.1.1.3.3.3.3" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.cmml"><mi id="S4.SS1.p2.4.m4.1.1.3.3.3.3.2" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.4.m4.1.1.3.3.3.3.1" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.4.m4.1.1.3.3.3.3.3" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.4.m4.1.1.3.3.3.3.1a" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.4.m4.1.1.3.3.3.3.4" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.4.cmml">t</mi></mrow></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><in id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1"></in><ci id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">𝑃</ci><apply id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.3.1.cmml" xref="S4.SS1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.4.m4.1.1.3.2.cmml" xref="S4.SS1.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S4.SS1.p2.4.m4.1.1.3.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3"><times id="S4.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.1"></times><ci id="S4.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.2">𝑝</ci><apply id="S4.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.3.3.3.1.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S4.SS1.p2.4.m4.1.1.3.3.3.2.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.3.2">𝑁</ci><apply id="S4.SS1.p2.4.m4.1.1.3.3.3.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3"><times id="S4.SS1.p2.4.m4.1.1.3.3.3.3.1.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.1"></times><ci id="S4.SS1.p2.4.m4.1.1.3.3.3.3.2.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.2">𝑝</ci><ci id="S4.SS1.p2.4.m4.1.1.3.3.3.3.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.3">𝑟</ci><ci id="S4.SS1.p2.4.m4.1.1.3.3.3.3.4.cmml" xref="S4.SS1.p2.4.m4.1.1.3.3.3.3.4">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">P\in\mathbb{R}^{p\times N_{prt}}</annotation></semantics></math>, where <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mi id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><ci id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">p</annotation></semantics></math> is the number of parts in the image and <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="N_{prt}" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><msub id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml"><mi id="S4.SS1.p2.6.m6.1.1.2" xref="S4.SS1.p2.6.m6.1.1.2.cmml">N</mi><mrow id="S4.SS1.p2.6.m6.1.1.3" xref="S4.SS1.p2.6.m6.1.1.3.cmml"><mi id="S4.SS1.p2.6.m6.1.1.3.2" xref="S4.SS1.p2.6.m6.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.6.m6.1.1.3.1" xref="S4.SS1.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p2.6.m6.1.1.3.3" xref="S4.SS1.p2.6.m6.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.6.m6.1.1.3.1a" xref="S4.SS1.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p2.6.m6.1.1.3.4" xref="S4.SS1.p2.6.m6.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><apply id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.6.m6.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p2.6.m6.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.2">𝑁</ci><apply id="S4.SS1.p2.6.m6.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.3"><times id="S4.SS1.p2.6.m6.1.1.3.1.cmml" xref="S4.SS1.p2.6.m6.1.1.3.1"></times><ci id="S4.SS1.p2.6.m6.1.1.3.2.cmml" xref="S4.SS1.p2.6.m6.1.1.3.2">𝑝</ci><ci id="S4.SS1.p2.6.m6.1.1.3.3.cmml" xref="S4.SS1.p2.6.m6.1.1.3.3">𝑟</ci><ci id="S4.SS1.p2.6.m6.1.1.3.4.cmml" xref="S4.SS1.p2.6.m6.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">N_{prt}</annotation></semantics></math> is the vocabulary size of the object parts. The <span id="S4.SS1.p2.18.5" class="ltx_text ltx_font_bold">object-part hierarchy</span> is represented by a binary matrix <math id="S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="H\in\mathbb{R}^{n\times p}" display="inline"><semantics id="S4.SS1.p2.7.m7.1a"><mrow id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml"><mi id="S4.SS1.p2.7.m7.1.1.2" xref="S4.SS1.p2.7.m7.1.1.2.cmml">H</mi><mo id="S4.SS1.p2.7.m7.1.1.1" xref="S4.SS1.p2.7.m7.1.1.1.cmml">∈</mo><msup id="S4.SS1.p2.7.m7.1.1.3" xref="S4.SS1.p2.7.m7.1.1.3.cmml"><mi id="S4.SS1.p2.7.m7.1.1.3.2" xref="S4.SS1.p2.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.p2.7.m7.1.1.3.3" xref="S4.SS1.p2.7.m7.1.1.3.3.cmml"><mi id="S4.SS1.p2.7.m7.1.1.3.3.2" xref="S4.SS1.p2.7.m7.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.7.m7.1.1.3.3.1" xref="S4.SS1.p2.7.m7.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.p2.7.m7.1.1.3.3.3" xref="S4.SS1.p2.7.m7.1.1.3.3.3.cmml">p</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><apply id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"><in id="S4.SS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1.1"></in><ci id="S4.SS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.p2.7.m7.1.1.2">𝐻</ci><apply id="S4.SS1.p2.7.m7.1.1.3.cmml" xref="S4.SS1.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.7.m7.1.1.3.1.cmml" xref="S4.SS1.p2.7.m7.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.7.m7.1.1.3.2.cmml" xref="S4.SS1.p2.7.m7.1.1.3.2">ℝ</ci><apply id="S4.SS1.p2.7.m7.1.1.3.3.cmml" xref="S4.SS1.p2.7.m7.1.1.3.3"><times id="S4.SS1.p2.7.m7.1.1.3.3.1.cmml" xref="S4.SS1.p2.7.m7.1.1.3.3.1"></times><ci id="S4.SS1.p2.7.m7.1.1.3.3.2.cmml" xref="S4.SS1.p2.7.m7.1.1.3.3.2">𝑛</ci><ci id="S4.SS1.p2.7.m7.1.1.3.3.3.cmml" xref="S4.SS1.p2.7.m7.1.1.3.3.3">𝑝</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">H\in\mathbb{R}^{n\times p}</annotation></semantics></math>, where <math id="S4.SS1.p2.8.m8.1" class="ltx_Math" alttext="H_{ij}=1" display="inline"><semantics id="S4.SS1.p2.8.m8.1a"><mrow id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml"><msub id="S4.SS1.p2.8.m8.1.1.2" xref="S4.SS1.p2.8.m8.1.1.2.cmml"><mi id="S4.SS1.p2.8.m8.1.1.2.2" xref="S4.SS1.p2.8.m8.1.1.2.2.cmml">H</mi><mrow id="S4.SS1.p2.8.m8.1.1.2.3" xref="S4.SS1.p2.8.m8.1.1.2.3.cmml"><mi id="S4.SS1.p2.8.m8.1.1.2.3.2" xref="S4.SS1.p2.8.m8.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.8.m8.1.1.2.3.1" xref="S4.SS1.p2.8.m8.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.p2.8.m8.1.1.2.3.3" xref="S4.SS1.p2.8.m8.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S4.SS1.p2.8.m8.1.1.1" xref="S4.SS1.p2.8.m8.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.8.m8.1.1.3" xref="S4.SS1.p2.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><apply id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"><eq id="S4.SS1.p2.8.m8.1.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1.1"></eq><apply id="S4.SS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.8.m8.1.1.2.1.cmml" xref="S4.SS1.p2.8.m8.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.8.m8.1.1.2.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2.2">𝐻</ci><apply id="S4.SS1.p2.8.m8.1.1.2.3.cmml" xref="S4.SS1.p2.8.m8.1.1.2.3"><times id="S4.SS1.p2.8.m8.1.1.2.3.1.cmml" xref="S4.SS1.p2.8.m8.1.1.2.3.1"></times><ci id="S4.SS1.p2.8.m8.1.1.2.3.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2.3.2">𝑖</ci><ci id="S4.SS1.p2.8.m8.1.1.2.3.3.cmml" xref="S4.SS1.p2.8.m8.1.1.2.3.3">𝑗</ci></apply></apply><cn type="integer" id="S4.SS1.p2.8.m8.1.1.3.cmml" xref="S4.SS1.p2.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">H_{ij}=1</annotation></semantics></math> if the object <math id="S4.SS1.p2.9.m9.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.p2.9.m9.1a"><mi id="S4.SS1.p2.9.m9.1.1" xref="S4.SS1.p2.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.9.m9.1b"><ci id="S4.SS1.p2.9.m9.1.1.cmml" xref="S4.SS1.p2.9.m9.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.9.m9.1c">i</annotation></semantics></math> contains the part <math id="S4.SS1.p2.10.m10.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS1.p2.10.m10.1a"><mi id="S4.SS1.p2.10.m10.1.1" xref="S4.SS1.p2.10.m10.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.10.m10.1b"><ci id="S4.SS1.p2.10.m10.1.1.cmml" xref="S4.SS1.p2.10.m10.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.10.m10.1c">j</annotation></semantics></math> or <math id="S4.SS1.p2.11.m11.1" class="ltx_Math" alttext="H_{ij}=0" display="inline"><semantics id="S4.SS1.p2.11.m11.1a"><mrow id="S4.SS1.p2.11.m11.1.1" xref="S4.SS1.p2.11.m11.1.1.cmml"><msub id="S4.SS1.p2.11.m11.1.1.2" xref="S4.SS1.p2.11.m11.1.1.2.cmml"><mi id="S4.SS1.p2.11.m11.1.1.2.2" xref="S4.SS1.p2.11.m11.1.1.2.2.cmml">H</mi><mrow id="S4.SS1.p2.11.m11.1.1.2.3" xref="S4.SS1.p2.11.m11.1.1.2.3.cmml"><mi id="S4.SS1.p2.11.m11.1.1.2.3.2" xref="S4.SS1.p2.11.m11.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.11.m11.1.1.2.3.1" xref="S4.SS1.p2.11.m11.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.p2.11.m11.1.1.2.3.3" xref="S4.SS1.p2.11.m11.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S4.SS1.p2.11.m11.1.1.1" xref="S4.SS1.p2.11.m11.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.11.m11.1.1.3" xref="S4.SS1.p2.11.m11.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.11.m11.1b"><apply id="S4.SS1.p2.11.m11.1.1.cmml" xref="S4.SS1.p2.11.m11.1.1"><eq id="S4.SS1.p2.11.m11.1.1.1.cmml" xref="S4.SS1.p2.11.m11.1.1.1"></eq><apply id="S4.SS1.p2.11.m11.1.1.2.cmml" xref="S4.SS1.p2.11.m11.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.11.m11.1.1.2.1.cmml" xref="S4.SS1.p2.11.m11.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.11.m11.1.1.2.2.cmml" xref="S4.SS1.p2.11.m11.1.1.2.2">𝐻</ci><apply id="S4.SS1.p2.11.m11.1.1.2.3.cmml" xref="S4.SS1.p2.11.m11.1.1.2.3"><times id="S4.SS1.p2.11.m11.1.1.2.3.1.cmml" xref="S4.SS1.p2.11.m11.1.1.2.3.1"></times><ci id="S4.SS1.p2.11.m11.1.1.2.3.2.cmml" xref="S4.SS1.p2.11.m11.1.1.2.3.2">𝑖</ci><ci id="S4.SS1.p2.11.m11.1.1.2.3.3.cmml" xref="S4.SS1.p2.11.m11.1.1.2.3.3">𝑗</ci></apply></apply><cn type="integer" id="S4.SS1.p2.11.m11.1.1.3.cmml" xref="S4.SS1.p2.11.m11.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.11.m11.1c">H_{ij}=0</annotation></semantics></math> otherwise. The attributes <math id="S4.SS1.p2.12.m12.1" class="ltx_Math" alttext="A^{o}\in\mathbb{R}^{n\times N_{att}}" display="inline"><semantics id="S4.SS1.p2.12.m12.1a"><mrow id="S4.SS1.p2.12.m12.1.1" xref="S4.SS1.p2.12.m12.1.1.cmml"><msup id="S4.SS1.p2.12.m12.1.1.2" xref="S4.SS1.p2.12.m12.1.1.2.cmml"><mi id="S4.SS1.p2.12.m12.1.1.2.2" xref="S4.SS1.p2.12.m12.1.1.2.2.cmml">A</mi><mi id="S4.SS1.p2.12.m12.1.1.2.3" xref="S4.SS1.p2.12.m12.1.1.2.3.cmml">o</mi></msup><mo id="S4.SS1.p2.12.m12.1.1.1" xref="S4.SS1.p2.12.m12.1.1.1.cmml">∈</mo><msup id="S4.SS1.p2.12.m12.1.1.3" xref="S4.SS1.p2.12.m12.1.1.3.cmml"><mi id="S4.SS1.p2.12.m12.1.1.3.2" xref="S4.SS1.p2.12.m12.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.p2.12.m12.1.1.3.3" xref="S4.SS1.p2.12.m12.1.1.3.3.cmml"><mi id="S4.SS1.p2.12.m12.1.1.3.3.2" xref="S4.SS1.p2.12.m12.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.12.m12.1.1.3.3.1" xref="S4.SS1.p2.12.m12.1.1.3.3.1.cmml">×</mo><msub id="S4.SS1.p2.12.m12.1.1.3.3.3" xref="S4.SS1.p2.12.m12.1.1.3.3.3.cmml"><mi id="S4.SS1.p2.12.m12.1.1.3.3.3.2" xref="S4.SS1.p2.12.m12.1.1.3.3.3.2.cmml">N</mi><mrow id="S4.SS1.p2.12.m12.1.1.3.3.3.3" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.cmml"><mi id="S4.SS1.p2.12.m12.1.1.3.3.3.3.2" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.12.m12.1.1.3.3.3.3.1" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.12.m12.1.1.3.3.3.3.3" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.12.m12.1.1.3.3.3.3.1a" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.12.m12.1.1.3.3.3.3.4" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.4.cmml">t</mi></mrow></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.12.m12.1b"><apply id="S4.SS1.p2.12.m12.1.1.cmml" xref="S4.SS1.p2.12.m12.1.1"><in id="S4.SS1.p2.12.m12.1.1.1.cmml" xref="S4.SS1.p2.12.m12.1.1.1"></in><apply id="S4.SS1.p2.12.m12.1.1.2.cmml" xref="S4.SS1.p2.12.m12.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.12.m12.1.1.2.1.cmml" xref="S4.SS1.p2.12.m12.1.1.2">superscript</csymbol><ci id="S4.SS1.p2.12.m12.1.1.2.2.cmml" xref="S4.SS1.p2.12.m12.1.1.2.2">𝐴</ci><ci id="S4.SS1.p2.12.m12.1.1.2.3.cmml" xref="S4.SS1.p2.12.m12.1.1.2.3">𝑜</ci></apply><apply id="S4.SS1.p2.12.m12.1.1.3.cmml" xref="S4.SS1.p2.12.m12.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.12.m12.1.1.3.1.cmml" xref="S4.SS1.p2.12.m12.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.12.m12.1.1.3.2.cmml" xref="S4.SS1.p2.12.m12.1.1.3.2">ℝ</ci><apply id="S4.SS1.p2.12.m12.1.1.3.3.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3"><times id="S4.SS1.p2.12.m12.1.1.3.3.1.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.1"></times><ci id="S4.SS1.p2.12.m12.1.1.3.3.2.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.2">𝑛</ci><apply id="S4.SS1.p2.12.m12.1.1.3.3.3.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.12.m12.1.1.3.3.3.1.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3">subscript</csymbol><ci id="S4.SS1.p2.12.m12.1.1.3.3.3.2.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3.2">𝑁</ci><apply id="S4.SS1.p2.12.m12.1.1.3.3.3.3.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3"><times id="S4.SS1.p2.12.m12.1.1.3.3.3.3.1.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.1"></times><ci id="S4.SS1.p2.12.m12.1.1.3.3.3.3.2.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.2">𝑎</ci><ci id="S4.SS1.p2.12.m12.1.1.3.3.3.3.3.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.3">𝑡</ci><ci id="S4.SS1.p2.12.m12.1.1.3.3.3.3.4.cmml" xref="S4.SS1.p2.12.m12.1.1.3.3.3.3.4">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.12.m12.1c">A^{o}\in\mathbb{R}^{n\times N_{att}}</annotation></semantics></math> and <math id="S4.SS1.p2.13.m13.1" class="ltx_Math" alttext="A^{p}\in\mathbb{R}^{p\times N_{att}}" display="inline"><semantics id="S4.SS1.p2.13.m13.1a"><mrow id="S4.SS1.p2.13.m13.1.1" xref="S4.SS1.p2.13.m13.1.1.cmml"><msup id="S4.SS1.p2.13.m13.1.1.2" xref="S4.SS1.p2.13.m13.1.1.2.cmml"><mi id="S4.SS1.p2.13.m13.1.1.2.2" xref="S4.SS1.p2.13.m13.1.1.2.2.cmml">A</mi><mi id="S4.SS1.p2.13.m13.1.1.2.3" xref="S4.SS1.p2.13.m13.1.1.2.3.cmml">p</mi></msup><mo id="S4.SS1.p2.13.m13.1.1.1" xref="S4.SS1.p2.13.m13.1.1.1.cmml">∈</mo><msup id="S4.SS1.p2.13.m13.1.1.3" xref="S4.SS1.p2.13.m13.1.1.3.cmml"><mi id="S4.SS1.p2.13.m13.1.1.3.2" xref="S4.SS1.p2.13.m13.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.p2.13.m13.1.1.3.3" xref="S4.SS1.p2.13.m13.1.1.3.3.cmml"><mi id="S4.SS1.p2.13.m13.1.1.3.3.2" xref="S4.SS1.p2.13.m13.1.1.3.3.2.cmml">p</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.13.m13.1.1.3.3.1" xref="S4.SS1.p2.13.m13.1.1.3.3.1.cmml">×</mo><msub id="S4.SS1.p2.13.m13.1.1.3.3.3" xref="S4.SS1.p2.13.m13.1.1.3.3.3.cmml"><mi id="S4.SS1.p2.13.m13.1.1.3.3.3.2" xref="S4.SS1.p2.13.m13.1.1.3.3.3.2.cmml">N</mi><mrow id="S4.SS1.p2.13.m13.1.1.3.3.3.3" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.cmml"><mi id="S4.SS1.p2.13.m13.1.1.3.3.3.3.2" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.13.m13.1.1.3.3.3.3.1" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.13.m13.1.1.3.3.3.3.3" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.13.m13.1.1.3.3.3.3.1a" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.13.m13.1.1.3.3.3.3.4" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.4.cmml">t</mi></mrow></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.13.m13.1b"><apply id="S4.SS1.p2.13.m13.1.1.cmml" xref="S4.SS1.p2.13.m13.1.1"><in id="S4.SS1.p2.13.m13.1.1.1.cmml" xref="S4.SS1.p2.13.m13.1.1.1"></in><apply id="S4.SS1.p2.13.m13.1.1.2.cmml" xref="S4.SS1.p2.13.m13.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.13.m13.1.1.2.1.cmml" xref="S4.SS1.p2.13.m13.1.1.2">superscript</csymbol><ci id="S4.SS1.p2.13.m13.1.1.2.2.cmml" xref="S4.SS1.p2.13.m13.1.1.2.2">𝐴</ci><ci id="S4.SS1.p2.13.m13.1.1.2.3.cmml" xref="S4.SS1.p2.13.m13.1.1.2.3">𝑝</ci></apply><apply id="S4.SS1.p2.13.m13.1.1.3.cmml" xref="S4.SS1.p2.13.m13.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.13.m13.1.1.3.1.cmml" xref="S4.SS1.p2.13.m13.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.13.m13.1.1.3.2.cmml" xref="S4.SS1.p2.13.m13.1.1.3.2">ℝ</ci><apply id="S4.SS1.p2.13.m13.1.1.3.3.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3"><times id="S4.SS1.p2.13.m13.1.1.3.3.1.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.1"></times><ci id="S4.SS1.p2.13.m13.1.1.3.3.2.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.2">𝑝</ci><apply id="S4.SS1.p2.13.m13.1.1.3.3.3.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.13.m13.1.1.3.3.3.1.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.3">subscript</csymbol><ci id="S4.SS1.p2.13.m13.1.1.3.3.3.2.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.3.2">𝑁</ci><apply id="S4.SS1.p2.13.m13.1.1.3.3.3.3.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3"><times id="S4.SS1.p2.13.m13.1.1.3.3.3.3.1.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.1"></times><ci id="S4.SS1.p2.13.m13.1.1.3.3.3.3.2.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.2">𝑎</ci><ci id="S4.SS1.p2.13.m13.1.1.3.3.3.3.3.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.3">𝑡</ci><ci id="S4.SS1.p2.13.m13.1.1.3.3.3.3.4.cmml" xref="S4.SS1.p2.13.m13.1.1.3.3.3.3.4">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.13.m13.1c">A^{p}\in\mathbb{R}^{p\times N_{att}}</annotation></semantics></math> containing probability scores of each object or part having a certain attribute or the value of bounding box. Here <math id="S4.SS1.p2.14.m14.1" class="ltx_Math" alttext="N_{att}" display="inline"><semantics id="S4.SS1.p2.14.m14.1a"><msub id="S4.SS1.p2.14.m14.1.1" xref="S4.SS1.p2.14.m14.1.1.cmml"><mi id="S4.SS1.p2.14.m14.1.1.2" xref="S4.SS1.p2.14.m14.1.1.2.cmml">N</mi><mrow id="S4.SS1.p2.14.m14.1.1.3" xref="S4.SS1.p2.14.m14.1.1.3.cmml"><mi id="S4.SS1.p2.14.m14.1.1.3.2" xref="S4.SS1.p2.14.m14.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.14.m14.1.1.3.1" xref="S4.SS1.p2.14.m14.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p2.14.m14.1.1.3.3" xref="S4.SS1.p2.14.m14.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.14.m14.1.1.3.1a" xref="S4.SS1.p2.14.m14.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p2.14.m14.1.1.3.4" xref="S4.SS1.p2.14.m14.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.14.m14.1b"><apply id="S4.SS1.p2.14.m14.1.1.cmml" xref="S4.SS1.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.14.m14.1.1.1.cmml" xref="S4.SS1.p2.14.m14.1.1">subscript</csymbol><ci id="S4.SS1.p2.14.m14.1.1.2.cmml" xref="S4.SS1.p2.14.m14.1.1.2">𝑁</ci><apply id="S4.SS1.p2.14.m14.1.1.3.cmml" xref="S4.SS1.p2.14.m14.1.1.3"><times id="S4.SS1.p2.14.m14.1.1.3.1.cmml" xref="S4.SS1.p2.14.m14.1.1.3.1"></times><ci id="S4.SS1.p2.14.m14.1.1.3.2.cmml" xref="S4.SS1.p2.14.m14.1.1.3.2">𝑎</ci><ci id="S4.SS1.p2.14.m14.1.1.3.3.cmml" xref="S4.SS1.p2.14.m14.1.1.3.3">𝑡</ci><ci id="S4.SS1.p2.14.m14.1.1.3.4.cmml" xref="S4.SS1.p2.14.m14.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.14.m14.1c">N_{att}</annotation></semantics></math> is the number of attributes including the 3D poses, location coordinates, colors, materials and sizes. <span id="S4.SS1.p2.18.6" class="ltx_text ltx_font_bold">Occlusion relationships</span> are represented by <math id="S4.SS1.p2.15.m15.1" class="ltx_Math" alttext="S\in\mathbb{R}^{(n+p)\times n}" display="inline"><semantics id="S4.SS1.p2.15.m15.1a"><mrow id="S4.SS1.p2.15.m15.1.2" xref="S4.SS1.p2.15.m15.1.2.cmml"><mi id="S4.SS1.p2.15.m15.1.2.2" xref="S4.SS1.p2.15.m15.1.2.2.cmml">S</mi><mo id="S4.SS1.p2.15.m15.1.2.1" xref="S4.SS1.p2.15.m15.1.2.1.cmml">∈</mo><msup id="S4.SS1.p2.15.m15.1.2.3" xref="S4.SS1.p2.15.m15.1.2.3.cmml"><mi id="S4.SS1.p2.15.m15.1.2.3.2" xref="S4.SS1.p2.15.m15.1.2.3.2.cmml">ℝ</mi><mrow id="S4.SS1.p2.15.m15.1.1.1" xref="S4.SS1.p2.15.m15.1.1.1.cmml"><mrow id="S4.SS1.p2.15.m15.1.1.1.1.1" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p2.15.m15.1.1.1.1.1.2" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p2.15.m15.1.1.1.1.1.1" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.15.m15.1.1.1.1.1.1.2" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.2.cmml">n</mi><mo id="S4.SS1.p2.15.m15.1.1.1.1.1.1.1" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.1.cmml">+</mo><mi id="S4.SS1.p2.15.m15.1.1.1.1.1.1.3" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.3.cmml">p</mi></mrow><mo rspace="0.055em" stretchy="false" id="S4.SS1.p2.15.m15.1.1.1.1.1.3" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S4.SS1.p2.15.m15.1.1.1.2" xref="S4.SS1.p2.15.m15.1.1.1.2.cmml">×</mo><mi id="S4.SS1.p2.15.m15.1.1.1.3" xref="S4.SS1.p2.15.m15.1.1.1.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.15.m15.1b"><apply id="S4.SS1.p2.15.m15.1.2.cmml" xref="S4.SS1.p2.15.m15.1.2"><in id="S4.SS1.p2.15.m15.1.2.1.cmml" xref="S4.SS1.p2.15.m15.1.2.1"></in><ci id="S4.SS1.p2.15.m15.1.2.2.cmml" xref="S4.SS1.p2.15.m15.1.2.2">𝑆</ci><apply id="S4.SS1.p2.15.m15.1.2.3.cmml" xref="S4.SS1.p2.15.m15.1.2.3"><csymbol cd="ambiguous" id="S4.SS1.p2.15.m15.1.2.3.1.cmml" xref="S4.SS1.p2.15.m15.1.2.3">superscript</csymbol><ci id="S4.SS1.p2.15.m15.1.2.3.2.cmml" xref="S4.SS1.p2.15.m15.1.2.3.2">ℝ</ci><apply id="S4.SS1.p2.15.m15.1.1.1.cmml" xref="S4.SS1.p2.15.m15.1.1.1"><times id="S4.SS1.p2.15.m15.1.1.1.2.cmml" xref="S4.SS1.p2.15.m15.1.1.1.2"></times><apply id="S4.SS1.p2.15.m15.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.15.m15.1.1.1.1.1"><plus id="S4.SS1.p2.15.m15.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.1"></plus><ci id="S4.SS1.p2.15.m15.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.2">𝑛</ci><ci id="S4.SS1.p2.15.m15.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.15.m15.1.1.1.1.1.1.3">𝑝</ci></apply><ci id="S4.SS1.p2.15.m15.1.1.1.3.cmml" xref="S4.SS1.p2.15.m15.1.1.1.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.15.m15.1c">S\in\mathbb{R}^{(n+p)\times n}</annotation></semantics></math>, where each element <math id="S4.SS1.p2.16.m16.1" class="ltx_Math" alttext="S_{ij}" display="inline"><semantics id="S4.SS1.p2.16.m16.1a"><msub id="S4.SS1.p2.16.m16.1.1" xref="S4.SS1.p2.16.m16.1.1.cmml"><mi id="S4.SS1.p2.16.m16.1.1.2" xref="S4.SS1.p2.16.m16.1.1.2.cmml">S</mi><mrow id="S4.SS1.p2.16.m16.1.1.3" xref="S4.SS1.p2.16.m16.1.1.3.cmml"><mi id="S4.SS1.p2.16.m16.1.1.3.2" xref="S4.SS1.p2.16.m16.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.16.m16.1.1.3.1" xref="S4.SS1.p2.16.m16.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p2.16.m16.1.1.3.3" xref="S4.SS1.p2.16.m16.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.16.m16.1b"><apply id="S4.SS1.p2.16.m16.1.1.cmml" xref="S4.SS1.p2.16.m16.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.16.m16.1.1.1.cmml" xref="S4.SS1.p2.16.m16.1.1">subscript</csymbol><ci id="S4.SS1.p2.16.m16.1.1.2.cmml" xref="S4.SS1.p2.16.m16.1.1.2">𝑆</ci><apply id="S4.SS1.p2.16.m16.1.1.3.cmml" xref="S4.SS1.p2.16.m16.1.1.3"><times id="S4.SS1.p2.16.m16.1.1.3.1.cmml" xref="S4.SS1.p2.16.m16.1.1.3.1"></times><ci id="S4.SS1.p2.16.m16.1.1.3.2.cmml" xref="S4.SS1.p2.16.m16.1.1.3.2">𝑖</ci><ci id="S4.SS1.p2.16.m16.1.1.3.3.cmml" xref="S4.SS1.p2.16.m16.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.16.m16.1c">S_{ij}</annotation></semantics></math> represents the score of object (or part) <math id="S4.SS1.p2.17.m17.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.p2.17.m17.1a"><mi id="S4.SS1.p2.17.m17.1.1" xref="S4.SS1.p2.17.m17.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.17.m17.1b"><ci id="S4.SS1.p2.17.m17.1.1.cmml" xref="S4.SS1.p2.17.m17.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.17.m17.1c">i</annotation></semantics></math> being occluded by object <math id="S4.SS1.p2.18.m18.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS1.p2.18.m18.1a"><mi id="S4.SS1.p2.18.m18.1.1" xref="S4.SS1.p2.18.m18.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.18.m18.1b"><ci id="S4.SS1.p2.18.m18.1.1.cmml" xref="S4.SS1.p2.18.m18.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.18.m18.1c">j</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multi-class 6D Scene Parsing</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">While most existing VQA methods <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">anderson2018bottom, </a>; <a href="#bib.bib59" title="" class="ltx_ref">zhang2021vinvl, </a>)</cite> encode the image using pretrained object detectors like Faster-RCNN <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib46" title="" class="ltx_ref">ren2015faster, </a>)</cite>, we build our 6D-aware scene parser in a different way, based on the idea of analysis-by-synthesis through inverse rendering <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib49" title="" class="ltx_ref">wang2021nemo, </a>)</cite> which has the following advantages:
first, the model prediction is more robust <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib49" title="" class="ltx_ref">wang2021nemo, </a>)</cite> as the render-and-compare process can naturally integrate a robust reconstruction loss to avoid distortion through occlusion;
second, while the object parts are usually very challenging for Faster-RCNN to detect due to their small size, they can be much easier located using the 3D object shape, by first finding the object and estimating its 3D pose, and subsequently locating the parts using the 3D object shape (as shown in our experimental evaluation).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">However, we observe two open challenges for applying existing 6D pose estimators that follow a render-and-compare approach <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib38" title="" class="ltx_ref">ma2022robust, </a>; <a href="#bib.bib49" title="" class="ltx_ref">wang2021nemo, </a>)</cite>: (a) these pose estimators assume that the object class is known, but in Super-CLEVR-3D the scene parser must learn to estimate the object class jointly with the pose; and (b) the scenes in Super-CLEVR-3D are very dense, containing multiple close-by objects that occlude each other. In order to address these two challenges, we introduce several improvements over <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib38" title="" class="ltx_ref">ma2022robust, </a>)</cite> that enable it to be integrated into a 3D-aware VQA model.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">In the following, we first describe neural meshes <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib49" title="" class="ltx_ref">wang2021nemo, </a>; <a href="#bib.bib38" title="" class="ltx_ref">ma2022robust, </a>)</cite>, which were proposed in prior work for pose estimation of <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">single objects</span> following an analysis-by-synthesis approach. Subsequently, we extend this method to complex scenes with densely located and possibly occluded objects to obtain a coherent scene representation, including object parts and attributes.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2310.17914/assets/medias/fig3-2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of intermediate steps in our scene parser. Given an image (a), per-category feature activation maps (shown in II) are computed through render-and-compare. Then the category-wise competition (3D-NMS) is performed (results shown in b) and a post-filtering step is taken to remove mis-detected objects (c). Based on the pose estimation results (d), we project the 3D object mesh back onto the image to locate parts and occlusions(e).</figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.8" class="ltx_p"><span id="S4.SS2.p4.8.1" class="ltx_text ltx_font_bold">Preliminaries.</span>
Our work builds on and significantly extends Neural Meshes <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib38" title="" class="ltx_ref">ma2022robust, </a>)</cite> that were introduced for 6D pose estimation through inverse rendering.
The task is to jointly estimate the 6D pose (2D location, distance to the camera and 3D pose) of objects in an image. An object category is represented with a category-level mesh <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib49" title="" class="ltx_ref">wang2021nemo, </a>)</cite> <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="M_{y}=\{v_{n}\in\mathbb{R}^{3}\}_{n=1}^{N}" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><msub id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml"><mi id="S4.SS2.p4.1.m1.1.1.3.2" xref="S4.SS2.p4.1.m1.1.1.3.2.cmml">M</mi><mi id="S4.SS2.p4.1.m1.1.1.3.3" xref="S4.SS2.p4.1.m1.1.1.3.3.cmml">y</mi></msub><mo id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">=</mo><msubsup id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml"><mrow id="S4.SS2.p4.1.m1.1.1.1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.1.1.1.1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.1.1.1.2.cmml">{</mo><mrow id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.cmml"><msub id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.2" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.2.cmml">v</mi><mi id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.3" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.3.cmml">n</mi></msub><mo id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.1.cmml">∈</mo><msup id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.2" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.2.cmml">ℝ</mi><mn id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.3" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.3.cmml">3</mn></msup></mrow><mo stretchy="false" id="S4.SS2.p4.1.m1.1.1.1.1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS2.p4.1.m1.1.1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.1.1.3.cmml"><mi id="S4.SS2.p4.1.m1.1.1.1.1.3.2" xref="S4.SS2.p4.1.m1.1.1.1.1.3.2.cmml">n</mi><mo id="S4.SS2.p4.1.m1.1.1.1.1.3.1" xref="S4.SS2.p4.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p4.1.m1.1.1.1.1.3.3" xref="S4.SS2.p4.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p4.1.m1.1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><eq id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2"></eq><apply id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.3.1.cmml" xref="S4.SS2.p4.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS2.p4.1.m1.1.1.3.2.cmml" xref="S4.SS2.p4.1.m1.1.1.3.2">𝑀</ci><ci id="S4.SS2.p4.1.m1.1.1.3.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3.3">𝑦</ci></apply><apply id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1">superscript</csymbol><apply id="S4.SS2.p4.1.m1.1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1">subscript</csymbol><set id="S4.SS2.p4.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1"><apply id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1"><in id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.1"></in><apply id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.2">𝑣</ci><ci id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.2">ℝ</ci><cn type="integer" id="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.1.1.1.3.3">3</cn></apply></apply></set><apply id="S4.SS2.p4.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.3"><eq id="S4.SS2.p4.1.m1.1.1.1.1.3.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.3.1"></eq><ci id="S4.SS2.p4.1.m1.1.1.1.1.3.2.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.3.2">𝑛</ci><cn type="integer" id="S4.SS2.p4.1.m1.1.1.1.1.3.3.cmml" xref="S4.SS2.p4.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p4.1.m1.1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.1.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">M_{y}=\{v_{n}\in\mathbb{R}^{3}\}_{n=1}^{N}</annotation></semantics></math> and a neural texture <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="T_{y}\in\mathbb{R}^{N\times c}" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><msub id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml"><mi id="S4.SS2.p4.2.m2.1.1.2.2" xref="S4.SS2.p4.2.m2.1.1.2.2.cmml">T</mi><mi id="S4.SS2.p4.2.m2.1.1.2.3" xref="S4.SS2.p4.2.m2.1.1.2.3.cmml">y</mi></msub><mo id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">∈</mo><msup id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml"><mi id="S4.SS2.p4.2.m2.1.1.3.2" xref="S4.SS2.p4.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS2.p4.2.m2.1.1.3.3" xref="S4.SS2.p4.2.m2.1.1.3.3.cmml"><mi id="S4.SS2.p4.2.m2.1.1.3.3.2" xref="S4.SS2.p4.2.m2.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p4.2.m2.1.1.3.3.1" xref="S4.SS2.p4.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S4.SS2.p4.2.m2.1.1.3.3.3" xref="S4.SS2.p4.2.m2.1.1.3.3.3.cmml">c</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><in id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1"></in><apply id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p4.2.m2.1.1.2.1.cmml" xref="S4.SS2.p4.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS2.p4.2.m2.1.1.2.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2.2">𝑇</ci><ci id="S4.SS2.p4.2.m2.1.1.2.3.cmml" xref="S4.SS2.p4.2.m2.1.1.2.3">𝑦</ci></apply><apply id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p4.2.m2.1.1.3.1.cmml" xref="S4.SS2.p4.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS2.p4.2.m2.1.1.3.2.cmml" xref="S4.SS2.p4.2.m2.1.1.3.2">ℝ</ci><apply id="S4.SS2.p4.2.m2.1.1.3.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3"><times id="S4.SS2.p4.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3.1"></times><ci id="S4.SS2.p4.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3.2">𝑁</ci><ci id="S4.SS2.p4.2.m2.1.1.3.3.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">T_{y}\in\mathbb{R}^{N\times c}</annotation></semantics></math> on the surface of the mesh <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="M_{y}" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><msub id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml"><mi id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml">M</mi><mi id="S4.SS2.p4.3.m3.1.1.3" xref="S4.SS2.p4.3.m3.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p4.3.m3.1.1.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2">𝑀</ci><ci id="S4.SS2.p4.3.m3.1.1.3.cmml" xref="S4.SS2.p4.3.m3.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">M_{y}</annotation></semantics></math>, where <math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mi id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><ci id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">c</annotation></semantics></math> is the dimension of the feature and <math id="S4.SS2.p4.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p4.5.m5.1a"><mi id="S4.SS2.p4.5.m5.1.1" xref="S4.SS2.p4.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.5.m5.1b"><ci id="S4.SS2.p4.5.m5.1.1.cmml" xref="S4.SS2.p4.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.5.m5.1c">y</annotation></semantics></math> is the object category.
Given the object 3D pose in camera view <math id="S4.SS2.p4.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.p4.6.m6.1a"><mi id="S4.SS2.p4.6.m6.1.1" xref="S4.SS2.p4.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.6.m6.1b"><ci id="S4.SS2.p4.6.m6.1.1.cmml" xref="S4.SS2.p4.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.6.m6.1c">\alpha</annotation></semantics></math>, we can render the neural mesh model <math id="S4.SS2.p4.7.m7.2" class="ltx_Math" alttext="O_{y}=\{M_{y},T_{y}\}" display="inline"><semantics id="S4.SS2.p4.7.m7.2a"><mrow id="S4.SS2.p4.7.m7.2.2" xref="S4.SS2.p4.7.m7.2.2.cmml"><msub id="S4.SS2.p4.7.m7.2.2.4" xref="S4.SS2.p4.7.m7.2.2.4.cmml"><mi id="S4.SS2.p4.7.m7.2.2.4.2" xref="S4.SS2.p4.7.m7.2.2.4.2.cmml">O</mi><mi id="S4.SS2.p4.7.m7.2.2.4.3" xref="S4.SS2.p4.7.m7.2.2.4.3.cmml">y</mi></msub><mo id="S4.SS2.p4.7.m7.2.2.3" xref="S4.SS2.p4.7.m7.2.2.3.cmml">=</mo><mrow id="S4.SS2.p4.7.m7.2.2.2.2" xref="S4.SS2.p4.7.m7.2.2.2.3.cmml"><mo stretchy="false" id="S4.SS2.p4.7.m7.2.2.2.2.3" xref="S4.SS2.p4.7.m7.2.2.2.3.cmml">{</mo><msub id="S4.SS2.p4.7.m7.1.1.1.1.1" xref="S4.SS2.p4.7.m7.1.1.1.1.1.cmml"><mi id="S4.SS2.p4.7.m7.1.1.1.1.1.2" xref="S4.SS2.p4.7.m7.1.1.1.1.1.2.cmml">M</mi><mi id="S4.SS2.p4.7.m7.1.1.1.1.1.3" xref="S4.SS2.p4.7.m7.1.1.1.1.1.3.cmml">y</mi></msub><mo id="S4.SS2.p4.7.m7.2.2.2.2.4" xref="S4.SS2.p4.7.m7.2.2.2.3.cmml">,</mo><msub id="S4.SS2.p4.7.m7.2.2.2.2.2" xref="S4.SS2.p4.7.m7.2.2.2.2.2.cmml"><mi id="S4.SS2.p4.7.m7.2.2.2.2.2.2" xref="S4.SS2.p4.7.m7.2.2.2.2.2.2.cmml">T</mi><mi id="S4.SS2.p4.7.m7.2.2.2.2.2.3" xref="S4.SS2.p4.7.m7.2.2.2.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="S4.SS2.p4.7.m7.2.2.2.2.5" xref="S4.SS2.p4.7.m7.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.7.m7.2b"><apply id="S4.SS2.p4.7.m7.2.2.cmml" xref="S4.SS2.p4.7.m7.2.2"><eq id="S4.SS2.p4.7.m7.2.2.3.cmml" xref="S4.SS2.p4.7.m7.2.2.3"></eq><apply id="S4.SS2.p4.7.m7.2.2.4.cmml" xref="S4.SS2.p4.7.m7.2.2.4"><csymbol cd="ambiguous" id="S4.SS2.p4.7.m7.2.2.4.1.cmml" xref="S4.SS2.p4.7.m7.2.2.4">subscript</csymbol><ci id="S4.SS2.p4.7.m7.2.2.4.2.cmml" xref="S4.SS2.p4.7.m7.2.2.4.2">𝑂</ci><ci id="S4.SS2.p4.7.m7.2.2.4.3.cmml" xref="S4.SS2.p4.7.m7.2.2.4.3">𝑦</ci></apply><set id="S4.SS2.p4.7.m7.2.2.2.3.cmml" xref="S4.SS2.p4.7.m7.2.2.2.2"><apply id="S4.SS2.p4.7.m7.1.1.1.1.1.cmml" xref="S4.SS2.p4.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.7.m7.1.1.1.1.1.1.cmml" xref="S4.SS2.p4.7.m7.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p4.7.m7.1.1.1.1.1.2.cmml" xref="S4.SS2.p4.7.m7.1.1.1.1.1.2">𝑀</ci><ci id="S4.SS2.p4.7.m7.1.1.1.1.1.3.cmml" xref="S4.SS2.p4.7.m7.1.1.1.1.1.3">𝑦</ci></apply><apply id="S4.SS2.p4.7.m7.2.2.2.2.2.cmml" xref="S4.SS2.p4.7.m7.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p4.7.m7.2.2.2.2.2.1.cmml" xref="S4.SS2.p4.7.m7.2.2.2.2.2">subscript</csymbol><ci id="S4.SS2.p4.7.m7.2.2.2.2.2.2.cmml" xref="S4.SS2.p4.7.m7.2.2.2.2.2.2">𝑇</ci><ci id="S4.SS2.p4.7.m7.2.2.2.2.2.3.cmml" xref="S4.SS2.p4.7.m7.2.2.2.2.2.3">𝑦</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.7.m7.2c">O_{y}=\{M_{y},T_{y}\}</annotation></semantics></math> into a feature map with soft rasterization <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib35" title="" class="ltx_ref">liu2019soft, </a>)</cite>: <math id="S4.SS2.p4.8.m8.3" class="ltx_Math" alttext="F_{y}(\alpha)=\mathfrak{R}(O_{y},\alpha)" display="inline"><semantics id="S4.SS2.p4.8.m8.3a"><mrow id="S4.SS2.p4.8.m8.3.3" xref="S4.SS2.p4.8.m8.3.3.cmml"><mrow id="S4.SS2.p4.8.m8.3.3.3" xref="S4.SS2.p4.8.m8.3.3.3.cmml"><msub id="S4.SS2.p4.8.m8.3.3.3.2" xref="S4.SS2.p4.8.m8.3.3.3.2.cmml"><mi id="S4.SS2.p4.8.m8.3.3.3.2.2" xref="S4.SS2.p4.8.m8.3.3.3.2.2.cmml">F</mi><mi id="S4.SS2.p4.8.m8.3.3.3.2.3" xref="S4.SS2.p4.8.m8.3.3.3.2.3.cmml">y</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p4.8.m8.3.3.3.1" xref="S4.SS2.p4.8.m8.3.3.3.1.cmml">​</mo><mrow id="S4.SS2.p4.8.m8.3.3.3.3.2" xref="S4.SS2.p4.8.m8.3.3.3.cmml"><mo stretchy="false" id="S4.SS2.p4.8.m8.3.3.3.3.2.1" xref="S4.SS2.p4.8.m8.3.3.3.cmml">(</mo><mi id="S4.SS2.p4.8.m8.1.1" xref="S4.SS2.p4.8.m8.1.1.cmml">α</mi><mo stretchy="false" id="S4.SS2.p4.8.m8.3.3.3.3.2.2" xref="S4.SS2.p4.8.m8.3.3.3.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p4.8.m8.3.3.2" xref="S4.SS2.p4.8.m8.3.3.2.cmml">=</mo><mrow id="S4.SS2.p4.8.m8.3.3.1" xref="S4.SS2.p4.8.m8.3.3.1.cmml"><mi id="S4.SS2.p4.8.m8.3.3.1.3" xref="S4.SS2.p4.8.m8.3.3.1.3.cmml">ℜ</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.8.m8.3.3.1.2" xref="S4.SS2.p4.8.m8.3.3.1.2.cmml">​</mo><mrow id="S4.SS2.p4.8.m8.3.3.1.1.1" xref="S4.SS2.p4.8.m8.3.3.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p4.8.m8.3.3.1.1.1.2" xref="S4.SS2.p4.8.m8.3.3.1.1.2.cmml">(</mo><msub id="S4.SS2.p4.8.m8.3.3.1.1.1.1" xref="S4.SS2.p4.8.m8.3.3.1.1.1.1.cmml"><mi id="S4.SS2.p4.8.m8.3.3.1.1.1.1.2" xref="S4.SS2.p4.8.m8.3.3.1.1.1.1.2.cmml">O</mi><mi id="S4.SS2.p4.8.m8.3.3.1.1.1.1.3" xref="S4.SS2.p4.8.m8.3.3.1.1.1.1.3.cmml">y</mi></msub><mo id="S4.SS2.p4.8.m8.3.3.1.1.1.3" xref="S4.SS2.p4.8.m8.3.3.1.1.2.cmml">,</mo><mi id="S4.SS2.p4.8.m8.2.2" xref="S4.SS2.p4.8.m8.2.2.cmml">α</mi><mo stretchy="false" id="S4.SS2.p4.8.m8.3.3.1.1.1.4" xref="S4.SS2.p4.8.m8.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.8.m8.3b"><apply id="S4.SS2.p4.8.m8.3.3.cmml" xref="S4.SS2.p4.8.m8.3.3"><eq id="S4.SS2.p4.8.m8.3.3.2.cmml" xref="S4.SS2.p4.8.m8.3.3.2"></eq><apply id="S4.SS2.p4.8.m8.3.3.3.cmml" xref="S4.SS2.p4.8.m8.3.3.3"><times id="S4.SS2.p4.8.m8.3.3.3.1.cmml" xref="S4.SS2.p4.8.m8.3.3.3.1"></times><apply id="S4.SS2.p4.8.m8.3.3.3.2.cmml" xref="S4.SS2.p4.8.m8.3.3.3.2"><csymbol cd="ambiguous" id="S4.SS2.p4.8.m8.3.3.3.2.1.cmml" xref="S4.SS2.p4.8.m8.3.3.3.2">subscript</csymbol><ci id="S4.SS2.p4.8.m8.3.3.3.2.2.cmml" xref="S4.SS2.p4.8.m8.3.3.3.2.2">𝐹</ci><ci id="S4.SS2.p4.8.m8.3.3.3.2.3.cmml" xref="S4.SS2.p4.8.m8.3.3.3.2.3">𝑦</ci></apply><ci id="S4.SS2.p4.8.m8.1.1.cmml" xref="S4.SS2.p4.8.m8.1.1">𝛼</ci></apply><apply id="S4.SS2.p4.8.m8.3.3.1.cmml" xref="S4.SS2.p4.8.m8.3.3.1"><times id="S4.SS2.p4.8.m8.3.3.1.2.cmml" xref="S4.SS2.p4.8.m8.3.3.1.2"></times><ci id="S4.SS2.p4.8.m8.3.3.1.3.cmml" xref="S4.SS2.p4.8.m8.3.3.1.3">ℜ</ci><interval closure="open" id="S4.SS2.p4.8.m8.3.3.1.1.2.cmml" xref="S4.SS2.p4.8.m8.3.3.1.1.1"><apply id="S4.SS2.p4.8.m8.3.3.1.1.1.1.cmml" xref="S4.SS2.p4.8.m8.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.8.m8.3.3.1.1.1.1.1.cmml" xref="S4.SS2.p4.8.m8.3.3.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p4.8.m8.3.3.1.1.1.1.2.cmml" xref="S4.SS2.p4.8.m8.3.3.1.1.1.1.2">𝑂</ci><ci id="S4.SS2.p4.8.m8.3.3.1.1.1.1.3.cmml" xref="S4.SS2.p4.8.m8.3.3.1.1.1.1.3">𝑦</ci></apply><ci id="S4.SS2.p4.8.m8.2.2.cmml" xref="S4.SS2.p4.8.m8.2.2">𝛼</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.8.m8.3c">F_{y}(\alpha)=\mathfrak{R}(O_{y},\alpha)</annotation></semantics></math>.
Following prior work in pose estimation <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib49" title="" class="ltx_ref">wang2021nemo, </a>)</cite> we formulate the render-and-compare process as an optimization of the likelihood model:</p>
<table id="A4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.4" class="ltx_Math" alttext="\displaystyle p(F\mid O_{y},\alpha_{y},B)=\prod_{i\in\mathcal{FG}}p(f_{i}\mid O_{y},\alpha_{y})\prod_{i\in\mathcal{BG}}p(f_{i}^{\prime}\mid B)\vspace{-0.3em}" display="inline"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.4" xref="S4.E1.m1.4.4.cmml"><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.cmml"><mi id="S4.E1.m1.2.2.1.3" xref="S4.E1.m1.2.2.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.2.cmml">​</mo><mrow id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.1.4.cmml">F</mi><mo id="S4.E1.m1.2.2.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.3.cmml">∣</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.2.2" xref="S4.E1.m1.2.2.1.1.1.1.2.3.cmml"><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">O</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">y</mi></msub><mo id="S4.E1.m1.2.2.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E1.m1.2.2.1.1.1.1.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.2.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.2.cmml">α</mi><mi id="S4.E1.m1.2.2.1.1.1.1.2.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.3.cmml">y</mi></msub><mo id="S4.E1.m1.2.2.1.1.1.1.2.2.4" xref="S4.E1.m1.2.2.1.1.1.1.2.3.cmml">,</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">B</mi></mrow></mrow><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.4.4.4" xref="S4.E1.m1.4.4.4.cmml">=</mo><mrow id="S4.E1.m1.4.4.3" xref="S4.E1.m1.4.4.3.cmml"><mstyle displaystyle="true" id="S4.E1.m1.4.4.3.3" xref="S4.E1.m1.4.4.3.3.cmml"><munder id="S4.E1.m1.4.4.3.3a" xref="S4.E1.m1.4.4.3.3.cmml"><mo movablelimits="false" id="S4.E1.m1.4.4.3.3.2" xref="S4.E1.m1.4.4.3.3.2.cmml">∏</mo><mrow id="S4.E1.m1.4.4.3.3.3" xref="S4.E1.m1.4.4.3.3.3.cmml"><mi id="S4.E1.m1.4.4.3.3.3.2" xref="S4.E1.m1.4.4.3.3.3.2.cmml">i</mi><mo id="S4.E1.m1.4.4.3.3.3.1" xref="S4.E1.m1.4.4.3.3.3.1.cmml">∈</mo><mrow id="S4.E1.m1.4.4.3.3.3.3" xref="S4.E1.m1.4.4.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.4.4.3.3.3.3.2" xref="S4.E1.m1.4.4.3.3.3.3.2.cmml">ℱ</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.3.3.3.3.1" xref="S4.E1.m1.4.4.3.3.3.3.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.4.4.3.3.3.3.3" xref="S4.E1.m1.4.4.3.3.3.3.3.cmml">𝒢</mi></mrow></mrow></munder></mstyle><mrow id="S4.E1.m1.4.4.3.2" xref="S4.E1.m1.4.4.3.2.cmml"><mi id="S4.E1.m1.4.4.3.2.4" xref="S4.E1.m1.4.4.3.2.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.3.2.3" xref="S4.E1.m1.4.4.3.2.3.cmml">​</mo><mrow id="S4.E1.m1.3.3.2.1.1.1" xref="S4.E1.m1.3.3.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.3.3.2.1.1.1.2" xref="S4.E1.m1.3.3.2.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.3.3.2.1.1.1.1" xref="S4.E1.m1.3.3.2.1.1.1.1.cmml"><msub id="S4.E1.m1.3.3.2.1.1.1.1.4" xref="S4.E1.m1.3.3.2.1.1.1.1.4.cmml"><mi id="S4.E1.m1.3.3.2.1.1.1.1.4.2" xref="S4.E1.m1.3.3.2.1.1.1.1.4.2.cmml">f</mi><mi id="S4.E1.m1.3.3.2.1.1.1.1.4.3" xref="S4.E1.m1.3.3.2.1.1.1.1.4.3.cmml">i</mi></msub><mo id="S4.E1.m1.3.3.2.1.1.1.1.3" xref="S4.E1.m1.3.3.2.1.1.1.1.3.cmml">∣</mo><mrow id="S4.E1.m1.3.3.2.1.1.1.1.2.2" xref="S4.E1.m1.3.3.2.1.1.1.1.2.3.cmml"><msub id="S4.E1.m1.3.3.2.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.2" xref="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.2.cmml">O</mi><mi id="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.3" xref="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.3.cmml">y</mi></msub><mo id="S4.E1.m1.3.3.2.1.1.1.1.2.2.3" xref="S4.E1.m1.3.3.2.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E1.m1.3.3.2.1.1.1.1.2.2.2" xref="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.cmml"><mi id="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.2" xref="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.2.cmml">α</mi><mi id="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.3" xref="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.3.cmml">y</mi></msub></mrow></mrow><mo stretchy="false" id="S4.E1.m1.3.3.2.1.1.1.3" xref="S4.E1.m1.3.3.2.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.3.2.3a" xref="S4.E1.m1.4.4.3.2.3.cmml">​</mo><mrow id="S4.E1.m1.4.4.3.2.2" xref="S4.E1.m1.4.4.3.2.2.cmml"><mstyle displaystyle="true" id="S4.E1.m1.4.4.3.2.2.2" xref="S4.E1.m1.4.4.3.2.2.2.cmml"><munder id="S4.E1.m1.4.4.3.2.2.2a" xref="S4.E1.m1.4.4.3.2.2.2.cmml"><mo movablelimits="false" id="S4.E1.m1.4.4.3.2.2.2.2" xref="S4.E1.m1.4.4.3.2.2.2.2.cmml">∏</mo><mrow id="S4.E1.m1.4.4.3.2.2.2.3" xref="S4.E1.m1.4.4.3.2.2.2.3.cmml"><mi id="S4.E1.m1.4.4.3.2.2.2.3.2" xref="S4.E1.m1.4.4.3.2.2.2.3.2.cmml">i</mi><mo id="S4.E1.m1.4.4.3.2.2.2.3.1" xref="S4.E1.m1.4.4.3.2.2.2.3.1.cmml">∈</mo><mrow id="S4.E1.m1.4.4.3.2.2.2.3.3" xref="S4.E1.m1.4.4.3.2.2.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.4.4.3.2.2.2.3.3.2" xref="S4.E1.m1.4.4.3.2.2.2.3.3.2.cmml">ℬ</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.3.2.2.2.3.3.1" xref="S4.E1.m1.4.4.3.2.2.2.3.3.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.4.4.3.2.2.2.3.3.3" xref="S4.E1.m1.4.4.3.2.2.2.3.3.3.cmml">𝒢</mi></mrow></mrow></munder></mstyle><mrow id="S4.E1.m1.4.4.3.2.2.1" xref="S4.E1.m1.4.4.3.2.2.1.cmml"><mi id="S4.E1.m1.4.4.3.2.2.1.3" xref="S4.E1.m1.4.4.3.2.2.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.3.2.2.1.2" xref="S4.E1.m1.4.4.3.2.2.1.2.cmml">​</mo><mrow id="S4.E1.m1.4.4.3.2.2.1.1.1" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.4.4.3.2.2.1.1.1.2" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.4.4.3.2.2.1.1.1.1" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.cmml"><msubsup id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.cmml"><mi id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.2" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.2.cmml">f</mi><mi id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.3" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.3.cmml">i</mi><mo id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.3" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S4.E1.m1.4.4.3.2.2.1.1.1.1.1" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.1.cmml">∣</mo><mi id="S4.E1.m1.4.4.3.2.2.1.1.1.1.3" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.3.cmml">B</mi></mrow><mo stretchy="false" id="S4.E1.m1.4.4.3.2.2.1.1.1.3" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.4.cmml" xref="S4.E1.m1.4.4"><eq id="S4.E1.m1.4.4.4.cmml" xref="S4.E1.m1.4.4.4"></eq><apply id="S4.E1.m1.2.2.1.cmml" xref="S4.E1.m1.2.2.1"><times id="S4.E1.m1.2.2.1.2.cmml" xref="S4.E1.m1.2.2.1.2"></times><ci id="S4.E1.m1.2.2.1.3.cmml" xref="S4.E1.m1.2.2.1.3">𝑝</ci><apply id="S4.E1.m1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.2.2.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.3">conditional</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.4.cmml" xref="S4.E1.m1.2.2.1.1.1.1.4">𝐹</ci><list id="S4.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2"><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.2">𝑂</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.3">𝑦</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.2">𝛼</ci><ci id="S4.E1.m1.2.2.1.1.1.1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.3">𝑦</ci></apply><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝐵</ci></list></apply></apply><apply id="S4.E1.m1.4.4.3.cmml" xref="S4.E1.m1.4.4.3"><apply id="S4.E1.m1.4.4.3.3.cmml" xref="S4.E1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.3.3.1.cmml" xref="S4.E1.m1.4.4.3.3">subscript</csymbol><csymbol cd="latexml" id="S4.E1.m1.4.4.3.3.2.cmml" xref="S4.E1.m1.4.4.3.3.2">product</csymbol><apply id="S4.E1.m1.4.4.3.3.3.cmml" xref="S4.E1.m1.4.4.3.3.3"><in id="S4.E1.m1.4.4.3.3.3.1.cmml" xref="S4.E1.m1.4.4.3.3.3.1"></in><ci id="S4.E1.m1.4.4.3.3.3.2.cmml" xref="S4.E1.m1.4.4.3.3.3.2">𝑖</ci><apply id="S4.E1.m1.4.4.3.3.3.3.cmml" xref="S4.E1.m1.4.4.3.3.3.3"><times id="S4.E1.m1.4.4.3.3.3.3.1.cmml" xref="S4.E1.m1.4.4.3.3.3.3.1"></times><ci id="S4.E1.m1.4.4.3.3.3.3.2.cmml" xref="S4.E1.m1.4.4.3.3.3.3.2">ℱ</ci><ci id="S4.E1.m1.4.4.3.3.3.3.3.cmml" xref="S4.E1.m1.4.4.3.3.3.3.3">𝒢</ci></apply></apply></apply><apply id="S4.E1.m1.4.4.3.2.cmml" xref="S4.E1.m1.4.4.3.2"><times id="S4.E1.m1.4.4.3.2.3.cmml" xref="S4.E1.m1.4.4.3.2.3"></times><ci id="S4.E1.m1.4.4.3.2.4.cmml" xref="S4.E1.m1.4.4.3.2.4">𝑝</ci><apply id="S4.E1.m1.3.3.2.1.1.1.1.cmml" xref="S4.E1.m1.3.3.2.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.3.3.2.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.3">conditional</csymbol><apply id="S4.E1.m1.3.3.2.1.1.1.1.4.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.2.1.1.1.1.4.1.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.4">subscript</csymbol><ci id="S4.E1.m1.3.3.2.1.1.1.1.4.2.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.4.2">𝑓</ci><ci id="S4.E1.m1.3.3.2.1.1.1.1.4.3.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.4.3">𝑖</ci></apply><list id="S4.E1.m1.3.3.2.1.1.1.1.2.3.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.2.2"><apply id="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.2">𝑂</ci><ci id="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.1.1.1.3">𝑦</ci></apply><apply id="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.1.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.2.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.2">𝛼</ci><ci id="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.3.cmml" xref="S4.E1.m1.3.3.2.1.1.1.1.2.2.2.3">𝑦</ci></apply></list></apply><apply id="S4.E1.m1.4.4.3.2.2.cmml" xref="S4.E1.m1.4.4.3.2.2"><apply id="S4.E1.m1.4.4.3.2.2.2.cmml" xref="S4.E1.m1.4.4.3.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.3.2.2.2.1.cmml" xref="S4.E1.m1.4.4.3.2.2.2">subscript</csymbol><csymbol cd="latexml" id="S4.E1.m1.4.4.3.2.2.2.2.cmml" xref="S4.E1.m1.4.4.3.2.2.2.2">product</csymbol><apply id="S4.E1.m1.4.4.3.2.2.2.3.cmml" xref="S4.E1.m1.4.4.3.2.2.2.3"><in id="S4.E1.m1.4.4.3.2.2.2.3.1.cmml" xref="S4.E1.m1.4.4.3.2.2.2.3.1"></in><ci id="S4.E1.m1.4.4.3.2.2.2.3.2.cmml" xref="S4.E1.m1.4.4.3.2.2.2.3.2">𝑖</ci><apply id="S4.E1.m1.4.4.3.2.2.2.3.3.cmml" xref="S4.E1.m1.4.4.3.2.2.2.3.3"><times id="S4.E1.m1.4.4.3.2.2.2.3.3.1.cmml" xref="S4.E1.m1.4.4.3.2.2.2.3.3.1"></times><ci id="S4.E1.m1.4.4.3.2.2.2.3.3.2.cmml" xref="S4.E1.m1.4.4.3.2.2.2.3.3.2">ℬ</ci><ci id="S4.E1.m1.4.4.3.2.2.2.3.3.3.cmml" xref="S4.E1.m1.4.4.3.2.2.2.3.3.3">𝒢</ci></apply></apply></apply><apply id="S4.E1.m1.4.4.3.2.2.1.cmml" xref="S4.E1.m1.4.4.3.2.2.1"><times id="S4.E1.m1.4.4.3.2.2.1.2.cmml" xref="S4.E1.m1.4.4.3.2.2.1.2"></times><ci id="S4.E1.m1.4.4.3.2.2.1.3.cmml" xref="S4.E1.m1.4.4.3.2.2.1.3">𝑝</ci><apply id="S4.E1.m1.4.4.3.2.2.1.1.1.1.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.4.4.3.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.1">conditional</csymbol><apply id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.1.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.2">𝑓</ci><ci id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.3.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.2.3">′</ci></apply><ci id="S4.E1.m1.4.4.3.2.2.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.3.2.2.1.1.1.1.3">𝐵</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">\displaystyle p(F\mid O_{y},\alpha_{y},B)=\prod_{i\in\mathcal{FG}}p(f_{i}\mid O_{y},\alpha_{y})\prod_{i\in\mathcal{BG}}p(f_{i}^{\prime}\mid B)\vspace{-0.3em}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p4.13" class="ltx_p">where <math id="S4.SS2.p4.9.m1.1" class="ltx_Math" alttext="\mathcal{FG}" display="inline"><semantics id="S4.SS2.p4.9.m1.1a"><mrow id="S4.SS2.p4.9.m1.1.1" xref="S4.SS2.p4.9.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p4.9.m1.1.1.2" xref="S4.SS2.p4.9.m1.1.1.2.cmml">ℱ</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.9.m1.1.1.1" xref="S4.SS2.p4.9.m1.1.1.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p4.9.m1.1.1.3" xref="S4.SS2.p4.9.m1.1.1.3.cmml">𝒢</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.9.m1.1b"><apply id="S4.SS2.p4.9.m1.1.1.cmml" xref="S4.SS2.p4.9.m1.1.1"><times id="S4.SS2.p4.9.m1.1.1.1.cmml" xref="S4.SS2.p4.9.m1.1.1.1"></times><ci id="S4.SS2.p4.9.m1.1.1.2.cmml" xref="S4.SS2.p4.9.m1.1.1.2">ℱ</ci><ci id="S4.SS2.p4.9.m1.1.1.3.cmml" xref="S4.SS2.p4.9.m1.1.1.3">𝒢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.9.m1.1c">\mathcal{FG}</annotation></semantics></math> and <math id="S4.SS2.p4.10.m2.1" class="ltx_Math" alttext="\mathcal{BG}" display="inline"><semantics id="S4.SS2.p4.10.m2.1a"><mrow id="S4.SS2.p4.10.m2.1.1" xref="S4.SS2.p4.10.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p4.10.m2.1.1.2" xref="S4.SS2.p4.10.m2.1.1.2.cmml">ℬ</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.10.m2.1.1.1" xref="S4.SS2.p4.10.m2.1.1.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p4.10.m2.1.1.3" xref="S4.SS2.p4.10.m2.1.1.3.cmml">𝒢</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.10.m2.1b"><apply id="S4.SS2.p4.10.m2.1.1.cmml" xref="S4.SS2.p4.10.m2.1.1"><times id="S4.SS2.p4.10.m2.1.1.1.cmml" xref="S4.SS2.p4.10.m2.1.1.1"></times><ci id="S4.SS2.p4.10.m2.1.1.2.cmml" xref="S4.SS2.p4.10.m2.1.1.2">ℬ</ci><ci id="S4.SS2.p4.10.m2.1.1.3.cmml" xref="S4.SS2.p4.10.m2.1.1.3">𝒢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.10.m2.1c">\mathcal{BG}</annotation></semantics></math> are the set of foreground and background locations on the 2D feature map and <math id="S4.SS2.p4.11.m3.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="S4.SS2.p4.11.m3.1a"><msub id="S4.SS2.p4.11.m3.1.1" xref="S4.SS2.p4.11.m3.1.1.cmml"><mi id="S4.SS2.p4.11.m3.1.1.2" xref="S4.SS2.p4.11.m3.1.1.2.cmml">f</mi><mi id="S4.SS2.p4.11.m3.1.1.3" xref="S4.SS2.p4.11.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.11.m3.1b"><apply id="S4.SS2.p4.11.m3.1.1.cmml" xref="S4.SS2.p4.11.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.11.m3.1.1.1.cmml" xref="S4.SS2.p4.11.m3.1.1">subscript</csymbol><ci id="S4.SS2.p4.11.m3.1.1.2.cmml" xref="S4.SS2.p4.11.m3.1.1.2">𝑓</ci><ci id="S4.SS2.p4.11.m3.1.1.3.cmml" xref="S4.SS2.p4.11.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.11.m3.1c">f_{i}</annotation></semantics></math> is the feature vector of <math id="S4.SS2.p4.12.m4.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S4.SS2.p4.12.m4.1a"><mi id="S4.SS2.p4.12.m4.1.1" xref="S4.SS2.p4.12.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.12.m4.1b"><ci id="S4.SS2.p4.12.m4.1.1.cmml" xref="S4.SS2.p4.12.m4.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.12.m4.1c">F</annotation></semantics></math> at location <math id="S4.SS2.p4.13.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p4.13.m5.1a"><mi id="S4.SS2.p4.13.m5.1.1" xref="S4.SS2.p4.13.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.13.m5.1b"><ci id="S4.SS2.p4.13.m5.1.1.cmml" xref="S4.SS2.p4.13.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.13.m5.1c">i</annotation></semantics></math>. Here the foreground and background likelihoods are modeled as Gaussian distributions.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.5" class="ltx_p">To train the feature extractor <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mi mathvariant="normal" id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">Φ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><ci id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">Φ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">\Phi</annotation></semantics></math>, the neural texture <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="\{T_{y}\}" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><mrow id="S4.SS2.p5.2.m2.1.1.1" xref="S4.SS2.p5.2.m2.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p5.2.m2.1.1.1.2" xref="S4.SS2.p5.2.m2.1.1.2.cmml">{</mo><msub id="S4.SS2.p5.2.m2.1.1.1.1" xref="S4.SS2.p5.2.m2.1.1.1.1.cmml"><mi id="S4.SS2.p5.2.m2.1.1.1.1.2" xref="S4.SS2.p5.2.m2.1.1.1.1.2.cmml">T</mi><mi id="S4.SS2.p5.2.m2.1.1.1.1.3" xref="S4.SS2.p5.2.m2.1.1.1.1.3.cmml">y</mi></msub><mo stretchy="false" id="S4.SS2.p5.2.m2.1.1.1.3" xref="S4.SS2.p5.2.m2.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><set id="S4.SS2.p5.2.m2.1.1.2.cmml" xref="S4.SS2.p5.2.m2.1.1.1"><apply id="S4.SS2.p5.2.m2.1.1.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p5.2.m2.1.1.1.1.2.cmml" xref="S4.SS2.p5.2.m2.1.1.1.1.2">𝑇</ci><ci id="S4.SS2.p5.2.m2.1.1.1.1.3.cmml" xref="S4.SS2.p5.2.m2.1.1.1.1.3">𝑦</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">\{T_{y}\}</annotation></semantics></math> and the background model <math id="S4.SS2.p5.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS2.p5.3.m3.1a"><mi id="S4.SS2.p5.3.m3.1.1" xref="S4.SS2.p5.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.3.m3.1b"><ci id="S4.SS2.p5.3.m3.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.3.m3.1c">B</annotation></semantics></math> jointly, we utilize the EM-type learning strategy as originally introduced for keypoint detection in CoKe<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib7" title="" class="ltx_ref">bai2023coke, </a>)</cite>. Specifically, the feature extractor is trained using stochastic gradient descent while the parameters of the generative model <math id="S4.SS2.p5.4.m4.1" class="ltx_Math" alttext="\{T_{y}\}" display="inline"><semantics id="S4.SS2.p5.4.m4.1a"><mrow id="S4.SS2.p5.4.m4.1.1.1" xref="S4.SS2.p5.4.m4.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p5.4.m4.1.1.1.2" xref="S4.SS2.p5.4.m4.1.1.2.cmml">{</mo><msub id="S4.SS2.p5.4.m4.1.1.1.1" xref="S4.SS2.p5.4.m4.1.1.1.1.cmml"><mi id="S4.SS2.p5.4.m4.1.1.1.1.2" xref="S4.SS2.p5.4.m4.1.1.1.1.2.cmml">T</mi><mi id="S4.SS2.p5.4.m4.1.1.1.1.3" xref="S4.SS2.p5.4.m4.1.1.1.1.3.cmml">y</mi></msub><mo stretchy="false" id="S4.SS2.p5.4.m4.1.1.1.3" xref="S4.SS2.p5.4.m4.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.4.m4.1b"><set id="S4.SS2.p5.4.m4.1.1.2.cmml" xref="S4.SS2.p5.4.m4.1.1.1"><apply id="S4.SS2.p5.4.m4.1.1.1.1.cmml" xref="S4.SS2.p5.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.4.m4.1.1.1.1.1.cmml" xref="S4.SS2.p5.4.m4.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p5.4.m4.1.1.1.1.2.cmml" xref="S4.SS2.p5.4.m4.1.1.1.1.2">𝑇</ci><ci id="S4.SS2.p5.4.m4.1.1.1.1.3.cmml" xref="S4.SS2.p5.4.m4.1.1.1.1.3">𝑦</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.4.m4.1c">\{T_{y}\}</annotation></semantics></math> and <math id="S4.SS2.p5.5.m5.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS2.p5.5.m5.1a"><mi id="S4.SS2.p5.5.m5.1.1" xref="S4.SS2.p5.5.m5.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.5.m5.1b"><ci id="S4.SS2.p5.5.m5.1.1.cmml" xref="S4.SS2.p5.5.m5.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.5.m5.1c">B</annotation></semantics></math> are trained using momentum update after every gradient step in the feature extractor, which was found to stabilize training convergence.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.2" class="ltx_p">At inference time, the object poses <math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><mi id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><ci id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">\alpha</annotation></semantics></math> can be inferred by minimizing the negative log-likelihood w.r.t.<span id="S4.SS2.p6.2.1" class="ltx_text"></span> the 3D pose <math id="S4.SS2.p6.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.p6.2.m2.1a"><mi id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><ci id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">\alpha</annotation></semantics></math> using gradient descent <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib38" title="" class="ltx_ref">ma2022robust, </a>)</cite>.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Multi-object competition with 3D-NMS.</span>
We extend Neural Meshes to predict the 6D object pose and class label in complex multi-object scenes. In particular, we introduce 3D-Non-Maximum-Suppression (3D-NMS) into the maximum likelihood inference process.
This introduces a competition between Neural Meshes of different categories in explaining the feature map. In contrast to classical 2D-NMS, our 3D-NMS also takes into account the distance of each object to the camera and hence naturally enables reasoning about occlusions of objects in the scene.
</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.8" class="ltx_p">We denote the 6D pose as <math id="S4.SS2.p8.1.m1.2" class="ltx_Math" alttext="\gamma=\{x,l\}" display="inline"><semantics id="S4.SS2.p8.1.m1.2a"><mrow id="S4.SS2.p8.1.m1.2.3" xref="S4.SS2.p8.1.m1.2.3.cmml"><mi id="S4.SS2.p8.1.m1.2.3.2" xref="S4.SS2.p8.1.m1.2.3.2.cmml">γ</mi><mo id="S4.SS2.p8.1.m1.2.3.1" xref="S4.SS2.p8.1.m1.2.3.1.cmml">=</mo><mrow id="S4.SS2.p8.1.m1.2.3.3.2" xref="S4.SS2.p8.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS2.p8.1.m1.2.3.3.2.1" xref="S4.SS2.p8.1.m1.2.3.3.1.cmml">{</mo><mi id="S4.SS2.p8.1.m1.1.1" xref="S4.SS2.p8.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.p8.1.m1.2.3.3.2.2" xref="S4.SS2.p8.1.m1.2.3.3.1.cmml">,</mo><mi id="S4.SS2.p8.1.m1.2.2" xref="S4.SS2.p8.1.m1.2.2.cmml">l</mi><mo stretchy="false" id="S4.SS2.p8.1.m1.2.3.3.2.3" xref="S4.SS2.p8.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.1.m1.2b"><apply id="S4.SS2.p8.1.m1.2.3.cmml" xref="S4.SS2.p8.1.m1.2.3"><eq id="S4.SS2.p8.1.m1.2.3.1.cmml" xref="S4.SS2.p8.1.m1.2.3.1"></eq><ci id="S4.SS2.p8.1.m1.2.3.2.cmml" xref="S4.SS2.p8.1.m1.2.3.2">𝛾</ci><set id="S4.SS2.p8.1.m1.2.3.3.1.cmml" xref="S4.SS2.p8.1.m1.2.3.3.2"><ci id="S4.SS2.p8.1.m1.1.1.cmml" xref="S4.SS2.p8.1.m1.1.1">𝑥</ci><ci id="S4.SS2.p8.1.m1.2.2.cmml" xref="S4.SS2.p8.1.m1.2.2">𝑙</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.1.m1.2c">\gamma=\{x,l\}</annotation></semantics></math>, where <math id="S4.SS2.p8.2.m2.2" class="ltx_Math" alttext="x=\{\alpha,\beta\}" display="inline"><semantics id="S4.SS2.p8.2.m2.2a"><mrow id="S4.SS2.p8.2.m2.2.3" xref="S4.SS2.p8.2.m2.2.3.cmml"><mi id="S4.SS2.p8.2.m2.2.3.2" xref="S4.SS2.p8.2.m2.2.3.2.cmml">x</mi><mo id="S4.SS2.p8.2.m2.2.3.1" xref="S4.SS2.p8.2.m2.2.3.1.cmml">=</mo><mrow id="S4.SS2.p8.2.m2.2.3.3.2" xref="S4.SS2.p8.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS2.p8.2.m2.2.3.3.2.1" xref="S4.SS2.p8.2.m2.2.3.3.1.cmml">{</mo><mi id="S4.SS2.p8.2.m2.1.1" xref="S4.SS2.p8.2.m2.1.1.cmml">α</mi><mo id="S4.SS2.p8.2.m2.2.3.3.2.2" xref="S4.SS2.p8.2.m2.2.3.3.1.cmml">,</mo><mi id="S4.SS2.p8.2.m2.2.2" xref="S4.SS2.p8.2.m2.2.2.cmml">β</mi><mo stretchy="false" id="S4.SS2.p8.2.m2.2.3.3.2.3" xref="S4.SS2.p8.2.m2.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.2.m2.2b"><apply id="S4.SS2.p8.2.m2.2.3.cmml" xref="S4.SS2.p8.2.m2.2.3"><eq id="S4.SS2.p8.2.m2.2.3.1.cmml" xref="S4.SS2.p8.2.m2.2.3.1"></eq><ci id="S4.SS2.p8.2.m2.2.3.2.cmml" xref="S4.SS2.p8.2.m2.2.3.2">𝑥</ci><set id="S4.SS2.p8.2.m2.2.3.3.1.cmml" xref="S4.SS2.p8.2.m2.2.3.3.2"><ci id="S4.SS2.p8.2.m2.1.1.cmml" xref="S4.SS2.p8.2.m2.1.1">𝛼</ci><ci id="S4.SS2.p8.2.m2.2.2.cmml" xref="S4.SS2.p8.2.m2.2.2">𝛽</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.2.m2.2c">x=\{\alpha,\beta\}</annotation></semantics></math> represents the 3D object pose <math id="S4.SS2.p8.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.p8.3.m3.1a"><mi id="S4.SS2.p8.3.m3.1.1" xref="S4.SS2.p8.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.3.m3.1b"><ci id="S4.SS2.p8.3.m3.1.1.cmml" xref="S4.SS2.p8.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.3.m3.1c">\alpha</annotation></semantics></math> and object distance to the camera <math id="S4.SS2.p8.4.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS2.p8.4.m4.1a"><mi id="S4.SS2.p8.4.m4.1.1" xref="S4.SS2.p8.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.4.m4.1b"><ci id="S4.SS2.p8.4.m4.1.1.cmml" xref="S4.SS2.p8.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.4.m4.1c">\beta</annotation></semantics></math>, and <math id="S4.SS2.p8.5.m5.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.SS2.p8.5.m5.1a"><mi id="S4.SS2.p8.5.m5.1.1" xref="S4.SS2.p8.5.m5.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.5.m5.1b"><ci id="S4.SS2.p8.5.m5.1.1.cmml" xref="S4.SS2.p8.5.m5.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.5.m5.1c">l</annotation></semantics></math> is the 2D object location in the feature map.
We first detect the 6D poses of each object category independently and apply 2D-NMS such that for each 2D location <math id="S4.SS2.p8.6.m6.1" class="ltx_Math" alttext="l^{\prime}" display="inline"><semantics id="S4.SS2.p8.6.m6.1a"><msup id="S4.SS2.p8.6.m6.1.1" xref="S4.SS2.p8.6.m6.1.1.cmml"><mi id="S4.SS2.p8.6.m6.1.1.2" xref="S4.SS2.p8.6.m6.1.1.2.cmml">l</mi><mo id="S4.SS2.p8.6.m6.1.1.3" xref="S4.SS2.p8.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.6.m6.1b"><apply id="S4.SS2.p8.6.m6.1.1.cmml" xref="S4.SS2.p8.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p8.6.m6.1.1.1.cmml" xref="S4.SS2.p8.6.m6.1.1">superscript</csymbol><ci id="S4.SS2.p8.6.m6.1.1.2.cmml" xref="S4.SS2.p8.6.m6.1.1.2">𝑙</ci><ci id="S4.SS2.p8.6.m6.1.1.3.cmml" xref="S4.SS2.p8.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.6.m6.1c">l^{\prime}</annotation></semantics></math> in a neighborhood defined by radius <math id="S4.SS2.p8.7.m7.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.SS2.p8.7.m7.1a"><mi id="S4.SS2.p8.7.m7.1.1" xref="S4.SS2.p8.7.m7.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.7.m7.1b"><ci id="S4.SS2.p8.7.m7.1.1.cmml" xref="S4.SS2.p8.7.m7.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.7.m7.1c">r</annotation></semantics></math>, the predicted 6D pose <math id="S4.SS2.p8.8.m8.2" class="ltx_Math" alttext="\{x,l\}" display="inline"><semantics id="S4.SS2.p8.8.m8.2a"><mrow id="S4.SS2.p8.8.m8.2.3.2" xref="S4.SS2.p8.8.m8.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p8.8.m8.2.3.2.1" xref="S4.SS2.p8.8.m8.2.3.1.cmml">{</mo><mi id="S4.SS2.p8.8.m8.1.1" xref="S4.SS2.p8.8.m8.1.1.cmml">x</mi><mo id="S4.SS2.p8.8.m8.2.3.2.2" xref="S4.SS2.p8.8.m8.2.3.1.cmml">,</mo><mi id="S4.SS2.p8.8.m8.2.2" xref="S4.SS2.p8.8.m8.2.2.cmml">l</mi><mo stretchy="false" id="S4.SS2.p8.8.m8.2.3.2.3" xref="S4.SS2.p8.8.m8.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.8.m8.2b"><set id="S4.SS2.p8.8.m8.2.3.1.cmml" xref="S4.SS2.p8.8.m8.2.3.2"><ci id="S4.SS2.p8.8.m8.1.1.cmml" xref="S4.SS2.p8.8.m8.1.1">𝑥</ci><ci id="S4.SS2.p8.8.m8.2.2.cmml" xref="S4.SS2.p8.8.m8.2.2">𝑙</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.8.m8.2c">\{x,l\}</annotation></semantics></math> yields the largest activation:</p>
<table id="A4.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.8" class="ltx_Math" alttext="\displaystyle\vspace{-0.2em}\max_{x}\;p(F\mid x,l)\;\;s.t.\;\;p(F\mid x,l)&gt;p(F\mid x,l^{\prime}),\;\;\forall l^{\prime}\in\{l^{\prime}\mid 0&lt;\lvert l^{\prime}-l\rvert&lt;r\}\vspace{-0.2em}" display="inline"><semantics id="S4.E2.m1.8a"><mrow id="S4.E2.m1.8.8.2" xref="S4.E2.m1.8.8.3.cmml"><mrow id="S4.E2.m1.7.7.1.1" xref="S4.E2.m1.7.7.1.1.cmml"><mrow id="S4.E2.m1.7.7.1.1.3" xref="S4.E2.m1.7.7.1.1.3.cmml"><munder id="S4.E2.m1.7.7.1.1.3.1" xref="S4.E2.m1.7.7.1.1.3.1.cmml"><mi id="S4.E2.m1.7.7.1.1.3.1.2" xref="S4.E2.m1.7.7.1.1.3.1.2.cmml">max</mi><mi id="S4.E2.m1.7.7.1.1.3.1.3" xref="S4.E2.m1.7.7.1.1.3.1.3.cmml">x</mi></munder><mo lspace="0.447em" id="S4.E2.m1.7.7.1.1.3a" xref="S4.E2.m1.7.7.1.1.3.cmml">⁡</mo><mi id="S4.E2.m1.7.7.1.1.3.2" xref="S4.E2.m1.7.7.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E2.m1.7.7.1.1.2" xref="S4.E2.m1.7.7.1.1.2.cmml">​</mo><mrow id="S4.E2.m1.7.7.1.1.1.1" xref="S4.E2.m1.7.7.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m1.7.7.1.1.1.1.2" xref="S4.E2.m1.7.7.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.7.7.1.1.1.1.1" xref="S4.E2.m1.7.7.1.1.1.1.1.cmml"><mi id="S4.E2.m1.7.7.1.1.1.1.1.2" xref="S4.E2.m1.7.7.1.1.1.1.1.2.cmml">F</mi><mo id="S4.E2.m1.7.7.1.1.1.1.1.1" xref="S4.E2.m1.7.7.1.1.1.1.1.1.cmml">∣</mo><mrow id="S4.E2.m1.7.7.1.1.1.1.1.3.2" xref="S4.E2.m1.7.7.1.1.1.1.1.3.1.cmml"><mi id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">x</mi><mo id="S4.E2.m1.7.7.1.1.1.1.1.3.2.1" xref="S4.E2.m1.7.7.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml">l</mi></mrow></mrow><mo stretchy="false" id="S4.E2.m1.7.7.1.1.1.1.3" xref="S4.E2.m1.7.7.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.560em" rspace="0em" id="S4.E2.m1.7.7.1.1.2a" xref="S4.E2.m1.7.7.1.1.2.cmml">​</mo><mi id="S4.E2.m1.7.7.1.1.4" xref="S4.E2.m1.7.7.1.1.4.cmml">s</mi></mrow><mo lspace="0em" rspace="0.167em" id="S4.E2.m1.8.8.2.3" xref="S4.E2.m1.8.8.3a.cmml">.</mo><mi id="S4.E2.m1.6.6" xref="S4.E2.m1.6.6.cmml">t</mi><mo lspace="0em" rspace="0.727em" id="S4.E2.m1.8.8.2.4" xref="S4.E2.m1.8.8.3a.cmml">.</mo><mrow id="S4.E2.m1.8.8.2.2.2" xref="S4.E2.m1.8.8.2.2.3.cmml"><mrow id="S4.E2.m1.8.8.2.2.1.1" xref="S4.E2.m1.8.8.2.2.1.1.cmml"><mrow id="S4.E2.m1.8.8.2.2.1.1.1" xref="S4.E2.m1.8.8.2.2.1.1.1.cmml"><mi id="S4.E2.m1.8.8.2.2.1.1.1.3" xref="S4.E2.m1.8.8.2.2.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.8.8.2.2.1.1.1.2" xref="S4.E2.m1.8.8.2.2.1.1.1.2.cmml">​</mo><mrow id="S4.E2.m1.8.8.2.2.1.1.1.1.1" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m1.8.8.2.2.1.1.1.1.1.2" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.2" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.2.cmml">F</mi><mo id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.1" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.1.cmml">∣</mo><mrow id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.3.2" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.3.1.cmml"><mi id="S4.E2.m1.3.3" xref="S4.E2.m1.3.3.cmml">x</mi><mo id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.3.2.1" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml">l</mi></mrow></mrow><mo stretchy="false" id="S4.E2.m1.8.8.2.2.1.1.1.1.1.3" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.8.8.2.2.1.1.3" xref="S4.E2.m1.8.8.2.2.1.1.3.cmml">&gt;</mo><mrow id="S4.E2.m1.8.8.2.2.1.1.2" xref="S4.E2.m1.8.8.2.2.1.1.2.cmml"><mi id="S4.E2.m1.8.8.2.2.1.1.2.3" xref="S4.E2.m1.8.8.2.2.1.1.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.8.8.2.2.1.1.2.2" xref="S4.E2.m1.8.8.2.2.1.1.2.2.cmml">​</mo><mrow id="S4.E2.m1.8.8.2.2.1.1.2.1.1" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m1.8.8.2.2.1.1.2.1.1.2" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.cmml"><mi id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.3" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.3.cmml">F</mi><mo id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.2" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.2.cmml">∣</mo><mrow id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.2.cmml"><mi id="S4.E2.m1.5.5" xref="S4.E2.m1.5.5.cmml">x</mi><mo id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.2" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.2.cmml">,</mo><msup id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.2" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.2.cmml">l</mi><mo id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.3" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.3.cmml">′</mo></msup></mrow></mrow><mo stretchy="false" id="S4.E2.m1.8.8.2.2.1.1.2.1.1.3" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.727em" id="S4.E2.m1.8.8.2.2.2.3" xref="S4.E2.m1.8.8.2.2.3a.cmml">,</mo><mrow id="S4.E2.m1.8.8.2.2.2.2" xref="S4.E2.m1.8.8.2.2.2.2.cmml"><mrow id="S4.E2.m1.8.8.2.2.2.2.4" xref="S4.E2.m1.8.8.2.2.2.2.4.cmml"><mo rspace="0.167em" id="S4.E2.m1.8.8.2.2.2.2.4.1" xref="S4.E2.m1.8.8.2.2.2.2.4.1.cmml">∀</mo><msup id="S4.E2.m1.8.8.2.2.2.2.4.2" xref="S4.E2.m1.8.8.2.2.2.2.4.2.cmml"><mi id="S4.E2.m1.8.8.2.2.2.2.4.2.2" xref="S4.E2.m1.8.8.2.2.2.2.4.2.2.cmml">l</mi><mo id="S4.E2.m1.8.8.2.2.2.2.4.2.3" xref="S4.E2.m1.8.8.2.2.2.2.4.2.3.cmml">′</mo></msup></mrow><mo id="S4.E2.m1.8.8.2.2.2.2.3" xref="S4.E2.m1.8.8.2.2.2.2.3.cmml">∈</mo><mrow id="S4.E2.m1.8.8.2.2.2.2.2.2" xref="S4.E2.m1.8.8.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E2.m1.8.8.2.2.2.2.2.2.3" xref="S4.E2.m1.8.8.2.2.2.2.2.3.1.cmml">{</mo><msup id="S4.E2.m1.8.8.2.2.2.2.1.1.1" xref="S4.E2.m1.8.8.2.2.2.2.1.1.1.cmml"><mi id="S4.E2.m1.8.8.2.2.2.2.1.1.1.2" xref="S4.E2.m1.8.8.2.2.2.2.1.1.1.2.cmml">l</mi><mo id="S4.E2.m1.8.8.2.2.2.2.1.1.1.3" xref="S4.E2.m1.8.8.2.2.2.2.1.1.1.3.cmml">′</mo></msup><mo fence="true" lspace="0em" rspace="0em" id="S4.E2.m1.8.8.2.2.2.2.2.2.4" xref="S4.E2.m1.8.8.2.2.2.2.2.3.1.cmml">∣</mo><mrow id="S4.E2.m1.8.8.2.2.2.2.2.2.2" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.cmml"><mn id="S4.E2.m1.8.8.2.2.2.2.2.2.2.3" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.3.cmml">0</mn><mo id="S4.E2.m1.8.8.2.2.2.2.2.2.2.4" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.4.cmml">&lt;</mo><mrow id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.2" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.2.1.cmml">|</mo><mrow id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.cmml"><msup id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.cmml"><mi id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.2" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.2.cmml">l</mi><mo id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.3" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.3.cmml">′</mo></msup><mo id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.1" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.1.cmml">−</mo><mi id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.3" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.3.cmml">l</mi></mrow><mo stretchy="false" id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.3" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.2.1.cmml">|</mo></mrow><mo id="S4.E2.m1.8.8.2.2.2.2.2.2.2.5" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.5.cmml">&lt;</mo><mi id="S4.E2.m1.8.8.2.2.2.2.2.2.2.6" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.6.cmml">r</mi></mrow><mo stretchy="false" id="S4.E2.m1.8.8.2.2.2.2.2.2.5" xref="S4.E2.m1.8.8.2.2.2.2.2.3.1.cmml">}</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.8b"><apply id="S4.E2.m1.8.8.3.cmml" xref="S4.E2.m1.8.8.2"><csymbol cd="ambiguous" id="S4.E2.m1.8.8.3a.cmml" xref="S4.E2.m1.8.8.2.3">formulae-sequence</csymbol><apply id="S4.E2.m1.7.7.1.1.cmml" xref="S4.E2.m1.7.7.1.1"><times id="S4.E2.m1.7.7.1.1.2.cmml" xref="S4.E2.m1.7.7.1.1.2"></times><apply id="S4.E2.m1.7.7.1.1.3.cmml" xref="S4.E2.m1.7.7.1.1.3"><apply id="S4.E2.m1.7.7.1.1.3.1.cmml" xref="S4.E2.m1.7.7.1.1.3.1"><csymbol cd="ambiguous" id="S4.E2.m1.7.7.1.1.3.1.1.cmml" xref="S4.E2.m1.7.7.1.1.3.1">subscript</csymbol><max id="S4.E2.m1.7.7.1.1.3.1.2.cmml" xref="S4.E2.m1.7.7.1.1.3.1.2"></max><ci id="S4.E2.m1.7.7.1.1.3.1.3.cmml" xref="S4.E2.m1.7.7.1.1.3.1.3">𝑥</ci></apply><ci id="S4.E2.m1.7.7.1.1.3.2.cmml" xref="S4.E2.m1.7.7.1.1.3.2">𝑝</ci></apply><apply id="S4.E2.m1.7.7.1.1.1.1.1.cmml" xref="S4.E2.m1.7.7.1.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.7.7.1.1.1.1.1.1.cmml" xref="S4.E2.m1.7.7.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E2.m1.7.7.1.1.1.1.1.2.cmml" xref="S4.E2.m1.7.7.1.1.1.1.1.2">𝐹</ci><list id="S4.E2.m1.7.7.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.7.7.1.1.1.1.1.3.2"><ci id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">𝑥</ci><ci id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2">𝑙</ci></list></apply><ci id="S4.E2.m1.7.7.1.1.4.cmml" xref="S4.E2.m1.7.7.1.1.4">𝑠</ci></apply><ci id="S4.E2.m1.6.6.cmml" xref="S4.E2.m1.6.6">𝑡</ci><apply id="S4.E2.m1.8.8.2.2.3.cmml" xref="S4.E2.m1.8.8.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.8.8.2.2.3a.cmml" xref="S4.E2.m1.8.8.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E2.m1.8.8.2.2.1.1.cmml" xref="S4.E2.m1.8.8.2.2.1.1"><gt id="S4.E2.m1.8.8.2.2.1.1.3.cmml" xref="S4.E2.m1.8.8.2.2.1.1.3"></gt><apply id="S4.E2.m1.8.8.2.2.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.1.1.1"><times id="S4.E2.m1.8.8.2.2.1.1.1.2.cmml" xref="S4.E2.m1.8.8.2.2.1.1.1.2"></times><ci id="S4.E2.m1.8.8.2.2.1.1.1.3.cmml" xref="S4.E2.m1.8.8.2.2.1.1.1.3">𝑝</ci><apply id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.2">𝐹</ci><list id="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.8.8.2.2.1.1.1.1.1.1.3.2"><ci id="S4.E2.m1.3.3.cmml" xref="S4.E2.m1.3.3">𝑥</ci><ci id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4">𝑙</ci></list></apply></apply><apply id="S4.E2.m1.8.8.2.2.1.1.2.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2"><times id="S4.E2.m1.8.8.2.2.1.1.2.2.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.2"></times><ci id="S4.E2.m1.8.8.2.2.1.1.2.3.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.3">𝑝</ci><apply id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1"><csymbol cd="latexml" id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.2.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.2">conditional</csymbol><ci id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.3.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.3">𝐹</ci><list id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.2.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1"><ci id="S4.E2.m1.5.5.cmml" xref="S4.E2.m1.5.5">𝑥</ci><apply id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.2">𝑙</ci><ci id="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.8.8.2.2.1.1.2.1.1.1.1.1.1.3">′</ci></apply></list></apply></apply></apply><apply id="S4.E2.m1.8.8.2.2.2.2.cmml" xref="S4.E2.m1.8.8.2.2.2.2"><in id="S4.E2.m1.8.8.2.2.2.2.3.cmml" xref="S4.E2.m1.8.8.2.2.2.2.3"></in><apply id="S4.E2.m1.8.8.2.2.2.2.4.cmml" xref="S4.E2.m1.8.8.2.2.2.2.4"><csymbol cd="latexml" id="S4.E2.m1.8.8.2.2.2.2.4.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.4.1">for-all</csymbol><apply id="S4.E2.m1.8.8.2.2.2.2.4.2.cmml" xref="S4.E2.m1.8.8.2.2.2.2.4.2"><csymbol cd="ambiguous" id="S4.E2.m1.8.8.2.2.2.2.4.2.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.4.2">superscript</csymbol><ci id="S4.E2.m1.8.8.2.2.2.2.4.2.2.cmml" xref="S4.E2.m1.8.8.2.2.2.2.4.2.2">𝑙</ci><ci id="S4.E2.m1.8.8.2.2.2.2.4.2.3.cmml" xref="S4.E2.m1.8.8.2.2.2.2.4.2.3">′</ci></apply></apply><apply id="S4.E2.m1.8.8.2.2.2.2.2.3.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2"><csymbol cd="latexml" id="S4.E2.m1.8.8.2.2.2.2.2.3.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.3">conditional-set</csymbol><apply id="S4.E2.m1.8.8.2.2.2.2.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.8.8.2.2.2.2.1.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.1.1.1">superscript</csymbol><ci id="S4.E2.m1.8.8.2.2.2.2.1.1.1.2.cmml" xref="S4.E2.m1.8.8.2.2.2.2.1.1.1.2">𝑙</ci><ci id="S4.E2.m1.8.8.2.2.2.2.1.1.1.3.cmml" xref="S4.E2.m1.8.8.2.2.2.2.1.1.1.3">′</ci></apply><apply id="S4.E2.m1.8.8.2.2.2.2.2.2.2.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2"><and id="S4.E2.m1.8.8.2.2.2.2.2.2.2a.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2"></and><apply id="S4.E2.m1.8.8.2.2.2.2.2.2.2b.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2"><lt id="S4.E2.m1.8.8.2.2.2.2.2.2.2.4.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.4"></lt><cn type="integer" id="S4.E2.m1.8.8.2.2.2.2.2.2.2.3.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.3">0</cn><apply id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.2.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1"><abs id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.2.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.2"></abs><apply id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1"><minus id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.1"></minus><apply id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2">superscript</csymbol><ci id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.2.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.2">𝑙</ci><ci id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.3.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.2.3">′</ci></apply><ci id="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.3.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.1.1.1.3">𝑙</ci></apply></apply></apply><apply id="S4.E2.m1.8.8.2.2.2.2.2.2.2c.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2"><lt id="S4.E2.m1.8.8.2.2.2.2.2.2.2.5.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.5"></lt><share href="#S4.E2.m1.8.8.2.2.2.2.2.2.2.1.cmml" id="S4.E2.m1.8.8.2.2.2.2.2.2.2d.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2"></share><ci id="S4.E2.m1.8.8.2.2.2.2.2.2.2.6.cmml" xref="S4.E2.m1.8.8.2.2.2.2.2.2.2.6">𝑟</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.8c">\displaystyle\vspace{-0.2em}\max_{x}\;p(F\mid x,l)\;\;s.t.\;\;p(F\mid x,l)&gt;p(F\mid x,l^{\prime}),\;\;\forall l^{\prime}\in\{l^{\prime}\mid 0&lt;\lvert l^{\prime}-l\rvert&lt;r\}\vspace{-0.2em}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p8.10" class="ltx_p">We enable multi-category 6D pose estimation by extending this formulation to a 3D non-maximum suppression (3D-NMS). Using <math id="S4.SS2.p8.9.m1.1" class="ltx_Math" alttext="\mathcal{Y}" display="inline"><semantics id="S4.SS2.p8.9.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p8.9.m1.1.1" xref="S4.SS2.p8.9.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.9.m1.1b"><ci id="S4.SS2.p8.9.m1.1.1.cmml" xref="S4.SS2.p8.9.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.9.m1.1c">\mathcal{Y}</annotation></semantics></math> to represent the set of all object categories, we model the category label <math id="S4.SS2.p8.10.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p8.10.m2.1a"><mi id="S4.SS2.p8.10.m2.1.1" xref="S4.SS2.p8.10.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.10.m2.1b"><ci id="S4.SS2.p8.10.m2.1.1.cmml" xref="S4.SS2.p8.10.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.10.m2.1c">y</annotation></semantics></math> from a generative perspective:</p>
<table id="A4.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E3.m1.5" class="ltx_Math" alttext="\displaystyle\vspace{-0.2em}\max_{x}\;p(F\mid x,l,y)\;\;s.t.\;\;" display="inline"><semantics id="S4.E3.m1.5a"><mrow id="S4.E3.m1.5.5.1"><mrow id="S4.E3.m1.5.5.1.1.1" xref="S4.E3.m1.5.5.1.1.2.cmml"><mrow id="S4.E3.m1.5.5.1.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.cmml"><mrow id="S4.E3.m1.5.5.1.1.1.1.3" xref="S4.E3.m1.5.5.1.1.1.1.3.cmml"><munder id="S4.E3.m1.5.5.1.1.1.1.3.1" xref="S4.E3.m1.5.5.1.1.1.1.3.1.cmml"><mi id="S4.E3.m1.5.5.1.1.1.1.3.1.2" xref="S4.E3.m1.5.5.1.1.1.1.3.1.2.cmml">max</mi><mi id="S4.E3.m1.5.5.1.1.1.1.3.1.3" xref="S4.E3.m1.5.5.1.1.1.1.3.1.3.cmml">x</mi></munder><mo lspace="0.447em" id="S4.E3.m1.5.5.1.1.1.1.3a" xref="S4.E3.m1.5.5.1.1.1.1.3.cmml">⁡</mo><mi id="S4.E3.m1.5.5.1.1.1.1.3.2" xref="S4.E3.m1.5.5.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E3.m1.5.5.1.1.1.1.2" xref="S4.E3.m1.5.5.1.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m1.5.5.1.1.1.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.5.5.1.1.1.1.1.1.2" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.5.5.1.1.1.1.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.5.5.1.1.1.1.1.1.1.2" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml">F</mi><mo id="S4.E3.m1.5.5.1.1.1.1.1.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">∣</mo><mrow id="S4.E3.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml"><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">x</mi><mo id="S4.E3.m1.5.5.1.1.1.1.1.1.1.3.2.1" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml">l</mi><mo id="S4.E3.m1.5.5.1.1.1.1.1.1.1.3.2.2" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E3.m1.3.3" xref="S4.E3.m1.3.3.cmml">y</mi></mrow></mrow><mo stretchy="false" id="S4.E3.m1.5.5.1.1.1.1.1.1.3" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.560em" rspace="0em" id="S4.E3.m1.5.5.1.1.1.1.2a" xref="S4.E3.m1.5.5.1.1.1.1.2.cmml">​</mo><mi id="S4.E3.m1.5.5.1.1.1.1.4" xref="S4.E3.m1.5.5.1.1.1.1.4.cmml">s</mi></mrow><mo lspace="0em" rspace="0.167em" id="S4.E3.m1.5.5.1.1.1.2" xref="S4.E3.m1.5.5.1.1.2a.cmml">.</mo><mi id="S4.E3.m1.4.4" xref="S4.E3.m1.4.4.cmml">t</mi></mrow><mo lspace="0em" id="S4.E3.m1.5.5.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.5b"><apply id="S4.E3.m1.5.5.1.1.2.cmml" xref="S4.E3.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.2a.cmml" xref="S4.E3.m1.5.5.1.1.1.2">formulae-sequence</csymbol><apply id="S4.E3.m1.5.5.1.1.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1"><times id="S4.E3.m1.5.5.1.1.1.1.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.2"></times><apply id="S4.E3.m1.5.5.1.1.1.1.3.cmml" xref="S4.E3.m1.5.5.1.1.1.1.3"><apply id="S4.E3.m1.5.5.1.1.1.1.3.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.1.1.3.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.3.1">subscript</csymbol><max id="S4.E3.m1.5.5.1.1.1.1.3.1.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.3.1.2"></max><ci id="S4.E3.m1.5.5.1.1.1.1.3.1.3.cmml" xref="S4.E3.m1.5.5.1.1.1.1.3.1.3">𝑥</ci></apply><ci id="S4.E3.m1.5.5.1.1.1.1.3.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.3.2">𝑝</ci></apply><apply id="S4.E3.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.2">𝐹</ci><list id="S4.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.1.1.3.2"><ci id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1">𝑥</ci><ci id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2">𝑙</ci><ci id="S4.E3.m1.3.3.cmml" xref="S4.E3.m1.3.3">𝑦</ci></list></apply><ci id="S4.E3.m1.5.5.1.1.1.1.4.cmml" xref="S4.E3.m1.5.5.1.1.1.1.4">𝑠</ci></apply><ci id="S4.E3.m1.4.4.cmml" xref="S4.E3.m1.4.4">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.5c">\displaystyle\vspace{-0.2em}\max_{x}\;p(F\mid x,l,y)\;\;s.t.\;\;</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E3.m2.7" class="ltx_Math" alttext="\displaystyle p(F\mid x,l,y)&gt;p(F\mid x,l^{\prime},y),\;\;\forall l^{\prime}\in\{l^{\prime}\mid 0&lt;\lvert l^{\prime}-l\rvert&lt;r\}" display="inline"><semantics id="S4.E3.m2.7a"><mrow id="S4.E3.m2.7.7.2" xref="S4.E3.m2.7.7.3.cmml"><mrow id="S4.E3.m2.6.6.1.1" xref="S4.E3.m2.6.6.1.1.cmml"><mrow id="S4.E3.m2.6.6.1.1.1" xref="S4.E3.m2.6.6.1.1.1.cmml"><mi id="S4.E3.m2.6.6.1.1.1.3" xref="S4.E3.m2.6.6.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E3.m2.6.6.1.1.1.2" xref="S4.E3.m2.6.6.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m2.6.6.1.1.1.1.1" xref="S4.E3.m2.6.6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m2.6.6.1.1.1.1.1.2" xref="S4.E3.m2.6.6.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m2.6.6.1.1.1.1.1.1" xref="S4.E3.m2.6.6.1.1.1.1.1.1.cmml"><mi id="S4.E3.m2.6.6.1.1.1.1.1.1.2" xref="S4.E3.m2.6.6.1.1.1.1.1.1.2.cmml">F</mi><mo id="S4.E3.m2.6.6.1.1.1.1.1.1.1" xref="S4.E3.m2.6.6.1.1.1.1.1.1.1.cmml">∣</mo><mrow id="S4.E3.m2.6.6.1.1.1.1.1.1.3.2" xref="S4.E3.m2.6.6.1.1.1.1.1.1.3.1.cmml"><mi id="S4.E3.m2.1.1" xref="S4.E3.m2.1.1.cmml">x</mi><mo id="S4.E3.m2.6.6.1.1.1.1.1.1.3.2.1" xref="S4.E3.m2.6.6.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E3.m2.2.2" xref="S4.E3.m2.2.2.cmml">l</mi><mo id="S4.E3.m2.6.6.1.1.1.1.1.1.3.2.2" xref="S4.E3.m2.6.6.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E3.m2.3.3" xref="S4.E3.m2.3.3.cmml">y</mi></mrow></mrow><mo stretchy="false" id="S4.E3.m2.6.6.1.1.1.1.1.3" xref="S4.E3.m2.6.6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m2.6.6.1.1.3" xref="S4.E3.m2.6.6.1.1.3.cmml">&gt;</mo><mrow id="S4.E3.m2.6.6.1.1.2" xref="S4.E3.m2.6.6.1.1.2.cmml"><mi id="S4.E3.m2.6.6.1.1.2.3" xref="S4.E3.m2.6.6.1.1.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E3.m2.6.6.1.1.2.2" xref="S4.E3.m2.6.6.1.1.2.2.cmml">​</mo><mrow id="S4.E3.m2.6.6.1.1.2.1.1" xref="S4.E3.m2.6.6.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m2.6.6.1.1.2.1.1.2" xref="S4.E3.m2.6.6.1.1.2.1.1.1.cmml">(</mo><mrow id="S4.E3.m2.6.6.1.1.2.1.1.1" xref="S4.E3.m2.6.6.1.1.2.1.1.1.cmml"><mi id="S4.E3.m2.6.6.1.1.2.1.1.1.3" xref="S4.E3.m2.6.6.1.1.2.1.1.1.3.cmml">F</mi><mo id="S4.E3.m2.6.6.1.1.2.1.1.1.2" xref="S4.E3.m2.6.6.1.1.2.1.1.1.2.cmml">∣</mo><mrow id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.2.cmml"><mi id="S4.E3.m2.4.4" xref="S4.E3.m2.4.4.cmml">x</mi><mo id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.2" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.2.cmml">,</mo><msup id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.cmml"><mi id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.2" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.2.cmml">l</mi><mo id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.3" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.3" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.2.cmml">,</mo><mi id="S4.E3.m2.5.5" xref="S4.E3.m2.5.5.cmml">y</mi></mrow></mrow><mo stretchy="false" id="S4.E3.m2.6.6.1.1.2.1.1.3" xref="S4.E3.m2.6.6.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.727em" id="S4.E3.m2.7.7.2.3" xref="S4.E3.m2.7.7.3a.cmml">,</mo><mrow id="S4.E3.m2.7.7.2.2" xref="S4.E3.m2.7.7.2.2.cmml"><mrow id="S4.E3.m2.7.7.2.2.4" xref="S4.E3.m2.7.7.2.2.4.cmml"><mo rspace="0.167em" id="S4.E3.m2.7.7.2.2.4.1" xref="S4.E3.m2.7.7.2.2.4.1.cmml">∀</mo><msup id="S4.E3.m2.7.7.2.2.4.2" xref="S4.E3.m2.7.7.2.2.4.2.cmml"><mi id="S4.E3.m2.7.7.2.2.4.2.2" xref="S4.E3.m2.7.7.2.2.4.2.2.cmml">l</mi><mo id="S4.E3.m2.7.7.2.2.4.2.3" xref="S4.E3.m2.7.7.2.2.4.2.3.cmml">′</mo></msup></mrow><mo id="S4.E3.m2.7.7.2.2.3" xref="S4.E3.m2.7.7.2.2.3.cmml">∈</mo><mrow id="S4.E3.m2.7.7.2.2.2.2" xref="S4.E3.m2.7.7.2.2.2.3.cmml"><mo stretchy="false" id="S4.E3.m2.7.7.2.2.2.2.3" xref="S4.E3.m2.7.7.2.2.2.3.1.cmml">{</mo><msup id="S4.E3.m2.7.7.2.2.1.1.1" xref="S4.E3.m2.7.7.2.2.1.1.1.cmml"><mi id="S4.E3.m2.7.7.2.2.1.1.1.2" xref="S4.E3.m2.7.7.2.2.1.1.1.2.cmml">l</mi><mo id="S4.E3.m2.7.7.2.2.1.1.1.3" xref="S4.E3.m2.7.7.2.2.1.1.1.3.cmml">′</mo></msup><mo fence="true" lspace="0em" rspace="0em" id="S4.E3.m2.7.7.2.2.2.2.4" xref="S4.E3.m2.7.7.2.2.2.3.1.cmml">∣</mo><mrow id="S4.E3.m2.7.7.2.2.2.2.2" xref="S4.E3.m2.7.7.2.2.2.2.2.cmml"><mn id="S4.E3.m2.7.7.2.2.2.2.2.3" xref="S4.E3.m2.7.7.2.2.2.2.2.3.cmml">0</mn><mo id="S4.E3.m2.7.7.2.2.2.2.2.4" xref="S4.E3.m2.7.7.2.2.2.2.2.4.cmml">&lt;</mo><mrow id="S4.E3.m2.7.7.2.2.2.2.2.1.1" xref="S4.E3.m2.7.7.2.2.2.2.2.1.2.cmml"><mo stretchy="false" id="S4.E3.m2.7.7.2.2.2.2.2.1.1.2" xref="S4.E3.m2.7.7.2.2.2.2.2.1.2.1.cmml">|</mo><mrow id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.cmml"><msup id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.cmml"><mi id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.2" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.2.cmml">l</mi><mo id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.3" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.3.cmml">′</mo></msup><mo id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.1" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.1.cmml">−</mo><mi id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.3" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.3.cmml">l</mi></mrow><mo stretchy="false" id="S4.E3.m2.7.7.2.2.2.2.2.1.1.3" xref="S4.E3.m2.7.7.2.2.2.2.2.1.2.1.cmml">|</mo></mrow><mo id="S4.E3.m2.7.7.2.2.2.2.2.5" xref="S4.E3.m2.7.7.2.2.2.2.2.5.cmml">&lt;</mo><mi id="S4.E3.m2.7.7.2.2.2.2.2.6" xref="S4.E3.m2.7.7.2.2.2.2.2.6.cmml">r</mi></mrow><mo stretchy="false" id="S4.E3.m2.7.7.2.2.2.2.5" xref="S4.E3.m2.7.7.2.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m2.7b"><apply id="S4.E3.m2.7.7.3.cmml" xref="S4.E3.m2.7.7.2"><csymbol cd="ambiguous" id="S4.E3.m2.7.7.3a.cmml" xref="S4.E3.m2.7.7.2.3">formulae-sequence</csymbol><apply id="S4.E3.m2.6.6.1.1.cmml" xref="S4.E3.m2.6.6.1.1"><gt id="S4.E3.m2.6.6.1.1.3.cmml" xref="S4.E3.m2.6.6.1.1.3"></gt><apply id="S4.E3.m2.6.6.1.1.1.cmml" xref="S4.E3.m2.6.6.1.1.1"><times id="S4.E3.m2.6.6.1.1.1.2.cmml" xref="S4.E3.m2.6.6.1.1.1.2"></times><ci id="S4.E3.m2.6.6.1.1.1.3.cmml" xref="S4.E3.m2.6.6.1.1.1.3">𝑝</ci><apply id="S4.E3.m2.6.6.1.1.1.1.1.1.cmml" xref="S4.E3.m2.6.6.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m2.6.6.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.6.6.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E3.m2.6.6.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.6.6.1.1.1.1.1.1.2">𝐹</ci><list id="S4.E3.m2.6.6.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m2.6.6.1.1.1.1.1.1.3.2"><ci id="S4.E3.m2.1.1.cmml" xref="S4.E3.m2.1.1">𝑥</ci><ci id="S4.E3.m2.2.2.cmml" xref="S4.E3.m2.2.2">𝑙</ci><ci id="S4.E3.m2.3.3.cmml" xref="S4.E3.m2.3.3">𝑦</ci></list></apply></apply><apply id="S4.E3.m2.6.6.1.1.2.cmml" xref="S4.E3.m2.6.6.1.1.2"><times id="S4.E3.m2.6.6.1.1.2.2.cmml" xref="S4.E3.m2.6.6.1.1.2.2"></times><ci id="S4.E3.m2.6.6.1.1.2.3.cmml" xref="S4.E3.m2.6.6.1.1.2.3">𝑝</ci><apply id="S4.E3.m2.6.6.1.1.2.1.1.1.cmml" xref="S4.E3.m2.6.6.1.1.2.1.1"><csymbol cd="latexml" id="S4.E3.m2.6.6.1.1.2.1.1.1.2.cmml" xref="S4.E3.m2.6.6.1.1.2.1.1.1.2">conditional</csymbol><ci id="S4.E3.m2.6.6.1.1.2.1.1.1.3.cmml" xref="S4.E3.m2.6.6.1.1.2.1.1.1.3">𝐹</ci><list id="S4.E3.m2.6.6.1.1.2.1.1.1.1.2.cmml" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.1"><ci id="S4.E3.m2.4.4.cmml" xref="S4.E3.m2.4.4">𝑥</ci><apply id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.cmml" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.2">𝑙</ci><ci id="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.6.6.1.1.2.1.1.1.1.1.1.3">′</ci></apply><ci id="S4.E3.m2.5.5.cmml" xref="S4.E3.m2.5.5">𝑦</ci></list></apply></apply></apply><apply id="S4.E3.m2.7.7.2.2.cmml" xref="S4.E3.m2.7.7.2.2"><in id="S4.E3.m2.7.7.2.2.3.cmml" xref="S4.E3.m2.7.7.2.2.3"></in><apply id="S4.E3.m2.7.7.2.2.4.cmml" xref="S4.E3.m2.7.7.2.2.4"><csymbol cd="latexml" id="S4.E3.m2.7.7.2.2.4.1.cmml" xref="S4.E3.m2.7.7.2.2.4.1">for-all</csymbol><apply id="S4.E3.m2.7.7.2.2.4.2.cmml" xref="S4.E3.m2.7.7.2.2.4.2"><csymbol cd="ambiguous" id="S4.E3.m2.7.7.2.2.4.2.1.cmml" xref="S4.E3.m2.7.7.2.2.4.2">superscript</csymbol><ci id="S4.E3.m2.7.7.2.2.4.2.2.cmml" xref="S4.E3.m2.7.7.2.2.4.2.2">𝑙</ci><ci id="S4.E3.m2.7.7.2.2.4.2.3.cmml" xref="S4.E3.m2.7.7.2.2.4.2.3">′</ci></apply></apply><apply id="S4.E3.m2.7.7.2.2.2.3.cmml" xref="S4.E3.m2.7.7.2.2.2.2"><csymbol cd="latexml" id="S4.E3.m2.7.7.2.2.2.3.1.cmml" xref="S4.E3.m2.7.7.2.2.2.2.3">conditional-set</csymbol><apply id="S4.E3.m2.7.7.2.2.1.1.1.cmml" xref="S4.E3.m2.7.7.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.7.7.2.2.1.1.1.1.cmml" xref="S4.E3.m2.7.7.2.2.1.1.1">superscript</csymbol><ci id="S4.E3.m2.7.7.2.2.1.1.1.2.cmml" xref="S4.E3.m2.7.7.2.2.1.1.1.2">𝑙</ci><ci id="S4.E3.m2.7.7.2.2.1.1.1.3.cmml" xref="S4.E3.m2.7.7.2.2.1.1.1.3">′</ci></apply><apply id="S4.E3.m2.7.7.2.2.2.2.2.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2"><and id="S4.E3.m2.7.7.2.2.2.2.2a.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2"></and><apply id="S4.E3.m2.7.7.2.2.2.2.2b.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2"><lt id="S4.E3.m2.7.7.2.2.2.2.2.4.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.4"></lt><cn type="integer" id="S4.E3.m2.7.7.2.2.2.2.2.3.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.3">0</cn><apply id="S4.E3.m2.7.7.2.2.2.2.2.1.2.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1"><abs id="S4.E3.m2.7.7.2.2.2.2.2.1.2.1.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.2"></abs><apply id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1"><minus id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.1"></minus><apply id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2">superscript</csymbol><ci id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.2.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.2">𝑙</ci><ci id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.3.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.2.3">′</ci></apply><ci id="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.3.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.1.1.1.3">𝑙</ci></apply></apply></apply><apply id="S4.E3.m2.7.7.2.2.2.2.2c.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2"><lt id="S4.E3.m2.7.7.2.2.2.2.2.5.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.5"></lt><share href="#S4.E3.m2.7.7.2.2.2.2.2.1.cmml" id="S4.E3.m2.7.7.2.2.2.2.2d.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2"></share><ci id="S4.E3.m2.7.7.2.2.2.2.2.6.cmml" xref="S4.E3.m2.7.7.2.2.2.2.2.6">𝑟</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m2.7c">\displaystyle p(F\mid x,l,y)&gt;p(F\mid x,l^{\prime},y),\;\;\forall l^{\prime}\in\{l^{\prime}\mid 0&lt;\lvert l^{\prime}-l\rvert&lt;r\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S4.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E4.m1.1" class="ltx_Math" alttext="\displaystyle and\;\;" display="inline"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><mi id="S4.E4.m1.1.1.2" xref="S4.E4.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml">​</mo><mi id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.1a" xref="S4.E4.m1.1.1.1.cmml">​</mo><mi id="S4.E4.m1.1.1.4" xref="S4.E4.m1.1.1.4.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><times id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1"></times><ci id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1.2">𝑎</ci><ci id="S4.E4.m1.1.1.3.cmml" xref="S4.E4.m1.1.1.3">𝑛</ci><ci id="S4.E4.m1.1.1.4.cmml" xref="S4.E4.m1.1.1.4">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">\displaystyle and\;\;</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E4.m2.7" class="ltx_Math" alttext="\displaystyle p(F\mid x,l,y)&gt;p(F\mid x,l,y^{\prime}),\;\;\forall y^{\prime}\neq y\in\mathcal{Y}\vspace{-0.2em}" display="inline"><semantics id="S4.E4.m2.7a"><mrow id="S4.E4.m2.7.7.2" xref="S4.E4.m2.7.7.3.cmml"><mrow id="S4.E4.m2.6.6.1.1" xref="S4.E4.m2.6.6.1.1.cmml"><mrow id="S4.E4.m2.6.6.1.1.1" xref="S4.E4.m2.6.6.1.1.1.cmml"><mi id="S4.E4.m2.6.6.1.1.1.3" xref="S4.E4.m2.6.6.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.6.6.1.1.1.2" xref="S4.E4.m2.6.6.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m2.6.6.1.1.1.1.1" xref="S4.E4.m2.6.6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m2.6.6.1.1.1.1.1.2" xref="S4.E4.m2.6.6.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m2.6.6.1.1.1.1.1.1" xref="S4.E4.m2.6.6.1.1.1.1.1.1.cmml"><mi id="S4.E4.m2.6.6.1.1.1.1.1.1.2" xref="S4.E4.m2.6.6.1.1.1.1.1.1.2.cmml">F</mi><mo id="S4.E4.m2.6.6.1.1.1.1.1.1.1" xref="S4.E4.m2.6.6.1.1.1.1.1.1.1.cmml">∣</mo><mrow id="S4.E4.m2.6.6.1.1.1.1.1.1.3.2" xref="S4.E4.m2.6.6.1.1.1.1.1.1.3.1.cmml"><mi id="S4.E4.m2.1.1" xref="S4.E4.m2.1.1.cmml">x</mi><mo id="S4.E4.m2.6.6.1.1.1.1.1.1.3.2.1" xref="S4.E4.m2.6.6.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E4.m2.2.2" xref="S4.E4.m2.2.2.cmml">l</mi><mo id="S4.E4.m2.6.6.1.1.1.1.1.1.3.2.2" xref="S4.E4.m2.6.6.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E4.m2.3.3" xref="S4.E4.m2.3.3.cmml">y</mi></mrow></mrow><mo stretchy="false" id="S4.E4.m2.6.6.1.1.1.1.1.3" xref="S4.E4.m2.6.6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E4.m2.6.6.1.1.3" xref="S4.E4.m2.6.6.1.1.3.cmml">&gt;</mo><mrow id="S4.E4.m2.6.6.1.1.2" xref="S4.E4.m2.6.6.1.1.2.cmml"><mi id="S4.E4.m2.6.6.1.1.2.3" xref="S4.E4.m2.6.6.1.1.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E4.m2.6.6.1.1.2.2" xref="S4.E4.m2.6.6.1.1.2.2.cmml">​</mo><mrow id="S4.E4.m2.6.6.1.1.2.1.1" xref="S4.E4.m2.6.6.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m2.6.6.1.1.2.1.1.2" xref="S4.E4.m2.6.6.1.1.2.1.1.1.cmml">(</mo><mrow id="S4.E4.m2.6.6.1.1.2.1.1.1" xref="S4.E4.m2.6.6.1.1.2.1.1.1.cmml"><mi id="S4.E4.m2.6.6.1.1.2.1.1.1.3" xref="S4.E4.m2.6.6.1.1.2.1.1.1.3.cmml">F</mi><mo id="S4.E4.m2.6.6.1.1.2.1.1.1.2" xref="S4.E4.m2.6.6.1.1.2.1.1.1.2.cmml">∣</mo><mrow id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.2.cmml"><mi id="S4.E4.m2.4.4" xref="S4.E4.m2.4.4.cmml">x</mi><mo id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.2" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.2.cmml">,</mo><mi id="S4.E4.m2.5.5" xref="S4.E4.m2.5.5.cmml">l</mi><mo id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.3" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.2.cmml">,</mo><msup id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.cmml"><mi id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.2" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.2.cmml">y</mi><mo id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.3" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.3.cmml">′</mo></msup></mrow></mrow><mo stretchy="false" id="S4.E4.m2.6.6.1.1.2.1.1.3" xref="S4.E4.m2.6.6.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.727em" id="S4.E4.m2.7.7.2.3" xref="S4.E4.m2.7.7.3a.cmml">,</mo><mrow id="S4.E4.m2.7.7.2.2" xref="S4.E4.m2.7.7.2.2.cmml"><mrow id="S4.E4.m2.7.7.2.2.2" xref="S4.E4.m2.7.7.2.2.2.cmml"><mo rspace="0.167em" id="S4.E4.m2.7.7.2.2.2.1" xref="S4.E4.m2.7.7.2.2.2.1.cmml">∀</mo><msup id="S4.E4.m2.7.7.2.2.2.2" xref="S4.E4.m2.7.7.2.2.2.2.cmml"><mi id="S4.E4.m2.7.7.2.2.2.2.2" xref="S4.E4.m2.7.7.2.2.2.2.2.cmml">y</mi><mo id="S4.E4.m2.7.7.2.2.2.2.3" xref="S4.E4.m2.7.7.2.2.2.2.3.cmml">′</mo></msup></mrow><mo id="S4.E4.m2.7.7.2.2.3" xref="S4.E4.m2.7.7.2.2.3.cmml">≠</mo><mi id="S4.E4.m2.7.7.2.2.4" xref="S4.E4.m2.7.7.2.2.4.cmml">y</mi><mo id="S4.E4.m2.7.7.2.2.5" xref="S4.E4.m2.7.7.2.2.5.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.E4.m2.7.7.2.2.6" xref="S4.E4.m2.7.7.2.2.6.cmml">𝒴</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m2.7b"><apply id="S4.E4.m2.7.7.3.cmml" xref="S4.E4.m2.7.7.2"><csymbol cd="ambiguous" id="S4.E4.m2.7.7.3a.cmml" xref="S4.E4.m2.7.7.2.3">formulae-sequence</csymbol><apply id="S4.E4.m2.6.6.1.1.cmml" xref="S4.E4.m2.6.6.1.1"><gt id="S4.E4.m2.6.6.1.1.3.cmml" xref="S4.E4.m2.6.6.1.1.3"></gt><apply id="S4.E4.m2.6.6.1.1.1.cmml" xref="S4.E4.m2.6.6.1.1.1"><times id="S4.E4.m2.6.6.1.1.1.2.cmml" xref="S4.E4.m2.6.6.1.1.1.2"></times><ci id="S4.E4.m2.6.6.1.1.1.3.cmml" xref="S4.E4.m2.6.6.1.1.1.3">𝑝</ci><apply id="S4.E4.m2.6.6.1.1.1.1.1.1.cmml" xref="S4.E4.m2.6.6.1.1.1.1.1"><csymbol cd="latexml" id="S4.E4.m2.6.6.1.1.1.1.1.1.1.cmml" xref="S4.E4.m2.6.6.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E4.m2.6.6.1.1.1.1.1.1.2.cmml" xref="S4.E4.m2.6.6.1.1.1.1.1.1.2">𝐹</ci><list id="S4.E4.m2.6.6.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m2.6.6.1.1.1.1.1.1.3.2"><ci id="S4.E4.m2.1.1.cmml" xref="S4.E4.m2.1.1">𝑥</ci><ci id="S4.E4.m2.2.2.cmml" xref="S4.E4.m2.2.2">𝑙</ci><ci id="S4.E4.m2.3.3.cmml" xref="S4.E4.m2.3.3">𝑦</ci></list></apply></apply><apply id="S4.E4.m2.6.6.1.1.2.cmml" xref="S4.E4.m2.6.6.1.1.2"><times id="S4.E4.m2.6.6.1.1.2.2.cmml" xref="S4.E4.m2.6.6.1.1.2.2"></times><ci id="S4.E4.m2.6.6.1.1.2.3.cmml" xref="S4.E4.m2.6.6.1.1.2.3">𝑝</ci><apply id="S4.E4.m2.6.6.1.1.2.1.1.1.cmml" xref="S4.E4.m2.6.6.1.1.2.1.1"><csymbol cd="latexml" id="S4.E4.m2.6.6.1.1.2.1.1.1.2.cmml" xref="S4.E4.m2.6.6.1.1.2.1.1.1.2">conditional</csymbol><ci id="S4.E4.m2.6.6.1.1.2.1.1.1.3.cmml" xref="S4.E4.m2.6.6.1.1.2.1.1.1.3">𝐹</ci><list id="S4.E4.m2.6.6.1.1.2.1.1.1.1.2.cmml" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.1"><ci id="S4.E4.m2.4.4.cmml" xref="S4.E4.m2.4.4">𝑥</ci><ci id="S4.E4.m2.5.5.cmml" xref="S4.E4.m2.5.5">𝑙</ci><apply id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.cmml" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.1.cmml" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.2.cmml" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.2">𝑦</ci><ci id="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.3.cmml" xref="S4.E4.m2.6.6.1.1.2.1.1.1.1.1.1.3">′</ci></apply></list></apply></apply></apply><apply id="S4.E4.m2.7.7.2.2.cmml" xref="S4.E4.m2.7.7.2.2"><and id="S4.E4.m2.7.7.2.2a.cmml" xref="S4.E4.m2.7.7.2.2"></and><apply id="S4.E4.m2.7.7.2.2b.cmml" xref="S4.E4.m2.7.7.2.2"><neq id="S4.E4.m2.7.7.2.2.3.cmml" xref="S4.E4.m2.7.7.2.2.3"></neq><apply id="S4.E4.m2.7.7.2.2.2.cmml" xref="S4.E4.m2.7.7.2.2.2"><csymbol cd="latexml" id="S4.E4.m2.7.7.2.2.2.1.cmml" xref="S4.E4.m2.7.7.2.2.2.1">for-all</csymbol><apply id="S4.E4.m2.7.7.2.2.2.2.cmml" xref="S4.E4.m2.7.7.2.2.2.2"><csymbol cd="ambiguous" id="S4.E4.m2.7.7.2.2.2.2.1.cmml" xref="S4.E4.m2.7.7.2.2.2.2">superscript</csymbol><ci id="S4.E4.m2.7.7.2.2.2.2.2.cmml" xref="S4.E4.m2.7.7.2.2.2.2.2">𝑦</ci><ci id="S4.E4.m2.7.7.2.2.2.2.3.cmml" xref="S4.E4.m2.7.7.2.2.2.2.3">′</ci></apply></apply><ci id="S4.E4.m2.7.7.2.2.4.cmml" xref="S4.E4.m2.7.7.2.2.4">𝑦</ci></apply><apply id="S4.E4.m2.7.7.2.2c.cmml" xref="S4.E4.m2.7.7.2.2"><in id="S4.E4.m2.7.7.2.2.5.cmml" xref="S4.E4.m2.7.7.2.2.5"></in><share href="#S4.E4.m2.7.7.2.2.4.cmml" id="S4.E4.m2.7.7.2.2d.cmml" xref="S4.E4.m2.7.7.2.2"></share><ci id="S4.E4.m2.7.7.2.2.6.cmml" xref="S4.E4.m2.7.7.2.2.6">𝒴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m2.7c">\displaystyle p(F\mid x,l,y)&gt;p(F\mid x,l,y^{\prime}),\;\;\forall y^{\prime}\neq y\in\mathcal{Y}\vspace{-0.2em}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.10" class="ltx_p"><span id="S4.SS2.p9.10.1" class="ltx_text ltx_font_bold">Dense scene parsing with greedy proposal generation.</span>
Typically, object detection in complex scenes requires well chosen thresholds and detection hyperparameters.
Our render-and-compare approach enables us to avoid tedious hyperparameter tuning by adopting a greedy approach to maximize the model likelihood (<a href="#S4.E1" title="In 4.2 Multi-class 6D Scene Parsing ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) using a greedy proposal strategy.
In particular, we optimize the likelihood greedily by starting from the object proposal that explains away the most parts of the image with highest likelihood, and subsequently update the likelihood of the overlapping proposals taking into account, that at every pixel in the feature map only one object can be visible <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib56" title="" class="ltx_ref">yuan2021robust, </a>)</cite>.
Formally, given a list of objects proposals <math id="S4.SS2.p9.1.m1.5" class="ltx_Math" alttext="\{o_{i}=(O_{y,i},\alpha_{y,i})\}_{i=1}^{k}" display="inline"><semantics id="S4.SS2.p9.1.m1.5a"><msubsup id="S4.SS2.p9.1.m1.5.5" xref="S4.SS2.p9.1.m1.5.5.cmml"><mrow id="S4.SS2.p9.1.m1.5.5.1.1.1" xref="S4.SS2.p9.1.m1.5.5.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p9.1.m1.5.5.1.1.1.2" xref="S4.SS2.p9.1.m1.5.5.1.1.2.cmml">{</mo><mrow id="S4.SS2.p9.1.m1.5.5.1.1.1.1" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.cmml"><msub id="S4.SS2.p9.1.m1.5.5.1.1.1.1.4" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.cmml"><mi id="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.2" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.2.cmml">o</mi><mi id="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.3" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.3.cmml">i</mi></msub><mo id="S4.SS2.p9.1.m1.5.5.1.1.1.1.3" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.3.cmml">=</mo><mrow id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.3" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.3.cmml">(</mo><msub id="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1.2" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1.2.cmml">O</mi><mrow id="S4.SS2.p9.1.m1.2.2.2.4" xref="S4.SS2.p9.1.m1.2.2.2.3.cmml"><mi id="S4.SS2.p9.1.m1.1.1.1.1" xref="S4.SS2.p9.1.m1.1.1.1.1.cmml">y</mi><mo id="S4.SS2.p9.1.m1.2.2.2.4.1" xref="S4.SS2.p9.1.m1.2.2.2.3.cmml">,</mo><mi id="S4.SS2.p9.1.m1.2.2.2.2" xref="S4.SS2.p9.1.m1.2.2.2.2.cmml">i</mi></mrow></msub><mo id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.4" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.3.cmml">,</mo><msub id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2.cmml"><mi id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2.2" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2.2.cmml">α</mi><mrow id="S4.SS2.p9.1.m1.4.4.2.4" xref="S4.SS2.p9.1.m1.4.4.2.3.cmml"><mi id="S4.SS2.p9.1.m1.3.3.1.1" xref="S4.SS2.p9.1.m1.3.3.1.1.cmml">y</mi><mo id="S4.SS2.p9.1.m1.4.4.2.4.1" xref="S4.SS2.p9.1.m1.4.4.2.3.cmml">,</mo><mi id="S4.SS2.p9.1.m1.4.4.2.2" xref="S4.SS2.p9.1.m1.4.4.2.2.cmml">i</mi></mrow></msub><mo stretchy="false" id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.5" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p9.1.m1.5.5.1.1.1.3" xref="S4.SS2.p9.1.m1.5.5.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS2.p9.1.m1.5.5.1.3" xref="S4.SS2.p9.1.m1.5.5.1.3.cmml"><mi id="S4.SS2.p9.1.m1.5.5.1.3.2" xref="S4.SS2.p9.1.m1.5.5.1.3.2.cmml">i</mi><mo id="S4.SS2.p9.1.m1.5.5.1.3.1" xref="S4.SS2.p9.1.m1.5.5.1.3.1.cmml">=</mo><mn id="S4.SS2.p9.1.m1.5.5.1.3.3" xref="S4.SS2.p9.1.m1.5.5.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p9.1.m1.5.5.3" xref="S4.SS2.p9.1.m1.5.5.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.1.m1.5b"><apply id="S4.SS2.p9.1.m1.5.5.cmml" xref="S4.SS2.p9.1.m1.5.5"><csymbol cd="ambiguous" id="S4.SS2.p9.1.m1.5.5.2.cmml" xref="S4.SS2.p9.1.m1.5.5">superscript</csymbol><apply id="S4.SS2.p9.1.m1.5.5.1.cmml" xref="S4.SS2.p9.1.m1.5.5"><csymbol cd="ambiguous" id="S4.SS2.p9.1.m1.5.5.1.2.cmml" xref="S4.SS2.p9.1.m1.5.5">subscript</csymbol><set id="S4.SS2.p9.1.m1.5.5.1.1.2.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1"><apply id="S4.SS2.p9.1.m1.5.5.1.1.1.1.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1"><eq id="S4.SS2.p9.1.m1.5.5.1.1.1.1.3.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.3"></eq><apply id="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.1.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.4">subscript</csymbol><ci id="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.2.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.2">𝑜</ci><ci id="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.3.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.4.3">𝑖</ci></apply><interval closure="open" id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.3.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2"><apply id="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.1.1.1.2">𝑂</ci><list id="S4.SS2.p9.1.m1.2.2.2.3.cmml" xref="S4.SS2.p9.1.m1.2.2.2.4"><ci id="S4.SS2.p9.1.m1.1.1.1.1.cmml" xref="S4.SS2.p9.1.m1.1.1.1.1">𝑦</ci><ci id="S4.SS2.p9.1.m1.2.2.2.2.cmml" xref="S4.SS2.p9.1.m1.2.2.2.2">𝑖</ci></list></apply><apply id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2.1.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2.2.cmml" xref="S4.SS2.p9.1.m1.5.5.1.1.1.1.2.2.2.2">𝛼</ci><list id="S4.SS2.p9.1.m1.4.4.2.3.cmml" xref="S4.SS2.p9.1.m1.4.4.2.4"><ci id="S4.SS2.p9.1.m1.3.3.1.1.cmml" xref="S4.SS2.p9.1.m1.3.3.1.1">𝑦</ci><ci id="S4.SS2.p9.1.m1.4.4.2.2.cmml" xref="S4.SS2.p9.1.m1.4.4.2.2">𝑖</ci></list></apply></interval></apply></set><apply id="S4.SS2.p9.1.m1.5.5.1.3.cmml" xref="S4.SS2.p9.1.m1.5.5.1.3"><eq id="S4.SS2.p9.1.m1.5.5.1.3.1.cmml" xref="S4.SS2.p9.1.m1.5.5.1.3.1"></eq><ci id="S4.SS2.p9.1.m1.5.5.1.3.2.cmml" xref="S4.SS2.p9.1.m1.5.5.1.3.2">𝑖</ci><cn type="integer" id="S4.SS2.p9.1.m1.5.5.1.3.3.cmml" xref="S4.SS2.p9.1.m1.5.5.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p9.1.m1.5.5.3.cmml" xref="S4.SS2.p9.1.m1.5.5.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.1.m1.5c">\{o_{i}=(O_{y,i},\alpha_{y,i})\}_{i=1}^{k}</annotation></semantics></math> (with predicted category label <math id="S4.SS2.p9.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p9.2.m2.1a"><mi id="S4.SS2.p9.2.m2.1.1" xref="S4.SS2.p9.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.2.m2.1b"><ci id="S4.SS2.p9.2.m2.1.1.cmml" xref="S4.SS2.p9.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.2.m2.1c">y</annotation></semantics></math> and 6D pose <math id="S4.SS2.p9.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.p9.3.m3.1a"><mi id="S4.SS2.p9.3.m3.1.1" xref="S4.SS2.p9.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.3.m3.1b"><ci id="S4.SS2.p9.3.m3.1.1.cmml" xref="S4.SS2.p9.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.3.m3.1c">\alpha</annotation></semantics></math>), we first order the object proposals based on their likelihood score <math id="S4.SS2.p9.4.m4.2" class="ltx_Math" alttext="s=p(F|o_{i},B)" display="inline"><semantics id="S4.SS2.p9.4.m4.2a"><mrow id="S4.SS2.p9.4.m4.2.2" xref="S4.SS2.p9.4.m4.2.2.cmml"><mi id="S4.SS2.p9.4.m4.2.2.3" xref="S4.SS2.p9.4.m4.2.2.3.cmml">s</mi><mo id="S4.SS2.p9.4.m4.2.2.2" xref="S4.SS2.p9.4.m4.2.2.2.cmml">=</mo><mrow id="S4.SS2.p9.4.m4.2.2.1" xref="S4.SS2.p9.4.m4.2.2.1.cmml"><mi id="S4.SS2.p9.4.m4.2.2.1.3" xref="S4.SS2.p9.4.m4.2.2.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p9.4.m4.2.2.1.2" xref="S4.SS2.p9.4.m4.2.2.1.2.cmml">​</mo><mrow id="S4.SS2.p9.4.m4.2.2.1.1.1" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p9.4.m4.2.2.1.1.1.2" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p9.4.m4.2.2.1.1.1.1" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.cmml"><mi id="S4.SS2.p9.4.m4.2.2.1.1.1.1.3" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.3.cmml">F</mi><mo fence="false" id="S4.SS2.p9.4.m4.2.2.1.1.1.1.2" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.2.cmml">|</mo><mrow id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.2.cmml"><msub id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.2" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.2.cmml">o</mi><mi id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.3" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.2" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.2.cmml">,</mo><mi id="S4.SS2.p9.4.m4.1.1" xref="S4.SS2.p9.4.m4.1.1.cmml">B</mi></mrow></mrow><mo stretchy="false" id="S4.SS2.p9.4.m4.2.2.1.1.1.3" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.4.m4.2b"><apply id="S4.SS2.p9.4.m4.2.2.cmml" xref="S4.SS2.p9.4.m4.2.2"><eq id="S4.SS2.p9.4.m4.2.2.2.cmml" xref="S4.SS2.p9.4.m4.2.2.2"></eq><ci id="S4.SS2.p9.4.m4.2.2.3.cmml" xref="S4.SS2.p9.4.m4.2.2.3">𝑠</ci><apply id="S4.SS2.p9.4.m4.2.2.1.cmml" xref="S4.SS2.p9.4.m4.2.2.1"><times id="S4.SS2.p9.4.m4.2.2.1.2.cmml" xref="S4.SS2.p9.4.m4.2.2.1.2"></times><ci id="S4.SS2.p9.4.m4.2.2.1.3.cmml" xref="S4.SS2.p9.4.m4.2.2.1.3">𝑝</ci><apply id="S4.SS2.p9.4.m4.2.2.1.1.1.1.cmml" xref="S4.SS2.p9.4.m4.2.2.1.1.1"><csymbol cd="latexml" id="S4.SS2.p9.4.m4.2.2.1.1.1.1.2.cmml" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.2">conditional</csymbol><ci id="S4.SS2.p9.4.m4.2.2.1.1.1.1.3.cmml" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.3">𝐹</ci><list id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.2.cmml" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1"><apply id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.2">𝑜</ci><ci id="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p9.4.m4.2.2.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S4.SS2.p9.4.m4.1.1.cmml" xref="S4.SS2.p9.4.m4.1.1">𝐵</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.4.m4.2c">s=p(F|o_{i},B)</annotation></semantics></math> such that <math id="S4.SS2.p9.5.m5.1" class="ltx_Math" alttext="s_{i}\leq s_{j}" display="inline"><semantics id="S4.SS2.p9.5.m5.1a"><mrow id="S4.SS2.p9.5.m5.1.1" xref="S4.SS2.p9.5.m5.1.1.cmml"><msub id="S4.SS2.p9.5.m5.1.1.2" xref="S4.SS2.p9.5.m5.1.1.2.cmml"><mi id="S4.SS2.p9.5.m5.1.1.2.2" xref="S4.SS2.p9.5.m5.1.1.2.2.cmml">s</mi><mi id="S4.SS2.p9.5.m5.1.1.2.3" xref="S4.SS2.p9.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.p9.5.m5.1.1.1" xref="S4.SS2.p9.5.m5.1.1.1.cmml">≤</mo><msub id="S4.SS2.p9.5.m5.1.1.3" xref="S4.SS2.p9.5.m5.1.1.3.cmml"><mi id="S4.SS2.p9.5.m5.1.1.3.2" xref="S4.SS2.p9.5.m5.1.1.3.2.cmml">s</mi><mi id="S4.SS2.p9.5.m5.1.1.3.3" xref="S4.SS2.p9.5.m5.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.5.m5.1b"><apply id="S4.SS2.p9.5.m5.1.1.cmml" xref="S4.SS2.p9.5.m5.1.1"><leq id="S4.SS2.p9.5.m5.1.1.1.cmml" xref="S4.SS2.p9.5.m5.1.1.1"></leq><apply id="S4.SS2.p9.5.m5.1.1.2.cmml" xref="S4.SS2.p9.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p9.5.m5.1.1.2.1.cmml" xref="S4.SS2.p9.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS2.p9.5.m5.1.1.2.2.cmml" xref="S4.SS2.p9.5.m5.1.1.2.2">𝑠</ci><ci id="S4.SS2.p9.5.m5.1.1.2.3.cmml" xref="S4.SS2.p9.5.m5.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.p9.5.m5.1.1.3.cmml" xref="S4.SS2.p9.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p9.5.m5.1.1.3.1.cmml" xref="S4.SS2.p9.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS2.p9.5.m5.1.1.3.2.cmml" xref="S4.SS2.p9.5.m5.1.1.3.2">𝑠</ci><ci id="S4.SS2.p9.5.m5.1.1.3.3.cmml" xref="S4.SS2.p9.5.m5.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.5.m5.1c">s_{i}\leq s_{j}</annotation></semantics></math> for <math id="S4.SS2.p9.6.m6.1" class="ltx_Math" alttext="i&lt;j" display="inline"><semantics id="S4.SS2.p9.6.m6.1a"><mrow id="S4.SS2.p9.6.m6.1.1" xref="S4.SS2.p9.6.m6.1.1.cmml"><mi id="S4.SS2.p9.6.m6.1.1.2" xref="S4.SS2.p9.6.m6.1.1.2.cmml">i</mi><mo id="S4.SS2.p9.6.m6.1.1.1" xref="S4.SS2.p9.6.m6.1.1.1.cmml">&lt;</mo><mi id="S4.SS2.p9.6.m6.1.1.3" xref="S4.SS2.p9.6.m6.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.6.m6.1b"><apply id="S4.SS2.p9.6.m6.1.1.cmml" xref="S4.SS2.p9.6.m6.1.1"><lt id="S4.SS2.p9.6.m6.1.1.1.cmml" xref="S4.SS2.p9.6.m6.1.1.1"></lt><ci id="S4.SS2.p9.6.m6.1.1.2.cmml" xref="S4.SS2.p9.6.m6.1.1.2">𝑖</ci><ci id="S4.SS2.p9.6.m6.1.1.3.cmml" xref="S4.SS2.p9.6.m6.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.6.m6.1c">i&lt;j</annotation></semantics></math>. Based on the ordering, we greedily update the 6D pose <math id="S4.SS2.p9.7.m7.1" class="ltx_Math" alttext="\alpha_{j}" display="inline"><semantics id="S4.SS2.p9.7.m7.1a"><msub id="S4.SS2.p9.7.m7.1.1" xref="S4.SS2.p9.7.m7.1.1.cmml"><mi id="S4.SS2.p9.7.m7.1.1.2" xref="S4.SS2.p9.7.m7.1.1.2.cmml">α</mi><mi id="S4.SS2.p9.7.m7.1.1.3" xref="S4.SS2.p9.7.m7.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.7.m7.1b"><apply id="S4.SS2.p9.7.m7.1.1.cmml" xref="S4.SS2.p9.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.7.m7.1.1.1.cmml" xref="S4.SS2.p9.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.p9.7.m7.1.1.2.cmml" xref="S4.SS2.p9.7.m7.1.1.2">𝛼</ci><ci id="S4.SS2.p9.7.m7.1.1.3.cmml" xref="S4.SS2.p9.7.m7.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.7.m7.1c">\alpha_{j}</annotation></semantics></math> and the corresponding proposal likelihood for object <math id="S4.SS2.p9.8.m8.1" class="ltx_Math" alttext="o_{j}" display="inline"><semantics id="S4.SS2.p9.8.m8.1a"><msub id="S4.SS2.p9.8.m8.1.1" xref="S4.SS2.p9.8.m8.1.1.cmml"><mi id="S4.SS2.p9.8.m8.1.1.2" xref="S4.SS2.p9.8.m8.1.1.2.cmml">o</mi><mi id="S4.SS2.p9.8.m8.1.1.3" xref="S4.SS2.p9.8.m8.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.8.m8.1b"><apply id="S4.SS2.p9.8.m8.1.1.cmml" xref="S4.SS2.p9.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.8.m8.1.1.1.cmml" xref="S4.SS2.p9.8.m8.1.1">subscript</csymbol><ci id="S4.SS2.p9.8.m8.1.1.2.cmml" xref="S4.SS2.p9.8.m8.1.1.2">𝑜</ci><ci id="S4.SS2.p9.8.m8.1.1.3.cmml" xref="S4.SS2.p9.8.m8.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.8.m8.1c">o_{j}</annotation></semantics></math> by masking out the foreground regions of previous objects <math id="S4.SS2.p9.9.m9.1" class="ltx_Math" alttext="o_{i}" display="inline"><semantics id="S4.SS2.p9.9.m9.1a"><msub id="S4.SS2.p9.9.m9.1.1" xref="S4.SS2.p9.9.m9.1.1.cmml"><mi id="S4.SS2.p9.9.m9.1.1.2" xref="S4.SS2.p9.9.m9.1.1.2.cmml">o</mi><mi id="S4.SS2.p9.9.m9.1.1.3" xref="S4.SS2.p9.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.9.m9.1b"><apply id="S4.SS2.p9.9.m9.1.1.cmml" xref="S4.SS2.p9.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.9.m9.1.1.1.cmml" xref="S4.SS2.p9.9.m9.1.1">subscript</csymbol><ci id="S4.SS2.p9.9.m9.1.1.2.cmml" xref="S4.SS2.p9.9.m9.1.1.2">𝑜</ci><ci id="S4.SS2.p9.9.m9.1.1.3.cmml" xref="S4.SS2.p9.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.9.m9.1c">o_{i}</annotation></semantics></math> with <math id="S4.SS2.p9.10.m10.1" class="ltx_Math" alttext="1\leq i\leq j-1" display="inline"><semantics id="S4.SS2.p9.10.m10.1a"><mrow id="S4.SS2.p9.10.m10.1.1" xref="S4.SS2.p9.10.m10.1.1.cmml"><mn id="S4.SS2.p9.10.m10.1.1.2" xref="S4.SS2.p9.10.m10.1.1.2.cmml">1</mn><mo id="S4.SS2.p9.10.m10.1.1.3" xref="S4.SS2.p9.10.m10.1.1.3.cmml">≤</mo><mi id="S4.SS2.p9.10.m10.1.1.4" xref="S4.SS2.p9.10.m10.1.1.4.cmml">i</mi><mo id="S4.SS2.p9.10.m10.1.1.5" xref="S4.SS2.p9.10.m10.1.1.5.cmml">≤</mo><mrow id="S4.SS2.p9.10.m10.1.1.6" xref="S4.SS2.p9.10.m10.1.1.6.cmml"><mi id="S4.SS2.p9.10.m10.1.1.6.2" xref="S4.SS2.p9.10.m10.1.1.6.2.cmml">j</mi><mo id="S4.SS2.p9.10.m10.1.1.6.1" xref="S4.SS2.p9.10.m10.1.1.6.1.cmml">−</mo><mn id="S4.SS2.p9.10.m10.1.1.6.3" xref="S4.SS2.p9.10.m10.1.1.6.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.10.m10.1b"><apply id="S4.SS2.p9.10.m10.1.1.cmml" xref="S4.SS2.p9.10.m10.1.1"><and id="S4.SS2.p9.10.m10.1.1a.cmml" xref="S4.SS2.p9.10.m10.1.1"></and><apply id="S4.SS2.p9.10.m10.1.1b.cmml" xref="S4.SS2.p9.10.m10.1.1"><leq id="S4.SS2.p9.10.m10.1.1.3.cmml" xref="S4.SS2.p9.10.m10.1.1.3"></leq><cn type="integer" id="S4.SS2.p9.10.m10.1.1.2.cmml" xref="S4.SS2.p9.10.m10.1.1.2">1</cn><ci id="S4.SS2.p9.10.m10.1.1.4.cmml" xref="S4.SS2.p9.10.m10.1.1.4">𝑖</ci></apply><apply id="S4.SS2.p9.10.m10.1.1c.cmml" xref="S4.SS2.p9.10.m10.1.1"><leq id="S4.SS2.p9.10.m10.1.1.5.cmml" xref="S4.SS2.p9.10.m10.1.1.5"></leq><share href="#S4.SS2.p9.10.m10.1.1.4.cmml" id="S4.SS2.p9.10.m10.1.1d.cmml" xref="S4.SS2.p9.10.m10.1.1"></share><apply id="S4.SS2.p9.10.m10.1.1.6.cmml" xref="S4.SS2.p9.10.m10.1.1.6"><minus id="S4.SS2.p9.10.m10.1.1.6.1.cmml" xref="S4.SS2.p9.10.m10.1.1.6.1"></minus><ci id="S4.SS2.p9.10.m10.1.1.6.2.cmml" xref="S4.SS2.p9.10.m10.1.1.6.2">𝑗</ci><cn type="integer" id="S4.SS2.p9.10.m10.1.1.6.3.cmml" xref="S4.SS2.p9.10.m10.1.1.6.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.10.m10.1c">1\leq i\leq j-1</annotation></semantics></math>.
In this way, we can largely avoid missing close-by objects or duplicated detection.</p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.1" class="ltx_p"><span id="S4.SS2.p10.1.1" class="ltx_text ltx_font_bold">Part and attribute prediction.</span>
Given the predicted location and pose of each object, we project the object mesh back onto the image to get the locations for each part. To predict the attributes for the objects and parts, we crop the region containing the object or part from the RGB image, and train an additional CNN classifier using the cropped patches to predict the attributes (color, size, material) and the fine-grained classes (<em id="S4.SS2.p10.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS2.p10.1.3" class="ltx_text"></span> different sub-types of cars) of each patch using a cross-entropy loss. The reason why this additional CNN classifier is needed instead of re-using the features from the 6D pose estimator is that the pose estimation features are learned to be invariant to scale and texture changes, which makes it unsuitable for attribute prediction.</p>
</div>
<div id="S4.SS2.p11" class="ltx_para">
<p id="S4.SS2.p11.1" class="ltx_p"><span id="S4.SS2.p11.1.1" class="ltx_text ltx_font_bold">Post-filtering.</span>
Finally, we post-process the located objects using the fine-grained CNN classifier. We compare the category labels predicted by the 6D pose estimator with the ones predicted by the CNN classifier, and remove the objects for which these two predictions do not agree. This post-filtering step helps with the duplicated detections that cannot be fully resolved with the 3D-NMS.</p>
</div>
<div id="S4.SS2.p12" class="ltx_para">
<p id="S4.SS2.p12.1" class="ltx_p"><span id="S4.SS2.p12.1.1" class="ltx_text ltx_font_bold">Summary.</span> <a href="#S4.F2" title="In 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of our scene parser and <a href="#S4.F3" title="In 4.2 Multi-class 6D Scene Parsing ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> visualize the intermediate results. With the idea of render-and-compare (shown in the green box of <a href="#S4.F2" title="In 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>), the model first computes an activation map for each possible object category (<a href="#S4.F3" title="In 4.2 Multi-class 6D Scene Parsing ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>II). Next, to infer the category for each object, the category-wise competition 3D-NMS is performed (<a href="#S4.F3" title="In 4.2 Multi-class 6D Scene Parsing ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>b) and a post-filtering step is taken to remove mis-detected objects (<a href="#S4.F3" title="In 4.2 Multi-class 6D Scene Parsing ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>c). <a href="#S4.F3" title="In 4.2 Multi-class 6D Scene Parsing ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>d shows the 6D pose estimation results. To predict parts, we project the 3D object mesh back onto the image to locate parts based on projected objects (<a href="#S4.F3" title="In 4.2 Multi-class 6D Scene Parsing ‣ 4 Method ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>e). In this way, the input image can be parsed into a 3D-aware representation, which is ready for the question reasoning with program execution.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Program execution</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">After the 3D-aware scene representations are predicted for the given image, the question is parsed into a reasoning program, which is then executed on the scene representation to predict the answer. The question parsing follows previous work <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib54" title="" class="ltx_ref">nsvqa, </a>)</cite>, where a LSTM sequence-to-sequence model is trained to parse the question into its corresponding program. Like P-NSVQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite>, each operation in the program is executed on the scene representation in a probabilistic way. In the following, we describe the execution of the new operations we introduced.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.7" class="ltx_p">The part-related operators are implemented by querying the object-part hierarchy matrix <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">H</annotation></semantics></math>, so that the object containing a given part (<span id="S4.SS3.p2.7.1" class="ltx_text ltx_font_typewriter">part_to_object</span>) and the parts belonging to the given object (<span id="S4.SS3.p2.7.2" class="ltx_text ltx_font_typewriter">object_to_part</span>) can be determined. The pose-related operators are based on the estimated 3D pose in the object attributes <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="A^{o}" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><msup id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">A</mi><mi id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">o</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">𝐴</ci><ci id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">A^{o}</annotation></semantics></math>. For the <span id="S4.SS3.p2.7.3" class="ltx_text ltx_font_typewriter">filter</span> and <span id="S4.SS3.p2.7.4" class="ltx_text ltx_font_typewriter">query</span> operations regarding pose, the 3D poses are quantified into four direction (left, right, front, back).
For the pair-wise pose relationships, the azimuth angle between two objects is used to determine the same/opposite/vertical directions.
The occlusion-related operations are implemented by querying the occlusion matrix <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mi id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><ci id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">S</annotation></semantics></math>. Based on the occlusion scores <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="S_{ij}" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><msub id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mi id="S4.SS3.p2.4.m4.1.1.2" xref="S4.SS3.p2.4.m4.1.1.2.cmml">S</mi><mrow id="S4.SS3.p2.4.m4.1.1.3" xref="S4.SS3.p2.4.m4.1.1.3.cmml"><mi id="S4.SS3.p2.4.m4.1.1.3.2" xref="S4.SS3.p2.4.m4.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.4.m4.1.1.3.1" xref="S4.SS3.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS3.p2.4.m4.1.1.3.3" xref="S4.SS3.p2.4.m4.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS3.p2.4.m4.1.1.2.cmml" xref="S4.SS3.p2.4.m4.1.1.2">𝑆</ci><apply id="S4.SS3.p2.4.m4.1.1.3.cmml" xref="S4.SS3.p2.4.m4.1.1.3"><times id="S4.SS3.p2.4.m4.1.1.3.1.cmml" xref="S4.SS3.p2.4.m4.1.1.3.1"></times><ci id="S4.SS3.p2.4.m4.1.1.3.2.cmml" xref="S4.SS3.p2.4.m4.1.1.3.2">𝑖</ci><ci id="S4.SS3.p2.4.m4.1.1.3.3.cmml" xref="S4.SS3.p2.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">S_{ij}</annotation></semantics></math> representing whether entity <math id="S4.SS3.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS3.p2.5.m5.1a"><mi id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><ci id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">i</annotation></semantics></math> being occluded by entity <math id="S4.SS3.p2.6.m6.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS3.p2.6.m6.1a"><mi id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><ci id="S4.SS3.p2.6.m6.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">j</annotation></semantics></math>, we can compute the score of one entity being occluded <math id="S4.SS3.p2.7.m7.1" class="ltx_Math" alttext="\sum_{j}S_{ij}" display="inline"><semantics id="S4.SS3.p2.7.m7.1a"><mrow id="S4.SS3.p2.7.m7.1.1" xref="S4.SS3.p2.7.m7.1.1.cmml"><msub id="S4.SS3.p2.7.m7.1.1.1" xref="S4.SS3.p2.7.m7.1.1.1.cmml"><mo id="S4.SS3.p2.7.m7.1.1.1.2" xref="S4.SS3.p2.7.m7.1.1.1.2.cmml">∑</mo><mi id="S4.SS3.p2.7.m7.1.1.1.3" xref="S4.SS3.p2.7.m7.1.1.1.3.cmml">j</mi></msub><msub id="S4.SS3.p2.7.m7.1.1.2" xref="S4.SS3.p2.7.m7.1.1.2.cmml"><mi id="S4.SS3.p2.7.m7.1.1.2.2" xref="S4.SS3.p2.7.m7.1.1.2.2.cmml">S</mi><mrow id="S4.SS3.p2.7.m7.1.1.2.3" xref="S4.SS3.p2.7.m7.1.1.2.3.cmml"><mi id="S4.SS3.p2.7.m7.1.1.2.3.2" xref="S4.SS3.p2.7.m7.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.7.m7.1.1.2.3.1" xref="S4.SS3.p2.7.m7.1.1.2.3.1.cmml">​</mo><mi id="S4.SS3.p2.7.m7.1.1.2.3.3" xref="S4.SS3.p2.7.m7.1.1.2.3.3.cmml">j</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.m7.1b"><apply id="S4.SS3.p2.7.m7.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1"><apply id="S4.SS3.p2.7.m7.1.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.7.m7.1.1.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1.1">subscript</csymbol><sum id="S4.SS3.p2.7.m7.1.1.1.2.cmml" xref="S4.SS3.p2.7.m7.1.1.1.2"></sum><ci id="S4.SS3.p2.7.m7.1.1.1.3.cmml" xref="S4.SS3.p2.7.m7.1.1.1.3">𝑗</ci></apply><apply id="S4.SS3.p2.7.m7.1.1.2.cmml" xref="S4.SS3.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p2.7.m7.1.1.2.1.cmml" xref="S4.SS3.p2.7.m7.1.1.2">subscript</csymbol><ci id="S4.SS3.p2.7.m7.1.1.2.2.cmml" xref="S4.SS3.p2.7.m7.1.1.2.2">𝑆</ci><apply id="S4.SS3.p2.7.m7.1.1.2.3.cmml" xref="S4.SS3.p2.7.m7.1.1.2.3"><times id="S4.SS3.p2.7.m7.1.1.2.3.1.cmml" xref="S4.SS3.p2.7.m7.1.1.2.3.1"></times><ci id="S4.SS3.p2.7.m7.1.1.2.3.2.cmml" xref="S4.SS3.p2.7.m7.1.1.2.3.2">𝑖</ci><ci id="S4.SS3.p2.7.m7.1.1.2.3.3.cmml" xref="S4.SS3.p2.7.m7.1.1.2.3.3">𝑗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.m7.1c">\sum_{j}S_{ij}</annotation></semantics></math> (<span id="S4.SS3.p2.7.5" class="ltx_text ltx_font_typewriter">filter_occludee</span>), find the entities that occlude a given entity (<span id="S4.SS3.p2.7.6" class="ltx_text ltx_font_typewriter">relate_occluded</span>), or find the entities that are occluded by a given entity (<span id="S4.SS3.p2.7.7" class="ltx_text ltx_font_typewriter">relate_occluded</span>).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluated methods</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We compare our model with three representative VQA models: FiLM <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib44" title="" class="ltx_ref">perez2018film, </a>)</cite>, mDETR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">kamath2021mdetr, </a>)</cite>, and PNSVQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite>. Additionally, we introduce a variant of PNSVQA, PNSVQA+Projection, to analyze the benefit of our generative 6D pose estimation approach.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">FiLM <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib44" title="" class="ltx_ref">perez2018film, </a>)</cite></span> <span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">Feature-wise Linear Modulation</span> is a representative two-stream feature fusion method. The FiLM model merges the question features extracted with GRU <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib12" title="" class="ltx_ref">cho2014learning, </a>)</cite> and image features extracted with CNN and predicts answers based on the merged features.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">mDETR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">kamath2021mdetr, </a>)</cite> </span> mDETR is a pretrained text-guided object detector based on transformers. The model is pretrained with 1.3M image and text pairs and shows strong performance when finetuned on downstream tasks like referring expression understanding or VQA.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">PNSVQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite></span> PNSVQA is a SoTA neural symbolic VQA model. It parses the scene using MaskRCNN <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">he2017mask, </a>)</cite> and an attribute extraction network, then executes the reasoning program on the parsed visual scenes with taking into account the uncertainty of the scene parser. To extend PNSVQA to the 3D questions in Super-CLEVR-3D, we add a regression head in the attribute extraction network to predict the 3D posefor each object; parts are detected in a similar way as objects by predicting 2D bounding boxes; the part-object associations and occlusions are computed using intersection-over-union: a part belongs to an intersected object if the part label matches the object label, otherwise it is occluded by this object.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">PNSVQA+Projection</span>
Similar with NSVQA, this model predicts the 6D poses, categories and attributes using MaskRCNN and the attribute extraction network. The difference is that the parts and occlusions are predicted by projecting the 3D object models onto the image using the predicted 6D pose and category (same with how we find parts and occlusions in our model). This model helps us ablate the influence of the two components in our model, <em id="S5.SS1.p5.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p5.1.3" class="ltx_text"></span> 6D pose prediction by render-and-compare, and part/occlusion detection with mesh projection.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experiment setup</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Dataset.</span>
Our Super-CLEVR-3D dataset shares the same visual scenes with Super-CLEVR dataset. We re-render the images
with more annotations recorded (camera parameters, parts annotations, occlusion maps). The dataset splits follow the Super-CLEVR dataset, where we have 20k images for training, 5k for validation, and 5k for testing. For question generation, we create 9 templates for part questions, 17 templates for pose questions, 35 templates for occlusion questions (with and without parts). For each of the three types, 8 to 10 questions are generated for each image by randomly sampling the templates. We ensure that the questions are not ill-posed and cannot be answered by taking shortcuts, <em id="S5.SS2.p1.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS2.p1.1.3" class="ltx_text"></span> the questions contain no redundant reasoning steps, following the no-redundancy setting in <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite>. More details including the list of question templates can be found in the Appendix.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Implementation details.</span>
We train the 6D pose estimator and CNN attribute classifier separately. We train the 6D pose estimator (including the contrastive feature backbone and the nerual mesh models for each of the 5 classes) for 15k iterations with batch size 15, which takes around 2 hours on NVIDIA RTX A5000 for each class. The attribute classifier, which is a ResNet50, is shared for objects and parts. It is trained for 100 epochs with batch size 64. During inference, it takes 22s for 6D pose estimation and 10s for object mesh projection for all the objects in one image. During inference of the 6D pose estimator, we assume the theta is 0. During 3D NMS filtering, we choose the radius <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">r</annotation></semantics></math> as 2, and we also filter the object proposals with a threshold of 15 on the score map.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Quantitative Results</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We trained our model and baselines on Super-CLEVR-3D’s training split, reporting answer accuracies on the test split in <a href="#S5.T1" title="In 5.3 Quantitative Results ‣ 5 Experiments ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Accuracies for each question type are detailed separately.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Model accuracies on the Super-CLEVR-3D testing split, reported for each question type, <em id="S5.T1.3.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.T1.4.2" class="ltx_text"></span> questions about parts, 3D poses, occlusions between objects, occlusions between objects and parts.</figcaption>
<table id="S5.T1.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T1.5.1" class="ltx_tr">
<td id="S5.T1.5.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S5.T1.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Mean</td>
<td id="S5.T1.5.1.3" class="ltx_td ltx_align_center ltx_border_tt">Part</td>
<td id="S5.T1.5.1.4" class="ltx_td ltx_align_center ltx_border_tt">Pose</td>
<td id="S5.T1.5.1.5" class="ltx_td ltx_align_center ltx_border_tt">Occ.</td>
<td id="S5.T1.5.1.6" class="ltx_td ltx_align_center ltx_border_tt">Part+Occ.</td>
</tr>
<tr id="S5.T1.5.2" class="ltx_tr">
<td id="S5.T1.5.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FiLM <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib44" title="" class="ltx_ref">perez2018film, </a>)</cite>
</td>
<td id="S5.T1.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.53</td>
<td id="S5.T1.5.2.3" class="ltx_td ltx_align_center ltx_border_t">38.24</td>
<td id="S5.T1.5.2.4" class="ltx_td ltx_align_center ltx_border_t">67.82</td>
<td id="S5.T1.5.2.5" class="ltx_td ltx_align_center ltx_border_t">51.41</td>
<td id="S5.T1.5.2.6" class="ltx_td ltx_align_center ltx_border_t">44.66</td>
</tr>
<tr id="S5.T1.5.3" class="ltx_tr">
<td id="S5.T1.5.3.1" class="ltx_td ltx_align_left ltx_border_r">mDETR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">kamath2021mdetr, </a>)</cite>
</td>
<td id="S5.T1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">55.72</td>
<td id="S5.T1.5.3.3" class="ltx_td ltx_align_center">41.52</td>
<td id="S5.T1.5.3.4" class="ltx_td ltx_align_center">71.76</td>
<td id="S5.T1.5.3.5" class="ltx_td ltx_align_center">64.99</td>
<td id="S5.T1.5.3.6" class="ltx_td ltx_align_center">50.47</td>
</tr>
<tr id="S5.T1.5.4" class="ltx_tr">
<td id="S5.T1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">PNSVQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">li2022super, </a>)</cite>
</td>
<td id="S5.T1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">64.39</td>
<td id="S5.T1.5.4.3" class="ltx_td ltx_align_center">50.61</td>
<td id="S5.T1.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.4.4.1" class="ltx_text ltx_font_bold">87.78</span></td>
<td id="S5.T1.5.4.5" class="ltx_td ltx_align_center">65.80</td>
<td id="S5.T1.5.4.6" class="ltx_td ltx_align_center">53.35</td>
</tr>
<tr id="S5.T1.5.5" class="ltx_tr">
<td id="S5.T1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">PNSVQA+Projection</td>
<td id="S5.T1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.15</td>
<td id="S5.T1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">56.30</td>
<td id="S5.T1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">86.70</td>
<td id="S5.T1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">70.70</td>
<td id="S5.T1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">58.90</td>
</tr>
<tr id="S5.T1.5.6" class="ltx_tr">
<td id="S5.T1.5.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T1.5.6.1.1" class="ltx_text ltx_font_bold">PO3D-VQA (Ours)</span></td>
<td id="S5.T1.5.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T1.5.6.2.1" class="ltx_text ltx_font_bold">75.64</span></td>
<td id="S5.T1.5.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T1.5.6.3.1" class="ltx_text ltx_font_bold">71.85</span></td>
<td id="S5.T1.5.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">86.40</td>
<td id="S5.T1.5.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T1.5.6.5.1" class="ltx_text ltx_font_bold">76.90</span></td>
<td id="S5.T1.5.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T1.5.6.6.1" class="ltx_text ltx_font_bold">67.40</span></td>
</tr>
</table>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">Comparison with baselines.</span> First, among all the baseline methods, the neural symbolic method PNSVQA performs the best (64.4% accuracy), outperforming the end-to-end methods mDETR and FiLM by a large margin (<math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="&gt;8\%" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml"></mi><mo id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.cmml">&gt;</mo><mrow id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml"><mn id="S5.SS3.p2.1.m1.1.1.3.2" xref="S5.SS3.p2.1.m1.1.1.3.2.cmml">8</mn><mo id="S5.SS3.p2.1.m1.1.1.3.1" xref="S5.SS3.p2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><gt id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">absent</csymbol><apply id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS3.p2.1.m1.1.1.3.1.cmml" xref="S5.SS3.p2.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS3.p2.1.m1.1.1.3.2.cmml" xref="S5.SS3.p2.1.m1.1.1.3.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">&gt;8\%</annotation></semantics></math>). This shows the advantage of the step-wise modular reasoning procedure, which agrees with the findings in prior works that the modular methods excel on the simulated benchmarks that require long-trace reasoning. Second, our model achieves 75.6% average accuracy, which significantly outperforms all the evaluated models. Especially, comparing our PO3D-VQA with its 2D counterpart NSVQA, we see that the injection of 3D knowledge brings a large performance boost of 11%, suggesting the importance of the 3D understanding.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Comparison with PNSVQA variants.</span> By analyzing the results of PNSVQA variants (<span id="S5.SS3.p3.1.2" class="ltx_text ltx_font_italic">PNSVQA</span>, <span id="S5.SS3.p3.1.3" class="ltx_text ltx_font_italic">PNSVQA+Projection</span>, and our <span id="S5.SS3.p3.1.4" class="ltx_text ltx_font_italic">PO3D-VQA</span>), we show (a) the benefit of estimating object 3D poses using our analysis-by-synthesis method over regression and (b) the benefit of object-part structure knowledge. First, by detecting part using 3D model projection, <span id="S5.SS3.p3.1.5" class="ltx_text ltx_font_italic">PNSVQA+Projection</span> improves the <span id="S5.SS3.p3.1.6" class="ltx_text ltx_font_italic">PNSVQA</span> results by 4%, which indicates that locating parts based on objects using the object-part structure knowledge is beneficial. Second, by estimating object 6D poses with our generative render-and-compare method, our <span id="S5.SS3.p3.1.7" class="ltx_text ltx_font_italic">PO3D-VQA</span> outperforms <span id="S5.SS3.p3.1.8" class="ltx_text ltx_font_italic">PNSVQA+Projection</span> by 7% (from 68.2% to 75.6%), showing the advantage of our render-and-compare model.
Moreover, looking at the per-type results, we find that the improvement of our PO3D-VQA is most significant on the part-related questions (21% improvement over PNSVQA) and part-with-occlusion questions (14%), while the accuracy on pose-related questions does not improve. The reason is that part and occlusion predictions require precise pose predictions for accurate mesh projection, while the pose questions only require a rough pose to determine the facing direction.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Analysis and discussions</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">To further analyze the advantage of PO3D-VQA over other PNSVQA variants, we compare the models on questions of different difficulty levels. It is shown that the benefit our model is the most significant on hard questions. In <a href="#S5.F4" title="In 5.4 Analysis and discussions ‣ 5 Experiments ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we plot the relative accuracy drop <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Relative accuracy drop means the ratio of absolute accuracy drop and the original accuracy. For example, if a model’s accuracy drops from 50% to 45%, its relative accuracy drop is 10%.</span></span></span> of each model on questions with different occlusion ratios and questions with different part sizes.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2310.17914/assets/x3.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="116" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Analysis on questions of different difficulty levels. The plots show the relative accuracy drop of models, on pose questions w.r.t.<span id="S5.F4.4.1" class="ltx_text"></span> different occlusion ratios (a), on part questions w.r.t.<span id="S5.F4.5.2" class="ltx_text"></span> different part sizes (b), and on part+occlusion questions w.r.t.<span id="S5.F4.6.3" class="ltx_text"></span> different part sizes (c).</figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.5" class="ltx_p"><span id="S5.SS4.p2.5.1" class="ltx_text ltx_font_bold">Questions with different occlusion ratios.</span>
We sort pose-related questions into different sub-groups based on their <span id="S5.SS4.p2.5.2" class="ltx_text ltx_font_italic">occlusion ratios</span> and evaluate the models on each of the sub-groups. The <span id="S5.SS4.p2.5.3" class="ltx_text ltx_font_italic">occlusion ratio</span> <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mi id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><ci id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">r</annotation></semantics></math> of a question is the <span id="S5.SS4.p2.5.4" class="ltx_text ltx_font_italic">minimum</span> of occlusion ratios for all the objects in its reasoning trace.
We choose <math id="S5.SS4.p2.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S5.SS4.p2.2.m2.1a"><mi id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><ci id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">r</annotation></semantics></math> from <math id="S5.SS4.p2.3.m3.1" class="ltx_Math" alttext="0\%" display="inline"><semantics id="S5.SS4.p2.3.m3.1a"><mrow id="S5.SS4.p2.3.m3.1.1" xref="S5.SS4.p2.3.m3.1.1.cmml"><mn id="S5.SS4.p2.3.m3.1.1.2" xref="S5.SS4.p2.3.m3.1.1.2.cmml">0</mn><mo id="S5.SS4.p2.3.m3.1.1.1" xref="S5.SS4.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.3.m3.1b"><apply id="S5.SS4.p2.3.m3.1.1.cmml" xref="S5.SS4.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS4.p2.3.m3.1.1.1.cmml" xref="S5.SS4.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.3.m3.1.1.2.cmml" xref="S5.SS4.p2.3.m3.1.1.2">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.3.m3.1c">0\%</annotation></semantics></math> to <math id="S5.SS4.p2.4.m4.1" class="ltx_Math" alttext="30\%" display="inline"><semantics id="S5.SS4.p2.4.m4.1a"><mrow id="S5.SS4.p2.4.m4.1.1" xref="S5.SS4.p2.4.m4.1.1.cmml"><mn id="S5.SS4.p2.4.m4.1.1.2" xref="S5.SS4.p2.4.m4.1.1.2.cmml">30</mn><mo id="S5.SS4.p2.4.m4.1.1.1" xref="S5.SS4.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.4.m4.1b"><apply id="S5.SS4.p2.4.m4.1.1.cmml" xref="S5.SS4.p2.4.m4.1.1"><csymbol cd="latexml" id="S5.SS4.p2.4.m4.1.1.1.cmml" xref="S5.SS4.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.4.m4.1.1.2.cmml" xref="S5.SS4.p2.4.m4.1.1.2">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.4.m4.1c">30\%</annotation></semantics></math>, in increment of <math id="S5.SS4.p2.5.m5.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="S5.SS4.p2.5.m5.1a"><mrow id="S5.SS4.p2.5.m5.1.1" xref="S5.SS4.p2.5.m5.1.1.cmml"><mn id="S5.SS4.p2.5.m5.1.1.2" xref="S5.SS4.p2.5.m5.1.1.2.cmml">5</mn><mo id="S5.SS4.p2.5.m5.1.1.1" xref="S5.SS4.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.5.m5.1b"><apply id="S5.SS4.p2.5.m5.1.1.cmml" xref="S5.SS4.p2.5.m5.1.1"><csymbol cd="latexml" id="S5.SS4.p2.5.m5.1.1.1.cmml" xref="S5.SS4.p2.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S5.SS4.p2.5.m5.1.1.2.cmml" xref="S5.SS4.p2.5.m5.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.5.m5.1c">5\%</annotation></semantics></math>. The results are shown is <a href="#S5.F4" title="In 5.4 Analysis and discussions ‣ 5 Experiments ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> (a).
Our PO3D-VQA is much more robust to occlusions compared to the other two methods: while the performances of all the three models decrease as the occlusion ratio increases, the relative drop of ours is much smaller than others.
The results show that our render-and-compare scene parser is more robust to heavy occlusions compared with the discriminative methods.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.3" class="ltx_p"><span id="S5.SS4.p3.3.1" class="ltx_text ltx_font_bold">Questions with different part sizes.</span>
Questions about small parts are harder than the ones about larger parts. We sort the questions into different part size intervals <math id="S5.SS4.p3.1.m1.2" class="ltx_Math" alttext="(s,t)" display="inline"><semantics id="S5.SS4.p3.1.m1.2a"><mrow id="S5.SS4.p3.1.m1.2.3.2" xref="S5.SS4.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S5.SS4.p3.1.m1.2.3.2.1" xref="S5.SS4.p3.1.m1.2.3.1.cmml">(</mo><mi id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">s</mi><mo id="S5.SS4.p3.1.m1.2.3.2.2" xref="S5.SS4.p3.1.m1.2.3.1.cmml">,</mo><mi id="S5.SS4.p3.1.m1.2.2" xref="S5.SS4.p3.1.m1.2.2.cmml">t</mi><mo stretchy="false" id="S5.SS4.p3.1.m1.2.3.2.3" xref="S5.SS4.p3.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.2b"><interval closure="open" id="S5.SS4.p3.1.m1.2.3.1.cmml" xref="S5.SS4.p3.1.m1.2.3.2"><ci id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">𝑠</ci><ci id="S5.SS4.p3.1.m1.2.2.cmml" xref="S5.SS4.p3.1.m1.2.2">𝑡</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.2c">(s,t)</annotation></semantics></math>, where the <span id="S5.SS4.p3.3.2" class="ltx_text ltx_font_italic">largest</span> part that the question refers to has an area (number of pixels occupied) larger than <math id="S5.SS4.p3.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S5.SS4.p3.2.m2.1a"><mi id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><ci id="S5.SS4.p3.2.m2.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">s</annotation></semantics></math> and smaller than <math id="S5.SS4.p3.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S5.SS4.p3.3.m3.1a"><mi id="S5.SS4.p3.3.m3.1.1" xref="S5.SS4.p3.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.3.m3.1b"><ci id="S5.SS4.p3.3.m3.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.3.m3.1c">t</annotation></semantics></math>.
We compare the models on the part questions and the part+occlusion questions with different part sizes in <a href="#S5.F4" title="In 5.4 Analysis and discussions ‣ 5 Experiments ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> (b) and (c). In (b), the accuracy drop of PO3D-VQA is smaller than PNSVQA+Projection and PNSVQA when parts get smaller. In (c), PNSVQA+Projection is slightly better than our model and they are both better than the original PNSVQA.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">In summary, by sorting questions into different difficulty levels based on occlusion ratios and part sizes, we show the advantage of our PO3D-VQA on harder questions, indicating that our model is robust to occlusions and small part sizes.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p"><span id="S5.SS4.p5.1.1" class="ltx_text ltx_font_bold">Qualitative results.</span>
<a href="#S5.F5" title="In 5.4 Analysis and discussions ‣ 5 Experiments ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows examples of predictions for our model and PNSVQA variants. In (a), the question asks about occlusion, but with a slight error in the pose prediction, PNSVQA+Projection misses the occluded bus and predicts the wrong answer, while our model is correct with <span id="S5.SS4.p5.1.2" class="ltx_text ltx_font_bold">accurate pose</span>. In (b), the question refers to the heavily occluded minivan that is difficult to detect, but our model gets the correct prediction thanks to its <span id="S5.SS4.p5.1.3" class="ltx_text ltx_font_bold">robustness to occlusions</span>.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2310.17914/assets/x4.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of models’ predictions. Our model (a) predicts the object pose accurately and (b) is robust to heavy occlusions. Red boxes are for visualization only.</figcaption>
</figure>
<div id="S5.SS4.p6" class="ltx_para">
<p id="S5.SS4.p6.1" class="ltx_p"><span id="S5.SS4.p6.1.1" class="ltx_text ltx_font_bold">Limitations and failure cases.</span>
Due to the difficulties of collecting real images with compositional scenes and 3D annotations, our work is currently limited by its synthetic nature. For PO3D-VQA, it sometimes fails to detect multiple objects if they are from the same category and heavily overlap (see Appendix D for more visualizations). 3D NMS can effectively improve the dense scene parsing results when objects are from different categories, but conceptually it is limited when objects are from the same category. However, 6D pose estimation in dense scenes is a challenging problem, whereas many current works on 6D pose estimation are still focusing on simple scenes with single objects <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib38" title="" class="ltx_ref">ma2022robust, </a>; <a href="#bib.bib50" title="" class="ltx_ref">xiang2014beyond, </a>; <a href="#bib.bib57" title="" class="ltx_ref">ze2022category, </a>)</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Further Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we discuss two meaningful extensions of our work: the incorporation of z-direction questions and the application of our model to real-world images.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Z-direction questions</span>.
While the proposed Super-CLEVR-3D dataset has been designed with 3D-aware questions, all objects within it are placed on the same surface. Introducing variability in the z direction can further enrich our dataset with more comprehensive 3D spatial relationships.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">We consider the scenario where aeroplane category, is in different elevations, introducing the z dimension into the spatial relationships (see Fig. <a href="#S6.F6" title="Figure 6 ‣ 6 Further Discussion ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). This allowed us to formulate questions that probe the model’s understanding of height relationships and depth perception.
We create a subset containing 100 images and 379 questions and test our PO3D-VQA model directly on it without retraining the 6D parser. On this dataset, our PO3D model achieves 90.33% accuracy on height relationship questions and 78.89% on depth-related questions, suggesting that our model can successfully handle questions about height. As the baseline models only use the bounding box to determine the spatial relationship between objects, they are not able to determine the height relationships.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/2310.17914/assets/rebuttal_images/z_question_2.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="126" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Example images and questions of objects with different elevations.</figcaption>
</figure>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold">Extension to real-world images</span>
While our PO3D-VQA model has demonstrated impressive performance on the synthetic Super-CLEVR-3D dataset, an essential research direction is extending it to real images or other 3D VQA datasets (such as GQA and FE-3DGQA). However, it’s not trivial to truly evaluate it on these real-world problems, and a primary challenge is the lack of 3D annotations and the highly articulated categories (like the human body) in these datasets.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">However, we show that our PO3D-VQA model can, in principle, work on realistic images. We generate several realistic image samples manually using the vehicle objects (e.g. car, bus, bicycle) from ImageNet with 3D annotation (see Fig. <a href="#S6.F7" title="Figure 7 ‣ 6 Further Discussion ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) and real-image background. In this experiment, the pose estimator is trained on the PASCAL3D+ dataset, and is used to predict the poses of objects from the image before pasting, as shown in (b). The attribute (color) prediction module is trained on Super-CLEVR-3D and the object shapes are predicted by a ResNet trained on ImageNet. Our model can correctly predict answers to questions about the object pose, parts, and occlusions, e.g. “Which object is occluded by the mountain bike”.</p>
</div>
<figure id="S6.F7" class="ltx_figure"><img src="/html/2310.17914/assets/rebuttal_images/real_image_2.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Examples of results on realistic images. Given a realistic image (a1, a2), our model can successfully estimate the 6D poses of objects (b1, b2) and answer the 3D-aware questions (c1, c2).</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, we study the task of 3D-aware VQA. We propose the Super-CLEVR-3D dataset containing questions explicitly querying 3D understanding including object parts, 3D poses, and occlusions. To address the task, a 3D-aware neural symbolic model PO3D-VQA is proposed, which enhances the probabilistic symbolic model with a robust 3D scene parser based on analysis-by-synthesis. With the merits of accurate 3D scene parsing and symbolic execution, our model outperforms existing methods by a large margin. Further analysis shows that the improvements are even larger on harder questions. With the dataset, the model, and the experiments, we highlight the benefit of symbolic execution and the importance of 3D understanding for 3D-aware VQA.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We thank the anonymous reviewers for their valuable comments. We thank Qing Liu, Chenxi Liu, Elias Stengel-Eskin, Benjamin Van Durme for the helpful discussions on early version of the project. This work is supported by Office of Naval Research with grants N00014-23-1-2641, N00014-21-1-2812. A. Kortylewski acknowledges support via his Emmy Noether Research Group funded by the German Science Foundation (DFG) under Grant No.468670075.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Analyzing the behavior of visual question answering models.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.07356</span>, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 6077–6086, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.

</span>
<span class="ltx_bibblock">Neural module networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 39–48, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe.

</span>
<span class="ltx_bibblock">Scanqa: 3d question answering for spatial scene understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 19129–19139, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Harm de Vries, Timothy J O’Donnell, Shikhar Murty, Philippe Beaudoin, Yoshua Bengio, and Aaron Courville.

</span>
<span class="ltx_bibblock">Closure: Assessing systematic generalization of clevr models.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.05783</span>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Yutong Bai, Angtian Wang, Adam Kortylewski, and Alan Yuille.

</span>
<span class="ltx_bibblock">Coke: Contrastive learning for robust keypoint detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span>, pages 65–74, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al.

</span>
<span class="ltx_bibblock">Rubi: Reducing unimodal biases for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 32, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Paola Cascante-Bonilla, Hui Wu, Letao Wang, Rogerio S Feris, and Vicente Ordonez.

</span>
<span class="ltx_bibblock">Simvqa: Exploring simulated environments for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 5056–5066, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang.

</span>
<span class="ltx_bibblock">Clip2scene: Towards label-efficient 3d scene understanding by clip.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.04926</span>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille.

</span>
<span class="ltx_bibblock">Detect what you can: Detecting and representing objects using holistic models and body parts.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 1971–1978, 2014.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Learning phrase representations using rnn encoder-decoder for statistical machine translation.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.1078</span>, 2014.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Embodied question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 1–10, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and visual grounding.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01847</span>, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Vipul Gupta, Zhuowan Li, Adam Kortylewski, Chenyu Zhang, Yingwei Li, and Alan Yuille.

</span>
<span class="ltx_bibblock">Swapmix: Diagnosing and regularizing the over-reliance on visual context in visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2204.02285</span>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span>, pages 3608–3617, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer vision</span>, pages 2961–2969, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan.

</span>
<span class="ltx_bibblock">3d concept learning and reasoning from multi-view images.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.11327</span>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yining Hong, Li Yi, Josh Tenenbaum, Antonio Torralba, and Chuang Gan.

</span>
<span class="ltx_bibblock">Ptr: A benchmark for part-based conceptual, relational, and physical reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 34, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.

</span>
<span class="ltx_bibblock">3d-llm: Injecting the 3d world into large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.12981</span>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Ronghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko.

</span>
<span class="ltx_bibblock">Explainable neural computation via stack neural module networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the European conference on computer vision (ECCV)</span>, pages 53–69, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning.

</span>
<span class="ltx_bibblock">Compositional attention networks for machine reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations (ICLR)</span>, 2018.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 2901–2910, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.

</span>
<span class="ltx_bibblock">Mdetr-modulated detection for end-to-end multi-modal understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 1780–1790, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Corentin Kervadec, Grigory Antipov, Moez Baccouche, and Christian Wolf.

</span>
<span class="ltx_bibblock">Roses are red, violets are blue… but should vqa expect them to?

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 2776–2785, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Corentin Kervadec, Theo Jaunet, Grigory Antipov, Moez Baccouche, Romain Vuillemot, and Christian Wolf.

</span>
<span class="ltx_bibblock">How transferable are reasoning patterns in vqa?

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 4205–4214, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.

</span>
<span class="ltx_bibblock">Bilinear attention networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 31, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Satwik Kottur, José MF Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Clevr-dialog: A diagnostic dataset for multi-round reasoning in visual dialog.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.03166</span>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Relation-aware graph attention network for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</span>, pages 10313–10322, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Oscar: Object-semantics aligned pre-training for vision-language tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">ECCV 2020</span>, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille.

</span>
<span class="ltx_bibblock">Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.00259</span>, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Qing Liu, Adam Kortylewski, Zhishuai Zhang, Zizhang Li, Mengqi Guo, Qihao Liu, Xiaoding Yuan, Jiteng Mu, Weichao Qiu, and Alan Yuille.

</span>
<span class="ltx_bibblock">Learning part segmentation through unsupervised domain adaptation from synthetic vehicles.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Runtao Liu, Chenxi Liu, Yutong Bai, and Alan L Yuille.

</span>
<span class="ltx_bibblock">Clevr-ref+: Diagnosing visual reasoning with referring expressions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 4185–4194, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Shichen Liu, Tianye Li, Weikai Chen, and Hao Li.

</span>
<span class="ltx_bibblock">Soft rasterizer: A differentiable renderer for image-based 3d reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 7708–7717, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 32, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson.

</span>
<span class="ltx_bibblock">Scalable 3d captioning with pretrained models.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.07279</span>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Wufei Ma, Angtian Wang, Alan Yuille, and Adam Kortylewski.

</span>
<span class="ltx_bibblock">Robust category-level 6d pose estimation with coarse-to-fine rendering of neural features.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IX</span>, pages 492–508. Springer, 2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang.

</span>
<span class="ltx_bibblock">Sqa3d: Situated question answering in 3d scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu.

</span>
<span class="ltx_bibblock">The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Counterfactual vqa: A cause-effect look at language bias.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 12700–12710, June 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xiansheng Hua, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Counterfactual vqa: A cause-effect look at language bias.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 12695–12705, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Songyou Peng, Kyle Genova, Chiyu "Max" Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser.

</span>
<span class="ltx_bibblock">Openscene: 3d scene understanding with open vocabularies.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville.

</span>
<span class="ltx_bibblock">Film: Visual reasoning with a general conditioning layer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume 32, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Leonard Salewski, A Koepke, Hendrik Lensch, and Zeynep Akata.

</span>
<span class="ltx_bibblock">Clevr-x: A visual reasoning dataset for natural language explanations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers</span>, pages 69–88. Springer, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.07490</span>, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Angtian Wang, Adam Kortylewski, and Alan Yuille.

</span>
<span class="ltx_bibblock">Nemo: Neural mesh models of contrastive features for robust 3d pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese.

</span>
<span class="ltx_bibblock">Beyond pascal: A benchmark for 3d object detection in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">IEEE winter conference on applications of computer vision</span>, pages 75–82. IEEE, 2014.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Xu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Zhen Li, and Shuguang Cui.

</span>
<span class="ltx_bibblock">Clevr3d: Compositional language and elementary visual reasoning for question answering in 3d real-world scenes.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2112.11691</span>, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Shuquan Ye, Dongdong Chen, Songfang Han, and Jing Liao.

</span>
<span class="ltx_bibblock">3d question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 2022.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum.

</span>
<span class="ltx_bibblock">Clevrer: Collision events for video representation and reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.01442</span>, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B Tenenbaum.

</span>
<span class="ltx_bibblock">Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NIPS)</span>, 2018.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 6281–6290, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Xiaoding Yuan, Adam Kortylewski, Yihong Sun, and Alan Yuille.

</span>
<span class="ltx_bibblock">Robust instance segmentation through reasoning about multi-object occlusion.

</span>
<span class="ltx_bibblock">In <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 11141–11150, 2021.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Yanjie Ze and Xiaolong Wang.

</span>
<span class="ltx_bibblock">Category-level 6d object pose estimation in the wild: A semi-supervised learning approach and a new dataset.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 35:27469–27483, 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu.

</span>
<span class="ltx_bibblock">Raven: A dataset for relational and analogical visual reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 5317–5327, 2019.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Vinvl: Making visual representations matter in vision-language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">CVPR 2021</span>, 2021.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Xingyi Zhou, Arjun Karpur, Linjie Luo, and Qixing Huang.

</span>
<span class="ltx_bibblock">Starmap for category-agnostic keypoint and viewpoint estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision (ECCV)</span>, pages 318–334, 2018.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 4995–5004, 2016.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset Details</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Part list</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">In Super-CLEVR-3D, the parts of each objects are listed in Tab. <a href="#A1.T2" title="Table 2 ‣ A.1 Part list ‣ Appendix A Dataset Details ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></p>
</div>
<figure id="A1.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>List of objects and parts.</figcaption>
<table id="A1.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T2.3.1" class="ltx_tr">
<td id="A1.T2.3.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">shape</span></td>
<td id="A1.T2.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_tt">
<span id="A1.T2.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.1.2.1.1" class="ltx_p"><span id="A1.T2.3.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">part list</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.2" class="ltx_tr">
<td id="A1.T2.3.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.2.1.1" class="ltx_text" style="font-size:90%;">airliner</span></td>
<td id="A1.T2.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.2.2.1.1" class="ltx_p"><span id="A1.T2.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">left door, front wheel, fin, right engine, propeller, back left wheel, left engine, back right wheel, left tailplane, right door, right tailplane, right wing, left wing</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.3" class="ltx_tr">
<td id="A1.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.3.1.1" class="ltx_text" style="font-size:90%;">biplane</span></td>
<td id="A1.T2.3.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.3.2.1.1" class="ltx_p"><span id="A1.T2.3.3.2.1.1.1" class="ltx_text" style="font-size:90%;">front wheel, fin, propeller, left tailplane, right tailplane, right wing, left wing</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.4" class="ltx_tr">
<td id="A1.T2.3.4.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.4.1.1" class="ltx_text" style="font-size:90%;">jet</span></td>
<td id="A1.T2.3.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.4.2.1.1" class="ltx_p"><span id="A1.T2.3.4.2.1.1.1" class="ltx_text" style="font-size:90%;">left door, front wheel, fin, right engine, propeller, back left wheel, left engine, back right wheel, left tailplane, right tailplane, right wing, left wing</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.5" class="ltx_tr">
<td id="A1.T2.3.5.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.5.1.1" class="ltx_text" style="font-size:90%;">fighter</span></td>
<td id="A1.T2.3.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.5.2.1.1" class="ltx_p"><span id="A1.T2.3.5.2.1.1.1" class="ltx_text" style="font-size:90%;">fin, right engine, left engine, left tailplane, right tailplane, right wing, left wing</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.6" class="ltx_tr">
<td id="A1.T2.3.6.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T2.3.6.1.1" class="ltx_text"></span><span id="A1.T2.3.6.1.2" class="ltx_text" style="font-size:90%;"> </span><span id="A1.T2.3.6.1.3" class="ltx_text" style="font-size:90%;">
<span id="A1.T2.3.6.1.3.1" class="ltx_tabular ltx_align_top">
<span id="A1.T2.3.6.1.3.1.1" class="ltx_tr">
<span id="A1.T2.3.6.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">utility</span></span>
<span id="A1.T2.3.6.1.3.1.2" class="ltx_tr">
<span id="A1.T2.3.6.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bike</span></span>
</span></span><span id="A1.T2.3.6.1.4" class="ltx_text"></span><span id="A1.T2.3.6.1.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="A1.T2.3.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.6.2.1.1" class="ltx_p"><span id="A1.T2.3.6.2.1.1.1" class="ltx_text" style="font-size:90%;">left handle, brake system, front wheel, left pedal, right handle, back wheel, saddle, carrier, fork, right crank arm, front fender, drive chain, back fender, left crank arm, side stand, right pedal</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.7" class="ltx_tr">
<td id="A1.T2.3.7.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T2.3.7.1.1" class="ltx_text"></span><span id="A1.T2.3.7.1.2" class="ltx_text" style="font-size:90%;"> </span><span id="A1.T2.3.7.1.3" class="ltx_text" style="font-size:90%;">
<span id="A1.T2.3.7.1.3.1" class="ltx_tabular ltx_align_top">
<span id="A1.T2.3.7.1.3.1.1" class="ltx_tr">
<span id="A1.T2.3.7.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">tandem</span></span>
<span id="A1.T2.3.7.1.3.1.2" class="ltx_tr">
<span id="A1.T2.3.7.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bike</span></span>
</span></span><span id="A1.T2.3.7.1.4" class="ltx_text"></span><span id="A1.T2.3.7.1.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="A1.T2.3.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.7.2.1.1" class="ltx_p"><span id="A1.T2.3.7.2.1.1.1" class="ltx_text" style="font-size:90%;">rearlight, front wheel, back wheel, fork, front fender, back fender</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.8" class="ltx_tr">
<td id="A1.T2.3.8.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T2.3.8.1.1" class="ltx_text"></span><span id="A1.T2.3.8.1.2" class="ltx_text" style="font-size:90%;"> </span><span id="A1.T2.3.8.1.3" class="ltx_text" style="font-size:90%;">
<span id="A1.T2.3.8.1.3.1" class="ltx_tabular ltx_align_top">
<span id="A1.T2.3.8.1.3.1.1" class="ltx_tr">
<span id="A1.T2.3.8.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">road</span></span>
<span id="A1.T2.3.8.1.3.1.2" class="ltx_tr">
<span id="A1.T2.3.8.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bike</span></span>
</span></span><span id="A1.T2.3.8.1.4" class="ltx_text"></span><span id="A1.T2.3.8.1.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="A1.T2.3.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.8.2.1.1" class="ltx_p"><span id="A1.T2.3.8.2.1.1.1" class="ltx_text" style="font-size:90%;">left handle, brake system, front wheel, left pedal, right handle, back wheel, saddle, fork, right crank arm, drive chain, left crank arm, right pedal</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.9" class="ltx_tr">
<td id="A1.T2.3.9.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T2.3.9.1.1" class="ltx_text"></span><span id="A1.T2.3.9.1.2" class="ltx_text" style="font-size:90%;"> </span><span id="A1.T2.3.9.1.3" class="ltx_text" style="font-size:90%;">
<span id="A1.T2.3.9.1.3.1" class="ltx_tabular ltx_align_top">
<span id="A1.T2.3.9.1.3.1.1" class="ltx_tr">
<span id="A1.T2.3.9.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">mountain</span></span>
<span id="A1.T2.3.9.1.3.1.2" class="ltx_tr">
<span id="A1.T2.3.9.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bike</span></span>
</span></span><span id="A1.T2.3.9.1.4" class="ltx_text"></span><span id="A1.T2.3.9.1.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="A1.T2.3.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.9.2.1.1" class="ltx_p"><span id="A1.T2.3.9.2.1.1.1" class="ltx_text" style="font-size:90%;">left handle, brake system, front wheel, left pedal, right handle, back wheel, saddle, fork, right crank arm, drive chain, left crank arm, right pedal</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.10" class="ltx_tr">
<td id="A1.T2.3.10.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T2.3.10.1.1" class="ltx_text"></span><span id="A1.T2.3.10.1.2" class="ltx_text" style="font-size:90%;"> </span><span id="A1.T2.3.10.1.3" class="ltx_text" style="font-size:90%;">
<span id="A1.T2.3.10.1.3.1" class="ltx_tabular ltx_align_top">
<span id="A1.T2.3.10.1.3.1.1" class="ltx_tr">
<span id="A1.T2.3.10.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">articulated</span></span>
<span id="A1.T2.3.10.1.3.1.2" class="ltx_tr">
<span id="A1.T2.3.10.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bus</span></span>
</span></span><span id="A1.T2.3.10.1.4" class="ltx_text"></span><span id="A1.T2.3.10.1.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="A1.T2.3.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.10.2.1.1" class="ltx_p"><span id="A1.T2.3.10.2.1.1.1" class="ltx_text" style="font-size:90%;">left tail light, front license plate, front right door, back bumper, right head light, front left wheel, left mirror, right tail light, back right door, back left wheel, back right wheel, back license plate, front right wheel, left head light, right mirror, trunk, mid right door, roof</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.11" class="ltx_tr">
<td id="A1.T2.3.11.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T2.3.11.1.1" class="ltx_text"></span><span id="A1.T2.3.11.1.2" class="ltx_text" style="font-size:90%;"> </span><span id="A1.T2.3.11.1.3" class="ltx_text" style="font-size:90%;">
<span id="A1.T2.3.11.1.3.1" class="ltx_tabular ltx_align_top">
<span id="A1.T2.3.11.1.3.1.1" class="ltx_tr">
<span id="A1.T2.3.11.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">double</span></span>
<span id="A1.T2.3.11.1.3.1.2" class="ltx_tr">
<span id="A1.T2.3.11.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bus</span></span>
</span></span><span id="A1.T2.3.11.1.4" class="ltx_text"></span><span id="A1.T2.3.11.1.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="A1.T2.3.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.11.2.1.1" class="ltx_p"><span id="A1.T2.3.11.2.1.1.1" class="ltx_text" style="font-size:90%;">left tail light, front license plate, front right door, front bumper, back bumper, right head light, front left wheel, left mirror, right tail light, back left wheel, back right wheel, back license plate, mid left door, front left door, front right wheel, left head light, right mirror, trunk, mid right door, roof</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.12" class="ltx_tr">
<td id="A1.T2.3.12.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T2.3.12.1.1" class="ltx_text"></span><span id="A1.T2.3.12.1.2" class="ltx_text" style="font-size:90%;"> </span><span id="A1.T2.3.12.1.3" class="ltx_text" style="font-size:90%;">
<span id="A1.T2.3.12.1.3.1" class="ltx_tabular ltx_align_top">
<span id="A1.T2.3.12.1.3.1.1" class="ltx_tr">
<span id="A1.T2.3.12.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">regular</span></span>
<span id="A1.T2.3.12.1.3.1.2" class="ltx_tr">
<span id="A1.T2.3.12.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bus</span></span>
</span></span><span id="A1.T2.3.12.1.4" class="ltx_text"></span><span id="A1.T2.3.12.1.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="A1.T2.3.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.12.2.1.1" class="ltx_p"><span id="A1.T2.3.12.2.1.1.1" class="ltx_text" style="font-size:90%;">left tail light, front license plate, front right door, front bumper, back bumper, right head light, front left wheel, left mirror, right tail light, back right door, back left wheel, back right wheel, back license plate, front right wheel, left head light, right mirror, trunk, mid right door, roof</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.13" class="ltx_tr">
<td id="A1.T2.3.13.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="A1.T2.3.13.1.1" class="ltx_text"></span><span id="A1.T2.3.13.1.2" class="ltx_text" style="font-size:90%;"> </span><span id="A1.T2.3.13.1.3" class="ltx_text" style="font-size:90%;">
<span id="A1.T2.3.13.1.3.1" class="ltx_tabular ltx_align_top">
<span id="A1.T2.3.13.1.3.1.1" class="ltx_tr">
<span id="A1.T2.3.13.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">school</span></span>
<span id="A1.T2.3.13.1.3.1.2" class="ltx_tr">
<span id="A1.T2.3.13.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bus</span></span>
</span></span><span id="A1.T2.3.13.1.4" class="ltx_text"></span><span id="A1.T2.3.13.1.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="A1.T2.3.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.13.2.1.1" class="ltx_p"><span id="A1.T2.3.13.2.1.1.1" class="ltx_text" style="font-size:90%;">left tail light, front license plate, front right door, front bumper, back bumper, right head light, front left wheel, left mirror, right tail light, back left wheel, back right wheel, back license plate, mid left door, front right wheel, left head light, right mirror, roof</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.14" class="ltx_tr">
<td id="A1.T2.3.14.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.14.1.1" class="ltx_text" style="font-size:90%;">truck</span></td>
<td id="A1.T2.3.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.14.2.1.1" class="ltx_p"><span id="A1.T2.3.14.2.1.1.1" class="ltx_text" style="font-size:90%;">front left door, left tail light, left head light, back right wheel, right head light, front bumper, right mirror, front license plate, front right wheel, back bumper, left mirror, back left wheel, right tail light, hood, trunk, front left wheel, roof, front right door</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.15" class="ltx_tr">
<td id="A1.T2.3.15.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.15.1.1" class="ltx_text" style="font-size:90%;">suv</span></td>
<td id="A1.T2.3.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.15.2.1.1" class="ltx_p"><span id="A1.T2.3.15.2.1.1.1" class="ltx_text" style="font-size:90%;">front left door, left tail light, left head light, back left door, back right wheel, right head light, front bumper, right mirror, front right wheel, back bumper, left mirror, back left wheel, right tail light, hood, trunk, front left wheel, back right door, roof, front right door</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.16" class="ltx_tr">
<td id="A1.T2.3.16.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.16.1.1" class="ltx_text" style="font-size:90%;">minivan</span></td>
<td id="A1.T2.3.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.16.2.1.1" class="ltx_p"><span id="A1.T2.3.16.2.1.1.1" class="ltx_text" style="font-size:90%;">front left door, left tail light, left head light, back left door, back right wheel, right head light, front bumper, right mirror, front license plate, front right wheel, back bumper, left mirror, back left wheel, right tail light, hood, trunk, front left wheel, back right door, roof, front right door, back license plate</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.17" class="ltx_tr">
<td id="A1.T2.3.17.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.17.1.1" class="ltx_text" style="font-size:90%;">sedan</span></td>
<td id="A1.T2.3.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.17.2.1.1" class="ltx_p"><span id="A1.T2.3.17.2.1.1.1" class="ltx_text" style="font-size:90%;">front left door, left tail light, left head light, back left door, back right wheel, right head light, front bumper, right mirror, front license plate, front right wheel, back bumper, left mirror, back left wheel, right tail light, hood, trunk, front left wheel, back right door, roof, front right door, back license plate</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.18" class="ltx_tr">
<td id="A1.T2.3.18.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.18.1.1" class="ltx_text" style="font-size:90%;">wagon</span></td>
<td id="A1.T2.3.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.18.2.1.1" class="ltx_p"><span id="A1.T2.3.18.2.1.1.1" class="ltx_text" style="font-size:90%;">front left door, left tail light, left head light, back left door, back right wheel, right head light, front bumper, right mirror, front license plate, front right wheel, back bumper, left mirror, back left wheel, right tail light, hood, trunk, front left wheel, back right door, roof, front right door, back license plate</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.19" class="ltx_tr">
<td id="A1.T2.3.19.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.19.1.1" class="ltx_text" style="font-size:90%;">chopper</span></td>
<td id="A1.T2.3.19.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.19.2.1.1" class="ltx_p"><span id="A1.T2.3.19.2.1.1.1" class="ltx_text" style="font-size:90%;">left handle, center headlight, front wheel, right handle, back wheel, center taillight, left mirror, gas tank, front fender, fork, drive chain, left footrest, right mirror, windscreen, engine, back fender, right exhaust, seat, panel, right footrest</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.20" class="ltx_tr">
<td id="A1.T2.3.20.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.20.1.1" class="ltx_text" style="font-size:90%;">scooter</span></td>
<td id="A1.T2.3.20.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.20.2.1.1" class="ltx_p"><span id="A1.T2.3.20.2.1.1.1" class="ltx_text" style="font-size:90%;">left handle, center headlight, front wheel, right handle, back cover, back wheel, center taillight, left mirror, front cover, fork, drive chain, right mirror, engine, left exhaust, back fender, seat, panel</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.21" class="ltx_tr">
<td id="A1.T2.3.21.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T2.3.21.1.1" class="ltx_text" style="font-size:90%;">cruiser</span></td>
<td id="A1.T2.3.21.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_t">
<span id="A1.T2.3.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.21.2.1.1" class="ltx_p"><span id="A1.T2.3.21.2.1.1.1" class="ltx_text" style="font-size:90%;">left handle, center headlight, right headlight, right taillight, front wheel, right handle, back cover, back wheel, left taillight, left mirror, left headlight, gas tank, front cover, front fender, fork, drive chain, left footrest, license plate, right mirror, windscreen, left exhaust, back fender, right exhaust, seat, panel, right footrest</span></span>
</span>
</td>
</tr>
<tr id="A1.T2.3.22" class="ltx_tr">
<td id="A1.T2.3.22.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T2.3.22.1.1" class="ltx_text" style="font-size:90%;">dirtbike</span></td>
<td id="A1.T2.3.22.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t">
<span id="A1.T2.3.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T2.3.22.2.1.1" class="ltx_p"><span id="A1.T2.3.22.2.1.1.1" class="ltx_text" style="font-size:90%;">left handle, front wheel, right handle, back cover, back wheel, gas tank, front cover, front fender, fork, drive chain, left footrest, engine, right exhaust, seat, panel, right footrest</span></span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Question templates</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p"><span id="A1.SS2.p1.1.1" class="ltx_text ltx_font_bold">Part Questions</span>
we collect 9 part-based templates when generating the part-based questions, as shown in Tab. <a href="#A1.T3" title="Table 3 ‣ A.2 Question templates ‣ Appendix A Dataset Details ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In the table, <span id="A1.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> means one attribute from shape, material, color or size to be queried, <span id="A1.SS2.p1.1.3" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> (or <span id="A1.SS2.p1.1.4" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span>, <span id="A1.SS2.p1.1.5" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span>) means one object to be filtered with a combination of shape, material, color, and size. Different from the pose and occlusion question, we don’t query the size of the object.</p>
</div>
<figure id="A1.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Templates of parts questions</figcaption>
<div id="A1.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:67.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.4pt,2.1pt) scale(0.941797388546222,0.941797388546222) ;">
<table id="A1.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T3.1.1.1" class="ltx_tr">
<td id="A1.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Templates</span></td>
<td id="A1.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Count</span></td>
</tr>
<tr id="A1.T3.1.1.2" class="ltx_tr">
<td id="A1.T3.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">What is the <span id="A1.T3.1.1.2.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T3.1.1.2.1.2" class="ltx_text ltx_font_typewriter">&lt;part&gt;</span> of the <span id="A1.T3.1.1.2.1.3" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span>?</td>
<td id="A1.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">3</td>
</tr>
<tr id="A1.T3.1.1.3" class="ltx_tr">
<td id="A1.T3.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">What is the <span id="A1.T3.1.1.3.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T3.1.1.3.1.2" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> that has a <span id="A1.T3.1.1.3.1.3" class="ltx_text ltx_font_typewriter">&lt;part&gt;</span>?</td>
<td id="A1.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">3</td>
</tr>
<tr id="A1.T3.1.1.4" class="ltx_tr">
<td id="A1.T3.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">What is the <span id="A1.T3.1.1.4.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T3.1.1.4.1.2" class="ltx_text ltx_font_typewriter">&lt;part 1&gt;</span> that belongs to the same object as the <span id="A1.T3.1.1.4.1.3" class="ltx_text ltx_font_typewriter">&lt;part 2&gt;</span>?</td>
<td id="A1.T3.1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">3</td>
</tr>
</table>
</span></div>
</figure>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p"><span id="A1.SS2.p2.1.1" class="ltx_text ltx_font_bold">3D Pose questions</span>
We design 17 3D pose-based templates in question generation (as shown in table <a href="#A1.T4" title="Table 4 ‣ A.2 Question templates ‣ Appendix A Dataset Details ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). The 17 templates consist of: 1 template of the query of the pose; 4 questions of the query of shape, material, color, size, where the pose is in the filtering conditions; 12 templates about the query of shape, material, color, size, where the relationship of the pose is the filtering condition.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Templates of pose questions</figcaption>
<div id="A1.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:100.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.3pt,3.8pt) scale(0.93006982890906,0.93006982890906) ;">
<table id="A1.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T4.1.1.1" class="ltx_tr">
<td id="A1.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Templates</span></td>
<td id="A1.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Count</span></td>
</tr>
<tr id="A1.T4.1.1.2" class="ltx_tr">
<td id="A1.T4.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Which direction the <span id="A1.T4.1.1.2.1.1" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> is facing?</td>
<td id="A1.T4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
</tr>
<tr id="A1.T4.1.1.3" class="ltx_tr">
<td id="A1.T4.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">What is the <span id="A1.T4.1.1.3.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T4.1.1.3.1.2" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> which face to the <span id="A1.T4.1.1.3.1.3" class="ltx_text ltx_font_typewriter">&lt;O&gt;</span>?</td>
<td id="A1.T4.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">4</td>
</tr>
<tr id="A1.T4.1.1.4" class="ltx_tr">
<td id="A1.T4.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">What is the <span id="A1.T4.1.1.4.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T4.1.1.4.1.2" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span> that faces the <span id="A1.T4.1.1.4.1.3" class="ltx_text ltx_font_bold">same direction</span> as a <span id="A1.T4.1.1.4.1.4" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span>
</td>
<td id="A1.T4.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t">4</td>
</tr>
<tr id="A1.T4.1.1.5" class="ltx_tr">
<td id="A1.T4.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">What is the <span id="A1.T4.1.1.5.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T4.1.1.5.1.2" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span> that faces the <span id="A1.T4.1.1.5.1.3" class="ltx_text ltx_font_bold">opposite direction</span> as a <span id="A1.T4.1.1.5.1.4" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span>
</td>
<td id="A1.T4.1.1.5.2" class="ltx_td ltx_align_center">4</td>
</tr>
<tr id="A1.T4.1.1.6" class="ltx_tr">
<td id="A1.T4.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">What is the <span id="A1.T4.1.1.6.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T4.1.1.6.1.2" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span> that faces the <span id="A1.T4.1.1.6.1.3" class="ltx_text ltx_font_bold">vertical direction</span> as a <span id="A1.T4.1.1.6.1.4" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span>
</td>
<td id="A1.T4.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">4</td>
</tr>
</table>
</span></div>
</figure>
<div id="A1.SS2.p3" class="ltx_para">
<p id="A1.SS2.p3.1" class="ltx_p"><span id="A1.SS2.p3.1.1" class="ltx_text ltx_font_bold">Occlusion Questions</span>
There are 35 templates in the occlusion question generation as shown in table <a href="#A1.T5" title="Table 5 ‣ A.2 Question templates ‣ Appendix A Dataset Details ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, which consists of occlusion of objects and occlusion of parts.</p>
</div>
<div id="A1.SS2.p4" class="ltx_para">
<p id="A1.SS2.p4.1" class="ltx_p">The occlusion of objects consists of occlusion status and occlusion relationship. For the occlusion status of the object, there are 4 templates to query the shape, color, material, and size respectively. There are 2 occlusion relationships of objects (occluded and occluding), and each of them has 4 templates.</p>
</div>
<div id="A1.SS2.p5" class="ltx_para">
<p id="A1.SS2.p5.1" class="ltx_p">Similarly, we then create a template about occlusion status and occlusion relationship for the parts. The only difference between object and part is that the parts only have 3 attributes to be queried: shape (name), material and color.</p>
</div>
<figure id="A1.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Templates of occlusion questions</figcaption>
<div id="A1.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:227.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.3pt,14.9pt) scale(0.884586254811102,0.884586254811102) ;">
<table id="A1.T5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T5.1.1.1" class="ltx_tr">
<td id="A1.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Templates</span></td>
<td id="A1.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Count</span></td>
</tr>
<tr id="A1.T5.1.1.2" class="ltx_tr">
<td id="A1.T5.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">What is the <span id="A1.T5.1.1.2.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.2.1.2" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> that is occluded?</td>
<td id="A1.T5.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">4</td>
</tr>
<tr id="A1.T5.1.1.3" class="ltx_tr">
<td id="A1.T5.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">What is the <span id="A1.T5.1.1.3.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.3.1.2" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span> that is occluded by the <span id="A1.T5.1.1.3.1.3" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span> ?</td>
<td id="A1.T5.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">4</td>
</tr>
<tr id="A1.T5.1.1.4" class="ltx_tr">
<td id="A1.T5.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">What is the <span id="A1.T5.1.1.4.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.4.1.2" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span> that occludes the <span id="A1.T5.1.1.4.1.3" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span>?</td>
<td id="A1.T5.1.1.4.2" class="ltx_td ltx_align_center">4</td>
</tr>
<tr id="A1.T5.1.1.5" class="ltx_tr">
<td id="A1.T5.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Is the <span id="A1.T5.1.1.5.1.1" class="ltx_text ltx_font_typewriter">&lt;part&gt;</span> of the <span id="A1.T5.1.1.5.1.2" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> occluded?</td>
<td id="A1.T5.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
</tr>
<tr id="A1.T5.1.1.6" class="ltx_tr">
<td id="A1.T5.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r">Which part of the <span id="A1.T5.1.1.6.1.1" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> is occluded?</td>
<td id="A1.T5.1.1.6.2" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="A1.T5.1.1.7" class="ltx_tr">
<td id="A1.T5.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r">What is the <span id="A1.T5.1.1.7.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.7.1.2" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> whose <span id="A1.T5.1.1.7.1.3" class="ltx_text ltx_font_typewriter">&lt;part&gt;</span> is occluded?</td>
<td id="A1.T5.1.1.7.2" class="ltx_td ltx_align_center">4</td>
</tr>
<tr id="A1.T5.1.1.8" class="ltx_tr">
<td id="A1.T5.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r">What is the <span id="A1.T5.1.1.8.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.8.1.2" class="ltx_text ltx_font_typewriter">&lt;part&gt;</span> which belongs to an occluded &lt;object&gt;?</td>
<td id="A1.T5.1.1.8.2" class="ltx_td ltx_align_center">3</td>
</tr>
<tr id="A1.T5.1.1.9" class="ltx_tr">
<td id="A1.T5.1.1.9.1" class="ltx_td ltx_align_left ltx_border_r">What is the <span id="A1.T5.1.1.9.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.9.1.2" class="ltx_text ltx_font_typewriter">&lt;part 1&gt;</span> which belongs to the <span id="A1.T5.1.1.9.1.3" class="ltx_text ltx_font_typewriter">&lt;object&gt;</span> whose <span id="A1.T5.1.1.9.1.4" class="ltx_text ltx_font_typewriter">&lt;part 2&gt;</span> is occluded?</td>
<td id="A1.T5.1.1.9.2" class="ltx_td ltx_align_center">3</td>
</tr>
<tr id="A1.T5.1.1.10" class="ltx_tr">
<td id="A1.T5.1.1.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Is the <span id="A1.T5.1.1.10.1.1" class="ltx_text ltx_font_typewriter">&lt;part&gt;</span> of the <span id="A1.T5.1.1.10.1.2" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span> occluded by the <span id="A1.T5.1.1.10.1.3" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span>
</td>
<td id="A1.T5.1.1.10.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
</tr>
<tr id="A1.T5.1.1.11" class="ltx_tr">
<td id="A1.T5.1.1.11.1" class="ltx_td ltx_align_left ltx_border_r">What is the <span id="A1.T5.1.1.11.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.11.1.2" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span> whose <span id="A1.T5.1.1.11.1.3" class="ltx_text ltx_font_typewriter">&lt;part&gt;</span> is occluded by the <span id="A1.T5.1.1.11.1.4" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span> ?</td>
<td id="A1.T5.1.1.11.2" class="ltx_td ltx_align_center">4</td>
</tr>
<tr id="A1.T5.1.1.12" class="ltx_tr">
<td id="A1.T5.1.1.12.1" class="ltx_td ltx_align_left ltx_border_r">What is the <span id="A1.T5.1.1.12.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.12.1.2" class="ltx_text ltx_font_typewriter">&lt;part&gt;</span> which belongs to <span id="A1.T5.1.1.12.1.3" class="ltx_text ltx_font_typewriter">&lt;object 1&gt;</span> which is occluded by the <span id="A1.T5.1.1.12.1.4" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span>
</td>
<td id="A1.T5.1.1.12.2" class="ltx_td ltx_align_center">3</td>
</tr>
<tr id="A1.T5.1.1.13" class="ltx_tr">
<td id="A1.T5.1.1.13.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">What is the <span id="A1.T5.1.1.13.1.1" class="ltx_text ltx_font_typewriter">&lt;attribute&gt;</span> of the <span id="A1.T5.1.1.13.1.2" class="ltx_text ltx_font_typewriter">&lt;part 1&gt;</span> which belongs to the same object whose <span id="A1.T5.1.1.13.1.3" class="ltx_text ltx_font_typewriter">&lt;part 2&gt;</span> is occluded by the <span id="A1.T5.1.1.13.1.4" class="ltx_text ltx_font_typewriter">&lt;object 2&gt;</span>?</td>
<td id="A1.T5.1.1.13.2" class="ltx_td ltx_align_center ltx_border_bb">3</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Statistics</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">As a result, a total of 314,988 part questions, 314,986 pose questions, and 228,397 occlusion questions and 314,988 occlusion questions with parts.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.1" class="ltx_p">In Fig. <a href="#A1.F8" title="Figure 8 ‣ A.3 Statistics ‣ Appendix A Dataset Details ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we show the distributions of all attributes of objects including categories, colors, sizes, and materials</p>
</div>
<figure id="A1.F8" class="ltx_figure"><img src="/html/2310.17914/assets/rebuttal_images/distribution.jpg" id="A1.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="160" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Distributions for all the attributes of objects including categories, colors, sizes, and materials </figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation details for the baselines</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">The FiLM and mDETR are trained with default settings as in the official
implementation. FiLM is trained for 100k iterations with batch size 256. mDETR is trained for 30 epochs with batch size 64 using 2 GPUs for both the grounding stage and the answer classification stage.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">For P-NSVQA, we first train a MaskRCNN for 30k iterations with batch size 16 to detect the objects and parts, then train the
attribute extraction model (using Res50 backbone) for 100 epochs with batch size 64. Different fully connected(FC) layers are used for a different type of question: the part questions and occlusion questions have 4 FC layers for the shape, material, color, and size classification (as the parts also have size annotations in the dataset when generating scene files, but they are meaningless in the question answering). The pose question includes pose prediction of an object, so we add a new FC layer with 1 output dimension to predict the rotations, followed by an MSE loss during training. For different types of questions (part, pose and occlusion), the MaskRCNN and attribute extraction model are trained separately.</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">In the PNSVQA+Projection baseline, we first train a MaskRCNN to detect all of the objects and predict their 3D pose (azimuth, elevation and theta) without category labels in the scene. This MaskRCNN is trained with batch size 8 and iteration 15000. We use an SGD optimizer with a learning rate of 0.02, momentum of 0.9 and weight decay 0.0001. Then, we use the same setting as our PO3D-VQA to train a CNN to classify the attributes of objects and parts.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Detailed results of Analysis</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">As an extension for section 5.4 in main paper, here we include the numerical value of accuracy and drop for the pose, part, occlusion + part question with reference to occlusion ratio or part size. The result is shown in Tab. <a href="#A3.T6" title="Table 6 ‣ Appendix C Detailed results of Analysis ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, Tab. <a href="#A3.T8" title="Table 8 ‣ Appendix C Detailed results of Analysis ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and Tab. <a href="#A3.T7" title="Table 7 ‣ Appendix C Detailed results of Analysis ‣ 3D-Aware Visual Question Answering about Parts, Poses and Occlusions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="A3.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span> Accuracy value and relative drop for pose questions wrt. occlusion ratio</figcaption>
<div id="A3.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:110.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.2pt,7.9pt) scale(0.874232112974126,0.874232112974126) ;">
<table id="A3.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A3.T6.1.1.1" class="ltx_tr">
<td id="A3.T6.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A3.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Occlusion Ratio</span></td>
<td id="A3.T6.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">0</span></td>
<td id="A3.T6.1.1.1.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">5</span></td>
<td id="A3.T6.1.1.1.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T6.1.1.1.5.1" class="ltx_text ltx_font_bold">10</span></td>
<td id="A3.T6.1.1.1.6" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T6.1.1.1.6.1" class="ltx_text ltx_font_bold">15</span></td>
<td id="A3.T6.1.1.1.7" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T6.1.1.1.7.1" class="ltx_text ltx_font_bold">20</span></td>
<td id="A3.T6.1.1.1.8" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T6.1.1.1.8.1" class="ltx_text ltx_font_bold">25</span></td>
<td id="A3.T6.1.1.1.9" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T6.1.1.1.9.1" class="ltx_text ltx_font_bold">30</span></td>
</tr>
<tr id="A3.T6.1.1.2" class="ltx_tr">
<td id="A3.T6.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A3.T6.1.1.2.1.1" class="ltx_text">PNSVQA</span></td>
<td id="A3.T6.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T6.1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">87.43</td>
<td id="A3.T6.1.1.2.4" class="ltx_td ltx_align_left ltx_border_t">74.09</td>
<td id="A3.T6.1.1.2.5" class="ltx_td ltx_align_left ltx_border_t">74.09</td>
<td id="A3.T6.1.1.2.6" class="ltx_td ltx_align_left ltx_border_t">63.16</td>
<td id="A3.T6.1.1.2.7" class="ltx_td ltx_align_left ltx_border_t">62.01</td>
<td id="A3.T6.1.1.2.8" class="ltx_td ltx_align_left ltx_border_t">60.33</td>
<td id="A3.T6.1.1.2.9" class="ltx_td ltx_align_left ltx_border_t">58.52</td>
</tr>
<tr id="A3.T6.1.1.3" class="ltx_tr">
<td id="A3.T6.1.1.3.1" class="ltx_td ltx_align_left">Drop</td>
<td id="A3.T6.1.1.3.2" class="ltx_td ltx_align_left">0.00%</td>
<td id="A3.T6.1.1.3.3" class="ltx_td ltx_align_left">15.26%</td>
<td id="A3.T6.1.1.3.4" class="ltx_td ltx_align_left">15.26%</td>
<td id="A3.T6.1.1.3.5" class="ltx_td ltx_align_left">27.76%</td>
<td id="A3.T6.1.1.3.6" class="ltx_td ltx_align_left">29.08%</td>
<td id="A3.T6.1.1.3.7" class="ltx_td ltx_align_left">31.00%</td>
<td id="A3.T6.1.1.3.8" class="ltx_td ltx_align_left">33.07%</td>
</tr>
<tr id="A3.T6.1.1.4" class="ltx_tr">
<td id="A3.T6.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A3.T6.1.1.4.1.1" class="ltx_text">PNSVQA + Projection</span></td>
<td id="A3.T6.1.1.4.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T6.1.1.4.3" class="ltx_td ltx_align_left ltx_border_t">86.30</td>
<td id="A3.T6.1.1.4.4" class="ltx_td ltx_align_left ltx_border_t">74.61</td>
<td id="A3.T6.1.1.4.5" class="ltx_td ltx_align_left ltx_border_t">67.20</td>
<td id="A3.T6.1.1.4.6" class="ltx_td ltx_align_left ltx_border_t">66.78</td>
<td id="A3.T6.1.1.4.7" class="ltx_td ltx_align_left ltx_border_t">60.26</td>
<td id="A3.T6.1.1.4.8" class="ltx_td ltx_align_left ltx_border_t">56.52</td>
<td id="A3.T6.1.1.4.9" class="ltx_td ltx_align_left ltx_border_t">55.56</td>
</tr>
<tr id="A3.T6.1.1.5" class="ltx_tr">
<td id="A3.T6.1.1.5.1" class="ltx_td ltx_align_left">Drop</td>
<td id="A3.T6.1.1.5.2" class="ltx_td ltx_align_left">0.00%</td>
<td id="A3.T6.1.1.5.3" class="ltx_td ltx_align_left">13.54%</td>
<td id="A3.T6.1.1.5.4" class="ltx_td ltx_align_left">22.13%</td>
<td id="A3.T6.1.1.5.5" class="ltx_td ltx_align_left">22.62%</td>
<td id="A3.T6.1.1.5.6" class="ltx_td ltx_align_left">30.17%</td>
<td id="A3.T6.1.1.5.7" class="ltx_td ltx_align_left">34.51%</td>
<td id="A3.T6.1.1.5.8" class="ltx_td ltx_align_left">35.63%</td>
</tr>
<tr id="A3.T6.1.1.6" class="ltx_tr">
<td id="A3.T6.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="A3.T6.1.1.6.1.1" class="ltx_text">Ours</span></td>
<td id="A3.T6.1.1.6.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T6.1.1.6.3" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T6.1.1.6.3.1" class="ltx_text ltx_font_bold">86.43</span></td>
<td id="A3.T6.1.1.6.4" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T6.1.1.6.4.1" class="ltx_text ltx_font_bold">86.05</span></td>
<td id="A3.T6.1.1.6.5" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T6.1.1.6.5.1" class="ltx_text ltx_font_bold">84.32</span></td>
<td id="A3.T6.1.1.6.6" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T6.1.1.6.6.1" class="ltx_text ltx_font_bold">75.00</span></td>
<td id="A3.T6.1.1.6.7" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T6.1.1.6.7.1" class="ltx_text ltx_font_bold">79.44</span></td>
<td id="A3.T6.1.1.6.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T6.1.1.6.8.1" class="ltx_text ltx_font_bold">73.22</span></td>
<td id="A3.T6.1.1.6.9" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T6.1.1.6.9.1" class="ltx_text ltx_font_bold">67.98</span></td>
</tr>
<tr id="A3.T6.1.1.7" class="ltx_tr">
<td id="A3.T6.1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">Drop</td>
<td id="A3.T6.1.1.7.2" class="ltx_td ltx_align_left ltx_border_bb">0.00%</td>
<td id="A3.T6.1.1.7.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T6.1.1.7.3.1" class="ltx_text ltx_font_bold">0.44%</span></td>
<td id="A3.T6.1.1.7.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T6.1.1.7.4.1" class="ltx_text ltx_font_bold">2.44%</span></td>
<td id="A3.T6.1.1.7.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T6.1.1.7.5.1" class="ltx_text ltx_font_bold">13.22%</span></td>
<td id="A3.T6.1.1.7.6" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T6.1.1.7.6.1" class="ltx_text ltx_font_bold">8.09%</span></td>
<td id="A3.T6.1.1.7.7" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T6.1.1.7.7.1" class="ltx_text ltx_font_bold">15.28%</span></td>
<td id="A3.T6.1.1.7.8" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T6.1.1.7.8.1" class="ltx_text ltx_font_bold">21.35%</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="A3.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span> Accuracy value and relative drop for occlusion + part wrt. part size</figcaption>
<div id="A3.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:130.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.4pt,-2.2pt) scale(1.03530238118615,1.03530238118615) ;">
<table id="A3.T7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A3.T7.1.1.1" class="ltx_tr">
<td id="A3.T7.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A3.T7.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T7.1.1.1.2.1" class="ltx_text ltx_font_bold">Part Size</span></td>
<td id="A3.T7.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T7.1.1.1.3.1" class="ltx_text ltx_font_bold">max</span></td>
<td id="A3.T7.1.1.1.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T7.1.1.1.4.1" class="ltx_text ltx_font_bold">300</span></td>
<td id="A3.T7.1.1.1.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T7.1.1.1.5.1" class="ltx_text ltx_font_bold">150</span></td>
<td id="A3.T7.1.1.1.6" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T7.1.1.1.6.1" class="ltx_text ltx_font_bold">100</span></td>
<td id="A3.T7.1.1.1.7" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T7.1.1.1.7.1" class="ltx_text ltx_font_bold">50</span></td>
<td id="A3.T7.1.1.1.8" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T7.1.1.1.8.1" class="ltx_text ltx_font_bold">20</span></td>
</tr>
<tr id="A3.T7.1.1.2" class="ltx_tr">
<td id="A3.T7.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A3.T7.1.1.2.1.1" class="ltx_text">PNSVQA</span></td>
<td id="A3.T7.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T7.1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">58.18</td>
<td id="A3.T7.1.1.2.4" class="ltx_td ltx_align_left ltx_border_t">54.98</td>
<td id="A3.T7.1.1.2.5" class="ltx_td ltx_align_left ltx_border_t">54.05</td>
<td id="A3.T7.1.1.2.6" class="ltx_td ltx_align_left ltx_border_t">52.09</td>
<td id="A3.T7.1.1.2.7" class="ltx_td ltx_align_left ltx_border_t">45.20</td>
<td id="A3.T7.1.1.2.8" class="ltx_td ltx_align_left ltx_border_t">21.28</td>
</tr>
<tr id="A3.T7.1.1.3" class="ltx_tr">
<td id="A3.T7.1.1.3.1" class="ltx_td ltx_align_left">Drop</td>
<td id="A3.T7.1.1.3.2" class="ltx_td ltx_align_left">0.00%</td>
<td id="A3.T7.1.1.3.3" class="ltx_td ltx_align_left">5.49%</td>
<td id="A3.T7.1.1.3.4" class="ltx_td ltx_align_left">7.10%</td>
<td id="A3.T7.1.1.3.5" class="ltx_td ltx_align_left">10.47%</td>
<td id="A3.T7.1.1.3.6" class="ltx_td ltx_align_left">22.31%</td>
<td id="A3.T7.1.1.3.7" class="ltx_td ltx_align_left">63.43%</td>
</tr>
<tr id="A3.T7.1.1.4" class="ltx_tr">
<td id="A3.T7.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A3.T7.1.1.4.1.1" class="ltx_text">PNSVQA + Projection</span></td>
<td id="A3.T7.1.1.4.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T7.1.1.4.3" class="ltx_td ltx_align_left ltx_border_t">61.85</td>
<td id="A3.T7.1.1.4.4" class="ltx_td ltx_align_left ltx_border_t">50.64</td>
<td id="A3.T7.1.1.4.5" class="ltx_td ltx_align_left ltx_border_t">56.77</td>
<td id="A3.T7.1.1.4.6" class="ltx_td ltx_align_left ltx_border_t">53.97</td>
<td id="A3.T7.1.1.4.7" class="ltx_td ltx_align_left ltx_border_t">55.29</td>
<td id="A3.T7.1.1.4.8" class="ltx_td ltx_align_left ltx_border_t">45.83</td>
</tr>
<tr id="A3.T7.1.1.5" class="ltx_tr">
<td id="A3.T7.1.1.5.1" class="ltx_td ltx_align_left">Drop</td>
<td id="A3.T7.1.1.5.2" class="ltx_td ltx_align_left">0.00%</td>
<td id="A3.T7.1.1.5.3" class="ltx_td ltx_align_left">18.11%</td>
<td id="A3.T7.1.1.5.4" class="ltx_td ltx_align_left">8.20%</td>
<td id="A3.T7.1.1.5.5" class="ltx_td ltx_align_left">12.74%</td>
<td id="A3.T7.1.1.5.6" class="ltx_td ltx_align_left"><span id="A3.T7.1.1.5.6.1" class="ltx_text ltx_font_bold">10.60%</span></td>
<td id="A3.T7.1.1.5.7" class="ltx_td ltx_align_left"><span id="A3.T7.1.1.5.7.1" class="ltx_text ltx_font_bold">25.89%</span></td>
</tr>
<tr id="A3.T7.1.1.6" class="ltx_tr">
<td id="A3.T7.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="A3.T7.1.1.6.1.1" class="ltx_text">Ours</span></td>
<td id="A3.T7.1.1.6.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T7.1.1.6.3" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T7.1.1.6.3.1" class="ltx_text ltx_font_bold">81.68</span></td>
<td id="A3.T7.1.1.6.4" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T7.1.1.6.4.1" class="ltx_text ltx_font_bold">75.32</span></td>
<td id="A3.T7.1.1.6.5" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T7.1.1.6.5.1" class="ltx_text ltx_font_bold">77.20</span></td>
<td id="A3.T7.1.1.6.6" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T7.1.1.6.6.1" class="ltx_text ltx_font_bold">71.54</span></td>
<td id="A3.T7.1.1.6.7" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T7.1.1.6.7.1" class="ltx_text ltx_font_bold">67.00</span></td>
<td id="A3.T7.1.1.6.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T7.1.1.6.8.1" class="ltx_text ltx_font_bold">53.19</span></td>
</tr>
<tr id="A3.T7.1.1.7" class="ltx_tr">
<td id="A3.T7.1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">Drop</td>
<td id="A3.T7.1.1.7.2" class="ltx_td ltx_align_left ltx_border_bb">0.00%</td>
<td id="A3.T7.1.1.7.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T7.1.1.7.3.1" class="ltx_text ltx_font_bold">7.78%</span></td>
<td id="A3.T7.1.1.7.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T7.1.1.7.4.1" class="ltx_text ltx_font_bold">5.49%</span></td>
<td id="A3.T7.1.1.7.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T7.1.1.7.5.1" class="ltx_text ltx_font_bold">12.41%</span></td>
<td id="A3.T7.1.1.7.6" class="ltx_td ltx_align_left ltx_border_bb">17.97%</td>
<td id="A3.T7.1.1.7.7" class="ltx_td ltx_align_left ltx_border_bb">34.88%</td>
</tr>
</table>
</span></div>
</figure>
<figure id="A3.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span> Accuracy value and relative drop for part wrt. part size</figcaption>
<div id="A3.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:127.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(3.2pt,-1.0pt) scale(1.01510531455269,1.01510531455269) ;">
<table id="A3.T8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A3.T8.1.1.1" class="ltx_tr">
<td id="A3.T8.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A3.T8.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Part Size</span></td>
<td id="A3.T8.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">max</span></td>
<td id="A3.T8.1.1.1.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">300</span></td>
<td id="A3.T8.1.1.1.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T8.1.1.1.5.1" class="ltx_text ltx_font_bold">150</span></td>
<td id="A3.T8.1.1.1.6" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T8.1.1.1.6.1" class="ltx_text ltx_font_bold">100</span></td>
<td id="A3.T8.1.1.1.7" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T8.1.1.1.7.1" class="ltx_text ltx_font_bold">50</span></td>
<td id="A3.T8.1.1.1.8" class="ltx_td ltx_align_left ltx_border_tt"><span id="A3.T8.1.1.1.8.1" class="ltx_text ltx_font_bold">20</span></td>
</tr>
<tr id="A3.T8.1.1.2" class="ltx_tr">
<td id="A3.T8.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A3.T8.1.1.2.1.1" class="ltx_text">PNSVQA</span></td>
<td id="A3.T8.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T8.1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">57.31</td>
<td id="A3.T8.1.1.2.4" class="ltx_td ltx_align_left ltx_border_t">51.00</td>
<td id="A3.T8.1.1.2.5" class="ltx_td ltx_align_left ltx_border_t">37.50</td>
<td id="A3.T8.1.1.2.6" class="ltx_td ltx_align_left ltx_border_t">44.18</td>
<td id="A3.T8.1.1.2.7" class="ltx_td ltx_align_left ltx_border_t">40.85</td>
<td id="A3.T8.1.1.2.8" class="ltx_td ltx_align_left ltx_border_t">29.73</td>
</tr>
<tr id="A3.T8.1.1.3" class="ltx_tr">
<td id="A3.T8.1.1.3.1" class="ltx_td ltx_align_left">Drop</td>
<td id="A3.T8.1.1.3.2" class="ltx_td ltx_align_left">0.00%</td>
<td id="A3.T8.1.1.3.3" class="ltx_td ltx_align_left">11.02%</td>
<td id="A3.T8.1.1.3.4" class="ltx_td ltx_align_left">34.57%</td>
<td id="A3.T8.1.1.3.5" class="ltx_td ltx_align_left">22.92%</td>
<td id="A3.T8.1.1.3.6" class="ltx_td ltx_align_left">28.73%</td>
<td id="A3.T8.1.1.3.7" class="ltx_td ltx_align_left">48.12%</td>
</tr>
<tr id="A3.T8.1.1.4" class="ltx_tr">
<td id="A3.T8.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A3.T8.1.1.4.1.1" class="ltx_text">PNSVQA + Projection</span></td>
<td id="A3.T8.1.1.4.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T8.1.1.4.3" class="ltx_td ltx_align_left ltx_border_t">58.89</td>
<td id="A3.T8.1.1.4.4" class="ltx_td ltx_align_left ltx_border_t">57.54</td>
<td id="A3.T8.1.1.4.5" class="ltx_td ltx_align_left ltx_border_t">42.64</td>
<td id="A3.T8.1.1.4.6" class="ltx_td ltx_align_left ltx_border_t">43.20</td>
<td id="A3.T8.1.1.4.7" class="ltx_td ltx_align_left ltx_border_t">46.73</td>
<td id="A3.T8.1.1.4.8" class="ltx_td ltx_align_left ltx_border_t">38.67</td>
</tr>
<tr id="A3.T8.1.1.5" class="ltx_tr">
<td id="A3.T8.1.1.5.1" class="ltx_td ltx_align_left">Drop</td>
<td id="A3.T8.1.1.5.2" class="ltx_td ltx_align_left">0.00%</td>
<td id="A3.T8.1.1.5.3" class="ltx_td ltx_align_left">2.30%</td>
<td id="A3.T8.1.1.5.4" class="ltx_td ltx_align_left">27.60%</td>
<td id="A3.T8.1.1.5.5" class="ltx_td ltx_align_left">26.65%</td>
<td id="A3.T8.1.1.5.6" class="ltx_td ltx_align_left"><span id="A3.T8.1.1.5.6.1" class="ltx_text ltx_font_bold">20.65%</span></td>
<td id="A3.T8.1.1.5.7" class="ltx_td ltx_align_left">34.34%</td>
</tr>
<tr id="A3.T8.1.1.6" class="ltx_tr">
<td id="A3.T8.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="A3.T8.1.1.6.1.1" class="ltx_text">Ours</span></td>
<td id="A3.T8.1.1.6.2" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A3.T8.1.1.6.3" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.1.6.3.1" class="ltx_text ltx_font_bold">64.04</span></td>
<td id="A3.T8.1.1.6.4" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.1.6.4.1" class="ltx_text ltx_font_bold">64.80</span></td>
<td id="A3.T8.1.1.6.5" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.1.6.5.1" class="ltx_text ltx_font_bold">60.16</span></td>
<td id="A3.T8.1.1.6.6" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.1.6.6.1" class="ltx_text ltx_font_bold">57.03</span></td>
<td id="A3.T8.1.1.6.7" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.1.6.7.1" class="ltx_text ltx_font_bold">49.05</span></td>
<td id="A3.T8.1.1.6.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.1.6.8.1" class="ltx_text ltx_font_bold">55.41</span></td>
</tr>
<tr id="A3.T8.1.1.7" class="ltx_tr">
<td id="A3.T8.1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">Drop</td>
<td id="A3.T8.1.1.7.2" class="ltx_td ltx_align_left ltx_border_bb">0.00%</td>
<td id="A3.T8.1.1.7.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T8.1.1.7.3.1" class="ltx_text ltx_font_bold">-1.19%</span></td>
<td id="A3.T8.1.1.7.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T8.1.1.7.4.1" class="ltx_text ltx_font_bold">6.06%</span></td>
<td id="A3.T8.1.1.7.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T8.1.1.7.5.1" class="ltx_text ltx_font_bold">10.94%</span></td>
<td id="A3.T8.1.1.7.6" class="ltx_td ltx_align_left ltx_border_bb">23.41%</td>
<td id="A3.T8.1.1.7.7" class="ltx_td ltx_align_left ltx_border_bb"><span id="A3.T8.1.1.7.7.1" class="ltx_text ltx_font_bold">13.48%</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Failure cases</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Examples of failure cases of our PO3D-VQA, as described in Section 5.4 in main paper. In (a) and (b), PO3D-VQA misses the bicycle behind when two bicycles have a heavy overlap, the same for the two motorbikes in (c) and (d).</p>
</div>
<figure id="A4.F9" class="ltx_figure"><img src="/html/2310.17914/assets/rebuttal_images/fail_ours.png" id="A4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Failure cases of our PO3D-VQA. (a) and (c) is the input images with the objects missed by the model. (b) and (c) is the re-projection results from the model.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.17913" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.17914" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.17914">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.17914" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.17915" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 00:18:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
