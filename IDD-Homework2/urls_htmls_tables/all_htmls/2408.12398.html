<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation</title>
<!--Generated on Thu Aug 22 13:37:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Faithfulness metrics; Citation evaluation; Large language models" lang="en" name="keywords"/>
<base href="/html/2408.12398v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S1" title="In A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S2" title="In A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Evaluation Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S2.SS1" title="In 2. Evaluation Framework ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Task Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S2.SS2" title="In 2. Evaluation Framework ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Evaluation Protocols</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S2.SS2.SSS1" title="In 2.2. Evaluation Protocols ‣ 2. Evaluation Framework ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Correlation Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S2.SS2.SSS2" title="In 2.2. Evaluation Protocols ‣ 2. Evaluation Framework ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Classification Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S2.SS2.SSS3" title="In 2.2. Evaluation Protocols ‣ 2. Evaluation Framework ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Retrieval Evaluation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S3" title="In A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Faithfulness Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S3.SS1" title="In 3. Faithfulness Metrics ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Similarity-Based Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S3.SS2" title="In 3. Faithfulness Metrics ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Entailment-Based Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S4" title="In A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S4.SS1" title="In 4. Experiments ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S4.SS1.SSS1" title="In 4.1. Datasets ‣ 4. Experiments ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Data Statistics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S4.SS1.SSS2" title="In 4.1. Datasets ‣ 4. Experiments ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Data Processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S4.SS2" title="In 4. Experiments ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Meta-Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S5" title="In A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results &amp; Analyses</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S5.SS1" title="In 5. Results &amp; Analyses ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Correlation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S5.SS2" title="In 5. Results &amp; Analyses ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Classification Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S5.SS3" title="In 5. Results &amp; Analyses ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Retrieval Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S5.SS4" title="In 5. Results &amp; Analyses ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Implications</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S6" title="In A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S6.SS1" title="In 6. Related Work ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Faithfulness Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S6.SS2" title="In 6. Related Work ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Citation Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#S7" title="In A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weijia Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of Amsterdam</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Amsterdam</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Netherlands</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Aliannejadi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">University of Amsterdam</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Amsterdam</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Netherlands</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiahuan Pei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Centrum Wiskunde &amp; Informatica</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Amsterdam</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Netherlands</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yifei Yuan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">University of Copenhagen</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Copenhagen</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">Denmark</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jia-Hong Huang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">University of Amsterdam</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Amsterdam</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">Netherlands</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Evangelos Kanoulas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">University of Amsterdam</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Amsterdam</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">Netherlands</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id19.id1">Large language models (LLMs) often generate content with unsupported or unverifiable content, known as “hallucinations.”
To address this, retrieval-augmented LLMs are employed to include citations in their content, grounding the content in verifiable sources.
Despite such developments, manually assessing how well a citation supports the associated statement remains a major challenge.
Previous studies tackle this challenge by leveraging faithfulness metrics to estimate citation support automatically.
However, they limit this citation support estimation to a binary classification scenario, neglecting fine-grained citation support in practical scenarios.
To investigate the effectiveness of faithfulness metrics in fine-grained scenarios, we propose a comparative evaluation framework that assesses the metric effectiveness in distinguishing citations between three-category support levels: <span class="ltx_text ltx_font_italic" id="id19.id1.1">full</span>, <span class="ltx_text ltx_font_italic" id="id19.id1.2">partial</span>, and <span class="ltx_text ltx_font_italic" id="id19.id1.3">no</span> support.
Our framework employs correlation analysis, classification evaluation, and retrieval evaluation to measure the alignment between metric scores and human judgments comprehensively.
Our results indicate no single metric consistently excels across all evaluations, highlighting the complexity of accurately evaluating fine-grained support levels.
Particularly, we find that the best-performing metrics struggle to distinguish partial support from full or no support.
Based on these findings, we provide practical recommendations for developing more effective metrics.</p>
</div>
<div class="ltx_keywords">Faithfulness metrics; Citation evaluation; Large language models
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>The First Workshop on Large Language Model for Evaluation in Information Retrieval; July 18, 2024; Washington D.C., USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>The First Workshop on Large Language Model for Evaluation in Information Retrieval (LLM4Eval@SIGIR2024), July 18, 2024, Washington D.C., USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span></span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language generation</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Evaluation of retrieval results</span></span></span>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="614" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.3.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S0.F1.4.2" style="font-size:90%;">An example of <span class="ltx_text ltx_font_italic" id="S0.F1.4.2.1">partial support</span> in citation evaluation. A retrieval-augmented LLM generates a response that includes citations based on a given user query. The human assessor annotates that the first citation partially supports the associated statement. Inconsistent metric scores are observed when assessing the statement using three distinct faithfulness metrics.</span></figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) suffer from generating content known as “hallucinations” <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib52" title="">2023a</a>)</cite>, which refers to content that either contradicts established world knowledge or cannot be verified by any reliable source of information.
Mainstream studies <cite class="ltx_cite ltx_citemacro_citep">(Bohnet et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib2" title="">2022</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib8" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib28" title="">2024c</a>)</cite> aims to mitigate this issue by leveraging retrieval-augmented LLMs to generate responses with in-line citations, which contain supporting evidence to verify the statements in the responses.
One primary challenge in this field is to assess how well a citation supports its associated statement, since manually evaluating citations is labor-intensive and time-consuming.
To this end, automated citation evaluation has been explored to minimize reliance on human assessments <cite class="ltx_cite ltx_citemacro_citep">(Bohnet et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib2" title="">2022</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib28" title="">2024c</a>)</cite>.
Given the early stage of this research, faithfulness evaluation metrics <cite class="ltx_cite ltx_citemacro_citep">(Maynez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib33" title="">2020</a>; Kryscinski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib24" title="">2020</a>; Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib11" title="">2022</a>)</cite> have been employed as proxies to automatically estimate the support levels of the citations <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib7" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib8" title="">b</a>)</cite>.
This is motivated by the observation that these metrics measure the extent to how faithful the model-generated text is to the sourced text, aligning closely with the objectives of automated citation evaluation.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Prior studies <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib7" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib8" title="">b</a>)</cite> have primarily limited the application of faithfulness metrics in automated citation evaluation to a binary classification scenario.
In this scenario, faithfulness metrics are solely tasked with determining whether a citation supports the associated statement.
This binary approach fails to capture the fine-grained citation support encountered in real-world applications.
For instance, consider a “<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">partial support</span>” scenario illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2408.12398v1#S0.F1" title="Figure 1 ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>. Given a user query “<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">Where is the most humid place in Australia?</span>”, a retrieval-augmented LLM generates a response along with multiple citations.
A human assessor categorizes the first citation as “partial support” since it only supports the initial segment of the statement: “<span class="ltx_text ltx_font_italic" id="S1.p2.1.3">the most humid place in Australia is Macquarie Island</span>”. However, it does not provide evidence for the latter part of the statement: “<span class="ltx_text ltx_font_italic" id="S1.p2.1.4">which is located in the Southern Ocean off the coast of Tasmania</span>”. The complexity of this partial support scenario leads to noticeable inconsistencies across three distinct faithfulness metrics.
However, the effectiveness of faithfulness metrics in accurately distinguishing citations in such fine-grained citation support scenarios remains largely under-explored.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the issue above, we propose a comparative evaluation framework designed to assess the effectiveness of faithfulness metrics against human judgments in fine-grained levels of support scenarios.
In our framework, we define “<span class="ltx_text ltx_font_italic" id="S1.p3.1.1">support levels</span>” as the extent to which a citation supports the corresponding statement <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib30" title="">2023</a>; Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib44" title="">2023</a>)</cite>.
More specifically, in contrast to previous studies that predominantly focus on binary classification scenarios, our framework aims to evaluate the effectiveness of faithfulness metrics in a three-category support level scenario: <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">full support</span>, <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">partial support</span>, and <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">no support</span>.
These categories indicate whether a citation provides <span class="ltx_text ltx_font_italic" id="S1.p3.1.5">full</span>, <span class="ltx_text ltx_font_italic" id="S1.p3.1.6">partial</span>, or <span class="ltx_text ltx_font_italic" id="S1.p3.1.7">no</span> support to the associated statement.
To comprehensively assess the metric effectiveness, we measure the alignment between metric scores and human judgments by employing three distinct types of evaluation protocols:

<span class="ltx_inline-enumerate" id="S1.I1">
<span class="ltx_inline-item" id="S1.I1.i1"><span class="ltx_tag ltx_tag_inline-item">1)</span> <span class="ltx_text ltx_font_italic" id="S1.I1.i1.1">Correlation analysis:</span><span class="ltx_text" id="S1.I1.i1.2">we employ a standard correlation analysis to determine the extent to which metric scores correlate with human judgments. This analysis highlights the general trend in the relationship between these two variables, offering insights into their alignment;
</span></span>
<span class="ltx_inline-item" id="S1.I1.i2"><span class="ltx_tag ltx_tag_inline-item">2)</span> <span class="ltx_text ltx_font_italic" id="S1.I1.i2.1">Classification evaluation:</span><span class="ltx_text" id="S1.I1.i2.2">we conduct a classification evaluation to assess the metrics’ capability to distinguish citations based on their support levels. This evaluation specifically measures the accuracy of the metrics in distinguishing between partial, full, and no support scenarios, providing a clear indication of their effectiveness in three-way classification scenarios; and
</span></span>
<span class="ltx_inline-item" id="S1.I1.i3"><span class="ltx_tag ltx_tag_inline-item">3)</span> <span class="ltx_text ltx_font_italic" id="S1.I1.i3.1">Retrieval evaluation:</span><span class="ltx_text" id="S1.I1.i3.2">we undertake a retrieval evaluation to assess the effectiveness of metrics in ranking citations according to their support levels. This is motivated by the observation that the previous two evaluation protocols assume citations are present within statements. However, this assumption is not always valid in practical applications <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib30" title="">2023</a>)</cite>. In these cases, faithfulness metrics are adapted to retrieve potential citations from a pool of candidates <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib7" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib8" title="">b</a>)</cite>. The retrieval evaluation thus plays a pivotal role in determining the practical utility of metric adaptations.
</span></span>
</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In our experiments, we assess seven widely used faithfulness metrics, categorizing them into <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">similarity-based</span> and <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">entailment-based</span> metrics. Our experimental findings are as follows:

<span class="ltx_inline-enumerate" id="S1.I2">
<span class="ltx_inline-item" id="S1.I2.i1"><span class="ltx_tag ltx_tag_inline-item">1)</span> <span class="ltx_text" id="S1.I2.i1.1">no single faithfulness metric consistently outperforms others across three evaluation protocols. This suggests that these protocols are complementary and should be integrated to provide a comprehensive evaluation of metric performance;
</span></span>
<span class="ltx_inline-item" id="S1.I2.i2"><span class="ltx_tag ltx_tag_inline-item">2)</span> <span class="ltx_text" id="S1.I2.i2.1">the best-performing metrics like the entailment-based <span class="ltx_text ltx_font_smallcaps" id="S1.I2.i2.1.1">AutoAIS</span> show promising results in distinguishing between full-support and no-support scenarios. Nonetheless, they struggle to identify cases of partial support, highlighting the inherent complexities of automated citation evaluation; and
</span></span>
<span class="ltx_inline-item" id="S1.I2.i3"><span class="ltx_tag ltx_tag_inline-item">3)</span> <span class="ltx_text" id="S1.I2.i3.1">in terms of retrieval evaluation, similarity-based metrics, such as <span class="ltx_text ltx_font_smallcaps" id="S1.I2.i3.1.1">BERTScore</span>, consistently surpass best-performing entailment-based metrics. This indicates that entailment-based metrics exhibit higher sensitivity to noisy data, which is introduced by a considerable number of irrelevant documents in such scenarios.
</span></span>
</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our primary contributions can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I3">
<li class="ltx_item" id="S1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I3.i1.p1">
<p class="ltx_p" id="S1.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I3.i1.p1.1.1">Exploration of fine-grained levels of support in citation evaluation:</span> to the best of our knowledge, we are the first to systematically investigate the effect of three-category support levels on faithfulness metrics in the task of automated citation evaluation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I3.i2.p1">
<p class="ltx_p" id="S1.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I3.i2.p1.1.1">Introduction of a comparative evaluation framework:</span> we propose a comparative evaluation framework designed to assess the alignment between metric scores and human judgments. This framework includes correlation analysis, classification, and retrieval evaluation to comprehensively evaluate the metric performance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I3.i3.p1">
<p class="ltx_p" id="S1.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I3.i3.p1.1.1">Comprehensive experimental findings:</span> our experimental results demonstrate the best-performing faithfulness metrics still struggle to identify partially supporting citations, underscoring the inherent challenges of automated citation evaluation. Based on these findings, we offer practical recommendations for the development of more effective metrics.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="165" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">Our proposed comparative evaluation framework. A faithfulness metric assigns scores to given statements and their corresponding citations. Subsequently, our framework comprehensively assesses the alignment between these metric scores and human judgments by employing correlation analysis, classification, and retrieval evaluation.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Evaluation Framework</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we introduce the proposed comparative evaluation framework. We begin by formalizing the task of automated citation evaluation. Subsequently, we detail three distinct evaluation protocols within this framework, ensuring a comprehensive assessment in alignment between faithfulness metrics and human judgments. Our framework is demonstrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2408.12398v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Task Formulation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.5">The objective of automated citation evaluation is to automatically quantify the support level of a citation based on the citation and its associated statement.
In this work, we assume access to a dataset for automated citation evaluation, comprising pairs of statements and their corresponding citations, denoted as <math alttext="(s_{i},c_{i})" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.2"><semantics id="S2.SS1.p1.1.m1.2a"><mrow id="S2.SS1.p1.1.m1.2.2.2" xref="S2.SS1.p1.1.m1.2.2.3.cmml"><mo id="S2.SS1.p1.1.m1.2.2.2.3" stretchy="false" xref="S2.SS1.p1.1.m1.2.2.3.cmml">(</mo><msub id="S2.SS1.p1.1.m1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.1.1.2.cmml">s</mi><mi id="S2.SS1.p1.1.m1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS1.p1.1.m1.2.2.2.4" xref="S2.SS1.p1.1.m1.2.2.3.cmml">,</mo><msub id="S2.SS1.p1.1.m1.2.2.2.2" xref="S2.SS1.p1.1.m1.2.2.2.2.cmml"><mi id="S2.SS1.p1.1.m1.2.2.2.2.2" xref="S2.SS1.p1.1.m1.2.2.2.2.2.cmml">c</mi><mi id="S2.SS1.p1.1.m1.2.2.2.2.3" xref="S2.SS1.p1.1.m1.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.1.m1.2.2.2.5" stretchy="false" xref="S2.SS1.p1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.2b"><interval closure="open" id="S2.SS1.p1.1.m1.2.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.2"><apply id="S2.SS1.p1.1.m1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.2">𝑠</ci><ci id="S2.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.SS1.p1.1.m1.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.2.2.1.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2">𝑐</ci><ci id="S2.SS1.p1.1.m1.2.2.2.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.3">𝑖</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.2c">(s_{i},c_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.2d">( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>. Each <math alttext="s_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">s</mi><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">𝑠</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a statement from the set <math alttext="S" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_S</annotation></semantics></math> of all statements produced by an LLM and each <math alttext="c_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">c</mi><mi id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">𝑐</ci><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a citation from a set <math alttext="C" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">C</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">italic_C</annotation></semantics></math> of citations returned by the LLM.
According to human evaluation in the dataset, we categorize the citations into three distinct levels of support: full, partial, and no support. We adopt the definition of these levels of support from <cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib30" title="">2023</a>)</cite>:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Full Support (<span class="ltx_text ltx_font_smallcaps" id="S2.I1.i1.p1.1.1">FS</span>): The citation fully supports every detail in the statement.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Partial Support (<span class="ltx_text ltx_font_smallcaps" id="S2.I1.i2.p1.1.1">PS</span>): The citation supports certain aspects of the statement, while other details remain unsupported or are contradicted.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">No Support (<span class="ltx_text ltx_font_smallcaps" id="S2.I1.i3.p1.1.1">NS</span>): None of the content in the statement is supported by the citation. For instance, the citation is entirely irrelevant or contradicts the statement.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.SS1.p1.8">To this end, without loss of generality, we define a faithfulness metric as a scoring function, denoted as <math alttext="F(s_{i},c_{i})\rightarrow\mathbb{R^{+}}" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m1.2"><semantics id="S2.SS1.p1.6.m1.2a"><mrow id="S2.SS1.p1.6.m1.2.2" xref="S2.SS1.p1.6.m1.2.2.cmml"><mrow id="S2.SS1.p1.6.m1.2.2.2" xref="S2.SS1.p1.6.m1.2.2.2.cmml"><mi id="S2.SS1.p1.6.m1.2.2.2.4" xref="S2.SS1.p1.6.m1.2.2.2.4.cmml">F</mi><mo id="S2.SS1.p1.6.m1.2.2.2.3" xref="S2.SS1.p1.6.m1.2.2.2.3.cmml">⁢</mo><mrow id="S2.SS1.p1.6.m1.2.2.2.2.2" xref="S2.SS1.p1.6.m1.2.2.2.2.3.cmml"><mo id="S2.SS1.p1.6.m1.2.2.2.2.2.3" stretchy="false" xref="S2.SS1.p1.6.m1.2.2.2.2.3.cmml">(</mo><msub id="S2.SS1.p1.6.m1.1.1.1.1.1.1" xref="S2.SS1.p1.6.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.6.m1.1.1.1.1.1.1.2" xref="S2.SS1.p1.6.m1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S2.SS1.p1.6.m1.1.1.1.1.1.1.3" xref="S2.SS1.p1.6.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS1.p1.6.m1.2.2.2.2.2.4" xref="S2.SS1.p1.6.m1.2.2.2.2.3.cmml">,</mo><msub id="S2.SS1.p1.6.m1.2.2.2.2.2.2" xref="S2.SS1.p1.6.m1.2.2.2.2.2.2.cmml"><mi id="S2.SS1.p1.6.m1.2.2.2.2.2.2.2" xref="S2.SS1.p1.6.m1.2.2.2.2.2.2.2.cmml">c</mi><mi id="S2.SS1.p1.6.m1.2.2.2.2.2.2.3" xref="S2.SS1.p1.6.m1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.6.m1.2.2.2.2.2.5" stretchy="false" xref="S2.SS1.p1.6.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p1.6.m1.2.2.3" stretchy="false" xref="S2.SS1.p1.6.m1.2.2.3.cmml">→</mo><msup id="S2.SS1.p1.6.m1.2.2.4" xref="S2.SS1.p1.6.m1.2.2.4.cmml"><mi id="S2.SS1.p1.6.m1.2.2.4.2" xref="S2.SS1.p1.6.m1.2.2.4.2.cmml">ℝ</mi><mo id="S2.SS1.p1.6.m1.2.2.4.3" xref="S2.SS1.p1.6.m1.2.2.4.3.cmml">+</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m1.2b"><apply id="S2.SS1.p1.6.m1.2.2.cmml" xref="S2.SS1.p1.6.m1.2.2"><ci id="S2.SS1.p1.6.m1.2.2.3.cmml" xref="S2.SS1.p1.6.m1.2.2.3">→</ci><apply id="S2.SS1.p1.6.m1.2.2.2.cmml" xref="S2.SS1.p1.6.m1.2.2.2"><times id="S2.SS1.p1.6.m1.2.2.2.3.cmml" xref="S2.SS1.p1.6.m1.2.2.2.3"></times><ci id="S2.SS1.p1.6.m1.2.2.2.4.cmml" xref="S2.SS1.p1.6.m1.2.2.2.4">𝐹</ci><interval closure="open" id="S2.SS1.p1.6.m1.2.2.2.2.3.cmml" xref="S2.SS1.p1.6.m1.2.2.2.2.2"><apply id="S2.SS1.p1.6.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.6.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.6.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.6.m1.1.1.1.1.1.1.2">𝑠</ci><ci id="S2.SS1.p1.6.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.6.m1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.SS1.p1.6.m1.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.6.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m1.2.2.2.2.2.2.1.cmml" xref="S2.SS1.p1.6.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.6.m1.2.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.6.m1.2.2.2.2.2.2.2">𝑐</ci><ci id="S2.SS1.p1.6.m1.2.2.2.2.2.2.3.cmml" xref="S2.SS1.p1.6.m1.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply><apply id="S2.SS1.p1.6.m1.2.2.4.cmml" xref="S2.SS1.p1.6.m1.2.2.4"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m1.2.2.4.1.cmml" xref="S2.SS1.p1.6.m1.2.2.4">superscript</csymbol><ci id="S2.SS1.p1.6.m1.2.2.4.2.cmml" xref="S2.SS1.p1.6.m1.2.2.4.2">ℝ</ci><plus id="S2.SS1.p1.6.m1.2.2.4.3.cmml" xref="S2.SS1.p1.6.m1.2.2.4.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m1.2c">F(s_{i},c_{i})\rightarrow\mathbb{R^{+}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m1.2d">italic_F ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) → blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math>.
For any given statement <math alttext="s_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m2.1"><semantics id="S2.SS1.p1.7.m2.1a"><msub id="S2.SS1.p1.7.m2.1.1" xref="S2.SS1.p1.7.m2.1.1.cmml"><mi id="S2.SS1.p1.7.m2.1.1.2" xref="S2.SS1.p1.7.m2.1.1.2.cmml">s</mi><mi id="S2.SS1.p1.7.m2.1.1.3" xref="S2.SS1.p1.7.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m2.1b"><apply id="S2.SS1.p1.7.m2.1.1.cmml" xref="S2.SS1.p1.7.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m2.1.1.1.cmml" xref="S2.SS1.p1.7.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.7.m2.1.1.2.cmml" xref="S2.SS1.p1.7.m2.1.1.2">𝑠</ci><ci id="S2.SS1.p1.7.m2.1.1.3.cmml" xref="S2.SS1.p1.7.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m2.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m2.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and its associated citation <math alttext="c_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m3.1"><semantics id="S2.SS1.p1.8.m3.1a"><msub id="S2.SS1.p1.8.m3.1.1" xref="S2.SS1.p1.8.m3.1.1.cmml"><mi id="S2.SS1.p1.8.m3.1.1.2" xref="S2.SS1.p1.8.m3.1.1.2.cmml">c</mi><mi id="S2.SS1.p1.8.m3.1.1.3" xref="S2.SS1.p1.8.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m3.1b"><apply id="S2.SS1.p1.8.m3.1.1.cmml" xref="S2.SS1.p1.8.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m3.1.1.1.cmml" xref="S2.SS1.p1.8.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m3.1.1.2.cmml" xref="S2.SS1.p1.8.m3.1.1.2">𝑐</ci><ci id="S2.SS1.p1.8.m3.1.1.3.cmml" xref="S2.SS1.p1.8.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m3.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m3.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, this scoring function provides a numeric score that indicates the extent of support provided by the citation to the statement.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Evaluation Protocols</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The objective of evaluation protocols is to comprehensively assess the extent to which metric scores align with human judgments. In this work, we assess this alignment across three distinct dimensions: <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">correlation analysis</span>, <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.2">classification performance</span>, and <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.3">retrieval effectiveness</span>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Correlation Analysis</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.4">The correlation analysis aims to measure the general trend in the relationship between metric scores and human judgments.
Previous research <cite class="ltx_cite ltx_citemacro_citep">(Kryscinski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib24" title="">2020</a>; Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib34" title="">2021</a>)</cite> has employed correlation analysis to meta-evaluate faithfulness metrics in abstractive text summarization.
They involve measuring the extent to which metric scores align with binary levels of faithfulness, which are annotated by human assessors as either faithful (<math alttext="1" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.1.m1.1"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mn id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><cn id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S2.SS2.SSS1.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.1.m1.1d">1</annotation></semantics></math>) or unfaithful (<math alttext="0" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.2.m2.1"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mn id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><cn id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" type="integer" xref="S2.SS2.SSS1.p1.2.m2.1.1">0</cn></annotation-xml></semantics></math>).
Following them, we adapt correlation analysis to the task of automated citation evaluation.
Specifically, Given statements and their associated citations, we assess how well predicted metric scores correlate with human-annotated support levels.
To facilitate correlation analysis, we arbitrarily assign support levels <math alttext="\{\textsc{FS},\textsc{PS},\textsc{NS}\}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.3.m3.3"><semantics id="S2.SS2.SSS1.p1.3.m3.3a"><mrow id="S2.SS2.SSS1.p1.3.m3.3.4.2" xref="S2.SS2.SSS1.p1.3.m3.3.4.1.cmml"><mo id="S2.SS2.SSS1.p1.3.m3.3.4.2.1" stretchy="false" xref="S2.SS2.SSS1.p1.3.m3.3.4.1.cmml">{</mo><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS1.p1.3.m3.1.1" xref="S2.SS2.SSS1.p1.3.m3.1.1a.cmml">FS</mtext><mo id="S2.SS2.SSS1.p1.3.m3.3.4.2.2" xref="S2.SS2.SSS1.p1.3.m3.3.4.1.cmml">,</mo><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS1.p1.3.m3.2.2" xref="S2.SS2.SSS1.p1.3.m3.2.2a.cmml">PS</mtext><mo id="S2.SS2.SSS1.p1.3.m3.3.4.2.3" xref="S2.SS2.SSS1.p1.3.m3.3.4.1.cmml">,</mo><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS1.p1.3.m3.3.3" xref="S2.SS2.SSS1.p1.3.m3.3.3a.cmml">NS</mtext><mo id="S2.SS2.SSS1.p1.3.m3.3.4.2.4" stretchy="false" xref="S2.SS2.SSS1.p1.3.m3.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m3.3b"><set id="S2.SS2.SSS1.p1.3.m3.3.4.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.3.4.2"><ci id="S2.SS2.SSS1.p1.3.m3.1.1a.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1"><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1">FS</mtext></ci><ci id="S2.SS2.SSS1.p1.3.m3.2.2a.cmml" xref="S2.SS2.SSS1.p1.3.m3.2.2"><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS1.p1.3.m3.2.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.2.2">PS</mtext></ci><ci id="S2.SS2.SSS1.p1.3.m3.3.3a.cmml" xref="S2.SS2.SSS1.p1.3.m3.3.3"><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS1.p1.3.m3.3.3.cmml" xref="S2.SS2.SSS1.p1.3.m3.3.3">NS</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.3c">\{\textsc{FS},\textsc{PS},\textsc{NS}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.3.m3.3d">{ FS , PS , NS }</annotation></semantics></math> to values <math alttext="\{0,1,2\}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.4.m4.3"><semantics id="S2.SS2.SSS1.p1.4.m4.3a"><mrow id="S2.SS2.SSS1.p1.4.m4.3.4.2" xref="S2.SS2.SSS1.p1.4.m4.3.4.1.cmml"><mo id="S2.SS2.SSS1.p1.4.m4.3.4.2.1" stretchy="false" xref="S2.SS2.SSS1.p1.4.m4.3.4.1.cmml">{</mo><mn id="S2.SS2.SSS1.p1.4.m4.1.1" xref="S2.SS2.SSS1.p1.4.m4.1.1.cmml">0</mn><mo id="S2.SS2.SSS1.p1.4.m4.3.4.2.2" xref="S2.SS2.SSS1.p1.4.m4.3.4.1.cmml">,</mo><mn id="S2.SS2.SSS1.p1.4.m4.2.2" xref="S2.SS2.SSS1.p1.4.m4.2.2.cmml">1</mn><mo id="S2.SS2.SSS1.p1.4.m4.3.4.2.3" xref="S2.SS2.SSS1.p1.4.m4.3.4.1.cmml">,</mo><mn id="S2.SS2.SSS1.p1.4.m4.3.3" xref="S2.SS2.SSS1.p1.4.m4.3.3.cmml">2</mn><mo id="S2.SS2.SSS1.p1.4.m4.3.4.2.4" stretchy="false" xref="S2.SS2.SSS1.p1.4.m4.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.4.m4.3b"><set id="S2.SS2.SSS1.p1.4.m4.3.4.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.3.4.2"><cn id="S2.SS2.SSS1.p1.4.m4.1.1.cmml" type="integer" xref="S2.SS2.SSS1.p1.4.m4.1.1">0</cn><cn id="S2.SS2.SSS1.p1.4.m4.2.2.cmml" type="integer" xref="S2.SS2.SSS1.p1.4.m4.2.2">1</cn><cn id="S2.SS2.SSS1.p1.4.m4.3.3.cmml" type="integer" xref="S2.SS2.SSS1.p1.4.m4.3.3">2</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.4.m4.3c">\{0,1,2\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.4.m4.3d">{ 0 , 1 , 2 }</annotation></semantics></math>. We then utilize standard correlation metrics such as the Pearson correlation coefficient to assess metric effectiveness.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Classification Evaluation</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">In addition to correlation analysis, we perform classification evaluation to determine the effectiveness of faithfulness metrics in discriminating citations based on their support level.
Specifically, the metrics need to categorize a citation into one of three support levels: <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.1.1">FS</span>, <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.1.2">PS</span>, <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS2.p1.1.3">NS</span>.
Notably, existing faithfulness metrics do not apply to this three-way classification scenario, as they are unable to accurately determine the extent to which a statement is partially supported by its corresponding citation <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib11" title="">2022</a>)</cite>.
To address this issue, we adopt a one-vs-one strategy, by effectively decomposing the three-way classification into three binary classification task settings:

<span class="ltx_inline-enumerate" id="S2.I2">
<span class="ltx_inline-item" id="S2.I2.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="S2.I2.i1.1">Full Support vs. No Support (<span class="ltx_text ltx_font_smallcaps" id="S2.I2.i1.1.1">FS-vs-NS</span>),
</span></span>
<span class="ltx_inline-item" id="S2.I2.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="S2.I2.i2.1">Full Support vs. Partial Support (<span class="ltx_text ltx_font_smallcaps" id="S2.I2.i2.1.1">FS-vs-PS</span>), and
</span></span>
<span class="ltx_inline-item" id="S2.I2.i3"><span class="ltx_tag ltx_tag_inline-item">(iii)</span> <span class="ltx_text" id="S2.I2.i3.1">Partial Support vs. No Support (<span class="ltx_text ltx_font_smallcaps" id="S2.I2.i3.1.1">PS-vs-NS</span>).
</span></span>
</span>
For each binary classification task setting, we construct a specialized dataset comprising only instances with corresponding binary support levels derived from the original dataset.
We assess the performance of metrics on these tailored binary datasets using standard binary classification evaluation metrics such as ROC-AUC.
The overall metric performance is then computed by averaging the results across all binary tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3. </span>Retrieval Evaluation</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.2">The objective of retrieval evaluation is to measure the effectiveness of metrics in ranking citations according to their support levels.
This evaluation is motivated by the observation that previous correlation and classification evaluations presuppose the presence of citations within generated statements.
However, real-world scenarios frequently present instances where citations are absent or irrelevant, highlighting the need for post-hoc retrieval to enhance citation quality <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib30" title="">2023</a>; Huang and Chang, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib13" title="">2023</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib12" title="">2024b</a>)</cite>.
In post-hoc retrieval, candidate documents are retrieved to form a pool of potential citations using information retrieval techniques <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib23" title="">2020</a>)</cite>.
Faithfulness metrics are then employed to rank citations based on their predicted metric scores, aiming to identify the citation with the highest support level <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib7" title="">2023a</a>; Bohnet et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib2" title="">2022</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib8" title="">2023b</a>)</cite>.
Ideally, a faithfulness metric should rank fully supporting citations at the top, followed by partially supporting citations, and finally non-supporting citations.
Similar to correlation analysis, we arbitrarily assign support levels <math alttext="\{\textsc{FS},\textsc{PS},\textsc{NS}\}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.1.m1.3"><semantics id="S2.SS2.SSS3.p1.1.m1.3a"><mrow id="S2.SS2.SSS3.p1.1.m1.3.4.2" xref="S2.SS2.SSS3.p1.1.m1.3.4.1.cmml"><mo id="S2.SS2.SSS3.p1.1.m1.3.4.2.1" stretchy="false" xref="S2.SS2.SSS3.p1.1.m1.3.4.1.cmml">{</mo><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS3.p1.1.m1.1.1" xref="S2.SS2.SSS3.p1.1.m1.1.1a.cmml">FS</mtext><mo id="S2.SS2.SSS3.p1.1.m1.3.4.2.2" xref="S2.SS2.SSS3.p1.1.m1.3.4.1.cmml">,</mo><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS3.p1.1.m1.2.2" xref="S2.SS2.SSS3.p1.1.m1.2.2a.cmml">PS</mtext><mo id="S2.SS2.SSS3.p1.1.m1.3.4.2.3" xref="S2.SS2.SSS3.p1.1.m1.3.4.1.cmml">,</mo><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS3.p1.1.m1.3.3" xref="S2.SS2.SSS3.p1.1.m1.3.3a.cmml">NS</mtext><mo id="S2.SS2.SSS3.p1.1.m1.3.4.2.4" stretchy="false" xref="S2.SS2.SSS3.p1.1.m1.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.1.m1.3b"><set id="S2.SS2.SSS3.p1.1.m1.3.4.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.3.4.2"><ci id="S2.SS2.SSS3.p1.1.m1.1.1a.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1"><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1">FS</mtext></ci><ci id="S2.SS2.SSS3.p1.1.m1.2.2a.cmml" xref="S2.SS2.SSS3.p1.1.m1.2.2"><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS3.p1.1.m1.2.2.cmml" xref="S2.SS2.SSS3.p1.1.m1.2.2">PS</mtext></ci><ci id="S2.SS2.SSS3.p1.1.m1.3.3a.cmml" xref="S2.SS2.SSS3.p1.1.m1.3.3"><mtext class="ltx_font_smallcaps" id="S2.SS2.SSS3.p1.1.m1.3.3.cmml" xref="S2.SS2.SSS3.p1.1.m1.3.3">NS</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.1.m1.3c">\{\textsc{FS},\textsc{PS},\textsc{NS}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.1.m1.3d">{ FS , PS , NS }</annotation></semantics></math> to relevance labels <math alttext="\{2,1,0\}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.2.m2.3"><semantics id="S2.SS2.SSS3.p1.2.m2.3a"><mrow id="S2.SS2.SSS3.p1.2.m2.3.4.2" xref="S2.SS2.SSS3.p1.2.m2.3.4.1.cmml"><mo id="S2.SS2.SSS3.p1.2.m2.3.4.2.1" stretchy="false" xref="S2.SS2.SSS3.p1.2.m2.3.4.1.cmml">{</mo><mn id="S2.SS2.SSS3.p1.2.m2.1.1" xref="S2.SS2.SSS3.p1.2.m2.1.1.cmml">2</mn><mo id="S2.SS2.SSS3.p1.2.m2.3.4.2.2" xref="S2.SS2.SSS3.p1.2.m2.3.4.1.cmml">,</mo><mn id="S2.SS2.SSS3.p1.2.m2.2.2" xref="S2.SS2.SSS3.p1.2.m2.2.2.cmml">1</mn><mo id="S2.SS2.SSS3.p1.2.m2.3.4.2.3" xref="S2.SS2.SSS3.p1.2.m2.3.4.1.cmml">,</mo><mn id="S2.SS2.SSS3.p1.2.m2.3.3" xref="S2.SS2.SSS3.p1.2.m2.3.3.cmml">0</mn><mo id="S2.SS2.SSS3.p1.2.m2.3.4.2.4" stretchy="false" xref="S2.SS2.SSS3.p1.2.m2.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.2.m2.3b"><set id="S2.SS2.SSS3.p1.2.m2.3.4.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.3.4.2"><cn id="S2.SS2.SSS3.p1.2.m2.1.1.cmml" type="integer" xref="S2.SS2.SSS3.p1.2.m2.1.1">2</cn><cn id="S2.SS2.SSS3.p1.2.m2.2.2.cmml" type="integer" xref="S2.SS2.SSS3.p1.2.m2.2.2">1</cn><cn id="S2.SS2.SSS3.p1.2.m2.3.3.cmml" type="integer" xref="S2.SS2.SSS3.p1.2.m2.3.3">0</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.2.m2.3c">\{2,1,0\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.2.m2.3d">{ 2 , 1 , 0 }</annotation></semantics></math>.
The metric effectiveness is assessed using standard information retrieval evaluation metrics, such as nDCG. This evaluation also provides a deeper understanding of metric performance in post-hoc citation retrieval scenarios.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Faithfulness Metrics</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In our experiments, we evaluate seven widely used faithfulness evaluation metrics, dividing them into similarity-based and entailment-based categories. Similarity-based metrics assess the level of support of a citation by measuring the degree of similarity between the citation and the associated statement. In contrast, entailment-based metrics leverage natural language inference (NLI) models <cite class="ltx_cite ltx_citemacro_citep">(Williams et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib41" title="">2018</a>)</cite> to estimate the support level based on the likelihood that the citation entails the statement.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Similarity-Based Metrics</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.SS1.p1.1.1">BERTScore</span>
<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib46" title="">2020</a>)</cite> adopts BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib3" title="">2019</a>)</cite> to measure semantic similarity between a pair of text by aggregating cosine similarity among token-level BERT representation without further fine-tuning.
We report the precision version of <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.2">BERTScore</span> since it correlates more with human judgments in faithfulness evaluation <cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib34" title="">2021</a>)</cite>, We use recommended <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.3">deberta-xlarge-mnli</span> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib9" title="">2021</a>)</cite> as the backbone of <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.4">BERTScore</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.SS1.p2.1.1">BARTScore</span>
<cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib43" title="">2021</a>)</cite> adopts BART <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib26" title="">2020</a>)</cite> to measure the similarity between two texts based on conditional log-likelihood of generating target text from source text.
In our experiments, we leverage the faithfulness version of <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p2.1.2">BARTScore</span>, in which we treat the citation and the statement as the source and target text, respectively. We use the BART model fine-tuned on the CNN/DailyMail dataset <cite class="ltx_cite ltx_citemacro_citep">(Hermann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib10" title="">2015</a>)</cite> as the backbone of <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p2.1.3">BARTScore</span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Entailment-Based Metrics</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.SS2.p1.1.1">FactCC</span>
<cite class="ltx_cite ltx_citemacro_citep">(Kryscinski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib24" title="">2020</a>)</cite> is a BERT-based model to verify whether a generated text is faithful to a source text, which is fine-tuned on synthetic training data which contains simulated examples with different factual errors <cite class="ltx_cite ltx_citemacro_citep">(Kryscinski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib24" title="">2020</a>)</cite>. This metric is also widely used for faithfulness evaluation in abstractive text summarization.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.SS2.p2.1.1">SummaC</span>
 <cite class="ltx_cite ltx_citemacro_citep">(Laban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib25" title="">2022</a>)</cite> is a RoBERTa-based model <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib31" title="">2019</a>)</cite> that is fine-tuned on NLI datasets. In this metric, a source text and its generated text are split into sentences. Entailment scores for all source/generated sentence pairs are then computed. Finally, the metric aggregates the scores of all pairs to obtain the final faithfulness score.
The metric has two variants:

<span class="ltx_inline-enumerate" id="S3.I1">
<span class="ltx_inline-item" id="S3.I1.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text ltx_font_smallcaps" id="S3.I1.i1.1">SummaC<sub class="ltx_sub" id="S3.I1.i1.1.1"><span class="ltx_text ltx_font_upright" id="S3.I1.i1.1.1.1">ZS</span></sub></span><span class="ltx_text" id="S3.I1.i1.2">is a zero-shot version that is only pre-trained on NLI datasets; and
</span></span>
<span class="ltx_inline-item" id="S3.I1.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text ltx_font_smallcaps" id="S3.I1.i2.1">SummaC<sub class="ltx_sub" id="S3.I1.i2.1.1"><span class="ltx_text ltx_font_upright" id="S3.I1.i2.1.1.1">Conv</span></sub></span><span class="ltx_text" id="S3.I1.i2.2">adds extra convolutional layers and is further fine-tuned on synthetic training data proposed in <cite class="ltx_cite ltx_citemacro_citet">Kryscinski et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib24" title="">2020</a>)</cite>. We include both variants in our experiments.
</span></span>
</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.SS2.p3.1.1">AutoAIS</span> <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib11" title="">2022</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib7" title="">2023a</a>)</cite>
is a T5-11B <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib35" title="">2020</a>)</cite> model trained on a collection of NLI datasets, which is commonly used in recent automated citation evaluation <cite class="ltx_cite ltx_citemacro_citep">(Bohnet et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib2" title="">2022</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib7" title="">2023a</a>)</cite>. As the original output of <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p3.1.2">AutoAIS</span> is a numeric, either “1” (faithful) or “0” (unfaithful), we use the generated token probability of “1” as the predicted metric score.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.SS2.p4.1.1">AlignScore</span> <cite class="ltx_cite ltx_citemacro_citep">(Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib45" title="">2023</a>)</cite> further fine-tunes a RoBERTa-based model <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib31" title="">2019</a>)</cite> with a unified alignment loss function. To this end, a unified dataset containing a variety of related natural language processing datasets, such as NLI, question answering, and fact verification datasets, have been collected. In this work, we adapt the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.1.2">large</span> version as it demonstrates the best performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we provide a description of the dataset statistics and the data processing method.
Subsequently, we discuss the evaluation metrics incorporated within our proposed framework, which assess the performance of faithfulness metrics in alignment with human judgments.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Datasets</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">Data statistics of the VeJudge dataset. The dataset comprises 12,681 statement-citation pairs. Each pair has been annotated by human assessors based on three categories: full, partial, and no support.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.4">
<tr class="ltx_tr" id="S4.T1.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.4.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1" style="font-size:90%;">Human Judgment</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T1.4.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.2.1" style="font-size:90%;"># Statement-Citation Pair</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.4.2.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S4.T1.4.2.1.1" style="font-size:90%;">Full Support</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.4.2.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S4.T1.4.2.2.1" style="font-size:90%;">6,616</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T1.4.3.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S4.T1.4.3.1.1" style="font-size:90%;">Partial Support</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.3.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S4.T1.4.3.2.1" style="font-size:90%;">1,445</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<td class="ltx_td ltx_align_left" id="S4.T1.4.4.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S4.T1.4.4.1.1" style="font-size:90%;">No Support</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.4.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S4.T1.4.4.2.1" style="font-size:90%;">4,620</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.4.5.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S4.T1.4.5.1.1" style="font-size:90%;">Total</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.4.5.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S4.T1.4.5.2.1" style="font-size:90%;">12,681</span></td>
</tr>
</table>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Data Statistics</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">In the experiments, we employ the dataset of verifiability judgments <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib30" title="">2023</a>)</cite> as our evaluation benchmark, referred to as VeJudge.
This dataset comprises a total of <math alttext="12,681" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.2"><semantics id="S4.SS1.SSS1.p1.1.m1.2a"><mrow id="S4.SS1.SSS1.p1.1.m1.2.3.2" xref="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml"><mn id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">12</mn><mo id="S4.SS1.SSS1.p1.1.m1.2.3.2.1" xref="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS1.p1.1.m1.2.2" xref="S4.SS1.SSS1.p1.1.m1.2.2.cmml">681</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.2b"><list id="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.2.3.2"><cn id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.1.m1.1.1">12</cn><cn id="S4.SS1.SSS1.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.1.m1.2.2">681</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.2c">12,681</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.2d">12 , 681</annotation></semantics></math> statement-citation pairs. For each pair, human assessors categorize the citation into one of three categories of support levels: full, partial, or no support.
These categories indicate whether a citation provides full, partial, or no support to the associated statement.
The data statistics are illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2408.12398v1#S4.T1" title="Table 1 ‣ 4.1. Datasets ‣ 4. Experiments ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
Notably, for citations classified under the full or partial support categories, human assessors additionally extract explicit evidence from the citation that substantiates the associated statement.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Data Processing</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.2">While the VeJudge dataset aligns well with our research objectives, we encounter a significant challenge: the extensive length of most citations within the dataset.
These citations often comprise a web document with thousands of words, far exceeding the maximum input capacity of most faithfulness metrics, which is limited to <math alttext="512" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mn id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><cn id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p1.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">512</annotation></semantics></math> words.
This limitation necessitates input truncation, potentially compromising the reliability of faithfulness metrics.
To mitigate this issue, we adopt a strategy similar to previous studies <cite class="ltx_cite ltx_citemacro_citep">(Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib45" title="">2023</a>)</cite>.
Specifically, we segment each cited document into shorter text chunks, with a maximum length of <math alttext="150" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mn id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><cn id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p1.2.m2.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">150</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.2.m2.1d">150</annotation></semantics></math> words per chunk. These text chunks, along with their corresponding statements, serve as the inputs for faithfulness metrics to predicted metric scores.
Furthermore, to construct human judgments on the text chunks, we employ the Jaccard similarity index to identify text chunks containing human-annotated evidence, classifying them as either fully or partially supporting text chunks.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Meta-Evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For correlation evaluation, we report partial Pearson, Spearman, and Kendall coefficients, as recommended by previous research <cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib34" title="">2021</a>)</cite>.
In terms of classification evaluation, following previous studies <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib11" title="">2022</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib32" title="">2023</a>)</cite>, we report the Receiver Operating Characteristic-Area Under Curve (ROC-AUC) score as it obviates the need for manual threshold setting for each binary classification task.
Moreover, to capture the comprehensive performance across all binary classification tasks, we compute and report the macro-averaged ROC-AUC score.
For retrieval evaluation, we report standard information retrieval metrics: mean reciprocal rank (MRR) and normalized discounted cumulative gain (nDCG@n) scores.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results &amp; Analyses</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we discuss the results of the performance of faithfulness metrics across three distinct evaluation protocols. Following this, we integrate the observations derived from these evaluation protocols to discuss our main implications, offering practical recommendations to enhance metric effectiveness in automated citation evaluation.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.2.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S5.T2.3.2" style="font-size:90%;">Partial correlation coefficients between human-annotated levels of support and faithfulness metric scores on the VeJudge dataset. The best correlations are marked in bold.</span></figcaption>
<p class="ltx_p ltx_align_center" id="S5.T2.4"><span class="ltx_text" id="S5.T2.4.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T2.4.1.1" style="width:226.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T2.4.1.1.1"><span class="ltx_text" id="S5.T2.4.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T2.4.1.1.1.1.1">
<span class="ltx_tr" id="S5.T2.4.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.4.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.T2.4.1.1.1.1.1.1.1.1">Metric</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.4.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1.1.1.2.1">Pearson</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.4.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1.1.1.3.1">Spearman</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T2.4.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1.1.1.4.1">Kendall</span></span></span>
<span class="ltx_tr" id="S5.T2.4.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.1.1.1.1.1.2.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.4.1.1.1.1.1.2.1.1">FactCC</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.1.1.1.1.2.2">0.108</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.1.1.1.1.2.3">-0.018</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.4.1.1.1.1.1.2.4">-0.008</span></span>
<span class="ltx_tr" id="S5.T2.4.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left" id="S5.T2.4.1.1.1.1.1.3.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.4.1.1.1.1.1.3.1.1">SummaC<sub class="ltx_sub" id="S5.T2.4.1.1.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_upright" id="S5.T2.4.1.1.1.1.1.3.1.1.1.1">ZS</span></sub></span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.3.2">0.326</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.3.3">0.143</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.4.1.1.1.1.1.3.4">0.106</span></span>
<span class="ltx_tr" id="S5.T2.4.1.1.1.1.1.4">
<span class="ltx_td ltx_align_left" id="S5.T2.4.1.1.1.1.1.4.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.4.1.1.1.1.1.4.1.1">BERTScore</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.4.2">0.512</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.4.3">0.218</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.4.1.1.1.1.1.4.4">0.165</span></span>
<span class="ltx_tr" id="S5.T2.4.1.1.1.1.1.5">
<span class="ltx_td ltx_align_left" id="S5.T2.4.1.1.1.1.1.5.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.4.1.1.1.1.1.5.1.1">AlignScore</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.5.2">0.551</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.5.3">0.275</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.4.1.1.1.1.1.5.4">0.199</span></span>
<span class="ltx_tr" id="S5.T2.4.1.1.1.1.1.6">
<span class="ltx_td ltx_align_left" id="S5.T2.4.1.1.1.1.1.6.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.4.1.1.1.1.1.6.1.1">BARTScore</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.6.2">0.593</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.6.3">0.279</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.4.1.1.1.1.1.6.4">0.211</span></span>
<span class="ltx_tr" id="S5.T2.4.1.1.1.1.1.7">
<span class="ltx_td ltx_align_left" id="S5.T2.4.1.1.1.1.1.7.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.4.1.1.1.1.1.7.1.1">AutoAIS</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.7.2"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1.1.7.2.1">0.604</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.1.1.1.1.1.7.3">0.407</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.4.1.1.1.1.1.7.4">0.297</span></span>
<span class="ltx_tr" id="S5.T2.4.1.1.1.1.1.8">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.4.1.1.1.1.1.8.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.4.1.1.1.1.1.8.1.1">SummaC<sub class="ltx_sub" id="S5.T2.4.1.1.1.1.1.8.1.1.1"><span class="ltx_text ltx_font_upright" id="S5.T2.4.1.1.1.1.1.8.1.1.1.1">Conv</span></sub></span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.1.1.1.1.1.8.2">0.565</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.1.1.1.1.1.8.3"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1.1.8.3.1">0.444</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T2.4.1.1.1.1.1.8.4"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1.1.8.4.1">0.342</span></span></span>
</span></span></span>
</span></span></span></p>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Correlation Results</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The correlation analysis results are demonstrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2408.12398v1#S5.T2" title="Table 2 ‣ 5. Results &amp; Analyses ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>. The following observations can be made:

<span class="ltx_inline-enumerate" id="S5.I1">
<span class="ltx_inline-item" id="S5.I1.i1"><span class="ltx_tag ltx_tag_inline-item">1)</span> <span class="ltx_text" id="S5.I1.i1.2">the best-performing metrics reveal moderate correlations when analyzed using the Pearson coefficient. Specifically, <span class="ltx_text ltx_font_smallcaps" id="S5.I1.i1.2.1">AutoAIS</span> achieves the highest Pearson coefficient, recording a value of <math alttext="0.604" class="ltx_Math" display="inline" id="S5.I1.i1.1.m1.1"><semantics id="S5.I1.i1.1.m1.1a"><mn id="S5.I1.i1.1.m1.1.1" xref="S5.I1.i1.1.m1.1.1.cmml">0.604</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i1.1.m1.1b"><cn id="S5.I1.i1.1.m1.1.1.cmml" type="float" xref="S5.I1.i1.1.m1.1.1">0.604</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.1.m1.1c">0.604</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.1.m1.1d">0.604</annotation></semantics></math>, marginally surpassing the second-best <span class="ltx_text ltx_font_smallcaps" id="S5.I1.i1.2.2">BARTScore</span>, which posts a coefficient of <math alttext="0.593" class="ltx_Math" display="inline" id="S5.I1.i1.2.m2.1"><semantics id="S5.I1.i1.2.m2.1a"><mn id="S5.I1.i1.2.m2.1.1" xref="S5.I1.i1.2.m2.1.1.cmml">0.593</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i1.2.m2.1b"><cn id="S5.I1.i1.2.m2.1.1.cmml" type="float" xref="S5.I1.i1.2.m2.1.1">0.593</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.2.m2.1c">0.593</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.2.m2.1d">0.593</annotation></semantics></math>;
</span></span>
<span class="ltx_inline-item" id="S5.I1.i2"><span class="ltx_tag ltx_tag_inline-item">2)</span> <span class="ltx_text" id="S5.I1.i2.1">there is a noticeable variation in correlation trends among high-performing metrics. Notably, <span class="ltx_text ltx_font_smallcaps" id="S5.I1.i2.1.1">AutoAIS</span> shows a more substantial Pearson correlation, whereas <span class="ltx_text ltx_font_smallcaps" id="S5.I1.i2.1.2">SummaC<sub class="ltx_sub" id="S5.I1.i2.1.2.1"><span class="ltx_text ltx_font_upright" id="S5.I1.i2.1.2.1.1">Conv</span></sub></span> outperforms in Spearman and Kendall correlations. This divergence might be attributed to the Pearson coefficient assuming linear relationships between two variables. Such an assumption is often invalid in automated citation evaluation, rendering the Pearson coefficient less suitable for capturing the true relationships between metric scores and human judgments; and
</span></span>
<span class="ltx_inline-item" id="S5.I1.i3"><span class="ltx_tag ltx_tag_inline-item">3)</span> <span class="ltx_text" id="S5.I1.i3.3">Generally, most metrics display relatively low Spearman and Kendall correlations compared to their Pearson correlations. For instance, <span class="ltx_text ltx_font_smallcaps" id="S5.I1.i3.3.1">SummaC<sub class="ltx_sub" id="S5.I1.i3.3.1.1"><span class="ltx_text ltx_font_upright" id="S5.I1.i3.3.1.1.1">Conv</span></sub></span> achieves the highest Spearman and Kendall correlations, with values of <math alttext="0.445" class="ltx_Math" display="inline" id="S5.I1.i3.1.m1.1"><semantics id="S5.I1.i3.1.m1.1a"><mn id="S5.I1.i3.1.m1.1.1" xref="S5.I1.i3.1.m1.1.1.cmml">0.445</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i3.1.m1.1b"><cn id="S5.I1.i3.1.m1.1.1.cmml" type="float" xref="S5.I1.i3.1.m1.1.1">0.445</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.1.m1.1c">0.445</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.1.m1.1d">0.445</annotation></semantics></math> and <math alttext="0.297" class="ltx_Math" display="inline" id="S5.I1.i3.2.m2.1"><semantics id="S5.I1.i3.2.m2.1a"><mn id="S5.I1.i3.2.m2.1.1" xref="S5.I1.i3.2.m2.1.1.cmml">0.297</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i3.2.m2.1b"><cn id="S5.I1.i3.2.m2.1.1.cmml" type="float" xref="S5.I1.i3.2.m2.1.1">0.297</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.2.m2.1c">0.297</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.2.m2.1d">0.297</annotation></semantics></math> respectively, which are considerably lower than its Pearson correlation of <math alttext="0.565" class="ltx_Math" display="inline" id="S5.I1.i3.3.m3.1"><semantics id="S5.I1.i3.3.m3.1a"><mn id="S5.I1.i3.3.m3.1.1" xref="S5.I1.i3.3.m3.1.1.cmml">0.565</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i3.3.m3.1b"><cn id="S5.I1.i3.3.m3.1.1.cmml" type="float" xref="S5.I1.i3.3.m3.1.1">0.565</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.3.m3.1c">0.565</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.3.m3.1d">0.565</annotation></semantics></math>. This disparity indicates that the metric scores of the best-performing metrics do not correlate well with human judgments, highlighting the limitations of existing metrics in scenarios involving fine-grained levels of support.
</span></span>
</span></p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.2.1.1" style="font-size:90%;">Table 3</span>. </span><span class="ltx_text" id="S5.T3.3.2" style="font-size:90%;">Classification performance of faithfulness metrics regarding ROC-AUC score (%) on the VeJudge dataset. The overall performance is the macro-averaged performance of three binary classification settings. The best scores are marked in bold.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4" style="width:427.1pt;height:221.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(74.7pt,-38.7pt) scale(1.537461051935,1.537461051935) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.4.1">
<tr class="ltx_tr" id="S5.T3.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.4.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.T3.4.1.1.1.1">Metric</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.4.1.1.2"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.T3.4.1.1.2.1">FS-vs-NS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.4.1.1.3"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.T3.4.1.1.3.1">FS-vs-PS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.4.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.T3.4.1.1.4.1">PS-vs-NS</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T3.4.1.1.5"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.T3.4.1.1.5.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.1.2.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.1.2.1.1">FactCC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.2.2">68.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.2.3">62.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.2.4">56.81</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.4.1.2.5">62.72</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.3">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.3.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.1.3.1.1">SummaC<sub class="ltx_sub" id="S5.T3.4.1.3.1.1.1"><span class="ltx_text ltx_font_upright" id="S5.T3.4.1.3.1.1.1.1">ZS</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.3.2">78.37</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.3.3">72.96</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.3.4">58.12</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.1.3.5">69.82</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.4">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.4.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.1.4.1.1">SummaC<sub class="ltx_sub" id="S5.T3.4.1.4.1.1.1"><span class="ltx_text ltx_font_upright" id="S5.T3.4.1.4.1.1.1.1">Conv</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.4.2">85.32</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.4.3">78.74</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.4.4">62.57</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.1.4.5">75.54</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.5">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.5.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.1.5.1.1">BARTScore</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.5.2">87.65</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.5.3">75.42</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.5.4">71.94</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.1.5.5">78.34</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.6">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.6.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.1.6.1.1">AlignScore</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.6.2">90.97</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.6.3">81.41</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.6.4">70.53</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.1.6.5">80.97</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.7">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.7.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.1.7.1.1">BERTScore</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.7.2">91.92</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.7.3">75.94</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.1.7.4"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.7.4.1">79.89</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.4.1.7.5">82.58</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.1.8.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.1.8.1.1">AutoAIS</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.2"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.8.2.1">92.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.3"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.8.3.1">82.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.4">74.21</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T3.4.1.8.5"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.8.5.1">83.06</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Classification Results</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2408.12398v1#S5.T3" title="Table 3 ‣ 5.1. Correlation Results ‣ 5. Results &amp; Analyses ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_tag">Table 3</span></a> presents the results of the classification evaluation. The observations can be summarized as follows:

<span class="ltx_inline-enumerate" id="S5.I2">
<span class="ltx_inline-item" id="S5.I2.i1"><span class="ltx_tag ltx_tag_inline-item">1)</span> <span class="ltx_text" id="S5.I2.i1.1">among all three binary classification task settings, most faithfulness metrics demonstrate superior performance in the <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i1.1.1">FS-vs-NS</span> setting. Notably, entailment-based <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i1.1.2">AutoAIS</span>, with the highest ROC-AUC score of <math alttext="92.65" class="ltx_Math" display="inline" id="S5.I2.i1.1.m1.1"><semantics id="S5.I2.i1.1.m1.1a"><mn id="S5.I2.i1.1.m1.1.1" xref="S5.I2.i1.1.m1.1.1.cmml">92.65</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i1.1.m1.1b"><cn id="S5.I2.i1.1.m1.1.1.cmml" type="float" xref="S5.I2.i1.1.m1.1.1">92.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i1.1.m1.1c">92.65</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i1.1.m1.1d">92.65</annotation></semantics></math>, exemplifies significant discriminability between full support and no support instances. This performance can be attributed to its extensive parameters, comprising 11 billion parameters, in contrast to the hundreds of millions of other metrics;
</span></span>
<span class="ltx_inline-item" id="S5.I2.i2"><span class="ltx_tag ltx_tag_inline-item">2)</span> <span class="ltx_text" id="S5.I2.i2.2">a pronounced decline in classification performance is observed across the other two settings. For instance, when comparing the <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i2.2.1">FS-vs-NS</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i2.2.2">PS-vs-NS</span> settings, the ROC-AUC score of <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i2.2.3">AutoAIS</span> diminishes from <math alttext="92.65" class="ltx_Math" display="inline" id="S5.I2.i2.1.m1.1"><semantics id="S5.I2.i2.1.m1.1a"><mn id="S5.I2.i2.1.m1.1.1" xref="S5.I2.i2.1.m1.1.1.cmml">92.65</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i2.1.m1.1b"><cn id="S5.I2.i2.1.m1.1.1.cmml" type="float" xref="S5.I2.i2.1.m1.1.1">92.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.1.m1.1c">92.65</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.1.m1.1d">92.65</annotation></semantics></math> to <math alttext="74.21" class="ltx_Math" display="inline" id="S5.I2.i2.2.m2.1"><semantics id="S5.I2.i2.2.m2.1a"><mn id="S5.I2.i2.2.m2.1.1" xref="S5.I2.i2.2.m2.1.1.cmml">74.21</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i2.2.m2.1b"><cn id="S5.I2.i2.2.m2.1.1.cmml" type="float" xref="S5.I2.i2.2.m2.1.1">74.21</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.2.m2.1c">74.21</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.2.m2.1d">74.21</annotation></semantics></math>. This decline indicates that even the best-performing metric struggles with granular sensitivity to varying levels of support; and
</span></span>
<span class="ltx_inline-item" id="S5.I2.i3"><span class="ltx_tag ltx_tag_inline-item">3)</span> <span class="ltx_text" id="S5.I2.i3.1">while entailment-based <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i3.1.1">AutoAIS</span> generally surpasses other metrics in overall performance, it is outperformed by similarity-based <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i3.1.2">BERTScore</span> in the <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i3.1.3">PS-vs-NS</span> setting. Interestingly, while most metrics exhibit their lowest performance in this particular setting, <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i3.1.4">BERTScore</span> shows its least effectiveness in another setting, <span class="ltx_text ltx_font_smallcaps" id="S5.I2.i3.1.5">FS-vs-PS</span>. This underscores the unique prediction behaviors displayed by different types of metrics across the binary classification settings.
</span></span>
</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Retrieval Results</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2408.12398v1#S5.T4" title="Table 4 ‣ 5.3. Retrieval Results ‣ 5. Results &amp; Analyses ‣ A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"><span class="ltx_text ltx_ref_tag">Table 4</span></a> presents the results of the retrieval evaluation. The key findings are as follows:

<span class="ltx_inline-enumerate" id="S5.I3">
<span class="ltx_inline-item" id="S5.I3.i1"><span class="ltx_tag ltx_tag_inline-item">1)</span> <span class="ltx_text" id="S5.I3.i1.2">similarity-based metrics, <span class="ltx_text ltx_font_smallcaps" id="S5.I3.i1.2.1">BARTScore</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.I3.i1.2.2">BERTScore</span>, outperforms other entailment-based metrics in both MRR and nDCG@n. For instance, entailment-based <span class="ltx_text ltx_font_smallcaps" id="S5.I3.i1.2.3">AutoAIS</span> exhibits weaker MMR scores than <span class="ltx_text ltx_font_smallcaps" id="S5.I3.i1.2.4">BARTScore</span> (<math alttext="0.846" class="ltx_Math" display="inline" id="S5.I3.i1.1.m1.1"><semantics id="S5.I3.i1.1.m1.1a"><mn id="S5.I3.i1.1.m1.1.1" xref="S5.I3.i1.1.m1.1.1.cmml">0.846</mn><annotation-xml encoding="MathML-Content" id="S5.I3.i1.1.m1.1b"><cn id="S5.I3.i1.1.m1.1.1.cmml" type="float" xref="S5.I3.i1.1.m1.1.1">0.846</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I3.i1.1.m1.1c">0.846</annotation><annotation encoding="application/x-llamapun" id="S5.I3.i1.1.m1.1d">0.846</annotation></semantics></math> vs. <math alttext="0.881" class="ltx_Math" display="inline" id="S5.I3.i1.2.m2.1"><semantics id="S5.I3.i1.2.m2.1a"><mn id="S5.I3.i1.2.m2.1.1" xref="S5.I3.i1.2.m2.1.1.cmml">0.881</mn><annotation-xml encoding="MathML-Content" id="S5.I3.i1.2.m2.1b"><cn id="S5.I3.i1.2.m2.1.1.cmml" type="float" xref="S5.I3.i1.2.m2.1.1">0.881</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I3.i1.2.m2.1c">0.881</annotation><annotation encoding="application/x-llamapun" id="S5.I3.i1.2.m2.1d">0.881</annotation></semantics></math>). This is likely because entailment-based metrics are more sensitive to noisy information than similarity-based metrics, as many irrelevant documents exist in retrieval scenarios. This suggests the need for the robustness improvements of metrics in post-hoc retrieval scenarios;
</span></span>
<span class="ltx_inline-item" id="S5.I3.i2"><span class="ltx_tag ltx_tag_inline-item">2)</span> <span class="ltx_text" id="S5.I3.i2.4">a significant correlation is observed between MRR and nDCG@n scores across all metrics. Notably, nDCG@n effectively captures the performance variations as the number of text chunks increases (i.e., 5, 10, 20). For instance, as the chunk count increases, <span class="ltx_text ltx_font_smallcaps" id="S5.I3.i2.4.1">BARTScore</span> shows a marginal performance improvement (from <math alttext="0.878" class="ltx_Math" display="inline" id="S5.I3.i2.1.m1.1"><semantics id="S5.I3.i2.1.m1.1a"><mn id="S5.I3.i2.1.m1.1.1" xref="S5.I3.i2.1.m1.1.1.cmml">0.878</mn><annotation-xml encoding="MathML-Content" id="S5.I3.i2.1.m1.1b"><cn id="S5.I3.i2.1.m1.1.1.cmml" type="float" xref="S5.I3.i2.1.m1.1.1">0.878</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I3.i2.1.m1.1c">0.878</annotation><annotation encoding="application/x-llamapun" id="S5.I3.i2.1.m1.1d">0.878</annotation></semantics></math> to <math alttext="0.897" class="ltx_Math" display="inline" id="S5.I3.i2.2.m2.1"><semantics id="S5.I3.i2.2.m2.1a"><mn id="S5.I3.i2.2.m2.1.1" xref="S5.I3.i2.2.m2.1.1.cmml">0.897</mn><annotation-xml encoding="MathML-Content" id="S5.I3.i2.2.m2.1b"><cn id="S5.I3.i2.2.m2.1.1.cmml" type="float" xref="S5.I3.i2.2.m2.1.1">0.897</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I3.i2.2.m2.1c">0.897</annotation><annotation encoding="application/x-llamapun" id="S5.I3.i2.2.m2.1d">0.897</annotation></semantics></math>), while <span class="ltx_text ltx_font_smallcaps" id="S5.I3.i2.4.2">FactCC</span>—the least performing metric—exhibits a more pronounced enhancement (from <math alttext="0.648" class="ltx_Math" display="inline" id="S5.I3.i2.3.m3.1"><semantics id="S5.I3.i2.3.m3.1a"><mn id="S5.I3.i2.3.m3.1.1" xref="S5.I3.i2.3.m3.1.1.cmml">0.648</mn><annotation-xml encoding="MathML-Content" id="S5.I3.i2.3.m3.1b"><cn id="S5.I3.i2.3.m3.1.1.cmml" type="float" xref="S5.I3.i2.3.m3.1.1">0.648</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I3.i2.3.m3.1c">0.648</annotation><annotation encoding="application/x-llamapun" id="S5.I3.i2.3.m3.1d">0.648</annotation></semantics></math> to <math alttext="0.710" class="ltx_Math" display="inline" id="S5.I3.i2.4.m4.1"><semantics id="S5.I3.i2.4.m4.1a"><mn id="S5.I3.i2.4.m4.1.1" xref="S5.I3.i2.4.m4.1.1.cmml">0.710</mn><annotation-xml encoding="MathML-Content" id="S5.I3.i2.4.m4.1b"><cn id="S5.I3.i2.4.m4.1.1.cmml" type="float" xref="S5.I3.i2.4.m4.1.1">0.710</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I3.i2.4.m4.1c">0.710</annotation><annotation encoding="application/x-llamapun" id="S5.I3.i2.4.m4.1d">0.710</annotation></semantics></math>); and
</span></span>
<span class="ltx_inline-item" id="S5.I3.i3"><span class="ltx_tag ltx_tag_inline-item">3)</span> <span class="ltx_text" id="S5.I3.i3.1">we observe an intriguing shift in performance ranking when comparing metrics between classification and retrieval evaluations. For instance, <span class="ltx_text ltx_font_smallcaps" id="S5.I3.i3.1.1">BARTScore</span> ascends from the fourth place to the first while <span class="ltx_text ltx_font_smallcaps" id="S5.I3.i3.1.2">AutoAIS</span> sees a decline from the top to the third. This divergence highlights that these evaluations offer unique insights into the capabilities of metrics.
</span></span>
</span></p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.8.4.1" style="font-size:90%;">Table 4</span>. </span><span class="ltx_text" id="S5.T4.6.3" style="font-size:90%;">Retrieval performance of faithfulness metrics regarding MRR and nDCG@n scores on the VeJudge dataset. Note that we assign relevance labels <math alttext="2" class="ltx_Math" display="inline" id="S5.T4.4.1.m1.1"><semantics id="S5.T4.4.1.m1.1b"><mn id="S5.T4.4.1.m1.1.1" xref="S5.T4.4.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.T4.4.1.m1.1c"><cn id="S5.T4.4.1.m1.1.1.cmml" type="integer" xref="S5.T4.4.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.1.m1.1d">2</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.1.m1.1e">2</annotation></semantics></math>, <math alttext="1" class="ltx_Math" display="inline" id="S5.T4.5.2.m2.1"><semantics id="S5.T4.5.2.m2.1b"><mn id="S5.T4.5.2.m2.1.1" xref="S5.T4.5.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.T4.5.2.m2.1c"><cn id="S5.T4.5.2.m2.1.1.cmml" type="integer" xref="S5.T4.5.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.2.m2.1d">1</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.2.m2.1e">1</annotation></semantics></math>, and <math alttext="0" class="ltx_Math" display="inline" id="S5.T4.6.3.m3.1"><semantics id="S5.T4.6.3.m3.1b"><mn id="S5.T4.6.3.m3.1.1" xref="S5.T4.6.3.m3.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S5.T4.6.3.m3.1c"><cn id="S5.T4.6.3.m3.1.1.cmml" type="integer" xref="S5.T4.6.3.m3.1.1">0</cn></annotation-xml></semantics></math> to full, partial, and no support, respectively. The best scores are marked in bold.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.9" style="width:427.1pt;height:219.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(73.3pt,-37.7pt) scale(1.52291877752664,1.52291877752664) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.9.1">
<tr class="ltx_tr" id="S5.T4.9.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.9.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.T4.9.1.1.1.1">Metric</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.9.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.9.1.1.2.1">MRR</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.9.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.9.1.1.3.1">nDCG@5</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.9.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.9.1.1.4.1">nDCG@10</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T4.9.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T4.9.1.1.5.1">nDCG@20</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.9.1.2.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T4.9.1.2.1.1">FactCC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.9.1.2.2">0.656</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.9.1.2.3">0.648</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.9.1.2.4">0.689</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4.9.1.2.5">0.710</td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.1.3">
<td class="ltx_td ltx_align_left" id="S5.T4.9.1.3.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T4.9.1.3.1.1">SummaC<sub class="ltx_sub" id="S5.T4.9.1.3.1.1.1"><span class="ltx_text ltx_font_upright" id="S5.T4.9.1.3.1.1.1.1">ZS</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.3.2">0.737</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.3.3">0.729</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.3.4">0.759</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.9.1.3.5">0.776</td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.1.4">
<td class="ltx_td ltx_align_left" id="S5.T4.9.1.4.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T4.9.1.4.1.1">SummaC<sub class="ltx_sub" id="S5.T4.9.1.4.1.1.1"><span class="ltx_text ltx_font_upright" id="S5.T4.9.1.4.1.1.1.1">Conv</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.4.2">0.776</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.4.3">0.772</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.4.4">0.798</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.9.1.4.5">0.811</td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.1.5">
<td class="ltx_td ltx_align_left" id="S5.T4.9.1.5.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T4.9.1.5.1.1">AlignScore</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.5.2">0.847</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.5.3">0.842</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.5.4">0.863</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.9.1.5.5">0.869</td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.1.6">
<td class="ltx_td ltx_align_left" id="S5.T4.9.1.6.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T4.9.1.6.1.1">AutoAIS</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.6.2">0.846</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.6.3">0.846</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.6.4">0.865</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.9.1.6.5">0.872</td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.1.7">
<td class="ltx_td ltx_align_left" id="S5.T4.9.1.7.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T4.9.1.7.1.1">BERTScore</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.7.2">0.867</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.7.3">0.865</td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.1.7.4">0.881</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.9.1.7.5">0.887</td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.1.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.9.1.8.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T4.9.1.8.1.1">BARTScore</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.9.1.8.2"><span class="ltx_text ltx_font_bold" id="S5.T4.9.1.8.2.1">0.881</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.9.1.8.3"><span class="ltx_text ltx_font_bold" id="S5.T4.9.1.8.3.1">0.878</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.9.1.8.4"><span class="ltx_text ltx_font_bold" id="S5.T4.9.1.8.4.1">0.891</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T4.9.1.8.5"><span class="ltx_text ltx_font_bold" id="S5.T4.9.1.8.5.1">0.897</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Implications</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Based on the evaluation results, we observe that no single faithfulness metric consistently excels across three distinct evaluation protocols.
For instance, <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.1">SummaC<sub class="ltx_sub" id="S5.SS4.p1.1.1.1"><span class="ltx_text ltx_font_upright" id="S5.SS4.p1.1.1.1.1">Conv</span></sub></span> achieves the highest performance in correlation analysis, yet it under-performs in classification and retrieval evaluations.
This disparity suggests that these evaluation protocols are complementary and should be integrated to comprehensively assess the effectiveness of different metrics.
This inconsistency also reveals that the best-performing metrics are insufficiently effective in addressing fine-grained support level scenarios. Particularly, they fail to effectively distinguish partial support from either full or no support scenarios.
Furthermore, when comparing entailment-based metrics with similarity-based metrics, a notable shift in performance ranking is observed between the classification and retrieval evaluations.
Specifically, the similarity-based <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.2">BARTScore</span> advances from fourth to first place, whereas the entailment-based <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.3">AutoAIS</span> declines from the top position to third place.
This shift may be attributed to the higher sensitivity of entailment-based metrics to noisy data, which is introduced by irrelevant documents in retrieval scenarios.
This suggests the need for improving the robustness of entailment-based metrics against irrelevant documents.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Consequently, we propose the following practical recommendations to develop more effective metrics for automated citation evaluation:

<span class="ltx_inline-enumerate" id="S5.I4">
<span class="ltx_inline-item" id="S5.I4.i1"><span class="ltx_tag ltx_tag_inline-item">1)</span> <span class="ltx_text ltx_font_bold" id="S5.I4.i1.1">Development of training resources:</span><span class="ltx_text" id="S5.I4.i1.2">motivated by the observation that the best-performing metrics still struggle with identifying partial support, we recommend the development of training resources that include fine-grained support level annotations. These resources could significantly enhance the metrics’ fine-grained sensitivity to varying support levels.
</span></span>
<span class="ltx_inline-item" id="S5.I4.i2"><span class="ltx_tag ltx_tag_inline-item">2)</span> <span class="ltx_text ltx_font_bold" id="S5.I4.i2.1">Introduction of contrastive learning:</span><span class="ltx_text" id="S5.I4.i2.2">to improve the robustness of metrics in post-hoc retrieval scenarios, we recommend fine-tuning metrics using contrastive learning frameworks. This method has demonstrated effectiveness in various information retrieval tasks <cite class="ltx_cite ltx_citemacro_citep">(Izacard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib21" title="">2022</a>)</cite>.
</span></span>
</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Related Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This section outlines two lines of related research: faithfulness evaluation metrics and citation evaluation.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Faithfulness Evaluation Metrics</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Faithfulness evaluation metrics are crucial for assessing the factual consistency of text generated by models relative to the source text.
It receives great interest within the field of natural language generation (NLG) <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib14" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib18" title="">2021b</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib50" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib51" title="">2023b</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib17" title="">2024c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib19" title="">d</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib53" title="">2024</a>)</cite>, particularly in abstractive summarization <cite class="ltx_cite ltx_citemacro_citep">(Maynez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib33" title="">2020</a>; Kryscinski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib24" title="">2020</a>; Huang and Worring, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib16" title="">2020</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib15" title="">2021a</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib48" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib49" title="">c</a>)</cite>.
In general, faithfulness metrics are categorized into three types: entailment-based, similarity-based, and QA-based metrics.
Entailment-based metrics employ natural language inference (NLI) models to determine if the source text entails the generated text <cite class="ltx_cite ltx_citemacro_citep">(Falke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib6" title="">2019</a>; Laban et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib25" title="">2022</a>; Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib11" title="">2022</a>; Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib45" title="">2023</a>)</cite>.
Similarity-based metrics, such as BERTScore <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib46" title="">2020</a>)</cite> and BARTScore <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib43" title="">2021</a>)</cite>, quantify text similarity and have demonstrated robust performance in faithfulness evaluation <cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib34" title="">2021</a>; Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib11" title="">2022</a>)</cite>.
QA-based metrics utilize a combination of question generation and question answering to estimate faithfulness levels <cite class="ltx_cite ltx_citemacro_citep">(Durmus et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib4" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib40" title="">2020</a>; Scialom et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib37" title="">2021</a>; Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib5" title="">2022</a>)</cite>.
In this work, we exclude QA-based metrics from our work, following recent works suggesting the challenging limitations in these metrics <cite class="ltx_cite ltx_citemacro_citep">(Kamoi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib22" title="">2023</a>)</cite>. We focus on the extrinsic evaluation of faithfulness metrics against human judgments in scenarios requiring fine-grained citation support.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Citation Evaluation</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Citation evaluation seeks to enhance the trustworthiness of retrieval-augmented LLMs by verifying the support provided by citations to the generated statements <cite class="ltx_cite ltx_citemacro_citep">(Rashkin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib36" title="">2023</a>; Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib44" title="">2023</a>; Huang and Chang, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib13" title="">2023</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib12" title="">2024b</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib47" title="">2024a</a>)</cite>.
Given the labor-intensive nature of manual citation evaluation, there has been a shift towards automated approaches to reduce dependence on human evaluation.
Since the goals of automated citation evaluation align closely with faithfulness evaluation in NLG, faithfulness metrics are employed to verify whether a citation supports the corresponding statement <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib27" title="">2024a</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib39" title="">2023</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib42" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib29" title="">2024b</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib38" title="">2024</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.12398v1#bib.bib20" title="">2024a</a>)</cite>.
Despite their widespread usage, the effectiveness of these metrics in more practical fine-grained citation support scenarios, such as those involving partial support by citations, has not been adequately addressed. Questions remain about the metrics’ capability to differentiate citations in these fine-grained scenarios.
This work addresses these gaps by examining the effectiveness of faithfulness metrics across three distinct levels of citation support: full, partial, and no support.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">LLMs are susceptible to generating hallucinated content, motivating the research on the integration of retrieval augmentation mechanisms to enhance the verifiability of generated statements through in-line citations.
However, evaluating how well these citations support the statements remains a major challenge due to the labor-intensive nature of manual citation evaluation.
Consequently, faithfulness metrics have been adopted to automate this evaluation, primarily determining citation support in a binary classification scenario.
This paper proposes a comparative evaluation framework to explore the efficacy of faithfulness metrics beyond the binary scenario by examining three levels of citation support: full, partial, and no support.
Our framework assesses the alignment between metric scores and human judgments across three evaluation protocols: correlation analysis, classification evaluation, and retrieval evaluation.
Experimental results reveal that no single metric consistently excels across all evaluation protocols, indicating the complexity of automated citation evaluation and the limitations of existing faithfulness metrics in identifying partial support scenarios.
Based on these findings, we further provide practical suggestions for the development of more effective metrics in automated citation evaluation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bohnet et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, et al<span class="ltx_text" id="bib.bib2.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Attributed question answering: Evaluation and modeling for attributed large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.4.1">arXiv preprint arXiv:2212.08037</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>. 4171–4186.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/n19-1423" title="">https://doi.org/10.18653/v1/n19-1423</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Durmus et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Esin Durmus, He He, and Mona Diab. 2020.

</span>
<span class="ltx_bibblock">FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. 5055–5070.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.acl-main.454" title="">https://doi.org/10.18653/v1/2020.acl-main.454</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022.

</span>
<span class="ltx_bibblock">QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. 2587–2601.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.naacl-main.187" title="">https://doi.org/10.18653/v1/2022.naacl-main.187</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Falke et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>. 2214–2220.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/P19-1213" title="">https://doi.org/10.18653/v1/P19-1213</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023a.

</span>
<span class="ltx_bibblock">RARR: Researching and Revising What Language Models Say, Using Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 16477–16508.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023b.

</span>
<span class="ltx_bibblock">Enabling Large Language Models to Generate Text with Citations. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 6465–6488.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.emnlp-main.398" title="">https://doi.org/10.18653/v1/2023.emnlp-main.398</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Deberta: Decoding-Enhanced Bert with Disentangled Attention. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=XPZIaotutsD" title="">https://openreview.net/forum?id=XPZIaotutsD</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hermann et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015.

</span>
<span class="ltx_bibblock">Teaching Machines to Read and Comprehend. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada</em>. 1693–1701.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html" title="">https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022.

</span>
<span class="ltx_bibblock">TRUE: Re-evaluating Factual Consistency Evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. 3905–3920.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.naacl-main.287" title="">https://doi.org/10.18653/v1/2022.naacl-main.287</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Chengyu Huang, Zeqiu Wu, Yushi Hu, and Wenya Wang. 2024b.

</span>
<span class="ltx_bibblock">Training Language Models to Generate Text with Citations via Fine-Grained Rewards.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">CoRR</em> abs/2402.04315 (2024).

</span>
<span class="ltx_bibblock">arXiv:2402.04315

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Chang (2023)</span>
<span class="ltx_bibblock">
Jie Huang and Kevin Chen-Chuan Chang. 2023.

</span>
<span class="ltx_bibblock">Citation: A Key to Building Responsible and Accountable Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em> abs/2307.02185 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2307.02185" title="">https://doi.org/10.48550/arXiv.2307.02185</a>
arXiv:2307.02185

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jia-Hong Huang, Cuong Duc Dao, Modar Alfadly, and Bernard Ghanem. 2019.

</span>
<span class="ltx_bibblock">A novel framework for robustness analysis of visual qa models. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 33. 8449–8456.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Jia-Hong Huang, Luka Murn, Marta Mrak, and Marcel Worring. 2021a.

</span>
<span class="ltx_bibblock">GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video Summarization. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the International Conference on Multimedia Retrieval</em>. 580–589.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Worring (2020)</span>
<span class="ltx_bibblock">
Jia-Hong Huang and Marcel Worring. 2020.

</span>
<span class="ltx_bibblock">Query-controllable video summarization. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the International Conference on Multimedia Retrieval</em>. 242–250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Jia-Hong Huang, Chao-Chun Yang, Yixian Shen, Alessio M Pacces, and Evangelos Kanoulas. 2024c.

</span>
<span class="ltx_bibblock">Optimizing Numerical Estimation and Operational Efficiency in the Legal Domain through Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">ACM International Conference on Information and Knowledge Management (CIKM)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Jia-Hong Huang, C-H Huck Yang, Fangyu Liu, Meng Tian, Yi-Chieh Liu, Ting-Wei Wu, I Lin, Kang Wang, Hiromasa Morikawa, Hernghua Chang, et al<span class="ltx_text" id="bib.bib18.3.1">.</span> 2021b.

</span>
<span class="ltx_bibblock">DeepOpht: medical report generation for retinal images via deep models and visual explanation. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.4.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>. 2442–2452.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Jia-Hong Huang, Hongyi Zhu, Yixian Shen, Stevan Rudinac, Alessio M. Pacces, and Evangelos Kanoulas. 2024d.

</span>
<span class="ltx_bibblock">A Novel Evaluation Framework for Image2Text Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">International ACM SIGIR Conference on Research and Development in Information Retrieval, LLM4Eval Workshop</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng, Duyu Tang, Dandan Tu, and Bing Qin. 2024a.

</span>
<span class="ltx_bibblock">Learning Fine-Grained Grounded Citations for Attributed Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Findings of the Association for Computational Linguistics ACL 2024</em>. 14095–14113.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Unsupervised Dense Information Retrieval with Contrastive Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Transactions on Machine Learning Research</em> 2022 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamoi et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ryo Kamoi, Tanya Goyal, and Greg Durrett. 2023.

</span>
<span class="ltx_bibblock">Shortcomings of Question Answering Based Factuality Frameworks for Error Localization. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>. 132–146.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense Passage Retrieval for Open-Domain Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. 6769–6781.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kryscinski et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020.

</span>
<span class="ltx_bibblock">Evaluating the Factual Consistency of Abstractive Text Summarization. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. 9332–9346.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.emnlp-main.750" title="">https://doi.org/10.18653/v1/2020.emnlp-main.750</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laban et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022.

</span>
<span class="ltx_bibblock">SummaC: Re-visiting NLI-based Models for Inconsistency Detection in Summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Transactions of the Association for Computational Linguistics</em> 10 (2022), 163–177.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/tacl_a_00453" title="">https://doi.org/10.1162/tacl_a_00453</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock">BART: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>. 7871–7880.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.acl-main.703" title="">https://doi.org/10.18653/v1/2020.acl-main.703</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xinze Li, Yixin Cao, Liangming Pan, Yubo Ma, and Aixin Sun. 2024a.

</span>
<span class="ltx_bibblock">Towards Verifiable Generation: A Benchmark for Knowledge-Aware Language Model Attribution. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Findings of the Association for Computational Linguistics ACL 2024</em>. 493–516.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, and Xipeng Qiu. 2024c.

</span>
<span class="ltx_bibblock">LLatrieval: LLM-verified Retrieval for Verifiable Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>. 5453–5471.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2024.naacl-long.305" title="">https://doi.org/10.18653/v1/2024.naacl-long.305</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yifei Li, Xiang Yue, Zeyi Liao, and Huan Sun. 2024b.

</span>
<span class="ltx_bibblock">AttributionBench: How Hard Is Automatic Attribution Evaluation?. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Findings of the Association for Computational Linguistics ACL 2024</em>. 14919–14935.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nelson Liu, Tianyi Zhang, and Percy Liang. 2023.

</span>
<span class="ltx_bibblock">Evaluating Verifiability in Generative Search Engines. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 7001–7025.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.findings-emnlp.467" title="">https://doi.org/10.18653/v1/2023.findings-emnlp.467</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">arXiv preprint arXiv:1907.11692</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Liang Ma, Shuyang Cao, Robert L Logan IV, Di Lu, Shihao Ran, Ke Zhang, Joel Tetreault, and Alejandro Jaimes. 2023.

</span>
<span class="ltx_bibblock">BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 12788–12812.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maynez et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.

</span>
<span class="ltx_bibblock">On Faithfulness and Factuality in Abstractive Summarization. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. 1906–1919.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.acl-main.173" title="">https://doi.org/10.18653/v1/2020.acl-main.173</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pagnoni et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021.

</span>
<span class="ltx_bibblock">Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. 4812–4829.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2021.naacl-main.383" title="">https://doi.org/10.18653/v1/2021.naacl-main.383</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, and et al. 2020.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Journal of Machine Learning Research</em> 21 (2020), 140:1–140:67.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v21/20-074.html" title="">http://jmlr.org/papers/v21/20-074.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rashkin et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023.

</span>
<span class="ltx_bibblock">Measuring attribution in natural language generation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Computational Linguistics</em> 49, 4 (2023), 777–840.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scialom et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021.

</span>
<span class="ltx_bibblock">QuestEval: Summarization Asks for Fact-Based Evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>. 6594–6604.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2021.emnlp-main.529" title="">https://doi.org/10.18653/v1/2021.emnlp-main.529</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jiajun Shen, Tong Zhou, Suifeng Zhao, Yubo Chen, and Kang Liu. 2024.

</span>
<span class="ltx_bibblock">Citekit: A Modular Toolkit for Large Language Model Citation Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv preprint arXiv:2408.04662</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, and Dawei Yin. 2023.

</span>
<span class="ltx_bibblock">Towards Verifiable Text Generation with Evolving Memory and Self-Reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">CoRR</em> abs/2312.09075 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2312.09075" title="">https://doi.org/10.48550/ARXIV.2312.09075</a>
arXiv:2312.09075

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.

</span>
<span class="ltx_bibblock">Asking and Answering Questions to Evaluate the Factual Consistency of Summaries. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. 5008–5020.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.acl-main.450" title="">https://doi.org/10.18653/v1/2020.acl-main.450</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.

</span>
<span class="ltx_bibblock">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>. 1112–1122.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xi Ye, Ruoxi Sun, Sercan Arik, and Tomas Pfister. 2024.

</span>
<span class="ltx_bibblock">Effective Large Language Model Adaptation for Improved Grounding and Citation Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>. 6237–6251.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.

</span>
<span class="ltx_bibblock">BARTScore: Evaluating Generated Text as Text Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual</em>. 27263–27277.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html" title="">https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023.

</span>
<span class="ltx_bibblock">Automatic Evaluation of Attribution by Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 4615–4635.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.findings-emnlp.307" title="">https://doi.org/10.18653/v1/2023.findings-emnlp.307</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.

</span>
<span class="ltx_bibblock">AlignScore: Evaluating Factual Consistency with A Unified Alignment Function. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 11328–11348.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2020.

</span>
<span class="ltx_bibblock">BERTScore: Evaluating Text Generation with BERT. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=SkeHuCVFDr" title="">https://openreview.net/forum?id=SkeHuCVFDr</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Weijia Zhang, Mohammad Aliannejadi, Yifei Yuan, Jiahuan Pei, Jia-Hong Huang, and Evangelos Kanoulas. 2024a.

</span>
<span class="ltx_bibblock">Towards Fine-Grained Citation Evaluation in Generated Text: A Comparative Analysis of Faithfulness Metrics. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">International Natural Language Generation Conference (INLG)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Weijia Zhang, Jia-Hong Huang, Svitlana Vakulenko, Yumo Xu, Thilina Rajapakse, and Evangelos Kanoulas. 2024b.

</span>
<span class="ltx_bibblock">Beyond Relevant Documents: A Knowledge-Intensive Approach for Query-Focused Summarization using Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Proceedings of the 2024 International Conference on Pattern Recognition (ICPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Weijia Zhang, Vaishali Pal, Jia-Hong Huang, Evangelos Kanoulas, and Maarten de Rijke. 2024c.

</span>
<span class="ltx_bibblock">QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Proceedings of the 27th European Conference on Artificial Intelligence (ECAI)</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2405.05109" title="">https://doi.org/10.48550/ARXIV.2405.05109</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Weijia Zhang, Svitlana Vakulenko, Thilina Rajapakse, and Evangelos Kanoulas. 2021.

</span>
<span class="ltx_bibblock">Scaling up query-focused summarization to meet open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">ArXiv preprint, abs/2112.07536</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Weijia Zhang, Svitlana Vakulenko, Thilina Rajapakse, Yumo Xu, and Evangelos Kanoulas. 2023b.

</span>
<span class="ltx_bibblock">Tackling query-focused summarization as a knowledge-intensive task: A pilot study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">The First Workshop on Generative Information Retrieval (Gen-IR) at SIGIR</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023a.

</span>
<span class="ltx_bibblock">Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">CoRR</em> abs/2309.01219 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.01219" title="">https://doi.org/10.48550/ARXIV.2309.01219</a>
arXiv:2309.01219

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hongyi Zhu, Jia-Hong Huang, Stevan Rudinac, and Evangelos Kanoulas. 2024.

</span>
<span class="ltx_bibblock">Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Proceedings of the 2024 International Conference on Multimedia Retrieval</em>. 978–987.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 22 13:37:30 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
