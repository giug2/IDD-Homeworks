<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024.</title>
<!--Generated on Mon Jun  3 07:48:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Machine learning,  recommendation model,  resource management,  resource scaling,  microservice,  model deployment
" lang="en" name="keywords"/>
<base href="/html/2406.06955v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S1" title="In ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2" title="In ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.SS1" title="In II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">DNN-based Recommendation Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.SS2" title="In II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Model Serving Architectures</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.SS3" title="In II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Microservices</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.SS4" title="In II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Related Work</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3" title="In ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Motivation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.SS1" title="In III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Heterogeneous Resource Demands of RecSys</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.SS2" title="In III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Skewed Access Pattern and Locality in Embeddings</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4" title="In ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">ElasticRec Model Serving Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS1" title="In IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Microservice-based Inference Server Design</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS2" title="In IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Utility-based Resource Allocation for Embeddings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS3" title="In IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Bucketization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS4" title="In IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Deploying ElasticRec at Scale using Kubernetes</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5" title="In ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Evaluation Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.SS1" title="In V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Hardware Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.SS2" title="In V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Software Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.SS3" title="In V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Workloads</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6" title="In ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS1" title="In VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Microbenchmarks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS2" title="In VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">State-of-the-art RecSys Workloads (CPU-only)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS3" title="In VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">State-of-the-art RecSys Workloads (CPU-GPU)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS4" title="In VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">Effectiveness to dynamic input query traffic</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS5" title="In VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-E</span> </span><span class="ltx_text ltx_font_italic">ElasticRec vs. GPU Embedding Caches</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S7" title="In ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:173%;">ElasticRec: A Microservice-based Model Serving Architecture
<br class="ltx_break"/>Enabling Elastic Resource Scaling for Recommendation Models
<span class="ltx_note ltx_role_thanks" id="id4.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yujeong Choi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Electrical Engineering
<br class="ltx_break"/>KAIST
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id5.1.id1">yjchoi0606@kaist.ac.kr</span>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiin Kim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Electrical Engineering
<br class="ltx_break"/>KAIST
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id6.1.id1">jiin.kim@kaist.ac.kr</span>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minsoo Rhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Electrical Engineering
<br class="ltx_break"/>KAIST
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.1.id1">mrhu@kaist.ac.kr</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.3">With the increasing popularity of recommendation systems (RecSys), the demand
for compute resources in datacenters has surged. However, the model-wise
resource allocation employed in current RecSys model serving architectures
falls short in effectively utilizing resources, leading to sub-optimal total
cost of ownership. We propose ElasticRec, a model serving
architecture for RecSys providing resource elasticity and high memory
efficiency. ElasticRec is based on a microservice-based software architecture
for fine-grained resource allocation, tailored to the heterogeneous resource
demands of RecSys. Additionally, ElasticRec achieves high memory efficiency via
our utility-based resource allocation. Overall, ElasticRec achieves an
average 3.3<math alttext="\times" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">×</annotation></semantics></math> reduction in memory allocation size and 8.1<math alttext="\times" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><times id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">×</annotation></semantics></math>
increase in memory utility, resulting in an average 1.6<math alttext="\times" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><times id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">×</annotation></semantics></math> reduction in
deployment cost compared to state-of-the-art RecSys inference serving
system.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Machine learning, recommendation model, resource management, resource scaling, microservice, model deployment

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Deep neural network (DNN) based recommendation system (RecSys) accounts for a significant portion of
machine learning (ML) inference cycles in modern datacenters (75% in
Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib20" title="">20</a>]</cite>, 25% in Google <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib24" title="">24</a>]</cite>).
These latency-critical
ML services operate with stringent service level agreement (SLA) goals on tail
latency, so maximizing latency-bounded throughput (i.e., number of service
queries processed per second that meets SLA, aka QPS) becomes critical.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To serve billions of service queries around the world, datacenters replicate a
large fleet of inference servers, each server replica provisioned with
its own copy of the entire model
parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib20" title="">20</a>]</cite>. Such
baseline “model-wise” resource allocation enables each server replica to independently service user queries, which helps utilize
query-level parallelism across the fleet and improve QPS. However, a critical
limitation of model-wise resource allocation is that the way resources are
allocated does not consider how well it is actually utilized, leading to
significant waste in resources. This work identifies two key
reasons behind the baseline’s sub-optimal resource allocation:</p>
</div>
<div class="ltx_para" id="S1.p3">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Heterogeneous resource demands of sparse and dense layers in
RecSys.</span> Modern RecSys combines sparse embedding layers with dense DNN
layers, each layer exhibiting notable differences in their compute and memory
characteristics. Consequently, the QPS of a dense DNN layer and sparse
embedding layer becomes uneven (i.e., one layer type typically shows much
lower QPS than the othey layer type), rendering the low-performance layer
to bottleneck the overall QPS
(Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.SS1" title="III-A Heterogeneous Resource Demands of RecSys ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>). To maximize end-to-end
model-wise throughput, an optimal resource allocation would have
<em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.2">more compute resources provisioned just to the bottlenecked layer</em>.
Unfortunately, the baseline model-wise resource allocation treats the
entire RecSys model as one monolithic unit for allocating resources. As
such, it is challenging to selectively provision more resources to only a
subset of model layer(s), failing to satisfy the unique resource
demands of each layer independently.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Sparse embedding table accesses and its skewed access distribution.</span> The embedding tables employed in sparse embedding layers are memory capacity limited, amounting to several tens of GBs. Access patterns to these tables generally exhibit a power-law
distribution where a small subset of (hot) table entries receive very
high access frequency while the remaining (cold) entries receive
only a small number of accesses. Because the baseline mechanism allocates resources in a
coarse-grained, model-wise fashion,
<em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.2">each server replica must allocate the entire embedding tables in memory without accounting for the individual embedding’s actual “utility”</em>. As such, the baseline model-wise resource allocation suffers from significant memory waste and limits the total number of server replicas that can be instantiated across the datacenter fleet, deteriorating fleet-wide QPS.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To this end, we propose ElasticRec, a RecSys model serving architecture
providing resource elasticity and high memory efficiency. The unique
aspects of ElasticRec are twofold.
First, our proposed system employs a
<em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">microservice-based</em> inference server for high resource
elasticity. Second, ElasticRec achieves high memory efficiency via our
<em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">utility-based</em> resource allocation policy.</p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I2.i1.p1.1.1">Microservice software architecture for RecSys.</span> ElasticRec employs the microservice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib44" title="">44</a>]</cite> programming model as the core mechanism to enable fine-grained resource allocation that meets the heterogeneous resource demands of RecSys. The benefit of microservices is that it helps partition a large monolithic application (in our case the model-wise replication of a RecSys inference server) into many fine-grained and loosely-coupled services. ElasticRec partitions a target RecSys model into fine-grained <em class="ltx_emph ltx_font_italic" id="S1.I2.i1.p1.1.2">model shards</em>, each of which is implemented as a microservice and containerized for deployment. These model shards are utilized as the unit of resource allocation which allows the container orchestration system, Kubernetes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib32" title="">32</a>]</cite>,
to independently scale the number of shard replicas, providing high resource elasticity.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I2.i2.p1.1.1">Utility-based resource allocation policy.</span> Our ElasticRec partitions the baseline monolithic RecSys model architecture into two distinct model shard types, a shard that handles dense DNN layers and sparse embedding layers. The embedding layer shard is further partitioned into hot and cold embedding shards based on the utility of embedding table entries.
Such design decision opens up unique opportunities to properly align its resource allocation with its actual utility. First, ElasticRec can now selectively scale-<em class="ltx_emph ltx_font_italic" id="S1.I2.i2.p1.1.2">out</em> the number of replicas for the hot embedding shards that matches its high memory access demands. On the other hand, it also prevents ElasticRec from needlessly over-provisioning shard replicas servicing cold embeddings thereby minimizing resource waste. ElasticRec exploits these properties to design a
utility-based resource allocation policy that identifies the
optimal embedding table partitioning algorithm, which Kubernetes’ autoscaling policy utilizes to replicate the appropriate number of shards to fulfill a target QPS goal.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.3">Overall, ElasticRec presents a model serving architecture for RecSys
that helps customize its resource allocation best suited for its resource
demand. We demonstrate that ElasticRec provides an average <math alttext="3.3\times" class="ltx_math_unparsed" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1b"><mn id="S1.p6.1.m1.1.1">3.3</mn><mo id="S1.p6.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">3.3\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">3.3 ×</annotation></semantics></math> reduction
in memory allocation size and <math alttext="8.1\times" class="ltx_math_unparsed" display="inline" id="S1.p6.2.m2.1"><semantics id="S1.p6.2.m2.1a"><mrow id="S1.p6.2.m2.1b"><mn id="S1.p6.2.m2.1.1">8.1</mn><mo id="S1.p6.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">8.1\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.2.m2.1d">8.1 ×</annotation></semantics></math> increase in memory utility, which
reduces deployment cost by an average <math alttext="1.6\times" class="ltx_math_unparsed" display="inline" id="S1.p6.3.m3.1"><semantics id="S1.p6.3.m3.1a"><mrow id="S1.p6.3.m3.1b"><mn id="S1.p6.3.m3.1.1">1.6</mn><mo id="S1.p6.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">1.6\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.3.m3.1d">1.6 ×</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">DNN-based Recommendation Models</span>
</h3>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="120" id="S2.F1.g1" src="x1.png" width="340"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
A modern DNN-based RecSys model architecture.
</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Recent RecSys combines both sparse and dense features to enhance model accuracy and utilize two major components, a dense DNN layer using multi-layer perceptrons (MLP) and a sparse embedding layer (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.F1" title="Figure 1 ‣ II-A DNN-based Recommendation Models ‣ II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>).
A sparse feature represents a categorical input (e.g., Ads a user has clicked in the past) and a dense feature represents a continuous input (e.g., user’s age). Sparse features cannot be used directly as inputs to a dense DNN layer because they represent categorical information. As such, RecSys models use <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">embedding tables</em>
to translate a given sparse feature value into a dense embedding vector. An embedding table is an array of
embedding vectors
and
a sparse feature input (which is an index ID to the embedding table) is used to read out a particular embedding vector from
this table.
Because the number of unique items that fall under a sparse feature category can amount to several millions to billions (e.g., number of product items sold in Amazon), an embedding table can be sized at several tens of GBs. In general, multiple embeddings are <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">gathered</em> from a given embedding table which are subsequently <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.3">pooled</em> into a single embedding vector using reduction operations like element-wise additions.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">There are two distinguishing aspects of embedding layers vs. dense DNN layers.
First, the compute intensity of embedding gather and pooling operations are
extremely low, exhibiting <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">memory bandwidth limited</em> behavior, especially
for embedding layers with large pooling values (number of embeddings to gather
from a table). Second, because a modern RecSys model employs
multiple embedding tables, deploying a RecSys model causes <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.2">high memory
capacity overheads</em>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Model Serving Architectures</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">ML inference server design.</span>
Current ML inference servers utilize
containers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib10" title="">10</a>]</cite> for their deployment because of its portability and scalability (e.g., TensorFlow Serving, TorchServe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib51" title="">51</a>]</cite>).
A containerized ML inference server is packaged as a Docker image which contains all the essential ML software packages, the necessary system environment settings, and importantly the <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.2">ML model</em> to deploy.
Because a ML model is treated as one <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.3">monolithic</em> application to be serviced and containers are the smallest unit of resource allocation and deployment, any given replica of the containerized inference server must contain a copy of the <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.4">entire</em> ML model parameters (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.F2" title="Figure 2 ‣ II-B Model Serving Architectures ‣ II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>(a)).
This paper refers to such baseline resource allocation mechanism as <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.5">model-wise</em> resource allocation.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="124" id="S2.F2.sf1.g1" src="x2.png" width="191"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S2.F2.sf2.g1" src="x3.png" width="191"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
(a) A containerized ML inference server using model-wise resource allocation, and (b) using Kubernetes to scale out multiple server replicas across the datacenter to meet a target QPS goal.
</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Deploying inference servers at scale.</span> The de facto standard in container
orchestration is Kubernetes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib32" title="">32</a>]</cite> which helps automate the
deployment, scaling, and resource management of the containerized inference
servers at scale (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.F2" title="Figure 2 ‣ II-B Model Serving Architectures ‣ II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>(b)). Kubernetes cluster
scheduler enables system designers to hand over the responsibility of resource
management by defining deployment policies. When deploying an application,
programmers can simply set the desired policy, and Kubernetes
transparently manages the allocation of resources based on the specified
policy. One important, automated resource management feature provided
with Kubernetes is the Horizontal Pod Autoscaling (HPA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib31" title="">31</a>]</cite>. HPA
automatically adjusts the number of container replicas of inference
servers to satisfy a target service throughput metric (e.g., QPS) and
guarantee high-quality service experience to the end users.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">When a new inference server is instantiated by Kubernetes, it provisions all the resources needed for that container.
As such, an initialized inference server uploads all of its ML model parameters in memory. Such model-wise allocation of resources enables each replica of the inference server to independently service user queries.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">System architectures for RecSys inference server.</span>
As noted in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.SS1" title="II-A DNN-based Recommendation Models ‣ II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>, the size of a single embedding table can be up to several tens of GBs. Therefore, <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.2">each inference server must be provisioned with a large enough memory to store these embedding tables</em>. Because high-bandwidth memory employed in GPUs are not large enough to store the entire embedding tables,
modern RecSys inference servers employ
CPU-only <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib23" title="">23</a>]</cite>
or hybrid CPU-GPU
systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib56" title="">56</a>]</cite>.
Both CPU-only and CPU-GPU systems share a common property
where the <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.3">memory-hungry embedding tables are stored in
capacity-optimized CPU memory</em>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Therefore, unlike compute-intensive DNN layers which get executed by the GPU in
a CPU-GPU system (i.e., CPU-only executes DNNs using the CPU), the
embedding layers are executed by the CPU in both CPU-only and CPU-GPU.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">Hence, effectively utilizing CPU memory with
maximum efficiency becomes vital to optimize cost. This is because the total
CPU memory size determines <em class="ltx_emph ltx_font_italic" id="S2.SS2.p6.1.1">how many</em> RecSys inference
servers can be deployed across the datacenter, which heavily impacts the
fleet-wide QPS.
In this work, we study the merits of ElasticRec’s resource allocation
by using <em class="ltx_emph ltx_font_italic" id="S2.SS2.p6.1.2">both</em> CPU-only
and CPU-GPU based RecSys inference servers, demonstrating its wide
applicability.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Microservices</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Microservices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib44" title="">44</a>]</cite> break apart complex <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">monolithic</em>
applications, whose functionality is implemented as a single service, into many
<em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.2">fine-grained</em> and <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.3">loosely-coupled</em> microservices. Each microservice
is designed to serve a small subset of the original application’s
functionality, communicating with other microservices using Remote Procedure
Calls (RPC) or a RESTful
API <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib9" title="">9</a>]</cite>.
A key advantage of microservices
is its elasticity. Specifically, because the granularity in which resource
allocation and scheduling are done is in individual microservices, it
facilitates deploying, scaling, and updating individual microservices
independently, improving the elasticity of resource allocation and its
scheduling.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Related Work</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">With the growing interest in
RecSys, there has been a large body of prior work exploring hardware/software
optimizations for RecSys which we summarize below.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.p2.1.1">Memory bandwidth bottleneck of RecSys.</span> As discussed in
Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.SS1" title="II-A DNN-based Recommendation Models ‣ II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>, the embedding vector gathers and pooling operations in
RecSys incur significant memory bandwidth demands causing a bottleneck.
Several prior work proposed near-/in-memory
processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib30" title="">30</a>]</cite>,
in-storage processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib55" title="">55</a>]</cite> to
alleviate embedding layer’s memory bandwidth demands.
Similar to our work, there are also studies that observes and utilizes the skewed embedding
table access pattern in RecSys for system-level optimizations.
For instance, several prior studies utilize the skewed embedding access
patterns to explore the efficacy of caching
to reduce overall memory bandwidth demands <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib41" title="">41</a>]</cite>.
Others leverage a heterogeneous memory
hierarchy <span class="ltx_text" id="S2.SS4.p2.1.2"> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib53" title="">53</a>]</cite></span> to
effectively lower the average latency to access slow memory. Overall, these prior
work alleviates the embedding layer’s memory bandwidth requirements by
exploiting the skewed access patterns. ElasticRec, on the other hand, utilizes
embedding table’s unique access pattern to develop a cost-efficient, elastic resource management
system.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.p3.1.1">Memory capacity bottleneck of RecSys.</span> Kwon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib35" title="">35</a>]</cite> proposed a disaggregated
memory architecture to store large embedding tables in a remote memory node and
Gouk et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib15" title="">15</a>]</cite> explored the viability of CXL-based memory
pooling for storing embedding tables. Lui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib39" title="">39</a>]</cite> explores the efficacy of distributed inference for RecSys where a model is partitioned
and distributed
across multiple machines for deployment. Such design point helps address
RecSys embedding layer’s memory capacity demands by storing
different embedding tables in remote CPU nodes, collecting the pooled embedding
vectors using RPC calls, which is similar to ElasticRec’s microservice based
design. Mudigere et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib42" title="">42</a>]</cite> explores
various model-parallel training schemes targeting embedding tables, discussing
different table partitioning plans (e.g., column-wise, row-wise, table-wise
partitioning) to address the memory capacity demands of RecSys training.
All of these prior work strictly focus on evaluating the efficacy of their
solution under a <em class="ltx_emph ltx_font_italic" id="S2.SS4.p3.1.2">single</em> inference and/or training server setting without
consideration of its deployment at scale nor its resource allocation
efficiencies. More importantly, the RecSys model architecture is implemented as
one monolithic application, unlike ElasticRec where we focus on partitioning its
implementation into fine-grained model shards using a microservice architecture to enable elastic resource scaling.
DisaggRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib29" title="">29</a>]</cite> proposes a disaggregated memory
 system augmented with near-memory processing architectures for RecSys inference,
which is designed to cost-effectively manage
large embedding tables. DisaggRec addresses resource
inefficiencies that arise from the varying computational and memory
requirements of dense and sparse layers.
Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib39" title="">39</a>]</cite>, DisaggRec
employs a distributed inference approach for RecSys and aims to tackle the resource underutilization
issue by distributing the allocation of dense layers and sparse embedding layers across compute-nodes and memory-nodes, respectively,
which helps improve machine utilization.
Unlike ElasticRec’s dynamic, elastic resource management system, however, DisaggRec
allocates a fixed amount of resources to each layer, and determining
the optimal distribution of resources requires an exhaustive search. Overall, the key contribution of 
ElasticRec is orthogonal to these related work.</p>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p" id="S2.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.p4.1.1">Runtime system and scheduling for RecSys.</span> While not necessarily employing microservices, there are several prior work suggesting RecSys optimized runtime systems or scheduling policies.
Prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib20" title="">20</a>]</cite> suggests multi-tenant scheduling to improve the throughput of RecSys inference servers. JiZhi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib38" title="">38</a>]</cite> optimizes the serving cost of RecSys by batching the model serving pipeline. MP-Rec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib22" title="">22</a>]</cite> dynamically selects the optimal hardware platform within a heterogeneous RecSys inference server containing GPUs, TPUs, and IPUs, for better performance. In general, the key contributions of ElasticRec is orthogonal to these prior work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Motivation</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">A critical limitation of model-wise allocation is that it is
difficult to flexibly allocate the appropriate amount of resources to
individual layers that match their utility and need, leading to
resource waste. This section describes the two key factors behind baseline
mechanism’s sub-optimal performance.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="90" id="S3.F3.sf1.g1" src="x4.png" width="224"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="88" id="S3.F3.sf2.g1" src="x5.png" width="153"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
The fraction of (a) FLOPs, memory consumption and (b) end-to-end inference latency (over CPU-only and CPU-GPU systems) the sparse embedding and dense DNN layers account for when evaluated over the three models studied in this paper (RM1, RM2, and RM3). FLOPs and memory consumption are architecture-independent, so its values are identical over CPU-only and CPU-GPU systems. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5" title="V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">V</span></a> details our methodology.
FLOPS percentage of sparse embedding layers in (a) are 2%, 1%, and 0.1% for
RM1, RM2, and RM3, respectively. Memory consumption percentage of dense
DNN layers in (a) are 0.02%, 0.02%, and 0.4% for RM1, RM2, and RM3, respectively.
</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Heterogeneous Resource Demands of RecSys</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.7">The sparse embedding layer and dense DNN layer exhibit notable differences in
their compute and memory characteristics, including compute intensity (FLOPs),
memory footprint, and memory access pattern. Compared to large
embedding tables, MLP’s model size is only in the range of several
MBs, yet their compute intensity is much higher than embedding gather
and pooling operations. For instance, in case of RM1
(Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T2" title="TABLE II ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a>), dense DNNs account for <math alttext="98\%" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">98</mn><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">percent</csymbol><cn id="S3.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.2">98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">98\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">98 %</annotation></semantics></math> of FLOPs and
<math alttext="67\%" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">67</mn><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1">percent</csymbol><cn id="S3.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1.2">67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">67\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">67 %</annotation></semantics></math>/<math alttext="19\%" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">19</mn><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1">percent</csymbol><cn id="S3.SS1.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.2">19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">19\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">19 %</annotation></semantics></math> of CPU-only/CPU-GPU’s end-to-end inference time, yet their model size only
accounts for <math alttext="0.02\%" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mn id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">0.02</mn><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1">percent</csymbol><cn id="S3.SS1.p1.4.m4.1.1.2.cmml" type="float" xref="S3.SS1.p1.4.m4.1.1.2">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">0.02\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">0.02 %</annotation></semantics></math> of overall memory consumption
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F3" title="Figure 3 ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">3</span></a>). From a memory access pattern’s
perspective, servicing a single query requires the entire MLP parameters
to be accessed, exhibiting <math alttext="100\%" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mn id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">100</mn><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="latexml" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1">percent</csymbol><cn id="S3.SS1.p1.5.m5.1.1.2.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">100 %</annotation></semantics></math> utility of the model parameters
allocated in memory. In contrast, embedding layers, due to its sparse
table access patterns, exhibit extremely low memory utility as it only
touches <math alttext="0.001\%" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mn id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">0.001</mn><mo id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="latexml" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1">percent</csymbol><cn id="S3.SS1.p1.6.m6.1.1.2.cmml" type="float" xref="S3.SS1.p1.6.m6.1.1.2">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">0.001\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">0.001 %</annotation></semantics></math> (with a pooling factor of 100 per table) of the
embedding tables per inference. This means that, on average, <math alttext="99.999\%" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mrow id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mn id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">99.999</mn><mo id="S3.SS1.p1.7.m7.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="latexml" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1">percent</csymbol><cn id="S3.SS1.p1.7.m7.1.1.2.cmml" type="float" xref="S3.SS1.p1.7.m7.1.1.2">99.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">99.999\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">99.999 %</annotation></semantics></math>
of the parameters allocated in memory are of waste whenever a query
is serviced.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="76" id="S3.F4.sf1.g1" src="x6.png" width="166"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="76" id="S3.F4.sf2.g1" src="x7.png" width="166"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
An example RecSys where the dense DNN layer exhibits half the QPS than the sparse embedding layer. (a) How the baseline model-wise resource allocation would replicate two servers to reach <math alttext="100" class="ltx_Math" display="inline" id="S3.F4.2.m1.1"><semantics id="S3.F4.2.m1.1b"><mn id="S3.F4.2.m1.1.1" xref="S3.F4.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.F4.2.m1.1c"><cn id="S3.F4.2.m1.1.1.cmml" type="integer" xref="S3.F4.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.m1.1d">100</annotation><annotation encoding="application/x-llamapun" id="S3.F4.2.m1.1e">100</annotation></semantics></math> queries/sec and (b) how our proposed ElasticRec would reach such QPS goal using fine-grained, per-layer resource allocation.
</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="89" id="S3.F5.sf1.g1" src="x8.png" width="183"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>CPU-only</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="90" id="S3.F5.sf2.g1" src="x9.png" width="183"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>CPU-GPU</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Service throughput (QPS) of dense DNN and sparse embedding layers over (a) CPU-only
and (b) CPU-GPU system when
separately measured over the three RecSys models used in our evaluation
(see Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T2" title="TABLE II ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a>). As shown, due to the heterogeneous resource demands of RecSys, a significant
QPS mismatch exists between sparse and dense layers, for both CPU-only and CPU-GPU system.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.5">The mismatch in compute intensity and memory utilization, leading to
significant resource waste, cannot be addressed under the
baseline model-wise resource allocation. Consider the example in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F4" title="Figure 4 ‣ III-A Heterogeneous Resource Demands of RecSys ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">4</span></a> which assumes
that a dense DNN and sparse embedding layer can each service <math alttext="50" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn id="S3.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">50</annotation></semantics></math> queries/sec
and <math alttext="100" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn id="S3.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">100</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">100</annotation></semantics></math> queries/sec, respectively. Such mismatch in QPS is common in RecSys
due to its heterogeneous model architecture (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F5" title="Figure 5 ‣ III-A Heterogeneous Resource Demands of RecSys ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">5</span></a>).
As the
DNN layer exhibits half the QPS of the embedding layer in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F4" title="Figure 4 ‣ III-A Heterogeneous Resource Demands of RecSys ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">4</span></a>, the
end-to-end model-wise throughput will be bounded at <math alttext="50" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><cn id="S3.SS1.p2.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">50</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">50</annotation></semantics></math> queries/sec. To
increase the system-wide throughput to <math alttext="100" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><cn id="S3.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S3.SS1.p2.4.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">100</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">100</annotation></semantics></math> queries/sec, the baseline
model-wise allocation would require two replicas of the inference server to be
instantiated. From a memory efficiency perspective, such model-wise replication
is a significant waste as the entire embedding tables are needlessly
duplicated without contributing much to improving QPS
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F4" title="Figure 4 ‣ III-A Heterogeneous Resource Demands of RecSys ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">4</span></a>(a)). A more
desirable solution would be to double the allocated resources <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.5.1">only</em> to
the dense DNN and improve its aggregate QPS to <math alttext="100" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><cn id="S3.SS1.p2.5.m5.1.1.cmml" type="integer" xref="S3.SS1.p2.5.m5.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">100</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">100</annotation></semantics></math> queries/sec and
resolve it from being a bottleneck
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F4" title="Figure 4 ‣ III-A Heterogeneous Resource Demands of RecSys ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">4</span></a>(b)).
Unfortunately, fine-tuning the resource allocation separately on a per-layer
basis is impossible with the baseline mechanism as each inference server is
containerized as one monolithic application, forcing Kubernetes to replicate
the <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.5.2">entire</em> model parameters whenever a higher system-wide QPS is desired
and a new server replica is deployed.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Skewed Access Pattern and Locality in Embeddings</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F6" title="Figure 6 ‣ III-B Skewed Access Pattern and Locality in Embeddings ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the access distribution of individual embedding table entries in real world RecSys datasets. As depicted, the
table access pattern exhibits a power-law distribution where the majority
of table accesses are covered by a very small subset of the table entries (e.g., <math alttext="94\%" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">94</mn><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1">percent</csymbol><cn id="S3.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.2">94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">94\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">94 %</annotation></semantics></math> of accesses covered by only <math alttext="10\%" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">10</mn><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1">percent</csymbol><cn id="S3.SS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">10 %</annotation></semantics></math> of the table entries in MovieLens).</p>
</div>
<figure class="ltx_figure" id="S3.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="104" id="S3.F6.sf1.g1" src="x10.png" width="134"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="103" id="S3.F6.sf2.g1" src="x11.png" width="133"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="103" id="S3.F6.sf3.g1" src="x12.png" width="133"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Sorted access frequency of embedding vectors in real world RecSys datasets: (a) Amazon books <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib6" title="">6</a>]</cite>, (b) Criteo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib8" title="">8</a>]</cite>, and (c) MovieLens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib16" title="">16</a>]</cite>. The y-axis is plotted on a log-scale.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Given such property, we can infer that an embedding layer’s throughput is primarily governed by how much performance can be reaped out for embedding vector gather operations targeting those embeddings that are accessed frequently, i.e., the “hot” embeddings. To put it differently, in order to enhance the effective throughput of an embedding layer, it is more advantageous to selectively allocate more resources to embedding gathers targeting hot embeddings rather than cold embeddings. Unfortunately, as in our example in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F4" title="Figure 4 ‣ III-A Heterogeneous Resource Demands of RecSys ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">4</span></a>(a), the baseline mechanism allocates resources in a coarse-grained, model-wise manner, having
the entire embedding tables be replicated in memory without consideration of its actual utility.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Overall, we conclude that the baseline model-wise resource allocation does not align well with the unique properties of RecSys model serving. This misalignment leads to significant waste in memory resources, which is particularly detrimental to the memory-capacity limited embedding tables.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">ElasticRec Model Serving Architecture</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Microservice-based Inference Server Design</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Server architecture overview.</span> Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F7" title="Figure 7 ‣ IV-A Microservice-based Inference Server Design ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">7</span></a> provides an
overview of ElasticRec’s model serving architecture. ElasticRec employs a <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.2">microservice</em> programming model to break down the monolithic
RecSys model serving architecture into different <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.3">model shards</em>, each of
which is implemented as a microservice. There are two types of model
shards, a dense DNN shard and a sparse embedding shard. The dense DNN shard
services all the computations related to the bottom/top MLP and feature
interactions (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.F1" title="Figure 1 ‣ II-A DNN-based Recommendation Models ‣ II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>). On the other hand, the sparse embedding
shard is responsible for gathering the requested embedding vectors stored
within that shard. An embedding table is partitioned into various sized
embedding shards based on the hotness of embeddings. The table partitioning is
done by our <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.4">dynamic programming based partitioning algorithm</em> (detailed
in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS2" title="IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>) which determines the optimal partitioning
plan that maximizes resource efficiency.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In ElasticRec, each model shard is containerized as a Docker image. For CPU-only
systems, all model shards (both dense and sparse) are CPU-centric so they are
designed as containers only requiring CPU resources. As for CPU-GPU systems,
the containers that service sparse embedding shards are similarly
designed with only CPU resource requirements. Because of dense DNN
layer’s high compute intensity and small memory footprint
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S3.F3" title="Figure 3 ‣ III Motivation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">3</span></a>), CPU-GPU systems service
dense DNN shards using a GPU-centric container utilizing both CPU/GPU
resources. In our design, model shard instances communicate with each
other using the gRPC protocol <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S4.F7.g1" src="x13.png" width="340"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
High-level overview of our ElasticRec server architecture. The example assumes that ElasticRec partitions the RecSys model into one dense DNN shard and three embedding shard types, each of which is containerized for deployment. To sustain a target QPS, the example assumes that Kubernetes instantiated <math alttext="5" class="ltx_Math" display="inline" id="S4.F7.5.m1.1"><semantics id="S4.F7.5.m1.1b"><mn id="S4.F7.5.m1.1.1" xref="S4.F7.5.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.F7.5.m1.1c"><cn id="S4.F7.5.m1.1.1.cmml" type="integer" xref="S4.F7.5.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.5.m1.1d">5</annotation><annotation encoding="application/x-llamapun" id="S4.F7.5.m1.1e">5</annotation></semantics></math>/<math alttext="4" class="ltx_Math" display="inline" id="S4.F7.6.m2.1"><semantics id="S4.F7.6.m2.1b"><mn id="S4.F7.6.m2.1.1" xref="S4.F7.6.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.F7.6.m2.1c"><cn id="S4.F7.6.m2.1.1.cmml" type="integer" xref="S4.F7.6.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.6.m2.1d">4</annotation><annotation encoding="application/x-llamapun" id="S4.F7.6.m2.1e">4</annotation></semantics></math>/<math alttext="1" class="ltx_Math" display="inline" id="S4.F7.7.m3.1"><semantics id="S4.F7.7.m3.1b"><mn id="S4.F7.7.m3.1.1" xref="S4.F7.7.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.F7.7.m3.1c"><cn id="S4.F7.7.m3.1.1.cmml" type="integer" xref="S4.F7.7.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.7.m3.1d">1</annotation><annotation encoding="application/x-llamapun" id="S4.F7.7.m3.1e">1</annotation></semantics></math>/<math alttext="2" class="ltx_Math" display="inline" id="S4.F7.8.m4.1"><semantics id="S4.F7.8.m4.1b"><mn id="S4.F7.8.m4.1.1" xref="S4.F7.8.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.F7.8.m4.1c"><cn id="S4.F7.8.m4.1.1.cmml" type="integer" xref="S4.F7.8.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.8.m4.1d">2</annotation><annotation encoding="application/x-llamapun" id="S4.F7.8.m4.1e">2</annotation></semantics></math> replicas of the dense DNN and embedding shard A, B, and C, respectively.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Life of an inference query.</span> When a user query arrives to the inference
server, the input data is routed to the dense DNN shard, which splits it into
two parts: the sparse input and the dense input. The dense DNN shard then
processes the bottom MLP layers using the dense input while concurrently
initiating RPC calls to the sparse embedding shards to collect the required
embeddings. Sending embedding gather requests across the sparse embedding
shards requires a <em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.2">bucketization</em> process that determines which among the
partitioned embedding shards the input query should gather embeddings from
(Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS3" title="IV-C Bucketization ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a> details the bucketization algorithm). The
embedding shards, each storing the partitioned embedding table, gather the
embeddings requested by the dense DNN shard. Once all embeddings are
gathered and pooled, the sparse embedding shards send them back to the caller
microservice, i.e., the dense DNN shard. Upon receiving the pooled embeddings
from the sparse embedding shards, the dense shard goes through the remaining
inference process including feature interaction, top MLP, and
finally calculating the event probability which is returned back to the user.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Scaling out inference servers using Kubernetes.</span> In ElasticRec,
the containers that service dense and sparse model shards
become the unit of resource allocation and scheduling by our
container orchestration system, Kubernetes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib32" title="">32</a>]</cite>.
This enables ElasticRec to
independently scale the number of model shard replicas to satisfy a
target QPS goal, whether it be a CPU-centric shard or a GPU-centric shard, achieving high resource elasticity. Kubernetes
horizontal pod autoscaling (HPA) policy defines when to scale
up/down the number of shard replicas under what condition. In
Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS4" title="IV-D Deploying ElasticRec at Scale using Kubernetes ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>, we detail how ElasticRec utilizes
such feature to adaptively adjust the shard replica numbers
according to the incoming query traffic.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Utility-based Resource Allocation for Embeddings</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The key objective of ElasticRec’s embedding table partitioning algorithm is to determine (1) the optimal number of embedding shards to partition the table and (2) how many embeddings to include within each embedding shard, which minimizes its deployment cost while sustaining target QPS goals.
ElasticRec proposes a <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.1">dynamic programming</em> (DP) based table partitioning algorithm which is based on our <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.2">profiling</em>-based deployment cost (i.e., memory consumption) estimation model.
We discuss each of these components below.</p>
</div>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="86" id="S4.F8.sf1.g1" src="x14.png" width="124"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="86" id="S4.F8.sf2.g1" src="x15.png" width="127"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
Partitioning (a) an example embedding table as-is without any preprocessing, and (b) sorting the table first based on the hotness of embeddings and <em class="ltx_emph ltx_font_italic" id="S4.F8.2.1">then</em> partitioning the table into two different shards, hot (red) vs. cold (blue) embedding shards. When it comes to partitioning an embedding table, this paper always assumes that an embedding shard includes a non-overlapping set of embeddings with consecutive index IDs.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Embedding table preprocessing.</span>
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F8" title="Figure 8 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">8</span></a>(a), hot embeddings are randomly dispersed across the embedding table, so partitioning the table as-is into multiple shards where each shard consists of a non-overlapping set of consecutive embeddings inevitably mixes up hot and cold embeddings altogether. Such partitioning plan reduces the effectiveness of ElasticRec’s fine-grained resource allocation and scaling policy as cold embeddings will needlessly be duplicated whenever a new embedding shard is replicated.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">To this end, ElasticRec first preprocesses the embedding table by sorting each
embedding’s location within the table based on its access frequency. The access
frequency of an embedding can be determined by keeping a history of each
embedding’s access count within a given time period, one that can easily be
implemented in production inference servers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib37" title="">37</a>]</cite>. As shown in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F8" title="Figure 8 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">8</span></a>(b), once the table is sorted, the hottest
embedding vector will be stored at the leftmost location indexed with an ID=<math alttext="1" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn id="S4.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">1</annotation></semantics></math>
and the coldest embedding located at the rightmost location indexed with
ID=(number of embedding vectors in table). Using the sorted embedding table, we
can now create a model shard that only includes embeddings much hotter than the
other ones, which functions as a vehicle for designing our utility-based
resource allocation policy, i.e., the ability to replicate a larger number of
model shards <em class="ltx_emph ltx_font_italic" id="S4.SS2.p3.1.1">only</em> for the hot embeddings without duplicating cold
embeddings. Note that sorting the embedding table incurs a one-time latency
overhead (approximately three seconds for the largest table we evaluate) and
more importantly, such preprocessing step is off the critical path of serving
online inference queries.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Deployment Cost Estimation Algorithm</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">1:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l1.2" style="font-size:80%;">function</span><span class="ltx_text" id="alg1.l1.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_smallcaps" id="alg1.l1.4" style="font-size:80%;">Cost</span><span class="ltx_text" id="alg1.l1.5" style="font-size:80%;">(k, j)
</span>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">2:</span></span><span class="ltx_text" id="alg1.l2.2" style="font-size:80%;">    num_replicas = REPLICAS(k, j)
</span>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">3:</span></span><span class="ltx_text" id="alg1.l3.2" style="font-size:80%;">    shard_size = CAPACITY(k, j) + min_mem_alloc
</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">4:</span></span><span class="ltx_text" id="alg1.l4.2" style="font-size:80%;">    memory_consumption = num_replicas </span><math alttext="\times" class="ltx_Math" display="inline" id="alg1.l4.m1.1"><semantics id="alg1.l4.m1.1a"><mo id="alg1.l4.m1.1.1" mathsize="80%" xref="alg1.l4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><times id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="alg1.l4.3" style="font-size:80%;"> shard_size
</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">5:</span></span><span class="ltx_text" id="alg1.l5.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_bold" id="alg1.l5.3" style="font-size:80%;">return</span><span class="ltx_text" id="alg1.l5.4" style="font-size:80%;"> memory_consumption
</span>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">6:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l6.2" style="font-size:80%;">end</span><span class="ltx_text" id="alg1.l6.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg1.l6.4" style="font-size:80%;">function</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l7.1.1.1" style="font-size:80%;">7:</span></span>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l8.1.1.1" style="font-size:80%;">8:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l8.2" style="font-size:80%;">function</span><span class="ltx_text" id="alg1.l8.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_smallcaps" id="alg1.l8.4" style="font-size:80%;">replicas</span><span class="ltx_text" id="alg1.l8.5" style="font-size:80%;">(k, j)
</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l9.1.1.1" style="font-size:80%;">9:</span></span><span class="ltx_text" id="alg1.l9.2" style="font-size:80%;">     #  </span><math alttext="n_{t}" class="ltx_Math" display="inline" id="alg1.l9.m1.1"><semantics id="alg1.l9.m1.1a"><msub id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><mi id="alg1.l9.m1.1.1.2" mathsize="80%" xref="alg1.l9.m1.1.1.2.cmml">n</mi><mi id="alg1.l9.m1.1.1.3" mathsize="80%" xref="alg1.l9.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><csymbol cd="ambiguous" id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1">subscript</csymbol><ci id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2">𝑛</ci><ci id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">n_{t}</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m1.1d">italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg1.l9.3" style="font-size:80%;">: average number of vectors to gather from the table
</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l10.1.1.1" style="font-size:80%;">10:</span></span><span class="ltx_text" id="alg1.l10.2" style="font-size:80%;">     #  </span><math alttext="target\_traffic" class="ltx_Math" display="inline" id="alg1.l10.m1.1"><semantics id="alg1.l10.m1.1a"><mrow id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml"><mi id="alg1.l10.m1.1.1.2" mathsize="80%" xref="alg1.l10.m1.1.1.2.cmml">t</mi><mo id="alg1.l10.m1.1.1.1" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.3" mathsize="80%" xref="alg1.l10.m1.1.1.3.cmml">a</mi><mo id="alg1.l10.m1.1.1.1a" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.4" mathsize="80%" xref="alg1.l10.m1.1.1.4.cmml">r</mi><mo id="alg1.l10.m1.1.1.1b" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.5" mathsize="80%" xref="alg1.l10.m1.1.1.5.cmml">g</mi><mo id="alg1.l10.m1.1.1.1c" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.6" mathsize="80%" xref="alg1.l10.m1.1.1.6.cmml">e</mi><mo id="alg1.l10.m1.1.1.1d" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.7" mathsize="80%" xref="alg1.l10.m1.1.1.7.cmml">t</mi><mo id="alg1.l10.m1.1.1.1e" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.8" mathsize="80%" mathvariant="normal" xref="alg1.l10.m1.1.1.8.cmml">_</mi><mo id="alg1.l10.m1.1.1.1f" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.9" mathsize="80%" xref="alg1.l10.m1.1.1.9.cmml">t</mi><mo id="alg1.l10.m1.1.1.1g" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.10" mathsize="80%" xref="alg1.l10.m1.1.1.10.cmml">r</mi><mo id="alg1.l10.m1.1.1.1h" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.11" mathsize="80%" xref="alg1.l10.m1.1.1.11.cmml">a</mi><mo id="alg1.l10.m1.1.1.1i" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.12" mathsize="80%" xref="alg1.l10.m1.1.1.12.cmml">f</mi><mo id="alg1.l10.m1.1.1.1j" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.13" mathsize="80%" xref="alg1.l10.m1.1.1.13.cmml">f</mi><mo id="alg1.l10.m1.1.1.1k" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.14" mathsize="80%" xref="alg1.l10.m1.1.1.14.cmml">i</mi><mo id="alg1.l10.m1.1.1.1l" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.15" mathsize="80%" xref="alg1.l10.m1.1.1.15.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1"><times id="alg1.l10.m1.1.1.1.cmml" xref="alg1.l10.m1.1.1.1"></times><ci id="alg1.l10.m1.1.1.2.cmml" xref="alg1.l10.m1.1.1.2">𝑡</ci><ci id="alg1.l10.m1.1.1.3.cmml" xref="alg1.l10.m1.1.1.3">𝑎</ci><ci id="alg1.l10.m1.1.1.4.cmml" xref="alg1.l10.m1.1.1.4">𝑟</ci><ci id="alg1.l10.m1.1.1.5.cmml" xref="alg1.l10.m1.1.1.5">𝑔</ci><ci id="alg1.l10.m1.1.1.6.cmml" xref="alg1.l10.m1.1.1.6">𝑒</ci><ci id="alg1.l10.m1.1.1.7.cmml" xref="alg1.l10.m1.1.1.7">𝑡</ci><ci id="alg1.l10.m1.1.1.8.cmml" xref="alg1.l10.m1.1.1.8">_</ci><ci id="alg1.l10.m1.1.1.9.cmml" xref="alg1.l10.m1.1.1.9">𝑡</ci><ci id="alg1.l10.m1.1.1.10.cmml" xref="alg1.l10.m1.1.1.10">𝑟</ci><ci id="alg1.l10.m1.1.1.11.cmml" xref="alg1.l10.m1.1.1.11">𝑎</ci><ci id="alg1.l10.m1.1.1.12.cmml" xref="alg1.l10.m1.1.1.12">𝑓</ci><ci id="alg1.l10.m1.1.1.13.cmml" xref="alg1.l10.m1.1.1.13">𝑓</ci><ci id="alg1.l10.m1.1.1.14.cmml" xref="alg1.l10.m1.1.1.14">𝑖</ci><ci id="alg1.l10.m1.1.1.15.cmml" xref="alg1.l10.m1.1.1.15">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">target\_traffic</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m1.1d">italic_t italic_a italic_r italic_g italic_e italic_t _ italic_t italic_r italic_a italic_f italic_f italic_i italic_c</annotation></semantics></math><span class="ltx_text" id="alg1.l10.3" style="font-size:80%;">: predefined constant representing user_traffic
</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l11.1.1.1" style="font-size:80%;">11:</span></span><span class="ltx_text" id="alg1.l11.2" style="font-size:80%;">     #  </span><math alttext="QPS(x)" class="ltx_Math" display="inline" id="alg1.l11.m1.1"><semantics id="alg1.l11.m1.1a"><mrow id="alg1.l11.m1.1.2" xref="alg1.l11.m1.1.2.cmml"><mi id="alg1.l11.m1.1.2.2" mathsize="80%" xref="alg1.l11.m1.1.2.2.cmml">Q</mi><mo id="alg1.l11.m1.1.2.1" xref="alg1.l11.m1.1.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.2.3" mathsize="80%" xref="alg1.l11.m1.1.2.3.cmml">P</mi><mo id="alg1.l11.m1.1.2.1a" xref="alg1.l11.m1.1.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.2.4" mathsize="80%" xref="alg1.l11.m1.1.2.4.cmml">S</mi><mo id="alg1.l11.m1.1.2.1b" xref="alg1.l11.m1.1.2.1.cmml">⁢</mo><mrow id="alg1.l11.m1.1.2.5.2" xref="alg1.l11.m1.1.2.cmml"><mo id="alg1.l11.m1.1.2.5.2.1" maxsize="80%" minsize="80%" xref="alg1.l11.m1.1.2.cmml">(</mo><mi id="alg1.l11.m1.1.1" mathsize="80%" xref="alg1.l11.m1.1.1.cmml">x</mi><mo id="alg1.l11.m1.1.2.5.2.2" maxsize="80%" minsize="80%" xref="alg1.l11.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.2.cmml" xref="alg1.l11.m1.1.2"><times id="alg1.l11.m1.1.2.1.cmml" xref="alg1.l11.m1.1.2.1"></times><ci id="alg1.l11.m1.1.2.2.cmml" xref="alg1.l11.m1.1.2.2">𝑄</ci><ci id="alg1.l11.m1.1.2.3.cmml" xref="alg1.l11.m1.1.2.3">𝑃</ci><ci id="alg1.l11.m1.1.2.4.cmml" xref="alg1.l11.m1.1.2.4">𝑆</ci><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">QPS(x)</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m1.1d">italic_Q italic_P italic_S ( italic_x )</annotation></semantics></math><span class="ltx_text" id="alg1.l11.3" style="font-size:80%;">: Estimated QPS of a shard that gathers </span><math alttext="x" class="ltx_Math" display="inline" id="alg1.l11.m2.1"><semantics id="alg1.l11.m2.1a"><mi id="alg1.l11.m2.1.1" mathsize="80%" xref="alg1.l11.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="alg1.l11.m2.1b"><ci id="alg1.l11.m2.1.1.cmml" xref="alg1.l11.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m2.1d">italic_x</annotation></semantics></math><span class="ltx_text" id="alg1.l11.4" style="font-size:80%;"> embeddings,
</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l12.1.1.1" style="font-size:80%;">12:</span></span><span class="ltx_text" id="alg1.l12.2" style="font-size:80%;">    which is derived by our profiling-based regression model
</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l13.1.1.1" style="font-size:80%;">13:</span></span><span class="ltx_text" id="alg1.l13.2" style="font-size:80%;">    probability = CDF(j) - CDF(k)
</span>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l14.1.1.1" style="font-size:80%;">14:</span></span><span class="ltx_text" id="alg1.l14.2" style="font-size:80%;">    n</span><sub class="ltx_sub" id="alg1.l14.3"><span class="ltx_text ltx_font_italic" id="alg1.l14.3.1" style="font-size:80%;">s</span></sub><span class="ltx_text" id="alg1.l14.4" style="font-size:80%;"> = probability </span><math alttext="\times" class="ltx_Math" display="inline" id="alg1.l14.m2.1"><semantics id="alg1.l14.m2.1a"><mo id="alg1.l14.m2.1.1" mathsize="80%" xref="alg1.l14.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="alg1.l14.m2.1b"><times id="alg1.l14.m2.1.1.cmml" xref="alg1.l14.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="alg1.l14.m2.1d">×</annotation></semantics></math><span class="ltx_text" id="alg1.l14.5" style="font-size:80%;"> </span><math alttext="n_{t}" class="ltx_Math" display="inline" id="alg1.l14.m3.1"><semantics id="alg1.l14.m3.1a"><msub id="alg1.l14.m3.1.1" xref="alg1.l14.m3.1.1.cmml"><mi id="alg1.l14.m3.1.1.2" mathsize="80%" xref="alg1.l14.m3.1.1.2.cmml">n</mi><mi id="alg1.l14.m3.1.1.3" mathsize="80%" xref="alg1.l14.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l14.m3.1b"><apply id="alg1.l14.m3.1.1.cmml" xref="alg1.l14.m3.1.1"><csymbol cd="ambiguous" id="alg1.l14.m3.1.1.1.cmml" xref="alg1.l14.m3.1.1">subscript</csymbol><ci id="alg1.l14.m3.1.1.2.cmml" xref="alg1.l14.m3.1.1.2">𝑛</ci><ci id="alg1.l14.m3.1.1.3.cmml" xref="alg1.l14.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m3.1c">n_{t}</annotation><annotation encoding="application/x-llamapun" id="alg1.l14.m3.1d">italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg1.l14.6" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l15.1.1.1" style="font-size:80%;">15:</span></span><span class="ltx_text" id="alg1.l15.2" style="font-size:80%;">    #  </span><math alttext="n_{s}" class="ltx_Math" display="inline" id="alg1.l15.m1.1"><semantics id="alg1.l15.m1.1a"><msub id="alg1.l15.m1.1.1" xref="alg1.l15.m1.1.1.cmml"><mi id="alg1.l15.m1.1.1.2" mathsize="80%" xref="alg1.l15.m1.1.1.2.cmml">n</mi><mi id="alg1.l15.m1.1.1.3" mathsize="80%" xref="alg1.l15.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l15.m1.1b"><apply id="alg1.l15.m1.1.1.cmml" xref="alg1.l15.m1.1.1"><csymbol cd="ambiguous" id="alg1.l15.m1.1.1.1.cmml" xref="alg1.l15.m1.1.1">subscript</csymbol><ci id="alg1.l15.m1.1.1.2.cmml" xref="alg1.l15.m1.1.1.2">𝑛</ci><ci id="alg1.l15.m1.1.1.3.cmml" xref="alg1.l15.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l15.m1.1c">n_{s}</annotation><annotation encoding="application/x-llamapun" id="alg1.l15.m1.1d">italic_n start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg1.l15.3" style="font-size:80%;">: average number of vectors gathered from the shard
</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l16.1.1.1" style="font-size:80%;">16:</span></span><span class="ltx_text" id="alg1.l16.2" style="font-size:80%;">    estimated_QPS = QPS(n</span><sub class="ltx_sub" id="alg1.l16.3"><span class="ltx_text ltx_font_italic" id="alg1.l16.3.1" style="font-size:80%;">s</span></sub><span class="ltx_text" id="alg1.l16.4" style="font-size:80%;">)
</span>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l17.1.1.1" style="font-size:80%;">17:</span></span><span class="ltx_text" id="alg1.l17.2" style="font-size:80%;">    num_replicas = target_traffic</span><math alttext="/" class="ltx_Math" display="inline" id="alg1.l17.m1.1"><semantics id="alg1.l17.m1.1a"><mo id="alg1.l17.m1.1.1" maxsize="80%" minsize="80%" stretchy="true" symmetric="true" xref="alg1.l17.m1.1.1.cmml">/</mo><annotation-xml encoding="MathML-Content" id="alg1.l17.m1.1b"><divide id="alg1.l17.m1.1.1.cmml" xref="alg1.l17.m1.1.1"></divide></annotation-xml><annotation encoding="application/x-tex" id="alg1.l17.m1.1c">/</annotation><annotation encoding="application/x-llamapun" id="alg1.l17.m1.1d">/</annotation></semantics></math><span class="ltx_text" id="alg1.l17.3" style="font-size:80%;">estimated_QPS
</span>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l18.1.1.1" style="font-size:80%;">18:</span></span><span class="ltx_text" id="alg1.l18.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_bold" id="alg1.l18.3" style="font-size:80%;">return</span><span class="ltx_text" id="alg1.l18.4" style="font-size:80%;"> num_replicas
</span>
</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l19.1.1.1" style="font-size:80%;">19:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l19.2" style="font-size:80%;">end</span><span class="ltx_text" id="alg1.l19.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg1.l19.4" style="font-size:80%;">function</span>
</div>
<div class="ltx_listingline" id="alg1.l20">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l20.1.1.1" style="font-size:80%;">20:</span></span>
</div>
<div class="ltx_listingline" id="alg1.l21">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l21.1.1.1" style="font-size:80%;">21:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l21.2" style="font-size:80%;">function</span><span class="ltx_text" id="alg1.l21.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_smallcaps" id="alg1.l21.4" style="font-size:80%;">capacity</span><span class="ltx_text" id="alg1.l21.5" style="font-size:80%;">(k, j)
</span>
</div>
<div class="ltx_listingline" id="alg1.l22">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l22.1.1.1" style="font-size:80%;">22:</span></span><span class="ltx_text" id="alg1.l22.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_bold" id="alg1.l22.3" style="font-size:80%;">return</span><span class="ltx_text" id="alg1.l22.4" style="font-size:80%;"> </span><math alttext="(j-k+1)\times(size\_of\_a\_single\_embedding\_vector)" class="ltx_Math" display="inline" id="alg1.l22.m1.2"><semantics id="alg1.l22.m1.2a"><mrow id="alg1.l22.m1.2.2" xref="alg1.l22.m1.2.2.cmml"><mrow id="alg1.l22.m1.1.1.1.1" xref="alg1.l22.m1.1.1.1.1.1.cmml"><mo id="alg1.l22.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="alg1.l22.m1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l22.m1.1.1.1.1.1" xref="alg1.l22.m1.1.1.1.1.1.cmml"><mrow id="alg1.l22.m1.1.1.1.1.1.2" xref="alg1.l22.m1.1.1.1.1.1.2.cmml"><mi id="alg1.l22.m1.1.1.1.1.1.2.2" mathsize="80%" xref="alg1.l22.m1.1.1.1.1.1.2.2.cmml">j</mi><mo id="alg1.l22.m1.1.1.1.1.1.2.1" mathsize="80%" xref="alg1.l22.m1.1.1.1.1.1.2.1.cmml">−</mo><mi id="alg1.l22.m1.1.1.1.1.1.2.3" mathsize="80%" xref="alg1.l22.m1.1.1.1.1.1.2.3.cmml">k</mi></mrow><mo id="alg1.l22.m1.1.1.1.1.1.1" mathsize="80%" xref="alg1.l22.m1.1.1.1.1.1.1.cmml">+</mo><mn id="alg1.l22.m1.1.1.1.1.1.3" mathsize="80%" xref="alg1.l22.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="alg1.l22.m1.1.1.1.1.3" maxsize="80%" minsize="80%" rspace="0.055em" xref="alg1.l22.m1.1.1.1.1.1.cmml">)</mo></mrow><mo id="alg1.l22.m1.2.2.3" mathsize="80%" rspace="0.222em" xref="alg1.l22.m1.2.2.3.cmml">×</mo><mrow id="alg1.l22.m1.2.2.2.1" xref="alg1.l22.m1.2.2.2.1.1.cmml"><mo id="alg1.l22.m1.2.2.2.1.2" maxsize="80%" minsize="80%" xref="alg1.l22.m1.2.2.2.1.1.cmml">(</mo><mrow id="alg1.l22.m1.2.2.2.1.1" xref="alg1.l22.m1.2.2.2.1.1.cmml"><mi id="alg1.l22.m1.2.2.2.1.1.2" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.2.cmml">s</mi><mo id="alg1.l22.m1.2.2.2.1.1.1" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.3" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.3.cmml">i</mi><mo id="alg1.l22.m1.2.2.2.1.1.1a" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.4" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.4.cmml">z</mi><mo id="alg1.l22.m1.2.2.2.1.1.1b" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.5" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.5.cmml">e</mi><mo id="alg1.l22.m1.2.2.2.1.1.1c" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.6" mathsize="80%" mathvariant="normal" xref="alg1.l22.m1.2.2.2.1.1.6.cmml">_</mi><mo id="alg1.l22.m1.2.2.2.1.1.1d" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.7" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.7.cmml">o</mi><mo id="alg1.l22.m1.2.2.2.1.1.1e" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.8" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.8.cmml">f</mi><mo id="alg1.l22.m1.2.2.2.1.1.1f" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.9" mathsize="80%" mathvariant="normal" xref="alg1.l22.m1.2.2.2.1.1.9.cmml">_</mi><mo id="alg1.l22.m1.2.2.2.1.1.1g" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.10" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.10.cmml">a</mi><mo id="alg1.l22.m1.2.2.2.1.1.1h" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.11" mathsize="80%" mathvariant="normal" xref="alg1.l22.m1.2.2.2.1.1.11.cmml">_</mi><mo id="alg1.l22.m1.2.2.2.1.1.1i" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.12" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.12.cmml">s</mi><mo id="alg1.l22.m1.2.2.2.1.1.1j" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.13" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.13.cmml">i</mi><mo id="alg1.l22.m1.2.2.2.1.1.1k" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.14" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.14.cmml">n</mi><mo id="alg1.l22.m1.2.2.2.1.1.1l" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.15" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.15.cmml">g</mi><mo id="alg1.l22.m1.2.2.2.1.1.1m" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.16" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.16.cmml">l</mi><mo id="alg1.l22.m1.2.2.2.1.1.1n" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.17" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.17.cmml">e</mi><mo id="alg1.l22.m1.2.2.2.1.1.1o" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.18" mathsize="80%" mathvariant="normal" xref="alg1.l22.m1.2.2.2.1.1.18.cmml">_</mi><mo id="alg1.l22.m1.2.2.2.1.1.1p" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.19" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.19.cmml">e</mi><mo id="alg1.l22.m1.2.2.2.1.1.1q" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.20" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.20.cmml">m</mi><mo id="alg1.l22.m1.2.2.2.1.1.1r" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.21" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.21.cmml">b</mi><mo id="alg1.l22.m1.2.2.2.1.1.1s" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.22" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.22.cmml">e</mi><mo id="alg1.l22.m1.2.2.2.1.1.1t" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.23" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.23.cmml">d</mi><mo id="alg1.l22.m1.2.2.2.1.1.1u" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.24" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.24.cmml">d</mi><mo id="alg1.l22.m1.2.2.2.1.1.1v" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.25" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.25.cmml">i</mi><mo id="alg1.l22.m1.2.2.2.1.1.1w" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.26" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.26.cmml">n</mi><mo id="alg1.l22.m1.2.2.2.1.1.1x" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.27" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.27.cmml">g</mi><mo id="alg1.l22.m1.2.2.2.1.1.1y" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.28" mathsize="80%" mathvariant="normal" xref="alg1.l22.m1.2.2.2.1.1.28.cmml">_</mi><mo id="alg1.l22.m1.2.2.2.1.1.1z" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.29" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.29.cmml">v</mi><mo id="alg1.l22.m1.2.2.2.1.1.1aa" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.30" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.30.cmml">e</mi><mo id="alg1.l22.m1.2.2.2.1.1.1ab" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.31" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.31.cmml">c</mi><mo id="alg1.l22.m1.2.2.2.1.1.1ac" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.32" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.32.cmml">t</mi><mo id="alg1.l22.m1.2.2.2.1.1.1ad" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.33" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.33.cmml">o</mi><mo id="alg1.l22.m1.2.2.2.1.1.1ae" xref="alg1.l22.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="alg1.l22.m1.2.2.2.1.1.34" mathsize="80%" xref="alg1.l22.m1.2.2.2.1.1.34.cmml">r</mi></mrow><mo id="alg1.l22.m1.2.2.2.1.3" maxsize="80%" minsize="80%" xref="alg1.l22.m1.2.2.2.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l22.m1.2b"><apply id="alg1.l22.m1.2.2.cmml" xref="alg1.l22.m1.2.2"><times id="alg1.l22.m1.2.2.3.cmml" xref="alg1.l22.m1.2.2.3"></times><apply id="alg1.l22.m1.1.1.1.1.1.cmml" xref="alg1.l22.m1.1.1.1.1"><plus id="alg1.l22.m1.1.1.1.1.1.1.cmml" xref="alg1.l22.m1.1.1.1.1.1.1"></plus><apply id="alg1.l22.m1.1.1.1.1.1.2.cmml" xref="alg1.l22.m1.1.1.1.1.1.2"><minus id="alg1.l22.m1.1.1.1.1.1.2.1.cmml" xref="alg1.l22.m1.1.1.1.1.1.2.1"></minus><ci id="alg1.l22.m1.1.1.1.1.1.2.2.cmml" xref="alg1.l22.m1.1.1.1.1.1.2.2">𝑗</ci><ci id="alg1.l22.m1.1.1.1.1.1.2.3.cmml" xref="alg1.l22.m1.1.1.1.1.1.2.3">𝑘</ci></apply><cn id="alg1.l22.m1.1.1.1.1.1.3.cmml" type="integer" xref="alg1.l22.m1.1.1.1.1.1.3">1</cn></apply><apply id="alg1.l22.m1.2.2.2.1.1.cmml" xref="alg1.l22.m1.2.2.2.1"><times id="alg1.l22.m1.2.2.2.1.1.1.cmml" xref="alg1.l22.m1.2.2.2.1.1.1"></times><ci id="alg1.l22.m1.2.2.2.1.1.2.cmml" xref="alg1.l22.m1.2.2.2.1.1.2">𝑠</ci><ci id="alg1.l22.m1.2.2.2.1.1.3.cmml" xref="alg1.l22.m1.2.2.2.1.1.3">𝑖</ci><ci id="alg1.l22.m1.2.2.2.1.1.4.cmml" xref="alg1.l22.m1.2.2.2.1.1.4">𝑧</ci><ci id="alg1.l22.m1.2.2.2.1.1.5.cmml" xref="alg1.l22.m1.2.2.2.1.1.5">𝑒</ci><ci id="alg1.l22.m1.2.2.2.1.1.6.cmml" xref="alg1.l22.m1.2.2.2.1.1.6">_</ci><ci id="alg1.l22.m1.2.2.2.1.1.7.cmml" xref="alg1.l22.m1.2.2.2.1.1.7">𝑜</ci><ci id="alg1.l22.m1.2.2.2.1.1.8.cmml" xref="alg1.l22.m1.2.2.2.1.1.8">𝑓</ci><ci id="alg1.l22.m1.2.2.2.1.1.9.cmml" xref="alg1.l22.m1.2.2.2.1.1.9">_</ci><ci id="alg1.l22.m1.2.2.2.1.1.10.cmml" xref="alg1.l22.m1.2.2.2.1.1.10">𝑎</ci><ci id="alg1.l22.m1.2.2.2.1.1.11.cmml" xref="alg1.l22.m1.2.2.2.1.1.11">_</ci><ci id="alg1.l22.m1.2.2.2.1.1.12.cmml" xref="alg1.l22.m1.2.2.2.1.1.12">𝑠</ci><ci id="alg1.l22.m1.2.2.2.1.1.13.cmml" xref="alg1.l22.m1.2.2.2.1.1.13">𝑖</ci><ci id="alg1.l22.m1.2.2.2.1.1.14.cmml" xref="alg1.l22.m1.2.2.2.1.1.14">𝑛</ci><ci id="alg1.l22.m1.2.2.2.1.1.15.cmml" xref="alg1.l22.m1.2.2.2.1.1.15">𝑔</ci><ci id="alg1.l22.m1.2.2.2.1.1.16.cmml" xref="alg1.l22.m1.2.2.2.1.1.16">𝑙</ci><ci id="alg1.l22.m1.2.2.2.1.1.17.cmml" xref="alg1.l22.m1.2.2.2.1.1.17">𝑒</ci><ci id="alg1.l22.m1.2.2.2.1.1.18.cmml" xref="alg1.l22.m1.2.2.2.1.1.18">_</ci><ci id="alg1.l22.m1.2.2.2.1.1.19.cmml" xref="alg1.l22.m1.2.2.2.1.1.19">𝑒</ci><ci id="alg1.l22.m1.2.2.2.1.1.20.cmml" xref="alg1.l22.m1.2.2.2.1.1.20">𝑚</ci><ci id="alg1.l22.m1.2.2.2.1.1.21.cmml" xref="alg1.l22.m1.2.2.2.1.1.21">𝑏</ci><ci id="alg1.l22.m1.2.2.2.1.1.22.cmml" xref="alg1.l22.m1.2.2.2.1.1.22">𝑒</ci><ci id="alg1.l22.m1.2.2.2.1.1.23.cmml" xref="alg1.l22.m1.2.2.2.1.1.23">𝑑</ci><ci id="alg1.l22.m1.2.2.2.1.1.24.cmml" xref="alg1.l22.m1.2.2.2.1.1.24">𝑑</ci><ci id="alg1.l22.m1.2.2.2.1.1.25.cmml" xref="alg1.l22.m1.2.2.2.1.1.25">𝑖</ci><ci id="alg1.l22.m1.2.2.2.1.1.26.cmml" xref="alg1.l22.m1.2.2.2.1.1.26">𝑛</ci><ci id="alg1.l22.m1.2.2.2.1.1.27.cmml" xref="alg1.l22.m1.2.2.2.1.1.27">𝑔</ci><ci id="alg1.l22.m1.2.2.2.1.1.28.cmml" xref="alg1.l22.m1.2.2.2.1.1.28">_</ci><ci id="alg1.l22.m1.2.2.2.1.1.29.cmml" xref="alg1.l22.m1.2.2.2.1.1.29">𝑣</ci><ci id="alg1.l22.m1.2.2.2.1.1.30.cmml" xref="alg1.l22.m1.2.2.2.1.1.30">𝑒</ci><ci id="alg1.l22.m1.2.2.2.1.1.31.cmml" xref="alg1.l22.m1.2.2.2.1.1.31">𝑐</ci><ci id="alg1.l22.m1.2.2.2.1.1.32.cmml" xref="alg1.l22.m1.2.2.2.1.1.32">𝑡</ci><ci id="alg1.l22.m1.2.2.2.1.1.33.cmml" xref="alg1.l22.m1.2.2.2.1.1.33">𝑜</ci><ci id="alg1.l22.m1.2.2.2.1.1.34.cmml" xref="alg1.l22.m1.2.2.2.1.1.34">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l22.m1.2c">(j-k+1)\times(size\_of\_a\_single\_embedding\_vector)</annotation><annotation encoding="application/x-llamapun" id="alg1.l22.m1.2d">( italic_j - italic_k + 1 ) × ( italic_s italic_i italic_z italic_e _ italic_o italic_f _ italic_a _ italic_s italic_i italic_n italic_g italic_l italic_e _ italic_e italic_m italic_b italic_e italic_d italic_d italic_i italic_n italic_g _ italic_v italic_e italic_c italic_t italic_o italic_r )</annotation></semantics></math><span class="ltx_text" id="alg1.l22.5" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l23">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l23.1.1.1" style="font-size:80%;">23:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l23.2" style="font-size:80%;">end</span><span class="ltx_text" id="alg1.l23.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg1.l23.4" style="font-size:80%;">function</span>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Deployment cost estimation.</span> Using the sorted embedding table,
ElasticRec iterates through the evaluation space of various <em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.2">partitioning
plans</em> (detailed in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#alg2" title="Algorithm 2 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>) and estimates each plan’s
memory consumption to identify the optimal partitioning plan, i.e., one
with the lowest memory consumption. We use Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#alg1" title="Algorithm 1 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a> to
explain how ElasticRec predicts the memory consumption of a given
partitioning plan, which is determined by both the size of each shard
and the number of replicas to instantiate for each shard (line 4).</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.15">The number of shard replicas to instantiate can be estimated by dividing up the
target QPS with the number of queries a specific shard is able to process
per second (line <math alttext="14" class="ltx_Math" display="inline" id="S4.SS2.p5.1.m1.1"><semantics id="S4.SS2.p5.1.m1.1a"><mn id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><cn id="S4.SS2.p5.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p5.1.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">14</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.1.m1.1d">14</annotation></semantics></math>). We observe that the QPS an embedding shard can sustain
is primarily determined by two key parameters: (1) the number of embeddings to
gather from that shard and (2) the size of each embedding vector which
determines the overall volume of data to fetch from memory. We employ a
<em class="ltx_emph ltx_font_italic" id="S4.SS2.p5.15.1">profiling-based</em> approach to estimate these parameters as explained
below. Suppose the number of vectors to gather from the original,
non-partitioned embedding table is defined as n<sub class="ltx_sub" id="S4.SS2.p5.15.2"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.15.2.1">t</span></sub> (line 8). We first need
to predict how many embeddings will be gathered from the partitioned embedding
shard (n<sub class="ltx_sub" id="S4.SS2.p5.15.3"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.15.3.1">s</span></sub>) out of the overall n<sub class="ltx_sub" id="S4.SS2.p5.15.4"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.15.4.1">t</span></sub>. The value of n<sub class="ltx_sub" id="S4.SS2.p5.15.5"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.15.5.1">s</span></sub> can be
estimated by predicting what fraction of the overall table accesses
(n<sub class="ltx_sub" id="S4.SS2.p5.15.6"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.15.6.1">t</span></sub>) is likely to fall under the given embedding shard. Since the
embedding vector’s access frequency (one which we already used to sort and
preprocess the table) is a direct indicator of which embeddings are
most likely to be accessed to service a query, we construct a CDF (cumulative
distribution function) using the “sorted” embedding table’s access frequency
information. Because ElasticRec constructs embedding shards over non-overlapping
set of embedding vectors with consecutive index IDs
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F8" title="Figure 8 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">8</span></a>(b)), an embedding shard starting from index ID
<math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p5.7.m7.1"><semantics id="S4.SS2.p5.7.m7.1a"><mi id="S4.SS2.p5.7.m7.1.1" xref="S4.SS2.p5.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.7.m7.1b"><ci id="S4.SS2.p5.7.m7.1.1.cmml" xref="S4.SS2.p5.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.7.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.7.m7.1d">italic_k</annotation></semantics></math> to <math alttext="j" class="ltx_Math" display="inline" id="S4.SS2.p5.8.m8.1"><semantics id="S4.SS2.p5.8.m8.1a"><mi id="S4.SS2.p5.8.m8.1.1" xref="S4.SS2.p5.8.m8.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.8.m8.1b"><ci id="S4.SS2.p5.8.m8.1.1.cmml" xref="S4.SS2.p5.8.m8.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.8.m8.1c">j</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.8.m8.1d">italic_j</annotation></semantics></math> (<math alttext="k&lt;j" class="ltx_Math" display="inline" id="S4.SS2.p5.9.m9.1"><semantics id="S4.SS2.p5.9.m9.1a"><mrow id="S4.SS2.p5.9.m9.1.1" xref="S4.SS2.p5.9.m9.1.1.cmml"><mi id="S4.SS2.p5.9.m9.1.1.2" xref="S4.SS2.p5.9.m9.1.1.2.cmml">k</mi><mo id="S4.SS2.p5.9.m9.1.1.1" xref="S4.SS2.p5.9.m9.1.1.1.cmml">&lt;</mo><mi id="S4.SS2.p5.9.m9.1.1.3" xref="S4.SS2.p5.9.m9.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.9.m9.1b"><apply id="S4.SS2.p5.9.m9.1.1.cmml" xref="S4.SS2.p5.9.m9.1.1"><lt id="S4.SS2.p5.9.m9.1.1.1.cmml" xref="S4.SS2.p5.9.m9.1.1.1"></lt><ci id="S4.SS2.p5.9.m9.1.1.2.cmml" xref="S4.SS2.p5.9.m9.1.1.2">𝑘</ci><ci id="S4.SS2.p5.9.m9.1.1.3.cmml" xref="S4.SS2.p5.9.m9.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.9.m9.1c">k&lt;j</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.9.m9.1d">italic_k &lt; italic_j</annotation></semantics></math>) is likely to account for (CDF(<math alttext="j" class="ltx_Math" display="inline" id="S4.SS2.p5.10.m10.1"><semantics id="S4.SS2.p5.10.m10.1a"><mi id="S4.SS2.p5.10.m10.1.1" xref="S4.SS2.p5.10.m10.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.10.m10.1b"><ci id="S4.SS2.p5.10.m10.1.1.cmml" xref="S4.SS2.p5.10.m10.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.10.m10.1c">j</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.10.m10.1d">italic_j</annotation></semantics></math>)<math alttext="-" class="ltx_Math" display="inline" id="S4.SS2.p5.11.m11.1"><semantics id="S4.SS2.p5.11.m11.1a"><mo id="S4.SS2.p5.11.m11.1.1" xref="S4.SS2.p5.11.m11.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.11.m11.1b"><minus id="S4.SS2.p5.11.m11.1.1.cmml" xref="S4.SS2.p5.11.m11.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.11.m11.1c">-</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.11.m11.1d">-</annotation></semantics></math>CDF(<math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p5.12.m12.1"><semantics id="S4.SS2.p5.12.m12.1a"><mi id="S4.SS2.p5.12.m12.1.1" xref="S4.SS2.p5.12.m12.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.12.m12.1b"><ci id="S4.SS2.p5.12.m12.1.1.cmml" xref="S4.SS2.p5.12.m12.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.12.m12.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.12.m12.1d">italic_k</annotation></semantics></math>)) percentage of
n<sub class="ltx_sub" id="S4.SS2.p5.15.7"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.15.7.1">t</span></sub> gathers (line 11). By multiplying this probability with n<sub class="ltx_sub" id="S4.SS2.p5.15.8"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.15.8.1">t</span></sub>, we
get a reliable estimation of the value of n<sub class="ltx_sub" id="S4.SS2.p5.15.9"><span class="ltx_text ltx_font_italic" id="S4.SS2.p5.15.9.1">s</span></sub> (line 12).</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.3">Now that we have determined the number of embeddings to gather from a shard
(n<sub class="ltx_sub" id="S4.SS2.p6.3.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p6.3.1.1">s</span></sub>), we discuss how to predict the estimated QPS for that shard. The QPS
of an embedding gather operation is determined not only by the number of
embeddings to gather from that shard but also the underlying hardware
architecture the gather operation is initiated. Given such, ElasticRec conducts
a one-time profiling of embedding vector gather operations, swept over various
number of vector gathers, and measures its QPS to construct a lookup table
indexed by the number of gathers (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F9" title="Figure 9 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">9</span></a>). We
utilize this profiled lookup table to generate a regression model (<math alttext="QPS(x)" class="ltx_Math" display="inline" id="S4.SS2.p6.2.m2.1"><semantics id="S4.SS2.p6.2.m2.1a"><mrow id="S4.SS2.p6.2.m2.1.2" xref="S4.SS2.p6.2.m2.1.2.cmml"><mi id="S4.SS2.p6.2.m2.1.2.2" xref="S4.SS2.p6.2.m2.1.2.2.cmml">Q</mi><mo id="S4.SS2.p6.2.m2.1.2.1" xref="S4.SS2.p6.2.m2.1.2.1.cmml">⁢</mo><mi id="S4.SS2.p6.2.m2.1.2.3" xref="S4.SS2.p6.2.m2.1.2.3.cmml">P</mi><mo id="S4.SS2.p6.2.m2.1.2.1a" xref="S4.SS2.p6.2.m2.1.2.1.cmml">⁢</mo><mi id="S4.SS2.p6.2.m2.1.2.4" xref="S4.SS2.p6.2.m2.1.2.4.cmml">S</mi><mo id="S4.SS2.p6.2.m2.1.2.1b" xref="S4.SS2.p6.2.m2.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.p6.2.m2.1.2.5.2" xref="S4.SS2.p6.2.m2.1.2.cmml"><mo id="S4.SS2.p6.2.m2.1.2.5.2.1" stretchy="false" xref="S4.SS2.p6.2.m2.1.2.cmml">(</mo><mi id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml">x</mi><mo id="S4.SS2.p6.2.m2.1.2.5.2.2" stretchy="false" xref="S4.SS2.p6.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><apply id="S4.SS2.p6.2.m2.1.2.cmml" xref="S4.SS2.p6.2.m2.1.2"><times id="S4.SS2.p6.2.m2.1.2.1.cmml" xref="S4.SS2.p6.2.m2.1.2.1"></times><ci id="S4.SS2.p6.2.m2.1.2.2.cmml" xref="S4.SS2.p6.2.m2.1.2.2">𝑄</ci><ci id="S4.SS2.p6.2.m2.1.2.3.cmml" xref="S4.SS2.p6.2.m2.1.2.3">𝑃</ci><ci id="S4.SS2.p6.2.m2.1.2.4.cmml" xref="S4.SS2.p6.2.m2.1.2.4">𝑆</ci><ci id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">QPS(x)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.2.m2.1d">italic_Q italic_P italic_S ( italic_x )</annotation></semantics></math> in
line 10, 13) that estimates the QPS of an embedding gather operator as a
function of n<sub class="ltx_sub" id="S4.SS2.p6.3.2"><span class="ltx_text ltx_font_italic" id="S4.SS2.p6.3.2.1">s</span></sub>. The estimated QPS of a shard is utilized to determine the
number of replicas required to meet a target QPS goal (line 14).
As for the target QPS goal in line 14,
it serves
as a constant value for the dynamic programming algorithm as all the
partitioning plans share the same QPS values. Any QPS values that make
the number of replicas larger than 1 can be utilized for the target
QPS. Here, we utilized 1000 for the QPS goal.
Since each
shard’s memory consumption (line 3) is determined by the embedding shard size
(line 18) and other minimally required memory allocations for each container
(e.g., code, input buffers, min_mem_alloc in line 3), we multiply the number
of replicas (line 2) with per-shard memory consumption (line 3) to get an
estimated memory consumption for deploying that shard (line 4).</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="130" id="S4.F9.g1" src="x16.png" width="315"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>
The changes in QPS (y-axis) as a function of the number of embedding gathers (x-axis) conducted over a 20M entry embedding table. We change the size of the embedding vector dimension (from 32 to 512 element vector) to illustrate how different data volume sizes that are fetched from memory impact the QPS, i.e., the larger the dimension size, the smaller its QPS due to higher read traffic.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p7.1.1">DP-based table partitioning algorithm. </span> DP is a problem-solving
technique that breaks up a complex problem into a set of sub-problems. DP
expresses the solution to the complex problem <em class="ltx_emph ltx_font_italic" id="S4.SS2.p7.1.2">recursively</em> in terms of
the sub-problems and solving the recursive relation without repeatedly solving
the same sub-problem twice by <em class="ltx_emph ltx_font_italic" id="S4.SS2.p7.1.3">memoizing</em> previously solved sub-problems.
We use Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F10" title="Figure 10 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">10</span></a> to explain how DP
sub-problems are defined and solved for embedding table partitioning.</p>
</div>
<div class="ltx_para" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.25">Consider an embedding table <math alttext="E" class="ltx_Math" display="inline" id="S4.SS2.p8.1.m1.1"><semantics id="S4.SS2.p8.1.m1.1a"><mi id="S4.SS2.p8.1.m1.1.1" xref="S4.SS2.p8.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.1.m1.1b"><ci id="S4.SS2.p8.1.m1.1.1.cmml" xref="S4.SS2.p8.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.1.m1.1d">italic_E</annotation></semantics></math> having N<sub class="ltx_sub" id="S4.SS2.p8.25.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p8.25.1.1">max</span></sub> embedding vectors already
sorted based on their hotness as discussed in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F8" title="Figure 8 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">8</span></a>(b). We define Mem[<math alttext="num_{shards}" class="ltx_Math" display="inline" id="S4.SS2.p8.3.m3.1"><semantics id="S4.SS2.p8.3.m3.1a"><mrow id="S4.SS2.p8.3.m3.1.1" xref="S4.SS2.p8.3.m3.1.1.cmml"><mi id="S4.SS2.p8.3.m3.1.1.2" xref="S4.SS2.p8.3.m3.1.1.2.cmml">n</mi><mo id="S4.SS2.p8.3.m3.1.1.1" xref="S4.SS2.p8.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p8.3.m3.1.1.3" xref="S4.SS2.p8.3.m3.1.1.3.cmml">u</mi><mo id="S4.SS2.p8.3.m3.1.1.1a" xref="S4.SS2.p8.3.m3.1.1.1.cmml">⁢</mo><msub id="S4.SS2.p8.3.m3.1.1.4" xref="S4.SS2.p8.3.m3.1.1.4.cmml"><mi id="S4.SS2.p8.3.m3.1.1.4.2" xref="S4.SS2.p8.3.m3.1.1.4.2.cmml">m</mi><mrow id="S4.SS2.p8.3.m3.1.1.4.3" xref="S4.SS2.p8.3.m3.1.1.4.3.cmml"><mi id="S4.SS2.p8.3.m3.1.1.4.3.2" xref="S4.SS2.p8.3.m3.1.1.4.3.2.cmml">s</mi><mo id="S4.SS2.p8.3.m3.1.1.4.3.1" xref="S4.SS2.p8.3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.3.m3.1.1.4.3.3" xref="S4.SS2.p8.3.m3.1.1.4.3.3.cmml">h</mi><mo id="S4.SS2.p8.3.m3.1.1.4.3.1a" xref="S4.SS2.p8.3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.3.m3.1.1.4.3.4" xref="S4.SS2.p8.3.m3.1.1.4.3.4.cmml">a</mi><mo id="S4.SS2.p8.3.m3.1.1.4.3.1b" xref="S4.SS2.p8.3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.3.m3.1.1.4.3.5" xref="S4.SS2.p8.3.m3.1.1.4.3.5.cmml">r</mi><mo id="S4.SS2.p8.3.m3.1.1.4.3.1c" xref="S4.SS2.p8.3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.3.m3.1.1.4.3.6" xref="S4.SS2.p8.3.m3.1.1.4.3.6.cmml">d</mi><mo id="S4.SS2.p8.3.m3.1.1.4.3.1d" xref="S4.SS2.p8.3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.3.m3.1.1.4.3.7" xref="S4.SS2.p8.3.m3.1.1.4.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.3.m3.1b"><apply id="S4.SS2.p8.3.m3.1.1.cmml" xref="S4.SS2.p8.3.m3.1.1"><times id="S4.SS2.p8.3.m3.1.1.1.cmml" xref="S4.SS2.p8.3.m3.1.1.1"></times><ci id="S4.SS2.p8.3.m3.1.1.2.cmml" xref="S4.SS2.p8.3.m3.1.1.2">𝑛</ci><ci id="S4.SS2.p8.3.m3.1.1.3.cmml" xref="S4.SS2.p8.3.m3.1.1.3">𝑢</ci><apply id="S4.SS2.p8.3.m3.1.1.4.cmml" xref="S4.SS2.p8.3.m3.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p8.3.m3.1.1.4.1.cmml" xref="S4.SS2.p8.3.m3.1.1.4">subscript</csymbol><ci id="S4.SS2.p8.3.m3.1.1.4.2.cmml" xref="S4.SS2.p8.3.m3.1.1.4.2">𝑚</ci><apply id="S4.SS2.p8.3.m3.1.1.4.3.cmml" xref="S4.SS2.p8.3.m3.1.1.4.3"><times id="S4.SS2.p8.3.m3.1.1.4.3.1.cmml" xref="S4.SS2.p8.3.m3.1.1.4.3.1"></times><ci id="S4.SS2.p8.3.m3.1.1.4.3.2.cmml" xref="S4.SS2.p8.3.m3.1.1.4.3.2">𝑠</ci><ci id="S4.SS2.p8.3.m3.1.1.4.3.3.cmml" xref="S4.SS2.p8.3.m3.1.1.4.3.3">ℎ</ci><ci id="S4.SS2.p8.3.m3.1.1.4.3.4.cmml" xref="S4.SS2.p8.3.m3.1.1.4.3.4">𝑎</ci><ci id="S4.SS2.p8.3.m3.1.1.4.3.5.cmml" xref="S4.SS2.p8.3.m3.1.1.4.3.5">𝑟</ci><ci id="S4.SS2.p8.3.m3.1.1.4.3.6.cmml" xref="S4.SS2.p8.3.m3.1.1.4.3.6">𝑑</ci><ci id="S4.SS2.p8.3.m3.1.1.4.3.7.cmml" xref="S4.SS2.p8.3.m3.1.1.4.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.3.m3.1c">num_{shards}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.3.m3.1d">italic_n italic_u italic_m start_POSTSUBSCRIPT italic_s italic_h italic_a italic_r italic_d italic_s end_POSTSUBSCRIPT</annotation></semantics></math>][<math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.p8.4.m4.1"><semantics id="S4.SS2.p8.4.m4.1a"><mi id="S4.SS2.p8.4.m4.1.1" xref="S4.SS2.p8.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.4.m4.1b"><ci id="S4.SS2.p8.4.m4.1.1.cmml" xref="S4.SS2.p8.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.4.m4.1d">italic_x</annotation></semantics></math>] as the
lowest memory cost incurred when a table <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="S4.SS2.p8.5.m5.1"><semantics id="S4.SS2.p8.5.m5.1a"><msup id="S4.SS2.p8.5.m5.1.1" xref="S4.SS2.p8.5.m5.1.1.cmml"><mi id="S4.SS2.p8.5.m5.1.1.2" xref="S4.SS2.p8.5.m5.1.1.2.cmml">E</mi><mo id="S4.SS2.p8.5.m5.1.1.3" xref="S4.SS2.p8.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.5.m5.1b"><apply id="S4.SS2.p8.5.m5.1.1.cmml" xref="S4.SS2.p8.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p8.5.m5.1.1.1.cmml" xref="S4.SS2.p8.5.m5.1.1">superscript</csymbol><ci id="S4.SS2.p8.5.m5.1.1.2.cmml" xref="S4.SS2.p8.5.m5.1.1.2">𝐸</ci><ci id="S4.SS2.p8.5.m5.1.1.3.cmml" xref="S4.SS2.p8.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.5.m5.1c">E^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.5.m5.1d">italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> containing only the
<math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.p8.6.m6.1"><semantics id="S4.SS2.p8.6.m6.1a"><mi id="S4.SS2.p8.6.m6.1.1" xref="S4.SS2.p8.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.6.m6.1b"><ci id="S4.SS2.p8.6.m6.1.1.cmml" xref="S4.SS2.p8.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.6.m6.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.6.m6.1d">italic_x</annotation></semantics></math> most hot embeddings of table <math alttext="E" class="ltx_Math" display="inline" id="S4.SS2.p8.7.m7.1"><semantics id="S4.SS2.p8.7.m7.1a"><mi id="S4.SS2.p8.7.m7.1.1" xref="S4.SS2.p8.7.m7.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.7.m7.1b"><ci id="S4.SS2.p8.7.m7.1.1.cmml" xref="S4.SS2.p8.7.m7.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.7.m7.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.7.m7.1d">italic_E</annotation></semantics></math> (i.e., <math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.p8.8.m8.1"><semantics id="S4.SS2.p8.8.m8.1a"><mi id="S4.SS2.p8.8.m8.1.1" xref="S4.SS2.p8.8.m8.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.8.m8.1b"><ci id="S4.SS2.p8.8.m8.1.1.cmml" xref="S4.SS2.p8.8.m8.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.8.m8.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.8.m8.1d">italic_x</annotation></semantics></math> <math alttext="\leq" class="ltx_Math" display="inline" id="S4.SS2.p8.9.m9.1"><semantics id="S4.SS2.p8.9.m9.1a"><mo id="S4.SS2.p8.9.m9.1.1" xref="S4.SS2.p8.9.m9.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.9.m9.1b"><leq id="S4.SS2.p8.9.m9.1.1.cmml" xref="S4.SS2.p8.9.m9.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.9.m9.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.9.m9.1d">≤</annotation></semantics></math> N<sub class="ltx_sub" id="S4.SS2.p8.25.2"><span class="ltx_text ltx_font_italic" id="S4.SS2.p8.25.2.1">max</span></sub>, so when <math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.p8.11.m11.1"><semantics id="S4.SS2.p8.11.m11.1a"><mi id="S4.SS2.p8.11.m11.1.1" xref="S4.SS2.p8.11.m11.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.11.m11.1b"><ci id="S4.SS2.p8.11.m11.1.1.cmml" xref="S4.SS2.p8.11.m11.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.11.m11.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.11.m11.1d">italic_x</annotation></semantics></math>
equals N<sub class="ltx_sub" id="S4.SS2.p8.25.3"><span class="ltx_text ltx_font_italic" id="S4.SS2.p8.25.3.1">max</span></sub>, <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="S4.SS2.p8.13.m13.1"><semantics id="S4.SS2.p8.13.m13.1a"><msup id="S4.SS2.p8.13.m13.1.1" xref="S4.SS2.p8.13.m13.1.1.cmml"><mi id="S4.SS2.p8.13.m13.1.1.2" xref="S4.SS2.p8.13.m13.1.1.2.cmml">E</mi><mo id="S4.SS2.p8.13.m13.1.1.3" xref="S4.SS2.p8.13.m13.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.13.m13.1b"><apply id="S4.SS2.p8.13.m13.1.1.cmml" xref="S4.SS2.p8.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS2.p8.13.m13.1.1.1.cmml" xref="S4.SS2.p8.13.m13.1.1">superscript</csymbol><ci id="S4.SS2.p8.13.m13.1.1.2.cmml" xref="S4.SS2.p8.13.m13.1.1.2">𝐸</ci><ci id="S4.SS2.p8.13.m13.1.1.3.cmml" xref="S4.SS2.p8.13.m13.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.13.m13.1c">E^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.13.m13.1d">italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> is equivalent to <math alttext="E" class="ltx_Math" display="inline" id="S4.SS2.p8.14.m14.1"><semantics id="S4.SS2.p8.14.m14.1a"><mi id="S4.SS2.p8.14.m14.1.1" xref="S4.SS2.p8.14.m14.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.14.m14.1b"><ci id="S4.SS2.p8.14.m14.1.1.cmml" xref="S4.SS2.p8.14.m14.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.14.m14.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.14.m14.1d">italic_E</annotation></semantics></math>) is partitioned into
<math alttext="num_{shards}" class="ltx_Math" display="inline" id="S4.SS2.p8.15.m15.1"><semantics id="S4.SS2.p8.15.m15.1a"><mrow id="S4.SS2.p8.15.m15.1.1" xref="S4.SS2.p8.15.m15.1.1.cmml"><mi id="S4.SS2.p8.15.m15.1.1.2" xref="S4.SS2.p8.15.m15.1.1.2.cmml">n</mi><mo id="S4.SS2.p8.15.m15.1.1.1" xref="S4.SS2.p8.15.m15.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p8.15.m15.1.1.3" xref="S4.SS2.p8.15.m15.1.1.3.cmml">u</mi><mo id="S4.SS2.p8.15.m15.1.1.1a" xref="S4.SS2.p8.15.m15.1.1.1.cmml">⁢</mo><msub id="S4.SS2.p8.15.m15.1.1.4" xref="S4.SS2.p8.15.m15.1.1.4.cmml"><mi id="S4.SS2.p8.15.m15.1.1.4.2" xref="S4.SS2.p8.15.m15.1.1.4.2.cmml">m</mi><mrow id="S4.SS2.p8.15.m15.1.1.4.3" xref="S4.SS2.p8.15.m15.1.1.4.3.cmml"><mi id="S4.SS2.p8.15.m15.1.1.4.3.2" xref="S4.SS2.p8.15.m15.1.1.4.3.2.cmml">s</mi><mo id="S4.SS2.p8.15.m15.1.1.4.3.1" xref="S4.SS2.p8.15.m15.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.15.m15.1.1.4.3.3" xref="S4.SS2.p8.15.m15.1.1.4.3.3.cmml">h</mi><mo id="S4.SS2.p8.15.m15.1.1.4.3.1a" xref="S4.SS2.p8.15.m15.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.15.m15.1.1.4.3.4" xref="S4.SS2.p8.15.m15.1.1.4.3.4.cmml">a</mi><mo id="S4.SS2.p8.15.m15.1.1.4.3.1b" xref="S4.SS2.p8.15.m15.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.15.m15.1.1.4.3.5" xref="S4.SS2.p8.15.m15.1.1.4.3.5.cmml">r</mi><mo id="S4.SS2.p8.15.m15.1.1.4.3.1c" xref="S4.SS2.p8.15.m15.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.15.m15.1.1.4.3.6" xref="S4.SS2.p8.15.m15.1.1.4.3.6.cmml">d</mi><mo id="S4.SS2.p8.15.m15.1.1.4.3.1d" xref="S4.SS2.p8.15.m15.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.15.m15.1.1.4.3.7" xref="S4.SS2.p8.15.m15.1.1.4.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.15.m15.1b"><apply id="S4.SS2.p8.15.m15.1.1.cmml" xref="S4.SS2.p8.15.m15.1.1"><times id="S4.SS2.p8.15.m15.1.1.1.cmml" xref="S4.SS2.p8.15.m15.1.1.1"></times><ci id="S4.SS2.p8.15.m15.1.1.2.cmml" xref="S4.SS2.p8.15.m15.1.1.2">𝑛</ci><ci id="S4.SS2.p8.15.m15.1.1.3.cmml" xref="S4.SS2.p8.15.m15.1.1.3">𝑢</ci><apply id="S4.SS2.p8.15.m15.1.1.4.cmml" xref="S4.SS2.p8.15.m15.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p8.15.m15.1.1.4.1.cmml" xref="S4.SS2.p8.15.m15.1.1.4">subscript</csymbol><ci id="S4.SS2.p8.15.m15.1.1.4.2.cmml" xref="S4.SS2.p8.15.m15.1.1.4.2">𝑚</ci><apply id="S4.SS2.p8.15.m15.1.1.4.3.cmml" xref="S4.SS2.p8.15.m15.1.1.4.3"><times id="S4.SS2.p8.15.m15.1.1.4.3.1.cmml" xref="S4.SS2.p8.15.m15.1.1.4.3.1"></times><ci id="S4.SS2.p8.15.m15.1.1.4.3.2.cmml" xref="S4.SS2.p8.15.m15.1.1.4.3.2">𝑠</ci><ci id="S4.SS2.p8.15.m15.1.1.4.3.3.cmml" xref="S4.SS2.p8.15.m15.1.1.4.3.3">ℎ</ci><ci id="S4.SS2.p8.15.m15.1.1.4.3.4.cmml" xref="S4.SS2.p8.15.m15.1.1.4.3.4">𝑎</ci><ci id="S4.SS2.p8.15.m15.1.1.4.3.5.cmml" xref="S4.SS2.p8.15.m15.1.1.4.3.5">𝑟</ci><ci id="S4.SS2.p8.15.m15.1.1.4.3.6.cmml" xref="S4.SS2.p8.15.m15.1.1.4.3.6">𝑑</ci><ci id="S4.SS2.p8.15.m15.1.1.4.3.7.cmml" xref="S4.SS2.p8.15.m15.1.1.4.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.15.m15.1c">num_{shards}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.15.m15.1d">italic_n italic_u italic_m start_POSTSUBSCRIPT italic_s italic_h italic_a italic_r italic_d italic_s end_POSTSUBSCRIPT</annotation></semantics></math> shards. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F10" title="Figure 10 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">10</span></a>, for instance, <math alttext="E" class="ltx_Math" display="inline" id="S4.SS2.p8.16.m16.1"><semantics id="S4.SS2.p8.16.m16.1a"><mi id="S4.SS2.p8.16.m16.1.1" xref="S4.SS2.p8.16.m16.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.16.m16.1b"><ci id="S4.SS2.p8.16.m16.1.1.cmml" xref="S4.SS2.p8.16.m16.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.16.m16.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.16.m16.1d">italic_E</annotation></semantics></math> is a
table with a total of N<sub class="ltx_sub" id="S4.SS2.p8.25.4"><span class="ltx_text ltx_font_italic" id="S4.SS2.p8.25.4.1">max</span></sub>=<math alttext="5" class="ltx_Math" display="inline" id="S4.SS2.p8.18.m18.1"><semantics id="S4.SS2.p8.18.m18.1a"><mn id="S4.SS2.p8.18.m18.1.1" xref="S4.SS2.p8.18.m18.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.18.m18.1b"><cn id="S4.SS2.p8.18.m18.1.1.cmml" type="integer" xref="S4.SS2.p8.18.m18.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.18.m18.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.18.m18.1d">5</annotation></semantics></math> embeddings. Also, Mem[2][3]
stores the smallest memory cost of partitioning the table <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="S4.SS2.p8.19.m19.1"><semantics id="S4.SS2.p8.19.m19.1a"><msup id="S4.SS2.p8.19.m19.1.1" xref="S4.SS2.p8.19.m19.1.1.cmml"><mi id="S4.SS2.p8.19.m19.1.1.2" xref="S4.SS2.p8.19.m19.1.1.2.cmml">E</mi><mo id="S4.SS2.p8.19.m19.1.1.3" xref="S4.SS2.p8.19.m19.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.19.m19.1b"><apply id="S4.SS2.p8.19.m19.1.1.cmml" xref="S4.SS2.p8.19.m19.1.1"><csymbol cd="ambiguous" id="S4.SS2.p8.19.m19.1.1.1.cmml" xref="S4.SS2.p8.19.m19.1.1">superscript</csymbol><ci id="S4.SS2.p8.19.m19.1.1.2.cmml" xref="S4.SS2.p8.19.m19.1.1.2">𝐸</ci><ci id="S4.SS2.p8.19.m19.1.1.3.cmml" xref="S4.SS2.p8.19.m19.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.19.m19.1c">E^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.19.m19.1d">italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> sized with <math alttext="3" class="ltx_Math" display="inline" id="S4.SS2.p8.20.m20.1"><semantics id="S4.SS2.p8.20.m20.1a"><mn id="S4.SS2.p8.20.m20.1.1" xref="S4.SS2.p8.20.m20.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.20.m20.1b"><cn id="S4.SS2.p8.20.m20.1.1.cmml" type="integer" xref="S4.SS2.p8.20.m20.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.20.m20.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.20.m20.1d">3</annotation></semantics></math>
most hot embeddings of <math alttext="E" class="ltx_Math" display="inline" id="S4.SS2.p8.21.m21.1"><semantics id="S4.SS2.p8.21.m21.1a"><mi id="S4.SS2.p8.21.m21.1.1" xref="S4.SS2.p8.21.m21.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.21.m21.1b"><ci id="S4.SS2.p8.21.m21.1.1.cmml" xref="S4.SS2.p8.21.m21.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.21.m21.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.21.m21.1d">italic_E</annotation></semantics></math> (i.e., E[1,2,3]) into <math alttext="num_{shards}" class="ltx_Math" display="inline" id="S4.SS2.p8.22.m22.1"><semantics id="S4.SS2.p8.22.m22.1a"><mrow id="S4.SS2.p8.22.m22.1.1" xref="S4.SS2.p8.22.m22.1.1.cmml"><mi id="S4.SS2.p8.22.m22.1.1.2" xref="S4.SS2.p8.22.m22.1.1.2.cmml">n</mi><mo id="S4.SS2.p8.22.m22.1.1.1" xref="S4.SS2.p8.22.m22.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p8.22.m22.1.1.3" xref="S4.SS2.p8.22.m22.1.1.3.cmml">u</mi><mo id="S4.SS2.p8.22.m22.1.1.1a" xref="S4.SS2.p8.22.m22.1.1.1.cmml">⁢</mo><msub id="S4.SS2.p8.22.m22.1.1.4" xref="S4.SS2.p8.22.m22.1.1.4.cmml"><mi id="S4.SS2.p8.22.m22.1.1.4.2" xref="S4.SS2.p8.22.m22.1.1.4.2.cmml">m</mi><mrow id="S4.SS2.p8.22.m22.1.1.4.3" xref="S4.SS2.p8.22.m22.1.1.4.3.cmml"><mi id="S4.SS2.p8.22.m22.1.1.4.3.2" xref="S4.SS2.p8.22.m22.1.1.4.3.2.cmml">s</mi><mo id="S4.SS2.p8.22.m22.1.1.4.3.1" xref="S4.SS2.p8.22.m22.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.22.m22.1.1.4.3.3" xref="S4.SS2.p8.22.m22.1.1.4.3.3.cmml">h</mi><mo id="S4.SS2.p8.22.m22.1.1.4.3.1a" xref="S4.SS2.p8.22.m22.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.22.m22.1.1.4.3.4" xref="S4.SS2.p8.22.m22.1.1.4.3.4.cmml">a</mi><mo id="S4.SS2.p8.22.m22.1.1.4.3.1b" xref="S4.SS2.p8.22.m22.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.22.m22.1.1.4.3.5" xref="S4.SS2.p8.22.m22.1.1.4.3.5.cmml">r</mi><mo id="S4.SS2.p8.22.m22.1.1.4.3.1c" xref="S4.SS2.p8.22.m22.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.22.m22.1.1.4.3.6" xref="S4.SS2.p8.22.m22.1.1.4.3.6.cmml">d</mi><mo id="S4.SS2.p8.22.m22.1.1.4.3.1d" xref="S4.SS2.p8.22.m22.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.22.m22.1.1.4.3.7" xref="S4.SS2.p8.22.m22.1.1.4.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.22.m22.1b"><apply id="S4.SS2.p8.22.m22.1.1.cmml" xref="S4.SS2.p8.22.m22.1.1"><times id="S4.SS2.p8.22.m22.1.1.1.cmml" xref="S4.SS2.p8.22.m22.1.1.1"></times><ci id="S4.SS2.p8.22.m22.1.1.2.cmml" xref="S4.SS2.p8.22.m22.1.1.2">𝑛</ci><ci id="S4.SS2.p8.22.m22.1.1.3.cmml" xref="S4.SS2.p8.22.m22.1.1.3">𝑢</ci><apply id="S4.SS2.p8.22.m22.1.1.4.cmml" xref="S4.SS2.p8.22.m22.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p8.22.m22.1.1.4.1.cmml" xref="S4.SS2.p8.22.m22.1.1.4">subscript</csymbol><ci id="S4.SS2.p8.22.m22.1.1.4.2.cmml" xref="S4.SS2.p8.22.m22.1.1.4.2">𝑚</ci><apply id="S4.SS2.p8.22.m22.1.1.4.3.cmml" xref="S4.SS2.p8.22.m22.1.1.4.3"><times id="S4.SS2.p8.22.m22.1.1.4.3.1.cmml" xref="S4.SS2.p8.22.m22.1.1.4.3.1"></times><ci id="S4.SS2.p8.22.m22.1.1.4.3.2.cmml" xref="S4.SS2.p8.22.m22.1.1.4.3.2">𝑠</ci><ci id="S4.SS2.p8.22.m22.1.1.4.3.3.cmml" xref="S4.SS2.p8.22.m22.1.1.4.3.3">ℎ</ci><ci id="S4.SS2.p8.22.m22.1.1.4.3.4.cmml" xref="S4.SS2.p8.22.m22.1.1.4.3.4">𝑎</ci><ci id="S4.SS2.p8.22.m22.1.1.4.3.5.cmml" xref="S4.SS2.p8.22.m22.1.1.4.3.5">𝑟</ci><ci id="S4.SS2.p8.22.m22.1.1.4.3.6.cmml" xref="S4.SS2.p8.22.m22.1.1.4.3.6">𝑑</ci><ci id="S4.SS2.p8.22.m22.1.1.4.3.7.cmml" xref="S4.SS2.p8.22.m22.1.1.4.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.22.m22.1c">num_{shards}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.22.m22.1d">italic_n italic_u italic_m start_POSTSUBSCRIPT italic_s italic_h italic_a italic_r italic_d italic_s end_POSTSUBSCRIPT</annotation></semantics></math>=<math alttext="2" class="ltx_Math" display="inline" id="S4.SS2.p8.23.m23.1"><semantics id="S4.SS2.p8.23.m23.1a"><mn id="S4.SS2.p8.23.m23.1.1" xref="S4.SS2.p8.23.m23.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.23.m23.1b"><cn id="S4.SS2.p8.23.m23.1.1.cmml" type="integer" xref="S4.SS2.p8.23.m23.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.23.m23.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.23.m23.1d">2</annotation></semantics></math> shards. The
key objective of ElasticRec’s DP algorithm is to iterate through the problem
space of Mem[<math alttext="num_{shards}" class="ltx_Math" display="inline" id="S4.SS2.p8.24.m24.1"><semantics id="S4.SS2.p8.24.m24.1a"><mrow id="S4.SS2.p8.24.m24.1.1" xref="S4.SS2.p8.24.m24.1.1.cmml"><mi id="S4.SS2.p8.24.m24.1.1.2" xref="S4.SS2.p8.24.m24.1.1.2.cmml">n</mi><mo id="S4.SS2.p8.24.m24.1.1.1" xref="S4.SS2.p8.24.m24.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p8.24.m24.1.1.3" xref="S4.SS2.p8.24.m24.1.1.3.cmml">u</mi><mo id="S4.SS2.p8.24.m24.1.1.1a" xref="S4.SS2.p8.24.m24.1.1.1.cmml">⁢</mo><msub id="S4.SS2.p8.24.m24.1.1.4" xref="S4.SS2.p8.24.m24.1.1.4.cmml"><mi id="S4.SS2.p8.24.m24.1.1.4.2" xref="S4.SS2.p8.24.m24.1.1.4.2.cmml">m</mi><mrow id="S4.SS2.p8.24.m24.1.1.4.3" xref="S4.SS2.p8.24.m24.1.1.4.3.cmml"><mi id="S4.SS2.p8.24.m24.1.1.4.3.2" xref="S4.SS2.p8.24.m24.1.1.4.3.2.cmml">s</mi><mo id="S4.SS2.p8.24.m24.1.1.4.3.1" xref="S4.SS2.p8.24.m24.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.24.m24.1.1.4.3.3" xref="S4.SS2.p8.24.m24.1.1.4.3.3.cmml">h</mi><mo id="S4.SS2.p8.24.m24.1.1.4.3.1a" xref="S4.SS2.p8.24.m24.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.24.m24.1.1.4.3.4" xref="S4.SS2.p8.24.m24.1.1.4.3.4.cmml">a</mi><mo id="S4.SS2.p8.24.m24.1.1.4.3.1b" xref="S4.SS2.p8.24.m24.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.24.m24.1.1.4.3.5" xref="S4.SS2.p8.24.m24.1.1.4.3.5.cmml">r</mi><mo id="S4.SS2.p8.24.m24.1.1.4.3.1c" xref="S4.SS2.p8.24.m24.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.24.m24.1.1.4.3.6" xref="S4.SS2.p8.24.m24.1.1.4.3.6.cmml">d</mi><mo id="S4.SS2.p8.24.m24.1.1.4.3.1d" xref="S4.SS2.p8.24.m24.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.24.m24.1.1.4.3.7" xref="S4.SS2.p8.24.m24.1.1.4.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.24.m24.1b"><apply id="S4.SS2.p8.24.m24.1.1.cmml" xref="S4.SS2.p8.24.m24.1.1"><times id="S4.SS2.p8.24.m24.1.1.1.cmml" xref="S4.SS2.p8.24.m24.1.1.1"></times><ci id="S4.SS2.p8.24.m24.1.1.2.cmml" xref="S4.SS2.p8.24.m24.1.1.2">𝑛</ci><ci id="S4.SS2.p8.24.m24.1.1.3.cmml" xref="S4.SS2.p8.24.m24.1.1.3">𝑢</ci><apply id="S4.SS2.p8.24.m24.1.1.4.cmml" xref="S4.SS2.p8.24.m24.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p8.24.m24.1.1.4.1.cmml" xref="S4.SS2.p8.24.m24.1.1.4">subscript</csymbol><ci id="S4.SS2.p8.24.m24.1.1.4.2.cmml" xref="S4.SS2.p8.24.m24.1.1.4.2">𝑚</ci><apply id="S4.SS2.p8.24.m24.1.1.4.3.cmml" xref="S4.SS2.p8.24.m24.1.1.4.3"><times id="S4.SS2.p8.24.m24.1.1.4.3.1.cmml" xref="S4.SS2.p8.24.m24.1.1.4.3.1"></times><ci id="S4.SS2.p8.24.m24.1.1.4.3.2.cmml" xref="S4.SS2.p8.24.m24.1.1.4.3.2">𝑠</ci><ci id="S4.SS2.p8.24.m24.1.1.4.3.3.cmml" xref="S4.SS2.p8.24.m24.1.1.4.3.3">ℎ</ci><ci id="S4.SS2.p8.24.m24.1.1.4.3.4.cmml" xref="S4.SS2.p8.24.m24.1.1.4.3.4">𝑎</ci><ci id="S4.SS2.p8.24.m24.1.1.4.3.5.cmml" xref="S4.SS2.p8.24.m24.1.1.4.3.5">𝑟</ci><ci id="S4.SS2.p8.24.m24.1.1.4.3.6.cmml" xref="S4.SS2.p8.24.m24.1.1.4.3.6">𝑑</ci><ci id="S4.SS2.p8.24.m24.1.1.4.3.7.cmml" xref="S4.SS2.p8.24.m24.1.1.4.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.24.m24.1c">num_{shards}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.24.m24.1d">italic_n italic_u italic_m start_POSTSUBSCRIPT italic_s italic_h italic_a italic_r italic_d italic_s end_POSTSUBSCRIPT</annotation></semantics></math>][-] and identify the value of <math alttext="num_{shards}" class="ltx_Math" display="inline" id="S4.SS2.p8.25.m25.1"><semantics id="S4.SS2.p8.25.m25.1a"><mrow id="S4.SS2.p8.25.m25.1.1" xref="S4.SS2.p8.25.m25.1.1.cmml"><mi id="S4.SS2.p8.25.m25.1.1.2" xref="S4.SS2.p8.25.m25.1.1.2.cmml">n</mi><mo id="S4.SS2.p8.25.m25.1.1.1" xref="S4.SS2.p8.25.m25.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p8.25.m25.1.1.3" xref="S4.SS2.p8.25.m25.1.1.3.cmml">u</mi><mo id="S4.SS2.p8.25.m25.1.1.1a" xref="S4.SS2.p8.25.m25.1.1.1.cmml">⁢</mo><msub id="S4.SS2.p8.25.m25.1.1.4" xref="S4.SS2.p8.25.m25.1.1.4.cmml"><mi id="S4.SS2.p8.25.m25.1.1.4.2" xref="S4.SS2.p8.25.m25.1.1.4.2.cmml">m</mi><mrow id="S4.SS2.p8.25.m25.1.1.4.3" xref="S4.SS2.p8.25.m25.1.1.4.3.cmml"><mi id="S4.SS2.p8.25.m25.1.1.4.3.2" xref="S4.SS2.p8.25.m25.1.1.4.3.2.cmml">s</mi><mo id="S4.SS2.p8.25.m25.1.1.4.3.1" xref="S4.SS2.p8.25.m25.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.25.m25.1.1.4.3.3" xref="S4.SS2.p8.25.m25.1.1.4.3.3.cmml">h</mi><mo id="S4.SS2.p8.25.m25.1.1.4.3.1a" xref="S4.SS2.p8.25.m25.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.25.m25.1.1.4.3.4" xref="S4.SS2.p8.25.m25.1.1.4.3.4.cmml">a</mi><mo id="S4.SS2.p8.25.m25.1.1.4.3.1b" xref="S4.SS2.p8.25.m25.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.25.m25.1.1.4.3.5" xref="S4.SS2.p8.25.m25.1.1.4.3.5.cmml">r</mi><mo id="S4.SS2.p8.25.m25.1.1.4.3.1c" xref="S4.SS2.p8.25.m25.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.25.m25.1.1.4.3.6" xref="S4.SS2.p8.25.m25.1.1.4.3.6.cmml">d</mi><mo id="S4.SS2.p8.25.m25.1.1.4.3.1d" xref="S4.SS2.p8.25.m25.1.1.4.3.1.cmml">⁢</mo><mi id="S4.SS2.p8.25.m25.1.1.4.3.7" xref="S4.SS2.p8.25.m25.1.1.4.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.25.m25.1b"><apply id="S4.SS2.p8.25.m25.1.1.cmml" xref="S4.SS2.p8.25.m25.1.1"><times id="S4.SS2.p8.25.m25.1.1.1.cmml" xref="S4.SS2.p8.25.m25.1.1.1"></times><ci id="S4.SS2.p8.25.m25.1.1.2.cmml" xref="S4.SS2.p8.25.m25.1.1.2">𝑛</ci><ci id="S4.SS2.p8.25.m25.1.1.3.cmml" xref="S4.SS2.p8.25.m25.1.1.3">𝑢</ci><apply id="S4.SS2.p8.25.m25.1.1.4.cmml" xref="S4.SS2.p8.25.m25.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p8.25.m25.1.1.4.1.cmml" xref="S4.SS2.p8.25.m25.1.1.4">subscript</csymbol><ci id="S4.SS2.p8.25.m25.1.1.4.2.cmml" xref="S4.SS2.p8.25.m25.1.1.4.2">𝑚</ci><apply id="S4.SS2.p8.25.m25.1.1.4.3.cmml" xref="S4.SS2.p8.25.m25.1.1.4.3"><times id="S4.SS2.p8.25.m25.1.1.4.3.1.cmml" xref="S4.SS2.p8.25.m25.1.1.4.3.1"></times><ci id="S4.SS2.p8.25.m25.1.1.4.3.2.cmml" xref="S4.SS2.p8.25.m25.1.1.4.3.2">𝑠</ci><ci id="S4.SS2.p8.25.m25.1.1.4.3.3.cmml" xref="S4.SS2.p8.25.m25.1.1.4.3.3">ℎ</ci><ci id="S4.SS2.p8.25.m25.1.1.4.3.4.cmml" xref="S4.SS2.p8.25.m25.1.1.4.3.4">𝑎</ci><ci id="S4.SS2.p8.25.m25.1.1.4.3.5.cmml" xref="S4.SS2.p8.25.m25.1.1.4.3.5">𝑟</ci><ci id="S4.SS2.p8.25.m25.1.1.4.3.6.cmml" xref="S4.SS2.p8.25.m25.1.1.4.3.6">𝑑</ci><ci id="S4.SS2.p8.25.m25.1.1.4.3.7.cmml" xref="S4.SS2.p8.25.m25.1.1.4.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.25.m25.1c">num_{shards}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.25.m25.1d">italic_n italic_u italic_m start_POSTSUBSCRIPT italic_s italic_h italic_a italic_r italic_d italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and its
partitioning plan that results in the least memory consumption.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="123" id="S4.F10.g1" src="x17.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>
Example of how our DP algorithm evaluates Mem[3][5] and its optimal partitioning plan. Different shards are colored differently (red/yellow/green).
For clarity of explanation, this example assumes that COST(k,j) is defined as a simple function (unlike Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#alg1" title="Algorithm 1 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>) that returns a value equal to (j - i + 1)<sup class="ltx_sup" id="S4.F10.13.1"><span class="ltx_text ltx_font_italic" id="S4.F10.13.1.1">2</span></sup>/i. For instance, COST(<math alttext="4" class="ltx_Math" display="inline" id="S4.F10.7.m2.1"><semantics id="S4.F10.7.m2.1b"><mn id="S4.F10.7.m2.1.1" xref="S4.F10.7.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.F10.7.m2.1c"><cn id="S4.F10.7.m2.1.1.cmml" type="integer" xref="S4.F10.7.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.7.m2.1d">4</annotation><annotation encoding="application/x-llamapun" id="S4.F10.7.m2.1e">4</annotation></semantics></math>,<math alttext="5" class="ltx_Math" display="inline" id="S4.F10.8.m3.1"><semantics id="S4.F10.8.m3.1b"><mn id="S4.F10.8.m3.1.1" xref="S4.F10.8.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.F10.8.m3.1c"><cn id="S4.F10.8.m3.1.1.cmml" type="integer" xref="S4.F10.8.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.8.m3.1d">5</annotation><annotation encoding="application/x-llamapun" id="S4.F10.8.m3.1e">5</annotation></semantics></math>) = (5 - 4 + 1)<sup class="ltx_sup" id="S4.F10.14.2"><span class="ltx_text ltx_font_italic" id="S4.F10.14.2.1">2</span></sup>/4 = <math alttext="1" class="ltx_Math" display="inline" id="S4.F10.10.m5.1"><semantics id="S4.F10.10.m5.1b"><mn id="S4.F10.10.m5.1.1" xref="S4.F10.10.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.F10.10.m5.1c"><cn id="S4.F10.10.m5.1.1.cmml" type="integer" xref="S4.F10.10.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.10.m5.1d">1</annotation><annotation encoding="application/x-llamapun" id="S4.F10.10.m5.1e">1</annotation></semantics></math>.
</figcaption>
</figure>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg2.2.1.1">Algorithm 2</span> </span> Embedding Table Partitioning Algorithm</figcaption>
<div class="ltx_listing ltx_listing" id="alg2.3">
<div class="ltx_listingline" id="alg2.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l1.1.1.1" style="font-size:80%;">1:</span></span><span class="ltx_text ltx_font_bold" id="alg2.l1.2" style="font-size:80%;">function</span><span class="ltx_text" id="alg2.l1.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_smallcaps" id="alg2.l1.4" style="font-size:80%;">Find_Optimal_Partitioning_Plan</span><span class="ltx_text" id="alg2.l1.5" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l2.1.1.1" style="font-size:80%;">2:</span></span><span class="ltx_text" id="alg2.l2.2" style="font-size:80%;">  #  </span><math alttext="Mem[num_{shards}][x]" class="ltx_Math" display="inline" id="alg2.l2.m1.2"><semantics id="alg2.l2.m1.2a"><mrow id="alg2.l2.m1.2.2" xref="alg2.l2.m1.2.2.cmml"><mi id="alg2.l2.m1.2.2.3" mathsize="80%" xref="alg2.l2.m1.2.2.3.cmml">M</mi><mo id="alg2.l2.m1.2.2.2" xref="alg2.l2.m1.2.2.2.cmml">⁢</mo><mi id="alg2.l2.m1.2.2.4" mathsize="80%" xref="alg2.l2.m1.2.2.4.cmml">e</mi><mo id="alg2.l2.m1.2.2.2a" xref="alg2.l2.m1.2.2.2.cmml">⁢</mo><mi id="alg2.l2.m1.2.2.5" mathsize="80%" xref="alg2.l2.m1.2.2.5.cmml">m</mi><mo id="alg2.l2.m1.2.2.2b" xref="alg2.l2.m1.2.2.2.cmml">⁢</mo><mrow id="alg2.l2.m1.2.2.1.1" xref="alg2.l2.m1.2.2.1.2.cmml"><mo id="alg2.l2.m1.2.2.1.1.2" maxsize="80%" minsize="80%" xref="alg2.l2.m1.2.2.1.2.1.cmml">[</mo><mrow id="alg2.l2.m1.2.2.1.1.1" xref="alg2.l2.m1.2.2.1.1.1.cmml"><mi id="alg2.l2.m1.2.2.1.1.1.2" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.2.cmml">n</mi><mo id="alg2.l2.m1.2.2.1.1.1.1" xref="alg2.l2.m1.2.2.1.1.1.1.cmml">⁢</mo><mi id="alg2.l2.m1.2.2.1.1.1.3" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.3.cmml">u</mi><mo id="alg2.l2.m1.2.2.1.1.1.1a" xref="alg2.l2.m1.2.2.1.1.1.1.cmml">⁢</mo><msub id="alg2.l2.m1.2.2.1.1.1.4" xref="alg2.l2.m1.2.2.1.1.1.4.cmml"><mi id="alg2.l2.m1.2.2.1.1.1.4.2" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.4.2.cmml">m</mi><mrow id="alg2.l2.m1.2.2.1.1.1.4.3" xref="alg2.l2.m1.2.2.1.1.1.4.3.cmml"><mi id="alg2.l2.m1.2.2.1.1.1.4.3.2" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.4.3.2.cmml">s</mi><mo id="alg2.l2.m1.2.2.1.1.1.4.3.1" xref="alg2.l2.m1.2.2.1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l2.m1.2.2.1.1.1.4.3.3" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.4.3.3.cmml">h</mi><mo id="alg2.l2.m1.2.2.1.1.1.4.3.1a" xref="alg2.l2.m1.2.2.1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l2.m1.2.2.1.1.1.4.3.4" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.4.3.4.cmml">a</mi><mo id="alg2.l2.m1.2.2.1.1.1.4.3.1b" xref="alg2.l2.m1.2.2.1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l2.m1.2.2.1.1.1.4.3.5" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.4.3.5.cmml">r</mi><mo id="alg2.l2.m1.2.2.1.1.1.4.3.1c" xref="alg2.l2.m1.2.2.1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l2.m1.2.2.1.1.1.4.3.6" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.4.3.6.cmml">d</mi><mo id="alg2.l2.m1.2.2.1.1.1.4.3.1d" xref="alg2.l2.m1.2.2.1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l2.m1.2.2.1.1.1.4.3.7" mathsize="80%" xref="alg2.l2.m1.2.2.1.1.1.4.3.7.cmml">s</mi></mrow></msub></mrow><mo id="alg2.l2.m1.2.2.1.1.3" maxsize="80%" minsize="80%" xref="alg2.l2.m1.2.2.1.2.1.cmml">]</mo></mrow><mo id="alg2.l2.m1.2.2.2c" xref="alg2.l2.m1.2.2.2.cmml">⁢</mo><mrow id="alg2.l2.m1.2.2.6.2" xref="alg2.l2.m1.2.2.6.1.cmml"><mo id="alg2.l2.m1.2.2.6.2.1" maxsize="80%" minsize="80%" xref="alg2.l2.m1.2.2.6.1.1.cmml">[</mo><mi id="alg2.l2.m1.1.1" mathsize="80%" xref="alg2.l2.m1.1.1.cmml">x</mi><mo id="alg2.l2.m1.2.2.6.2.2" maxsize="80%" minsize="80%" xref="alg2.l2.m1.2.2.6.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l2.m1.2b"><apply id="alg2.l2.m1.2.2.cmml" xref="alg2.l2.m1.2.2"><times id="alg2.l2.m1.2.2.2.cmml" xref="alg2.l2.m1.2.2.2"></times><ci id="alg2.l2.m1.2.2.3.cmml" xref="alg2.l2.m1.2.2.3">𝑀</ci><ci id="alg2.l2.m1.2.2.4.cmml" xref="alg2.l2.m1.2.2.4">𝑒</ci><ci id="alg2.l2.m1.2.2.5.cmml" xref="alg2.l2.m1.2.2.5">𝑚</ci><apply id="alg2.l2.m1.2.2.1.2.cmml" xref="alg2.l2.m1.2.2.1.1"><csymbol cd="latexml" id="alg2.l2.m1.2.2.1.2.1.cmml" xref="alg2.l2.m1.2.2.1.1.2">delimited-[]</csymbol><apply id="alg2.l2.m1.2.2.1.1.1.cmml" xref="alg2.l2.m1.2.2.1.1.1"><times id="alg2.l2.m1.2.2.1.1.1.1.cmml" xref="alg2.l2.m1.2.2.1.1.1.1"></times><ci id="alg2.l2.m1.2.2.1.1.1.2.cmml" xref="alg2.l2.m1.2.2.1.1.1.2">𝑛</ci><ci id="alg2.l2.m1.2.2.1.1.1.3.cmml" xref="alg2.l2.m1.2.2.1.1.1.3">𝑢</ci><apply id="alg2.l2.m1.2.2.1.1.1.4.cmml" xref="alg2.l2.m1.2.2.1.1.1.4"><csymbol cd="ambiguous" id="alg2.l2.m1.2.2.1.1.1.4.1.cmml" xref="alg2.l2.m1.2.2.1.1.1.4">subscript</csymbol><ci id="alg2.l2.m1.2.2.1.1.1.4.2.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.2">𝑚</ci><apply id="alg2.l2.m1.2.2.1.1.1.4.3.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.3"><times id="alg2.l2.m1.2.2.1.1.1.4.3.1.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.3.1"></times><ci id="alg2.l2.m1.2.2.1.1.1.4.3.2.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.3.2">𝑠</ci><ci id="alg2.l2.m1.2.2.1.1.1.4.3.3.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.3.3">ℎ</ci><ci id="alg2.l2.m1.2.2.1.1.1.4.3.4.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.3.4">𝑎</ci><ci id="alg2.l2.m1.2.2.1.1.1.4.3.5.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.3.5">𝑟</ci><ci id="alg2.l2.m1.2.2.1.1.1.4.3.6.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.3.6">𝑑</ci><ci id="alg2.l2.m1.2.2.1.1.1.4.3.7.cmml" xref="alg2.l2.m1.2.2.1.1.1.4.3.7">𝑠</ci></apply></apply></apply></apply><apply id="alg2.l2.m1.2.2.6.1.cmml" xref="alg2.l2.m1.2.2.6.2"><csymbol cd="latexml" id="alg2.l2.m1.2.2.6.1.1.cmml" xref="alg2.l2.m1.2.2.6.2.1">delimited-[]</csymbol><ci id="alg2.l2.m1.1.1.cmml" xref="alg2.l2.m1.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l2.m1.2c">Mem[num_{shards}][x]</annotation><annotation encoding="application/x-llamapun" id="alg2.l2.m1.2d">italic_M italic_e italic_m [ italic_n italic_u italic_m start_POSTSUBSCRIPT italic_s italic_h italic_a italic_r italic_d italic_s end_POSTSUBSCRIPT ] [ italic_x ]</annotation></semantics></math><span class="ltx_text" id="alg2.l2.3" style="font-size:80%;">: The smallest
memory cost when partitioning
</span>
</div>
<div class="ltx_listingline" id="alg2.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l3.1.1.1" style="font-size:80%;">3:</span></span><span class="ltx_text" id="alg2.l3.2" style="font-size:80%;">   the table </span><math alttext="E^{\prime}" class="ltx_Math" display="inline" id="alg2.l3.m1.1"><semantics id="alg2.l3.m1.1a"><msup id="alg2.l3.m1.1.1" xref="alg2.l3.m1.1.1.cmml"><mi id="alg2.l3.m1.1.1.2" mathsize="80%" xref="alg2.l3.m1.1.1.2.cmml">E</mi><mo id="alg2.l3.m1.1.1.3" mathsize="80%" xref="alg2.l3.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="alg2.l3.m1.1b"><apply id="alg2.l3.m1.1.1.cmml" xref="alg2.l3.m1.1.1"><csymbol cd="ambiguous" id="alg2.l3.m1.1.1.1.cmml" xref="alg2.l3.m1.1.1">superscript</csymbol><ci id="alg2.l3.m1.1.1.2.cmml" xref="alg2.l3.m1.1.1.2">𝐸</ci><ci id="alg2.l3.m1.1.1.3.cmml" xref="alg2.l3.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.m1.1c">E^{\prime}</annotation><annotation encoding="application/x-llamapun" id="alg2.l3.m1.1d">italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l3.3" style="font-size:80%;"> including </span><math alttext="x" class="ltx_Math" display="inline" id="alg2.l3.m2.1"><semantics id="alg2.l3.m2.1a"><mi id="alg2.l3.m2.1.1" mathsize="80%" xref="alg2.l3.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="alg2.l3.m2.1b"><ci id="alg2.l3.m2.1.1.cmml" xref="alg2.l3.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="alg2.l3.m2.1d">italic_x</annotation></semantics></math><span class="ltx_text" id="alg2.l3.4" style="font-size:80%;"> most hot embeddings to </span><math alttext="num_{shards}" class="ltx_Math" display="inline" id="alg2.l3.m3.1"><semantics id="alg2.l3.m3.1a"><mrow id="alg2.l3.m3.1.1" xref="alg2.l3.m3.1.1.cmml"><mi id="alg2.l3.m3.1.1.2" mathsize="80%" xref="alg2.l3.m3.1.1.2.cmml">n</mi><mo id="alg2.l3.m3.1.1.1" xref="alg2.l3.m3.1.1.1.cmml">⁢</mo><mi id="alg2.l3.m3.1.1.3" mathsize="80%" xref="alg2.l3.m3.1.1.3.cmml">u</mi><mo id="alg2.l3.m3.1.1.1a" xref="alg2.l3.m3.1.1.1.cmml">⁢</mo><msub id="alg2.l3.m3.1.1.4" xref="alg2.l3.m3.1.1.4.cmml"><mi id="alg2.l3.m3.1.1.4.2" mathsize="80%" xref="alg2.l3.m3.1.1.4.2.cmml">m</mi><mrow id="alg2.l3.m3.1.1.4.3" xref="alg2.l3.m3.1.1.4.3.cmml"><mi id="alg2.l3.m3.1.1.4.3.2" mathsize="80%" xref="alg2.l3.m3.1.1.4.3.2.cmml">s</mi><mo id="alg2.l3.m3.1.1.4.3.1" xref="alg2.l3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l3.m3.1.1.4.3.3" mathsize="80%" xref="alg2.l3.m3.1.1.4.3.3.cmml">h</mi><mo id="alg2.l3.m3.1.1.4.3.1a" xref="alg2.l3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l3.m3.1.1.4.3.4" mathsize="80%" xref="alg2.l3.m3.1.1.4.3.4.cmml">a</mi><mo id="alg2.l3.m3.1.1.4.3.1b" xref="alg2.l3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l3.m3.1.1.4.3.5" mathsize="80%" xref="alg2.l3.m3.1.1.4.3.5.cmml">r</mi><mo id="alg2.l3.m3.1.1.4.3.1c" xref="alg2.l3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l3.m3.1.1.4.3.6" mathsize="80%" xref="alg2.l3.m3.1.1.4.3.6.cmml">d</mi><mo id="alg2.l3.m3.1.1.4.3.1d" xref="alg2.l3.m3.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l3.m3.1.1.4.3.7" mathsize="80%" xref="alg2.l3.m3.1.1.4.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.l3.m3.1b"><apply id="alg2.l3.m3.1.1.cmml" xref="alg2.l3.m3.1.1"><times id="alg2.l3.m3.1.1.1.cmml" xref="alg2.l3.m3.1.1.1"></times><ci id="alg2.l3.m3.1.1.2.cmml" xref="alg2.l3.m3.1.1.2">𝑛</ci><ci id="alg2.l3.m3.1.1.3.cmml" xref="alg2.l3.m3.1.1.3">𝑢</ci><apply id="alg2.l3.m3.1.1.4.cmml" xref="alg2.l3.m3.1.1.4"><csymbol cd="ambiguous" id="alg2.l3.m3.1.1.4.1.cmml" xref="alg2.l3.m3.1.1.4">subscript</csymbol><ci id="alg2.l3.m3.1.1.4.2.cmml" xref="alg2.l3.m3.1.1.4.2">𝑚</ci><apply id="alg2.l3.m3.1.1.4.3.cmml" xref="alg2.l3.m3.1.1.4.3"><times id="alg2.l3.m3.1.1.4.3.1.cmml" xref="alg2.l3.m3.1.1.4.3.1"></times><ci id="alg2.l3.m3.1.1.4.3.2.cmml" xref="alg2.l3.m3.1.1.4.3.2">𝑠</ci><ci id="alg2.l3.m3.1.1.4.3.3.cmml" xref="alg2.l3.m3.1.1.4.3.3">ℎ</ci><ci id="alg2.l3.m3.1.1.4.3.4.cmml" xref="alg2.l3.m3.1.1.4.3.4">𝑎</ci><ci id="alg2.l3.m3.1.1.4.3.5.cmml" xref="alg2.l3.m3.1.1.4.3.5">𝑟</ci><ci id="alg2.l3.m3.1.1.4.3.6.cmml" xref="alg2.l3.m3.1.1.4.3.6">𝑑</ci><ci id="alg2.l3.m3.1.1.4.3.7.cmml" xref="alg2.l3.m3.1.1.4.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.m3.1c">num_{shards}</annotation><annotation encoding="application/x-llamapun" id="alg2.l3.m3.1d">italic_n italic_u italic_m start_POSTSUBSCRIPT italic_s italic_h italic_a italic_r italic_d italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l3.5" style="font-size:80%;"> shards
</span>
</div>
<div class="ltx_listingline" id="alg2.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l4.1.1.1" style="font-size:80%;">4:</span></span><span class="ltx_text" id="alg2.l4.2" style="font-size:80%;">  #  </span><math alttext="COST(start_{ID},end_{ID})" class="ltx_Math" display="inline" id="alg2.l4.m1.2"><semantics id="alg2.l4.m1.2a"><mrow id="alg2.l4.m1.2.2" xref="alg2.l4.m1.2.2.cmml"><mi id="alg2.l4.m1.2.2.4" mathsize="80%" xref="alg2.l4.m1.2.2.4.cmml">C</mi><mo id="alg2.l4.m1.2.2.3" xref="alg2.l4.m1.2.2.3.cmml">⁢</mo><mi id="alg2.l4.m1.2.2.5" mathsize="80%" xref="alg2.l4.m1.2.2.5.cmml">O</mi><mo id="alg2.l4.m1.2.2.3a" xref="alg2.l4.m1.2.2.3.cmml">⁢</mo><mi id="alg2.l4.m1.2.2.6" mathsize="80%" xref="alg2.l4.m1.2.2.6.cmml">S</mi><mo id="alg2.l4.m1.2.2.3b" xref="alg2.l4.m1.2.2.3.cmml">⁢</mo><mi id="alg2.l4.m1.2.2.7" mathsize="80%" xref="alg2.l4.m1.2.2.7.cmml">T</mi><mo id="alg2.l4.m1.2.2.3c" xref="alg2.l4.m1.2.2.3.cmml">⁢</mo><mrow id="alg2.l4.m1.2.2.2.2" xref="alg2.l4.m1.2.2.2.3.cmml"><mo id="alg2.l4.m1.2.2.2.2.3" maxsize="80%" minsize="80%" xref="alg2.l4.m1.2.2.2.3.cmml">(</mo><mrow id="alg2.l4.m1.1.1.1.1.1" xref="alg2.l4.m1.1.1.1.1.1.cmml"><mi id="alg2.l4.m1.1.1.1.1.1.2" mathsize="80%" xref="alg2.l4.m1.1.1.1.1.1.2.cmml">s</mi><mo id="alg2.l4.m1.1.1.1.1.1.1" xref="alg2.l4.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg2.l4.m1.1.1.1.1.1.3" mathsize="80%" xref="alg2.l4.m1.1.1.1.1.1.3.cmml">t</mi><mo id="alg2.l4.m1.1.1.1.1.1.1a" xref="alg2.l4.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg2.l4.m1.1.1.1.1.1.4" mathsize="80%" xref="alg2.l4.m1.1.1.1.1.1.4.cmml">a</mi><mo id="alg2.l4.m1.1.1.1.1.1.1b" xref="alg2.l4.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg2.l4.m1.1.1.1.1.1.5" mathsize="80%" xref="alg2.l4.m1.1.1.1.1.1.5.cmml">r</mi><mo id="alg2.l4.m1.1.1.1.1.1.1c" xref="alg2.l4.m1.1.1.1.1.1.1.cmml">⁢</mo><msub id="alg2.l4.m1.1.1.1.1.1.6" xref="alg2.l4.m1.1.1.1.1.1.6.cmml"><mi id="alg2.l4.m1.1.1.1.1.1.6.2" mathsize="80%" xref="alg2.l4.m1.1.1.1.1.1.6.2.cmml">t</mi><mrow id="alg2.l4.m1.1.1.1.1.1.6.3" xref="alg2.l4.m1.1.1.1.1.1.6.3.cmml"><mi id="alg2.l4.m1.1.1.1.1.1.6.3.2" mathsize="80%" xref="alg2.l4.m1.1.1.1.1.1.6.3.2.cmml">I</mi><mo id="alg2.l4.m1.1.1.1.1.1.6.3.1" xref="alg2.l4.m1.1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="alg2.l4.m1.1.1.1.1.1.6.3.3" mathsize="80%" xref="alg2.l4.m1.1.1.1.1.1.6.3.3.cmml">D</mi></mrow></msub></mrow><mo id="alg2.l4.m1.2.2.2.2.4" mathsize="80%" xref="alg2.l4.m1.2.2.2.3.cmml">,</mo><mrow id="alg2.l4.m1.2.2.2.2.2" xref="alg2.l4.m1.2.2.2.2.2.cmml"><mi id="alg2.l4.m1.2.2.2.2.2.2" mathsize="80%" xref="alg2.l4.m1.2.2.2.2.2.2.cmml">e</mi><mo id="alg2.l4.m1.2.2.2.2.2.1" xref="alg2.l4.m1.2.2.2.2.2.1.cmml">⁢</mo><mi id="alg2.l4.m1.2.2.2.2.2.3" mathsize="80%" xref="alg2.l4.m1.2.2.2.2.2.3.cmml">n</mi><mo id="alg2.l4.m1.2.2.2.2.2.1a" xref="alg2.l4.m1.2.2.2.2.2.1.cmml">⁢</mo><msub id="alg2.l4.m1.2.2.2.2.2.4" xref="alg2.l4.m1.2.2.2.2.2.4.cmml"><mi id="alg2.l4.m1.2.2.2.2.2.4.2" mathsize="80%" xref="alg2.l4.m1.2.2.2.2.2.4.2.cmml">d</mi><mrow id="alg2.l4.m1.2.2.2.2.2.4.3" xref="alg2.l4.m1.2.2.2.2.2.4.3.cmml"><mi id="alg2.l4.m1.2.2.2.2.2.4.3.2" mathsize="80%" xref="alg2.l4.m1.2.2.2.2.2.4.3.2.cmml">I</mi><mo id="alg2.l4.m1.2.2.2.2.2.4.3.1" xref="alg2.l4.m1.2.2.2.2.2.4.3.1.cmml">⁢</mo><mi id="alg2.l4.m1.2.2.2.2.2.4.3.3" mathsize="80%" xref="alg2.l4.m1.2.2.2.2.2.4.3.3.cmml">D</mi></mrow></msub></mrow><mo id="alg2.l4.m1.2.2.2.2.5" maxsize="80%" minsize="80%" xref="alg2.l4.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l4.m1.2b"><apply id="alg2.l4.m1.2.2.cmml" xref="alg2.l4.m1.2.2"><times id="alg2.l4.m1.2.2.3.cmml" xref="alg2.l4.m1.2.2.3"></times><ci id="alg2.l4.m1.2.2.4.cmml" xref="alg2.l4.m1.2.2.4">𝐶</ci><ci id="alg2.l4.m1.2.2.5.cmml" xref="alg2.l4.m1.2.2.5">𝑂</ci><ci id="alg2.l4.m1.2.2.6.cmml" xref="alg2.l4.m1.2.2.6">𝑆</ci><ci id="alg2.l4.m1.2.2.7.cmml" xref="alg2.l4.m1.2.2.7">𝑇</ci><interval closure="open" id="alg2.l4.m1.2.2.2.3.cmml" xref="alg2.l4.m1.2.2.2.2"><apply id="alg2.l4.m1.1.1.1.1.1.cmml" xref="alg2.l4.m1.1.1.1.1.1"><times id="alg2.l4.m1.1.1.1.1.1.1.cmml" xref="alg2.l4.m1.1.1.1.1.1.1"></times><ci id="alg2.l4.m1.1.1.1.1.1.2.cmml" xref="alg2.l4.m1.1.1.1.1.1.2">𝑠</ci><ci id="alg2.l4.m1.1.1.1.1.1.3.cmml" xref="alg2.l4.m1.1.1.1.1.1.3">𝑡</ci><ci id="alg2.l4.m1.1.1.1.1.1.4.cmml" xref="alg2.l4.m1.1.1.1.1.1.4">𝑎</ci><ci id="alg2.l4.m1.1.1.1.1.1.5.cmml" xref="alg2.l4.m1.1.1.1.1.1.5">𝑟</ci><apply id="alg2.l4.m1.1.1.1.1.1.6.cmml" xref="alg2.l4.m1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="alg2.l4.m1.1.1.1.1.1.6.1.cmml" xref="alg2.l4.m1.1.1.1.1.1.6">subscript</csymbol><ci id="alg2.l4.m1.1.1.1.1.1.6.2.cmml" xref="alg2.l4.m1.1.1.1.1.1.6.2">𝑡</ci><apply id="alg2.l4.m1.1.1.1.1.1.6.3.cmml" xref="alg2.l4.m1.1.1.1.1.1.6.3"><times id="alg2.l4.m1.1.1.1.1.1.6.3.1.cmml" xref="alg2.l4.m1.1.1.1.1.1.6.3.1"></times><ci id="alg2.l4.m1.1.1.1.1.1.6.3.2.cmml" xref="alg2.l4.m1.1.1.1.1.1.6.3.2">𝐼</ci><ci id="alg2.l4.m1.1.1.1.1.1.6.3.3.cmml" xref="alg2.l4.m1.1.1.1.1.1.6.3.3">𝐷</ci></apply></apply></apply><apply id="alg2.l4.m1.2.2.2.2.2.cmml" xref="alg2.l4.m1.2.2.2.2.2"><times id="alg2.l4.m1.2.2.2.2.2.1.cmml" xref="alg2.l4.m1.2.2.2.2.2.1"></times><ci id="alg2.l4.m1.2.2.2.2.2.2.cmml" xref="alg2.l4.m1.2.2.2.2.2.2">𝑒</ci><ci id="alg2.l4.m1.2.2.2.2.2.3.cmml" xref="alg2.l4.m1.2.2.2.2.2.3">𝑛</ci><apply id="alg2.l4.m1.2.2.2.2.2.4.cmml" xref="alg2.l4.m1.2.2.2.2.2.4"><csymbol cd="ambiguous" id="alg2.l4.m1.2.2.2.2.2.4.1.cmml" xref="alg2.l4.m1.2.2.2.2.2.4">subscript</csymbol><ci id="alg2.l4.m1.2.2.2.2.2.4.2.cmml" xref="alg2.l4.m1.2.2.2.2.2.4.2">𝑑</ci><apply id="alg2.l4.m1.2.2.2.2.2.4.3.cmml" xref="alg2.l4.m1.2.2.2.2.2.4.3"><times id="alg2.l4.m1.2.2.2.2.2.4.3.1.cmml" xref="alg2.l4.m1.2.2.2.2.2.4.3.1"></times><ci id="alg2.l4.m1.2.2.2.2.2.4.3.2.cmml" xref="alg2.l4.m1.2.2.2.2.2.4.3.2">𝐼</ci><ci id="alg2.l4.m1.2.2.2.2.2.4.3.3.cmml" xref="alg2.l4.m1.2.2.2.2.2.4.3.3">𝐷</ci></apply></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l4.m1.2c">COST(start_{ID},end_{ID})</annotation><annotation encoding="application/x-llamapun" id="alg2.l4.m1.2d">italic_C italic_O italic_S italic_T ( italic_s italic_t italic_a italic_r italic_t start_POSTSUBSCRIPT italic_I italic_D end_POSTSUBSCRIPT , italic_e italic_n italic_d start_POSTSUBSCRIPT italic_I italic_D end_POSTSUBSCRIPT )</annotation></semantics></math><span class="ltx_text" id="alg2.l4.3" style="font-size:80%;">: Expected memory consumption of a
</span>
</div>
<div class="ltx_listingline" id="alg2.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l5.1.1.1" style="font-size:80%;">5:</span></span><span class="ltx_text" id="alg2.l5.2" style="font-size:80%;">   shard that contains embeddings with ID from </span><math alttext="start_{ID}" class="ltx_Math" display="inline" id="alg2.l5.m1.1"><semantics id="alg2.l5.m1.1a"><mrow id="alg2.l5.m1.1.1" xref="alg2.l5.m1.1.1.cmml"><mi id="alg2.l5.m1.1.1.2" mathsize="80%" xref="alg2.l5.m1.1.1.2.cmml">s</mi><mo id="alg2.l5.m1.1.1.1" xref="alg2.l5.m1.1.1.1.cmml">⁢</mo><mi id="alg2.l5.m1.1.1.3" mathsize="80%" xref="alg2.l5.m1.1.1.3.cmml">t</mi><mo id="alg2.l5.m1.1.1.1a" xref="alg2.l5.m1.1.1.1.cmml">⁢</mo><mi id="alg2.l5.m1.1.1.4" mathsize="80%" xref="alg2.l5.m1.1.1.4.cmml">a</mi><mo id="alg2.l5.m1.1.1.1b" xref="alg2.l5.m1.1.1.1.cmml">⁢</mo><mi id="alg2.l5.m1.1.1.5" mathsize="80%" xref="alg2.l5.m1.1.1.5.cmml">r</mi><mo id="alg2.l5.m1.1.1.1c" xref="alg2.l5.m1.1.1.1.cmml">⁢</mo><msub id="alg2.l5.m1.1.1.6" xref="alg2.l5.m1.1.1.6.cmml"><mi id="alg2.l5.m1.1.1.6.2" mathsize="80%" xref="alg2.l5.m1.1.1.6.2.cmml">t</mi><mrow id="alg2.l5.m1.1.1.6.3" xref="alg2.l5.m1.1.1.6.3.cmml"><mi id="alg2.l5.m1.1.1.6.3.2" mathsize="80%" xref="alg2.l5.m1.1.1.6.3.2.cmml">I</mi><mo id="alg2.l5.m1.1.1.6.3.1" xref="alg2.l5.m1.1.1.6.3.1.cmml">⁢</mo><mi id="alg2.l5.m1.1.1.6.3.3" mathsize="80%" xref="alg2.l5.m1.1.1.6.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.l5.m1.1b"><apply id="alg2.l5.m1.1.1.cmml" xref="alg2.l5.m1.1.1"><times id="alg2.l5.m1.1.1.1.cmml" xref="alg2.l5.m1.1.1.1"></times><ci id="alg2.l5.m1.1.1.2.cmml" xref="alg2.l5.m1.1.1.2">𝑠</ci><ci id="alg2.l5.m1.1.1.3.cmml" xref="alg2.l5.m1.1.1.3">𝑡</ci><ci id="alg2.l5.m1.1.1.4.cmml" xref="alg2.l5.m1.1.1.4">𝑎</ci><ci id="alg2.l5.m1.1.1.5.cmml" xref="alg2.l5.m1.1.1.5">𝑟</ci><apply id="alg2.l5.m1.1.1.6.cmml" xref="alg2.l5.m1.1.1.6"><csymbol cd="ambiguous" id="alg2.l5.m1.1.1.6.1.cmml" xref="alg2.l5.m1.1.1.6">subscript</csymbol><ci id="alg2.l5.m1.1.1.6.2.cmml" xref="alg2.l5.m1.1.1.6.2">𝑡</ci><apply id="alg2.l5.m1.1.1.6.3.cmml" xref="alg2.l5.m1.1.1.6.3"><times id="alg2.l5.m1.1.1.6.3.1.cmml" xref="alg2.l5.m1.1.1.6.3.1"></times><ci id="alg2.l5.m1.1.1.6.3.2.cmml" xref="alg2.l5.m1.1.1.6.3.2">𝐼</ci><ci id="alg2.l5.m1.1.1.6.3.3.cmml" xref="alg2.l5.m1.1.1.6.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l5.m1.1c">start_{ID}</annotation><annotation encoding="application/x-llamapun" id="alg2.l5.m1.1d">italic_s italic_t italic_a italic_r italic_t start_POSTSUBSCRIPT italic_I italic_D end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l5.3" style="font-size:80%;"> to </span><math alttext="end_{ID}" class="ltx_Math" display="inline" id="alg2.l5.m2.1"><semantics id="alg2.l5.m2.1a"><mrow id="alg2.l5.m2.1.1" xref="alg2.l5.m2.1.1.cmml"><mi id="alg2.l5.m2.1.1.2" mathsize="80%" xref="alg2.l5.m2.1.1.2.cmml">e</mi><mo id="alg2.l5.m2.1.1.1" xref="alg2.l5.m2.1.1.1.cmml">⁢</mo><mi id="alg2.l5.m2.1.1.3" mathsize="80%" xref="alg2.l5.m2.1.1.3.cmml">n</mi><mo id="alg2.l5.m2.1.1.1a" xref="alg2.l5.m2.1.1.1.cmml">⁢</mo><msub id="alg2.l5.m2.1.1.4" xref="alg2.l5.m2.1.1.4.cmml"><mi id="alg2.l5.m2.1.1.4.2" mathsize="80%" xref="alg2.l5.m2.1.1.4.2.cmml">d</mi><mrow id="alg2.l5.m2.1.1.4.3" xref="alg2.l5.m2.1.1.4.3.cmml"><mi id="alg2.l5.m2.1.1.4.3.2" mathsize="80%" xref="alg2.l5.m2.1.1.4.3.2.cmml">I</mi><mo id="alg2.l5.m2.1.1.4.3.1" xref="alg2.l5.m2.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l5.m2.1.1.4.3.3" mathsize="80%" xref="alg2.l5.m2.1.1.4.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.l5.m2.1b"><apply id="alg2.l5.m2.1.1.cmml" xref="alg2.l5.m2.1.1"><times id="alg2.l5.m2.1.1.1.cmml" xref="alg2.l5.m2.1.1.1"></times><ci id="alg2.l5.m2.1.1.2.cmml" xref="alg2.l5.m2.1.1.2">𝑒</ci><ci id="alg2.l5.m2.1.1.3.cmml" xref="alg2.l5.m2.1.1.3">𝑛</ci><apply id="alg2.l5.m2.1.1.4.cmml" xref="alg2.l5.m2.1.1.4"><csymbol cd="ambiguous" id="alg2.l5.m2.1.1.4.1.cmml" xref="alg2.l5.m2.1.1.4">subscript</csymbol><ci id="alg2.l5.m2.1.1.4.2.cmml" xref="alg2.l5.m2.1.1.4.2">𝑑</ci><apply id="alg2.l5.m2.1.1.4.3.cmml" xref="alg2.l5.m2.1.1.4.3"><times id="alg2.l5.m2.1.1.4.3.1.cmml" xref="alg2.l5.m2.1.1.4.3.1"></times><ci id="alg2.l5.m2.1.1.4.3.2.cmml" xref="alg2.l5.m2.1.1.4.3.2">𝐼</ci><ci id="alg2.l5.m2.1.1.4.3.3.cmml" xref="alg2.l5.m2.1.1.4.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l5.m2.1c">end_{ID}</annotation><annotation encoding="application/x-llamapun" id="alg2.l5.m2.1d">italic_e italic_n italic_d start_POSTSUBSCRIPT italic_I italic_D end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l5.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l6.1.1.1" style="font-size:80%;">6:</span></span><span class="ltx_text" id="alg2.l6.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_bold" id="alg2.l6.3" style="font-size:80%;">for</span><span class="ltx_text" id="alg2.l6.4" style="font-size:80%;"> </span><math alttext="end_{ID}" class="ltx_Math" display="inline" id="alg2.l6.m1.1"><semantics id="alg2.l6.m1.1a"><mrow id="alg2.l6.m1.1.1" xref="alg2.l6.m1.1.1.cmml"><mi id="alg2.l6.m1.1.1.2" mathsize="80%" xref="alg2.l6.m1.1.1.2.cmml">e</mi><mo id="alg2.l6.m1.1.1.1" xref="alg2.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg2.l6.m1.1.1.3" mathsize="80%" xref="alg2.l6.m1.1.1.3.cmml">n</mi><mo id="alg2.l6.m1.1.1.1a" xref="alg2.l6.m1.1.1.1.cmml">⁢</mo><msub id="alg2.l6.m1.1.1.4" xref="alg2.l6.m1.1.1.4.cmml"><mi id="alg2.l6.m1.1.1.4.2" mathsize="80%" xref="alg2.l6.m1.1.1.4.2.cmml">d</mi><mrow id="alg2.l6.m1.1.1.4.3" xref="alg2.l6.m1.1.1.4.3.cmml"><mi id="alg2.l6.m1.1.1.4.3.2" mathsize="80%" xref="alg2.l6.m1.1.1.4.3.2.cmml">I</mi><mo id="alg2.l6.m1.1.1.4.3.1" xref="alg2.l6.m1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l6.m1.1.1.4.3.3" mathsize="80%" xref="alg2.l6.m1.1.1.4.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.l6.m1.1b"><apply id="alg2.l6.m1.1.1.cmml" xref="alg2.l6.m1.1.1"><times id="alg2.l6.m1.1.1.1.cmml" xref="alg2.l6.m1.1.1.1"></times><ci id="alg2.l6.m1.1.1.2.cmml" xref="alg2.l6.m1.1.1.2">𝑒</ci><ci id="alg2.l6.m1.1.1.3.cmml" xref="alg2.l6.m1.1.1.3">𝑛</ci><apply id="alg2.l6.m1.1.1.4.cmml" xref="alg2.l6.m1.1.1.4"><csymbol cd="ambiguous" id="alg2.l6.m1.1.1.4.1.cmml" xref="alg2.l6.m1.1.1.4">subscript</csymbol><ci id="alg2.l6.m1.1.1.4.2.cmml" xref="alg2.l6.m1.1.1.4.2">𝑑</ci><apply id="alg2.l6.m1.1.1.4.3.cmml" xref="alg2.l6.m1.1.1.4.3"><times id="alg2.l6.m1.1.1.4.3.1.cmml" xref="alg2.l6.m1.1.1.4.3.1"></times><ci id="alg2.l6.m1.1.1.4.3.2.cmml" xref="alg2.l6.m1.1.1.4.3.2">𝐼</ci><ci id="alg2.l6.m1.1.1.4.3.3.cmml" xref="alg2.l6.m1.1.1.4.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l6.m1.1c">end_{ID}</annotation><annotation encoding="application/x-llamapun" id="alg2.l6.m1.1d">italic_e italic_n italic_d start_POSTSUBSCRIPT italic_I italic_D end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l6.5" style="font-size:80%;"> = 1 to </span><math alttext="N_{max}" class="ltx_Math" display="inline" id="alg2.l6.m2.1"><semantics id="alg2.l6.m2.1a"><msub id="alg2.l6.m2.1.1" xref="alg2.l6.m2.1.1.cmml"><mi id="alg2.l6.m2.1.1.2" mathsize="80%" xref="alg2.l6.m2.1.1.2.cmml">N</mi><mrow id="alg2.l6.m2.1.1.3" xref="alg2.l6.m2.1.1.3.cmml"><mi id="alg2.l6.m2.1.1.3.2" mathsize="80%" xref="alg2.l6.m2.1.1.3.2.cmml">m</mi><mo id="alg2.l6.m2.1.1.3.1" xref="alg2.l6.m2.1.1.3.1.cmml">⁢</mo><mi id="alg2.l6.m2.1.1.3.3" mathsize="80%" xref="alg2.l6.m2.1.1.3.3.cmml">a</mi><mo id="alg2.l6.m2.1.1.3.1a" xref="alg2.l6.m2.1.1.3.1.cmml">⁢</mo><mi id="alg2.l6.m2.1.1.3.4" mathsize="80%" xref="alg2.l6.m2.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg2.l6.m2.1b"><apply id="alg2.l6.m2.1.1.cmml" xref="alg2.l6.m2.1.1"><csymbol cd="ambiguous" id="alg2.l6.m2.1.1.1.cmml" xref="alg2.l6.m2.1.1">subscript</csymbol><ci id="alg2.l6.m2.1.1.2.cmml" xref="alg2.l6.m2.1.1.2">𝑁</ci><apply id="alg2.l6.m2.1.1.3.cmml" xref="alg2.l6.m2.1.1.3"><times id="alg2.l6.m2.1.1.3.1.cmml" xref="alg2.l6.m2.1.1.3.1"></times><ci id="alg2.l6.m2.1.1.3.2.cmml" xref="alg2.l6.m2.1.1.3.2">𝑚</ci><ci id="alg2.l6.m2.1.1.3.3.cmml" xref="alg2.l6.m2.1.1.3.3">𝑎</ci><ci id="alg2.l6.m2.1.1.3.4.cmml" xref="alg2.l6.m2.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l6.m2.1c">N_{max}</annotation><annotation encoding="application/x-llamapun" id="alg2.l6.m2.1d">italic_N start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l6.6" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l6.7" style="font-size:80%;">do</span><span class="ltx_text" id="alg2.l6.8" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l7.1.1.1" style="font-size:80%;">7:</span></span><span class="ltx_text" id="alg2.l7.2" style="font-size:80%;">        Mem[1][end</span><sub class="ltx_sub" id="alg2.l7.3"><span class="ltx_text ltx_font_italic" id="alg2.l7.3.1" style="font-size:80%;">ID</span></sub><span class="ltx_text" id="alg2.l7.4" style="font-size:80%;">] = COST(1, end</span><sub class="ltx_sub" id="alg2.l7.5"><span class="ltx_text ltx_font_italic" id="alg2.l7.5.1" style="font-size:80%;">ID</span></sub><span class="ltx_text" id="alg2.l7.6" style="font-size:80%;">)
</span>
</div>
<div class="ltx_listingline" id="alg2.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l8.1.1.1" style="font-size:80%;">8:</span></span><span class="ltx_text" id="alg2.l8.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_bold" id="alg2.l8.3" style="font-size:80%;">end</span><span class="ltx_text" id="alg2.l8.4" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l8.5" style="font-size:80%;">for</span>
</div>
<div class="ltx_listingline" id="alg2.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l9.1.1.1" style="font-size:80%;">9:</span></span><span class="ltx_text" id="alg2.l9.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_bold" id="alg2.l9.3" style="font-size:80%;">for</span><span class="ltx_text" id="alg2.l9.4" style="font-size:80%;"> </span><math alttext="num_{shards}" class="ltx_Math" display="inline" id="alg2.l9.m1.1"><semantics id="alg2.l9.m1.1a"><mrow id="alg2.l9.m1.1.1" xref="alg2.l9.m1.1.1.cmml"><mi id="alg2.l9.m1.1.1.2" mathsize="80%" xref="alg2.l9.m1.1.1.2.cmml">n</mi><mo id="alg2.l9.m1.1.1.1" xref="alg2.l9.m1.1.1.1.cmml">⁢</mo><mi id="alg2.l9.m1.1.1.3" mathsize="80%" xref="alg2.l9.m1.1.1.3.cmml">u</mi><mo id="alg2.l9.m1.1.1.1a" xref="alg2.l9.m1.1.1.1.cmml">⁢</mo><msub id="alg2.l9.m1.1.1.4" xref="alg2.l9.m1.1.1.4.cmml"><mi id="alg2.l9.m1.1.1.4.2" mathsize="80%" xref="alg2.l9.m1.1.1.4.2.cmml">m</mi><mrow id="alg2.l9.m1.1.1.4.3" xref="alg2.l9.m1.1.1.4.3.cmml"><mi id="alg2.l9.m1.1.1.4.3.2" mathsize="80%" xref="alg2.l9.m1.1.1.4.3.2.cmml">s</mi><mo id="alg2.l9.m1.1.1.4.3.1" xref="alg2.l9.m1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l9.m1.1.1.4.3.3" mathsize="80%" xref="alg2.l9.m1.1.1.4.3.3.cmml">h</mi><mo id="alg2.l9.m1.1.1.4.3.1a" xref="alg2.l9.m1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l9.m1.1.1.4.3.4" mathsize="80%" xref="alg2.l9.m1.1.1.4.3.4.cmml">a</mi><mo id="alg2.l9.m1.1.1.4.3.1b" xref="alg2.l9.m1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l9.m1.1.1.4.3.5" mathsize="80%" xref="alg2.l9.m1.1.1.4.3.5.cmml">r</mi><mo id="alg2.l9.m1.1.1.4.3.1c" xref="alg2.l9.m1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l9.m1.1.1.4.3.6" mathsize="80%" xref="alg2.l9.m1.1.1.4.3.6.cmml">d</mi><mo id="alg2.l9.m1.1.1.4.3.1d" xref="alg2.l9.m1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l9.m1.1.1.4.3.7" mathsize="80%" xref="alg2.l9.m1.1.1.4.3.7.cmml">s</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.l9.m1.1b"><apply id="alg2.l9.m1.1.1.cmml" xref="alg2.l9.m1.1.1"><times id="alg2.l9.m1.1.1.1.cmml" xref="alg2.l9.m1.1.1.1"></times><ci id="alg2.l9.m1.1.1.2.cmml" xref="alg2.l9.m1.1.1.2">𝑛</ci><ci id="alg2.l9.m1.1.1.3.cmml" xref="alg2.l9.m1.1.1.3">𝑢</ci><apply id="alg2.l9.m1.1.1.4.cmml" xref="alg2.l9.m1.1.1.4"><csymbol cd="ambiguous" id="alg2.l9.m1.1.1.4.1.cmml" xref="alg2.l9.m1.1.1.4">subscript</csymbol><ci id="alg2.l9.m1.1.1.4.2.cmml" xref="alg2.l9.m1.1.1.4.2">𝑚</ci><apply id="alg2.l9.m1.1.1.4.3.cmml" xref="alg2.l9.m1.1.1.4.3"><times id="alg2.l9.m1.1.1.4.3.1.cmml" xref="alg2.l9.m1.1.1.4.3.1"></times><ci id="alg2.l9.m1.1.1.4.3.2.cmml" xref="alg2.l9.m1.1.1.4.3.2">𝑠</ci><ci id="alg2.l9.m1.1.1.4.3.3.cmml" xref="alg2.l9.m1.1.1.4.3.3">ℎ</ci><ci id="alg2.l9.m1.1.1.4.3.4.cmml" xref="alg2.l9.m1.1.1.4.3.4">𝑎</ci><ci id="alg2.l9.m1.1.1.4.3.5.cmml" xref="alg2.l9.m1.1.1.4.3.5">𝑟</ci><ci id="alg2.l9.m1.1.1.4.3.6.cmml" xref="alg2.l9.m1.1.1.4.3.6">𝑑</ci><ci id="alg2.l9.m1.1.1.4.3.7.cmml" xref="alg2.l9.m1.1.1.4.3.7">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l9.m1.1c">num_{shards}</annotation><annotation encoding="application/x-llamapun" id="alg2.l9.m1.1d">italic_n italic_u italic_m start_POSTSUBSCRIPT italic_s italic_h italic_a italic_r italic_d italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l9.5" style="font-size:80%;"> = 2 to </span><math alttext="S_{max}" class="ltx_Math" display="inline" id="alg2.l9.m2.1"><semantics id="alg2.l9.m2.1a"><msub id="alg2.l9.m2.1.1" xref="alg2.l9.m2.1.1.cmml"><mi id="alg2.l9.m2.1.1.2" mathsize="80%" xref="alg2.l9.m2.1.1.2.cmml">S</mi><mrow id="alg2.l9.m2.1.1.3" xref="alg2.l9.m2.1.1.3.cmml"><mi id="alg2.l9.m2.1.1.3.2" mathsize="80%" xref="alg2.l9.m2.1.1.3.2.cmml">m</mi><mo id="alg2.l9.m2.1.1.3.1" xref="alg2.l9.m2.1.1.3.1.cmml">⁢</mo><mi id="alg2.l9.m2.1.1.3.3" mathsize="80%" xref="alg2.l9.m2.1.1.3.3.cmml">a</mi><mo id="alg2.l9.m2.1.1.3.1a" xref="alg2.l9.m2.1.1.3.1.cmml">⁢</mo><mi id="alg2.l9.m2.1.1.3.4" mathsize="80%" xref="alg2.l9.m2.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg2.l9.m2.1b"><apply id="alg2.l9.m2.1.1.cmml" xref="alg2.l9.m2.1.1"><csymbol cd="ambiguous" id="alg2.l9.m2.1.1.1.cmml" xref="alg2.l9.m2.1.1">subscript</csymbol><ci id="alg2.l9.m2.1.1.2.cmml" xref="alg2.l9.m2.1.1.2">𝑆</ci><apply id="alg2.l9.m2.1.1.3.cmml" xref="alg2.l9.m2.1.1.3"><times id="alg2.l9.m2.1.1.3.1.cmml" xref="alg2.l9.m2.1.1.3.1"></times><ci id="alg2.l9.m2.1.1.3.2.cmml" xref="alg2.l9.m2.1.1.3.2">𝑚</ci><ci id="alg2.l9.m2.1.1.3.3.cmml" xref="alg2.l9.m2.1.1.3.3">𝑎</ci><ci id="alg2.l9.m2.1.1.3.4.cmml" xref="alg2.l9.m2.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l9.m2.1c">S_{max}</annotation><annotation encoding="application/x-llamapun" id="alg2.l9.m2.1d">italic_S start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l9.6" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l9.7" style="font-size:80%;">do</span><span class="ltx_text" id="alg2.l9.8" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l10.1.1.1" style="font-size:80%;">10:</span></span><span class="ltx_text" id="alg2.l10.2" style="font-size:80%;">        </span><span class="ltx_text ltx_font_bold" id="alg2.l10.3" style="font-size:80%;">for</span><span class="ltx_text" id="alg2.l10.4" style="font-size:80%;"> </span><math alttext="end_{ID}" class="ltx_Math" display="inline" id="alg2.l10.m1.1"><semantics id="alg2.l10.m1.1a"><mrow id="alg2.l10.m1.1.1" xref="alg2.l10.m1.1.1.cmml"><mi id="alg2.l10.m1.1.1.2" mathsize="80%" xref="alg2.l10.m1.1.1.2.cmml">e</mi><mo id="alg2.l10.m1.1.1.1" xref="alg2.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg2.l10.m1.1.1.3" mathsize="80%" xref="alg2.l10.m1.1.1.3.cmml">n</mi><mo id="alg2.l10.m1.1.1.1a" xref="alg2.l10.m1.1.1.1.cmml">⁢</mo><msub id="alg2.l10.m1.1.1.4" xref="alg2.l10.m1.1.1.4.cmml"><mi id="alg2.l10.m1.1.1.4.2" mathsize="80%" xref="alg2.l10.m1.1.1.4.2.cmml">d</mi><mrow id="alg2.l10.m1.1.1.4.3" xref="alg2.l10.m1.1.1.4.3.cmml"><mi id="alg2.l10.m1.1.1.4.3.2" mathsize="80%" xref="alg2.l10.m1.1.1.4.3.2.cmml">I</mi><mo id="alg2.l10.m1.1.1.4.3.1" xref="alg2.l10.m1.1.1.4.3.1.cmml">⁢</mo><mi id="alg2.l10.m1.1.1.4.3.3" mathsize="80%" xref="alg2.l10.m1.1.1.4.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.l10.m1.1b"><apply id="alg2.l10.m1.1.1.cmml" xref="alg2.l10.m1.1.1"><times id="alg2.l10.m1.1.1.1.cmml" xref="alg2.l10.m1.1.1.1"></times><ci id="alg2.l10.m1.1.1.2.cmml" xref="alg2.l10.m1.1.1.2">𝑒</ci><ci id="alg2.l10.m1.1.1.3.cmml" xref="alg2.l10.m1.1.1.3">𝑛</ci><apply id="alg2.l10.m1.1.1.4.cmml" xref="alg2.l10.m1.1.1.4"><csymbol cd="ambiguous" id="alg2.l10.m1.1.1.4.1.cmml" xref="alg2.l10.m1.1.1.4">subscript</csymbol><ci id="alg2.l10.m1.1.1.4.2.cmml" xref="alg2.l10.m1.1.1.4.2">𝑑</ci><apply id="alg2.l10.m1.1.1.4.3.cmml" xref="alg2.l10.m1.1.1.4.3"><times id="alg2.l10.m1.1.1.4.3.1.cmml" xref="alg2.l10.m1.1.1.4.3.1"></times><ci id="alg2.l10.m1.1.1.4.3.2.cmml" xref="alg2.l10.m1.1.1.4.3.2">𝐼</ci><ci id="alg2.l10.m1.1.1.4.3.3.cmml" xref="alg2.l10.m1.1.1.4.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l10.m1.1c">end_{ID}</annotation><annotation encoding="application/x-llamapun" id="alg2.l10.m1.1d">italic_e italic_n italic_d start_POSTSUBSCRIPT italic_I italic_D end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l10.5" style="font-size:80%;"> = num</span><sub class="ltx_sub" id="alg2.l10.6"><span class="ltx_text ltx_font_italic" id="alg2.l10.6.1" style="font-size:80%;">shards</span></sub><span class="ltx_text" id="alg2.l10.7" style="font-size:80%;"> to </span><math alttext="N_{max}" class="ltx_Math" display="inline" id="alg2.l10.m3.1"><semantics id="alg2.l10.m3.1a"><msub id="alg2.l10.m3.1.1" xref="alg2.l10.m3.1.1.cmml"><mi id="alg2.l10.m3.1.1.2" mathsize="80%" xref="alg2.l10.m3.1.1.2.cmml">N</mi><mrow id="alg2.l10.m3.1.1.3" xref="alg2.l10.m3.1.1.3.cmml"><mi id="alg2.l10.m3.1.1.3.2" mathsize="80%" xref="alg2.l10.m3.1.1.3.2.cmml">m</mi><mo id="alg2.l10.m3.1.1.3.1" xref="alg2.l10.m3.1.1.3.1.cmml">⁢</mo><mi id="alg2.l10.m3.1.1.3.3" mathsize="80%" xref="alg2.l10.m3.1.1.3.3.cmml">a</mi><mo id="alg2.l10.m3.1.1.3.1a" xref="alg2.l10.m3.1.1.3.1.cmml">⁢</mo><mi id="alg2.l10.m3.1.1.3.4" mathsize="80%" xref="alg2.l10.m3.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg2.l10.m3.1b"><apply id="alg2.l10.m3.1.1.cmml" xref="alg2.l10.m3.1.1"><csymbol cd="ambiguous" id="alg2.l10.m3.1.1.1.cmml" xref="alg2.l10.m3.1.1">subscript</csymbol><ci id="alg2.l10.m3.1.1.2.cmml" xref="alg2.l10.m3.1.1.2">𝑁</ci><apply id="alg2.l10.m3.1.1.3.cmml" xref="alg2.l10.m3.1.1.3"><times id="alg2.l10.m3.1.1.3.1.cmml" xref="alg2.l10.m3.1.1.3.1"></times><ci id="alg2.l10.m3.1.1.3.2.cmml" xref="alg2.l10.m3.1.1.3.2">𝑚</ci><ci id="alg2.l10.m3.1.1.3.3.cmml" xref="alg2.l10.m3.1.1.3.3">𝑎</ci><ci id="alg2.l10.m3.1.1.3.4.cmml" xref="alg2.l10.m3.1.1.3.4">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l10.m3.1c">N_{max}</annotation><annotation encoding="application/x-llamapun" id="alg2.l10.m3.1d">italic_N start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="alg2.l10.8" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l10.9" style="font-size:80%;">do</span><span class="ltx_text" id="alg2.l10.10" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l11.1.1.1" style="font-size:80%;">11:</span></span><span class="ltx_text" id="alg2.l11.2" style="font-size:80%;">           min_estimation </span><math alttext="=" class="ltx_Math" display="inline" id="alg2.l11.m1.1"><semantics id="alg2.l11.m1.1a"><mo id="alg2.l11.m1.1.1" mathsize="80%" xref="alg2.l11.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="alg2.l11.m1.1b"><eq id="alg2.l11.m1.1.1.cmml" xref="alg2.l11.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="alg2.l11.m1.1c">=</annotation><annotation encoding="application/x-llamapun" id="alg2.l11.m1.1d">=</annotation></semantics></math><span class="ltx_text" id="alg2.l11.3" style="font-size:80%;"> float(inf)
</span>
</div>
<div class="ltx_listingline" id="alg2.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l12.1.1.1" style="font-size:80%;">12:</span></span><span class="ltx_text" id="alg2.l12.2" style="font-size:80%;">           </span><span class="ltx_text ltx_font_bold" id="alg2.l12.3" style="font-size:80%;">for</span><span class="ltx_text" id="alg2.l12.4" style="font-size:80%;"> start</span><sub class="ltx_sub" id="alg2.l12.5"><span class="ltx_text ltx_font_italic" id="alg2.l12.5.1" style="font-size:80%;">ID</span></sub><span class="ltx_text" id="alg2.l12.6" style="font-size:80%;"> = num</span><sub class="ltx_sub" id="alg2.l12.7"><span class="ltx_text ltx_font_italic" id="alg2.l12.7.1" style="font-size:80%;">shards</span></sub><span class="ltx_text" id="alg2.l12.8" style="font-size:80%;"> to end</span><sub class="ltx_sub" id="alg2.l12.9"><span class="ltx_text ltx_font_italic" id="alg2.l12.9.1" style="font-size:80%;">ID</span></sub><span class="ltx_text" id="alg2.l12.10" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l12.11" style="font-size:80%;">do</span><span class="ltx_text" id="alg2.l12.12" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l13.1.1.1" style="font-size:80%;">13:</span></span><span class="ltx_text" id="alg2.l13.2" style="font-size:80%;">               prev_shards_mem = Mem[num</span><sub class="ltx_sub" id="alg2.l13.3"><span class="ltx_text ltx_font_italic" id="alg2.l13.3.1" style="font-size:80%;">shards</span></sub><span class="ltx_text" id="alg2.l13.4" style="font-size:80%;">-1][start</span><sub class="ltx_sub" id="alg2.l13.5"><span class="ltx_text ltx_font_italic" id="alg2.l13.5.1" style="font-size:80%;">ID</span></sub><span class="ltx_text" id="alg2.l13.6" style="font-size:80%;"> - 1]
</span>
</div>
<div class="ltx_listingline" id="alg2.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l14.1.1.1" style="font-size:80%;">14:</span></span><span class="ltx_text" id="alg2.l14.2" style="font-size:80%;">               last_shard_mem = COST(start</span><sub class="ltx_sub" id="alg2.l14.3"><span class="ltx_text ltx_font_italic" id="alg2.l14.3.1" style="font-size:80%;">ID</span></sub><span class="ltx_text" id="alg2.l14.4" style="font-size:80%;">, end</span><sub class="ltx_sub" id="alg2.l14.5"><span class="ltx_text ltx_font_italic" id="alg2.l14.5.1" style="font-size:80%;">ID</span></sub><span class="ltx_text" id="alg2.l14.6" style="font-size:80%;">)
</span>
</div>
<div class="ltx_listingline" id="alg2.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l15.1.1.1" style="font-size:80%;">15:</span></span><span class="ltx_text" id="alg2.l15.2" style="font-size:80%;">               cur_estimation = prev_shards_mem + last_shard_mem
</span>
</div>
<div class="ltx_listingline" id="alg2.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l16.1.1.1" style="font-size:80%;">16:</span></span><span class="ltx_text" id="alg2.l16.2" style="font-size:80%;">               </span><span class="ltx_text ltx_font_bold" id="alg2.l16.3" style="font-size:80%;">if</span><span class="ltx_text" id="alg2.l16.4" style="font-size:80%;"> cur_estimation </span><math alttext="&lt;" class="ltx_Math" display="inline" id="alg2.l16.m1.1"><semantics id="alg2.l16.m1.1a"><mo id="alg2.l16.m1.1.1" mathsize="80%" xref="alg2.l16.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="alg2.l16.m1.1b"><lt id="alg2.l16.m1.1.1.cmml" xref="alg2.l16.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="alg2.l16.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="alg2.l16.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text" id="alg2.l16.5" style="font-size:80%;"> min_estimation </span><span class="ltx_text ltx_font_bold" id="alg2.l16.6" style="font-size:80%;">then</span><span class="ltx_text" id="alg2.l16.7" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l17.1.1.1" style="font-size:80%;">17:</span></span><span class="ltx_text" id="alg2.l17.2" style="font-size:80%;">                  min_estimation = cur_estimation
</span>
</div>
<div class="ltx_listingline" id="alg2.l18">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l18.1.1.1" style="font-size:80%;">18:</span></span><span class="ltx_text" id="alg2.l18.2" style="font-size:80%;">                  </span><em class="ltx_emph ltx_font_italic" id="alg2.l18.3" style="font-size:80%;">Memorize current partitioning points</em><span class="ltx_text" id="alg2.l18.4" style="font-size:80%;">
</span>
</div>
<div class="ltx_listingline" id="alg2.l19">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l19.1.1.1" style="font-size:80%;">19:</span></span><span class="ltx_text" id="alg2.l19.2" style="font-size:80%;">               </span><span class="ltx_text ltx_font_bold" id="alg2.l19.3" style="font-size:80%;">end</span><span class="ltx_text" id="alg2.l19.4" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l19.5" style="font-size:80%;">if</span>
</div>
<div class="ltx_listingline" id="alg2.l20">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l20.1.1.1" style="font-size:80%;">20:</span></span><span class="ltx_text" id="alg2.l20.2" style="font-size:80%;">           </span><span class="ltx_text ltx_font_bold" id="alg2.l20.3" style="font-size:80%;">end</span><span class="ltx_text" id="alg2.l20.4" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l20.5" style="font-size:80%;">for</span>
</div>
<div class="ltx_listingline" id="alg2.l21">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l21.1.1.1" style="font-size:80%;">21:</span></span><span class="ltx_text" id="alg2.l21.2" style="font-size:80%;">           Mem[num</span><sub class="ltx_sub" id="alg2.l21.3"><span class="ltx_text ltx_font_italic" id="alg2.l21.3.1" style="font-size:80%;">shards</span></sub><span class="ltx_text" id="alg2.l21.4" style="font-size:80%;">][end</span><sub class="ltx_sub" id="alg2.l21.5"><span class="ltx_text ltx_font_italic" id="alg2.l21.5.1" style="font-size:80%;">ID</span></sub><span class="ltx_text" id="alg2.l21.6" style="font-size:80%;">] = min_estimation
</span>
</div>
<div class="ltx_listingline" id="alg2.l22">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l22.1.1.1" style="font-size:80%;">22:</span></span><span class="ltx_text" id="alg2.l22.2" style="font-size:80%;">        </span><span class="ltx_text ltx_font_bold" id="alg2.l22.3" style="font-size:80%;">end</span><span class="ltx_text" id="alg2.l22.4" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l22.5" style="font-size:80%;">for</span>
</div>
<div class="ltx_listingline" id="alg2.l23">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l23.1.1.1" style="font-size:80%;">23:</span></span><span class="ltx_text" id="alg2.l23.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_bold" id="alg2.l23.3" style="font-size:80%;">end</span><span class="ltx_text" id="alg2.l23.4" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l23.5" style="font-size:80%;">for</span>
</div>
<div class="ltx_listingline" id="alg2.l24">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l24.1.1.1" style="font-size:80%;">24:</span></span><span class="ltx_text" id="alg2.l24.2" style="font-size:80%;">    </span><span class="ltx_text ltx_font_bold" id="alg2.l24.3" style="font-size:80%;">return</span><span class="ltx_text" id="alg2.l24.4" style="font-size:80%;"> partitioning points corresponding to smallest Mem value
</span>
</div>
<div class="ltx_listingline" id="alg2.l25">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l25.1.1.1" style="font-size:80%;">25:</span></span><span class="ltx_text ltx_font_bold" id="alg2.l25.2" style="font-size:80%;">end</span><span class="ltx_text" id="alg2.l25.3" style="font-size:80%;"> </span><span class="ltx_text ltx_font_bold" id="alg2.l25.4" style="font-size:80%;">function</span>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS2.p9">
<p class="ltx_p" id="S4.SS2.p9.13">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F10" title="Figure 10 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">10</span></a>, we illustrate the process of deriving Mem[3][5], which
represents the minimum memory cost when the table <math alttext="E" class="ltx_Math" display="inline" id="S4.SS2.p9.1.m1.1"><semantics id="S4.SS2.p9.1.m1.1a"><mi id="S4.SS2.p9.1.m1.1.1" xref="S4.SS2.p9.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.1.m1.1b"><ci id="S4.SS2.p9.1.m1.1.1.cmml" xref="S4.SS2.p9.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.1.m1.1d">italic_E</annotation></semantics></math> is partitioned
into three shards. Each partitioning plan’s memory consumption is the
summation of two values: (1) the estimated memory consumption of the first two
shards (red/yellow) and (2) the third (green) shard’s memory consumption.
Because of the recursive relationship in DP, the optimal memory consumption for
the first two shards can be determined by referencing the memoized value of
Mem[<math alttext="2" class="ltx_Math" display="inline" id="S4.SS2.p9.2.m2.1"><semantics id="S4.SS2.p9.2.m2.1a"><mn id="S4.SS2.p9.2.m2.1.1" xref="S4.SS2.p9.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.2.m2.1b"><cn id="S4.SS2.p9.2.m2.1.1.cmml" type="integer" xref="S4.SS2.p9.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.2.m2.1d">2</annotation></semantics></math>][<math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.p9.3.m3.1"><semantics id="S4.SS2.p9.3.m3.1a"><mi id="S4.SS2.p9.3.m3.1.1" xref="S4.SS2.p9.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.3.m3.1b"><ci id="S4.SS2.p9.3.m3.1.1.cmml" xref="S4.SS2.p9.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.3.m3.1d">italic_x</annotation></semantics></math>] (<math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.p9.4.m4.1"><semantics id="S4.SS2.p9.4.m4.1a"><mi id="S4.SS2.p9.4.m4.1.1" xref="S4.SS2.p9.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.4.m4.1b"><ci id="S4.SS2.p9.4.m4.1.1.cmml" xref="S4.SS2.p9.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.4.m4.1d">italic_x</annotation></semantics></math>=<math alttext="4" class="ltx_Math" display="inline" id="S4.SS2.p9.5.m5.1"><semantics id="S4.SS2.p9.5.m5.1a"><mn id="S4.SS2.p9.5.m5.1.1" xref="S4.SS2.p9.5.m5.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.5.m5.1b"><cn id="S4.SS2.p9.5.m5.1.1.cmml" type="integer" xref="S4.SS2.p9.5.m5.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.5.m5.1c">4</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.5.m5.1d">4</annotation></semantics></math>/<math alttext="3" class="ltx_Math" display="inline" id="S4.SS2.p9.6.m6.1"><semantics id="S4.SS2.p9.6.m6.1a"><mn id="S4.SS2.p9.6.m6.1.1" xref="S4.SS2.p9.6.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.6.m6.1b"><cn id="S4.SS2.p9.6.m6.1.1.cmml" type="integer" xref="S4.SS2.p9.6.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.6.m6.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.6.m6.1d">3</annotation></semantics></math>/<math alttext="2" class="ltx_Math" display="inline" id="S4.SS2.p9.7.m7.1"><semantics id="S4.SS2.p9.7.m7.1a"><mn id="S4.SS2.p9.7.m7.1.1" xref="S4.SS2.p9.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.7.m7.1b"><cn id="S4.SS2.p9.7.m7.1.1.cmml" type="integer" xref="S4.SS2.p9.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.7.m7.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.7.m7.1d">2</annotation></semantics></math>), which represents the least memory consumed
when <math alttext="E^{\prime}" class="ltx_Math" display="inline" id="S4.SS2.p9.8.m8.1"><semantics id="S4.SS2.p9.8.m8.1a"><msup id="S4.SS2.p9.8.m8.1.1" xref="S4.SS2.p9.8.m8.1.1.cmml"><mi id="S4.SS2.p9.8.m8.1.1.2" xref="S4.SS2.p9.8.m8.1.1.2.cmml">E</mi><mo id="S4.SS2.p9.8.m8.1.1.3" xref="S4.SS2.p9.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.8.m8.1b"><apply id="S4.SS2.p9.8.m8.1.1.cmml" xref="S4.SS2.p9.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.8.m8.1.1.1.cmml" xref="S4.SS2.p9.8.m8.1.1">superscript</csymbol><ci id="S4.SS2.p9.8.m8.1.1.2.cmml" xref="S4.SS2.p9.8.m8.1.1.2">𝐸</ci><ci id="S4.SS2.p9.8.m8.1.1.3.cmml" xref="S4.SS2.p9.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.8.m8.1c">E^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.8.m8.1d">italic_E start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> is partitioned into two shards. Since the memory consumption of the
third (green) shard can be evaluated using the COST function
(Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#alg1" title="Algorithm 1 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>), we arrive at the estimated memory consumption of
option <math alttext="1" class="ltx_Math" display="inline" id="S4.SS2.p9.9.m9.1"><semantics id="S4.SS2.p9.9.m9.1a"><mn id="S4.SS2.p9.9.m9.1.1" xref="S4.SS2.p9.9.m9.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.9.m9.1b"><cn id="S4.SS2.p9.9.m9.1.1.cmml" type="integer" xref="S4.SS2.p9.9.m9.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.9.m9.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.9.m9.1d">1</annotation></semantics></math>/<math alttext="2" class="ltx_Math" display="inline" id="S4.SS2.p9.10.m10.1"><semantics id="S4.SS2.p9.10.m10.1a"><mn id="S4.SS2.p9.10.m10.1.1" xref="S4.SS2.p9.10.m10.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.10.m10.1b"><cn id="S4.SS2.p9.10.m10.1.1.cmml" type="integer" xref="S4.SS2.p9.10.m10.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.10.m10.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.10.m10.1d">2</annotation></semantics></math>/<math alttext="3" class="ltx_Math" display="inline" id="S4.SS2.p9.11.m11.1"><semantics id="S4.SS2.p9.11.m11.1a"><mn id="S4.SS2.p9.11.m11.1.1" xref="S4.SS2.p9.11.m11.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.11.m11.1b"><cn id="S4.SS2.p9.11.m11.1.1.cmml" type="integer" xref="S4.SS2.p9.11.m11.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.11.m11.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.11.m11.1d">3</annotation></semantics></math> as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F10" title="Figure 10 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">10</span></a>. By comparing the estimated
memory consumption for each partitioning option, we can identify the
optimal solution to this problem (option <math alttext="2" class="ltx_Math" display="inline" id="S4.SS2.p9.12.m12.1"><semantics id="S4.SS2.p9.12.m12.1a"><mn id="S4.SS2.p9.12.m12.1.1" xref="S4.SS2.p9.12.m12.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.12.m12.1b"><cn id="S4.SS2.p9.12.m12.1.1.cmml" type="integer" xref="S4.SS2.p9.12.m12.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.12.m12.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.12.m12.1d">2</annotation></semantics></math>) that incurs the lowest memory
cost. In the given example, the first two (red/yellow) shards partitioned with
Mem[2][3] (i.e., red and yellow shards containing E[1] and E[2,3] embeddings
respectively) and having the third (green) shard include the the remaining
two embeddings (E[4,5]) yields the least memory consumption. Thus, Mem[3][5] is
updated with a memory cost of <math alttext="4" class="ltx_Math" display="inline" id="S4.SS2.p9.13.m13.1"><semantics id="S4.SS2.p9.13.m13.1a"><mn id="S4.SS2.p9.13.m13.1.1" xref="S4.SS2.p9.13.m13.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.13.m13.1b"><cn id="S4.SS2.p9.13.m13.1.1.cmml" type="integer" xref="S4.SS2.p9.13.m13.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.13.m13.1c">4</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p9.13.m13.1d">4</annotation></semantics></math> and the <em class="ltx_emph ltx_font_italic" id="S4.SS2.p9.13.1">partitioning points</em> of [1, 3,
5] (which stores the last index ID of each shard) is separately stored
as the corresponding, optimal partitioning plan for this example.</p>
</div>
<div class="ltx_para" id="S4.SS2.p10">
<p class="ltx_p" id="S4.SS2.p10.10">In Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#alg2" title="Algorithm 2 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>, we detail ElasticRec’s table partitioning
algorithm, which is a generalization of the aforementioned example. The
initialization step of our DP algorithm partitions the table into a single
shard. Here the values of Mem[1][end<sub class="ltx_sub" id="S4.SS2.p10.10.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p10.10.1.1">ID</span></sub>] represent the optimal memory
consumption when a single shard contains the end<sub class="ltx_sub" id="S4.SS2.p10.10.2"><span class="ltx_text ltx_font_italic" id="S4.SS2.p10.10.2.1">ID</span></sub> most hot embeddings
(line 2-4), one which is derived using our COST function
(Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#alg1" title="Algorithm 1 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>). The remaining Mem[num<sub class="ltx_sub" id="S4.SS2.p10.10.3"><span class="ltx_text ltx_font_italic" id="S4.SS2.p10.10.3.1">shards</span></sub>][-] values are
derived by exploiting the recursive relation between the table
partitioned with (<math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p10.4.m4.1"><semantics id="S4.SS2.p10.4.m4.1a"><mi id="S4.SS2.p10.4.m4.1.1" xref="S4.SS2.p10.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.4.m4.1b"><ci id="S4.SS2.p10.4.m4.1.1.cmml" xref="S4.SS2.p10.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.4.m4.1d">italic_i</annotation></semantics></math> - 1) shards and the table partitioned with <math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p10.5.m5.1"><semantics id="S4.SS2.p10.5.m5.1a"><mi id="S4.SS2.p10.5.m5.1.1" xref="S4.SS2.p10.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.5.m5.1b"><ci id="S4.SS2.p10.5.m5.1.1.cmml" xref="S4.SS2.p10.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.5.m5.1d">italic_i</annotation></semantics></math>
shards where the optimal Mem value for the (<math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p10.6.m6.1"><semantics id="S4.SS2.p10.6.m6.1a"><mi id="S4.SS2.p10.6.m6.1.1" xref="S4.SS2.p10.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.6.m6.1b"><ci id="S4.SS2.p10.6.m6.1.1.cmml" xref="S4.SS2.p10.6.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.6.m6.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.6.m6.1d">italic_i</annotation></semantics></math> - 1) shards can always be
retrieved through the memoized solution, without re-computation (line 9).
Similar to the example in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F10" title="Figure 10 ‣ IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">10</span></a>, we iterate through all possible
shard sizes for the last shard by changing start<sub class="ltx_sub" id="S4.SS2.p10.10.4"><span class="ltx_text ltx_font_italic" id="S4.SS2.p10.10.4.1">ID</span></sub>, from num<sub class="ltx_sub" id="S4.SS2.p10.10.5"><span class="ltx_text ltx_font_italic" id="S4.SS2.p10.10.5.1">shards</span></sub> to
end<sub class="ltx_sub" id="S4.SS2.p10.10.6"><span class="ltx_text ltx_font_italic" id="S4.SS2.p10.10.6.1">ID</span></sub> (line 8), and evaluate its COST function (line 10) in order to
determine the overall minimum memory consumption under that partitioning plan
(line 17). After the entire design space of Mem[-][-] is evaluated up to
maximum possible number of shards (S<sub class="ltx_sub" id="S4.SS2.p10.10.7"><span class="ltx_text ltx_font_italic" id="S4.SS2.p10.10.7.1">max</span></sub>), the one with the minimum memory
cost is chosen as the final partitioning plan (i.e., the number of shards to
partition the original table and its partitioning points).</p>
</div>
<div class="ltx_para" id="S4.SS2.p11">
<p class="ltx_p" id="S4.SS2.p11.4">The cost of our DP algorithm is O(S<sub class="ltx_sub" id="S4.SS2.p11.4.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p11.4.1.1">max</span></sub><math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.p11.2.m2.1"><semantics id="S4.SS2.p11.2.m2.1a"><mo id="S4.SS2.p11.2.m2.1.1" xref="S4.SS2.p11.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.2.m2.1b"><times id="S4.SS2.p11.2.m2.1.1.cmml" xref="S4.SS2.p11.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p11.2.m2.1d">×</annotation></semantics></math>N<sub class="ltx_sub" id="S4.SS2.p11.4.2"><span class="ltx_text ltx_font_italic" id="S4.SS2.p11.4.2.1">max</span></sub>) which can be
calculated within <math alttext="18" class="ltx_Math" display="inline" id="S4.SS2.p11.4.m4.1"><semantics id="S4.SS2.p11.4.m4.1a"><mn id="S4.SS2.p11.4.m4.1.1" xref="S4.SS2.p11.4.m4.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.4.m4.1b"><cn id="S4.SS2.p11.4.m4.1.1.cmml" type="integer" xref="S4.SS2.p11.4.m4.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.4.m4.1c">18</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p11.4.m4.1d">18</annotation></semantics></math> seconds for an embedding table with 20M entries.
Importantly, executing the DP algorithm is off the critical path of serving
online inference queries.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Bucketization</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Since ElasticRec partitions an embedding table into multiple embedding shards, the index IDs used to lookup the original embedding table should be remapped appropriately, in accordance to the partitioned embedding shards. We refer to such process as <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.1">bucketization</em> which we explain below.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.6">Consider the example
in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F11" title="Figure 11 ‣ IV-C Bucketization ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">11</span></a> which assumes that a table with <math alttext="10" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mn id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><cn id="S4.SS3.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">10</annotation></semantics></math> embeddings are partitioned into two shards.
To improve throughput, a single query contains multiple inputs that are batched together for concurrent processing. As such, when accessing an embedding table, two arrays are utilized, the index array and the offset array. The index array stores the list of IDs to lookup from the table, whereas the offset array is used to separate out which elements within the index array should different inputs within the query utilize. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F11" title="Figure 11 ‣ IV-C Bucketization ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">11</span></a>(a), for instance, the first element in the offset array (value <math alttext="0" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn id="S4.SS3.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS3.p2.2.m2.1.1">0</cn></annotation-xml></semantics></math>, red) indicates that input 0 requires index IDs starting from offset <math alttext="0" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mn id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><cn id="S4.SS3.p2.3.m3.1.1.cmml" type="integer" xref="S4.SS3.p2.3.m3.1.1">0</cn></annotation-xml></semantics></math> of the index array, whereas input 1 should utilize IDs starting from offset <math alttext="2" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><cn id="S4.SS3.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p2.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">2</annotation></semantics></math> (gray) of the index array. Since these two arrays can no longer be used as-is to access the partitioned embedding shards, our proposed algorithm bucketizes the original input into two partitions as follows. First, it iterates through the original index array (and offset array) and determines which embedding shard each embedding should be gathered from, generating the intermediate index and offset arrays as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.F11" title="Figure 11 ‣ IV-C Bucketization ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">11</span></a>(b). The values stored in shard B’s index array is then subtracted by <math alttext="6" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mn id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><cn id="S4.SS3.p2.5.m5.1.1.cmml" type="integer" xref="S4.SS3.p2.5.m5.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">6</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">6</annotation></semantics></math> (i.e., the size of the first shard A) so that the index IDs used to lookup the sharded table can start from a base value of <math alttext="0" class="ltx_Math" display="inline" id="S4.SS3.p2.6.m6.1"><semantics id="S4.SS3.p2.6.m6.1a"><mn id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><cn id="S4.SS3.p2.6.m6.1.1.cmml" type="integer" xref="S4.SS3.p2.6.m6.1.1">0</cn></annotation-xml></semantics></math>. The bucketization algorithm is simple to implement and highly parallelizable. We omit the pseudo-code that summarizes its implementation for brevity.</p>
</div>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="135" id="S4.F11.g1" src="x18.png" width="340"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>An example bucketization process that partitions a <math alttext="10" class="ltx_Math" display="inline" id="S4.F11.2.m1.1"><semantics id="S4.F11.2.m1.1b"><mn id="S4.F11.2.m1.1.1" xref="S4.F11.2.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.F11.2.m1.1c"><cn id="S4.F11.2.m1.1.1.cmml" type="integer" xref="S4.F11.2.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F11.2.m1.1d">10</annotation><annotation encoding="application/x-llamapun" id="S4.F11.2.m1.1e">10</annotation></semantics></math> element table into
two shards. Indices and offsets used for input 0 and 1 are highlighted in red and gray, respectively.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Deploying ElasticRec at Scale using Kubernetes</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Since each model shard is encapsulated as a container, Kubernetes
can independently scale each shard replicas as dictated by the HPA
policy. We employ a throughput-centric metric as the HPA target for sparse
shards while a latency-centric metricis is used for dense shards’ HPA target.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.3">For sparse shards, we utilize shard’s maximum QPS as
the autoscaling target. Specifically, ElasticRec measures the
maximum QPS each sparse shard can sustain (QPS<sub class="ltx_sub" id="S4.SS4.p2.3.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.p2.3.1.1">max</span></sub>),
stress-testing each one of them by gradually increasing input
query traffic intensity and monitoring at which point the tail
latency increases rapidly. ElasticRec then configures the HPA
policy to have each sparse shard’s respective QPS<sub class="ltx_sub" id="S4.SS4.p2.3.2"><span class="ltx_text ltx_font_italic" id="S4.SS4.p2.3.2.1">max</span></sub> value
be set as the threshold to trigger each sparse microservice to
replicate an additional shard instance. For dense
shards, we define a latency threshold where the auto-scaling
HPA target is set to <math alttext="65\%" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m3.1"><semantics id="S4.SS4.p2.3.m3.1a"><mrow id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mn id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">65</mn><mo id="S4.SS4.p2.3.m3.1.1.1" xref="S4.SS4.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1.1">percent</csymbol><cn id="S4.SS4.p2.3.m3.1.1.2.cmml" type="integer" xref="S4.SS4.p2.3.m3.1.1.2">65</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">65\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.3.m3.1d">65 %</annotation></semantics></math> of the SLA, ensuring that service
latency remains within acceptable bounds and does not lead to
SLA violations. Overall, ElasticRec can adaptively
adjust the replicas of each shard type that satisfies the
demands of incoming query traffic while also achieving
high memory efficiency.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Evaluation Methodology</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Hardware Architecture</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.3"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.3.1">CPU-only inference server.</span> In Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6" title="VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">VI</span></a>, we first
evaluate ElasticRec over CPU-only systems using a multi-node CPU cluster consisting
of one master node and eleven compute nodes. Each compute node’s configuration
is in line with those employed in production CPU-only RecSys inference
servers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib20" title="">20</a>]</cite>. Specifically, each CPU node is
equipped with a dual-socket Intel Xeon Gold 6242 Skylake CPU containing <math alttext="32" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><cn id="S5.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">32</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">32</annotation></semantics></math>
logical cores and <math alttext="192" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mn id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">192</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><cn id="S5.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS1.p1.2.m2.1.1">192</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">192</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">192</annotation></semantics></math> GB of DRAM per socket, each socket providing <math alttext="128" class="ltx_Math" display="inline" id="S5.SS1.p1.3.m3.1"><semantics id="S5.SS1.p1.3.m3.1a"><mn id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><cn id="S5.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS1.p1.3.m3.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">128</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.3.m3.1d">128</annotation></semantics></math>
GB/sec of memory bandwidth. The compute nodes communicate over a 10 Gbps
network.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">CPU-GPU inference server.</span> We also evaluate ElasticRec’s applicability over
CPU-GPU systems using Google Kubernetes Engine (GKE) in Google
Cloud <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib14" title="">14</a>]</cite>. In this setup, we utilize a GKE cluster that contains twenty
hybrid CPU-GPU compute nodes (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.2">n1-standard-32</span> node <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib13" title="">13</a>]</cite>
containing 32 CPU logical cores and 120 GB of DRAM, and
connected to an NVIDIA Tesla T4 GPU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib47" title="">47</a>]</cite> over PCIe). The compute
nodes communicate over a 32 Gbps network.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Software Architecture</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Each model shard communicates with one another using C++ gRPC protocol. RecSys
models are designed using PyTorch’s libtorch (v1.12) and the DLRM GitHub
repository <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib40" title="">40</a>]</cite>. Resource management is handled by Kubernetes
(v1.26), which is responsible for scaling in/out each model shard replicas
according to input query traffic. Load balancing is managed using Linkerd
(v2.12), routing the input queries to the shard replicas as appropriate. We
also use a Prometheus metrics server <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib50" title="">50</a>]</cite> to collect various
custom statistics, e.g., CPU usage, memory consumption, tail latency, and QPS.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Workloads</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To better illustrate ElasticRec’s effectiveness
on model serving, we use both microbenchmarks
(Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T1" title="TABLE I ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">I</span></a>) and state-of-the-art RecSys model
configurations (Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T2" title="TABLE II ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a>) used in prior work.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Microbenchmarks.</span>
We construct several microbenchmarks using DLRM’s RM1 (Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T2" title="TABLE II ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a>) as our default model configuration. The microbenchmarks are designed to better cover the large evaluation space by changing some of its key model parameters in terms of (1) the dense MLP layer size, (2) embedding table’s locality, (3) number of tables, and (4) the number of shards to partition a table (Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T1" title="TABLE I ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">I</span></a>). We use these microbenchmarks to evaluate the sensitivity of ElasticRec across a wide range of DLRM configurations, focusing on ElasticRec’s effectiveness in reducing memory allocation size.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">State-of-the-art RecSys workloads.</span> We also evaluate ElasticRec across
multiple dimensions in detail using three representative DLRM configurations
(Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T2" title="TABLE II ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a>) used in prior
work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib42" title="">42</a>]</cite>.
The SLA target is set to 400ms to be consistent with industry recommendations
on SLA for RecSys, which is several hundreds of
milliseconds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib18" title="">18</a>]</cite>. All experiments are collected while
ensuring that the <math alttext="95" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><mn id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">95</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><cn id="S5.SS3.p3.1.m1.1.1.cmml" type="integer" xref="S5.SS3.p3.1.m1.1.1">95</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">95</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">95</annotation></semantics></math> percentile tail latency does not violate SLA.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The key parameters changed in our microbenchmark based evaluations in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS1" title="VI-A Microbenchmarks ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-A</span></span></a>. The default RecSys model configuration for our microbenchmark is based on DLRM RM1 (Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T2" title="TABLE II ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a>).</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.9.10.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.9.10.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S5.T1.9.10.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.9.10.1.2.1" style="font-size:70%;">Configurations</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.9.11.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S5.T1.9.11.2.1" rowspan="2"><span class="ltx_text" id="S5.T1.9.11.2.1.1" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S5.T1.9.11.2.1.1.1">
<span class="ltx_tr" id="S5.T1.9.11.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.11.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.9.11.2.1.1.1.1.1.1">MLP</span></span></span>
<span class="ltx_tr" id="S5.T1.9.11.2.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.11.2.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T1.9.11.2.1.1.1.2.1.1">layer</span></span></span>
<span class="ltx_tr" id="S5.T1.9.11.2.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.11.2.1.1.1.3.1"><span class="ltx_text ltx_font_bold" id="S5.T1.9.11.2.1.1.1.3.1.1">size</span></span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T1.9.11.2.2"><span class="ltx_text" id="S5.T1.9.11.2.2.1" style="font-size:70%;">Light</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T1.9.11.2.3"><span class="ltx_text" id="S5.T1.9.11.2.3.1" style="font-size:70%;">Medium</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T1.9.11.2.4"><span class="ltx_text" id="S5.T1.9.11.2.4.1" style="font-size:70%;">Heavy</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.9.12.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.9.12.3.1">
<table class="ltx_tabular ltx_align_middle" id="S5.T1.9.12.3.1.1">
<tr class="ltx_tr" id="S5.T1.9.12.3.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.12.3.1.1.1.1"><span class="ltx_text" id="S5.T1.9.12.3.1.1.1.1.1" style="font-size:70%;">Bottom: 64-32-32</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.12.3.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.12.3.1.1.2.1"><span class="ltx_text" id="S5.T1.9.12.3.1.1.2.1.1" style="font-size:70%;">Top: 64-32-1</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.9.12.3.2">
<table class="ltx_tabular ltx_align_middle" id="S5.T1.9.12.3.2.1">
<tr class="ltx_tr" id="S5.T1.9.12.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.12.3.2.1.1.1"><span class="ltx_text" id="S5.T1.9.12.3.2.1.1.1.1" style="font-size:70%;">Bottom: 256-128-32</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.12.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.12.3.2.1.2.1"><span class="ltx_text" id="S5.T1.9.12.3.2.1.2.1.1" style="font-size:70%;">Top: 256-64-1</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.9.12.3.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T1.9.12.3.3.1">
<tr class="ltx_tr" id="S5.T1.9.12.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.12.3.3.1.1.1"><span class="ltx_text" id="S5.T1.9.12.3.3.1.1.1.1" style="font-size:70%;">Bottom: 512-256-32</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.12.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.12.3.3.1.2.1"><span class="ltx_text" id="S5.T1.9.12.3.3.1.2.1.1" style="font-size:70%;">Top: 512-64-1</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.9.13.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S5.T1.9.13.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.9.13.1.1.1" style="font-size:70%;">Locality</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.9.13.1.2"><span class="ltx_text" id="S5.T1.9.13.1.2.1" style="font-size:70%;">Low</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.9.13.1.3"><span class="ltx_text" id="S5.T1.9.13.1.3.1" style="font-size:70%;">Medium</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T1.9.13.1.4"><span class="ltx_text" id="S5.T1.9.13.1.4.1" style="font-size:70%;">High</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.14.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.9.14.2.1"><span class="ltx_text" id="S5.T1.9.14.2.1.1" style="font-size:70%;">P: 10%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.9.14.2.2"><span class="ltx_text" id="S5.T1.9.14.2.2.1" style="font-size:70%;">P: 50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.9.14.2.3"><span class="ltx_text" id="S5.T1.9.14.2.3.1" style="font-size:70%;">P: 90%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S5.T1.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T1.4.4.5.1" style="font-size:70%;">Table (N)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T1.4.4.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T1.4.4.4.4">
<tr class="ltx_tr" id="S5.T1.4.4.4.4.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.4.4.4.4.4.4">
<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.1" style="font-size:70%;">Total number of embedding tables: </span><math alttext="1" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.1.1.m1.1a"><mn id="S5.T1.1.1.1.1.1.1.m1.1.1" mathsize="70%" xref="S5.T1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.1.m1.1b"><cn id="S5.T1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.T1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.1.1.m1.1d">1</annotation></semantics></math><span class="ltx_text" id="S5.T1.4.4.4.4.4.4.2" style="font-size:70%;">, </span><math alttext="4" class="ltx_Math" display="inline" id="S5.T1.2.2.2.2.2.2.m2.1"><semantics id="S5.T1.2.2.2.2.2.2.m2.1a"><mn id="S5.T1.2.2.2.2.2.2.m2.1.1" mathsize="70%" xref="S5.T1.2.2.2.2.2.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.2.2.m2.1b"><cn id="S5.T1.2.2.2.2.2.2.m2.1.1.cmml" type="integer" xref="S5.T1.2.2.2.2.2.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.2.2.m2.1c">4</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.2.2.2.m2.1d">4</annotation></semantics></math><span class="ltx_text" id="S5.T1.4.4.4.4.4.4.3" style="font-size:70%;">, </span><math alttext="10" class="ltx_Math" display="inline" id="S5.T1.3.3.3.3.3.3.m3.1"><semantics id="S5.T1.3.3.3.3.3.3.m3.1a"><mn id="S5.T1.3.3.3.3.3.3.m3.1.1" mathsize="70%" xref="S5.T1.3.3.3.3.3.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.3.3.m3.1b"><cn id="S5.T1.3.3.3.3.3.3.m3.1.1.cmml" type="integer" xref="S5.T1.3.3.3.3.3.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.3.3.m3.1c">10</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.3.3.3.m3.1d">10</annotation></semantics></math><span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4" style="font-size:70%;"> , </span><math alttext="16" class="ltx_Math" display="inline" id="S5.T1.4.4.4.4.4.4.m4.1"><semantics id="S5.T1.4.4.4.4.4.4.m4.1a"><mn id="S5.T1.4.4.4.4.4.4.m4.1.1" mathsize="70%" xref="S5.T1.4.4.4.4.4.4.m4.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.4.4.m4.1b"><cn id="S5.T1.4.4.4.4.4.4.m4.1.1.cmml" type="integer" xref="S5.T1.4.4.4.4.4.4.m4.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.4.4.m4.1c">16</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.4.4.4.m4.1d">16</annotation></semantics></math>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_tt" id="S5.T1.9.9.6"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.6.1" style="font-size:70%;">Shard</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" colspan="3" id="S5.T1.9.9.5">
<span class="ltx_text" id="S5.T1.9.9.5.1" style="font-size:70%;">Number of shards to partition the table: </span><math alttext="1" class="ltx_Math" display="inline" id="S5.T1.5.5.1.m1.1"><semantics id="S5.T1.5.5.1.m1.1a"><mn id="S5.T1.5.5.1.m1.1.1" mathsize="70%" xref="S5.T1.5.5.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.1.m1.1b"><cn id="S5.T1.5.5.1.m1.1.1.cmml" type="integer" xref="S5.T1.5.5.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.1.m1.1d">1</annotation></semantics></math><span class="ltx_text" id="S5.T1.9.9.5.2" style="font-size:70%;">, </span><math alttext="2" class="ltx_Math" display="inline" id="S5.T1.6.6.2.m2.1"><semantics id="S5.T1.6.6.2.m2.1a"><mn id="S5.T1.6.6.2.m2.1.1" mathsize="70%" xref="S5.T1.6.6.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.2.m2.1b"><cn id="S5.T1.6.6.2.m2.1.1.cmml" type="integer" xref="S5.T1.6.6.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.2.m2.1d">2</annotation></semantics></math><span class="ltx_text" id="S5.T1.9.9.5.3" style="font-size:70%;">, </span><math alttext="4" class="ltx_Math" display="inline" id="S5.T1.7.7.3.m3.1"><semantics id="S5.T1.7.7.3.m3.1a"><mn id="S5.T1.7.7.3.m3.1.1" mathsize="70%" xref="S5.T1.7.7.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.3.m3.1b"><cn id="S5.T1.7.7.3.m3.1.1.cmml" type="integer" xref="S5.T1.7.7.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.3.m3.1c">4</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.3.m3.1d">4</annotation></semantics></math><span class="ltx_text" id="S5.T1.9.9.5.4" style="font-size:70%;">, </span><math alttext="8" class="ltx_Math" display="inline" id="S5.T1.8.8.4.m4.1"><semantics id="S5.T1.8.8.4.m4.1a"><mn id="S5.T1.8.8.4.m4.1.1" mathsize="70%" xref="S5.T1.8.8.4.m4.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.4.m4.1b"><cn id="S5.T1.8.8.4.m4.1.1.cmml" type="integer" xref="S5.T1.8.8.4.m4.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.4.m4.1c">8</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.4.m4.1d">8</annotation></semantics></math><span class="ltx_text" id="S5.T1.9.9.5.5" style="font-size:70%;">, and </span><math alttext="16" class="ltx_Math" display="inline" id="S5.T1.9.9.5.m5.1"><semantics id="S5.T1.9.9.5.m5.1a"><mn id="S5.T1.9.9.5.m5.1.1" mathsize="70%" xref="S5.T1.9.9.5.m5.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.5.m5.1b"><cn id="S5.T1.9.9.5.m5.1.1.cmml" type="integer" xref="S5.T1.9.9.5.m5.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.5.m5.1c">16</annotation><annotation encoding="application/x-llamapun" id="S5.T1.9.9.5.m5.1d">16</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.8"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.8.1">Query modeling.</span> A query consists of multiple items to be ranked for a given user, thus the size of a query determines the input batch size.
We follow the methodology from prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib18" title="">18</a>]</cite> to model the query
distribution by setting the batch size as <math alttext="32" class="ltx_Math" display="inline" id="S5.SS3.p4.1.m1.1"><semantics id="S5.SS3.p4.1.m1.1a"><mn id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><cn id="S5.SS3.p4.1.m1.1.1.cmml" type="integer" xref="S5.SS3.p4.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">32</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.1.m1.1d">32</annotation></semantics></math>. To model the effect of locality on embedding table
accesses, we introduce a locality metric <math alttext="P" class="ltx_Math" display="inline" id="S5.SS3.p4.2.m2.1"><semantics id="S5.SS3.p4.2.m2.1a"><mi id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><ci id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.2.m2.1d">italic_P</annotation></semantics></math>, which indicates the percentage
of total accesses that are captured by the top <math alttext="10\%" class="ltx_Math" display="inline" id="S5.SS3.p4.3.m3.1"><semantics id="S5.SS3.p4.3.m3.1a"><mrow id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml"><mn id="S5.SS3.p4.3.m3.1.1.2" xref="S5.SS3.p4.3.m3.1.1.2.cmml">10</mn><mo id="S5.SS3.p4.3.m3.1.1.1" xref="S5.SS3.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><apply id="S5.SS3.p4.3.m3.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p4.3.m3.1.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1.1">percent</csymbol><cn id="S5.SS3.p4.3.m3.1.1.2.cmml" type="integer" xref="S5.SS3.p4.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.3.m3.1d">10 %</annotation></semantics></math> most frequently accessed
vectors (e.g., <math alttext="P" class="ltx_Math" display="inline" id="S5.SS3.p4.4.m4.1"><semantics id="S5.SS3.p4.4.m4.1a"><mi id="S5.SS3.p4.4.m4.1.1" xref="S5.SS3.p4.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.4.m4.1b"><ci id="S5.SS3.p4.4.m4.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.4.m4.1c">P</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.4.m4.1d">italic_P</annotation></semantics></math>=<math alttext="94\%" class="ltx_Math" display="inline" id="S5.SS3.p4.5.m5.1"><semantics id="S5.SS3.p4.5.m5.1a"><mrow id="S5.SS3.p4.5.m5.1.1" xref="S5.SS3.p4.5.m5.1.1.cmml"><mn id="S5.SS3.p4.5.m5.1.1.2" xref="S5.SS3.p4.5.m5.1.1.2.cmml">94</mn><mo id="S5.SS3.p4.5.m5.1.1.1" xref="S5.SS3.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.5.m5.1b"><apply id="S5.SS3.p4.5.m5.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1"><csymbol cd="latexml" id="S5.SS3.p4.5.m5.1.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1.1">percent</csymbol><cn id="S5.SS3.p4.5.m5.1.1.2.cmml" type="integer" xref="S5.SS3.p4.5.m5.1.1.2">94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.5.m5.1c">94\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.5.m5.1d">94 %</annotation></semantics></math> for MoveLens dataset, indicating that <math alttext="94\%" class="ltx_Math" display="inline" id="S5.SS3.p4.6.m6.1"><semantics id="S5.SS3.p4.6.m6.1a"><mrow id="S5.SS3.p4.6.m6.1.1" xref="S5.SS3.p4.6.m6.1.1.cmml"><mn id="S5.SS3.p4.6.m6.1.1.2" xref="S5.SS3.p4.6.m6.1.1.2.cmml">94</mn><mo id="S5.SS3.p4.6.m6.1.1.1" xref="S5.SS3.p4.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.6.m6.1b"><apply id="S5.SS3.p4.6.m6.1.1.cmml" xref="S5.SS3.p4.6.m6.1.1"><csymbol cd="latexml" id="S5.SS3.p4.6.m6.1.1.1.cmml" xref="S5.SS3.p4.6.m6.1.1.1">percent</csymbol><cn id="S5.SS3.p4.6.m6.1.1.2.cmml" type="integer" xref="S5.SS3.p4.6.m6.1.1.2">94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.6.m6.1c">94\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.6.m6.1d">94 %</annotation></semantics></math> of
embedding table lookups are covered by the top <math alttext="10\%" class="ltx_Math" display="inline" id="S5.SS3.p4.7.m7.1"><semantics id="S5.SS3.p4.7.m7.1a"><mrow id="S5.SS3.p4.7.m7.1.1" xref="S5.SS3.p4.7.m7.1.1.cmml"><mn id="S5.SS3.p4.7.m7.1.1.2" xref="S5.SS3.p4.7.m7.1.1.2.cmml">10</mn><mo id="S5.SS3.p4.7.m7.1.1.1" xref="S5.SS3.p4.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.7.m7.1b"><apply id="S5.SS3.p4.7.m7.1.1.cmml" xref="S5.SS3.p4.7.m7.1.1"><csymbol cd="latexml" id="S5.SS3.p4.7.m7.1.1.1.cmml" xref="S5.SS3.p4.7.m7.1.1.1">percent</csymbol><cn id="S5.SS3.p4.7.m7.1.1.2.cmml" type="integer" xref="S5.SS3.p4.7.m7.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.7.m7.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.7.m7.1d">10 %</annotation></semantics></math> hottest embeddings).
Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T1" title="TABLE I ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">I</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T2" title="TABLE II ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a> shows the <math alttext="P" class="ltx_Math" display="inline" id="S5.SS3.p4.8.m8.1"><semantics id="S5.SS3.p4.8.m8.1a"><mi id="S5.SS3.p4.8.m8.1.1" xref="S5.SS3.p4.8.m8.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.8.m8.1b"><ci id="S5.SS3.p4.8.m8.1.1.cmml" xref="S5.SS3.p4.8.m8.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.8.m8.1c">P</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.8.m8.1d">italic_P</annotation></semantics></math> values in our
evaluated microbenchmarks and state-of-the-art RecSys workloads.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>State-of-the-art RecSys workload configurations.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1" style="font-size:70%;">RM1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1" style="font-size:70%;">RM2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.4.1" style="font-size:70%;">RM3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.2.1.1" style="font-size:70%;">Bottom MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.2.2.2"><span class="ltx_text" id="S5.T2.1.2.2.2.1" style="font-size:70%;">256-128-32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.2.2.3"><span class="ltx_text" id="S5.T2.1.2.2.3.1" style="font-size:70%;">256-128-32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.2.2.4"><span class="ltx_text" id="S5.T2.1.2.2.4.1" style="font-size:70%;">2560-512-32</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.3.3.1.1" style="font-size:70%;">Top MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.3.3.2"><span class="ltx_text" id="S5.T2.1.3.3.2.1" style="font-size:70%;">256-64-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.3.3.3"><span class="ltx_text" id="S5.T2.1.3.3.3.1" style="font-size:70%;">512-128-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.3.3.4"><span class="ltx_text" id="S5.T2.1.3.3.4.1" style="font-size:70%;">512-128-1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.1.1" style="font-size:70%;">Number of embeddings</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.4.4.2"><span class="ltx_text" id="S5.T2.1.4.4.2.1" style="font-size:70%;">20M</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.4.4.3"><span class="ltx_text" id="S5.T2.1.4.4.3.1" style="font-size:70%;">20M</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.4.4.4"><span class="ltx_text" id="S5.T2.1.4.4.4.1" style="font-size:70%;">20M</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.1.1" style="font-size:70%;">Number of tables</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.5.5.2"><span class="ltx_text" id="S5.T2.1.5.5.2.1" style="font-size:70%;">10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.5.5.3"><span class="ltx_text" id="S5.T2.1.5.5.3.1" style="font-size:70%;">32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.5.5.4"><span class="ltx_text" id="S5.T2.1.5.5.4.1" style="font-size:70%;">10</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.6.6.1.1" style="font-size:70%;">Embedding dimension</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.6.6.2"><span class="ltx_text" id="S5.T2.1.6.6.2.1" style="font-size:70%;">32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.6.6.3"><span class="ltx_text" id="S5.T2.1.6.6.3.1" style="font-size:70%;">32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.6.6.4"><span class="ltx_text" id="S5.T2.1.6.6.4.1" style="font-size:70%;">32</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.7.7.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.7.7.1.1" style="font-size:70%;">Number of embedding gathers</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.7.7.2"><span class="ltx_text" id="S5.T2.1.7.7.2.1" style="font-size:70%;">128</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.7.7.3"><span class="ltx_text" id="S5.T2.1.7.7.3.1" style="font-size:70%;">128</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.7.7.4"><span class="ltx_text" id="S5.T2.1.7.7.4.1" style="font-size:70%;">32</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.8.8.1.1" style="font-size:70%;">Locality (P)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.1.8.8.2"><span class="ltx_text" id="S5.T2.1.8.8.2.1" style="font-size:70%;">90%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.1.8.8.3"><span class="ltx_text" id="S5.T2.1.8.8.3.1" style="font-size:70%;">90%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.1.8.8.4"><span class="ltx_text" id="S5.T2.1.8.8.4.1" style="font-size:70%;">90%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Evaluation</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This section evaluates ElasticRec over both CPU-only and CPU-GPU systems.
For brevity and clarity of explanation, we focus our evaluation over CPU-only systems
when studying our microbenchmarks in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS1" title="VI-A Microbenchmarks ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-A</span></span></a>.
We then evaluate state-of-the-art RecSys workloads over CPU-only and CPU-GPU
systems in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS2" title="VI-B State-of-the-art RecSys Workloads (CPU-only) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-B</span></span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.SS3" title="VI-C State-of-the-art RecSys Workloads (CPU-GPU) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-C</span></span></a>, respectively.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Microbenchmarks</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p1.1.1">MLP layer size.</span> When the number of parameters in MLP layers is
increased (from “Light” to “Heavy” in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F12" title="Figure 12 ‣ VI-A Microbenchmarks ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">12</span></a>(a)), the MLP
layers become more compute-intensive and experiences lower QPS. To meet the
target system-wide QPS goal, model-wise allocation must instantiate additional
server replicas which in turn ends up duplicating the entire embedding tables.
Consequently, as the MLP layer’s compute requirement increases, the overall
memory consumption under model-wise allocation also increases rapidly. In
contrast, when the MLP size is increased to “Heavy”, ElasticRec is able to
provision additional resources only to the bottlenecked MLP layers, allowing
only a modest increase in memory consumption.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">Locality in embedding tables.</span>
We discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4" title="IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">IV</span></a> that ElasticRec can allocate more resources only to those embedding shards that are accessed more frequently.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F12" title="Figure 12 ‣ VI-A Microbenchmarks ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">12</span></a>(b), when the locality in table accesses is “High”, ElasticRec instantiates a larger number of replicas for the hot embedding shards while spawning a relatively smaller number of cold embedding shards. Such feature helps ElasticRec minimize wasted memory resources allocated for servicing embedding that are not accessed frequently, achieving <math alttext="2.2\times" class="ltx_math_unparsed" display="inline" id="S6.SS1.p2.1.m1.1"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1b"><mn id="S6.SS1.p2.1.m1.1.1">2.2</mn><mo id="S6.SS1.p2.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">2.2\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.1.m1.1d">2.2 ×</annotation></semantics></math> memory consumption savings when locality is “High”. The baseline model-wise allocation, on the other hand, is not able to save memory allocations at all by exploiting the table’s locality, experiencing almost a constant memory consumption regardless of the level of locality.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1">Total number of tables.</span> Recent large-scale RecSys model architectures contain a large number of sparse features, which translates into a large number of embedding tables. The microbenchmarks in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F12" title="Figure 12 ‣ VI-A Microbenchmarks ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">12</span></a>(c) is designed to demonstrate the scalability of ElasticRec’s table partitioning algorithm when the number of tables is increased (the experiment assumes that all tables are sized identically, i.e., the larger the number of tables, the larger its aggregate memory consumption).
When a model contains multiple tables, ElasticRec applies its table partitioning algorithm separately for each individual table. For instance, if ElasticRec’s partitioning algorithm decides that an embedding table should be partitioned into 4 shards and there exists 10 tables, a total of 40 shards (4 shards <math alttext="\times" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><mo id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><times id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">×</annotation></semantics></math> 10 tables) will be generated, each of which will be subject for resource allocation independently by Kubernetes. Such fine-grained resource management provides ElasticRec with high scalability to multiple tables, showing a large performance gap against baseline model-wise allocation.</p>
</div>
<figure class="ltx_figure" id="S6.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F12.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="110" id="S6.F12.sf1.g1" src="x19.png" width="183"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F12.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="112" id="S6.F12.sf2.g1" src="x20.png" width="183"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F12.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="110" id="S6.F12.sf3.g1" src="x21.png" width="183"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F12.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="110" id="S6.F12.sf4.g1" src="x22.png" width="183"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span> Memory consumption in our microbenchmarks, exploring the impact of (a) MLP size, (b) embedding table locality, (c) number of tables, and (d) the number of shards to partition a table.
</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Number of shards to partition a table.</span>
In Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS2" title="IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>, we discussed how our table partitioning algorithm identifies the optimal number of shards to partition a table. To demonstrate the effectiveness of ElasticRec’s table partitioning, Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F12" title="Figure 12 ‣ VI-A Microbenchmarks ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">12</span></a>(d) shows the overall memory consumption when we manually change the number of partitioned shards. As depicted, as the number of shards increases, the memory consumption generally decreases. Note that the memory consumption plateaus at <math alttext="4" class="ltx_Math" display="inline" id="S6.SS1.p4.1.m1.1"><semantics id="S6.SS1.p4.1.m1.1a"><mn id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><cn id="S6.SS1.p4.1.m1.1.1.cmml" type="integer" xref="S6.SS1.p4.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">4</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p4.1.m1.1d">4</annotation></semantics></math> shards, a point which ElasticRec’s table partitioning algorithm also determines as the optimal partitioning plan to minimize memory consumption. As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S4.SS2" title="IV-B Utility-based Resource Allocation for Embeddings ‣ IV ElasticRec Model Serving Architecture ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>, every container replica incurs a minimally required memory consumption (e.g., code, input buffers) to prevent containers from an out-of-memory error. As such, having an excessively large number of container replicas adds high memory overheads, leading to diminishing returns.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">State-of-the-art RecSys Workloads (CPU-only)</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.10"><span class="ltx_text ltx_font_bold" id="S6.SS2.p1.10.1">Memory consumption.</span> Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F13" title="Figure 13 ‣ VI-B State-of-the-art RecSys Workloads (CPU-only) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">13</span></a> shows the overall memory
consumption when both model-wise allocation (denoted “MW”) and ElasticRec allocate server resources to meet the same target QPS goal. For each of RM1,
RM2, and RM3, ElasticRec’s partitioning algorithm decides to partition
the embedding tables into <math alttext="4" class="ltx_Math" display="inline" id="S6.SS2.p1.1.m1.1"><semantics id="S6.SS2.p1.1.m1.1a"><mn id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><cn id="S6.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S6.SS2.p1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">4</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.1.m1.1d">4</annotation></semantics></math>, <math alttext="3" class="ltx_Math" display="inline" id="S6.SS2.p1.2.m2.1"><semantics id="S6.SS2.p1.2.m2.1a"><mn id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><cn id="S6.SS2.p1.2.m2.1.1.cmml" type="integer" xref="S6.SS2.p1.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">3</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.2.m2.1d">3</annotation></semantics></math>, and <math alttext="3" class="ltx_Math" display="inline" id="S6.SS2.p1.3.m3.1"><semantics id="S6.SS2.p1.3.m3.1a"><mn id="S6.SS2.p1.3.m3.1.1" xref="S6.SS2.p1.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.3.m3.1b"><cn id="S6.SS2.p1.3.m3.1.1.cmml" type="integer" xref="S6.SS2.p1.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.3.m3.1c">3</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.3.m3.1d">3</annotation></semantics></math> shards, respectively. As
such, a total of <math alttext="40" class="ltx_Math" display="inline" id="S6.SS2.p1.4.m4.1"><semantics id="S6.SS2.p1.4.m4.1a"><mn id="S6.SS2.p1.4.m4.1.1" xref="S6.SS2.p1.4.m4.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.4.m4.1b"><cn id="S6.SS2.p1.4.m4.1.1.cmml" type="integer" xref="S6.SS2.p1.4.m4.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.4.m4.1c">40</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.4.m4.1d">40</annotation></semantics></math> shards (4 shards <math alttext="\times" class="ltx_Math" display="inline" id="S6.SS2.p1.5.m5.1"><semantics id="S6.SS2.p1.5.m5.1a"><mo id="S6.SS2.p1.5.m5.1.1" xref="S6.SS2.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.5.m5.1b"><times id="S6.SS2.p1.5.m5.1.1.cmml" xref="S6.SS2.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.5.m5.1d">×</annotation></semantics></math> 10 tables), 96 shards
(3 shards <math alttext="\times" class="ltx_Math" display="inline" id="S6.SS2.p1.6.m6.1"><semantics id="S6.SS2.p1.6.m6.1a"><mo id="S6.SS2.p1.6.m6.1.1" xref="S6.SS2.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.6.m6.1b"><times id="S6.SS2.p1.6.m6.1.1.cmml" xref="S6.SS2.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.6.m6.1d">×</annotation></semantics></math> 32 tables), and 30 shards (3 shards <math alttext="\times" class="ltx_Math" display="inline" id="S6.SS2.p1.7.m7.1"><semantics id="S6.SS2.p1.7.m7.1a"><mo id="S6.SS2.p1.7.m7.1.1" xref="S6.SS2.p1.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.7.m7.1b"><times id="S6.SS2.p1.7.m7.1.1.cmml" xref="S6.SS2.p1.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.7.m7.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.7.m7.1d">×</annotation></semantics></math> 10
tables) for RM1, RM2, and RM3 (see Table <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.T2" title="TABLE II ‣ V-C Workloads ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a>) are
generated for each model’s deployment, allowing Kubernetes to flexibly
tune the number of shard replicas in a fine-grained manner. Overall,
ElasticRec shows substantial reduction in memory usage, achieving
<math alttext="2.2\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p1.8.m8.1"><semantics id="S6.SS2.p1.8.m8.1a"><mrow id="S6.SS2.p1.8.m8.1b"><mn id="S6.SS2.p1.8.m8.1.1">2.2</mn><mo id="S6.SS2.p1.8.m8.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p1.8.m8.1c">2.2\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.8.m8.1d">2.2 ×</annotation></semantics></math>, <math alttext="2.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p1.9.m9.1"><semantics id="S6.SS2.p1.9.m9.1a"><mrow id="S6.SS2.p1.9.m9.1b"><mn id="S6.SS2.p1.9.m9.1.1">2.6</mn><mo id="S6.SS2.p1.9.m9.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p1.9.m9.1c">2.6\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.9.m9.1d">2.6 ×</annotation></semantics></math>, <math alttext="8.1\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p1.10.m10.1"><semantics id="S6.SS2.p1.10.m10.1a"><mrow id="S6.SS2.p1.10.m10.1b"><mn id="S6.SS2.p1.10.m10.1.1">8.1</mn><mo id="S6.SS2.p1.10.m10.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p1.10.m10.1c">8.1\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.10.m10.1d">8.1 ×</annotation></semantics></math> reduction in memory consumption.
Note that ElasticRec’s memory saving is particularly significant for
RM3. This is because the MLP layers in RM3 are much more
compute-intensive than the other two models, causing the model-wise
allocation to replicate a larger number of inference servers as a
whole (detailed later in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F14" title="Figure 14 ‣ VI-B State-of-the-art RecSys Workloads (CPU-only) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">14</span></a>) and suffer more from
needless duplication of cold embedding vectors.</p>
</div>
<figure class="ltx_figure" id="S6.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="95" id="S6.F13.g1" src="x23.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>
CPU-only system’s memory consumption over three state-of-the-art RecSys models (100 queries/sec).
</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="129" id="S6.F14.g1" src="x24.png" width="340"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>
CPU memory utility (left axis) and
number of shard replicas instantiated to meet target QPS (right axis) in CPU-only system.
</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="92" id="S6.F15.g1" src="x25.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>
The number of CPU server nodes required to meet the same QPS target (100 queries/sec) in CPU-only system.
</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.4"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.4.1">Memory utility.</span> ElasticRec’s significant memory reduction
can be attributed to intelligently allocating memory resources based on
its actual utility. We demonstrate how well memory is utilized by measuring
the percentage of embeddings that are actually accessed within a shard
while servicing the first 1,000 queries. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F14" title="Figure 14 ‣ VI-B State-of-the-art RecSys Workloads (CPU-only) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">14</span></a> illustrates the
memory utility of each embedding shard. For brevity, we only show the utility
of the first embedding table from each workload. Each embedding shard is
denoted as S(<math alttext="N" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><mi id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">italic_N</annotation></semantics></math>), where <math alttext="N" class="ltx_Math" display="inline" id="S6.SS2.p2.2.m2.1"><semantics id="S6.SS2.p2.2.m2.1a"><mi id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><ci id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.2.m2.1d">italic_N</annotation></semantics></math> represents the shard ID. For ElasticRec, embedding
shards with a smaller ID contains hotter embeddings (e.g., embeddings in
S1 are hotter than those in S2). Because model-wise allocation does not
partition the embedding tables, a single embedding shard exists that includes
the entire embeddings (denoted S1 under “MW”). On average, model-wise
allocation achieves only <math alttext="6\%" class="ltx_Math" display="inline" id="S6.SS2.p2.3.m3.1"><semantics id="S6.SS2.p2.3.m3.1a"><mrow id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml"><mn id="S6.SS2.p2.3.m3.1.1.2" xref="S6.SS2.p2.3.m3.1.1.2.cmml">6</mn><mo id="S6.SS2.p2.3.m3.1.1.1" xref="S6.SS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><apply id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S6.SS2.p2.3.m3.1.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1.1">percent</csymbol><cn id="S6.SS2.p2.3.m3.1.1.2.cmml" type="integer" xref="S6.SS2.p2.3.m3.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">6\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.3.m3.1d">6 %</annotation></semantics></math> of memory utility. Despite such low memory
utility, model-wise allocation must replicate the entire inference server to
meet target QPS, substantially wasting memory. Such problem becomes especially
more pronounced for the compute-intensive RM3, leading to a large number of
replicated servers and high memory consumption (as discussed in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F13" title="Figure 13 ‣ VI-B State-of-the-art RecSys Workloads (CPU-only) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">13</span></a>). With our ElasticRec, hotter shards consistently
exhibit higher memory utility. More importantly, the number of shard replicas
is proportional to the hotness of each individual shard,
allowing <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.4.2">memory resources to be preferentially allocated to those
shards that will actually utilize it efficiently</em>. Overall, ElasticRec achieves an average <math alttext="8.1\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p2.4.m4.1"><semantics id="S6.SS2.p2.4.m4.1a"><mrow id="S6.SS2.p2.4.m4.1b"><mn id="S6.SS2.p2.4.m4.1.1">8.1</mn><mo id="S6.SS2.p2.4.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p2.4.m4.1c">8.1\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.4.m4.1d">8.1 ×</annotation></semantics></math> higher memory utility.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.6"><span class="ltx_text ltx_font_bold" id="S6.SS2.p3.6.1">Cost.</span> We quantify ElasticRec’s cost
savings by measuring the total number of CPU servers required to satisfy the
same target throughput of <math alttext="100" class="ltx_Math" display="inline" id="S6.SS2.p3.1.m1.1"><semantics id="S6.SS2.p3.1.m1.1a"><mn id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><cn id="S6.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S6.SS2.p3.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.1.m1.1d">100</annotation></semantics></math> QPS (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F15" title="Figure 15 ‣ VI-B State-of-the-art RecSys Workloads (CPU-only) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">15</span></a>).
While the additional communication overheads of ElasticRec adds 31 ms of average latency (<math alttext="8\%" class="ltx_Math" display="inline" id="S6.SS2.p3.2.m2.1"><semantics id="S6.SS2.p3.2.m2.1a"><mrow id="S6.SS2.p3.2.m2.1.1" xref="S6.SS2.p3.2.m2.1.1.cmml"><mn id="S6.SS2.p3.2.m2.1.1.2" xref="S6.SS2.p3.2.m2.1.1.2.cmml">8</mn><mo id="S6.SS2.p3.2.m2.1.1.1" xref="S6.SS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.2.m2.1b"><apply id="S6.SS2.p3.2.m2.1.1.cmml" xref="S6.SS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S6.SS2.p3.2.m2.1.1.1.cmml" xref="S6.SS2.p3.2.m2.1.1.1">percent</csymbol><cn id="S6.SS2.p3.2.m2.1.1.2.cmml" type="integer" xref="S6.SS2.p3.2.m2.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.2.m2.1c">8\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.2.m2.1d">8 %</annotation></semantics></math> of SLA), our proposal
demonstrates its efficiency by cutting down the number of deployed servers
(<math alttext="1.67\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p3.3.m3.1"><semantics id="S6.SS2.p3.3.m3.1a"><mrow id="S6.SS2.p3.3.m3.1b"><mn id="S6.SS2.p3.3.m3.1.1">1.67</mn><mo id="S6.SS2.p3.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p3.3.m3.1c">1.67\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.3.m3.1d">1.67 ×</annotation></semantics></math>, <math alttext="1.67\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p3.4.m4.1"><semantics id="S6.SS2.p3.4.m4.1a"><mrow id="S6.SS2.p3.4.m4.1b"><mn id="S6.SS2.p3.4.m4.1.1">1.67</mn><mo id="S6.SS2.p3.4.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p3.4.m4.1c">1.67\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.4.m4.1d">1.67 ×</annotation></semantics></math>, <math alttext="2.0\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p3.5.m5.1"><semantics id="S6.SS2.p3.5.m5.1a"><mrow id="S6.SS2.p3.5.m5.1b"><mn id="S6.SS2.p3.5.m5.1.1">2.0</mn><mo id="S6.SS2.p3.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p3.5.m5.1c">2.0\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.5.m5.1d">2.0 ×</annotation></semantics></math> reduction vs.
model-wise allocation for RM1/2/3, respectively) and substantially reducing
cost by an average <math alttext="1.7\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p3.6.m6.1"><semantics id="S6.SS2.p3.6.m6.1a"><mrow id="S6.SS2.p3.6.m6.1b"><mn id="S6.SS2.p3.6.m6.1.1">1.7</mn><mo id="S6.SS2.p3.6.m6.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p3.6.m6.1c">1.7\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.6.m6.1d">1.7 ×</annotation></semantics></math> vs. model-wise allocation. These results
highlight the practical benefits and cost-efficiency of ElasticRec’s
utility-based resource allocation.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">State-of-the-art RecSys Workloads (CPU-GPU)</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">We now demonstrate ElasticRec’s effectiveness over CPU-GPU systems. In
ElasticRec, containers that service sparse embedding shards are designed with
only CPU resource requirements while compute-intensive dense DNN shards are
designed as GPU-centric containers utilizing <em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.1">both</em> GPU and CPU resources.
The baseline model-wise allocation, on the other
hand, encapsulates all CPU (sparse embedding layers) and GPU (dense DNN layers)
resources in a single container, having coarse-grained resource allocation.
Below we evaluate ElasticRec’s effect on memory consumption/utility and cost.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.9"><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.9.1">Memory consumption.</span> In our CPU-GPU server, the CPU architecture
specification is different vs. our CPU-only setting
(Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5.SS1" title="V-A Hardware Architecture ‣ V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>). As such, ElasticRec’s partitioning algorithm
decides to partition all the embedding tables into <math alttext="3" class="ltx_Math" display="inline" id="S6.SS3.p2.1.m1.1"><semantics id="S6.SS3.p2.1.m1.1a"><mn id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><cn id="S6.SS3.p2.1.m1.1.1.cmml" type="integer" xref="S6.SS3.p2.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.1.m1.1d">3</annotation></semantics></math> shards per table for all
three models, amounting to a total of <math alttext="30" class="ltx_Math" display="inline" id="S6.SS3.p2.2.m2.1"><semantics id="S6.SS3.p2.2.m2.1a"><mn id="S6.SS3.p2.2.m2.1.1" xref="S6.SS3.p2.2.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.2.m2.1b"><cn id="S6.SS3.p2.2.m2.1.1.cmml" type="integer" xref="S6.SS3.p2.2.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.2.m2.1c">30</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.2.m2.1d">30</annotation></semantics></math> shards, <math alttext="96" class="ltx_Math" display="inline" id="S6.SS3.p2.3.m3.1"><semantics id="S6.SS3.p2.3.m3.1a"><mn id="S6.SS3.p2.3.m3.1.1" xref="S6.SS3.p2.3.m3.1.1.cmml">96</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.3.m3.1b"><cn id="S6.SS3.p2.3.m3.1.1.cmml" type="integer" xref="S6.SS3.p2.3.m3.1.1">96</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.3.m3.1c">96</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.3.m3.1d">96</annotation></semantics></math> shards, and <math alttext="30" class="ltx_Math" display="inline" id="S6.SS3.p2.4.m4.1"><semantics id="S6.SS3.p2.4.m4.1a"><mn id="S6.SS3.p2.4.m4.1.1" xref="S6.SS3.p2.4.m4.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.4.m4.1b"><cn id="S6.SS3.p2.4.m4.1.1.cmml" type="integer" xref="S6.SS3.p2.4.m4.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.4.m4.1c">30</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.4.m4.1d">30</annotation></semantics></math> shards
for RM1, RM2, and RM3, repectively. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F16" title="Figure 16 ‣ VI-C State-of-the-art RecSys Workloads (CPU-GPU) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">16</span></a> summarizes
ElasticRec’s effect on memory consumption. It is worth pointing out that the
benefit of ElasticRec’s memory consumption saving for RM3 (<math alttext="2.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p2.5.m5.1"><semantics id="S6.SS3.p2.5.m5.1a"><mrow id="S6.SS3.p2.5.m5.1b"><mn id="S6.SS3.p2.5.m5.1.1">2.6</mn><mo id="S6.SS3.p2.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.5.m5.1c">2.6\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.5.m5.1d">2.6 ×</annotation></semantics></math>
reduction) is less pronounced compared to CPU-only systems (<math alttext="8.1\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p2.6.m6.1"><semantics id="S6.SS3.p2.6.m6.1a"><mrow id="S6.SS3.p2.6.m6.1b"><mn id="S6.SS3.p2.6.m6.1.1">8.1</mn><mo id="S6.SS3.p2.6.m6.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.6.m6.1c">8.1\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.6.m6.1d">8.1 ×</annotation></semantics></math>
reduction). RM3 has relatively larger MLP layers than RM1/2 so it leads to
lower QPS in a CPU-only system, necessitating a larger number of replicas to fulfill its
compute/memory demands. With CPU-GPU systems, these compute-intensive dense
DNNs are offloaded to the GPU and are executed more efficiently, requiring less
replicas. As such, the inefficiency of duplicated resource allocation is
alleviated under CPU-GPU systems which leads to a smaller gap in memory
consumption between baseline and ElasticRec. Nonetheless, ElasticRec still shows
significant reduction in memory usage, achieving <math alttext="2.7\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p2.7.m7.1"><semantics id="S6.SS3.p2.7.m7.1a"><mrow id="S6.SS3.p2.7.m7.1b"><mn id="S6.SS3.p2.7.m7.1.1">2.7</mn><mo id="S6.SS3.p2.7.m7.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.7.m7.1c">2.7\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.7.m7.1d">2.7 ×</annotation></semantics></math>,
<math alttext="3.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p2.8.m8.1"><semantics id="S6.SS3.p2.8.m8.1a"><mrow id="S6.SS3.p2.8.m8.1b"><mn id="S6.SS3.p2.8.m8.1.1">3.6</mn><mo id="S6.SS3.p2.8.m8.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.8.m8.1c">3.6\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.8.m8.1d">3.6 ×</annotation></semantics></math>, <math alttext="2.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p2.9.m9.1"><semantics id="S6.SS3.p2.9.m9.1a"><mrow id="S6.SS3.p2.9.m9.1b"><mn id="S6.SS3.p2.9.m9.1.1">2.6</mn><mo id="S6.SS3.p2.9.m9.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.9.m9.1c">2.6\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.9.m9.1d">2.6 ×</annotation></semantics></math> smaller memory allocation size.</p>
</div>
<figure class="ltx_figure" id="S6.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="98" id="S6.F16.g1" src="x26.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>
CPU-GPU system’s memory consumption over three state-of-the-art RecSys models (200 queries/sec).
</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="129" id="S6.F17.g1" src="x27.png" width="340"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>
CPU memory utility (left axis) and
number of shard replicas instantiated to meet target QPS (right axis) in CPU-GPU system.
</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="93" id="S6.F18.g1" src="x28.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>
The number of CPU-GPU server nodes required to meet the same QPS target (200 queries/sec) in CPU-GPU system.
</figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.2"><span class="ltx_text ltx_font_bold" id="S6.SS3.p3.2.1">Memory utility.</span> Similar to CPU-only systems, baseline model-wise allocation
still suffers from significant memory underutilization averaging <math alttext="6\%" class="ltx_Math" display="inline" id="S6.SS3.p3.1.m1.1"><semantics id="S6.SS3.p3.1.m1.1a"><mrow id="S6.SS3.p3.1.m1.1.1" xref="S6.SS3.p3.1.m1.1.1.cmml"><mn id="S6.SS3.p3.1.m1.1.1.2" xref="S6.SS3.p3.1.m1.1.1.2.cmml">6</mn><mo id="S6.SS3.p3.1.m1.1.1.1" xref="S6.SS3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p3.1.m1.1b"><apply id="S6.SS3.p3.1.m1.1.1.cmml" xref="S6.SS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S6.SS3.p3.1.m1.1.1.1.cmml" xref="S6.SS3.p3.1.m1.1.1.1">percent</csymbol><cn id="S6.SS3.p3.1.m1.1.1.2.cmml" type="integer" xref="S6.SS3.p3.1.m1.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p3.1.m1.1c">6\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p3.1.m1.1d">6 %</annotation></semantics></math> memory utility
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F17" title="Figure 17 ‣ VI-C State-of-the-art RecSys Workloads (CPU-GPU) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">17</span></a>). ElasticRec again demonstrates its effectiveness in improving
memory utility where hotter shards consistently exhibit higher memory utility.
Furthermore, the number of shards replicated is proportional to the hotness of
each individual shard, making sure that memory resources are allocated to those
shards that actually utilize it effeciently. On average, ElasticRec achieves
an average <math alttext="8\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p3.2.m2.1"><semantics id="S6.SS3.p3.2.m2.1a"><mrow id="S6.SS3.p3.2.m2.1b"><mn id="S6.SS3.p3.2.m2.1.1">8</mn><mo id="S6.SS3.p3.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p3.2.m2.1c">8\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p3.2.m2.1d">8 ×</annotation></semantics></math> higher memory utilization.</p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.5"><span class="ltx_text ltx_font_bold" id="S6.SS3.p4.5.1">Cost.</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F18" title="Figure 18 ‣ VI-C State-of-the-art RecSys Workloads (CPU-GPU) ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">18</span></a> shows the number of CPU-GPU server nodes needed to
reach a target throughput of <math alttext="200" class="ltx_Math" display="inline" id="S6.SS3.p4.1.m1.1"><semantics id="S6.SS3.p4.1.m1.1a"><mn id="S6.SS3.p4.1.m1.1.1" xref="S6.SS3.p4.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.1.m1.1b"><cn id="S6.SS3.p4.1.m1.1.1.cmml" type="integer" xref="S6.SS3.p4.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.1.m1.1c">200</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.1.m1.1d">200</annotation></semantics></math> QPS.
While the additional communication overheads of <span class="ltx_text" id="S6.SS3.p4.5.2">ElasticRec</span> adds 60 ms of average latency (<math alttext="15\%" class="ltx_Math" display="inline" id="S6.SS3.p4.2.m2.1"><semantics id="S6.SS3.p4.2.m2.1a"><mrow id="S6.SS3.p4.2.m2.1.1" xref="S6.SS3.p4.2.m2.1.1.cmml"><mn id="S6.SS3.p4.2.m2.1.1.2" xref="S6.SS3.p4.2.m2.1.1.2.cmml">15</mn><mo id="S6.SS3.p4.2.m2.1.1.1" xref="S6.SS3.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.2.m2.1b"><apply id="S6.SS3.p4.2.m2.1.1.cmml" xref="S6.SS3.p4.2.m2.1.1"><csymbol cd="latexml" id="S6.SS3.p4.2.m2.1.1.1.cmml" xref="S6.SS3.p4.2.m2.1.1.1">percent</csymbol><cn id="S6.SS3.p4.2.m2.1.1.2.cmml" type="integer" xref="S6.SS3.p4.2.m2.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.2.m2.1c">15\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.2.m2.1d">15 %</annotation></semantics></math> of SLA),
ElasticRec requires
<math alttext="1.4\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p4.3.m3.1"><semantics id="S6.SS3.p4.3.m3.1a"><mrow id="S6.SS3.p4.3.m3.1b"><mn id="S6.SS3.p4.3.m3.1.1">1.4</mn><mo id="S6.SS3.p4.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p4.3.m3.1c">1.4\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.3.m3.1d">1.4 ×</annotation></semantics></math>, <math alttext="1.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p4.4.m4.1"><semantics id="S6.SS3.p4.4.m4.1a"><mrow id="S6.SS3.p4.4.m4.1b"><mn id="S6.SS3.p4.4.m4.1.1">1.6</mn><mo id="S6.SS3.p4.4.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p4.4.m4.1c">1.6\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.4.m4.1d">1.6 ×</annotation></semantics></math>, <math alttext="1.2\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p4.5.m5.1"><semantics id="S6.SS3.p4.5.m5.1a"><mrow id="S6.SS3.p4.5.m5.1b"><mn id="S6.SS3.p4.5.m5.1.1">1.2</mn><mo id="S6.SS3.p4.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p4.5.m5.1c">1.2\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.5.m5.1d">1.2 ×</annotation></semantics></math> fewer servers for RM1, RM2, and RM3, respectively,
than baseline model-wise allocation. Overall, these results highlight the
wide applicability of ElasticRec across different hardware platforms.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">Effectiveness to dynamic input query traffic</span>
</h3>
<figure class="ltx_figure" id="S6.F19"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="333" id="S6.F19.g1" src="x29.png" width="403"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Changes in QPS, memory consumption, and tail latency in response to the fluctuation in input traffic. The yellow line represents the target QPS in response to the input traffic. As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S5" title="V Evaluation Methodology ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">V</span></a>, the SLA target is set to 400ms. For brevity, we only show the results over a CPU-only system.
</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.6">At-scale datacenters have a
constantly changing input query traffic, necessitating Kubernetes to adaptively
adjust the number of inference server replicas to deploy. In
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F19" title="Figure 19 ‣ VI-D Effectiveness to dynamic input query traffic ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">19</span></a>, we demonstrate the robustness of ElasticRec to
dynamically changing target QPS goals when executing RM1. We collect the resulting QPS achieved
with baseline and ElasticRec, its memory consumption, and tail latency. The
input query traffic is changed in a total of <math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p1.1.m1.1"><semantics id="S6.SS4.p1.1.m1.1a"><mn id="S6.SS4.p1.1.m1.1.1" xref="S6.SS4.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.1.m1.1b"><cn id="S6.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S6.SS4.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.1.m1.1d">5</annotation></semantics></math> increments, from Time=<math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p1.2.m2.1"><semantics id="S6.SS4.p1.2.m2.1a"><mn id="S6.SS4.p1.2.m2.1.1" xref="S6.SS4.p1.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.2.m2.1b"><cn id="S6.SS4.p1.2.m2.1.1.cmml" type="integer" xref="S6.SS4.p1.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.2.m2.1d">5</annotation></semantics></math>
until Time=<math alttext="20" class="ltx_Math" display="inline" id="S6.SS4.p1.3.m3.1"><semantics id="S6.SS4.p1.3.m3.1a"><mn id="S6.SS4.p1.3.m3.1.1" xref="S6.SS4.p1.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.3.m3.1b"><cn id="S6.SS4.p1.3.m3.1.1.cmml" type="integer" xref="S6.SS4.p1.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.3.m3.1c">20</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.3.m3.1d">20</annotation></semantics></math>, and then decreased at Time=<math alttext="24" class="ltx_Math" display="inline" id="S6.SS4.p1.4.m4.1"><semantics id="S6.SS4.p1.4.m4.1a"><mn id="S6.SS4.p1.4.m4.1.1" xref="S6.SS4.p1.4.m4.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.4.m4.1b"><cn id="S6.SS4.p1.4.m4.1.1.cmml" type="integer" xref="S6.SS4.p1.4.m4.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.4.m4.1c">24</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.4.m4.1d">24</annotation></semantics></math>. As the input traffic changes,
Kubernetes scales in/out the number of replicas based on the underlying
HPA policy. For every change in target QPS, ElasticRec’s achieved QPS
slightly drops in response to the traffic change and the accompanied
change in the deployed shard replicas. However, after the shard replicas
are appropriately provisioned, ElasticRec is able to quickly reach the
target QPS goal while also meeting the tail
latency in a stable manner. The baseline
model-wise allocation, on the other hand, exhibits several shortcomings
as follows. First, the amount of memory allocated is significantly
higher with baseline, reaching <math alttext="3.1\times" class="ltx_math_unparsed" display="inline" id="S6.SS4.p1.5.m5.1"><semantics id="S6.SS4.p1.5.m5.1a"><mrow id="S6.SS4.p1.5.m5.1b"><mn id="S6.SS4.p1.5.m5.1.1">3.1</mn><mo id="S6.SS4.p1.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS4.p1.5.m5.1c">3.1\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.5.m5.1d">3.1 ×</annotation></semantics></math> higher memory consumption than
ElasticRec at its peak usage. Second, model-wise allocation responds much
more slowly than ElasticRec to reach the target QPS (e.g., the QPS of
model-wise starts to increase at around Time=<math alttext="20" class="ltx_Math" display="inline" id="S6.SS4.p1.6.m6.1"><semantics id="S6.SS4.p1.6.m6.1a"><mn id="S6.SS4.p1.6.m6.1.1" xref="S6.SS4.p1.6.m6.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.6.m6.1b"><cn id="S6.SS4.p1.6.m6.1.1.cmml" type="integer" xref="S6.SS4.p1.6.m6.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.6.m6.1c">20</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.6.m6.1d">20</annotation></semantics></math>), exhibiting much
more frequent spikes in tail latency that violates SLA (400ms). These
drawbacks arises because the granularity of resource allocation is much
more coarse-grained under model-wise allocation, taking more time to
initialize an inference server, load the model parameters into memory,
and get ready to service queries. Overall, these experiments illustrates
the ElasticRec’s ability to effectively adjust its resource allocation to
the dynamically fluctuating input query traffic.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS5.5.1.1">VI-E</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS5.6.2">ElasticRec vs. GPU Embedding Caches</span>
</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.6">As mentioned in <span class="ltx_text" id="S6.SS5.p1.6.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S2.SS4" title="II-D Related Work ‣ II Background ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-D</span></span></a></span>,
there exists prior work that utilizes the skewed embedding table access patterns to cache hot embedding vectors inside a GPU-side embedding cache, which helps alleviate the CPU memory bandwidth pressure of embedding table lookups and increase embedding layer’s throughput.
In this section, we compare ElasticRec’s fine-grained resource management vs. baseline monolithic model-wise
resource management augmented with a GPU-side embedding cache. In
<span class="ltx_text" id="S6.SS5.p1.6.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#S6.F20" title="Figure 20 ‣ VI-E ElasticRec vs. GPU Embedding Caches ‣ VI Evaluation ‣ ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">20</span></a></span>,
the baseline model-wise allocation augmented with a GPU embedding cache is
denoted as “model-wise (cache)”. Depending on the
size of the GPU-side embedding cache (which must be implemented inside GPU’s
capacity-constrained HBM), the amount of embedding table lookup operations
captured within the GPU’s embedding cache (HBM) can vary significantly (e.g., <span class="ltx_text" id="S6.SS5.p1.6.3"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib36" title="">36</a>]</cite></span> reports that a GPU-side embedding cache that uses up to
<math alttext="20\%" class="ltx_Math" display="inline" id="S6.SS5.p1.1.m1.1"><semantics id="S6.SS5.p1.1.m1.1a"><mrow id="S6.SS5.p1.1.m1.1.1" xref="S6.SS5.p1.1.m1.1.1.cmml"><mn id="S6.SS5.p1.1.m1.1.1.2" xref="S6.SS5.p1.1.m1.1.1.2.cmml">20</mn><mo id="S6.SS5.p1.1.m1.1.1.1" xref="S6.SS5.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.1.m1.1b"><apply id="S6.SS5.p1.1.m1.1.1.cmml" xref="S6.SS5.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.SS5.p1.1.m1.1.1.1.cmml" xref="S6.SS5.p1.1.m1.1.1.1">percent</csymbol><cn id="S6.SS5.p1.1.m1.1.1.2.cmml" type="integer" xref="S6.SS5.p1.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.1.m1.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p1.1.m1.1d">20 %</annotation></semantics></math> of GPU’s 32 GB HBM can capture <math alttext="40\%" class="ltx_Math" display="inline" id="S6.SS5.p1.2.m2.1"><semantics id="S6.SS5.p1.2.m2.1a"><mrow id="S6.SS5.p1.2.m2.1.1" xref="S6.SS5.p1.2.m2.1.1.cmml"><mn id="S6.SS5.p1.2.m2.1.1.2" xref="S6.SS5.p1.2.m2.1.1.2.cmml">40</mn><mo id="S6.SS5.p1.2.m2.1.1.1" xref="S6.SS5.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.2.m2.1b"><apply id="S6.SS5.p1.2.m2.1.1.cmml" xref="S6.SS5.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.SS5.p1.2.m2.1.1.1.cmml" xref="S6.SS5.p1.2.m2.1.1.1">percent</csymbol><cn id="S6.SS5.p1.2.m2.1.1.2.cmml" type="integer" xref="S6.SS5.p1.2.m2.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.2.m2.1c">40\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p1.2.m2.1d">40 %</annotation></semantics></math> to <math alttext="90\%" class="ltx_Math" display="inline" id="S6.SS5.p1.3.m3.1"><semantics id="S6.SS5.p1.3.m3.1a"><mrow id="S6.SS5.p1.3.m3.1.1" xref="S6.SS5.p1.3.m3.1.1.cmml"><mn id="S6.SS5.p1.3.m3.1.1.2" xref="S6.SS5.p1.3.m3.1.1.2.cmml">90</mn><mo id="S6.SS5.p1.3.m3.1.1.1" xref="S6.SS5.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.3.m3.1b"><apply id="S6.SS5.p1.3.m3.1.1.cmml" xref="S6.SS5.p1.3.m3.1.1"><csymbol cd="latexml" id="S6.SS5.p1.3.m3.1.1.1.cmml" xref="S6.SS5.p1.3.m3.1.1.1">percent</csymbol><cn id="S6.SS5.p1.3.m3.1.1.2.cmml" type="integer" xref="S6.SS5.p1.3.m3.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.3.m3.1c">90\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p1.3.m3.1d">90 %</annotation></semantics></math> of embedding table accesses in GPU
memory). The
purpose of this study is to evaluate the implication of GPU-side embedding
cache on memory consumption savings and its overall competitiveness vs.
ElasticRec, so we conservatively model the baseline “model-wise (cache)” as
follows. Following the methodology by Kwon et al.<span class="ltx_text" id="S6.SS5.p1.6.4"> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.06955v1#bib.bib36" title="">36</a>]</cite></span> we
assume that model-wise (cache) contains a large enough cache to always capture
<math alttext="90\%" class="ltx_Math" display="inline" id="S6.SS5.p1.4.m4.1"><semantics id="S6.SS5.p1.4.m4.1a"><mrow id="S6.SS5.p1.4.m4.1.1" xref="S6.SS5.p1.4.m4.1.1.cmml"><mn id="S6.SS5.p1.4.m4.1.1.2" xref="S6.SS5.p1.4.m4.1.1.2.cmml">90</mn><mo id="S6.SS5.p1.4.m4.1.1.1" xref="S6.SS5.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.4.m4.1b"><apply id="S6.SS5.p1.4.m4.1.1.cmml" xref="S6.SS5.p1.4.m4.1.1"><csymbol cd="latexml" id="S6.SS5.p1.4.m4.1.1.1.cmml" xref="S6.SS5.p1.4.m4.1.1.1">percent</csymbol><cn id="S6.SS5.p1.4.m4.1.1.2.cmml" type="integer" xref="S6.SS5.p1.4.m4.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.4.m4.1c">90\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p1.4.m4.1d">90 %</annotation></semantics></math> of its embedding gather operations within GPU’s local memory while the
remaining <math alttext="10\%" class="ltx_Math" display="inline" id="S6.SS5.p1.5.m5.1"><semantics id="S6.SS5.p1.5.m5.1a"><mrow id="S6.SS5.p1.5.m5.1.1" xref="S6.SS5.p1.5.m5.1.1.cmml"><mn id="S6.SS5.p1.5.m5.1.1.2" xref="S6.SS5.p1.5.m5.1.1.2.cmml">10</mn><mo id="S6.SS5.p1.5.m5.1.1.1" xref="S6.SS5.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.5.m5.1b"><apply id="S6.SS5.p1.5.m5.1.1.cmml" xref="S6.SS5.p1.5.m5.1.1"><csymbol cd="latexml" id="S6.SS5.p1.5.m5.1.1.1.cmml" xref="S6.SS5.p1.5.m5.1.1.1">percent</csymbol><cn id="S6.SS5.p1.5.m5.1.1.2.cmml" type="integer" xref="S6.SS5.p1.5.m5.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.5.m5.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p1.5.m5.1d">10 %</annotation></semantics></math> of embedding gathers are serviced from the CPU.
Compared to baseline model-wise, model-wise (cache) is able to reduce the average latency for embedding layer’s execution by 47%, leading to an increase in each shard instance’s throughput
and thereby reducing the total system-wide memory consumption by 41%.
However, the challenges of the coarse-grained model-wise resource allocation still
remains with model-wise (cache), allowing ElasticRec to reduce overall memory
consumption by <math alttext="1.7\times" class="ltx_math_unparsed" display="inline" id="S6.SS5.p1.6.m6.1"><semantics id="S6.SS5.p1.6.m6.1a"><mrow id="S6.SS5.p1.6.m6.1b"><mn id="S6.SS5.p1.6.m6.1.1">1.7</mn><mo id="S6.SS5.p1.6.m6.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS5.p1.6.m6.1c">1.7\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p1.6.m6.1d">1.7 ×</annotation></semantics></math> vs. model-wise (cache).</p>
</div>
<figure class="ltx_figure" id="S6.F20"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="119" id="S6.F20.g1" src="x30.png" width="403"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>CPU-GPU system’s memory consumption (200 queries/sec).</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We present ElasticRec, a RecSys model serving architecture providing
resource elasticity and high memory efficiency. ElasticRec overcomes the
limitations of conventional model-wise resource allocation by employing a
microservice software architecture to partition a RecSys model into
fine-grained model shards, which act as the unit of resource allocation. By
independently scaling the number of shard replicas, we demonstrated how
ElasticRec effectively addresses the heterogeneous resource demands of sparse
and dense layers in RecSys.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2021R1A2C2091753). Minsoo Rhu is the corresponding author.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Acun, M. Murphy, X. Wang, J. Nie, C.-J. Wu, and K. Hazelwood,
“Understanding training efficiency of deep learning recommendation models
at scale,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the International Symposium on
High-Performance Computer Architecture (HPCA)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Adnan, Y. E. Maboud, D. Mahajan, and P. J. Nair, “Accelerating
Recommendation System Training by Leveraging Popular Choices,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the VLDB Endowment</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
E. K. Ardestani, C. Kim, S. J. Lee, L. Pan, J. Axboe, V. Rampersad, B. Agrawal,
F. Yu, A. Yu, T. Le <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et al.</em>, “Supporting Massive DLRM Inference
Through Software Defined Memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2">Proceedings of the IEEE
International Conference on Distributed Computing Systems (ICDCS)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. Asgari, R. Hadidi, J. Cao, S.-K. Lim, H. Kim <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">et al.</em>, “FAFNIR:
Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent
Reduction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">Proceedings of the International Symposium on
High-Performance Computer Architecture (HPCA)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Balasubramanian, A. Alshabanah, J. D. Choe, and M. Annavaram, “cDLRM: Look
Ahead Caching for Scalable Training of Recommendation Models,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the ACM Conference on Recommender Systems (RecSys)</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bekheet, Mohamed, “Amazon Books Reviews Dataset,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews" title="">https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews</a>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,
G. Anderson, G. Corrado, W. Chai, M. Ispir, R. Anil, Z. Haque, L. Hong,
V. Jain, X. Liu, and H. Shah, “Wide &amp; Deep Learning for Recommender
Systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 1st workshop on deep learning for
recommender systems</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Criteo, “Display Advertising Challenge,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/c/criteo-display-ad-challenge" title="">https://www.kaggle.com/c/criteo-display-ad-challenge</a>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
N. Dmitry and S.-S. Manfred, “On Micro-Services Architecture,”
<em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International Journal of Open Information Technologies</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Docker, “What is a Container?”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.docker.com/resources/what-container/" title="">https://www.docker.com/resources/what-container/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Eisenman, M. Naumov, D. Gardner, M. Smelyanskiy, S. Pupyrev, K. Hazelwood,
A. Cidon, and S. Katti, “Bandana: Using Non-Volatile Memory for Storing
Deep Learning Models,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of Machine Learning and Systems
(MLSys)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Feng, J. Shen, and Y. Fan, “REST: An Alternative to RPC for Web Services
Architecture,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Conference on Future Information
Networks</em>, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Google Cloud, “General-purpose Machine Family for Compute Engine,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/compute/docs/general-purpose-machines" title="">https://cloud.google.com/compute/docs/general-purpose-machines</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Google Cloud, “Google Kubernetes Engine (GKE),”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/kubernetes-engine" title="">https://cloud.google.com/kubernetes-engine</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D. Gouk, S. Lee, M. Kwon, and M. Jung, “Direct
Access,<math alttext="\{" class="ltx_Math" display="inline" id="bib.bib15.1.m1.1"><semantics id="bib.bib15.1.m1.1a"><mo id="bib.bib15.1.m1.1.1" stretchy="false" xref="bib.bib15.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib15.1.m1.1b"><ci id="bib.bib15.1.m1.1.1.cmml" xref="bib.bib15.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib15.1.m1.1d">{</annotation></semantics></math>High-Performance<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib15.2.m2.1"><semantics id="bib.bib15.2.m2.1a"><mo id="bib.bib15.2.m2.1.1" stretchy="false" xref="bib.bib15.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib15.2.m2.1b"><ci id="bib.bib15.2.m2.1.1.cmml" xref="bib.bib15.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib15.2.m2.1d">}</annotation></semantics></math> Memory Disaggregation with
<math alttext="\{" class="ltx_Math" display="inline" id="bib.bib15.3.m3.1"><semantics id="bib.bib15.3.m3.1a"><mo id="bib.bib15.3.m3.1.1" stretchy="false" xref="bib.bib15.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib15.3.m3.1b"><ci id="bib.bib15.3.m3.1.1.cmml" xref="bib.bib15.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib15.3.m3.1d">{</annotation></semantics></math>DirectCXL<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib15.4.m4.1"><semantics id="bib.bib15.4.m4.1a"><mo id="bib.bib15.4.m4.1.1" stretchy="false" xref="bib.bib15.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib15.4.m4.1b"><ci id="bib.bib15.4.m4.1.1.cmml" xref="bib.bib15.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib15.4.m4.1d">}</annotation></semantics></math>,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.5.1">Proceedings of USENIX Annual Technical
Conference</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
GroupLens, “MovieLens Dataset,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://grouplens.org/datasets/movielens/" title="">https://grouplens.org/datasets/movielens/</a>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
gRPC, “gRPC: A High Performance, Open Source Universal RPC Framework,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://grpc.io/" title="">https://grpc.io/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G.-Y. Wei, H.-H. S. Lee,
D. Brooks, and C.-J. Wu, “DeepRecSys: A System for Optimizing End-to-end
At-scale Neural Recommendation Inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the
International Symposium on Computer Architecture (ISCA)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
U. Gupta, S. Hsia, J. Zhang, M. Wilkening, J. Pombra, H.-H. S. Lee, G.-Y. Wei,
C.-J. Wu, and D. Brooks, “RecPipe: Co-Designing Models and Hardware to
Jointly Optimize Recommendation Quality and Performance,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the International Symposium on Microarchitecture
(MICRO)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel,
K. Hazelwood, M. Hempstead, B. Jia, H.-H. S. Lee, A. Malevich, D. Mudigere,
M. Smelyanskiy, L. Xiong, and X. Zhang, “The Architectural Implications of
Facebook’s DNN-based Personalized Recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of
the International Symposium on High-Performance Computer Architecture
(HPCA)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov,
M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis,
M. Smelyanskiy, L. Xiong, and X. Wang, “Applied Machine Learning at
Facebook: A Datacenter Infrastructure Perspective,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of
the International Symposium on High-Performance Computer Architecture
(HPCA)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Hsia, U. Gupta, B. Acun, N. Ardalani, P. Zhong, G.-Y. Wei, D. Brooks, and
C.-J. Wu, “MP-Rec: Hardware-Software Co-design to Enable Multi-Path
Recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the International Conference on
Architectural Support for Programming Languages and Operating Systems
(ASPLOS)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Jain, S. Cheng, V. Kalagi, V. Sanghavi, S. Kaul, M. Arunachalam, K. Maeng,
A. Jog, A. Sivasubramaniam, M. T. Kandemir, and C. R. Das, “Optimizing CPU
Performance for Recommendation Systems At-Scale,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of
the International Symposium on Computer Architecture (ISCA)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai, N. Patil,
S. Subramanian, A. Swing, B. Towles, C. Young, X. Zhou, Z. Zhou, and D. A.
Patterson, “TPU v4: An Optically Reconfigurable Supercomputer for Machine
Learning with Hardware Support for Embeddings,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the
International Symposium on Computer Architecture (ISCA)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H. Kal, S. Lee, G. Ko, and W. W. Ro, “SPACE: Locality-Aware Processing in
Heterogeneous Memory for Personalized Recommendations,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the International Symposium on Computer Architecture
(ISCA)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
G. Kaur and M. M. Fuad, “An Evaluation of Protocol Buffer,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE Southeastcon</em>, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
L. Ke, U. Gupta, B. Y. Cho, D. Brooks, V. Chandra, U. Diril, A. Firoozshahian,
K. Hazelwood, B. Jia, H.-H. S. Lee, M. Li, B. Maher, D. Mudigere, M. Naumov,
M. Schatz, M. Smelyanskiy, X. Wang, B. Reagen, C.-J. Wu, M. Hempstead, and
X. Zhang, “RecNMP: Accelerating Personalized Recommendation with
Near-Memory Processing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the International
Symposium on Computer Architecture (ISCA)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
L. Ke, U. Gupta, M. Hempstead, C.-J. Wu, H.-H. S. Lee, and X. Zhang,
“Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized
Recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the International Symposium on
High-Performance Computer Architecture (HPCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L. Ke, X. Zhang, B. Lee, G. E. Suh, and H.-H. S. Lee, “DisaggRec:
Architecting Disaggregated Systems for Large-Scale Personalized
Recommendation,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2212.00939</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
B. Kim, J. Park, E. Lee, M. Rhu, and J. H. Ahn, “TRiM: Tensor Reduction in
Memory,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">IEEE Computer Architecture Letters</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Kubernetes, “Horizontal Pod Autoscaling,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" title="">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Kubernetes, “Kubernetes: Production-Grade Container Orchestration,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://kubernetes.io/" title="">https://kubernetes.io/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
D. H. Kurniawan, R. Wang, K. S. Zulkifli, F. A. Wiranata, J. Bent,
Y. Vigfusson, and H. S. Gunawi, “EVStore: Storage and Caching Capabilities
for Scaling Embedding Tables in Deep Recommendation Systems,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the International Conference on Architectural Support
for Programming Languages and Operating Systems (ASPLOS)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Y. Kwon, Y. Lee, and M. Rhu, “TensorDIMM: A Practical Near-Memory Processing
Architecture for Embeddings and Tensor Operations in Deep Learning,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the International Symposium on Microarchitecture
(MICRO)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y. Kwon, Y. Lee, and M. Rhu, “Tensor Casting: Co-Designing
Algorithm-Architecture for Personalized Recommendation Training,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the International Symposium on High-Performance Computer
Architecture (HPCA)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y. Kwon and M. Rhu, “Training Personalized Recommendation Systems from (GPU)
scratch: Look Forward Not Backwards,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the
International Symposium on Computer Architecture (ISCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Lee, S. H. Seo, H. Choi, H. U. Sul, S. Kim, J. W. Lee, and T. J. Ham,
“MERCI: Efficient Embedding Reduction on Commodity Hardware via Sub-Query
Memoization,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the International Conference on
Architectural Support for Programming Languages and Operating Systems
(ASPLOS)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
H. Liu, Q. Gao, J. Li, X. Liao, H. Xiong, G. Chen, W. Wang, G. Yang, Z. Zha,
D. Dong, D. Dou, and H. Xiong, “JIZHI: A Fast and Cost-Effective
Model-As-A-Service System for Web-Scale Online Inference at Baidu,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the ACM SIGKDD International Conference on Knowledge
Discovery &amp; Data Mining</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
M. Lui, Y. Yetim, Ö. Özkan, Z. Zhao, S.-Y. Tsai, C.-J. Wu, and
M. Hempstead, “Understanding Capacity-Driven Scale-Out Neural
Recommendation Inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the IEEE International
Symposium on Performance Analysis of Systems and Software (ISPASS)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Meta, “Deep Learning Recommendation Model for Personalization and
Recommendation Systems,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/dlrm" title="">https://github.com/facebookresearch/dlrm</a>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
X. Miao, H. Zhang, Y. Shi, X. Nie, Z. Yang, Y. Tao, and B. Cui, “HET: Scaling
Out Huge Embedding Model Training via Cache-Enabled Distributed Framework,”
<em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2112.07221</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
D. Mudigere, Y. Hao, J. Huang, Z. Jia, A. Tulloch, S. Sridharan, X. Liu,
M. Ozdal, J. Nie, J. Park, L. Luo, J. A. Yang, L. Gao, D. Ivchenko,
A. Basant, Y. Hu, J. Yang, E. K. Ardestani, X. Wang, R. Komuravelli, C.-H.
Chu, S. Yilmaz, H. Li, J. Qian, Z. Feng, Y. Ma, J. Yang, E. Wen, H. Li,
L. Yang, C. Sun, W. Zhao, D. Melts, K. Dhulipala, K. Kishore, T. Graf,
A. Eisenman, K. K. Matam, A. Gangidi, G. J. Chen, M. Krishnan, A. Nayak,
K. Nair, B. Muthiah, M. khorashadi, P. Bhattacharya, P. Lapukhov, M. Naumov,
A. Mathews, L. Qiao, M. Smelyanskiy, B. Jia, and V. Rao, “Software-Hardware
Co-Design for Fast and Scalable Training of Deep Learning Recommendation
Models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the International Symposium on Computer
Architecture (ISCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park,
X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini, D. Dzhulgakov, A. Mallevich,
I. Cherniavskii, Y. Lu, R. Krishnamoorthi, A. Yu, V. Kondratenko, S. Pereira,
X. Chen, W. Chen, V. Rao, B. Jia, L. Xiong, and M. Smelyanskiy, “Deep
Learning Recommendation Model for Personalization and Recommendation
Systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:1906.00091</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
S. Newman, <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Building Microservices</em>.   ” O’Reilly Media, Inc.”, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
NVIDIA, “Accelerating Recommendation System Inference Performance with
TensorRT,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/blog/accelerating-recommendation-system-inference-performance-with-tensorrt/" title="">https://developer.nvidia.com/blog/accelerating-recommendation-system-inference-performance-with-tensorrt/</a>,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
NVIDIA, “NVIDIA Merlin: Recommender System Framework,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/merlin" title="">https://developer.nvidia.com/merlin</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
NVIDIA, “NVIDIA T4,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nvidia.com/en-us/data-center/tesla-t4/" title="">https://www.nvidia.com/en-us/data-center/tesla-t4/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
C. Olston, N. Fiedel, K. Gorovoy, J. Harmsen, L. Lao, F. Li, V. Rajashekhar,
S. Ramesh, and J. Soyke, “TensorFlow-Serving: Flexible, High-Performance ML
Serving,” <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:1712.06139</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Park, M. Naumov, P. Basu, S. Deng, A. Kalaiah, D. Khudia, J. Law, P. Malani,
A. Malevich, S. Nadathur, J. Pino, M. Schatz, A. Sidorov, V. Sivakumar,
A. Tulloch, X. Wang, Y. Wu, H. Yuen, U. Diril, D. Dzhulgakov, K. Hazelwood,
B. Jia, Y. Jia, L. Qiao, V. Rao, N. Rotem, S. Yoo, and M. Smelyanskiy,
“Deep Learning Inference in Facebook Data Centers: Characterization,
Performance Optimizations and Hardware Implications,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint
arXiv:1811.09886</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Prometheus, “Prometheus:From Metrics to Insight,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://prometheus.io/" title="">https://prometheus.io/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
PyTorch, “Torch Serve,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pytorch.org/serve/" title="">https://pytorch.org/serve/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
G. Sethi, B. Acun, N. Agarwal, C. Kozyrakis, C. Trippel, and C.-J. Wu,
“RecShard: Statistical Feature-Based Memory Optimization for Industry-Scale
Neural Recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the International
Conference on Architectural Support for Programming Languages and Operating
Systems (ASPLOS)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
X. Song, Y. Zhang, R. Chen, and H. Chen, “UGACHE: A Unified GPU Cache for
Embedding-based Deep Learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the ACM Symposium
on Operating System Principles (SOSP)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
R. Srinivasan, “RPC: Remote Procedure Call Protocol Specification Version
2,” Tech. Rep., 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, “RM-SSD:
In-Storage Computing for Large-Scale Recommendation Inference,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the International Symposium on High-Performance Computer
Architecture (HPCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Z. Wang, Y. Wei, M. Lee, M. Langer, F. Yu, J. Liu, S. Liu, D. G. Abel, X. Guo,
J. Dong <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">et al.</em>, “Merlin HugeCTR: GPU-Accelerated Recommender System
Training and Inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib56.2.2">Proceedings of the ACM Conference on
Recommender Systems (RecSys)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
M. Wilkening, U. Gupta, S. Hsia, C. Trippel, C.-J. Wu, D. Brooks, and G.-Y.
Wei, “RecSSD: Near Data Processing for Solid State Drive Based
Recommendation Inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the International
Conference on Architectural Support for Programming Languages and Operating
Systems (ASPLOS)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
M. Xie, Y. Lu, J. Lin, Q. Wang, J. Gao, K. Ren, and J. Shu, “Fleche: An
Efficient GPU Embedding Cache for Personalized Recommendations,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the European Conference on Computer Systems (EuroSys)</em>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
C. Yin, B. Acun, C.-J. Wu, and X. Liu, “TT-Rec: Tensor Train Compression for
Deep Learning Recommendation Models,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
W. Zhao, J. Zhang, D. Xie, Y. Qian, R. Jia, and P. Li, “AIBox: CTR Prediction
Model Training on A Single Node,” in <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the ACM
International Conference on Information and Knowledge Management</em>, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun  3 07:48:11 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
