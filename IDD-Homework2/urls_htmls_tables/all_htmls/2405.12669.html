<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges</title>
<!--Generated on Fri May 24 14:50:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.12669v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S1" title="In A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2" title="In A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Scene-Image Multi-modal Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS1" title="In 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Model Design</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS1.SSS1" title="In 2.1 Model Design ‣ 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Double Attention Mechanism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS1.SSS2" title="In 2.1 Model Design ‣ 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Images as A Supplement to Texts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS1.SSS3" title="In 2.1 Model Design ‣ 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Text-to-image Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS1.SSS4" title="In 2.1 Model Design ‣ 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>Text-to-image Retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS2" title="In 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Model Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS2.SSS1" title="In 2.2 Model Training ‣ 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Multi-task Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS2.SSS2" title="In 2.2 Model Training ‣ 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Contrastive Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS2.SSS3" title="In 2.2 Model Training ‣ 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Unsupervised Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS2.SSS4" title="In 2.2 Model Training ‣ 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>Pre-traning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S2.SS3" title="In 2 Scene-Image Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Model Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S3" title="In A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Other Types of Multi-modal Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S3.SS1" title="In 3 Other Types of Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>E-commerce Product-oriented Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S3.SS2" title="In 3 Other Types of Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Text Image Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S3.SS3" title="In 3 Other Types of Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Video-guided Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S3.SS4" title="In 3 Other Types of Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Multi-modal Simultaneous Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S3.SS5" title="In 3 Other Types of Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Multi-modal Chat Machine Translation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S4" title="In A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S5" title="In A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S6" title="In A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Comparison between Existing Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S7" title="In A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Future Directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S8" title="In A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id1.1.id1">\name</span>Huangjun Shen (Co-first author) <span class="ltx_ERROR undefined" id="id2.2.id2">\email</span>huangjunshen@stu.xmu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id3.3.id3">\name</span>Liangying Shao (Co-first author) <span class="ltx_ERROR undefined" id="id4.4.id4">\email</span>liangyingshao@stu.xmu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id5.5.id5">\name</span>Wenbo Li <span class="ltx_ERROR undefined" id="id6.6.id6">\email</span>liwenbo@stu.xmu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id7.7.id7">\name</span>Zhibin Lan <span class="ltx_ERROR undefined" id="id8.8.id8">\email</span>lanzhibin@stu.xmu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id9.9.id9">\name</span>Zhanyu Liu <span class="ltx_ERROR undefined" id="id10.10.id10">\email</span>zhanyuliu@stu.xmu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id11.11.id11">\name</span>Jinsong Su <span class="ltx_ERROR undefined" id="id12.12.id12">\email</span>jssu@xmu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id13.13.id13">\addr</span>School of Informatics, Xiamen University 
<br class="ltx_break"/>361005 Xiamen, China
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">In recent years, multi-modal machine translation has attracted significant interest in both academia and industry due to its superior performance. It takes both textual and visual modalities as inputs, leveraging visual context to tackle the ambiguities in source texts.
In this paper, we begin by offering an exhaustive overview of 99 prior works, comprehensively summarizing representative studies from the perspectives of dominant models, datasets, and evaluation metrics. Afterwards, we analyze the impact of various factors on model performance and finally discuss the possible research directions for this task in the future. Over time, multi-modal machine translation has developed more types to meet diverse needs. Unlike previous surveys confined to the early stage of multi-modal machine translation, our survey thoroughly concludes these emerging types from different aspects, so as to provide researchers with a better understanding of its current state.
</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">As an important natural language processing (NLP) task, machine translation (MT) has undergone several paradigm shifts over the past few decades, from early rule-based translation approaches to current end-to-end neural network based models. However, traditional machine translation models only utilize textual information, neglecting useful information from visual modalities such as images and videos. Therefore, an increasing amount of research focuses on multi-modal machine translation (MMT), which integrates visual information to improve MT.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In general, MMT has important research and application significance. On the one hand, utilizing visual information can provide supplementary context information for source texts, thus alleviating the ambiguity problem caused by the polysemy or omission in text-only MT. Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_tag">1</span></a> gives an example from the movie <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">“In the Heart of the Sea”</em>. It illustrates the difference between MMT and text-only MT. Polysemous words such as <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">“course”</em>, which can refer to both a curriculum and a route, pose a significant challenge for text-only MT models. However, with the help of the image indicating the current location at sea, MMT models can easily determine that the word <em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">“course”</em> means <em class="ltx_emph ltx_font_italic" id="S1.p2.1.4">“route/direction”</em>. On the other hand, MMT has been widely used in various applications, such as subtitle translation and cross-border e-commerce product-oriented MT, showing great commercial value. For example, YouTube<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.youtube.com</span></span></span></span> is capable of translating subtitles automatically, and Alibaba<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.aliyun.com/product/ai/ecommerce_language</span></span></span></span> offers cross-border e-commerce product-oriented MT service.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Therefore, MMT has attracted much attention and become one of the hot research topics in the community of neural machine translation. Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_tag">2</span></a> presents the number of papers related to MMT that are published at top computer science conferences and journals. The increasing number demonstrates the growing research passion for this task in recent years.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we provide a comprehensive review of studies on MMT. Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S1.F3.fig1" title="Figure 3 ‣ 1 Introduction ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_tag">3</span></a> shows the taxonomy of representative studies. First, we provide a preliminary classification of MMT: scene-image MMT and other types of MMT. For scene-image MMT which current studies mainly focus on, we discuss the relevant studies from three perspectives: model design, model training, and model analysis. As for other types of MMT that have only emerged in recent years, we introduce their task definitions, challenges, and related works in detail. Subsequently, we list the commonly used datasets and evaluation metrics for MMT. Furthermore, we compare the impact of different approaches on model performance, including various image encoding methods and performance-boosting techniques. Finally, we point out the future research directions of this task.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Before our work, few surveys (?, ?) have mentioned MMT and only cover a limited number of related works until 2019, when the task was still in its nascent stage. In contrast, our work fully concentrates on MMT, summarizing up to 99 previous papers and thoroughly including representative studies to date. Moreover, with the development of MMT, more and more types of MMT have been created for different needs. Compared to previous surveys, our work extensively covers these emerging types, providing researchers with a comprehensive understanding of the current state of MMT.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S1.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example shows the difference between the conventional text-only MT and MMT models. </figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S1.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Paper publications of MMT at the top computer science conferences and journals.</figcaption>
</figure>
<figure class="ltx_figure ltx_minipage ltx_align_top" id="S1.F3.fig1" style="width:433.6pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S1.F3.fig1.1" style="width:433.6pt;height:140.5pt;vertical-align:-133.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.4pt,0.0pt) scale(0.989182825224905,0.989182825224905) ;"><span class="ltx_ERROR undefined" id="S1.F3.fig1.1.1">{forest}</span>
<p class="ltx_p" id="S1.F3.fig1.1.2">for tree=
grow’=east,
forked edges,
draw,
rounded corners,
node options=,
text width=2.7cm,
anchor=west,

[MMT, parent
[
Scene-image MMT, for tree=child
[
Model Design,
[
Double Attention Mechanism, [
(?, ?, ?, ?, ?, ?, ?)
, model]
]
[
Images as A Supplement to Texts,
[ (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?),model]
]
[
Text-to-image Generation,
[ (?, ?, ?, ?, ?, ?),model]
]
[
Text-to-image Retrieval,
[ (?, ?, ?),model]
]
]
[
Model Training,
[
Multi-Task Learning,
[ (?, ?, ?, ?, ?, ?, ?, ?, ?, ?),model]
]
[
Contrastive Learning,
[ (?, ?, ?, ?, ?),model]
]
[
Unsupervised Learning,
[ (?, ?, ?, ?, ?, ?, ?, ?),model]
]
[
Pre-training,
[ (?, ?, ?, ?),model]
]
]
[
Model Analysis,
[ (?, ?, ?, ?, ?, ?),model]
]
]
[ Other Types of MMT, for tree=child
[
E-commerce Product-oriented Machine Translation,
[ (?, ?, ?),model]
]
[
Text Image Machine Translation,
[ (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?),model]
]
[
Video-guided Machine Translation,
[ (?, ?, ?, ?, ?, ?, ?),model]
]
[
Multi-modal Simultaneous Machine Translation,
[ (?, ?, ?, ?),model]
]
[
Multi-modal Chat Machine Translation,
[ (?),model]
]
]
]

</p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The Taxonomy of Representative Studies on MMT.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Scene-Image Multi-modal Machine Translation</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The scene-image MMT task takes source texts and corresponding scene images that usually depict scenes of people’s daily activities as inputs. Current studies on MMT mainly focus on this task. In the following subsections, we will introduce these studies from three perspectives: model design, model training, and model analysis.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Model Design</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">This line of research concentrates on designing models to utilize the scene image for translation, where the commonly used approaches can be further divided into four categories: using two individual attention mechanisms to extract the text and image contexts (double attention mechanism), using image information as a supplement to textual information, producing text-related image representations and using texts to retrieve images.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Double Attention Mechanism</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">This type of approach considers textual and image information to be equally important,
and thus uses two attention mechanisms to capture modality-specific contextual information.
MNMT (?) is the first attempt in this aspect. They utilize two bi-directional GRU (?) encoders to encode the source text and image separately, and then apply two individual attention mechanisms to obtain a source and an image context vector, which are finally fused to obtain a multi-modal context vector. Along this line, ? (?) introduce an additional gating scalar to quantify the importance of the image context vector at each decoding timestep. Besides, ? (?) propose a hierarchical attention mechanism for scene-image MMT. At each timestep, the attention mechanism at the bottom generates two modality-specific context vectors, and then the attention mechanism at the top fuses the two context vectors into a final multi-modal context vector.
Unlike prior studies that fuse these context vectors through sum or concatenation operations, ? (?) investigate the effectiveness of out product on fusing modality-specific context vectors. To capture fine-grained semantic alignments between text and image, ? (?) first introduce a bi-directional attention network to refine the textual and image representations, which is the basis of two modality-specific attention mechanisms.
Then, they introduce a co-attention mechanism to better fuse modality-specific context vectors on the decoder side. Instead of relying on global or local spatial image features like previous studies, ? (?) only attend to semantic image regions to filter the irrelevant information in input images. Based on word-region alignments, ? (?) employ a word-region similarity matrix to enhance image representations with relevant textual representations.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Images as A Supplement to Texts</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Some researchers argue that source texts are more critical than input images in scene-image MMT. Thus, they mainly focus on how to leverage images as a supplement to source texts.
The related approaches can be further classified into the following three types.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p2.1.1">Using Image Information in The Encoder.</span> In some studies, image information is only incorporated into textual representations during the encoding phase. ? (?) append global and regional image features to the source text, and then feed the concatenated inputs into the encoder for learning contextual representations. ? (?) apply a visual attention mechanism to integrate image information into textual representations. ? (?) employ a Transformer with a multi-modal self-attention mechanism to incorporate the information of two modalities. Specifically, they take the concatenation of textual and image representations as query, and retain only textual representations as key and value, which can better extract the relevant image information.
In contrast to the aforementioned methods,? (?) propose a graph-based multi-modal fusion encoder, which fully exploits fine-grained semantic correspondences between text and image for translation. To build this graph, they treat all words in the source text as textual nodes and the detected image objects as visual nodes. Besides, any two nodes in the same modality are connected by an intra-modal edge, while each textual node representing any noun phrase and the corresponding visual node are connected by an inter-modal edge. Based on the above graph, they sequentially conduct intra-modal and inter-modal fusions in the encoder to update all node states. ? (?) leverage both global and regional image features to enrich textual representations, and then adopt a multi-modal mixup strategy to fuse textual representations and image features. To narrow the modality representation gap, ? (?) propose a layer-level progressive multi-modal fusion strategy. They design a modality difference-aware module to dynamically quantify the modality gap between the source text and image in each encoder layer. Compared to the low-level encoder layers, the high-level encoder layers incorporate more image information into the text.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p3.1.1">Using Image Information in The Decoder.</span> Different from the above-mentioned studies, some researchers focus on only learning textual representations during encoding and then introducing image information to assist translation during decoding. For example, ? (?) propose a two-stage decoding approach for scene-image MMT. They first only utilize the textual information to generate an initial translation, and then leverage this translation, textual representations and image features together to generate a refined translation. ? (?) introduce a context-guided multi-modal capsule network to dynamically produce multi-modal context vectors. Concretely, they stack two capsule networks (?) on the last layer of the decoder, which capture the global and regional image features respectively, and then use the timestep-specific source-side context vector as the guiding signal to dynamically produce multi-modal context vectors for translation. To mitigate the noise caused by irrelevant image regions in scene-image MMT, ? (?) compute a mask matrix between each image region and the source text, which selects the most relevant regions to the text for subsequent image features modeling. Finally, they employ a cross-modal gated fusion method to fuse textual and image features.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p4">
<p class="ltx_p" id="S2.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.p4.1.1">Using Image Information in Both The Encoder and Decoder.</span> More researchers concentrate on the effects of simultaneously integrating image information into both the encoder and decoder of scene-image MMT models. ? (?) study the impacts of initializing encoder and decoder hidden states with global image features. ? (?) explore three ways of incorporating image information: 1) to encode it as the first/last token in the source text, 2) to initialize encoder hidden states, 3) to initialize decoder hidden states. Particularly, they draw the similar conclusion as the previous study (?) that using image features to update the decoder hidden states leads to overfitting. Instead of using image features to represent image information, ? (?) use the predicted class distribution of an image classification network, which contains richer textual semantic information and is more interpretable. Concretely, they initialize the encoder or decoder with the predicted class distribution, and add the projected representation of this distribution to each source word representation.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Text-to-image Generation</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">Some researchers leverage source texts to predict image representations or generate synthetic images to aid the subsequent translation.
Unlike the approaches mentioned above, no images are required during inference in this line of research. There are two reasons for doing so: 1) imagining visual representations from texts is an instinctive reaction of humans, and these visual representations can act as supplementary context to guide the translation, 2) previous studies typically require the image-text pairs as inputs during inference, however, sometimes it is difficult to acquire such pairs in real-world scenarios.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1">Typically, ? (?) use
Conditional Variational Autoencoder (CVAE) (?)
to model a joint distribution over translations and images. During training, they generate two multi-modal joint distributions: one is based on the source text, while the other is based on the source text, target text and image. By minimizing the discrepancy between these two distributions, the model can directly generate the multi-modal joint distribution from the source text during inference, from which a multi-modal representation can be sampled to assist the subsequent translation.
Moreover, ? (?) utilize Generative Adversarial Networks (GAN) (?) to generate photo-realistic and semantic-consistent image representations conditioned on the source text.
Their training tasks include text-to-image generation, image captioning and scene-image MMT.
? (?) employ an additional visual hallucination Transformer to predict hallucinated image representations. In addition to the conventional translation loss, they introduce a hallucination loss to supervise the model generating
the corresponding hallucinated image representations based on the source text. Furthermore, they propose a consistency loss that narrows the gap between training and inference by drawing close the translation distributions from the hallucinated and ground truth image representations.
? (?) introduce a generator to derive a multi-modal representation from the source text.
They propose two kinds of knowledge distillation methods to optimize this generator. The first one directs the generator to extract vital image information from the source text, while the second one encourages the generator to profoundly learn the distribution of real images.
</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p3">
<p class="ltx_p" id="S2.SS1.SSS3.p3.1">Unlike the above-mentioned studies, the following studies utilize synthetic images rather than image representations to aid the translation. Aiming to eliminate the irrelevant content in the image, ? (?) utilize a latent diffusion model (?) to convert the original image into a synthetic image highly corresponding to the source text, and then perform translation based on the synthetic image. ? (?) concentrate on bridging the gap between real images used in training and synthetic images used in inference. They first utilize a latent diffusion model to generate synthetic images, and then feed real and synthetic images to the translation model respectively. During training, they minimize the gap between two types of images by drawing close their image representations on the source side, and the translation distributions based on two types of images on the target side.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Text-to-image Retrieval</h4>
<div class="ltx_para" id="S2.SS1.SSS4.p1">
<p class="ltx_p" id="S2.SS1.SSS4.p1.1">Although the studies on text-to-image generation have achieved certain results, they are severely affected by the effects of text-generated images or image representations.
Thus, some researchers resort to image retrieval
to obtain multiple images semantically relevant to the source text, which not only enriches the image representations
but also extends the applicability of scene-image MMT.
</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p2">
<p class="ltx_p" id="S2.SS1.SSS4.p2.1">For example, ? (?) construct a topic-image lookup table by extracting topic words from source texts in the training set and assuming that these words are relevant to the paired images. During retrieval,
they first search the lookup table with the topic words extracted from the source text to obtain the top-<span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p2.1.1">k</span> ranked images, and then aggregate the representations of the source text and these retrieved images with a visual attention mechanism.
However, there are three issues in this study: 1) such sentence-level retrieval is difficult to obtain the images that properly match with the source text, 2) the irrelevant image regions also introduce noise information even in the matched images, 3) during inference, the source text may contain out-of-vocabulary (OOV) words that do not exist in the topic-image lookup table. To deal with the first two issues, ? (?) present a fine-grained phrase-level retrieval approach. Specifically, they extract the grounded image regions related to the noun phrases in the source text to construct a phrase-level image set. Afterwards, they adopt CVAE to reconstruct the noun phrases using the retrieved image regions, which can effectively filter noise image information. To solve the third issue, ? (?) use the topic words to retrieve images from a search engine and then employ a text-aware attentive visual encoder to filter noise images.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model Training</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The following studies mainly concentrate on the improvement of model training strategies, where the commonly used strategies include multi-task learning, contrastive learning, unsupervised learning as well as pre-training.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Multi-task Learning</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Inspired by the success of other NLP tasks, some studies explore multi-task learning to enhance the semantic alignments between text and image.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">? (?) decompose scene-image MMT into two sub-tasks: text translation and grounded representation prediction. The latter one uses textual representations to predict
the corresponding image representations.
In this way, the text encoder is encouraged to learn the visually grounded representations for the source language. Via these two sub-tasks, they can fully exploit parallel texts and monolingual image-text data to effectively train their model. Furthermore, ? (?) propose an image-text shared space learning objective, which draws close the matched textual and image features, while pushing the mismatched ones further. By doing so, the model is able to construct an image-text shared space to generate better modality-shared representations. To fully utilize the image information, ? (?) propose a visual agreement regularized training loss. They adopt a joint training strategy for both source-to-target and target-to-source translation models, and encourage the models to share the same focus on the image when generating semantic equivalent vision-related words.
? (?) supervise the attention mechanism across the source text and image by annotated word-region alignments, and the cross-lingual attention mechanism across the source text and target text is supervised by word alignments between two languages. Inspired by the finding (?) that entities are most informative in images, ? (?) put forward a reconstruction task to enhance the entity representations. They first replace the embeddings of the visually grounded entities in the source text with the corresponding image object representations to generate a multi-modal input, and then force the model to reconstruct the original source text from the multi-modal input.
? (?) introduce two auxiliary training objectives to assist the main translation task. One is an object-masking loss which grounds the translation on the source-relevant image objects by masking the irrelevant ones. This loss is estimated by the similarity between the masked objects and the source text, which would penalize the undesirable object masking. The other objective is a vision-weighted translation loss that tends to reward the generation of vision-related words. In contrast to previous works that always focus on the alignments of bilingual texts or the combination of the source/target text and the paired image, ? (?) propose a novel framework comprising three tasks to establish a triplet alignment among the source and target texts together with the paired image. The first task is the basic multi-modal translation, the second one utilizes a multi-modal context vector to reconstruct the source text, and the third one employs the multi-modal context vector to perform multi-label classification.
To fully exploit monolingual image-text data, ? (?) introduce an additional image caption denoising task that randomly masks some tokens in the caption and predicts them based on the rest caption and image. Meanwhile, ? (?) enhance scene-image MMT with an image caption denoising task and a text translation task by utilizing both monolingual image-text data and parallel texts. Their image caption denoising task is similar to the task considered by ? (?), except that it predicts the complete caption. Besides, they present two ways to incorporate image information: using an image encoder to encode continuous image features, and using an image captioning model to generate keywords of the image, which are then appended to the original source text.
? (?) propose an auxiliary visual question answering (VQA) task to enhance interactions between two modalities, and utilize large language models (LLMs) to transform traditional datasets like Multi30K (?) into VQA patterns.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Contrastive Learning</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Motivated by the achievements of contrastive learning in other NLP tasks, researchers also implement this strategy to better learn image representations and enhance semantic alignments between text and image.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">In this regard, ? (?) construct positive samples of the original input image via image spatial and appearance transformation, and sample negative samples from the rest images in the same batch.
Despite the promising performance, scene-image MMT models still face the challenge of input degradation, that is, the models tend to focus more on textual information while overlooking image information. To increase image awareness, ? (?) employ contrastive learning to maximize the mutual information between text and image on both the source and target sides. On the source side, they use InfoNCE (?) where the matched source texts and images are positive pairs, and the mismatched image-text pairs in the same batch are the negative pairs. On the target side, they follow ? (?) to maximize the discrepancy between two translation distributions based on the original and the deteriorated images, respectively.
To enable zero-shot and few-shot translations for low-resource languages, ? (?) propose a cross-modal contrastive learning method, which aligns different languages with images as a pivot. They introduce two contrastive learning objectives: 1) the sentence-level objective that involves positive pairs consisting of the matched source texts and images, and negative pairs including other mismatched texts and images in the same batch, 2) the token-level objective that focuses on the source text tokens and the text-aware image tokens. To obtain these text-aware image tokens, they apply an attention mechanism where the query is word-level text tokens, and the key, value are patch-level image tokens, with the text tokens and the generated text-aware image tokens of the same index constituting positive pairs and others constituting negative pairs.
Likewise, ? (?) apply contrastive learning at both the sentence and word levels to scene-image MMT. They first leverage an image captioning model to generate the captions of input images, and an object detection model to generate the object labels of image objects. When using contrastive learning at the sentence level, the generated captions are used as positive samples of the original source texts, and other irrelevant source texts are negative samples. When adopting the word-level contrastive learning, the generated object labels are used as positive samples of the corresponding image objects, and other irrelevant object labels in the same image are negative samples.
? (?) extend their previous work (?) by proposing a progressive contrastive learning strategy to refine the model training. Concretely, for each training sample, they simply apply a different dropout mask to the graph encoding to construct a positive sample, and consider two kinds of negative samples: 1) random negative samples that are other multi-modal graphs within the same batch, 2) hard negative samples that are constructed by corrupting the cross-modality alignments or image features of the input graph. Particularly, the hard negative samples are gradually introduced as the training progresses.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Unsupervised Learning</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">In general, with the help of large-scale training corpora, supervised scene-image MMT models show competitive translation performance. However, for many low-resource language pairs, it is expensive to collect large-scale high-quality parallel corpora. Notice that it is easier to obtain the monolingual image-text data for these languages, thus researchers deviate themselves into unsupervised learning based scene-image MMT.
Moreover, the semantically equivalent visual descriptions in different languages usually refer to the same visual content (e.g., the word <em class="ltx_emph ltx_font_italic" id="S2.SS2.SSS3.p1.1.1">“bicycle”</em> in English and the word <em class="ltx_emph ltx_font_italic" id="S2.SS2.SSS3.p1.1.2">“vélo”</em> in French both refer to the bicycle objects in images). Therefore, the utilization of large-scale monolingual image-text datasets can facilitate learning a unified semantic space of different languages with images as a pivot.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">? (?) first explore zero-resource scene-image MMT. They apply a pair-wise ranking loss to align the matched image-text pairs in both source and target languages, which can force encoders to map different modalities into a common multi-modal semantic space. Besides, the decoder is required to generate the target language description of the input image.
? (?) divide image captioning into two steps: the image is first translated to a source sentence using an image-to-source captioning model, and then the source sentence is translated to a target sentence using a source-to-target translation model. Notice that only the source-to-target translation model is utilized during inference. Different from (?), ? (?) introduce two training objectives, the denoising auto-encoding loss based on the source text and image, and the cycle-consistency loss based on back-translation (?) which pulls close the input text and the output of the back-translation model with the image as a pivot. Besides, they also design a controllable attention module to deal with both uni-modal and multi-modal inputs. ? (?) propose to learn the translation in an easy-to-hard progressive way. Their model first learns the word-level translation by generating image captions in the source and target languages, and then generates source-target pseudo text pairs pivoted on the same images. Afterwards, the model learns the sentence-level translation by re-weighting the pseudo pairs at both the sentence and token levels. Besides, ? (?) define four tasks for joint training. The first one is an unsupervised scene-image MMT task based on back-translation, and the second task performs image-text matching in both source and target languages to map two modalities into a modality-shared semantic space. The third task feeds the image into pre-trained image captioning models of different languages, generating pseudo text pairs pivoted on the same image for translation. Similar to the first one, the last task pulls close the texts generated by image captioning models and the outputs of the back-translation models. ? (?) introduce a transitive relation to estimate the similarity between two samples in the monolingual image-text data: if two images are similar to each other, their corresponding texts are also similar. Based on this, they propose several contrastive learning objectives to strengthen the semantic alignments between text and image, including image-to-image, image-to-text, and text-to-text contrastive objectives. ? (?) leverage language scene graphs (LSGs) and visual scene graphs (VSGs) to better represent source texts and images. They design four learning strategies for unsupervised training. The first one is cross-modal scene graph (SG) aligning, which encourages the textual and visual nodes that serve a similar role in the LSG and the VSG to be closer. The second strategy aims to reconstruct the source text from the VSG, and the image representations from the LSG. The third one performs back-translation with the SG as a pivot, and the last strategy draws close the texts generated by image captioning models and the outputs of the back-translation models.
? (?) incorporate images at the word level to augment the lexical mappings of different languages. They concatenate the features of related images to the embeddings of corresponding words in the source text, and modify the embedding layer information. Based on this multi-modal input, they train the model with unsupervised back-translation and denoising auto-encoding. Besides, a mask matrix is adopted to highlight the relationship between images and their corresponding subwords and isolate the impact of images on other words.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Pre-traning</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">In recent years, the pre-training and fine-tuning paradigm has gained wide attention in both academia and industry due to its superior performance. This paradigm enables scene-image MMT models to acquire foundational knowledge through pre-training tasks, and then enhance their performance on downstream tasks via fine-tuning. In scene-image MMT, tasks related to cross-modal alignments between text and image are often introduced during pre-training, which allows models to generate better modality representations for downstream applications.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p2">
<p class="ltx_p" id="S2.SS2.SSS4.p2.1">In this respect, ? (?) present a pre-training task named visual translation language modeling. Specifically, the model takes the source text, target text and the detected image objects as inputs. Subsequently, a portion of the input tokens are randomly masked, and the model is required to predict the masked tokens. If an image object is masked, the model is required to restore its correct object label.
Moreover, ? (?) utilize a large-scale external image-text dataset as pre-training data, and feed their model with the source text, detected image objects and their object labels as inputs. They introduce two pre-training tasks: the first task aims to restore the text tokens masked in the inputs, and the second one is to judge whether the given text and image are matched.
? (?) point out that the common random selection strategy adopted in masked language modeling ignores the fact that task-related information varies from token to token. Therefore, they design a more informed masking strategy which masks more pronouns and objects with gender information.
To mitigate the scarcity of annotated scene-image MMT data, especially for low-resource languages, ? (?) propose a two-stage learning approach with the pre-trained mBART (?) as the text model and M-CLIP (?) as the image encoder. In the first stage, the model is trained with the image captioning task by using M-CLIP to encode the image, which forces the decoder to rely on image information to generate corresponding captions.
In the second stage, the model is trained with the text-to-text translation task by using M-CLIP to obtain relevant image information from the source text.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Model Analysis</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The basic intuition behind scene-image MMT is that models can improve translation quality by incorporating image context. However, some studies show that the utilization of image context does not consistently enhance model performance. In order to better understand the role of image context in scene-image MMT, researchers have conducted a series of analyses, which suggest that image context can play an important role when textual context is ambiguous or insufficient, but tends to be less effective when textual context is sufficient.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">To determine whether scene-image MMT models are aware of image context, ? (?) quantify the performance difference of a model given the congruent image or a random incongruent image. They evaluate some publicly available models and find that not all models actually use the image context to produce better translations. However, ? (?) posit that the texts in the widely-used Multi30K dataset are often very simple, short and repetitive, leading to the limited effectiveness of image context. To investigate this hypothesis, they introduce several input degradation strategies that remove some crucial information from source texts to create limited textual context. The results indicate that models are capable of leveraging the input images to generate better translations under such limited textual context. Furthermore, ? (?) follow ? (?) to reconduct experiments on the large-scale How2 dataset (?), where the texts are longer and non-repetitive. Their findings reveal that the quality of visual embeddings rather than the complexity of texts in the existing datasets should be improved. Along this line, ? (?) explore the impact of image encoders of scene-image MMT models. As implemented in (?), they design some probing tasks and find that stronger image encoders are more helpful for learning translation from the image modality. Meanwhile, they also find that models with the enhanced image features achieve improvements in the BLEU score (?) but show no advantage in the probing tasks, which indicates current automatic evaluation metrics might not be suitable for scene-image MMT. To deeply analyze the contribution of image context, ? (?) adopt the gated fusion approach during training, which allows the model to voluntarily decide the usefulness of image context. Through the gating matrix, they find that image context only influences the early training stage when textual representations are poorly learned. They further discover that under sufficient textual context, the improvements achieved by the multi-modal models over text-only models result from the regularization effect of image context. Using gender-specific images, ? (?) explore the translation from a gender-neutral language into a language with natural gender. The results show that the integration of image context largely assists models in inferring the correct gender.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Other Types of Multi-modal Machine Translation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Although the majority of research has primarily centered around scene-image MMT, there are also notable efforts dedicated to other types of MMT, including e-commerce product-oriented MT, text image machine translation (TIMT), video-guided MT, multi-modal simultaneous machine translation (multi-modal SiMT) and multi-modal chat MT. Each of these tasks exhibits distinct characteristics, and we will introduce them from three aspects: task definition, challenge and related works.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>E-commerce Product-oriented Machine Translation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this task, the translation models take both product images and their descriptions as inputs, and then output description translations. It finds wide applications in cross-border e-commerce, providing significant convenience for consumers interested in purchasing goods from overseas.
Overall, the research on this task still faces the following challenges. First, this task suffers from data scarcity. Second, product descriptions often contain specialized jargons that are ambiguous to be translated without product images.
Third, unlike conventional image descriptions, product descriptions are related to images in more complex ways, involving various visual aspects such as objects, shapes, colors, or even subjective styles.
</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">(?) is the first attempt to explore this task. First, the authors build a bilingual product description dataset collected from an online shop website. In this dataset, each sample is made up of a product image, an English description and its German translation. Then they compare the performance of three different models on this dataset: a phrase-based statistical MT (PBSMT) model, a text-only MT model and a MMT model. Their findings reveal that the PBSMT model performs best, followed by the MMT model, and the text-only MT model performs worst. In addition, they observe that using the MMT model to rerank the outputs of the PBSMT model can significantly improve the Translation Edit Rate (TER) score (?). In order to further evaluate the performance of different translation models, ? (?) conduct a human evaluation on the outputs of the aforementioned three models. The results indicate that human evaluators prefer PBSMT translations to both the text-only MT and MMT models in over 56% of the cases. Nonetheless, human evaluators rank the translations from the MMT model higher than those from the text-only MT model in over 88% of the cases, suggesting that images indeed assist translation.
Compared to (?), ? (?) construct a larger and more complex dataset, where each sample consists of an English product description, its Chinese translation, product images of different colors and poses, along with product categories and attribute labels. To learn better semantic alignments between bilingual texts and product images, they propose a unified pre-training and fine-tuning framework for this task. Concretely, they introduce three pre-training tasks. The first task aims to reconstruct masked words in both languages with bilingual textual and image context information, the second task conducts semantic matching between the source text and image, and the third task masks the source words conveying product attributes, forcing the model to predict the attributes according to the input product images.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Text Image Machine Translation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this task, images containing texts in the source language are fed into the translation models to generate either the translations in the target language or images containing the translations. There are a number of commercial applications involving TIMT, such as Google Translate’s Instant Camera<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://blog.google/products/translate/googletranslates-instant-camera-translation-gets-upgrade</span></span></span></span> and Google Lens<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.googleblog.com/2019/09/giving-lens-new-readingcapabilities-in.html</span></span></span></span>.
In sum, the research on this task faces the following challenges. First, data scarcity still remains a major challenge for this task. Second, dominant approaches are cascaded, suffering from error propagation and high latency. They first use an optical character recognition (OCR) model to detect source texts in the input image, followed by a text-only MT model to translate these texts. Sometimes, the translations need to be rendered back into the original input image with optimized font size and location. In this way, existing large-scale OCR and MT datasets and various submodels can be exploited to construct high-quality cascaded models.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">To address the above challenges, several studies develop their own datasets, such as BLATID (?) and OCRMT30K (?), and present various end-to-end frameworks for this task.
In this regard, ? (?) explore generating images containing translations for the first time. During training, their model predicts the next timestep target image conditioned on the source image and ground truth target image at the previous timestep.
Along this line, ? (?) regard the source and target images as pixel sequences converting from their grayscale maps, and apply Byte Pair Encoding (BPE) (?) to segment the images.
Different from the first two studies, the following studies mainly focus on generating target texts rather than target images. For example, ? (?) adopt a multi-task learning framework with two sub-tasks. The main task tends to translate the source image into the target text, while the auxiliary task aims to recognize the source text from the source image.
Moreover, ? (?) incorporate the recognition history textual information into the translation decoder via the attention mechanisms when optimizing the model with the above two sub-tasks.
Unlike the above-mentioned studies with two sub-tasks,
? (?) improve the model performance by jointly training with OCR, MT and TIMT tasks. In this way, the model can fully exploit large-scale external OCR and MT datasets to enhance its image encoder and target text decoder. In (?), the translation decoder is initialized with a text decoder pre-trained on MT data. Experimental results indicate that their model outperforms the cascaded model in the scenarios of both single-line and multi-line translations. Furthermore, ? (?) combine pre-training with multi-task learning methods. In the first stage, they pre-train an OCR model to recognize the source text from the image and a MT model to translate the source text. Then in the second stage, the OCR encoder and text-only MT decoder, along with an additional module to bridge the semantic gap between the encoder and decoder, are integrated to construct an end-to-end model, which performs both OCR and TIMT tasks.
? (?) present a multi-hierarchy cross-modal knowledge distillation strategy. They first pre-train a teacher model with a large bilingual text corpus. Subsequently, they conduct the global knowledge distillation by pulling close the outputs between the teacher encoder and the student encoder, while the local knowledge distillation is performed by elementwise matching the representations derived from the cross-attention mechanisms of the teacher and student models. Furthermore, ? (?) introduce a multi-teacher knowledge distillation approach, which can transfer different kinds of knowledge to corresponding sub-modules of a student model. First, they pre-train an OCR model and a text-only MT model as the teacher models. Afterwards, the student image encoder is optimized with the guidance from the OCR image encoder, and the student contextual encoder and decoder are improved by transferring the knowledge from the text-only MT encoder and decoder. Particularly, both sentence-level and token-level knowledge distillation are incorporated to better enhance the translation performance. The previous study (?) links the OCR encoder and MT decoder to facilitate the TIMT task, which, however, ignores the gap between the OCR and MT tasks and thus leads to limited performance. To deal with this issue, ? (?) propose an architecture with two types of adapters to eliminate the task gap. The first type is inserted between the OCR image encoder and the text-only MT encoder, aiming to align the image embeddings and text embeddings. The second type connects the OCR contextual encoder and the text-only MT decoder, with the aim to align the contextual semantic feature space.
To improve model performance, ? (?) adopt both intra-modal and inter-modal contrastive learning. When using the intra-modal contrastive learning, they mainly consider two kinds of representation differences: 1) text-text, where the source texts and the outputs of the back-translation models are positive pairs, 2) image-image, where different-format images corresponding to the same source texts are positive pairs. Besides, the inter-modal contrastive learning draws close the representations of the matched source texts and images.
Unlike the above-mentioned studies, ? (?) apply TIMT to the comics domain. They extract semantic tags of each comic scene as image context and prepend these tags to the source text.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">In complex and realistic scenarios, cascaded translation systems exhibit superior performance. Different from the above-mentioned end-to-end approaches, ? (?) present a cascaded TIMT model with a multi-modal codebook, which can leverage the input image to generate latent codes encoding the information of relevant or correct texts, thus providing useful supplementary information to alleviate the OCR error propagation. Moreover, they propose a multi-stage training framework that makes full use of additional bilingual texts and OCR data to enhance the model training.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Video-guided Machine Translation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In this task, given pairs of source text and video, the translation models are required to automatically generate the target translations by combining both textual and video information. This task has many real-world applications, such as subtitle translation for social media.
Similar to the aforementioned types of MMT tasks, the studies on this task also face many challenges. First, this task suffers from data scarcity. Second, compared with images, videos provide richer visual information such as actions and temporal transitions, while also introducing some visual redundancy. Therefore, how to effectively extract and utilize video information is an important challenge in this task.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The studies on this task can be roughly divided into two categories: model design and dataset construction. ? (?) first explore the task of video-guided MT, where they propose a multi-modal sequence-to-sequence model with two attention mechanisms to respectively capture textual and video information. Considering the order information in video frames, ? (?) add positional embeddings to keyframe-based video representations. Besides, they employ not only a video encoder to capture motion features in videos, but also an image encoder to capture object and scene features in each keyframe. Note that previous studies mainly focus on the representations of video motions to solve the verb sense ambiguity in the source text, leaving the noun sense ambiguity unsolved. To deal with this issue, ? (?) propose a spatial hierarchical attention module that utilizes the spatial representations in input videos. Concretely, they first apply an object-level attention layer to summarize the object-level spatial representations into frame-level spatial representations, and then employ another frame-level attention layer to summarize all ordered frame-level spatial representations into video-level representations.
Along this line, ? (?) introduce two training objectives based on the selective attention model (?): 1) the frame attention loss allowing the model to focus more on the central frames where the subtitles occur, 2) the ambiguity augmentation loss that enables the model to pay more attention to the possibly-ambiguous data.
? (?) introduce a video-guided MT model with a cross-modal encoder. This model is trained with a contrastive learning objective, which brings close the representations of the matched source texts and videos, while pushing the representations of the unrelated source texts and videos farther.
To better exploit video information, ? (?) employ a spatial-temporal graph network (?) to capture object information among frames in videos. They define four tasks for unsupervised training: 1) unsupervised video-guided MT, which pulls close the input text and the output of the back-translation model with the video as a pivot, 2) video-text matching, which can map textual and video representations into a modality-shared semantic space, 3) video captioning for paired-translation, which feeds the video into pre-trained video captioning models of different languages to generate source-target pseudo text pairs for translation, 4) video captioning for back-translation, which pulls close the texts generated by video captioning models and the outputs of the back-translation models.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">To overcome data scarcity, some studies (?, ?, ?) release several multi-modal video datasets. The How2 dataset (?) consists of videos, utterance-level English subtitles, aligned Portuguese translations, and video-level English summaries. To increase the diversity of video captions, each video in the VaTeX dataset (?) is equipped with 10 English and 10 Chinese descriptions. ? (?) release the BigVideo dataset, whose size is an order of magnitude larger than the size of the previous largest available dataset, and ? (?) release the large-scale EVA dataset in the movie and TV domain.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-modal Simultaneous Machine Translation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Similar to traditional simultaneous machine translation (SiMT), the speech is first converted to the source texts in this task, and then the translation models are required to generate the target translations based on the continuous source text streams and given images. This task can be applied to some realistic applications where images are presented before the complete source text streams are available, such as presentations with slides and news video broadcasts.
The primary challenge of this task lies in how to effectively use image information as the supplementary context to enrich incomplete textual information, so as to obtain target translations with low latency and high quality.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">In this respect, (?) is the first attempt to explore the multi-modal SiMT task, where a hierarchical attention mechanism (?) is used to incorporate textual and image information. Following the previous works (?, ?), ? (?) integrate image information into the encoder or decoder modules through a multi-modal attention mechanism. Experimental results show that utilizing image information can provide the model with the missing source context, allowing it to correctly translate the gender-marked words and deal with the differences in word order. Besides, regional image features are more effective than global image features in this task. ? (?) further introduce reinforcement learning to this task. They propose three strategies to integrate image information: 1) using image features to initialize the agent network, 2) applying a multi-modal attention mechanism to generate the image context vector in the agent network, 3) taking a MMT model as the environment network. In contrast to previous methods that use RNN networks, ? (?) pioneer the use of Transformer architectures. Apart from the main multi-modal SiMT task, they design an auxiliary training task where the visual attention mechanism is supervised by annotated phrase-region alignments, so that the additional image information can better complement the missing source context.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Multi-modal Chat Machine Translation</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Conversations in real-life scenarios often involve multi-modal information and their content largely depends on the scenes that speakers observe.
Therefore, visual information can be a good supplement to dialogue history context. In this task, the model leverages both bilingual dialogue history contexts and the associated visual context to translate the current source utterance. This task finds its applications in subtitle translation for movies and TV episodes, especially some conversational scenes.
In real life, conversations usually involve multi-sense words and pronominal anaphora issues. Thus, how to efficiently utilize visual information to resolve these problems remains a challenge.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">(?) is the first attempt to explore this task. In this work, the authors create a Multimodal Sentiment Chat Translation Dataset (MSCTD), aiming to generate more accurate dialogue translations with the guidance of additional visual context. This dataset includes 17,841 multi-modal bilingual conversations, each consisting of multiple quadruples in the format of ⟨<em class="ltx_emph ltx_font_italic" id="S3.SS5.p2.1.1">English utterance, Chinese/German utterance, image, sentiment</em>⟩. Based on this dataset, they adapt existing MMT models and textual chat translation models to construct several benchmarks for this task. Experimental results demonstrate that integrating image information indeed improves the quality of dialogue translations.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<p class="ltx_p ltx_align_center" id="S3.T1.6"><span class="ltx_text" id="S3.T1.6.6" style="font-size:50%;">[width=0.3]



<span class="ltx_tabular ltx_align_middle" id="S3.T1.6.6.6">
<span class="ltx_tr" id="S3.T1.6.6.6.7">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T1.6.6.6.7.1" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.1.1">Dataset</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.6.6.6.7.2" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.2.1">Sub-Dataset</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.6.6.6.7.3" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.3.1">Ambigious</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.6.6.6.7.4" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.4.1">Domain</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.6.6.6.7.5" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.5.1">Task</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.6.6.6.7.6" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.6.1">Lauguage</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.6.6.6.7.7" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.7.1">Image</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.6.6.6.7.8" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.8.1">Video</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.6.6.6.7.9" style="padding:1.25pt 2.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.7.9.1">Text</span></span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.8">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.8.1" style="padding:1.25pt 2.3pt;">IAPR TC-12</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.8.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.8.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.8.4" style="padding:1.25pt 2.3pt;">Daily Activity</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.8.5" style="padding:1.25pt 2.3pt;">S-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.8.6" style="padding:1.25pt 2.3pt;">DE, EN</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.8.7" style="padding:1.25pt 2.3pt;">20.0K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.8.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.8.9" style="padding:1.25pt 2.3pt;">20.0K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.9">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.9.1" style="padding:1.25pt 2.3pt;">Multi30K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.9.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.9.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.9.4" style="padding:1.25pt 2.3pt;">Daily Activity</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.9.5" style="padding:1.25pt 2.3pt;">S-MT, Si-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.9.6" style="padding:1.25pt 2.3pt;">CS, DE, EN, FR</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.9.7" style="padding:1.25pt 2.3pt;">31.0K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.9.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.9.9" style="padding:1.25pt 2.3pt;">31.0K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.10">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.10.1" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.10.1.1">MLT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.10.2" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.10.2.1">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.10.3" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.10.3.1">✓</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.10.4" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.10.4.1">Daily Activity</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.10.5" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.10.5.1">S-MT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.10.6" style="padding:1.25pt 2.3pt;">DE, EN</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.10.7" style="padding:1.25pt 2.3pt;">53.9K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.10.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.10.9" style="padding:1.25pt 2.3pt;">53.9K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.11">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.11.1" style="padding:1.25pt 2.3pt;">EN, FR</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.11.2" style="padding:1.25pt 2.3pt;">44.8K</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.11.3" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.6.11.4" style="padding:1.25pt 2.3pt;">44.8K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.12">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.12.1" style="padding:1.25pt 2.3pt;">MultiSense</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.12.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.12.3" style="padding:1.25pt 2.3pt;">✓</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.12.4" style="padding:1.25pt 2.3pt;">Daily Activity</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.12.5" style="padding:1.25pt 2.3pt;">S-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.12.6" style="padding:1.25pt 2.3pt;">DE, EN, ES</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.12.7" style="padding:1.25pt 2.3pt;">9.5K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.12.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.12.9" style="padding:1.25pt 2.3pt;">9.5K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.13">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.13.1" style="padding:1.25pt 2.3pt;">AmbigCaps</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.13.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.13.3" style="padding:1.25pt 2.3pt;">✓</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.13.4" style="padding:1.25pt 2.3pt;">Daily Activity</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.13.5" style="padding:1.25pt 2.3pt;">S-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.13.6" style="padding:1.25pt 2.3pt;">EN, TR</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.13.7" style="padding:1.25pt 2.3pt;">91.6K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.13.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.13.9" style="padding:1.25pt 2.3pt;">91.6K</span></span>
<span class="ltx_tr" id="S3.T1.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.1.1.1.1.1" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.1"><math alttext="\mathrm{M}^{3}" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.1.1.m1.1a"><msup id="S3.T1.1.1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.1.1.1.m1.1.1.2" mathvariant="normal" xref="S3.T1.1.1.1.1.1.1.m1.1.1.2.cmml">M</mi><mn id="S3.T1.1.1.1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S3.T1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.2">M</ci><cn id="S3.T1.1.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.1.m1.1c">\mathrm{M}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.1.1.m1.1d">roman_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math></span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.2.2" style="padding:1.25pt 2.3pt;"><math alttext="{\rm{M}^{3}}" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.2.2.m1.1a"><msup id="S3.T1.2.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.2.m1.1.1.cmml"><mi id="S3.T1.2.2.2.2.2.m1.1.1.2" mathvariant="normal" xref="S3.T1.2.2.2.2.2.m1.1.1.2.cmml">M</mi><mn id="S3.T1.2.2.2.2.2.m1.1.1.3" xref="S3.T1.2.2.2.2.2.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.2.2.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.2.2.m1.1.1">superscript</csymbol><ci id="S3.T1.2.2.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.2.2.m1.1.1.2">M</ci><cn id="S3.T1.2.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S3.T1.2.2.2.2.2.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.2.m1.1c">{\rm{M}^{3}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.2.m1.1d">roman_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>-Multi30K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.2.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.2.2.2.2.4" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.4.1">Daily Activity</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.2.2.2.2.5" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.5.1">S-MT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.2.2.2.2.6" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.6.1">CS, DE, EN, FR, HI, LV, TR</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.2.7" style="padding:1.25pt 2.3pt;">31.0K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.2.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.2.2.2.2.9" style="padding:1.25pt 2.3pt;">31.0K</span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.1" style="padding:1.25pt 2.3pt;"><math alttext="{\rm{M}^{3}}" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.1.m1.1"><semantics id="S3.T1.3.3.3.3.1.m1.1a"><msup id="S3.T1.3.3.3.3.1.m1.1.1" xref="S3.T1.3.3.3.3.1.m1.1.1.cmml"><mi id="S3.T1.3.3.3.3.1.m1.1.1.2" mathvariant="normal" xref="S3.T1.3.3.3.3.1.m1.1.1.2.cmml">M</mi><mn id="S3.T1.3.3.3.3.1.m1.1.1.3" xref="S3.T1.3.3.3.3.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.1.m1.1b"><apply id="S3.T1.3.3.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.3.3.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.3.3.1.m1.1.1">superscript</csymbol><ci id="S3.T1.3.3.3.3.1.m1.1.1.2.cmml" xref="S3.T1.3.3.3.3.1.m1.1.1.2">M</ci><cn id="S3.T1.3.3.3.3.1.m1.1.1.3.cmml" type="integer" xref="S3.T1.3.3.3.3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.1.m1.1c">{\rm{M}^{3}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.1.m1.1d">roman_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>-AmbigCaps</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.2" style="padding:1.25pt 2.3pt;">✓</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.3" style="padding:1.25pt 2.3pt;">91.6K</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.3.3.3.3.4" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.3.3.3.5" style="padding:1.25pt 2.3pt;">91.6K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.14">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.14.1" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.14.1.1">Fashion-MMT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.14.2" style="padding:1.25pt 2.3pt;">Fashion-MMT(L)</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.14.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.14.4" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.14.4.1">E-commerce</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.14.5" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.14.5.1">E-MT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.14.6" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.14.7" style="padding:1.25pt 2.3pt;">885.2K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.14.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.14.9" style="padding:1.25pt 2.3pt;">114.3K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.15">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.15.1" style="padding:1.25pt 2.3pt;">Fashion-MMT(C)</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.15.2" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.15.3" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.15.4" style="padding:1.25pt 2.3pt;">312.7K</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.15.5" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.6.15.6" style="padding:1.25pt 2.3pt;">40.0K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.16">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.16.1" style="padding:1.25pt 2.3pt;">EMMT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.16.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.16.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.16.4" style="padding:1.25pt 2.3pt;">E-commerce</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.16.5" style="padding:1.25pt 2.3pt;">E-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.16.6" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.16.7" style="padding:1.25pt 2.3pt;">22.0K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.16.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.16.9" style="padding:1.25pt 2.3pt;">875.0K</span></span>
<span class="ltx_tr" id="S3.T1.4.4.4.4">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T1.4.4.4.4.2" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.2.1">TIT Dataset</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T1.4.4.4.4.3" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.3.1">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T1.4.4.4.4.4" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.1">✗</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T1.4.4.4.4.5" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.5.1">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T1.4.4.4.4.6" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.6.1">TIMT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4.4.1" style="padding:1.25pt 2.3pt;">ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.T1.4.4.4.4.1.m1.1"><semantics id="S3.T1.4.4.4.4.1.m1.1a"><mo id="S3.T1.4.4.4.4.1.m1.1.1" stretchy="false" xref="S3.T1.4.4.4.4.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.1.m1.1b"><ci id="S3.T1.4.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.4.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.4.1.m1.1d">⇒</annotation></semantics></math>EN</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4.4.7" style="padding:1.25pt 2.3pt;">1.0M</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4.4.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.4.4.4.4.9" style="padding:1.25pt 2.3pt;">1.0M</span></span>
<span class="ltx_tr" id="S3.T1.5.5.5.5">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.5.5.5.1" style="padding:1.25pt 2.3pt;">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.T1.5.5.5.5.1.m1.1"><semantics id="S3.T1.5.5.5.5.1.m1.1a"><mo id="S3.T1.5.5.5.5.1.m1.1.1" stretchy="false" xref="S3.T1.5.5.5.5.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.1.m1.1b"><ci id="S3.T1.5.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.5.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.5.1.m1.1d">⇒</annotation></semantics></math>ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.5.5.5.2" style="padding:1.25pt 2.3pt;">1.0M</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.5.5.5.3" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.5.5.5.5.4" style="padding:1.25pt 2.3pt;">1.0M</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.6">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.6.1" style="padding:1.25pt 2.3pt;">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.T1.6.6.6.6.1.m1.1"><semantics id="S3.T1.6.6.6.6.1.m1.1a"><mo id="S3.T1.6.6.6.6.1.m1.1.1" stretchy="false" xref="S3.T1.6.6.6.6.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.1.m1.1b"><ci id="S3.T1.6.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.6.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.6.1.m1.1d">⇒</annotation></semantics></math>DE</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.6.2" style="padding:1.25pt 2.3pt;">1.0M</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.6.3" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.6.6.4" style="padding:1.25pt 2.3pt;">1.0M</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.17">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.17.1" style="padding:1.25pt 2.3pt;">BLATID</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.17.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.17.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.17.4" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.17.5" style="padding:1.25pt 2.3pt;">TIMT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.17.6" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.17.7" style="padding:1.25pt 2.3pt;">1.2M</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.17.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.17.9" style="padding:1.25pt 2.3pt;">1.2M</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.18">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.18.1" style="padding:1.25pt 2.3pt;">OCRMT30K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.18.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.18.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.18.4" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.18.5" style="padding:1.25pt 2.3pt;">TIMT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.18.6" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.18.7" style="padding:1.25pt 2.3pt;">30.2K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.18.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.18.9" style="padding:1.25pt 2.3pt;">164.7K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.19">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.19.1" style="padding:1.25pt 2.3pt;">How2</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.19.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.19.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.19.4" style="padding:1.25pt 2.3pt;">Social Media</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.19.5" style="padding:1.25pt 2.3pt;">V-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.19.6" style="padding:1.25pt 2.3pt;">EN, PT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.19.7" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.19.8" style="padding:1.25pt 2.3pt;">191.6K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.19.9" style="padding:1.25pt 2.3pt;">191.6K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.20">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.20.1" style="padding:1.25pt 2.3pt;">VaTeX</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.20.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.20.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.20.4" style="padding:1.25pt 2.3pt;">Social Media</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.20.5" style="padding:1.25pt 2.3pt;">V-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.20.6" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.20.7" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.20.8" style="padding:1.25pt 2.3pt;">41.3K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.20.9" style="padding:1.25pt 2.3pt;">412.7K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.21">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.21.1" style="padding:1.25pt 2.3pt;">BigVideo</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.21.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.21.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.21.4" style="padding:1.25pt 2.3pt;">Social Media</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.21.5" style="padding:1.25pt 2.3pt;">V-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.21.6" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.21.7" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.21.8" style="padding:1.25pt 2.3pt;">4.5M</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.21.9" style="padding:1.25pt 2.3pt;">4.5M</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.22">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.22.1" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.22.1.1">VISA</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.22.2" style="padding:1.25pt 2.3pt;">VISA-Polysemy</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.22.3" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.22.3.1">✓</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.22.4" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.22.4.1">Movie and TV</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.22.5" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.22.5.1">V-MT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.22.6" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.22.6.1">EN, JA</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.22.7" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.22.8" style="padding:1.25pt 2.3pt;">20.7K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.22.9" style="padding:1.25pt 2.3pt;">20.7K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.23">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.23.1" style="padding:1.25pt 2.3pt;">VISA-Omission</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.23.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.23.3" style="padding:1.25pt 2.3pt;">19.2K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.6.23.4" style="padding:1.25pt 2.3pt;">19.2K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.24">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.24.1" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.24.1.1">EVA</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.24.2" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.24.2.1">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.24.3" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.24.3.1">✓</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.24.4" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.24.4.1">Movie and TV</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.24.5" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.24.5.1">V-MT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.24.6" style="padding:1.25pt 2.3pt;">EN, JA</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.24.7" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.24.8" style="padding:1.25pt 2.3pt;">852.4K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.24.9" style="padding:1.25pt 2.3pt;">852.4K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.25">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.25.1" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.25.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.25.3" style="padding:1.25pt 2.3pt;">519.7K</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.6.25.4" style="padding:1.25pt 2.3pt;">519.7K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.26">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.26.1" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.26.1.1">MSCTD</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.26.2" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.26.2.1">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.26.3" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.26.3.1">✗</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.26.4" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.26.4.1">Dialogue</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T1.6.6.6.26.5" style="padding:1.25pt 2.3pt;"><span class="ltx_text" id="S3.T1.6.6.6.26.5.1">C-MT</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.26.6" style="padding:1.25pt 2.3pt;">EN, ZH</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.26.7" style="padding:1.25pt 2.3pt;">142.9K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.26.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.26.9" style="padding:1.25pt 2.3pt;">142.9K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.27">
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.27.1" style="padding:1.25pt 2.3pt;">DE, EN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.27.2" style="padding:1.25pt 2.3pt;">30.4K</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.6.27.3" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.6.27.4" style="padding:1.25pt 2.3pt;">30.4K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.28">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.6.6.6.28.1" style="padding:1.25pt 2.3pt;">BIG-C</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.28.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.28.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.28.4" style="padding:1.25pt 2.3pt;">Dialogue</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.28.5" style="padding:1.25pt 2.3pt;">C-MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.28.6" style="padding:1.25pt 2.3pt;">BE, EN</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.28.7" style="padding:1.25pt 2.3pt;">16.2K</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.28.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.6.6.6.28.9" style="padding:1.25pt 2.3pt;">92.1K</span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.29">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.6.6.6.29.1" style="padding:1.25pt 2.3pt;">HaVQA</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.6.6.6.29.2" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.6.6.6.29.3" style="padding:1.25pt 2.3pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.6.6.6.29.4" style="padding:1.25pt 2.3pt;">QA</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.6.6.6.29.5" style="padding:1.25pt 2.3pt;">Q-MT</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.6.6.6.29.6" style="padding:1.25pt 2.3pt;">EN, HA</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.6.6.6.29.7" style="padding:1.25pt 2.3pt;">1.6K</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.6.6.6.29.8" style="padding:1.25pt 2.3pt;">-</span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.6.6.6.29.9" style="padding:1.25pt 2.3pt;">12.0K</span></span>
</span></span></p>
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary statistics from commonly-used MMT datasets. Note that Ambiguous column refers to whether the dataset contains ambiguous words, and video refers to video clips. S-MT, Si-MT, V-MT, C-MT, Q-MT, E-MT refer to scene-image MMT, multi-modal SiMT, video-guided MT, multi-modal chat MT, multi-modal QA MT, e-commerce product-oriented MT, respectively. Twelve languages are covered in these datasets: English (EN), French (FR), German (DE), Spanish (ES), Czech (CS), Turkish (TR), Hindi (HI), Latvian (LV), Japanese (JA), Portuguese (PT), Chinese (ZH), Bemba (BE), Hausa (HA).</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Datasets</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S3.T1" title="Table 1 ‣ 3.5 Multi-modal Chat Machine Translation ‣ 3 Other Types of Multi-modal Machine Translation ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_tag">1</span></a> shows the information of the commonly used datasets used in MMT. All datasets are English-centric translation corpora. They cover a diverse range of domains, including daily activity, movie and TV, social media, e-commerce and QA. In the following, we will introduce these datasets in terms of data source, data quantity, data composition and so on.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">IAPR TC-12</em> (?). This dataset consists of 20,000 images from a private photographic image collection, which involves many categories such as sports, actions, photographs of people, animals, cities, landscapes and many other aspects of contemporary life. In this dataset, each image is annotated with a German description and its English translation. Besides, the dataset contains some additional annotations such as titles and locations in German, English and Spanish.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><em class="ltx_emph ltx_font_italic" id="S4.p3.1.1">Multi30K</em> (?). As the most frequently used dataset in the scene-image MMT task, Multi30K is extended from Flickr30K (?), where images are about human daily activities and come from online photo-sharing websites. It contains 31,014 images and each image is paired with an original English description and its German-French-Czech translations. Apart from the translations, it also contains non-parallel English and German descriptions for each image.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><em class="ltx_emph ltx_font_italic" id="S4.p4.1.1">MLT</em> (?). This dataset is generated from Multi30K, consisting of 53,868 samples for English to German and 44,779 samples for English to French. Each sample is a quadruple in the format of ⟨<em class="ltx_emph ltx_font_italic" id="S4.p4.1.2">source ambiguous word, target word, source sentence, image</em>⟩.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><em class="ltx_emph ltx_font_italic" id="S4.p5.1.1">How2</em> (?). Unlike other single-task datasets, How2 crawls videos along with various types of metadata from YouTube, obtaining 2,000 hours of videos in total. Among the crawled videos, 300 hours of them are paired with utterance-level English subtitles, aligned Portuguese translations and video-level English summaries, covering 22 topics. Therefore, this dataset can be widely used for various multi-modal tasks, including automatic speech recognition, speech-to-text translation and MMT.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><em class="ltx_emph ltx_font_italic" id="S4.p6.1.1">VaTeX</em> (?). This dataset is a large-scale English-Chinese video captioning dataset, where videos come from the widely-used action classification dataset, Kinetics-600 (?). It contains 41,269 video clips and 825,380 captions in total, and these video clips of the train and validation sets are labeled with 600 fine-grained action labels. Moreover, to increase the caption diversity, each video clip in VaTeX is annotated with 10 English and 10 Chinese descriptions, half of which are independent annotations and the other half are paired translations of each other.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1"><em class="ltx_emph ltx_font_italic" id="S4.p7.1.1">MultiSense</em> (?). This dataset contains 9,504 images annotated with ambiguous English verbs and their context-consistent translations in German and Spanish. Additionally, the authors annotate a subset of 995 ⟨<em class="ltx_emph ltx_font_italic" id="S4.p7.1.2">English description, German translation, image</em>⟩ triplets.</p>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1"><em class="ltx_emph ltx_font_italic" id="S4.p8.1.1">AmbigCaps</em> (?). It is a gender-ambiguous dataset containing 91,601 sentences automatically translated from English to Turkish via Google Translate and their associated images. Particularly, it filters out the sentences in Conceptual Captions (?) that contain nouns with gender information or professions referring to one gender.</p>
</div>
<div class="ltx_para" id="S4.p9">
<p class="ltx_p" id="S4.p9.1"><em class="ltx_emph ltx_font_italic" id="S4.p9.1.1">Fashion-MMT</em> (?). It is the first public bilingual product description dataset, which is based on the fashion captioning dataset FACAD (?). In this dataset, each description is aligned with an average of 6 to 7 product images of different colors and poses. Product categories and attribute labels are also provided for each product.
Most importantly, there are two types of translations provided, forming two sub-datasets. The first one is a noisy version, denoted as Fashion-MMT(L), which contains 114,257 automatic Chinese translations of original English product descriptions via Google Translate. The second one is a clean version, denoted as Fashion-MMT(C), containing 40,000 samples with manually annotated Chinese translations.</p>
</div>
<div class="ltx_para" id="S4.p10">
<p class="ltx_p" id="S4.p10.1"><em class="ltx_emph ltx_font_italic" id="S4.p10.1.1">MSCTD</em> (?). This dataset is proposed for multi-modal chat translation. To build MSCTD, researchers select the multi-modal dialogs from the OpenViDial dataset (?). In total, it contains 17,841 bilingual conversations, where each original English utterance is paired with a Chinese/German translation, an image depicting the current conversational scene and a sentiment label.</p>
</div>
<div class="ltx_para" id="S4.p11">
<p class="ltx_p" id="S4.p11.6"><em class="ltx_emph ltx_font_italic" id="S4.p11.1.1"><math alttext="{M^{3}}" class="ltx_Math" display="inline" id="S4.p11.1.1.m1.1"><semantics id="S4.p11.1.1.m1.1a"><msup id="S4.p11.1.1.m1.1.1" xref="S4.p11.1.1.m1.1.1.cmml"><mi id="S4.p11.1.1.m1.1.1.2" xref="S4.p11.1.1.m1.1.1.2.cmml">M</mi><mn id="S4.p11.1.1.m1.1.1.3" xref="S4.p11.1.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.p11.1.1.m1.1b"><apply id="S4.p11.1.1.m1.1.1.cmml" xref="S4.p11.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p11.1.1.m1.1.1.1.cmml" xref="S4.p11.1.1.m1.1.1">superscript</csymbol><ci id="S4.p11.1.1.m1.1.1.2.cmml" xref="S4.p11.1.1.m1.1.1.2">𝑀</ci><cn id="S4.p11.1.1.m1.1.1.3.cmml" type="integer" xref="S4.p11.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p11.1.1.m1.1c">{M^{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.p11.1.1.m1.1d">italic_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math></em> (?). There are two multi-lingual versions in <math alttext="{\rm M^{3}}" class="ltx_Math" display="inline" id="S4.p11.2.m1.1"><semantics id="S4.p11.2.m1.1a"><msup id="S4.p11.2.m1.1.1" xref="S4.p11.2.m1.1.1.cmml"><mi id="S4.p11.2.m1.1.1.2" mathvariant="normal" xref="S4.p11.2.m1.1.1.2.cmml">M</mi><mn id="S4.p11.2.m1.1.1.3" xref="S4.p11.2.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.p11.2.m1.1b"><apply id="S4.p11.2.m1.1.1.cmml" xref="S4.p11.2.m1.1.1"><csymbol cd="ambiguous" id="S4.p11.2.m1.1.1.1.cmml" xref="S4.p11.2.m1.1.1">superscript</csymbol><ci id="S4.p11.2.m1.1.1.2.cmml" xref="S4.p11.2.m1.1.1.2">M</ci><cn id="S4.p11.2.m1.1.1.3.cmml" type="integer" xref="S4.p11.2.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p11.2.m1.1c">{\rm M^{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.p11.2.m1.1d">roman_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>: <math alttext="{\rm M^{3}}" class="ltx_Math" display="inline" id="S4.p11.3.m2.1"><semantics id="S4.p11.3.m2.1a"><msup id="S4.p11.3.m2.1.1" xref="S4.p11.3.m2.1.1.cmml"><mi id="S4.p11.3.m2.1.1.2" mathvariant="normal" xref="S4.p11.3.m2.1.1.2.cmml">M</mi><mn id="S4.p11.3.m2.1.1.3" xref="S4.p11.3.m2.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.p11.3.m2.1b"><apply id="S4.p11.3.m2.1.1.cmml" xref="S4.p11.3.m2.1.1"><csymbol cd="ambiguous" id="S4.p11.3.m2.1.1.1.cmml" xref="S4.p11.3.m2.1.1">superscript</csymbol><ci id="S4.p11.3.m2.1.1.2.cmml" xref="S4.p11.3.m2.1.1.2">M</ci><cn id="S4.p11.3.m2.1.1.3.cmml" type="integer" xref="S4.p11.3.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p11.3.m2.1c">{\rm M^{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.p11.3.m2.1d">roman_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>-Multi30K and <math alttext="{\rm M^{3}}" class="ltx_Math" display="inline" id="S4.p11.4.m3.1"><semantics id="S4.p11.4.m3.1a"><msup id="S4.p11.4.m3.1.1" xref="S4.p11.4.m3.1.1.cmml"><mi id="S4.p11.4.m3.1.1.2" mathvariant="normal" xref="S4.p11.4.m3.1.1.2.cmml">M</mi><mn id="S4.p11.4.m3.1.1.3" xref="S4.p11.4.m3.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.p11.4.m3.1b"><apply id="S4.p11.4.m3.1.1.cmml" xref="S4.p11.4.m3.1.1"><csymbol cd="ambiguous" id="S4.p11.4.m3.1.1.1.cmml" xref="S4.p11.4.m3.1.1">superscript</csymbol><ci id="S4.p11.4.m3.1.1.2.cmml" xref="S4.p11.4.m3.1.1.2">M</ci><cn id="S4.p11.4.m3.1.1.3.cmml" type="integer" xref="S4.p11.4.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p11.4.m3.1c">{\rm M^{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.p11.4.m3.1d">roman_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>-AmbigCaps. <math alttext="{\rm M^{3}}" class="ltx_Math" display="inline" id="S4.p11.5.m4.1"><semantics id="S4.p11.5.m4.1a"><msup id="S4.p11.5.m4.1.1" xref="S4.p11.5.m4.1.1.cmml"><mi id="S4.p11.5.m4.1.1.2" mathvariant="normal" xref="S4.p11.5.m4.1.1.2.cmml">M</mi><mn id="S4.p11.5.m4.1.1.3" xref="S4.p11.5.m4.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.p11.5.m4.1b"><apply id="S4.p11.5.m4.1.1.cmml" xref="S4.p11.5.m4.1.1"><csymbol cd="ambiguous" id="S4.p11.5.m4.1.1.1.cmml" xref="S4.p11.5.m4.1.1">superscript</csymbol><ci id="S4.p11.5.m4.1.1.2.cmml" xref="S4.p11.5.m4.1.1.2">M</ci><cn id="S4.p11.5.m4.1.1.3.cmml" type="integer" xref="S4.p11.5.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p11.5.m4.1c">{\rm M^{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.p11.5.m4.1d">roman_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>-Multi30K is an extension of the existing Multi30K dataset with additional Turkish, Hindi, Latvian translations, and <math alttext="{\rm M^{3}}" class="ltx_Math" display="inline" id="S4.p11.6.m5.1"><semantics id="S4.p11.6.m5.1a"><msup id="S4.p11.6.m5.1.1" xref="S4.p11.6.m5.1.1.cmml"><mi id="S4.p11.6.m5.1.1.2" mathvariant="normal" xref="S4.p11.6.m5.1.1.2.cmml">M</mi><mn id="S4.p11.6.m5.1.1.3" xref="S4.p11.6.m5.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.p11.6.m5.1b"><apply id="S4.p11.6.m5.1.1.cmml" xref="S4.p11.6.m5.1.1"><csymbol cd="ambiguous" id="S4.p11.6.m5.1.1.1.cmml" xref="S4.p11.6.m5.1.1">superscript</csymbol><ci id="S4.p11.6.m5.1.1.2.cmml" xref="S4.p11.6.m5.1.1.2">M</ci><cn id="S4.p11.6.m5.1.1.3.cmml" type="integer" xref="S4.p11.6.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p11.6.m5.1c">{\rm M^{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.p11.6.m5.1d">roman_M start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>-AmbigCaps is an extension of the existing AmbigCaps dataset with additional French, Czech, Turkish, Hindi, Latvian translations.</p>
</div>
<div class="ltx_para" id="S4.p12">
<p class="ltx_p" id="S4.p12.1"><em class="ltx_emph ltx_font_italic" id="S4.p12.1.1">VISA</em> (?). This dataset consists of 39,880 Japanese-English bilingual subtitles and corresponding video clips from movies and TV episodes. Particularly, the Japanese subtitles are ambiguous, and the whole dataset is divided into Polysemy and Omission according to the causes of ambiguity.</p>
</div>
<div class="ltx_para" id="S4.p13">
<p class="ltx_p" id="S4.p13.3"><em class="ltx_emph ltx_font_italic" id="S4.p13.3.1">Text Image Translation (TIT) Dataset</em> (?). This dataset comprises a synthetic text image dataset for training and two real-world datasets for evaluation, including subtitle and street-view test sets. The synthetic text image dataset considers three language pairs: English<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S4.p13.1.m1.1"><semantics id="S4.p13.1.m1.1a"><mo id="S4.p13.1.m1.1.1" stretchy="false" xref="S4.p13.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S4.p13.1.m1.1b"><ci id="S4.p13.1.m1.1.1.cmml" xref="S4.p13.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p13.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.p13.1.m1.1d">⇒</annotation></semantics></math>Chinese, English<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S4.p13.2.m2.1"><semantics id="S4.p13.2.m2.1a"><mo id="S4.p13.2.m2.1.1" stretchy="false" xref="S4.p13.2.m2.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S4.p13.2.m2.1b"><ci id="S4.p13.2.m2.1.1.cmml" xref="S4.p13.2.m2.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p13.2.m2.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.p13.2.m2.1d">⇒</annotation></semantics></math>German, and Chinese<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S4.p13.3.m3.1"><semantics id="S4.p13.3.m3.1a"><mo id="S4.p13.3.m3.1.1" stretchy="false" xref="S4.p13.3.m3.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S4.p13.3.m3.1b"><ci id="S4.p13.3.m3.1.1.cmml" xref="S4.p13.3.m3.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p13.3.m3.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.p13.3.m3.1d">⇒</annotation></semantics></math>English, with each pair containing 1 million training samples and 2,000 validation samples and each sample consisting of a source image and a target text. Additionally, the subtitle test set contains 1,040 samples, while the street-view test set has 1,198 samples.</p>
</div>
<div class="ltx_para" id="S4.p14">
<p class="ltx_p" id="S4.p14.1"><em class="ltx_emph ltx_font_italic" id="S4.p14.1.1">BLATID</em> (?). BLATID is a Chinese-English bilingual annotation TIMT dataset generated from the existing MT corpus, AIC<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/AIChallenger/AI_Challenger_2018</span></span></span></span>. It contains 1 million training samples, 100K validation samples and 55K test samples. Especially, the test samples are derived from movies and their related bilingual subtitles. Each sample in this dataset is made up of a Chinese source image, a Chinese source text along with an English target text.</p>
</div>
<div class="ltx_para" id="S4.p15">
<p class="ltx_p" id="S4.p15.1"><em class="ltx_emph ltx_font_italic" id="S4.p15.1.1">OCRMT30K</em> (?). Previous studies on the TITM task mainly center around constructing synthetic TITM datasets, which are far from the real scenarios. To address this issue, OCRMT30K is annotated over five commonly used Chinese OCR datasets (?, ?, ?, ?, ?) where the images are freely captured in the streets. It totally includes 30,186 images and 164,674 Chinese-English parallel texts.</p>
</div>
<div class="ltx_para" id="S4.p16">
<p class="ltx_p" id="S4.p16.1"><em class="ltx_emph ltx_font_italic" id="S4.p16.1.1">EMMT</em> (?). This dataset is an English-Chinese e-commerce MMT dataset crawled from TikTok Shop and Shoppee. It incorporates three types of data, including 22K bilingual texts with images, 750K parallel texts and 103K monolingual captions. Particularly, 500 samples that contain ambiguous words and will be translated mistakenly without considering the image information are carefully selected as the test set.</p>
</div>
<div class="ltx_para" id="S4.p17">
<p class="ltx_p" id="S4.p17.1"><em class="ltx_emph ltx_font_italic" id="S4.p17.1.1">BigVideo</em> (?). Consisting of 4.5 million sentence pairs and 9,981 hours of videos, BigVideo is the largest English-Chinese video subtitle dataset to date, where videos are collected from two popular online video platforms, YouTube and Xigua. Each video clip in this dataset is paired with a source subtitle and its translation. Besides, two test sets are introduced to verify the necessity of visual information: AMBIGUOUS and UNAMBIGUOUS. The former contains 877 samples with the presence of ambiguous words, while the latter contains 1,517 samples where the textual context is sufficient for translation.</p>
</div>
<div class="ltx_para" id="S4.p18">
<p class="ltx_p" id="S4.p18.1"><em class="ltx_emph ltx_font_italic" id="S4.p18.1.1">EVA</em> (?). This dataset is the largest video subtitle translation dataset in the movie and TV domain, containing 852,440 Japanese-English parallel subtitle pairs, 519,673 Chinese-English parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In particular, the source subtitles in its evaluation set are ambiguous and the corresponding videos are guaranteed to be helpful for disambiguation.</p>
</div>
<div class="ltx_para" id="S4.p19">
<p class="ltx_p" id="S4.p19.1"><em class="ltx_emph ltx_font_italic" id="S4.p19.1.1">BIG-C</em> (?). This dataset is a multi-modal one in Bemba, which can be applied to many NLP tasks. It is made up of multi-turn dialogues between Bemba speakers, totally containing 16,229 images, 92,117 utterances and 185 hours of audio data. Each sample consists of an image, audio data grounded on the image, its dialogue’s corresponding audio transcriptions and English translations.</p>
</div>
<div class="ltx_para" id="S4.p20">
<p class="ltx_p" id="S4.p20.1"><em class="ltx_emph ltx_font_italic" id="S4.p20.1.1">HaVQA</em> (?). It is the first multi-modal dataset for various multi-modal tasks in Hausa, such as MMT, VQA and visual question elicitation. Totally, this dataset provides 1,555 images and 6,020 questions/answers in both English and Hausa.</p>
</div>
<div class="ltx_para" id="S4.p21">
<p class="ltx_p" id="S4.p21.1">In summary, compared with existing large-scale bilingual corpora, these datasets suffer from relatively smaller sizes, limited language pairs and domains. These factors seriously constrain the applications of existing MMT models. Therefore, we expect the emergence of large-quantity and high-quality datasets in the future, which will significantly prompt the development of MMT.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The commonly used automatic evaluation metrics for MMT include BLEU (?), METEOR (?, ?), TER (?), chrF (?), CIDEr (?), and COMET (?), all of which have been widely used in text-only MT. In the following, we will provide a detailed description of each metric.</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="S5.I1.i1.p1.1.1">BLEU</em>. As the most commonly used evaluation metric in MT, BLEU aims to measure the precision of n-gram matches between translation and reference at the word level. It possesses the advantages of efficient and convenient computation as well as considering n-gram information. However, it also has limitations such as disregarding the grammatical correctness of translations, exhibiting a bias towards shorter translations, and not effectively dealing with translations involving synonyms or conveying the same meaning.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="S5.I1.i2.p1.1.1">TER</em>. It is a distance-based evaluation metric, which assesses the quality of a translation by calculating its minimum number of edits to the reference.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="S5.I1.i3.p1.1.1">chrF</em>. Unlike BLEU, chrF measures the overlap of n-grams between translation and reference at the character level. Besides, it considers the morphological complexity of languages (e.g., different tenses).</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><em class="ltx_emph ltx_font_italic" id="S5.I1.i4.p1.1.1">METEOR</em>. To address some limitations of BLEU, METEOR comes up with a relaxed matching strategy to perform exact word, stem word, synonym and paraphrase matching, and considers precision more than recall.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1"><em class="ltx_emph ltx_font_italic" id="S5.I1.i5.p1.1.1">CIDEr</em>. This metric is specifically designed for image captioning, where each image is paired with multiple reference captions. It uses Term-Frequency Inverse Document Frequency (TF-IDF) (?) to weigh each n-gram in the translation and reference.
Unlike previous metrics, CIDEr distinguishes the importance of different n-grams through TF-IDF weights and focuses more on whether keywords in the reference are present in the translation.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1"><em class="ltx_emph ltx_font_italic" id="S5.I1.i6.p1.1.1">COMET</em>. In comparison to the above string-based metrics (BLEU, TER, chrF, METEOR), COMET is a neural network based metric that supports multiple languages, achieving better evaluation results than conventional metrics. Concretely, it utilizes XLM-RoBERTa (?) for model initialization and takes the source text, translation, and reference as inputs to compute a score, which accounts for the semantic similarity among these inputs.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Notice that the above-mentioned metrics perform evaluation by assessing the similarity between translation and reference, and thus the quality of references becomes a crucial factor impacting the reliability of evaluation results. We expect more evaluation metrics that are not limited by references and effectively integrate the unique characteristics of MMT in the future.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">In addition to academic papers published in conferences and journals, shared tasks play a significant role in promoting the development of MMT. One prominent event in this regard is the annual Workshop on Machine Translation (WMT)<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://machinetranslate.org/wmt</span></span></span></span>, which involves various tasks related to MT, such as MMT tasks<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.statmt.org/wmt16/multimodal-task.html</span></span></span></span>. From 2016 to 2018, WMT organizes three shared tasks of MMT and summarizes submissions from participants around the world (?, ?, ?). ? (?) explore the MMT task for the first time, where the Multi30K dataset is used and BLEU, METEOR, TER are employed as evaluation metrics. They conclude that the neural networks based MMT models do not perform as well as the text-only SMT models in any of the submissions. Following ? (?), ? (?) add additional French translations to Multi30K and construct two new evaluation sets: Test2017 and Ambiguous COCO (MSCOCO) which contains ambiguous verbs in the source language. Their work highlights some improvements in the performance of MMT models compared to last year, emphasizing that the incorporation of external resources can further enhance these models. Moreover, ? (?) extend Multi30K to include another new language, Czech. They also propose a novel evaluation metric called Lexical Translation Accuracy (LTA), which measures the accuracy of translating ambiguous words. In 2018, almost all submitted models achieved better results compared to the text-only SMT.
In conclusion, the MMT shared tasks have played a pivotal role in enriching the associated datasets, refining evaluation metrics, and fostering gradual improvements in the performance of the models proposed by diverse teams.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Prior studies show that image context is only needed in some specific scenarios, such as translating incorrect or ambiguous words or gender-neutral words that need to be marked for gender in the target language (?).
In order to investigate the impact of image context in different scenarios, some researchers create samples with limited textual context by designing various input degradation strategies to mask words with specific attributes in the source text, such as random words, ambiguous words, gender-neutral words, color words, entity words, words referring to people (e.g.,<em class="ltx_emph ltx_font_italic" id="S5.p4.1.1">“man”</em> and <em class="ltx_emph ltx_font_italic" id="S5.p4.1.2">“woman”</em>) (?, ?, ?, ?, ?, ?, ?, ?).
Other researchers also explore whether image context can help eliminate gender ambiguity by evaluating gender accuracy in the translations of gender-neutral languages (?).</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Comparison between Existing Models</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we summarize the experimental results of some representative MMT models. Early research predominantly employs RNN architectures, while more and more recent studies have shifted towards Transformer architectures. Since there are a lot of studies focusing on scene-image MMT and their experimental results indicate that the performance of Transformer-based models is usually better than that of RNN architectures, we only present experimental results of Transformer-based models in the scene-image MMT task. Notice that experimental configurations of different studies are not exactly the same, such as image encoders, evaluation datasets and metrics, so direct and quantitative comparison between these studies is challenging. We can only infer the relative advantages of each model and identify the factors that influence translation performance.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The experimental results of the scene-image MMT task are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S6.T2" title="Table 2 ‣ 6 Comparison between Existing Models ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_tag">2</span></a>, from which we can draw the following conclusions:</p>
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1">Early works tend to use ResNet (?) and Faster-RCNN (?) as image encoders to extract global and regional image features. However, in recent years, an increasing number of studies opt for more complicated image encoders, such as CLIP (?) or Vision Transformer (?, ?), to capture richer image semantic information.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1">Incorporating various performance-boosting techniques can result in high-quality translations. Notably, Transformer-based models that adopt contrastive learning (?, ?), double attention mechanism (?, ?), multi-task learning (?, ?) and pre-training (?, ?) techniques exhibit better performance.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1">In most real-world scenarios, images paired with source texts are not always available during inference. Therefore, more and more studies concentrate on image-free scenarios, leveraging imagination, image retrieval and unsupervised learning techniques to achieve high-quality translations.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">In summary, for the scene-image MMT task, the utilization of effective image encoders and diverse performance-boosting techniques can lead to high-performance models. Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S6.T3" title="Table 3 ‣ 6 Comparison between Existing Models ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12669v2#S6.T4" title="Table 4 ‣ 6 Comparison between Existing Models ‣ A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges"><span class="ltx_text ltx_ref_tag">4</span></a> show experimental results of the video-guided MT and the multi-modal SiMT tasks, respectively.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Since there are few studies on the e-commerce product-oriented MT and the multi-modal chat MT tasks, we do not present their experimental results here. Besides, due to data scarcity, the studies on the TIMT task usually conduct experiments on the self-constructed datasets, which prevents us from comparing these works. Therefore, we also omit the experimental results of the TIMT task. </span></span></span>
Generally, the models integrating visual information yield superior performance compared to the text-only models.</p>
</div>
<div class="ltx_table ltx_transformed_outer" id="S6.T2" style="width:705.8pt;height:520.1pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:520.1pt;transform:translate(92.85pt,94.72pt) rotate(-90deg) ;"><figure>
<p class="ltx_p ltx_align_center" id="S6.T2.2"><span class="ltx_text" id="S6.T2.2.2" style="font-size:70%;">[width=0.5]


<span class="ltx_tabular ltx_align_middle" id="S6.T2.2.2.2">
<span class="ltx_tr" id="S6.T2.2.2.2.2">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="S6.T2.2.2.2.2.3" style="padding-bottom:3.0pt;padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.2.3.1">
<span class="ltx_p" id="S6.T2.2.2.2.2.3.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.2.3.1.1.1">Model</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_3" id="S6.T2.2.2.2.2.4" style="padding-bottom:3.0pt;padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.2.4.1">Image Encoder</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_3" id="S6.T2.2.2.2.2.5" style="padding-bottom:3.0pt;padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.2.5.1">Technique</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_3" id="S6.T2.2.2.2.2.6" style="padding-bottom:3.0pt;padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.2.6.1">Image-free</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_3" id="S6.T2.1.1.1.1.1" style="padding-bottom:3.0pt;padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.1.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S6.T2.1.1.1.1.1.1.m1.1"><semantics id="S6.T2.1.1.1.1.1.1.m1.1a"><mo id="S6.T2.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S6.T2.1.1.1.1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S6.T2.1.1.1.1.1.1.m1.1b"><ci id="S6.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T2.1.1.1.1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.1.1.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T2.1.1.1.1.1.1.m1.1d">⇒</annotation></semantics></math>DE</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3" id="S6.T2.2.2.2.2.2" style="padding-bottom:3.0pt;padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.2.2.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S6.T2.2.2.2.2.2.1.m1.1"><semantics id="S6.T2.2.2.2.2.2.1.m1.1a"><mo id="S6.T2.2.2.2.2.2.1.m1.1.1" stretchy="false" xref="S6.T2.2.2.2.2.2.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S6.T2.2.2.2.2.2.1.m1.1b"><ci id="S6.T2.2.2.2.2.2.1.m1.1.1.cmml" xref="S6.T2.2.2.2.2.2.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.2.2.2.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T2.2.2.2.2.2.1.m1.1d">⇒</annotation></semantics></math>FR</span></span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.3">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.3.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.3.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.3.1.1.1" style="width:85.4pt;"><span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.3.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.3.2.1">Test16</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.3.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.3.3.1">Test17</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.2.2.2.3.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.3.4.1">MSCOCO</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.3.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.3.5.1">Test16</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.3.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.3.6.1">Test17</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.3.7" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.3.7.1">MSCOCO</span></span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.4">
<span class="ltx_td ltx_align_top ltx_border_r" id="S6.T2.2.2.2.4.1" style="padding-left:3.4pt;padding-right:3.4pt;"></span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.4.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.4.2.1">B/M</span></span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.4.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.4.3.1">B/M</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.4.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.4.4.1">B/M</span></span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.4.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.4.5.1">B/M</span></span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.4.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.4.6.1">B/M</span></span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.4.7" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S6.T2.2.2.2.4.7.1">B/M</span></span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.5">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S6.T2.2.2.2.5.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.5.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.5.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.2.2.2.5.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.2.2.2.5.3" style="padding-left:3.4pt;padding-right:3.4pt;">DA+MD</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.2.2.2.5.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.5.5" style="padding-left:3.4pt;padding-right:3.4pt;">38.0/55.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.5.6" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.2.2.2.5.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.5.8" style="padding-left:3.4pt;padding-right:3.4pt;">60.1/74.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.5.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.2.2.2.5.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.6">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.6.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.6.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.6.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.6.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.6.3" style="padding-left:3.4pt;padding-right:3.4pt;">ML+PT+UL</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.6.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.6.5" style="padding-left:3.4pt;padding-right:3.4pt;">23.5/26.1</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.6.6" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.6.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.6.8" style="padding-left:3.4pt;padding-right:3.4pt;">39.8/35.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.6.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.6.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.7">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.7.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.7.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.7.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.7.2" style="padding-left:3.4pt;padding-right:3.4pt;">Faster R-CNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.7.3" style="padding-left:3.4pt;padding-right:3.4pt;">GNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.7.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.7.5" style="padding-left:3.4pt;padding-right:3.4pt;">39.8/57.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.7.6" style="padding-left:3.4pt;padding-right:3.4pt;">32.2/51.9</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.7.7" style="padding-left:3.4pt;padding-right:3.4pt;">28.7/47.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.7.8" style="padding-left:3.4pt;padding-right:3.4pt;">60.9/74.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.7.9" style="padding-left:3.4pt;padding-right:3.4pt;">53.9/69.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.7.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.8">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.8.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.8.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.8.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.8.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.8.3" style="padding-left:3.4pt;padding-right:3.4pt;">BT+CFM</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.8.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.8.5" style="padding-left:3.4pt;padding-right:3.4pt;">39.5/56.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.8.6" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.8.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.8.8" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.8.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.8.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.9">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.9.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.9.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.9.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.9.2" style="padding-left:3.4pt;padding-right:3.4pt;">Fast R-CNN+ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.9.3" style="padding-left:3.4pt;padding-right:3.4pt;">CN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.9.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.9.5" style="padding-left:3.4pt;padding-right:3.4pt;">39.7/56.8</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.9.6" style="padding-left:3.4pt;padding-right:3.4pt;">31.0/49.9</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.9.7" style="padding-left:3.4pt;padding-right:3.4pt;">26.7/45.7</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.9.8" style="padding-left:3.4pt;padding-right:3.4pt;">61.2/76.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.9.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.3/70.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.9.10" style="padding-left:3.4pt;padding-right:3.4pt;">45.4/65.0</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.10">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.10.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.10.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.10.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.10.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.10.3" style="padding-left:3.4pt;padding-right:3.4pt;">GF+IR</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.10.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.10.5" style="padding-left:3.4pt;padding-right:3.4pt;">35.7/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.10.6" style="padding-left:3.4pt;padding-right:3.4pt;">26.9/-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.10.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.10.8" style="padding-left:3.4pt;padding-right:3.4pt;">58.3/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.10.9" style="padding-left:3.4pt;padding-right:3.4pt;">48.7/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.10.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.11">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.11.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.11.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.11.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.11.2" style="padding-left:3.4pt;padding-right:3.4pt;">Faster R-CNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.11.3" style="padding-left:3.4pt;padding-right:3.4pt;">ML</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.11.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.11.5" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.11.6" style="padding-left:3.4pt;padding-right:3.4pt;">29.5/50.3</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.11.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.11.8" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.11.9" style="padding-left:3.4pt;padding-right:3.4pt;">53.3/70.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.11.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.12">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.12.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.12.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.12.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.12.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.12.3" style="padding-left:3.4pt;padding-right:3.4pt;">ML</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.12.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.12.5" style="padding-left:3.4pt;padding-right:3.4pt;">40.5/59.1</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.12.6" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.12.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.12.8" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.12.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.12.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.13">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.13.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.13.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.13.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.13.2" style="padding-left:3.4pt;padding-right:3.4pt;">Fast R-CNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.13.3" style="padding-left:3.4pt;padding-right:3.4pt;">ML+PT+UL</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.13.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.13.5" style="padding-left:3.4pt;padding-right:3.4pt;">33.9/54.1</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.13.6" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.13.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.13.8" style="padding-left:3.4pt;padding-right:3.4pt;">52.3/67.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.13.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.13.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.14">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.14.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.14.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.14.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.14.2" style="padding-left:3.4pt;padding-right:3.4pt;">GAN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.14.3" style="padding-left:3.4pt;padding-right:3.4pt;">AT+IG</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.14.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.14.5" style="padding-left:3.4pt;padding-right:3.4pt;">38.6/55.7</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.14.6" style="padding-left:3.4pt;padding-right:3.4pt;">32.4/52.5</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.14.7" style="padding-left:3.4pt;padding-right:3.4pt;">28.8/48.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.14.8" style="padding-left:3.4pt;padding-right:3.4pt;">59.9/74.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.14.9" style="padding-left:3.4pt;padding-right:3.4pt;">52.8/68.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.14.10" style="padding-left:3.4pt;padding-right:3.4pt;">45.3/65.1</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.15">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.15.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.15.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.15.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.15.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.15.3" style="padding-left:3.4pt;padding-right:3.4pt;">ML+MU</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.15.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.15.5" style="padding-left:3.4pt;padding-right:3.4pt;">39.7/57.8</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.15.6" style="padding-left:3.4pt;padding-right:3.4pt;">32.9/52.1</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.15.7" style="padding-left:3.4pt;padding-right:3.4pt;">29.1/47.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.15.8" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.15.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.15.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.16">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.16.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.16.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.16.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.16.2" style="padding-left:3.4pt;padding-right:3.4pt;">Faster R-CNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.16.3" style="padding-left:3.4pt;padding-right:3.4pt;">PT</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.16.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.16.5" style="padding-left:3.4pt;padding-right:3.4pt;">44.0/61.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.16.6" style="padding-left:3.4pt;padding-right:3.4pt;">38.1/57.2</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.16.7" style="padding-left:3.4pt;padding-right:3.4pt;">35.2/53.8</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.16.8" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.16.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.16.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.17">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.17.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.17.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.17.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.17.2" style="padding-left:3.4pt;padding-right:3.4pt;">Faster R-CNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.17.3" style="padding-left:3.4pt;padding-right:3.4pt;">PT</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.17.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.17.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.7/60.7</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.17.6" style="padding-left:3.4pt;padding-right:3.4pt;">35.5/54.9</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.17.7" style="padding-left:3.4pt;padding-right:3.4pt;">32.8/52.2</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.17.8" style="padding-left:3.4pt;padding-right:3.4pt;">65.8/79.1</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.17.9" style="padding-left:3.4pt;padding-right:3.4pt;">58.2/73.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.17.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.18">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.18.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.18.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.18.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.18.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.18.3" style="padding-left:3.4pt;padding-right:3.4pt;">GF</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.18.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.18.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.0/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.18.6" style="padding-left:3.4pt;padding-right:3.4pt;">33.6/-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.18.7" style="padding-left:3.4pt;padding-right:3.4pt;">29.0/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.18.8" style="padding-left:3.4pt;padding-right:3.4pt;">61.7/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.18.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.9/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.18.10" style="padding-left:3.4pt;padding-right:3.4pt;">44.9/-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.19">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.19.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.19.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.19.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.19.2" style="padding-left:3.4pt;padding-right:3.4pt;">Fast R-CNN+ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.19.3" style="padding-left:3.4pt;padding-right:3.4pt;">DA</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.19.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.19.5" style="padding-left:3.4pt;padding-right:3.4pt;">38.6/57.7</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.19.6" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.19.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.19.8" style="padding-left:3.4pt;padding-right:3.4pt;">60.1/75.0</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.19.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.19.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.20">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.20.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.20.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.20.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.20.2" style="padding-left:3.4pt;padding-right:3.4pt;">Fast R-CNN+ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.20.3" style="padding-left:3.4pt;padding-right:3.4pt;">CFM+DA</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.20.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.20.5" style="padding-left:3.4pt;padding-right:3.4pt;">39.3/58.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.20.6" style="padding-left:3.4pt;padding-right:3.4pt;">32.3/52.8</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.20.7" style="padding-left:3.4pt;padding-right:3.4pt;">28.5/48.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.20.8" style="padding-left:3.4pt;padding-right:3.4pt;">61.8/76.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.20.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.1/70.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.20.10" style="padding-left:3.4pt;padding-right:3.4pt;">43.4/63.8</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.21">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.21.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.21.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.21.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.21.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.21.3" style="padding-left:3.4pt;padding-right:3.4pt;">CFM+GF</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.21.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.21.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.6/60.0</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.21.6" style="padding-left:3.4pt;padding-right:3.4pt;">35.1/54.5</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.21.7" style="padding-left:3.4pt;padding-right:3.4pt;">31.1/50.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.21.8" style="padding-left:3.4pt;padding-right:3.4pt;">63.2/77.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.21.9" style="padding-left:3.4pt;padding-right:3.4pt;">55.5/72.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.21.10" style="padding-left:3.4pt;padding-right:3.4pt;">46.3/67.4</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.22">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.22.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.22.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.22.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.2.2.2.22.2" style="padding-left:3.4pt;padding-right:3.4pt;">Fast R-CNN+ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.22.3" style="padding-left:3.4pt;padding-right:3.4pt;">GF+MU</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.22.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.22.5" style="padding-left:3.4pt;padding-right:3.4pt;">41.8/58.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.22.6" style="padding-left:3.4pt;padding-right:3.4pt;">33.1/51.9</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.22.7" style="padding-left:3.4pt;padding-right:3.4pt;">29.9/49.1</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.22.8" style="padding-left:3.4pt;padding-right:3.4pt;">62.2/76.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.22.9" style="padding-left:3.4pt;padding-right:3.4pt;">55.2/73.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.22.10" style="padding-left:3.4pt;padding-right:3.4pt;">44.4/66.4</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.23">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.23.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.23.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.23.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.23.2" style="padding-left:3.4pt;padding-right:3.4pt;">VQGAN-VAE</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.23.3" style="padding-left:3.4pt;padding-right:3.4pt;">IG</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.23.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.23.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.6/69.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.23.6" style="padding-left:3.4pt;padding-right:3.4pt;">35.1/62.8</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.23.7" style="padding-left:3.4pt;padding-right:3.4pt;">30.7/57.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.23.8" style="padding-left:3.4pt;padding-right:3.4pt;">63.1/81.8</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.23.9" style="padding-left:3.4pt;padding-right:3.4pt;">56.0/77.1</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.23.10" style="padding-left:3.4pt;padding-right:3.4pt;">46.4/71.3</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.24">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.24.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.24.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.24.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.24.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.24.3" style="padding-left:3.4pt;padding-right:3.4pt;">IG+KD</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.24.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.24.5" style="padding-left:3.4pt;padding-right:3.4pt;">41.3/58.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.24.6" style="padding-left:3.4pt;padding-right:3.4pt;">33.8/53.2</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.24.7" style="padding-left:3.4pt;padding-right:3.4pt;">30.2/48.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.24.8" style="padding-left:3.4pt;padding-right:3.4pt;">62.5/77.2</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.24.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.8/71.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.24.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.25">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.25.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.25.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.25.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.25.2" style="padding-left:3.4pt;padding-right:3.4pt;">Faster R-CNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.25.3" style="padding-left:3.4pt;padding-right:3.4pt;">GF+IR</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.25.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.25.5" style="padding-left:3.4pt;padding-right:3.4pt;">40.3/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.25.6" style="padding-left:3.4pt;padding-right:3.4pt;">33.5/-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.25.7" style="padding-left:3.4pt;padding-right:3.4pt;">30.3/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.25.8" style="padding-left:3.4pt;padding-right:3.4pt;">61.3/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.25.9" style="padding-left:3.4pt;padding-right:3.4pt;">53.2/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.25.10" style="padding-left:3.4pt;padding-right:3.4pt;">43.7/-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.26">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.26.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.26.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.26.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.26.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.26.3" style="padding-left:3.4pt;padding-right:3.4pt;">GF+ML</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.26.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.26.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.2/59.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.26.6" style="padding-left:3.4pt;padding-right:3.4pt;">34.1/53.1</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.26.7" style="padding-left:3.4pt;padding-right:3.4pt;">30.8/49.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.26.8" style="padding-left:3.4pt;padding-right:3.4pt;">63/77.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.26.9" style="padding-left:3.4pt;padding-right:3.4pt;">55.5/72.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.26.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.27">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.27.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.27.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.27.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.27.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.27.3" style="padding-left:3.4pt;padding-right:3.4pt;">CT</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.27.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.27.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.2/68.8</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.27.6" style="padding-left:3.4pt;padding-right:3.4pt;">34.3/62.1</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.27.7" style="padding-left:3.4pt;padding-right:3.4pt;">30.6/57.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.27.8" style="padding-left:3.4pt;padding-right:3.4pt;">62.8/81.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.27.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.7/76.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.27.10" style="padding-left:3.4pt;padding-right:3.4pt;">45.5/71.0</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.28">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.28.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.28.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.28.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.28.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.28.3" style="padding-left:3.4pt;padding-right:3.4pt;">CT+GF</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.28.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.28.5" style="padding-left:3.4pt;padding-right:3.4pt;">41.8/68.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.28.6" style="padding-left:3.4pt;padding-right:3.4pt;">34.6/62.4</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.28.7" style="padding-left:3.4pt;padding-right:3.4pt;">30.6/56.7</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.28.8" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.28.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.28.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.29">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.29.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.29.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.29.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.29.2" style="padding-left:3.4pt;padding-right:3.4pt;">Vision Transformer</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.29.3" style="padding-left:3.4pt;padding-right:3.4pt;">CFM+GF</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.29.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.29.5" style="padding-left:3.4pt;padding-right:3.4pt;">41.8/68.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.29.6" style="padding-left:3.4pt;padding-right:3.4pt;">34.3/62.3</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.29.7" style="padding-left:3.4pt;padding-right:3.4pt;">30.2/56.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.29.8" style="padding-left:3.4pt;padding-right:3.4pt;">62.2/81.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.29.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.5/76.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.29.10" style="padding-left:3.4pt;padding-right:3.4pt;">44.8/70.6</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.30">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.30.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.30.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.30.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.30.2" style="padding-left:3.4pt;padding-right:3.4pt;">Vision Transformer</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.30.3" style="padding-left:3.4pt;padding-right:3.4pt;">CFM+GF+SI</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.30.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.30.5" style="padding-left:3.4pt;padding-right:3.4pt;">41.2/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.30.6" style="padding-left:3.4pt;padding-right:3.4pt;">32.2/-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.30.7" style="padding-left:3.4pt;padding-right:3.4pt;">28.3/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.30.8" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.30.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.30.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.31">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.31.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.31.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.31.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.31.2" style="padding-left:3.4pt;padding-right:3.4pt;">MDETR+CLIP</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.31.3" style="padding-left:3.4pt;padding-right:3.4pt;">ML+PT</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.31.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.31.5" style="padding-left:3.4pt;padding-right:3.4pt;">43.3/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.31.6" style="padding-left:3.4pt;padding-right:3.4pt;">38.3/-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.31.7" style="padding-left:3.4pt;padding-right:3.4pt;">35.7/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.31.8" style="padding-left:3.4pt;padding-right:3.4pt;">67.2/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.31.9" style="padding-left:3.4pt;padding-right:3.4pt;">61.6/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.31.10" style="padding-left:3.4pt;padding-right:3.4pt;">51.1/-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.32">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.32.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.32.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.32.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.32.2" style="padding-left:3.4pt;padding-right:3.4pt;">X-VLM</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.32.3" style="padding-left:3.4pt;padding-right:3.4pt;">GF+ML</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.32.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.32.5" style="padding-left:3.4pt;padding-right:3.4pt;">41.0/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.32.6" style="padding-left:3.4pt;padding-right:3.4pt;">34.6/-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.32.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.32.8" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.32.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.32.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.33">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.33.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.33.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.33.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.33.2" style="padding-left:3.4pt;padding-right:3.4pt;">Faster R-CNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.33.3" style="padding-left:3.4pt;padding-right:3.4pt;">CT+GNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.33.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.33.5" style="padding-left:3.4pt;padding-right:3.4pt;">40.5/58.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.33.6" style="padding-left:3.4pt;padding-right:3.4pt;">33.2/52.3</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.33.7" style="padding-left:3.4pt;padding-right:3.4pt;">29.6/48.8</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.33.8" style="padding-left:3.4pt;padding-right:3.4pt;">61.6/76.0</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.33.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.4/70.7</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.33.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.34">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.34.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.34.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.34.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.34.2" style="padding-left:3.4pt;padding-right:3.4pt;">Faster R-CNN</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.34.3" style="padding-left:3.4pt;padding-right:3.4pt;">ML+UL</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.34.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.34.5" style="padding-left:3.4pt;padding-right:3.4pt;">37.4/57.2</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.34.6" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.34.7" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.34.8" style="padding-left:3.4pt;padding-right:3.4pt;">56.9/70.7</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.34.9" style="padding-left:3.4pt;padding-right:3.4pt;">-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.34.10" style="padding-left:3.4pt;padding-right:3.4pt;">-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.35">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.35.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.35.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.35.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.35.2" style="padding-left:3.4pt;padding-right:3.4pt;">M-CLIP</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.35.3" style="padding-left:3.4pt;padding-right:3.4pt;">PT</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.35.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.35.5" style="padding-left:3.4pt;padding-right:3.4pt;">43.9/70.2</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.35.6" style="padding-left:3.4pt;padding-right:3.4pt;">37.2/65.4</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.35.7" style="padding-left:3.4pt;padding-right:3.4pt;">34.5/61.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.35.8" style="padding-left:3.4pt;padding-right:3.4pt;">64.6/82.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.35.9" style="padding-left:3.4pt;padding-right:3.4pt;">57.6/77.8</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.35.10" style="padding-left:3.4pt;padding-right:3.4pt;">48.8/72.8</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.36">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.36.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.36.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.36.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.36.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.36.3" style="padding-left:3.4pt;padding-right:3.4pt;">CFM+GF+MU</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.36.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.36.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.0/59.4</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.36.6" style="padding-left:3.4pt;padding-right:3.4pt;">34.1/52.5</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.36.7" style="padding-left:3.4pt;padding-right:3.4pt;">30.4/49.6</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.36.8" style="padding-left:3.4pt;padding-right:3.4pt;">62.4/77.2</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.36.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.1/72.1</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.36.10" style="padding-left:3.4pt;padding-right:3.4pt;">46.5/66.7</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.37">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.37.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.37.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.37.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.37.2" style="padding-left:3.4pt;padding-right:3.4pt;">CLIP</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.37.3" style="padding-left:3.4pt;padding-right:3.4pt;">SI</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.37.4" style="padding-left:3.4pt;padding-right:3.4pt;">✓</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.37.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.5/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.37.6" style="padding-left:3.4pt;padding-right:3.4pt;">36.0/-</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.37.7" style="padding-left:3.4pt;padding-right:3.4pt;">32.0/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.37.8" style="padding-left:3.4pt;padding-right:3.4pt;">63.7/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.37.9" style="padding-left:3.4pt;padding-right:3.4pt;">56.2/-</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.37.10" style="padding-left:3.4pt;padding-right:3.4pt;">46.4/-</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.38">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.38.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.38.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.38.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.38.2" style="padding-left:3.4pt;padding-right:3.4pt;">MAE</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.38.3" style="padding-left:3.4pt;padding-right:3.4pt;">ML</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.38.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.38.5" style="padding-left:3.4pt;padding-right:3.4pt;">42.6/69.0</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.38.6" style="padding-left:3.4pt;padding-right:3.4pt;">34.6/62.0</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.38.7" style="padding-left:3.4pt;padding-right:3.4pt;">31.0/57.2</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.38.8" style="padding-left:3.4pt;padding-right:3.4pt;">62.2/81.8</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.38.9" style="padding-left:3.4pt;padding-right:3.4pt;">54.9/76.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.38.10" style="padding-left:3.4pt;padding-right:3.4pt;">45.8/71.2</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.39">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S6.T2.2.2.2.39.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.39.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.39.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.39.2" style="padding-left:3.4pt;padding-right:3.4pt;">ResNet</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.39.3" style="padding-left:3.4pt;padding-right:3.4pt;">CT</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.39.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.39.5" style="padding-left:3.4pt;padding-right:3.4pt;">44.3/69.9</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.39.6" style="padding-left:3.4pt;padding-right:3.4pt;">37.4/63.7</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.39.7" style="padding-left:3.4pt;padding-right:3.4pt;">34.3/60.1</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.39.8" style="padding-left:3.4pt;padding-right:3.4pt;">64.9/83.3</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.39.9" style="padding-left:3.4pt;padding-right:3.4pt;">57.5/78.5</span>
<span class="ltx_td ltx_align_center" id="S6.T2.2.2.2.39.10" style="padding-left:3.4pt;padding-right:3.4pt;">49.4/73.7</span></span>
<span class="ltx_tr" id="S6.T2.2.2.2.40">
<span class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" id="S6.T2.2.2.2.40.1" style="padding-left:3.4pt;padding-right:3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.2.2.2.40.1.1">
<span class="ltx_p" id="S6.T2.2.2.2.40.1.1.1" style="width:85.4pt;">(?)</span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T2.2.2.2.40.2" style="padding-left:3.4pt;padding-right:3.4pt;">CLIP</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T2.2.2.2.40.3" style="padding-left:3.4pt;padding-right:3.4pt;">CFM+MU+UL</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T2.2.2.2.40.4" style="padding-left:3.4pt;padding-right:3.4pt;">✗</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.2.2.2.40.5" style="padding-left:3.4pt;padding-right:3.4pt;">33.1/-</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.2.2.2.40.6" style="padding-left:3.4pt;padding-right:3.4pt;">29.1/-</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T2.2.2.2.40.7" style="padding-left:3.4pt;padding-right:3.4pt;">24.8/-</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.2.2.2.40.8" style="padding-left:3.4pt;padding-right:3.4pt;">54.1/-</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.2.2.2.40.9" style="padding-left:3.4pt;padding-right:3.4pt;">47.0/-</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.2.2.2.40.10" style="padding-left:3.4pt;padding-right:3.4pt;">44.4/-</span></span>
</span></span></p>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental results of Transformer-based scene-image MMT models. The meanings of symbols in the Techniques column are as follows: adversarial training (AT), back translation (BT), cross-modal filter mechanism (CFM), capsule network (CN), contrastive learning (CT), double attention (DA), gated fusion (GF), graph neural network (GNN), imagination (IG), knowledge distillation (KD), image retrieval (IR), multiple decoding (MD), multitask learning (ML), mix-up (MU), pre-training (PT), reinforcement learning (RL), synthetic images (SI) and unsupervised learning (UL). B/M refers to BLEU/METEOR.</figcaption>
</figure></div></div>
<figure class="ltx_table" id="S6.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T3.2">
<tr class="ltx_tr" id="S6.T3.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T3.2.2.3"><span class="ltx_text ltx_font_bold" id="S6.T3.2.2.3.1" style="font-size:70%;">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.2.2.4"><span class="ltx_text ltx_font_bold" id="S6.T3.2.2.4.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.2.2.5"><span class="ltx_text ltx_font_bold" id="S6.T3.2.2.5.1" style="font-size:70%;">Video Encoder</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.2.2.6"><span class="ltx_text ltx_font_bold" id="S6.T3.2.2.6.1" style="font-size:70%;">Technique</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1" style="font-size:70%;">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S6.T3.1.1.1.1.m1.1"><semantics id="S6.T3.1.1.1.1.m1.1a"><mo id="S6.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S6.T3.1.1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.1.m1.1b"><ci id="S6.T3.1.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.1.1.1.1.m1.1d">⇒</annotation></semantics></math>ZH</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T3.2.2.2.1" style="font-size:70%;">ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S6.T3.2.2.2.1.m1.1"><semantics id="S6.T3.2.2.2.1.m1.1a"><mo id="S6.T3.2.2.2.1.m1.1.1" stretchy="false" xref="S6.T3.2.2.2.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.2.1.m1.1b"><ci id="S6.T3.2.2.2.1.m1.1.1.cmml" xref="S6.T3.2.2.2.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.2.2.1.m1.1d">⇒</annotation></semantics></math>EN</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.2.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.2.3.1" style="padding-bottom:2.0pt;">
<span class="ltx_rule" style="width:0.0pt;height:0.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T3.2.3.1.1" style="font-size:70%;">
(?)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.3.2" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.3.2.1" style="font-size:70%;">VaTex</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.3.3" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.3.3.1" style="font-size:70%;">3D ConvNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.3.4" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.3.4.1" style="font-size:70%;">DA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.3.5" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.3.5.1" style="font-size:70%;">29.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.2.3.6" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.3.6.1" style="font-size:70%;">26.4</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.2.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.2.4.1" style="padding-bottom:2.0pt;">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T3.2.4.1.1" style="font-size:70%;">
(?)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.4.2" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.4.2.1" style="font-size:70%;">VaTex</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.4.3" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.4.3.1" style="font-size:70%;">ResNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.4.4" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.4.4.1" style="font-size:70%;">DA+HAN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.4.5" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.4.5.1" style="font-size:70%;">35.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.2.4.6" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.4.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.2.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.2.5.1" style="padding-bottom:2.0pt;">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T3.2.5.1.1" style="font-size:70%;">
(?)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.5.2" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.5.2.1" style="font-size:70%;">VaTex</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.5.3" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.5.3.1" style="font-size:70%;">3D ConvNet+Faster R-CNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.5.4" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.5.4.1" style="font-size:70%;">DA+HAN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.5.5" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.5.5.1" style="font-size:70%;">35.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.2.5.6" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.5.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.2.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.2.6.1" rowspan="2">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T3.2.6.1.1" style="font-size:70%;">
</span><span class="ltx_text" id="S6.T3.2.6.1.2" style="font-size:70%;">(?)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.6.2"><span class="ltx_text" id="S6.T3.2.6.2.1" style="font-size:70%;">VaTex</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.6.3"><span class="ltx_text" id="S6.T3.2.6.3.1" style="font-size:70%;">SlowFast</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.6.4" rowspan="2"><span class="ltx_text" id="S6.T3.2.6.4.1" style="font-size:70%;">CT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.6.5"><span class="ltx_text" id="S6.T3.2.6.5.1" style="font-size:70%;">37.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.2.6.6"><span class="ltx_text" id="S6.T3.2.6.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.2.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.2.7.1" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.7.1.1" style="font-size:70%;">BigVideo</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.2.7.2" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.7.2.1" style="font-size:70%;">Vision Transformer</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.2.7.3" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.7.3.1" style="font-size:70%;">44.8</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.2.7.4" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.7.4.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.2.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.2.8.1" style="padding-bottom:2.0pt;">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T3.2.8.1.1" style="font-size:70%;">
(?)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.8.2" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.8.2.1" style="font-size:70%;">VaTex</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.8.3" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.8.3.1" style="font-size:70%;">ResNet+Faster-RCNN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.8.4" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.8.4.1" style="font-size:70%;">ML+PT+UL</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.8.5" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.8.5.1" style="font-size:70%;">27.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.2.8.6" style="padding-bottom:2.0pt;"><span class="ltx_text" id="S6.T3.2.8.6.1" style="font-size:70%;">24.3</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.2.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.2.9.1">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T3.2.9.1.1" style="font-size:70%;">
(?)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.2.9.2"><span class="ltx_text" id="S6.T3.2.9.2.1" style="font-size:70%;">EVA</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.2.9.3"><span class="ltx_text" id="S6.T3.2.9.3.1" style="font-size:70%;">CLIP4Clip</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.2.9.4"><span class="ltx_text" id="S6.T3.2.9.4.1" style="font-size:70%;">CFM+GF+ML</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.2.9.5"><span class="ltx_text" id="S6.T3.2.9.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T3.2.9.6"><span class="ltx_text" id="S6.T3.2.9.6.1" style="font-size:70%;">27.6</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>The BLEU scores of video-guided MT models. The HAN technique refers to the hierarchical attention network.</figcaption>
</figure>
<figure class="ltx_table" id="S6.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T4.2">
<tr class="ltx_tr" id="S6.T4.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt" id="S6.T4.2.2.3" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.3.1" style="font-size:70%;">Model</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S6.T4.2.2.4" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.4.1" style="font-size:70%;">Image Encoder</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S6.T4.2.2.5" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.5.1" style="font-size:70%;">Technique</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S6.T4.2.2.6" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.6.1" style="font-size:70%;">Decoding Strategy</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S6.T4.1.1.1" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1" style="font-size:70%;">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S6.T4.1.1.1.1.m1.1"><semantics id="S6.T4.1.1.1.1.m1.1a"><mo id="S6.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S6.T4.1.1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T4.1.1.1.1.m1.1d">⇒</annotation></semantics></math>DE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S6.T4.2.2.2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.2.2.2.1" style="font-size:70%;">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S6.T4.2.2.2.1.m1.1"><semantics id="S6.T4.2.2.2.1.m1.1a"><mo id="S6.T4.2.2.2.1.m1.1.1" stretchy="false" xref="S6.T4.2.2.2.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.1.m1.1b"><ci id="S6.T4.2.2.2.1.m1.1.1.cmml" xref="S6.T4.2.2.2.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T4.2.2.2.1.m1.1d">⇒</annotation></semantics></math>FR</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.2.3.1" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;">
<span class="ltx_rule" style="width:0.0pt;height:0.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T4.2.3.1.1" style="font-size:70%;">
</span><span class="ltx_text" id="S6.T4.2.3.1.2" style="font-size:70%;">(?)</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.3.2" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.3.2.1" style="font-size:70%;">Resnet</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.3.3" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.3.3.1" style="font-size:70%;">HAN</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.3.4" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.3.4.1" style="font-size:70%;">consecutive</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.3.5" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.3.5.1" style="font-size:70%;">34.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T4.2.3.6" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.3.6.1" style="font-size:70%;">53.8</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S6.T4.2.4.1" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.4.1.1" style="font-size:70%;">wait-1/2/3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S6.T4.2.4.2" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.4.2.1" style="font-size:70%;">19.9/-/28.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S6.T4.2.4.3" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.4.3.1" style="font-size:70%;">32.5/-/44.0</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.2.5.1" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T4.2.5.1.1" style="font-size:70%;">
</span><span class="ltx_text" id="S6.T4.2.5.1.2" style="font-size:70%;">(?)</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.5.2" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.5.2.1" style="font-size:70%;">Faster R-CNN+Resnet</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.5.3" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.5.3.1" style="font-size:70%;">RL</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.5.4" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.5.4.1" style="font-size:70%;">consecutive</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.5.5" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.5.5.1" style="font-size:70%;">35.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T4.2.5.6" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.5.6.1" style="font-size:70%;">58.1</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S6.T4.2.6.1" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.6.1.1" style="font-size:70%;">wait-1/2/3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S6.T4.2.6.2" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.6.2.1" style="font-size:70%;">21.3/28.1/32.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S6.T4.2.6.3" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.6.3.1" style="font-size:70%;">42.1/49.2/54.8</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.2.7.1" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T4.2.7.1.1" style="font-size:70%;">
</span><span class="ltx_text" id="S6.T4.2.7.1.2" style="font-size:70%;">(?)</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.7.2" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.7.2.1" style="font-size:70%;">Faster R-CNN+Resnet</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.7.3" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.7.3.1" style="font-size:70%;">ML+RL</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.7.4" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.7.4.1" style="font-size:70%;">consecutive</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.7.5" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.7.5.1" style="font-size:70%;">35.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T4.2.7.6" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.7.6.1" style="font-size:70%;">59.1</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S6.T4.2.8.1" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.8.1.1" style="font-size:70%;">wait-1/2/3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S6.T4.2.8.2" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.8.2.1" style="font-size:70%;">-/28.3/32.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S6.T4.2.8.3" style="padding-bottom:2.0pt;padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.8.3.1" style="font-size:70%;">-/48.1/54.0</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S6.T4.2.9.1" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;">
<span class="ltx_rule" style="width:0.0pt;height:9.0pt;background:black;display:inline-block;"></span><span class="ltx_text" id="S6.T4.2.9.1.1" style="font-size:70%;">
</span><span class="ltx_text" id="S6.T4.2.9.1.2" style="font-size:70%;">(?)</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T4.2.9.2" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.9.2.1" style="font-size:70%;">Faster R-CNN+Resnet</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T4.2.9.3" rowspan="2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.9.3.1" style="font-size:70%;">ML</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.9.4" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.9.4.1" style="font-size:70%;">consecutive</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.9.5" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.9.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S6.T4.2.9.6" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.9.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" id="S6.T4.2.10.1" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.10.1.1" style="font-size:70%;">wait-1/2/3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" id="S6.T4.2.10.2" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.10.2.1" style="font-size:70%;">21.4/28.0/31.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S6.T4.2.10.3" style="padding-left:1.3pt;padding-right:1.3pt;"><span class="ltx_text" id="S6.T4.2.10.3.1" style="font-size:70%;">42.9/51.5/56.2</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>The BLEU scores of multi-modal SiMT models in the Test2016 test set of Multi30K. When using the <em class="ltx_emph ltx_font_italic" id="S6.T4.11.1">wait-k</em> decoding, the decoder waits <em class="ltx_emph ltx_font_italic" id="S6.T4.12.2">k</em> words to be read before committing the next translation. While using the <em class="ltx_emph ltx_font_italic" id="S6.T4.13.3">consecutive</em> decoding, the decoder performs translation without waiting.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Future Directions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">To summarize, previous studies not only employ various techniques to improve translation quality, but also prompt diverse applications of MMT, which have greatly contributed to the development of this task. In our opinion, the future directions of MMT include the following aspects.</p>
</div>
<div class="ltx_para" id="S7.p2">
<ul class="ltx_itemize" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p" id="S7.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i1.p1.1.1">Visual Information Integration in LLMs.</span> Very recently, with the rapid development of LLMs (?, ?, ?), conventional text-only MT has evolved into LLM-based text-only MT. Likewise, utilizing LLM for MMT becomes one future direction in this task. However, existing multi-modal LLMs simply use a linear model (?) or Query Transformer (?) to project the visual representations into the semantic space of the textual modality, which may lead to significant loss of visual information. Thus how to enhance the alignments between textual and visual modalities is worth exploring. Besides, existing studies usually apply single-pass methods to extract visual information without any filtering, which will introduce visual noise into the LLMs. How to adaptively extract useful visual information greatly impacts the performance of multi-modal LLMs in the MMT task.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p" id="S7.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i2.p1.1.1">Tailored Task Evaluation.</span> Presently, most existing evaluation metrics for MMT rely on text-only MT metrics, such as BLEU and METEOR. These metrics only consider the similarity between translation and reference within the textual modality, disregarding the semantic information provided by the visual modalities. As a consequence, there is a pressing need for the development of automatic evaluation metrics that concentrate on the semantic matching between translations (especially the ambiguous words) and given images/videos. In addition, more and more studies are beginning to apply LLMs for task evaluation due to their strong semantic comprehension capabilities (?, ?, ?). Therefore, how to use LLMs for more comprehensive automatic evaluation of MMT is also one of the future research directions.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p" id="S7.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i3.p1.1.1">More Extensions.</span> The conventional MMT mainly focuses on translating a single sentence along with a single image. Along the line of the development of conventional text-only MT, MMT can also be extended to various multi-modal scenarios, such as multilingual MMT, translating blogs containing both textual content and accompanying images or videos posted on social media, educational materials with texts and illustrations. All of them have wide applications in daily life.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S7.I1.i4.p1">
<p class="ltx_p" id="S7.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i4.p1.1.1">High-quality Datasets.</span> Compared with the datasets used in text-only MT, the commonly used MMT datasets are scale-limited and cover few domains. Therefore, how to efficiently construct high-quality datasets at a larger scale and across multiple domains is also a hot spot for future research. Moreover, in the era of LLMs, instruction-tuning datasets play a pivotal role in fine-tuning LLMs, so it is of vital importance to construct high-quality instruction-tuning datasets for LLM-based MMT.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this paper, we have presented a comprehensive overview of studies on MMT. First, we describe the methods used in scene-image MMT and other types of MMT. Then we provide detailed information about the datasets used in this task, and introduce some commonly used evaluation metrics. Subsequently, we carefully present and analyze experimental results from representative studies on the unified datasets, observing which techniques perform relatively better. Finally, we discuss possible future research directions in this task. We hope that this survey will serve as a valuable resource for researchers interested in MMT.</p>
</div>
<div class="ltx_para" id="S8.p2">
<br class="ltx_break"/>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al.</span>
<span class="ltx_bibblock">
Barrault, L., Bougares, F., Specia, L., Lala, C., Elliott, D., and Frank, S. (2018).

</span>
<span class="ltx_bibblock">Findings of the third shared task on multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx1.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018</span>, pp. 304–323.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al.</span>
<span class="ltx_bibblock">
Caglayan, O., Barrault, L., and Bougares, F. (2016).

</span>
<span class="ltx_bibblock">Multimodal attention for neural machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx2.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx2.2.2">abs/1609.03976</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al.</span>
<span class="ltx_bibblock">
Caglayan, O., Ive, J., Haralampieva, V., Madhyastha, P., Barrault, L., and Specia, L. (2020).

</span>
<span class="ltx_bibblock">Simultaneous machine translation with visual context. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx3.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</span>, pp. 2350–2361.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al.</span>
<span class="ltx_bibblock">
Caglayan, O., Kuyu, M., Amac, M. S., Madhyastha, P., Erdem, E., Erdem, A., and Specia, L. (2021).

</span>
<span class="ltx_bibblock">Cross-lingual visual pre-training for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx4.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021</span>, pp. 1317–1324.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al.</span>
<span class="ltx_bibblock">
Caglayan, O., Madhyastha, P., Specia, L., and Barrault, L. (2019).

</span>
<span class="ltx_bibblock">Probing the need for visual context in multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx5.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</span>, pp. 4159–4170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calixto et al.</span>
<span class="ltx_bibblock">
Calixto, I., Elliott, D., and Frank, S. (2016).

</span>
<span class="ltx_bibblock">Dcu-uva multimodal MT system report. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx6.1.1">Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Germany</span>, pp. 634–638.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calixto and Liu</span>
<span class="ltx_bibblock">
Calixto, I.,  and Liu, Q. (2017).

</span>
<span class="ltx_bibblock">Incorporating global visual features into attention-based neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017</span>, pp. 992–1003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calixto et al.</span>
<span class="ltx_bibblock">
Calixto, I., Liu, Q., and Campbell, N. (2017).

</span>
<span class="ltx_bibblock">Doubly-attentive decoder for multi-modal neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers</span>, pp. 1913–1924.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calixto et al.</span>
<span class="ltx_bibblock">
Calixto, I., Rios, M., and Aziz, W. (2019).

</span>
<span class="ltx_bibblock">Latent variable model for multi-modal translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx9.1.1">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</span>, pp. 6392–6405.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calixto et al.</span>
<span class="ltx_bibblock">
Calixto, I., Stein, D., Matusov, E., Castilho, S., and Way, A. (2017a).

</span>
<span class="ltx_bibblock">Human evaluation of multi-modal neural machine translation: A case-study on e-commerce listing titles. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx10.1.1">Proceedings of the Sixth Workshop on Vision and Language, VL@EACL 2017, Valencia, Spain, April 4, 2017</span>, pp. 31–37.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calixto et al.</span>
<span class="ltx_bibblock">
Calixto, I., Stein, D., Matusov, E., Lohar, P., Castilho, S., and Way, A. (2017b).

</span>
<span class="ltx_bibblock">Using images to improve machine-translating e-commerce product listings. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx11.1.1">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 2: Short Papers</span>, pp. 637–643.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al.</span>
<span class="ltx_bibblock">
Chen, G., Hou, L., Chen, Y., Dai, W., Shang, L., Jiang, X., Liu, Q., Pan, J., and Wang, W. (2023).

</span>
<span class="ltx_bibblock">mclip: Multilingual CLIP via cross-lingual transfer. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx12.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 13028–13043.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al.</span>
<span class="ltx_bibblock">
Chen, S., Jin, Q., and Fu, J. (2019).

</span>
<span class="ltx_bibblock">From words to sentences: A progressive learning approach for zero-resource machine translation with visual pivots. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx13.1.1">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019</span>, pp. 4932–4938.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al.</span>
<span class="ltx_bibblock">
Chen, Y., Liu, Y., and Li, V. O. K. (2018).

</span>
<span class="ltx_bibblock">Zero-resource neural machine translation with multi-agent communication game. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx14.1.1">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018</span>, pp. 5086–5093.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al.</span>
<span class="ltx_bibblock">
Chen, Z., Yin, F., Yang, Q., and Liu, C. (2023).

</span>
<span class="ltx_bibblock">Cross-lingual text image recognition via multi-hierarchy cross-modal mimic. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx15.1.1">IEEE Trans. Multim.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx15.2.2">25</span>, 4830–4841.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al.</span>
<span class="ltx_bibblock">
Chen, Z., Yin, F., Zhang, X., Yang, Q., and Liu, C. (2020).

</span>
<span class="ltx_bibblock">Cross-lingual text image recognition via multi-task sequence to sequence learning. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx16.1.1">25th International Conference on Pattern Recognition, ICPR 2020, Virtual Event / Milan, Italy, January 10-15, 2021</span>, pp. 3122–3129.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al.</span>
<span class="ltx_bibblock">
Cheng, X., Zhu, Z., Li, Y., Li, H., and Zou, Y. (2023).

</span>
<span class="ltx_bibblock">DAS-CL: towards multimodal machine translation via dual-level asymmetric contrastive learning. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx17.1.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023</span>, pp. 337–347.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang and Lee</span>
<span class="ltx_bibblock">
Chiang, D. C.,  and Lee, H. (2023).

</span>
<span class="ltx_bibblock">Can large language models be an alternative to human evaluations?. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx18.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 15607–15631.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chng et al.</span>
<span class="ltx_bibblock">
Chng, C. K., Ding, E., Liu, J., Karatzas, D., Chan, C. S., Jin, L., Liu, Y., Sun, Y., Ng, C. C., Luo, C., Ni, Z., Fang, C., Zhang, S., and Han, J. (2019).

</span>
<span class="ltx_bibblock">ICDAR2019 robust reading challenge on arbitrary-shaped text - rrc-art. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx19.1.1">2019 International Conference on Document Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25, 2019</span>, pp. 1571–1576.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al.</span>
<span class="ltx_bibblock">
Cho, K., van Merrienboer, B., Gülçehre, Ç., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014).

</span>
<span class="ltx_bibblock">Learning phrase representations using RNN encoder-decoder for statistical machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL</span>, pp. 1724–1734.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau and Lample</span>
<span class="ltx_bibblock">
Conneau, A.,  and Lample, G. (2019).

</span>
<span class="ltx_bibblock">Cross-lingual language model pretraining. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx21.1.1">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</span>, pp. 7057–7067.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Delbrouck and Dupont</span>
<span class="ltx_bibblock">
Delbrouck, J.,  and Dupont, S. (2017a).

</span>
<span class="ltx_bibblock">Modulating and attending the source image during encoding improves multimodal translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx22.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx22.2.2">abs/1712.03449</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Delbrouck and Dupont</span>
<span class="ltx_bibblock">
Delbrouck, J.,  and Dupont, S. (2017b).

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for multimodal neural machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx23.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx23.2.2">abs/1703.08084</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denkowski and Lavie</span>
<span class="ltx_bibblock">
Denkowski, M. J.,  and Lavie, A. (2014).

</span>
<span class="ltx_bibblock">Meteor universal: Language specific translation evaluation for any target language. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx24.1.1">Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014, June 26-27, 2014, Baltimore, Maryland, USA</span>, pp. 376–380.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al.</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021).

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx25.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott</span>
<span class="ltx_bibblock">
Elliott, D. (2018).

</span>
<span class="ltx_bibblock">Adversarial evaluation of multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx26.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018</span>, pp. 2974–2978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al.</span>
<span class="ltx_bibblock">
Elliott, D., Frank, S., Barrault, L., Bougares, F., and Specia, L. (2017).

</span>
<span class="ltx_bibblock">Findings of the second shared task on multimodal machine translation and multilingual image description. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx27.1.1">Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017</span>, pp. 215–233.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al.</span>
<span class="ltx_bibblock">
Elliott, D., Frank, S., and Hasler, E. (2015).

</span>
<span class="ltx_bibblock">Multi-language image description with neural sequence models. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx28.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx28.2.2">abs/1510.04709</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al.</span>
<span class="ltx_bibblock">
Elliott, D., Frank, S., Sima’an, K., and Specia, L. (2016).

</span>
<span class="ltx_bibblock">Multi30k: Multilingual english-german image descriptions. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx29.1.1">Proceedings of the 5th Workshop on Vision and Language, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, VL@ACL 2016, August 12, Berlin, Germany</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott and Kádár</span>
<span class="ltx_bibblock">
Elliott, D.,  and Kádár, Á. (2017).

</span>
<span class="ltx_bibblock">Imagination improves multimodal translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx30.1.1">Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long Papers</span>, pp. 130–141.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang and Feng</span>
<span class="ltx_bibblock">
Fang, Q.,  and Feng, Y. (2022).

</span>
<span class="ltx_bibblock">Neural machine translation with phrase-level universal visual representations. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx31.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</span>, pp. 5687–5698.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al.</span>
<span class="ltx_bibblock">
Fei, H., Liu, Q., Zhang, M., Zhang, M., and Chua, T. (2023).

</span>
<span class="ltx_bibblock">Scene graph as pivoting: Inference-time image-free unsupervised multimodal machine translation with visual scene hallucination. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx32.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 5980–5994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frank et al.</span>
<span class="ltx_bibblock">
Frank, S., Elliott, D., and Specia, L. (2018).

</span>
<span class="ltx_bibblock">Assessing multilingual multimodal image description: Studies of native speaker preferences and translator choices. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx33.1.1">Nat. Lang. Eng.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx33.2.2">24</span>, 393–413.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al.</span>
<span class="ltx_bibblock">
Fu, C., Feng, X., Huang, Y., Huo, W., Wang, H., Qin, B., and Liu, T. (2023).

</span>
<span class="ltx_bibblock">Enabling unsupervised neural machine translation with word-level visual representations. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx34.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</span>, pp. 12608–12618.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Futeral et al.</span>
<span class="ltx_bibblock">
Futeral, M., Schmid, C., Laptev, I., Sagot, B., and Bawden, R. (2023).

</span>
<span class="ltx_bibblock">Tackling ambiguity with images: Improved multimodal machine translation and contrastive evaluation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx35.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 5394–5413.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gella et al.</span>
<span class="ltx_bibblock">
Gella, S., Elliott, D., and Keller, F. (2019).

</span>
<span class="ltx_bibblock">Cross-lingual visual verb sense disambiguation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx36.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</span>, pp. 1998–2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grubinger et al.</span>
<span class="ltx_bibblock">
Grubinger, M., Clough, P. D., Müller, H., and Deselaers, T. (2006).

</span>
<span class="ltx_bibblock">The iapr tc-12 benchmark: A new evaluation resource for visual information systems..

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al.</span>
<span class="ltx_bibblock">
Gu, W., Song, H., Chu, C., and Kurohashi, S. (2021).

</span>
<span class="ltx_bibblock">Video-guided machine translation with spatial hierarchical attention network. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx38.1.1">Proceedings of the ACL-IJCNLP 2021 Student Research Workshop, ACL 2021, Online, JUli 5-10, 2021</span>, pp. 87–92.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al.</span>
<span class="ltx_bibblock">
Guo, H., Liu, J., Huang, H., Yang, J., Li, Z., Zhang, D., and Cui, Z. (2022).

</span>
<span class="ltx_bibblock">LVP-M3: language-aware visual prompt for multilingual multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx39.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</span>, pp. 2862–2872.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al.</span>
<span class="ltx_bibblock">
Guo, J., Ye, J., Xiang, Y., and Yu, Z. (2023a).

</span>
<span class="ltx_bibblock">Layer-level progressive transformer with modality difference awareness for multi-modal neural machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx40.1.1">IEEE ACM Trans. Audio Speech Lang. Process.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx40.2.2">31</span>, 3015–3026.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al.</span>
<span class="ltx_bibblock">
Guo, W., Fang, Q., Yu, D., and Feng, Y. (2023b).

</span>
<span class="ltx_bibblock">Bridging the gap between synthetic and authentic images for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx41.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</span>, pp. 2863–2874.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al.</span>
<span class="ltx_bibblock">
Gupta, D., Kharbanda, S., Zhou, J., Li, W., Pfister, H., and Wei, D. (2023).

</span>
<span class="ltx_bibblock">Cliptrans: Transferring visual knowledge with pre-trained models for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx42.1.1">IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023</span>, pp. 2863–2874.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haralampieva et al.</span>
<span class="ltx_bibblock">
Haralampieva, V., Caglayan, O., and Specia, L. (2022).

</span>
<span class="ltx_bibblock">Supervised visual attention for simultaneous multimodal machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx43.1.1">J. Artif. Intell. Res.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx43.2.2">74</span>, 1059–1089.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al.</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., and Sun, J. (2016).

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx44.1.1">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</span>, pp. 770–778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al.</span>
<span class="ltx_bibblock">
He, W., Zhang, X., Yin, F., and Liu, C. (2018).

</span>
<span class="ltx_bibblock">Multi-oriented and multi-lingual scene text detection with direct regression. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx45.1.1">IEEE Trans. Image Process.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx45.2.2">27</span>, 5406–5419.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinami et al.</span>
<span class="ltx_bibblock">
Hinami, R., Ishiwatari, S., Yasuda, K., and Matsui, Y. (2021).

</span>
<span class="ltx_bibblock">Towards fully automated manga translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx46.1.1">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021</span>, pp. 12998–13008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirasawa et al.</span>
<span class="ltx_bibblock">
Hirasawa, T., Yang, Z., Komachi, M., and Okazaki, N. (2020).

</span>
<span class="ltx_bibblock">Keyframe segmentation and positional encoding for video-guided machine translation challenge 2020. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx47.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx47.2.2">abs/2006.12799</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al.</span>
<span class="ltx_bibblock">
Huang, J., Li, Y., Ping, W., and Huang, L. (2018).

</span>
<span class="ltx_bibblock">Large margin neural language model. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx48.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018</span>, pp. 1183–1191.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al.</span>
<span class="ltx_bibblock">
Huang, P., Hu, J., Chang, X., and Hauptmann, A. G. (2020).

</span>
<span class="ltx_bibblock">Unsupervised multimodal neural machine translation with pseudo visual pivoting. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx49.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</span>, pp. 8226–8237.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al.</span>
<span class="ltx_bibblock">
Huang, P., Liu, F., Shiang, S., Oh, J., and Dyer, C. (2016).

</span>
<span class="ltx_bibblock">Attention-based multimodal neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx50.1.1">Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Germany</span>, pp. 639–645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al.</span>
<span class="ltx_bibblock">
Huang, X., Zhang, J., and Zong, C. (2021).

</span>
<span class="ltx_bibblock">Entity-level cross-modal learning improves multi-modal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx51.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021</span>, pp. 1067–1080.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imankulova et al.</span>
<span class="ltx_bibblock">
Imankulova, A., Kaneko, M., Hirasawa, T., and Komachi, M. (2020).

</span>
<span class="ltx_bibblock">Towards multimodal simultaneous neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx52.1.1">Proceedings of the Fifth Conference on Machine Translation, WMT@EMNLP 2020, Online, November 19-20, 2020</span>, pp. 594–603.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ive et al.</span>
<span class="ltx_bibblock">
Ive, J., Li, A. M., Miao, Y., Caglayan, O., Madhyastha, P., and Specia, L. (2021).

</span>
<span class="ltx_bibblock">Exploiting multimodal reinforcement learning for simultaneous machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx53.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021</span>, pp. 3222–3233.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ive et al.</span>
<span class="ltx_bibblock">
Ive, J., Madhyastha, P., and Specia, L. (2019).

</span>
<span class="ltx_bibblock">Distilling translations with visual awareness. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx54.1.1">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</span>, pp. 6525–6538.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al.</span>
<span class="ltx_bibblock">
Jain, P., Firat, O., Ge, Q., and Liang, S. (2021).

</span>
<span class="ltx_bibblock">Image translation network. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx55.1.1">Image Translation Model</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al.</span>
<span class="ltx_bibblock">
Ji, B., Zhang, T., Zou, Y., Hu, B., and Shen, S. (2022).

</span>
<span class="ltx_bibblock">Increasing visual awareness in multimodal neural machine translation from an information theoretic perspective. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx56.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</span>, pp. 6755–6764.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamalloo et al.</span>
<span class="ltx_bibblock">
Kamalloo, E., Dziri, N., Clarke, C. L. A., and Rafiei, D. (2023).

</span>
<span class="ltx_bibblock">Evaluating open-domain question answering in the era of large language models. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx57.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 5591–5606.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al.</span>
<span class="ltx_bibblock">
Kang, L., Huang, L., Peng, N., Zhu, P., Sun, Z., Cheng, S., Wang, M., Huang, D., and Su, J. (2023).

</span>
<span class="ltx_bibblock">Bigvideo: A large-scale video subtitle translation dataset for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx58.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 8456–8473.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kay et al.</span>
<span class="ltx_bibblock">
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., and Zisserman, A. (2017).

</span>
<span class="ltx_bibblock">The kinetics human action video dataset. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx59.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx59.2.2">abs/1705.06950</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong and Fan</span>
<span class="ltx_bibblock">
Kong, Y.,  and Fan, K. (2021).

</span>
<span class="ltx_bibblock">Probing multi-modal machine translation with pre-trained language model. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx60.1.1">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021</span>, Vol. ACL/IJCNLP 2021, pp. 3689–3699.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lala and Specia</span>
<span class="ltx_bibblock">
Lala, C.,  and Specia, L. (2018).

</span>
<span class="ltx_bibblock">Multimodal lexical translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx61.1.1">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al.</span>
<span class="ltx_bibblock">
Lan, Z., Yu, J., Li, X., Zhang, W., Luan, J., Wang, B., Huang, D., and Su, J. (2023).

</span>
<span class="ltx_bibblock">Exploring better text image translation with multimodal codebook. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx62.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 3479–3491.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lavie and Agarwal</span>
<span class="ltx_bibblock">
Lavie, A.,  and Agarwal, A. (2007).

</span>
<span class="ltx_bibblock">METEOR: an automatic metric for MT evaluation with high levels of correlation with human judgments. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx63.1.1">Proceedings of the Second Workshop on Statistical Machine Translation, WMT@ACL 2007, Prague, Czech Republic, June 23, 2007</span>, pp. 228–231.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.</span>
<span class="ltx_bibblock">
Li, B., Lv, C., Zhou, Z., Zhou, T., Xiao, T., Ma, A., and Zhu, J. (2022).

</span>
<span class="ltx_bibblock">On vision features in multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx64.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</span>, pp. 6327–6337.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.</span>
<span class="ltx_bibblock">
Li, J., Ataman, D., and Sennrich, R. (2021).

</span>
<span class="ltx_bibblock">Vision matters when it should: Sanity checking multimodal machine translation models. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx65.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</span>, pp. 8556–8562.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.</span>
<span class="ltx_bibblock">
Li, J., Li, D., Savarese, S., and Hoi, S. C. H. (2023a).

</span>
<span class="ltx_bibblock">BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx66.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</span>, Vol. 202, pp. 19730–19742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.</span>
<span class="ltx_bibblock">
Li, M., Huang, P., Chang, X., Hu, J., Yang, Y., and Hauptmann, A. (2023b).

</span>
<span class="ltx_bibblock">Video pivoting unsupervised multi-modal machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx67.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx67.2.2">45</span>, 3918–3932.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.</span>
<span class="ltx_bibblock">
Li, Y., Panda, R., Kim, Y., Chen, C. R., Feris, R., Cox, D. D., and Vasconcelos, N. (2022).

</span>
<span class="ltx_bibblock">VALHALLA: visual hallucination for machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx68.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</span>, pp. 5206–5216.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.</span>
<span class="ltx_bibblock">
Li, Y., Shimizu, S., Chu, C., Kurohashi, S., and Li, W. (2023).

</span>
<span class="ltx_bibblock">Video-helpful multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx69.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</span>, pp. 4281–4299.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al.</span>
<span class="ltx_bibblock">
Li, Y., Shimizu, S., Gu, W., Chu, C., and Kurohashi, S. (2022).

</span>
<span class="ltx_bibblock">VISA: an ambiguous subtitles dataset for visual scene-aware machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx70.1.1">Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 2022, Marseille, France, 20-25 June 2022</span>, pp. 6735–6743.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al.</span>
<span class="ltx_bibblock">
Liang, Y., Meng, F., Xu, J., Chen, Y., and Zhou, J. (2022).

</span>
<span class="ltx_bibblock">MSCTD: A multimodal sentiment chat translation dataset. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx71.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</span>, pp. 2601–2613.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Libovický and Helcl</span>
<span class="ltx_bibblock">
Libovický, J.,  and Helcl, J. (2017).

</span>
<span class="ltx_bibblock">Attention strategies for multi-source sequence-to-sequence learning. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx72.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 2: Short Papers</span>, pp. 196–202.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al.</span>
<span class="ltx_bibblock">
Lin, H., Meng, F., Su, J., Yin, Y., Yang, Z., Ge, Y., Zhou, J., and Luo, J. (2020).

</span>
<span class="ltx_bibblock">Dynamic context-guided capsule network for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx73.1.1">MM ’20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020</span>, pp. 1320–1329.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al.</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Wu, Q., and Lee, Y. J. (2023).

</span>
<span class="ltx_bibblock">Visual instruction tuning. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx74.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx74.2.2">abs/2304.08485</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al.</span>
<span class="ltx_bibblock">
Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., and Zettlemoyer, L. (2020).

</span>
<span class="ltx_bibblock">Multilingual denoising pre-training for neural machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx75.1.1">Trans. Assoc. Comput. Linguistics</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx75.2.2">8</span>, 726–742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al.</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. (2021).

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted windows. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx76.1.1">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021</span>, pp. 9992–10002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Logeswaran and Lee</span>
<span class="ltx_bibblock">
Logeswaran, L.,  and Lee, H. (2018).

</span>
<span class="ltx_bibblock">An efficient framework for learning sentence representations. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx77.1.1">6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al.</span>
<span class="ltx_bibblock">
Long, Q., Wang, M., and Li, L. (2021).

</span>
<span class="ltx_bibblock">Generative imagination elevates machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx78.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</span>, pp. 5738–5748.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al.</span>
<span class="ltx_bibblock">
Ma, C., Han, X., Wu, L., Zhang, Y., Zhao, Y., Zhou, Y., and Zong, C. (2024).

</span>
<span class="ltx_bibblock">Modal contrastive learning based end-to-end text image machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx79.1.1">IEEE ACM Trans. Audio Speech Lang. Process.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx79.2.2">32</span>, 2153–2165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al.</span>
<span class="ltx_bibblock">
Ma, C., Zhang, Y., Tu, M., Han, X., Wu, L., Zhao, Y., and Zhou, Y. (2022).

</span>
<span class="ltx_bibblock">Improving end-to-end text image translation from the auxiliary text translation task. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx80.1.1">26th International Conference on Pattern Recognition, ICPR 2022, Montreal, QC, Canada, August 21-25, 2022</span>, pp. 1664–1670.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al.</span>
<span class="ltx_bibblock">
Ma, C., Zhang, Y., Tu, M., Zhao, Y., Zhou, Y., and Zong, C. (2023a).

</span>
<span class="ltx_bibblock">CCIM: cross-modal cross-lingual interactive image translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx81.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</span>, pp. 4959–4965.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al.</span>
<span class="ltx_bibblock">
Ma, C., Zhang, Y., Tu, M., Zhao, Y., Zhou, Y., and Zong, C. (2023b).

</span>
<span class="ltx_bibblock">E2TIMT: efficient and effective modal adapter for text image machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx82.1.1">Document Analysis and Recognition - ICDAR 2023 - 17th International Conference, San José, CA, USA, August 21-26, 2023, Proceedings, Part VI</span>, Vol. 14192, pp. 70–88.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al.</span>
<span class="ltx_bibblock">
Ma, C., Zhang, Y., Tu, M., Zhao, Y., Zhou, Y., and Zong, C. (2023c).

</span>
<span class="ltx_bibblock">Multi-teacher knowledge distillation for end-to-end text image machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx83.1.1">Document Analysis and Recognition - ICDAR 2023 - 17th International Conference, San José, CA, USA, August 21-26, 2023, Proceedings, Part I</span>, Vol. 14187, pp. 484–501.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madhyastha et al.</span>
<span class="ltx_bibblock">
Madhyastha, P. S., Wang, J., and Specia, L. (2017).

</span>
<span class="ltx_bibblock">Sheffield multimt: Using object posterior predictions for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx84.1.1">Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017</span>, pp. 470–476.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mansimov et al.</span>
<span class="ltx_bibblock">
Mansimov, E., Stern, M., Chen, M., Firat, O., Uszkoreit, J., and Jain, P. (2020).

</span>
<span class="ltx_bibblock">Towards end-to-end in-image neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx85.1.1">Proceedings of the First International Workshop on Natural Language Processing Beyond Text</span>, pp. 70–74, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mogadala et al.</span>
<span class="ltx_bibblock">
Mogadala, A., Kalimuthu, M., and Klakow, D. (2021).

</span>
<span class="ltx_bibblock">Trends in integration of vision and language research: A survey of tasks, datasets, and methods. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx86.1.1">J. Artif. Intell. Res.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx86.2.2">71</span>, 1183–1317.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakayama and Nishida</span>
<span class="ltx_bibblock">
Nakayama, H.,  and Nishida, N. (2017).

</span>
<span class="ltx_bibblock">Zero-resource machine translation by multimodal encoder-decoder network with multimedia pivot. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx87.1.1">Mach. Transl.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx87.2.2">31</span>, 49–64.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nayef et al.</span>
<span class="ltx_bibblock">
Nayef, N., Liu, C., Ogier, J., Patel, Y., Busta, M., Chowdhury, P. N., Karatzas, D., Khlif, W., Matas, J., Pal, U., and Burie, J. (2019).

</span>
<span class="ltx_bibblock">ICDAR2019 robust reading challenge on multi-lingual scene text detection and recognition - RRC-MLT-2019. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx88.1.1">2019 International Conference on Document Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25, 2019</span>, pp. 1582–1587.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishihara et al.</span>
<span class="ltx_bibblock">
Nishihara, T., Tamura, A., Ninomiya, T., Omote, Y., and Nakayama, H. (2020).

</span>
<span class="ltx_bibblock">Supervised visual attention for multimodal neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx89.1.1">Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020</span>, pp. 4304–4314.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI</span>
<span class="ltx_bibblock">
OpenAI (2023).

</span>
<span class="ltx_bibblock">GPT-4 technical report. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx90.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx90.2.2">abs/2303.08774</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al.</span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. (2022).

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx91.1.1">NeurIPS</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al.</span>
<span class="ltx_bibblock">
Pan, B., Cai, H., Huang, D., Lee, K., Gaidon, A., Adeli, E., and Niebles, J. C. (2020).

</span>
<span class="ltx_bibblock">Spatio-temporal graph for video captioning with knowledge distillation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx92.1.1">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020</span>, pp. 10867–10876.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al.</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., and Zhu, W. (2002).

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx93.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA</span>, pp. 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parida et al.</span>
<span class="ltx_bibblock">
Parida, S., Abdulmumin, I., Muhammad, S. H., Bose, A., Kohli, G. S., Ahmad, I. S., Kotwal, K., Sarkar, S. D., Bojar, O., and Kakudi, H. A. (2023).

</span>
<span class="ltx_bibblock">Havqa: A dataset for visual question answering and multimodal research in hausa language. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx94.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 10162–10183.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al.</span>
<span class="ltx_bibblock">
Peng, R., Zeng, Y., and Zhao, J. (2022a).

</span>
<span class="ltx_bibblock">Distill the image to nowhere: Inversion knowledge distillation for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx95.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</span>, pp. 2379–2390.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al.</span>
<span class="ltx_bibblock">
Peng, R., Zeng, Y., and Zhao, J. (2022b).

</span>
<span class="ltx_bibblock">Hybridvocab: Towards multi-modal machine translation via multi-aspect alignment. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx96.1.1">ICMR ’22: International Conference on Multimedia Retrieval, Newark, NJ, USA, June 27 - 30, 2022</span>, pp. 380–388.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popovic</span>
<span class="ltx_bibblock">
Popovic, M. (2015).

</span>
<span class="ltx_bibblock">chrf: character n-gram f-score for automatic MT evaluation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx97.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lisbon, Portugal</span>, pp. 392–395.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al.</span>
<span class="ltx_bibblock">
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021).

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx98.1.1">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</span>, Vol. 139, pp. 8748–8763.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raunak et al.</span>
<span class="ltx_bibblock">
Raunak, V., Choe, S. K., Lu, Q., Xu, Y., and Metze, F. (2019).

</span>
<span class="ltx_bibblock">On leveraging the visual modality for neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx99.1.1">Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, Tokyo, Japan, October 29 - November 1, 2019</span>, pp. 147–151.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al.</span>
<span class="ltx_bibblock">
Rei, R., Stewart, C., Farinha, A. C., and Lavie, A. (2020).

</span>
<span class="ltx_bibblock">COMET: A neural framework for MT evaluation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx100.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</span>, pp. 2685–2702.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al.</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R. B., and Sun, J. (2015).

</span>
<span class="ltx_bibblock">Faster R-CNN: towards real-time object detection with region proposal networks. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx101.1.1">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada</span>, pp. 91–99.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson</span>
<span class="ltx_bibblock">
Robertson, S. (2004).

</span>
<span class="ltx_bibblock">Understanding inverse document frequency: on theoretical arguments for IDF. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx102.1.1">J. Documentation</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx102.2.2">60</span>, 503–520.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al.</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022).

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx103.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</span>, pp. 10674–10685.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sabour et al.</span>
<span class="ltx_bibblock">
Sabour, S., Frosst, N., and Hinton, G. E. (2017).

</span>
<span class="ltx_bibblock">Dynamic routing between capsules. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx104.1.1">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</span>, pp. 3856–3866.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanabria et al.</span>
<span class="ltx_bibblock">
Sanabria, R., Caglayan, O., Palaskar, S., Elliott, D., Barrault, L., Specia, L., and Metze, F. (2018).

</span>
<span class="ltx_bibblock">How2: A large-scale dataset for multimodal language understanding. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx105.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx105.2.2">abs/1811.00347</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sato et al.</span>
<span class="ltx_bibblock">
Sato, J., de Medeiros Caseli, H., and Specia, L. (2023).

</span>
<span class="ltx_bibblock">Choosing what to mask: More informed masking for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx106.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 244–253.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al.</span>
<span class="ltx_bibblock">
Sennrich, R., Haddow, B., and Birch, A. (2016a).

</span>
<span class="ltx_bibblock">Improving neural machine translation models with monolingual data. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx107.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al.</span>
<span class="ltx_bibblock">
Sennrich, R., Haddow, B., and Birch, A. (2016b).

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx108.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al.</span>
<span class="ltx_bibblock">
Sharma, P., Ding, N., Goodman, S., and Soricut, R. (2018).

</span>
<span class="ltx_bibblock">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx109.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers</span>, pp. 2556–2565.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al.</span>
<span class="ltx_bibblock">
Shi, B., Yao, C., Liao, M., Yang, M., Xu, P., Cui, L., Belongie, S. J., Lu, S., and Bai, X. (2017).

</span>
<span class="ltx_bibblock">ICDAR2017 competition on reading chinese text in the wild (RCTW-17). 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx110.1.1">14th IAPR International Conference on Document Analysis and Recognition, ICDAR 2017, Kyoto, Japan, November 9-15, 2017</span>, pp. 1429–1434.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sikasote et al.</span>
<span class="ltx_bibblock">
Sikasote, C., Mukonde, E., Alam, M. M. I., and Anastasopoulos, A. (2023).

</span>
<span class="ltx_bibblock">BIG-C: a multimodal multi-purpose dataset for bemba. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx111.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 2062–2078.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snover et al.</span>
<span class="ltx_bibblock">
Snover, M. G., Dorr, B. J., Schwartz, R. M., Micciulla, L., and Makhoul, J. (2006).

</span>
<span class="ltx_bibblock">A study of translation edit rate with targeted human annotation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx112.1.1">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, AMTA 2006, Cambridge, Massachusetts, USA, August 8-12, 2006</span>, pp. 223–231.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohn et al.</span>
<span class="ltx_bibblock">
Sohn, K., Lee, H., and Yan, X. (2015).

</span>
<span class="ltx_bibblock">Learning structured output representation using deep conditional generative models. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx113.1.1">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada</span>, pp. 3483–3491.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al.</span>
<span class="ltx_bibblock">
Song, Y., Chen, S., Jin, Q., Luo, W., Xie, J., and Huang, F. (2021).

</span>
<span class="ltx_bibblock">Product-oriented machine translation with cross-modal cross-lingual pre-training. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx114.1.1">MM ’21: ACM Multimedia Conference, Virtual Event, China, October 20 - 24, 2021</span>, pp. 2843–2852.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Specia et al.</span>
<span class="ltx_bibblock">
Specia, L., Frank, S., Sima’an, K., and Elliott, D. (2016).

</span>
<span class="ltx_bibblock">A shared task on multimodal machine translation and crosslingual image description. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx115.1.1">Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Germany</span>, pp. 543–553.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al.</span>
<span class="ltx_bibblock">
Su, J., Chen, J., Jiang, H., Zhou, C., Lin, H., Ge, Y., Wu, Q., and Lai, Y. (2021a).

</span>
<span class="ltx_bibblock">Multi-modal neural machine translation with deep semantic interactions. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx116.1.1">Inf. Sci.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx116.2.2">554</span>, 47–60.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al.</span>
<span class="ltx_bibblock">
Su, T., Liu, S., and Zhou, S. (2021b).

</span>
<span class="ltx_bibblock">Rtnet: An end-to-end method for handwritten text image translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx117.1.1">16th International Conference on Document Analysis and Recognition, ICDAR 2021, Lausanne, Switzerland, September 5-10, 2021, Proceedings, Part II</span>, Vol. 12822, pp. 99–113.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al.</span>
<span class="ltx_bibblock">
Su, Y., Fan, K., Bach, N., Kuo, C. J., and Huang, F. (2019).

</span>
<span class="ltx_bibblock">Unsupervised multi-modal neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx118.1.1">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</span>, pp. 10482–10491.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sulubacak et al.</span>
<span class="ltx_bibblock">
Sulubacak, U., Caglayan, O., Grönroos, S., Rouhe, A., Elliott, D., Specia, L., and Tiedemann, J. (2020).

</span>
<span class="ltx_bibblock">Multimodal machine translation through visuals and speech. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx119.1.1">Mach. Transl.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx119.2.2">34</span>, 97–147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al.</span>
<span class="ltx_bibblock">
Sun, Y., Liu, J., Liu, W., Han, J., Ding, E., and Liu, J. (2019).

</span>
<span class="ltx_bibblock">Chinese street view text: Large-scale chinese text reading with partially supervised learning. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx120.1.1">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</span>, pp. 9085–9094.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Surís et al.</span>
<span class="ltx_bibblock">
Surís, D., Epstein, D., and Vondrick, C. (2022).

</span>
<span class="ltx_bibblock">Globetrotter: Connecting languages by connecting images. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx121.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</span>, pp. 16453–16463.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al.</span>
<span class="ltx_bibblock">
Tang, Z., Zhang, X., Long, Z., and Fu, X. (2022).

</span>
<span class="ltx_bibblock">Multimodal neural machine translation with search engine based image retrieval. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx122.1.1">Proceedings of the 9th Workshop on Asian Translation, WAT@COLING 2022, Gyeongju, Republic of Korea, October 17, 2022</span>, pp. 89–98.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al.</span>
<span class="ltx_bibblock">
Tian, Y., Li, X., Liu, Z., Guo, Y., and Wang, B. (2023).

</span>
<span class="ltx_bibblock">In-image neural machine translation with segmented pixel sequence-to-sequence model. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx123.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</span>, pp. 15046–15057.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al.</span>
<span class="ltx_bibblock">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. (2023).

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx124.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx124.2.2">abs/2302.13971</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedantam et al.</span>
<span class="ltx_bibblock">
Vedantam, R., Zitnick, C. L., and Parikh, D. (2015).

</span>
<span class="ltx_bibblock">Cider: Consensus-based image description evaluation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx125.1.1">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015</span>, pp. 4566–4575.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vinyals et al.</span>
<span class="ltx_bibblock">
Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015).

</span>
<span class="ltx_bibblock">Show and tell: A neural image caption generator. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx126.1.1">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015</span>, pp. 3156–3164.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wadhwa et al.</span>
<span class="ltx_bibblock">
Wadhwa, S., Amir, S., and Wallace, B. C. (2023).

</span>
<span class="ltx_bibblock">Revisiting relation extraction in the era of large language models. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx127.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 15566–15589.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Xiong</span>
<span class="ltx_bibblock">
Wang, D.,  and Xiong, D. (2021).

</span>
<span class="ltx_bibblock">Efficient object-level visual context modeling for multimodal machine translation: Masking irrelevant objects helps grounding. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx128.1.1">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021</span>, pp. 2720–2728.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al.</span>
<span class="ltx_bibblock">
Wang, S., Zhang, W., Guo, W., Yu, D., and Liu, P. (2022).

</span>
<span class="ltx_bibblock">Contrastive learning based visual representation enhancement for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx129.1.1">International Joint Conference on Neural Networks, IJCNN 2022, Padua, Italy, July 18-23, 2022</span>, pp. 1–8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al.</span>
<span class="ltx_bibblock">
Wang, S., Meng, Y., Li, X., Sun, X., Ouyang, R., and Li, J. (2021).

</span>
<span class="ltx_bibblock">Openvidial 2.0: A larger-scale, open-domain dialogue generation dataset with visual contexts. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx130.1.1">CoRR</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx130.2.2">abs/2109.12761</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al.</span>
<span class="ltx_bibblock">
Wang, X., Wu, J., Chen, J., Li, L., Wang, Y., and Wang, W. Y. (2019).

</span>
<span class="ltx_bibblock">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx131.1.1">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</span>, pp. 4580–4590.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al.</span>
<span class="ltx_bibblock">
Wu, Z., Kong, L., Bi, W., Li, X., and Kao, B. (2021).

</span>
<span class="ltx_bibblock">Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx132.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</span>, pp. 6153–6166.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al.</span>
<span class="ltx_bibblock">
Yang, P., Chen, B., Zhang, P., and Sun, X. (2020a).

</span>
<span class="ltx_bibblock">Visual agreement regularized training for multi-modal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx133.1.1">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</span>, pp. 9418–9425.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al.</span>
<span class="ltx_bibblock">
Yang, X., Zhang, H., Jin, D., Liu, Y., Wu, C., Tan, J., Xie, D., Wang, J., and Wang, X. (2020b).

</span>
<span class="ltx_bibblock">Fashion captioning: Towards generating accurate descriptions with semantic rewards. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx134.1.1">Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIII</span>, Vol. 12358, pp. 1–17.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al.</span>
<span class="ltx_bibblock">
Yang, Z., Fang, Q., and Feng, Y. (2022).

</span>
<span class="ltx_bibblock">Low-resource neural machine translation with cross-modal alignment. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx135.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</span>, pp. 10134–10146.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao and Wan</span>
<span class="ltx_bibblock">
Yao, S.,  and Wan, X. (2020).

</span>
<span class="ltx_bibblock">Multimodal transformer for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx136.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</span>, pp. 4346–4350.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye and Guo</span>
<span class="ltx_bibblock">
Ye, J.,  and Guo, J. (2022).

</span>
<span class="ltx_bibblock">Dual-level interactive multimodal-mixup encoder for multi-modal neural machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx137.1.1">Appl. Intell.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx137.2.2">52</span>, 14194–14203.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al.</span>
<span class="ltx_bibblock">
Ye, J., Guo, J., Xiang, Y., Tan, K., and Yu, Z. (2022).

</span>
<span class="ltx_bibblock">Noise-robust cross-modal interactive learning with text2image mask for multi-modal neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx138.1.1">Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022</span>, pp. 5098–5108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al.</span>
<span class="ltx_bibblock">
Yin, Y., Meng, F., Su, J., Zhou, C., Yang, Z., Zhou, J., and Luo, J. (2020).

</span>
<span class="ltx_bibblock">A novel graph-based multi-modal fusion encoder for neural machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx139.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</span>, pp. 3025–3035.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al.</span>
<span class="ltx_bibblock">
Yin, Y., Zeng, J., Su, J., Zhou, C., Meng, F., Zhou, J., Huang, D., and Luo, J. (2023).

</span>
<span class="ltx_bibblock">Multi-modal graph contrastive encoding for neural machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx140.1.1">Artificial Intelligence</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx140.2.2">323</span>, 103986.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et al.</span>
<span class="ltx_bibblock">
Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. (2014).

</span>
<span class="ltx_bibblock">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx141.1.1">Trans. Assoc. Comput. Linguistics</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx141.2.2">2</span>, 67–78.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuasa et al.</span>
<span class="ltx_bibblock">
Yuasa, R., Tamura, A., Kajiwara, T., Ninomiya, T., and Kato, T. (2023).

</span>
<span class="ltx_bibblock">Multimodal neural machine translation using synthetic images transformed by latent diffusion model. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx142.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 76–82.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al.</span>
<span class="ltx_bibblock">
Zhang, H., Xu, T., and Li, H. (2017).

</span>
<span class="ltx_bibblock">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx143.1.1">IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017</span>, pp. 5908–5916.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al.</span>
<span class="ltx_bibblock">
Zhang, Z., Chen, K., Wang, R., Utiyama, M., Sumita, E., Li, Z., and Zhao, H. (2020).

</span>
<span class="ltx_bibblock">Neural machine translation with universal visual representation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx144.1.1">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al.</span>
<span class="ltx_bibblock">
Zhao, Y., Komachi, M., Kajiwara, T., and Chu, C. (2022a).

</span>
<span class="ltx_bibblock">Region-attentive multimodal neural machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx145.1.1">Neurocomputing</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx145.2.2">476</span>, 1–13.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al.</span>
<span class="ltx_bibblock">
Zhao, Y., Komachi, M., Kajiwara, T., and Chu, C. (2022b).

</span>
<span class="ltx_bibblock">Word-region alignment-guided multimodal neural machine translation. 
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx146.1.1">IEEE ACM Trans. Audio Speech Lang. Process.</span>, <span class="ltx_text ltx_font_italic" id="bib.bibx146.2.2">30</span>, 244–259.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al.</span>
<span class="ltx_bibblock">
Zhou, M., Cheng, R., Lee, Y. J., and Yu, Z. (2018).

</span>
<span class="ltx_bibblock">A visual attention grounding neural model for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx147.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018</span>, pp. 3643–3653.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al.</span>
<span class="ltx_bibblock">
Zhu, Y., Sun, Z., Cheng, S., Huang, L., Wu, L., and Wang, M. (2023).

</span>
<span class="ltx_bibblock">Beyond triplet: Leveraging the most data for multimodal machine translation. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx148.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</span>, pp. 2679–2697.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuo et al.</span>
<span class="ltx_bibblock">
Zuo, Y., Li, B., Lv, C., Zheng, T., Xiao, T., and Zhu, J. (2023).

</span>
<span class="ltx_bibblock">Incorporating probing signals into multimodal machine translation via visual question-answering pairs. 
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx149.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</span>, pp. 14689–14701.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 24 14:50:03 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
