<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation</title>
<!--Generated on Wed Sep 18 07:05:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.07970v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S1" title="In Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S2" title="In Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>ICL Using Quality Estimation for MT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S2.SS1" title="In 2 ICL Using Quality Estimation for MT ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Unsupervised Retriever Ranking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S2.SS2" title="In 2 ICL Using Quality Estimation for MT ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Search Algorithm Coupled with QE</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3" title="In Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS1" title="In 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Search Algorithm</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS1.SSS0.Px1" title="In 3.1 Search Algorithm ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">Mode 1:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS1.SSS0.Px2" title="In 3.1 Search Algorithm ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">Mode 2:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS1.SSS0.Px3" title="In 3.1 Search Algorithm ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">Mode 3:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS2" title="In 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Quality Estimation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS2.SSS1" title="In 3.2 Quality Estimation ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>QE data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS3" title="In 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Multilingual Large Language Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS4" title="In 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Dataset and Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS5" title="In 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Number of ICEs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS6" title="In 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Compared Systems</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS6.SSS0.Px1" title="In 3.6 Compared Systems ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">Random:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS6.SSS0.Px2" title="In 3.6 Compared Systems ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">Task-level:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS6.SSS0.Px3" title="In 3.6 Compared Systems ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">BM25:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS6.SSS0.Px4" title="In 3.6 Compared Systems ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">Re-rank BM25 (R-BM25):</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS6.SSS0.Px5" title="In 3.6 Compared Systems ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">Fine-tuning mBART-50:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS7" title="In 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Computational Costs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S4" title="In Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S4.SS1" title="In 4 Experiments Results ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Time to Prediction (TTP)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S5" title="In Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S5.SS0.SSS0.Px1" title="In 5 Analysis ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">Output analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S5.SS0.SSS0.Px2" title="In 5 Analysis ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">ICE Number Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S5.SS0.SSS0.Px3" title="In 5 Analysis ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">CO<sub class="ltx_sub">2</sub> Emissions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S6" title="In Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S6.SS0.SSS0.Px1" title="In 6 Related Work ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">ICL for MT.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S6.SS0.SSS0.Px2" title="In 6 Related Work ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title">QE in MT Evaluation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S7" title="In Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S8" title="In Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgments</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">Guiding In-Context Learning of LLMs 
<br class="ltx_break"/>through Quality Estimation for Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_ERROR undefined" id="id1.1.id1">\name</span><span class="ltx_text ltx_font_bold" id="id2.2.id2">Javad Pourmostafa Roshan Sharami</span> <span class="ltx_ERROR undefined" id="id3.3.id3">\addr</span>j.pourmostafa@tilburguniversity.edu
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id4.4.id4">\name</span><span class="ltx_text ltx_font_bold" id="id5.5.id5">Dimitar Shterionov</span> <span class="ltx_ERROR undefined" id="id6.6.id6">\addr</span>d.shterionov@tilburguniversity.edu
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id7.7.id7">\name</span><span class="ltx_text ltx_font_bold" id="id8.8.id8">Pieter Spronck</span> <span class="ltx_ERROR undefined" id="id9.9.id9">\addr</span>p.spronck@tilburguniversity.edu
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id10.10.id10">\addr</span>Department of Cognitive Science and Artificial Intelligence, Tilburg University
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">The quality of output from large language models (LLMs), particularly in machine translation (MT), is closely tied to the quality of in-context examples (ICEs) provided along with the query, i.e., the text to translate. The effectiveness of these ICEs is influenced by various factors, such as the domain of the source text, the order in which the ICEs are presented, the number of these examples, and the prompt templates used. Naturally, selecting the most impactful ICEs depends on understanding how these affect the resulting translation quality, which ultimately relies on translation references or human judgment. This paper presents a novel methodology for in-context learning (ICL) that relies on a search algorithm guided by domain-specific quality estimation (QE). Leveraging the XGLM model, our methodology estimates the resulting translation quality without the need for translation references, selecting effective ICEs for MT to maximize translation quality. Our results demonstrate significant improvements over existing ICL methods and higher translation performance compared to fine-tuning a pre-trained language model (PLM), specifically mBART-50.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Pre-trained large language models (LLMs) quickly gained popularity (and continue to do so) due to their performance on a large set of natural language processing (NLP) tasks, including machine translation (MT) <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib44" title="">2023</a>; Xu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib40" title="">2024</a>)</cite>. However, the accuracy of their outputs is significantly influenced by the quality of the <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">in-context</em> examples (ICEs) provided to them <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib10" title="">2020</a>; Alves et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib3" title="">2023</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For simplicity, we sometimes refer to it as “example(s)” throughout this paper.</span></span></span>
If these examples do not align well with the specific task and source domain, the LLMs’ outputs can be inaccurate. Therefore, there is a critical need to develop (better) methods for selecting appropriate examples that match the task and source domain being translated. These methods collectively fall under the umbrella of in-context learning (ICL) <cite class="ltx_cite ltx_citemacro_citep">(Liu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib17" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditionally, creating ICEs for MT involves either random selection <cite class="ltx_cite ltx_citemacro_citep">(Sia and Duh,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib33" title="">2023</a>)</cite> or using a strategy such as maximizing an evaluation metric like BLEU, to choose examples that improve the metric <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite>. The former was initially used for its simplicity and ease of implementation. However, relying on randomness can lead to inconsistent results and pose significant computational costs <cite class="ltx_cite ltx_citemacro_citep">(Lu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib18" title="">2022</a>)</cite>. Recent state-of-the-art (SOTA) ICL approaches focus on retrieving training examples that are closely relevant to the context of source sentences of test sets using unsupervised retrievers, such as BM25 <cite class="ltx_cite ltx_citemacro_citep">(Robertson and Zaragoza,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib30" title="">2009</a>)</cite>.
Recent studies have also shown that a range of factors, such as order <cite class="ltx_cite ltx_citemacro_citep">(Lu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib18" title="">2022</a>)</cite>, template <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib10" title="">2020</a>)</cite>, domain, and number of ICEs, significantly impact the performance <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib27" title="">Raunak et al., 2023a, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Naturally, the most effective ICEs for a given source text are the ones that would impact the resulting translation quality, which would ultimately depend on translation references or human judgment. In MT, quality estimation (QE) has become a standard approach for evaluating an MT system’s output without relying on reference translations <cite class="ltx_cite ltx_citemacro_cite">Blain et al., (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib5" title="">2023</a>)</cite>.
Recently, <cite class="ltx_cite ltx_citemacro_citet">Lee, (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib15" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Ye and Li, (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib41" title="">2023</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Sharami et al., (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib32" title="">2023</a>)</cite> showed the effectiveness of domain-specific QE when it comes to domain-specific MT (in contrast to the ineffectiveness of generic QE). Building on this and to address the aforementioned challenges, our work proposes to leverage domain-specific QE to assist in the selection of ICEs, with the goal of determining the suboptimal number and combination of ICEs to maximize MT quality, all without reference translations.
As QE would assess the impact of different ICE combinations and sequences, we hypothesize that this integration has the potential to not only improve translation performance but also reduce processing time, as QE could result in smaller sets of ICEs, which would reduce the inference times <cite class="ltx_cite ltx_citemacro_citep">(Petrov et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib22" title="">2023</a>)</cite>. This is particularly crucial considering the limited number of ICEs that can be fed into LLMs <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite>. Therefore, our study aims to investigate the feasibility of selecting ICEs on a per-source basis. Specifically, we aim to answer the following research question (RQ): <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">How effective are domain-specific QE models in determining ICEs for translation tasks in an LLM?</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our proposed ICL methodology for MT combines an unsupervised retriever to select ICEs with QE to assess their impact on the translation quality, determining which ICE combination to include. Instead of feeding all selected examples, we only select examples whose QE points to maximizing the LLM translation quality.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our findings on German-English translations demonstrate that our proposed approach outperforms the current SOTA ICL methods for MT as well as a fine-tuned mBART-50 <cite class="ltx_cite ltx_citemacro_citep">(Tang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib36" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>ICL Using Quality Estimation for MT</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To utilize LLMs for effective MT, as noted in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S1" title="1 Introduction ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, what is needed is a set of examples to provide the context (and thus guide or steer the LLM toward a correct, context-specific translation) –– that is, a set of ICEs –– and what is further important is the number of ICEs and their combination.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The question of the order of examples is not specifically discussed in this paper but is left for future work.</span></span></span> Ultimately, what is required is that the ICEs provide context that is neither too specific nor too broad and can effectively boost the translation. Our goal with this work is to develop a methodology that optimizes both these aspects in order to deliver high-quality MT. Our methodology for identifying effective ICEs involves two key components: (1) an unsupervised retriever that locates examples closely related to the sentence to be translated and (2) a search algorithm that uses QE to select a combination of examples that leads to the improvement of translation quality, i.e., aiming to maximize the BLEU score.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Unsupervised Retriever Ranking</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.2">We employ the <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.2.1">BM25</em> ranking algorithm <cite class="ltx_cite ltx_citemacro_citep">(Trotman et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib38" title="">2014</a>)</cite> due to the effective utilization of unsupervised retriever methods demonstrated in previous research, such as <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite>. BM25 sorts training pairs (source text and their translations) based on their relevance to a given query, i.e. the sentence to be translated.
Subsequently, we select the top <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_K</annotation></semantics></math> sentence pairs ranked by the algorithm, where <math alttext="K" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_K</annotation></semantics></math> is a hyperparameter that controls the number of pairs to be fed into the search algorithm.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Search Algorithm Coupled with QE</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Our search algorithm comprises three main phases: <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">Selection, Translation</em>, and <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.2">Estimation</em>. During the Selection phase, the algorithm selects the highest-ranked training example from the initial ICEs provided by the unsupervised retriever ranking method (out of <math alttext="K" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_K</annotation></semantics></math> ICEs). This selected example is then concatenated with the previously selected ICEs. In the first iteration, no ICEs have been selected before. In the Translation phase, the selected ICE is
translated by the model. In the Estimation phase, the LLM output (translated text) and the original source text are inputted into the domain-specific QE model to estimate the quality of the translation. Our proposed methodology relies on sentence-level QE.
</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Next, the selected ICE, together with its estimated quality and the LLM translation output, are appended to an intermediate list. To track the highest quality obtained thus far, the algorithm sorts the list in descending order based on the estimated quality. To avoid duplication, the selected ICE is removed before the next iteration. This iterative process continues until the best-estimated translation quality no longer improves within the specified patience threshold. Alternatively, the process terminates once all <math alttext="K" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">italic_K</annotation></semantics></math> ICEs have been selected.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">This methodology allows for the systematic selection of ICEs that improve translation quality compared to previous ICL methodologies while efficiently managing the computational resources required for the search process. This efficiency is achieved by integrating early stopping conditions with predetermined patience. Notably, we do not explore permutations of initial ICEs, as doing so would require a large number of attempts, leading to high computational costs during the search process.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>A pseudocode outlining the search methodology can be found in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#alg1" title="Algorithm 1 ‣ Appendices ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> in the Appendix. The phases of translating a source text of a test set using our methodology are depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#Ax1.F2" title="Figure 2 ‣ Appendices ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.</span></span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments Setup</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We conducted four main experiments to test the effectiveness of our methodology. Three of these experiments compare our methodology to existing ICL ones in different settings, or <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">Modes</em>. The fourth experiment compares our methodology to a fine-tuned mBART-50, aiming to assess which method is preferred (with respect to obtaining better translations).
</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">It is important to note that we do not fine-tune the LLM. The process of building the QE model used in our experiments is detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS2" title="3.2 Quality Estimation ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Search Algorithm</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We conducted experiments using the search algorithm outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S2.SS2" title="2.2 Search Algorithm Coupled with QE ‣ 2 ICL Using Quality Estimation for MT ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">2.2</span></a> across three operational modes:</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Mode 1:</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">This mode uses QE with ICEs ordered by BM25 to assess the effectiveness of combining BM25 and QE in the proposed ICL methodology.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Mode 2:</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">This mode investigates the impact of ordering ICEs by n-gram overlap, particularly unigrams, alongside QE, on the proposed methodology. Given the success of ordering ICEs based on their n-gram overlap match with the source, as demonstrated in <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite>, we assess how this ordering, based on ICEs’ n-gram overlap with the source text, influences the translation quality. This involves reordering ICEs according to their n-gram overlap, which is calculated using the NLTK word tokenizer. Higher overlap matches prioritize ICEs in the list and feed them into LLMs earlier.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Mode 3:</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">Instead of relying on QE, in this mode, we compute the BLUE score on the existing <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS0.Px3.p1.1.1">test set</span>. This approach is not a realistic case, but it is the most favorable scenario, and we use it as the highest bound to compare with Mode 1.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p2.1">The search algorithm generates up to 16 candidates. In each mode, we conducted experiments using three early stopping patience values (3, 8, and 16), determining the maximum number of ICEs (<math alttext="K" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p2.1.m1.1"><semantics id="S3.SS1.SSS0.Px3.p2.1.m1.1a"><mi id="S3.SS1.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p2.1.m1.1b"><ci id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p2.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p2.1.m1.1d">italic_K</annotation></semantics></math>) generated. We included Patience 16, which implies no early stopping, to evaluate the model’s performance with the maximum ICEs. Additionally, the search process halts if the estimated label reaches or exceeds 100, preventing further evaluations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Quality Estimation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Following <cite class="ltx_cite ltx_citemacro_citep">(Ranasinghe et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib26" title="">2020</a>; Lee,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib15" title="">2020</a>; Sharami et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib32" title="">2023</a>)</cite>, we develop a domain-specific QE model. First, we trained a QE model using out-of-domain (OOD) data (as detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS2.SSS1" title="3.2.1 QE data ‣ 3.2 Quality Estimation ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) to ensure generalizability; and second, we fine-tuned the model using the training set described in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS4" title="3.4 Dataset and Evaluation Metrics ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3.4</span></a> to provide domain-specific QE model and address domain mismatch, which is critical <cite class="ltx_cite ltx_citemacro_citep">(Koehn and Knowles,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib12" title="">2017</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In our experiments, we used BLEU as the quality label because our study focused on translation performance rather than post-editing effort, which is typically evaluated using (H)TER <cite class="ltx_cite ltx_citemacro_citep">(Specia and Farzindar,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib34" title="">2010</a>)</cite>. We employed the “MonoTransQuest” architecture from the TransQuest framework <cite class="ltx_cite ltx_citemacro_citep">(Ranasinghe et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib26" title="">2020</a>)</cite>, known for its success in prior QE studies. However, instead of employing softmax computation, we directly utilized logits to estimate the quality labels. This strategy saves computation time, as softmax computation can be resource-intensive <cite class="ltx_cite ltx_citemacro_citep">(Ruder,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib31" title="">2016</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>QE data</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">We utilized the German-English “EuroPat” dataset, accessed through Opus <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib37" title="">2012</a>)</cite>, to develop our generic QE model. We chose this dataset because it provides ample data samples, ensuring broad coverage of vocabulary –– a critical aspect in developing generic models.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">However, as MT datasets like EuroPat typically consist of pairs of source and translated text, it was necessary to synthetically create post-editing text (since the QE data creation process requires a triplet input: source text, machine-translated text, and post-edited text). To accomplish this, we used a pre-trained multilingual MT model, namely mBART-50 that supported the language pair used in our experiment. This involves translating 1M randomly chosen source texts from EuroPat. Afterward, the resulting translations were considered as machine-translated text, with the corresponding reference translations acting as post-edited text.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">Using SacreBLEU, we calculated the BLEU score, comparing the translated text with its corresponding post-edited text. This approach, which has been demonstrated to be effective in QE <cite class="ltx_cite ltx_citemacro_citep">(Negri et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib21" title="">2018</a>; Lee,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib15" title="">2020</a>; Sharami et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib32" title="">2023</a>)</cite>, enabled us to use the source and (machine-) translated text as input and the BLEU score as the target value for the QE model.
For building domain-specific QE, we utilized the training set detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS4" title="3.4 Dataset and Evaluation Metrics ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3.4</span></a> and applied the aforementioned approach to synthetically generate BLEU scores for the entire dataset.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multilingual Large Language Model</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">For our experiments and hypothesis validation, we used XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib16" title="">2022</a>)</cite>. This choice stems from the outstanding performance of the model in the MT field. This also ensures a fair comparison of our proposed methodology with previous research, such as <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite>, which introduced SOTA approaches in ICL for MT.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We used the 7.5 billion-parameter XGLM implementation and tokenizer by Hugging Face<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/docs/transformers/model_doc/xglm" title="">https://huggingface.co/docs/transformers/model_doc/xglm</a></span></span></span>, consistent with previous research. We employed a template from <cite class="ltx_cite ltx_citemacro_cite">Lin et al., (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib16" title="">2022</a>)</cite> to maximize translation performance. <math alttext="&lt;/s&gt;" class="ltx_math_unparsed" display="inline" id="S3.SS3.p2.1.m1.2"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2b"><mo id="S3.SS3.p2.1.m1.1.1" rspace="0em">&lt;</mo><mo id="S3.SS3.p2.1.m1.2.2" lspace="0em">/</mo><mi id="S3.SS3.p2.1.m1.2.3">s</mi><mo id="S3.SS3.p2.1.m1.2.4">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">&lt;/s&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.2d">&lt; / italic_s &gt;</annotation></semantics></math> serves as the ICE separator in this template. “BLANK” denotes an empty string within the template.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{\begin{split}\text{\{source text}_{1}\}&amp;=\text{\{target text}_{1}\}&lt;/s&gt;\\
\text{\{source text}_{2}\}&amp;=\text{\{{target text}}_{2}\}&lt;/s&gt;\\
\ldots&amp;=\ldots&lt;/s&gt;\\
\text{\{source text}_{n}\}&amp;=\text{BLANK}\end{split}}" class="ltx_math_unparsed" display="block" id="S3.Ex1.m1.34"><semantics id="S3.Ex1.m1.34a"><mtable columnspacing="0pt" displaystyle="true" id="S3.Ex1.m1.34.34" rowspacing="0pt"><mtr id="S3.Ex1.m1.34.34a"><mtd class="ltx_align_right" columnalign="right" id="S3.Ex1.m1.34.34b"><mrow id="S3.Ex1.m1.3.3.3.3.3"><msub id="S3.Ex1.m1.3.3.3.3.3.4"><mtext id="S3.Ex1.m1.1.1.1.1.1.1">{source text</mtext><mn id="S3.Ex1.m1.2.2.2.2.2.2.1">1</mn></msub><mo id="S3.Ex1.m1.3.3.3.3.3.3" stretchy="false">}</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.34.34c"><mrow id="S3.Ex1.m1.11.11.11.11.8"><mo id="S3.Ex1.m1.4.4.4.4.1.1">=</mo><msub id="S3.Ex1.m1.11.11.11.11.8.9"><mtext id="S3.Ex1.m1.5.5.5.5.2.2">{target text</mtext><mn id="S3.Ex1.m1.6.6.6.6.3.3.1">1</mn></msub><mo id="S3.Ex1.m1.7.7.7.7.4.4" stretchy="false">}</mo><mo id="S3.Ex1.m1.8.8.8.8.5.5" rspace="0em">&lt;</mo><mo id="S3.Ex1.m1.9.9.9.9.6.6" lspace="0em">/</mo><mi id="S3.Ex1.m1.10.10.10.10.7.7">s</mi><mo id="S3.Ex1.m1.11.11.11.11.8.8">&gt;</mo></mrow></mtd></mtr><mtr id="S3.Ex1.m1.34.34d"><mtd class="ltx_align_right" columnalign="right" id="S3.Ex1.m1.34.34e"><mrow id="S3.Ex1.m1.14.14.14.3.3"><msub id="S3.Ex1.m1.14.14.14.3.3.4"><mtext id="S3.Ex1.m1.12.12.12.1.1.1">{source text</mtext><mn id="S3.Ex1.m1.13.13.13.2.2.2.1">2</mn></msub><mo id="S3.Ex1.m1.14.14.14.3.3.3" stretchy="false">}</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.34.34f"><mrow id="S3.Ex1.m1.22.22.22.11.8"><mo id="S3.Ex1.m1.15.15.15.4.1.1">=</mo><msub id="S3.Ex1.m1.22.22.22.11.8.9"><mtext id="S3.Ex1.m1.16.16.16.5.2.2">{target text</mtext><mn id="S3.Ex1.m1.17.17.17.6.3.3.1">2</mn></msub><mo id="S3.Ex1.m1.18.18.18.7.4.4" stretchy="false">}</mo><mo id="S3.Ex1.m1.19.19.19.8.5.5" rspace="0em">&lt;</mo><mo id="S3.Ex1.m1.20.20.20.9.6.6" lspace="0em">/</mo><mi id="S3.Ex1.m1.21.21.21.10.7.7">s</mi><mo id="S3.Ex1.m1.22.22.22.11.8.8">&gt;</mo></mrow></mtd></mtr><mtr id="S3.Ex1.m1.34.34g"><mtd class="ltx_align_right" columnalign="right" id="S3.Ex1.m1.34.34h"><mi id="S3.Ex1.m1.23.23.23.1.1.1" mathvariant="normal">…</mi></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.34.34i"><mrow id="S3.Ex1.m1.29.29.29.7.6"><mo id="S3.Ex1.m1.24.24.24.2.1.1">=</mo><mi id="S3.Ex1.m1.25.25.25.3.2.2" mathvariant="normal">…</mi><mo id="S3.Ex1.m1.26.26.26.4.3.3" rspace="0em">&lt;</mo><mo id="S3.Ex1.m1.27.27.27.5.4.4" lspace="0em">/</mo><mi id="S3.Ex1.m1.28.28.28.6.5.5">s</mi><mo id="S3.Ex1.m1.29.29.29.7.6.6">&gt;</mo></mrow></mtd></mtr><mtr id="S3.Ex1.m1.34.34j"><mtd class="ltx_align_right" columnalign="right" id="S3.Ex1.m1.34.34k"><mrow id="S3.Ex1.m1.32.32.32.3.3"><msub id="S3.Ex1.m1.32.32.32.3.3.4"><mtext id="S3.Ex1.m1.30.30.30.1.1.1">{source text</mtext><mi id="S3.Ex1.m1.31.31.31.2.2.2.1">n</mi></msub><mo id="S3.Ex1.m1.32.32.32.3.3.3" stretchy="false">}</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.34.34l"><mrow id="S3.Ex1.m1.34.34.34.5.2"><mi id="S3.Ex1.m1.34.34.34.5.2.3"></mi><mo id="S3.Ex1.m1.33.33.33.4.1.1">=</mo><mtext id="S3.Ex1.m1.34.34.34.5.2.2">BLANK</mtext></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S3.Ex1.m1.34b">{\begin{split}\text{\{source text}_{1}\}&amp;=\text{\{target text}_{1}\}&lt;/s&gt;\\
\text{\{source text}_{2}\}&amp;=\text{\{{target text}}_{2}\}&lt;/s&gt;\\
\ldots&amp;=\ldots&lt;/s&gt;\\
\text{\{source text}_{n}\}&amp;=\text{BLANK}\end{split}}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.34c">start_ROW start_CELL {source text start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT } end_CELL start_CELL = {target text start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT } &lt; / italic_s &gt; end_CELL end_ROW start_ROW start_CELL {source text start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT } end_CELL start_CELL = {target text start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT } &lt; / italic_s &gt; end_CELL end_ROW start_ROW start_CELL … end_CELL start_CELL = … &lt; / italic_s &gt; end_CELL end_ROW start_ROW start_CELL {source text start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } end_CELL start_CELL = BLANK end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Dataset and Evaluation Metrics</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We used a dataset comprising German-to-English translation pairs within the IT domain, sourced from <cite class="ltx_cite ltx_citemacro_citep">(Aharoni and Goldberg,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib2" title="">2020</a>)</cite>. This dataset was chosen due to the challenges that MT systems and LLMs face when translating out-of-domain contexts, particularly in specialized fields, as noted in previous studies <cite class="ltx_cite ltx_citemacro_citep">(Koehn and Knowles,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib12" title="">2017</a>; Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite>. The specialized and constrained nature of the IT domain provided an ideal setting for evaluating our methodology’s performance under these conditions.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The dataset utilized in this study consisted of approximately 222k training sentences, 2k development sentences, and 2k test sentences.
To assess the translation effectiveness of the models, we employed metrics such as BLEU from SacreBLEU <cite class="ltx_cite ltx_citemacro_citep">(Post,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib24" title="">2018</a>)</cite> and COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib29" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Number of ICEs</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.3">We use between 1 and 16 ICEs. These may originate either from a random approach or from an advanced (guided) selection. To keep these separated in our analysis, we designate two different counts – <math alttext="p" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_p</annotation></semantics></math> and <math alttext="q" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><mi id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><ci id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_q</annotation></semantics></math>. This choice and naming convention is grounded by previous research exploring the impact of varying ICE numbers. While our study explicitly caps the upper limit of <math alttext="q" class="ltx_Math" display="inline" id="S3.SS5.p1.3.m3.1"><semantics id="S3.SS5.p1.3.m3.1a"><mi id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><ci id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.3.m3.1d">italic_q</annotation></semantics></math> at 16, values spanning from 1 to 16 remain feasible options –– unlike the fixed value in the compared systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Compared Systems</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">We conducted a comparative analysis with methods from previous studies; <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.1.1">random</span> and <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.1.2">task-level sampling</span>, <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.1.3">BM25</span>, <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.1.4">R-BM25</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.1.5">fine-tuned mBART-50</span>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS6.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Random:</h5>
<div class="ltx_para" id="S3.SS6.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS6.SSS0.Px1.p1.1">We conducted three random trials, generating random numbers based on parameter <math alttext="p" class="ltx_Math" display="inline" id="S3.SS6.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS6.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS6.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS6.SSS0.Px1.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS6.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS6.SSS0.Px1.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS0.Px1.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS0.Px1.p1.1.m1.1d">italic_p</annotation></semantics></math>. These numbers, ranging from 1 to the size of the training set, selected corresponding translation pairs. To create the prompt<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>In the literature, the term “prompt” is frequently used interchangeably with “ICE”</span></span></span>, in addition to the training examples (i.e., ICEs), we need the source side intended for translation. We utilize the source from the development set, in contrast to the advanced methods in ICL, where the source text from the test set is typically employed. The reason for selecting the development set over the test set in this approach is that development sets are generally from the same distribution, domain, and context as the test set. This similarity increases the likelihood that the examples in the development set will better match the content and context of the test set, thereby enhancing the relevance and effectiveness of the prompts.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS6.SSS0.Px1.p2.1">The generated prompt is inputted into the LLM for translation. Then, the BLEU score of the development set is computed. The random number that produces the highest score among the trials is selected, and the training examples linked to this number are concatenated with the test set’s source text.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS6.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Task-level:</h5>
<div class="ltx_para" id="S3.SS6.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS6.SSS0.Px2.p1.1">Based on the work of <cite class="ltx_cite ltx_citemacro_citet">Agrawal et al., (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite>, the task-level approach is similar to the random approach but differs in the number of trials used. We employ 100 trials for the task-level approach, a significantly higher number than the random approach. The reason for using more trials is to generate a greater variety of ICEs, aiming to enhance the performance of LLMs in the translation task. However, this results in longer execution times compared to the random approach.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS6.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">BM25:</h5>
<div class="ltx_para" id="S3.SS6.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS6.SSS0.Px3.p1.1">Using the Moses Tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Koehn et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib11" title="">2007</a>)</cite>, we first tokenize the training set’s source samples. Then, a BM25 model is created for the tokenized corpus by employing the <span class="ltx_text ltx_font_italic" id="S3.SS6.SSS0.Px3.p1.1.1">BM25Okapi</span> implementation within the <span class="ltx_text ltx_font_italic" id="S3.SS6.SSS0.Px3.p1.1.2">rank_bm25</span> package.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/dorianbrown/rank_bm25" title="">https://github.com/dorianbrown/rank_bm25</a></span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS6.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS6.SSS0.Px3.p2.1">Next, the test set is tokenized using the tokenized source.
The algorithm then searches for similar training samples based on BM25 criteria, selecting the top <math alttext="q" class="ltx_Math" display="inline" id="S3.SS6.SSS0.Px3.p2.1.m1.1"><semantics id="S3.SS6.SSS0.Px3.p2.1.m1.1a"><mi id="S3.SS6.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS6.SSS0.Px3.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.SSS0.Px3.p2.1.m1.1b"><ci id="S3.SS6.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS6.SSS0.Px3.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.SSS0.Px3.p2.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.SSS0.Px3.p2.1.m1.1d">italic_q</annotation></semantics></math> matches for the model. This methodology utilizes the test set as opposed to random and task-level approaches using the development set.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS6.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Re-rank BM25 (R-BM25):</h5>
<div class="ltx_para" id="S3.SS6.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS6.SSS0.Px4.p1.1">BM25 aims to find translation examples with the highest n-gram overlap with the source sentence <cite class="ltx_cite ltx_citemacro_citep">(Luo et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib19" title="">2023</a>)</cite>. However, since retrieved examples score independently, top matches may lack coverage of all source n-grams. This poses an issue in ICL due to LLM input size limitations. To address this, <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al., (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite> proposed R-BM25.
R-BM25 employs a recall-based n-gram overlap <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite> to extract word n-grams and their numbers from the test source and BM25 retrieved examples.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS6.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Fine-tuning mBART-50:</h5>
<div class="ltx_para" id="S3.SS6.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS6.SSS0.Px5.p1.1">Different ICL methodologies, including our own, are assessed in comparison to the process of fine-tuning a pre-trained multilingual MT model, specifically mBART-50. The selection of mBART-50 is based on its alignment with the language specifications of the experiment and its proven track record of achieving success in MT tasks through the utilization of pre-trained models <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib42" title="">2022</a>; Pham et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib23" title="">2022</a>)</cite>. The fine-tuning of mBART-50 is carried out using the training data outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS4" title="3.4 Dataset and Evaluation Metrics ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Computational Costs</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">We monitored and reported the computational costs of the models utilized in our experiments using the <em class="ltx_emph ltx_font_italic" id="S3.SS7.p1.1.1">carbontracker</em> package.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lfwa/carbontracker" title="">https://github.com/lfwa/carbontracker</a></span></span></span> This involved calculating the carbon footprint (CO<sub class="ltx_sub" id="S3.SS7.p1.1.2">2</sub>eq) emissions, time to prediction (TTP), and electricity consumption (kWh) associated with our experiments. Our experiments were conducted using NVIDIA A40 GPUs.</p>
</div>
<div class="ltx_para" id="S3.SS7.p2">
<p class="ltx_p" id="S3.SS7.p2.1">The script for running our experiments is publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/JoyeBright/ICLviaQE/" title="">https://github.com/JoyeBright/ICLviaQE/</a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This section presents the results of our experiments. To ensure a fair comparison, we conducted a statistical analysis test (t-test) to determine if our models significantly outperformed the baseline.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Comparing to previous work, the results shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S4.T1" title="Table 1 ‣ 4 Experiments Results ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, indicate that R-BM25 with 16 ICEs outperforms other methods. It is notable that there is a positive correlation between the number of examples and evaluation scores (consistent through all methods – Random, Task-level, BM25, and R-BM25), although at the expense of prediction time (i.e., TTP). Employing 16 examples significantly improved performance compared to using only one example in the random approach.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Analyzing the performance of our methods in Mode 1 (referred to as “M 1”, with P = 3, 8, or 16 in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S4.T1" title="Table 1 ‣ 4 Experiments Results ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>), we observe that our proposed methodology with different patience thresholds consistently outperforms all previous methods, including the baseline. This trend holds for both the COMET and BLEU metrics across all the methods. Specifically, our method exhibits a minimum improvement of 0.52 points in the BLEU score (from 45.20 to 45.72) with patience threshold of 3 and a maximum improvement of 1.58 points in the BLEU score (from 45.20 to 46.78) with a patience threshold of 16 compared to R-BM25 with 16 examples.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.2" style="width:433.6pt;height:590.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.6pt,-115.1pt) scale(1.63956371182465,1.63956371182465) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt" id="S4.T1.2.2.2.3" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.3.1">Method</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.2.2.2.2" style="padding:1pt 1.5pt;">
<math alttext="p" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mi id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">italic_p</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.2.1"> + <math alttext="q" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.1.m1.1"><semantics id="S4.T1.2.2.2.2.1.m1.1a"><mi id="S4.T1.2.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.2.1.m1.1d">italic_q</annotation></semantics></math></span>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.2.4" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.4.1">BLEU</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.2.5" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.5.1">COMET</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.2.6" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.6.1">TTP</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.2.7" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.7.1">CO2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T1.2.2.2.8" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.8.1">GPU</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.3.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.3.1.1" style="padding:1pt 1.5pt;"></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.3.1.2" style="padding:1pt 1.5pt;"></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" id="S4.T1.2.2.3.1.3" style="padding:1pt 1.5pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" id="S4.T1.2.2.3.1.4" style="padding:1pt 1.5pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.3.1.5" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1.5.1">(hh:mm)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.3.1.6" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1.6.1">(kg)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.3.1.7" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1.7.1">(kWh)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.4.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t" id="S4.T1.2.2.4.2.1" style="padding:1pt 1.5pt;">Random</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.2" style="padding:1pt 1.5pt;">1 + 0</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.3" style="padding:1pt 1.5pt;">10.38</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.4" style="padding:1pt 1.5pt;">0.6895</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.5" style="padding:1pt 1.5pt;">01:51</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.6" style="padding:1pt 1.5pt;">00.13</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.2.2.4.2.7" style="padding:1pt 1.5pt;">00.39</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.5.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.5.3.1" style="padding:1pt 1.5pt;">Random</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.5.3.2" style="padding:1pt 1.5pt;">16 + 0</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.5.3.3" style="padding:1pt 1.5pt;">31.65</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.5.3.4" style="padding:1pt 1.5pt;">0.7844</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.5.3.5" style="padding:1pt 1.5pt;">02:20</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.5.3.6" style="padding:1pt 1.5pt;">00.19</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.5.3.7" style="padding:1pt 1.5pt;">00.58</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.6.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.6.4.1" style="padding:1pt 1.5pt;">Task-level</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.6.4.2" style="padding:1pt 1.5pt;">1 + 0</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.6.4.3" style="padding:1pt 1.5pt;">29.17</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.6.4.4" style="padding:1pt 1.5pt;">0.7586</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.6.4.5" style="padding:1pt 1.5pt;">62:50</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.6.4.6" style="padding:1pt 1.5pt;">09.83</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.6.4.7" style="padding:1pt 1.5pt;">29.10</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.7.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.7.5.1" style="padding:1pt 1.5pt;">Task-level</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.7.5.2" style="padding:1pt 1.5pt;">16 + 0</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.7.5.3" style="padding:1pt 1.5pt;">32.88</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.7.5.4" style="padding:1pt 1.5pt;">0.8083</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.7.5.5" style="padding:1pt 1.5pt;">78:30</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.7.5.6" style="padding:1pt 1.5pt;">12.80</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.7.5.7" style="padding:1pt 1.5pt;">35.91</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.8.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.8.6.1" style="padding:1pt 1.5pt;">BM25</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.8.6.2" style="padding:1pt 1.5pt;">0 + 1</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.8.6.3" style="padding:1pt 1.5pt;">39.24</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.8.6.4" style="padding:1pt 1.5pt;">0.7833</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.8.6.5" style="padding:1pt 1.5pt;">00:56</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.8.6.6" style="padding:1pt 1.5pt;">00.06</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.8.6.7" style="padding:1pt 1.5pt;">00.19</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.9.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.9.7.1" style="padding:1pt 1.5pt;">BM25</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.9.7.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.9.7.3" style="padding:1pt 1.5pt;">44.50</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.9.7.4" style="padding:1pt 1.5pt;">0.8120</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.9.7.5" style="padding:1pt 1.5pt;">00:58</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.9.7.6" style="padding:1pt 1.5pt;">00.07</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.9.7.7" style="padding:1pt 1.5pt;">00.19</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.10.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.10.8.1" style="padding:1pt 1.5pt;">R-BM25</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.10.8.2" style="padding:1pt 1.5pt;">0 + 1</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.10.8.3" style="padding:1pt 1.5pt;">40.88</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.10.8.4" style="padding:1pt 1.5pt;">0.7990</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.10.8.5" style="padding:1pt 1.5pt;">01:01</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.10.8.6" style="padding:1pt 1.5pt;">00.06</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.10.8.7" style="padding:1pt 1.5pt;">00.21</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.11.9">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.11.9.1" style="padding:1pt 1.5pt;">R-BM25</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.11.9.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.11.9.3" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.11.9.3.1">45.20</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.11.9.4" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.11.9.4.1">0.8218</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.11.9.5" style="padding:1pt 1.5pt;">01:04</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.11.9.6" style="padding:1pt 1.5pt;">00.07</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.11.9.7" style="padding:1pt 1.5pt;">00.21</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.12.10">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t" id="S4.T1.2.2.12.10.1" style="padding:1pt 1.5pt;">M 1, P = 3</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.3" style="padding:1pt 1.5pt;">45.72</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.4" style="padding:1pt 1.5pt;">0.8395</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.5" style="padding:1pt 1.5pt;">01:49</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.6" style="padding:1pt 1.5pt;">00.22</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.2.2.12.10.7" style="padding:1pt 1.5pt;">00.67</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.13.11">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.13.11.1" style="padding:1pt 1.5pt;">M 1, P = 8</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.13.11.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.13.11.3" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.13.11.3.1">46.43</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.13.11.4" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.13.11.4.1">0.8501</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.13.11.5" style="padding:1pt 1.5pt;">03:48</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.13.11.6" style="padding:1pt 1.5pt;">00.50</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.13.11.7" style="padding:1pt 1.5pt;">01.51</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.14.12">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.14.12.1" style="padding:1pt 1.5pt;">M 1, P = 16</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.14.12.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.14.12.3" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.14.12.3.1">46.78</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.14.12.4" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.14.12.4.1">0.8554</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.14.12.5" style="padding:1pt 1.5pt;">05:11</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.14.12.6" style="padding:1pt 1.5pt;">00.68</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.14.12.7" style="padding:1pt 1.5pt;">02.05</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.15.13">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t" id="S4.T1.2.2.15.13.1" style="padding:1pt 1.5pt;">M 2, P = 3</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.3" style="padding:1pt 1.5pt;">46.05</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.4" style="padding:1pt 1.5pt;">0.8400</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.5" style="padding:1pt 1.5pt;">01:30</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.6" style="padding:1pt 1.5pt;">00.21</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.2.2.15.13.7" style="padding:1pt 1.5pt;">00.64</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.16.14">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.16.14.1" style="padding:1pt 1.5pt;">M 2, P = 8</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.16.14.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.16.14.3" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.16.14.3.1">46.59</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.16.14.4" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.16.14.4.1">0.8518</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.16.14.5" style="padding:1pt 1.5pt;">03:52</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.16.14.6" style="padding:1pt 1.5pt;">00.51</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.16.14.7" style="padding:1pt 1.5pt;">01.52</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.17.15">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.17.15.1" style="padding:1pt 1.5pt;">M 2, P = 16</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.17.15.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.17.15.3" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.17.15.3.1">46.52</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.17.15.4" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.17.15.4.1">0.8564</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.17.15.5" style="padding:1pt 1.5pt;">05:00</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.17.15.6" style="padding:1pt 1.5pt;">00.66</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.17.15.7" style="padding:1pt 1.5pt;">02.01</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.18.16">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t" id="S4.T1.2.2.18.16.1" style="padding:1pt 1.5pt;">M 3, P = 3</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.2.2.18.16.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.18.16.3" style="padding:1pt 1.5pt;">49.89</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.18.16.4" style="padding:1pt 1.5pt;">0.8532</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.18.16.5" style="padding:1pt 1.5pt;">01:36</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.18.16.6" style="padding:1pt 1.5pt;">00.22</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.2.2.18.16.7" style="padding:1pt 1.5pt;">00.66</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.19.17">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.19.17.1" style="padding:1pt 1.5pt;">M 3, P = 8</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.19.17.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.19.17.3" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.19.17.3.1">52.63</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.19.17.4" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.19.17.4.1">0.8725</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.19.17.5" style="padding:1pt 1.5pt;">03:14</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.19.17.6" style="padding:1pt 1.5pt;">00.45</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.19.17.7" style="padding:1pt 1.5pt;">01.40</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.20.18">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_rr" id="S4.T1.2.2.20.18.1" style="padding:1pt 1.5pt;">M 3, P = 16</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.2.2.20.18.2" style="padding:1pt 1.5pt;">0 + 16</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.20.18.3" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.20.18.3.1">53.50</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.20.18.4" style="padding:1pt 1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.20.18.4.1">0.8791</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.20.18.5" style="padding:1pt 1.5pt;">04:08</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T1.2.2.20.18.6" style="padding:1pt 1.5pt;">00.55</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T1.2.2.20.18.7" style="padding:1pt 1.5pt;">01.65</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.21.19">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_rr ltx_border_t" id="S4.T1.2.2.21.19.1" style="padding:1pt 1.5pt;">mBART-50</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.2.2.21.19.2" style="padding:1pt 1.5pt;">N/A</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.2.2.21.19.3" style="padding:1pt 1.5pt;">42.76</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.2.2.21.19.4" style="padding:1pt 1.5pt;">0.8659</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.2.2.21.19.5" style="padding:1pt 1.5pt;">11:20</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.2.2.21.19.6" style="padding:1pt 1.5pt;">01.88</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.2.21.19.7" style="padding:1pt 1.5pt;">04.82</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S4.T1.14.1">Method Performance in BLEU and COMET Scores.</span> <math alttext="M" class="ltx_Math" display="inline" id="S4.T1.8.m1.1"><semantics id="S4.T1.8.m1.1b"><mi id="S4.T1.8.m1.1.1" xref="S4.T1.8.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.T1.8.m1.1c"><ci id="S4.T1.8.m1.1.1.cmml" xref="S4.T1.8.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.m1.1d">M</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.m1.1e">italic_M</annotation></semantics></math> 1 to 3 denotes Mode 1 to 3; <math alttext="P" class="ltx_Math" display="inline" id="S4.T1.9.m2.1"><semantics id="S4.T1.9.m2.1b"><mi id="S4.T1.9.m2.1.1" xref="S4.T1.9.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.T1.9.m2.1c"><ci id="S4.T1.9.m2.1.1.cmml" xref="S4.T1.9.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.m2.1d">P</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.m2.1e">italic_P</annotation></semantics></math> is the patience value; <math alttext="p" class="ltx_Math" display="inline" id="S4.T1.10.m3.1"><semantics id="S4.T1.10.m3.1b"><mi id="S4.T1.10.m3.1.1" xref="S4.T1.10.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.T1.10.m3.1c"><ci id="S4.T1.10.m3.1.1.cmml" xref="S4.T1.10.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.m3.1d">p</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.m3.1e">italic_p</annotation></semantics></math> and <math alttext="q" class="ltx_Math" display="inline" id="S4.T1.11.m4.1"><semantics id="S4.T1.11.m4.1b"><mi id="S4.T1.11.m4.1.1" xref="S4.T1.11.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.T1.11.m4.1c"><ci id="S4.T1.11.m4.1.1.cmml" xref="S4.T1.11.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.m4.1d">q</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.m4.1e">italic_q</annotation></semantics></math> are as defined in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S3.SS5" title="3.5 Number of ICEs ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3.5</span></a>. “N/A” (not applicable) indicates that fine-tuning does not use ICEs. Bold font represents the highest translation performance. Two numbers are in bold if they are statistically similar (t-test, <math alttext="p\_value=0.05" class="ltx_Math" display="inline" id="S4.T1.12.m5.1"><semantics id="S4.T1.12.m5.1b"><mrow id="S4.T1.12.m5.1.1" xref="S4.T1.12.m5.1.1.cmml"><mrow id="S4.T1.12.m5.1.1.2" xref="S4.T1.12.m5.1.1.2.cmml"><mi id="S4.T1.12.m5.1.1.2.2" xref="S4.T1.12.m5.1.1.2.2.cmml">p</mi><mo id="S4.T1.12.m5.1.1.2.1" xref="S4.T1.12.m5.1.1.2.1.cmml">⁢</mo><mi id="S4.T1.12.m5.1.1.2.3" mathvariant="normal" xref="S4.T1.12.m5.1.1.2.3.cmml">_</mi><mo id="S4.T1.12.m5.1.1.2.1b" xref="S4.T1.12.m5.1.1.2.1.cmml">⁢</mo><mi id="S4.T1.12.m5.1.1.2.4" xref="S4.T1.12.m5.1.1.2.4.cmml">v</mi><mo id="S4.T1.12.m5.1.1.2.1c" xref="S4.T1.12.m5.1.1.2.1.cmml">⁢</mo><mi id="S4.T1.12.m5.1.1.2.5" xref="S4.T1.12.m5.1.1.2.5.cmml">a</mi><mo id="S4.T1.12.m5.1.1.2.1d" xref="S4.T1.12.m5.1.1.2.1.cmml">⁢</mo><mi id="S4.T1.12.m5.1.1.2.6" xref="S4.T1.12.m5.1.1.2.6.cmml">l</mi><mo id="S4.T1.12.m5.1.1.2.1e" xref="S4.T1.12.m5.1.1.2.1.cmml">⁢</mo><mi id="S4.T1.12.m5.1.1.2.7" xref="S4.T1.12.m5.1.1.2.7.cmml">u</mi><mo id="S4.T1.12.m5.1.1.2.1f" xref="S4.T1.12.m5.1.1.2.1.cmml">⁢</mo><mi id="S4.T1.12.m5.1.1.2.8" xref="S4.T1.12.m5.1.1.2.8.cmml">e</mi></mrow><mo id="S4.T1.12.m5.1.1.1" xref="S4.T1.12.m5.1.1.1.cmml">=</mo><mn id="S4.T1.12.m5.1.1.3" xref="S4.T1.12.m5.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.12.m5.1c"><apply id="S4.T1.12.m5.1.1.cmml" xref="S4.T1.12.m5.1.1"><eq id="S4.T1.12.m5.1.1.1.cmml" xref="S4.T1.12.m5.1.1.1"></eq><apply id="S4.T1.12.m5.1.1.2.cmml" xref="S4.T1.12.m5.1.1.2"><times id="S4.T1.12.m5.1.1.2.1.cmml" xref="S4.T1.12.m5.1.1.2.1"></times><ci id="S4.T1.12.m5.1.1.2.2.cmml" xref="S4.T1.12.m5.1.1.2.2">𝑝</ci><ci id="S4.T1.12.m5.1.1.2.3.cmml" xref="S4.T1.12.m5.1.1.2.3">_</ci><ci id="S4.T1.12.m5.1.1.2.4.cmml" xref="S4.T1.12.m5.1.1.2.4">𝑣</ci><ci id="S4.T1.12.m5.1.1.2.5.cmml" xref="S4.T1.12.m5.1.1.2.5">𝑎</ci><ci id="S4.T1.12.m5.1.1.2.6.cmml" xref="S4.T1.12.m5.1.1.2.6">𝑙</ci><ci id="S4.T1.12.m5.1.1.2.7.cmml" xref="S4.T1.12.m5.1.1.2.7">𝑢</ci><ci id="S4.T1.12.m5.1.1.2.8.cmml" xref="S4.T1.12.m5.1.1.2.8">𝑒</ci></apply><cn id="S4.T1.12.m5.1.1.3.cmml" type="float" xref="S4.T1.12.m5.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.m5.1d">p\_value=0.05</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.m5.1e">italic_p _ italic_v italic_a italic_l italic_u italic_e = 0.05</annotation></semantics></math>).</figcaption>
</figure>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Consequently, our methods in Mode 1 are ranked based on their performance, with patience 3 being the least effective model, followed by patience 8, and finally patience 16, representing the most effective method. This ranking indicates that increasing the patience threshold can significantly enhance the translation performance. However, the improvement with patience 16 is not statistically significant compared to patience 8, suggesting that more ICEs do not necessarily enhance translation performance. Similarly, while more substantial contextual improvement (as indicated by the COMET) is observed at the maximum patience threshold (16), it is not statistically significant compared to patience 8.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">The Mode 2 results demonstrate that all three patience thresholds surpass the methods in the literature. However, this improvement is not statistically significant when compared with the respective experiments in Mode 1. This suggests that ordering the examples according to n-gram (unigram) similarity does not enhance the translation performance in our methodology.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">When it comes to Mode 3, we should stress that this is an unrealistic scenario, but used as the highest bound. The results indicate that with a patience of 3, the BLEU score is 4.17 points lower (49.89-45.72). With a patience of 8, this gap increases to 6.2 points (52.63-46.43), and with a patience of 16, it widens further to 6.72 points (53.50-46.78). These differences arise from the QE model estimations in our experiment compared to the scenario where reference labels are available to the search algorithm.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Time to Prediction (TTP)</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Among the methods examined, task-level execution required the most time, with approximately 62 hours for one example and 78 hours for 16 examples. Our method (Mode 1) with a patience value of 16 is relatively time-intensive, taking approximately 5 hours, while a patience value of 3 is comparable to the baseline method, differing by only around 50 minutes. Mode 2 is nearly equivalent to Mode 1 in terms of TTP, whereas Mode 3, where the reference labels are accessed, requires less time than Modes 1 and 2. In addition, the search algorithm incorporates a termination condition, and given that QE estimation rarely triggers this condition, numerous ICEs are left unattempted, resulting in significant time savings.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.2">It is also important to note the time required to train the QE models used in the prediction process. As provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#Ax1.T3" title="Table 3 ‣ Appendices ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, the training time for the generic QE model is <math alttext="+/-" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.3"><semantics id="S4.SS1.p2.1.m1.3a"><mrow id="S4.SS1.p2.1.m1.3.4.2" xref="S4.SS1.p2.1.m1.3.4.1.cmml"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">+</mo><mo id="S4.SS1.p2.1.m1.3.4.2.1" lspace="0em" xref="S4.SS1.p2.1.m1.3.4.1.cmml">⁣</mo><mo id="S4.SS1.p2.1.m1.2.2" xref="S4.SS1.p2.1.m1.2.2.cmml">/</mo><mo id="S4.SS1.p2.1.m1.3.4.2.2" lspace="0em" xref="S4.SS1.p2.1.m1.3.4.1.cmml">⁣</mo><mo id="S4.SS1.p2.1.m1.3.3" xref="S4.SS1.p2.1.m1.3.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.3b"><list id="S4.SS1.p2.1.m1.3.4.1.cmml" xref="S4.SS1.p2.1.m1.3.4.2"><plus id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></plus><divide id="S4.SS1.p2.1.m1.2.2.cmml" xref="S4.SS1.p2.1.m1.2.2"></divide><minus id="S4.SS1.p2.1.m1.3.3.cmml" xref="S4.SS1.p2.1.m1.3.3"></minus></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.3c">+/-</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.3d">+ / -</annotation></semantics></math> 5 hours and 55 minutes, while the specific QE model takes about <math alttext="+/-" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.3"><semantics id="S4.SS1.p2.2.m2.3a"><mrow id="S4.SS1.p2.2.m2.3.4.2" xref="S4.SS1.p2.2.m2.3.4.1.cmml"><mo id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">+</mo><mo id="S4.SS1.p2.2.m2.3.4.2.1" lspace="0em" xref="S4.SS1.p2.2.m2.3.4.1.cmml">⁣</mo><mo id="S4.SS1.p2.2.m2.2.2" xref="S4.SS1.p2.2.m2.2.2.cmml">/</mo><mo id="S4.SS1.p2.2.m2.3.4.2.2" lspace="0em" xref="S4.SS1.p2.2.m2.3.4.1.cmml">⁣</mo><mo id="S4.SS1.p2.2.m2.3.3" xref="S4.SS1.p2.2.m2.3.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.3b"><list id="S4.SS1.p2.2.m2.3.4.1.cmml" xref="S4.SS1.p2.2.m2.3.4.2"><plus id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"></plus><divide id="S4.SS1.p2.2.m2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2"></divide><minus id="S4.SS1.p2.2.m2.3.3.cmml" xref="S4.SS1.p2.2.m2.3.3"></minus></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.3c">+/-</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.3d">+ / -</annotation></semantics></math> 6 hours and 54 minutes. Although these training times are significant, it is crucial to recognize that QE models, similar to MT models, can be reused for the same language pair and domain, thereby amortizing the initial training cost over multiple predictions.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The last row of Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S4.T1" title="Table 1 ‣ 4 Experiments Results ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the scores of the translations obtained with the mBART-50 model fine-tuned on the same training set as in ICL. Despite mBART-50 being tailored for MT across 50 languages, it did not outperform the R-BM25 method with 16 examples (best from the existing methods); it was better only than Random, Task-level, BM25, and R-BM25, each with only 1 example. However, when considering translation performance from a contextual perspective, the COMET results indicate that fine-tuning mBART-50 leads to superior performance compared with lexical overlap. Nevertheless, fine-tuning took significantly longer than identifying ICEs and obtaining inferences from the XGLM.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Compared to our methodology, especially when considering the least performing method (M 1, P = 3), it is significantly worse – 6.47% (42.76 to 45.72). This highlights the substantial efficacy of ICL compared to fine-tuning. Nonetheless, it is noteworthy that various factors might contribute to this observation: e.g., the model’s size might be a critical factor, especially during deployment, where larger models like XGLM could pose challenges.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Output analysis</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Pre-trained LLMs often exhibit over-generation, i.e., the generation of a larger number of tokens than expected by a human (in comparison to a reference), necessitating extensive post-processing (e.g., post-editing) <cite class="ltx_cite ltx_citemacro_citep">(Bawden and Yvon,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib4" title="">2023</a>)</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S5.F1" title="Figure 1 ‣ Output analysis ‣ 5 Analysis ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the tokenized output lengths (translations) for our model (Mode 1, patience 8),<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Our other models in Mode 1 exhibited similar distributions.</span></span></span> alongside the R-BM25 with 16 examples. The analysis shows that the length distributions for both models align with the reference distribution, suggesting that the models do not over-generate.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p2.4">To quantitatively compare these distributions to the reference, we employed the Kolmogorov-Smirnov (KS) test <cite class="ltx_cite ltx_citemacro_citep">(Kolmogorov,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib13" title="">1933</a>)</cite>. The results indicate that for R-BM25 versus the reference, the KS statistic is relatively high (<math alttext="0.0749" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p2.1.m1.1"><semantics id="S5.SS0.SSS0.Px1.p2.1.m1.1a"><mn id="S5.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">0.0749</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p2.1.m1.1b"><cn id="S5.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px1.p2.1.m1.1.1">0.0749</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p2.1.m1.1c">0.0749</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p2.1.m1.1d">0.0749</annotation></semantics></math>), reflecting a significant difference between the translation lengths of R-BM25 and the reference distribution. The extremely low p-value (<math alttext="2.39\times 10^{-5}" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p2.2.m2.1"><semantics id="S5.SS0.SSS0.Px1.p2.2.m2.1a"><mrow id="S5.SS0.SSS0.Px1.p2.2.m2.1.1" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.2" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.2.cmml">2.39</mn><mo id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.1.cmml">×</mo><msup id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.cmml"><mn id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.2" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.cmml"><mo id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3a" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.2" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p2.2.m2.1b"><apply id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1"><times id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.1"></times><cn id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.2.cmml" type="float" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.2">2.39</cn><apply id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3">superscript</csymbol><cn id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.2.cmml" type="integer" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.2">10</cn><apply id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3"><minus id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.1.cmml" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3"></minus><cn id="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.2.cmml" type="integer" xref="S5.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p2.2.m2.1c">2.39\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p2.2.m2.1d">2.39 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>) further confirms this significant discrepancy. Conversely, for Mode 1 with P=8 versus the reference, the KS statistic is considerably lower (<math alttext="0.0232" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p2.3.m3.1"><semantics id="S5.SS0.SSS0.Px1.p2.3.m3.1a"><mn id="S5.SS0.SSS0.Px1.p2.3.m3.1.1" xref="S5.SS0.SSS0.Px1.p2.3.m3.1.1.cmml">0.0232</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p2.3.m3.1b"><cn id="S5.SS0.SSS0.Px1.p2.3.m3.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px1.p2.3.m3.1.1">0.0232</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p2.3.m3.1c">0.0232</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p2.3.m3.1d">0.0232</annotation></semantics></math>), indicating a much smaller difference in translation lengths. The higher p-value (<math alttext="0.6451" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p2.4.m4.1"><semantics id="S5.SS0.SSS0.Px1.p2.4.m4.1a"><mn id="S5.SS0.SSS0.Px1.p2.4.m4.1.1" xref="S5.SS0.SSS0.Px1.p2.4.m4.1.1.cmml">0.6451</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p2.4.m4.1b"><cn id="S5.SS0.SSS0.Px1.p2.4.m4.1.1.cmml" type="float" xref="S5.SS0.SSS0.Px1.p2.4.m4.1.1">0.6451</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p2.4.m4.1c">0.6451</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p2.4.m4.1d">0.6451</annotation></semantics></math>) suggests no significant difference, implying that the distribution of Mode 1, P=8 is similar to the reference distribution.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p3.1">These findings suggest that our proposed methodology could yield translations closer in length to the reference, potentially reducing the need for labor-intensive post-processing efforts and enhancing computational efficiency.</p>
</div>
<figure class="ltx_figure" id="S5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="351" id="S5.F1.g1" src="extracted/5862440/tokenization_lengths_distribution.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S5.F1.2.1">Tokenized Translation Lengths</span> comparison between R-BM25, our Mode 1, P=8, and the reference. “KS” denotes the Kolmogorov-Smirnov test, with the p-value indicating significance.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">ICE Number Analysis</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">The number of selected ICEs holds a significant importance within the ICL algorithm, as it directly impacts the token processing time and the capacity of LLMs to handle additional ICE instances. We analyzed the number of ICEs that our algorithm selected across all three modes. The results (Table <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#S5.T2" title="Table 2 ‣ ICE Number Analysis ‣ 5 Analysis ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>) show that the minimum number of ICEs selected is 1, while the maximum is: 12 for Mode 1, 16 for Mode 2, and 16 for Mode 3.
The average (mean) number of ICEs is found to be lowest in Mode 3 and highest in Mode 1. In addition, Mode 2 results in a reduction in the number of ICEs within our proposed algorithm. The notably lower average number of ICE instances in Mode 3 can be attributed to its access to the test set, allowing for the selection of optimal ICE combinations based on test set performance and activating an early stopping condition if the score exceeds 100. Contrarily, while Mode 1 exhibits similarities to Mode 3, its relatively higher average can be linked to inaccuracies in QE estimation. Moreover, our analysis shows that QE estimations rarely reach a score of 100, thus rendering the early stopping condition inactive.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.1" style="width:433.6pt;height:137.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(103.1pt,-32.6pt) scale(1.9064839542281,1.9064839542281) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.1.1">Mode</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.1.2">Min</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.1.1.1.1.3">Mean</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.4">Max</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.1.1.2.1.1">#1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.1.2.1.2">[1, 1, 1]</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.1.2.1.3">[2.25, 3.76, 4.84]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.1.4">[12, 16, 16]</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.1.1.3.2.1">#2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.1.3.2.2">[1, 1, 1]</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.1.3.2.3">[2.20, 3.70, 4.74]</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.4">[12, 16, 16]</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.1.1.4.3.1">#3</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.1.1.4.3.2">[1, 1, 1]</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.1.1.4.3.3">[2.15, 3.47, 4.47]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.4.3.4">[12, 16, 16]</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S5.T2.3.1">Number of ICEs selected for each mode at different patience thresholds.</span> Labels [x, y, z] correspond to patience values 3, 8, and 16.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">CO<sub class="ltx_sub" id="S5.SS0.SSS0.Px3.1.1">2</sub> Emissions</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">Our analysis reveals that using XGLM for translation yields lower CO<sub class="ltx_sub" id="S5.SS0.SSS0.Px3.p1.1.1">2</sub> emissions than fine-tuning mBART-50, making it a more environmentally sustainable choice. In Mode 1 of our proposed methodology, with patience 16, XGLM emitted 0.68 KG of CO<sub class="ltx_sub" id="S5.SS0.SSS0.Px3.p1.1.2">2</sub>, while fine-tuning mBART-50 emitted 1.88 KG. Interestingly, the task-level method with 16 ICEs emitted the highest amount of CO<sub class="ltx_sub" id="S5.SS0.SSS0.Px3.p1.1.3">2</sub>, totaling 12.80 KG. Our proposed approach leads to higher CO<sub class="ltx_sub" id="S5.SS0.SSS0.Px3.p1.1.4">2</sub> emissions than R-BM25.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">ICL for MT.</h5>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">ICL<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Also referred to as the prompt retrieval method</span></span></span> represents a relatively new paradigm in natural language understanding. Unlike traditional fine-tuning approaches, where a PLM undergoes parameter updates using a specific dataset, ICL typically directly generates the output without any modification to its parameters <cite class="ltx_cite ltx_citemacro_citep">(Radford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib25" title="">2019</a>; Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib6" title="">2020</a>)</cite>. This is achieved by solely providing the model with a few examples, known as ICEs, which prime the PLM to enhance its performance for the given task <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib10" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p2.1">As shown by <cite class="ltx_cite ltx_citemacro_cite">Vilar et al., (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib39" title="">2023</a>)</cite>, the quality of translation is directly proportionate to the quality of ICEs, where quality refers to ICEs being relevant, clear, accurate, and domain-specific. However, considering all ICEs during processing is computationally demanding <cite class="ltx_cite ltx_citemacro_citep">(Alves et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib3" title="">2023</a>)</cite>. Hence, it is crucial to selectively choose ICEs that can enhance MT quality. <cite class="ltx_cite ltx_citemacro_cite">Goyal et al., (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib8" title="">2022</a>)</cite> conducted a study where ICEs were randomly selected. Despite finding that this random selection of ICEs resulted in good translation performance, the neglect of their order, which was identified as important <cite class="ltx_cite ltx_citemacro_citep">(Liu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib17" title="">2022</a>; Lu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib18" title="">2022</a>)</cite>, was a drawback in this approach. To address this, methodologies such as <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib1" title="">2023</a>)</cite> introduced a re-ranking technique (R-BM25). However, their methodology relies solely on n-grams to order examples, which can enhance fluency but may overlook contextual factors. In our approach, we investigated the unigram order of initial ICEs provided by the BM25 algorithm. We leave the in-depth analysis of ICE order for future work. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Kumar et al., (<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib14" title="">2023</a>)</cite> highlighted the advantages of using multiple features in ICE selection to improve translation quality, while our QE-based approach simplifies ICE selection without needing to generate additional features, ensuring efficiency.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">QE in MT Evaluation.</h5>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">QE models offer a quick solution to the assessment of the overall usefulness of translated text. These models do not rely on reference translations, thereby reducing the human effort required for quality evaluation <cite class="ltx_cite ltx_citemacro_citep">(Tamchyna,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib35" title="">2021</a>; Murgolo et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib20" title="">2022</a>; Zerva et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib43" title="">2022</a>; Blain et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib5" title="">2023</a>)</cite>.
Similar to MT models, previous studies highlight the importance of domain-specific QE for accurately estimating translation quality across diverse domains <cite class="ltx_cite ltx_citemacro_citep">(Lee,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib15" title="">2020</a>; Sharami et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib32" title="">2023</a>)</cite>. This is why, in our work, we employed a domain-specific QE model instead of a generic one to enhance the selection of ICEs.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p2.1">Integrating QE into ICL offers significant, yet largely unexplored, potential. QE can also better capture out-of-domain gender and word-sense-disambiguation errors <cite class="ltx_cite ltx_citemacro_citep">(Dinh and Niehues,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib7" title="">2023</a>)</cite>. Additionally, integrating QE can mitigate reference bias, a significant challenge in accurately estimating the output quality of LLMs <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib9" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib28" title="">Raunak et al., 2023b, </a>)</cite>. The introduction of COMET-QE <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib27" title="">Raunak et al., 2023a, </a>)</cite> exemplifies this pursuit, providing a metric tailored to evaluate the quality of perturbed prompts provided to GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2406.07970v3#bib.bib6" title="">2020</a>)</cite>, aiming to mitigate reference bias. While in our approach, we employ domain-specific QE to guide the selection of ICEs, this underscores the potential of QE in refining LLM inputs (i.e., ICEs).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We propose a novel in-context learning (ICL) methodology for enhancing the translation capabilities of large language models (LLMs) while optimizing computational resources. Our approach leverages domain-specific quality estimation (QE) to guide in-context selection, particularly focusing on determining the suboptimal number and the combinations of in-context examples (ICEs). This novel strategy moves beyond the conventional reliance solely on translation references from development sets seen in prior methods.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">We evaluated our approach across different modes and early stopping patience values on the German-to-English IT dataset. Our experiments consistently showed the superior performance of our methodology, surpassing all prior works across both BLEU and COMET metrics. Our method consistently improves BLEU scores, although this comes at the cost of increased computation time. We also investigated the impact of ordering the ICEs based on their unigram overlap with the source text and found it to be not statistically significant. Furthermore, our experiments highlighted the value of ICL compared to fine-tuning a pre-trained large model, namely mBART-50. We also highlighted that our method leads to less carbon emissions while achieving better translation performance.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">In the future, we would like to conduct further research on the impact of our proposed methodology across different language pairs, domains and LLMs. Also, we aim to explore alternative metrics beyond BLEU to tailor the selection process, as well as additional features such as bigram, type/token ratio, and length when ordering examples prior to their input into LLMs.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgments</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We would like to thank the anonymous reviewers for their helpful comments.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al.,  (2023)</span>
<span class="ltx_bibblock">
Agrawal, S., Zhou, C., Lewis, M., Zettlemoyer, L., and Ghazvininejad, M. (2023).

</span>
<span class="ltx_bibblock">In-context examples selection for machine translation.

</span>
<span class="ltx_bibblock">In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Findings of the Association for Computational Linguistics: ACL 2023</span>, pages 8857–8873, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni and Goldberg,  (2020)</span>
<span class="ltx_bibblock">
Aharoni, R. and Goldberg, Y. (2020).

</span>
<span class="ltx_bibblock">Unsupervised domain clusters in pretrained language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alves et al.,  (2023)</span>
<span class="ltx_bibblock">
Alves, D., Guerreiro, N., Alves, J., Pombal, J., Rei, R., de Souza, J., Colombo, P., and Martins, A. (2023).

</span>
<span class="ltx_bibblock">Steering large language models for machine translation with finetuning and in-context learning.

</span>
<span class="ltx_bibblock">In Bouamor, H., Pino, J., and Bali, K., editors, <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pages 11127–11148, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden and Yvon,  (2023)</span>
<span class="ltx_bibblock">
Bawden, R. and Yvon, F. (2023).

</span>
<span class="ltx_bibblock">Investigating the translation performance of a large multilingual language model: the case of BLOOM.

</span>
<span class="ltx_bibblock">In Nurminen, M., Brenner, J., Koponen, M., Latomaa, S., Mikhailov, M., Schierl, F., Ranasinghe, T., Vanmassenhove, E., Vidal, S. A., Aranberri, N., Nunziatini, M., Escartín, C. P., Forcada, M., Popovic, M., Scarton, C., and Moniz, H., editors, <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</span>, pages 157–170, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blain et al.,  (2023)</span>
<span class="ltx_bibblock">
Blain, F., Zerva, C., Rei, R., Guerreiro, N. M., Kanojia, D., C. de Souza, J. G., Silva, B., Vaz, T., Jingxuan, Y., Azadi, F., Orasan, C., and Martins, A. (2023).

</span>
<span class="ltx_bibblock">Findings of the WMT 2023 shared task on quality estimation.

</span>
<span class="ltx_bibblock">In Koehn, P., Haddow, B., Kocmi, T., and Monz, C., editors, <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Eighth Conference on Machine Translation</span>, pages 629–653, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al.,  (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020).

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</span>, volume 33, pages 1877–1901. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinh and Niehues,  (2023)</span>
<span class="ltx_bibblock">
Dinh, T. A. and Niehues, J. (2023).

</span>
<span class="ltx_bibblock">Perturbation-based QE: An explainable, unsupervised word-level quality estimation method for blackbox machine translation.

</span>
<span class="ltx_bibblock">In Utiyama, M. and Wang, R., editors, <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track</span>, pages 59–71, Macau SAR, China. Asia-Pacific Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al.,  (2022)</span>
<span class="ltx_bibblock">
Goyal, N., Gao, C., Chaudhary, V., Chen, P.-J., Wenzek, G., Ju, D., Krishnan, S., Ranzato, M., Guzmán, F., and Fan, A. (2022).

</span>
<span class="ltx_bibblock">The Flores-101 evaluation benchmark for low-resource and multilingual machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Transactions of the Association for Computational Linguistics</span>, 10:522–538.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al.,  (2023)</span>
<span class="ltx_bibblock">
Goyal, T., Li, J. J., and Durrett, G. (2023).

</span>
<span class="ltx_bibblock">News summarization and evaluation in the era of gpt-3.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al.,  (2020)</span>
<span class="ltx_bibblock">
Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. (2020).

</span>
<span class="ltx_bibblock">How Can We Know What Language Models Know?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Transactions of the Association for Computational Linguistics</span>, 8:423–438.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et al.,  (2007)</span>
<span class="ltx_bibblock">
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007).

</span>
<span class="ltx_bibblock">Moses: Open source toolkit for statistical machine translation.

</span>
<span class="ltx_bibblock">In Ananiadou, S., editor, <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</span>, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn and Knowles,  (2017)</span>
<span class="ltx_bibblock">
Koehn, P. and Knowles, R. (2017).

</span>
<span class="ltx_bibblock">Six challenges for neural machine translation.

</span>
<span class="ltx_bibblock">In Luong, T., Birch, A., Neubig, G., and Finch, A., editors, <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Proceedings of the First Workshop on Neural Machine Translation</span>, pages 28–39, Vancouver. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolmogorov,  (1933)</span>
<span class="ltx_bibblock">
Kolmogorov, A. N. (1933).

</span>
<span class="ltx_bibblock">Sulla determinazione empirica di una legge di distribuzione.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Giornale dell’Istituto Italiano degli Attuari</span>, 4:83–91.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al.,  (2023)</span>
<span class="ltx_bibblock">
Kumar, A., Puduppully, R., Dabre, R., and Kunchukuttan, A. (2023).

</span>
<span class="ltx_bibblock">CTQScorer: Combining multiple features for in-context example selection for machine translation.

</span>
<span class="ltx_bibblock">In Bouamor, H., Pino, J., and Bali, K., editors, <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pages 7736–7752, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee,  (2020)</span>
<span class="ltx_bibblock">
Lee, D. (2020).

</span>
<span class="ltx_bibblock">Two-phase cross-lingual language model fine-tuning for machine translation quality estimation.

</span>
<span class="ltx_bibblock">In Barrault, L., Bojar, O., Bougares, F., Chatterjee, R., Costa-jussà, M. R., Federmann, C., Fishel, M., Fraser, A., Graham, Y., Guzman, P., Haddow, B., Huck, M., Yepes, A. J., Koehn, P., Martins, A., Morishita, M., Monz, C., Nagata, M., Nakazawa, T., and Negri, M., editors, <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the Fifth Conference on Machine Translation</span>, pages 1024–1028, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al.,  (2022)</span>
<span class="ltx_bibblock">
Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V., O’Horo, B., Wang, J., Zettlemoyer, L., Kozareva, Z., Diab, M., Stoyanov, V., and Li, X. (2022).

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In Goldberg, Y., Kozareva, Z., and Zhang, Y., editors, <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</span>, pages 9019–9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al.,  (2022)</span>
<span class="ltx_bibblock">
Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. (2022).

</span>
<span class="ltx_bibblock">What makes good in-context examples for GPT-3?

</span>
<span class="ltx_bibblock">In Agirre, E., Apidianaki, M., and Vulić, I., editors, <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</span>, pages 100–114, Dublin, Ireland and Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al.,  (2022)</span>
<span class="ltx_bibblock">
Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. (2022).

</span>
<span class="ltx_bibblock">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.

</span>
<span class="ltx_bibblock">In Muresan, S., Nakov, P., and Villavicencio, A., editors, <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 8086–8098, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al.,  (2023)</span>
<span class="ltx_bibblock">
Luo, M., Xu, X., Dai, Z., Pasupat, P., Kazemi, M., Baral, C., Imbrasaite, V., and Zhao, V. Y. (2023).

</span>
<span class="ltx_bibblock">Dr.icl: Demonstration-retrieved in-context learning.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murgolo et al.,  (2022)</span>
<span class="ltx_bibblock">
Murgolo, E., Sharami, J. P. R., and Shterionov, D. (2022).

</span>
<span class="ltx_bibblock">A quality estimation and quality evaluation tool for the translation industry.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 23rd Annual Conference of the European Association for Machine Translation</span>, pages 307–308, Ghent, Belgium. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Negri et al.,  (2018)</span>
<span class="ltx_bibblock">
Negri, M., Turchi, M., Chatterjee, R., and Bertoldi, N. (2018).

</span>
<span class="ltx_bibblock">ESCAPE: a large-scale synthetic corpus for automatic post-editing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</span>, Miyazaki, Japan. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov et al.,  (2023)</span>
<span class="ltx_bibblock">
Petrov, A., Malfa, E. L., Torr, P. H. S., and Bibi, A. (2023).

</span>
<span class="ltx_bibblock">Language model tokenizers introduce unfairness between languages.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et al.,  (2022)</span>
<span class="ltx_bibblock">
Pham, N.-Q., Nguyen, T. N., Nguyen, T.-B., Liu, D., Mullov, C., Niehues, J., and Waibel, A. (2022).

</span>
<span class="ltx_bibblock">Effective combination of pretrained models - KIT@IWSLT2022.

</span>
<span class="ltx_bibblock">In Salesky, E., Federico, M., and Costa-jussà, M., editors, <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)</span>, pages 190–197, Dublin, Ireland (in-person and online). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post,  (2018)</span>
<span class="ltx_bibblock">
Post, M. (2018).

</span>
<span class="ltx_bibblock">A call for clarity in reporting BLEU scores.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</span>, pages 186–191, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al.,  (2019)</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019).

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranasinghe et al.,  (2020)</span>
<span class="ltx_bibblock">
Ranasinghe, T., Orasan, C., and Mitkov, R. (2020).

</span>
<span class="ltx_bibblock">TransQuest: Translation quality estimation with cross-lingual transformers.

</span>
<span class="ltx_bibblock">In Scott, D., Bel, N., and Zong, C., editors, <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 28th International Conference on Computational Linguistics</span>, pages 5070–5081, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
Raunak, V., Menezes, A., and Awadalla, H. (2023a).

</span>
<span class="ltx_bibblock">Dissecting in-context learning of translations in GPT-3.

</span>
<span class="ltx_bibblock">In Bouamor, H., Pino, J., and Bali, K., editors, <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pages 866–872, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
Raunak, V., Menezes, A., Post, M., and Hassan, H. (2023b).

</span>
<span class="ltx_bibblock">Do GPTs produce less literal translations?

</span>
<span class="ltx_bibblock">In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</span>, pages 1041–1050, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al.,  (2020)</span>
<span class="ltx_bibblock">
Rei, R., Stewart, C., Farinha, A. C., and Lavie, A. (2020).

</span>
<span class="ltx_bibblock">COMET: A neural framework for MT evaluation.

</span>
<span class="ltx_bibblock">In Webber, B., Cohn, T., He, Y., and Liu, Y., editors, <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 2685–2702, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza,  (2009)</span>
<span class="ltx_bibblock">
Robertson, S. and Zaragoza, H. (2009).

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Found. Trends Inf. Retr.</span>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruder,  (2016)</span>
<span class="ltx_bibblock">
Ruder, S. (2016).

</span>
<span class="ltx_bibblock">On word embeddings - Part 2: Approximating the Softmax.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ruder.io/word-embeddings-softmax" title="">http://ruder.io/word-embeddings-softmax</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharami et al.,  (2023)</span>
<span class="ltx_bibblock">
Sharami, J. P. R., Shterionov, D., Blain, F., Vanmassenhove, E., Sisto, M. D., Emmery, C., and Spronck, P. (2023).

</span>
<span class="ltx_bibblock">Tailoring domain adaptation for machine translation quality estimation.

</span>
<span class="ltx_bibblock">In Nurminen, M., Brenner, J., Koponen, M., Latomaa, S., Mikhailov, M., Schierl, F., Ranasinghe, T., Vanmassenhove, E., Vidal, S. A., Aranberri, N., Nunziatini, M., Escartín, C. P., Forcada, M., Popovic, M., Scarton, C., and Moniz, H., editors, <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</span>, pages 9–20, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sia and Duh,  (2023)</span>
<span class="ltx_bibblock">
Sia, S. and Duh, K. (2023).

</span>
<span class="ltx_bibblock">In-context learning as maintaining coherency: A study of on-the-fly machine translation using large language models.

</span>
<span class="ltx_bibblock">In Utiyama, M. and Wang, R., editors, <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track</span>, pages 173–185, Macau SAR, China. Asia-Pacific Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Specia and Farzindar,  (2010)</span>
<span class="ltx_bibblock">
Specia, L. and Farzindar, A. (2010).

</span>
<span class="ltx_bibblock">Estimating machine translation post-editing effort with HTER.

</span>
<span class="ltx_bibblock">In Zhechev, V., editor, <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the Second Joint EM+/CNGL Workshop: Bringing MT to the User: Research on Integrating MT in the Translation Industry</span>, pages 33–43, Denver, Colorado, USA. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tamchyna,  (2021)</span>
<span class="ltx_bibblock">
Tamchyna, A. (2021).

</span>
<span class="ltx_bibblock">Deploying MT quality estimation on a large scale: Lessons learned and open questions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Proceedings of Machine Translation Summit XVIII: Users and Providers Track</span>, pages 291–305, Virtual. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al.,  (2020)</span>
<span class="ltx_bibblock">
Tang, Y., Tran, C., Li, X., Chen, P.-J., Goyal, N., Chaudhary, V., Gu, J., and Fan, A. (2020).

</span>
<span class="ltx_bibblock">Multilingual translation with extensible multilingual pretraining and finetuning.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann,  (2012)</span>
<span class="ltx_bibblock">
Tiedemann, J. (2012).

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in OPUS.

</span>
<span class="ltx_bibblock">In Calzolari, N., Choukri, K., Declerck, T., Doğan, M. U., Maegaard, B., Mariani, J., Moreno, A., Odijk, J., and Piperidis, S., editors, <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12)</span>, pages 2214–2218, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trotman et al.,  (2014)</span>
<span class="ltx_bibblock">
Trotman, A., Puurula, A., and Burgess, B. (2014).

</span>
<span class="ltx_bibblock">Improvements to bm25 and language models examined.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 19th Australasian Document Computing Symposium</span>, ADCS ’14, page 58–65, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar et al.,  (2023)</span>
<span class="ltx_bibblock">
Vilar, D., Freitag, M., Cherry, C., Luo, J., Ratnakar, V., and Foster, G. (2023).

</span>
<span class="ltx_bibblock">Prompting PaLM for translation: Assessing strategies and performance.

</span>
<span class="ltx_bibblock">In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 15406–15427, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al.,  (2024)</span>
<span class="ltx_bibblock">
Xu, H., Kim, Y. J., Sharaf, A., and Awadalla, H. H. (2024).

</span>
<span class="ltx_bibblock">A paradigm shift in machine translation: Boosting translation performance of large language models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye and Li,  (2023)</span>
<span class="ltx_bibblock">
Ye, N. and Li, J. (2023).

</span>
<span class="ltx_bibblock">A k-nearest neighbor approach for domain-specific translation quality estimation.

</span>
<span class="ltx_bibblock">In Feng, Y. and Feng, C., editors, <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Machine Translation</span>, pages 69–80, Singapore. Springer Nature Singapore.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al.,  (2022)</span>
<span class="ltx_bibblock">
Yuan, B., Li, Y., Chen, K., Lu, H., Yang, M., and Cao, H. (2022).

</span>
<span class="ltx_bibblock">An improved multi-task approach to pre-trained model based mt quality estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">CCMT</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zerva et al.,  (2022)</span>
<span class="ltx_bibblock">
Zerva, C., Blain, F., Rei, R., Lertvittayakumjorn, P., C. De Souza, J. G., Eger, S., Kanojia, D., Alves, D., Orăsan, C., Fomicheva, M., Martins, A. F. T., and Specia, L. (2022).

</span>
<span class="ltx_bibblock">Findings of the WMT 2022 shared task on quality estimation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</span>, pages 69–99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al.,  (2023)</span>
<span class="ltx_bibblock">
Zhu, W., Liu, H., Dong, Q., Xu, J., Huang, S., Kong, L., Chen, J., and Li, L. (2023).

</span>
<span class="ltx_bibblock">Multilingual machine translation with large language models: Empirical results and analysis.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendices</h2>
<figure class="ltx_figure" id="Ax1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="Ax1.F2.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="Ax1.F2.2.1">Overview illustration showing an iteration of our proposed methodology.</span> </figcaption>
</figure>
<figure class="ltx_table" id="Ax1.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ax1.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ax1.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Ax1.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Ax1.T3.1.1.1.1.1">Metric</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax1.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Ax1.T3.1.1.1.2.1">Generic Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax1.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Ax1.T3.1.1.1.3.1">Specific Model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ax1.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Ax1.T3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ax1.T3.1.2.1.1.1">Training Time (hh:mm)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax1.T3.1.2.1.2">05:55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax1.T3.1.2.1.3">06:54</td>
</tr>
<tr class="ltx_tr" id="Ax1.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Ax1.T3.1.3.2.1"><span class="ltx_text ltx_font_bold" id="Ax1.T3.1.3.2.1.1">CO2 Emissions (kg)</span></th>
<td class="ltx_td ltx_align_center" id="Ax1.T3.1.3.2.2">1.41</td>
<td class="ltx_td ltx_align_center" id="Ax1.T3.1.3.2.3">1.46</td>
</tr>
<tr class="ltx_tr" id="Ax1.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Ax1.T3.1.4.3.1"><span class="ltx_text ltx_font_bold" id="Ax1.T3.1.4.3.1.1">Electricity Consumption (kWh)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax1.T3.1.4.3.2">3.63</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax1.T3.1.4.3.3">3.76</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="Ax1.T3.3.1">Training Time, CO2 Emissions, and Electricity Consumption for QE Models.</span></figcaption>
</figure>
<figure class="ltx_table" id="Ax1.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Ax1.T4.4" style="width:235.8pt;height:201.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.1pt,11.2pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_align_middle" id="Ax1.T4.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ax1.T4.4.4.5.1">
<td class="ltx_td ltx_align_left ltx_border_tt" colspan="2" id="Ax1.T4.4.4.5.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="Ax1.T4.4.4.5.1.1.1">ICEs:</span></td>
</tr>
<tr class="ltx_tr" id="Ax1.T4.4.4.4">
<td class="ltx_td ltx_align_left" colspan="2" id="Ax1.T4.4.4.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_inline-block ltx_align_top" id="Ax1.T4.4.4.4.4.4">
<span class="ltx_p" id="Ax1.T4.4.4.4.4.4.4" style="width:426.8pt;">Die Sockets, die im except Array aufgelistet sind, werden auf Ausnahmen überwacht. = The sockets listed in the except array will be watched for exceptions. <math alttext="&lt;/s&gt;" class="ltx_math_unparsed" display="inline" id="Ax1.T4.1.1.1.1.1.1.m1.2"><semantics id="Ax1.T4.1.1.1.1.1.1.m1.2a"><mrow id="Ax1.T4.1.1.1.1.1.1.m1.2b"><mo id="Ax1.T4.1.1.1.1.1.1.m1.1.1" rspace="0em">&lt;</mo><mo id="Ax1.T4.1.1.1.1.1.1.m1.2.2" lspace="0em">/</mo><mi id="Ax1.T4.1.1.1.1.1.1.m1.2.3">s</mi><mo id="Ax1.T4.1.1.1.1.1.1.m1.2.4">&gt;</mo></mrow><annotation encoding="application/x-tex" id="Ax1.T4.1.1.1.1.1.1.m1.2c">&lt;/s&gt;</annotation><annotation encoding="application/x-llamapun" id="Ax1.T4.1.1.1.1.1.1.m1.2d">&lt; / italic_s &gt;</annotation></semantics></math> Geben Sie den Namen der Variablen ein, deren Wert überwacht werden soll. = Enter the name of the variable whose value is to be monitored. <math alttext="&lt;/s&gt;" class="ltx_math_unparsed" display="inline" id="Ax1.T4.2.2.2.2.2.2.m2.2"><semantics id="Ax1.T4.2.2.2.2.2.2.m2.2a"><mrow id="Ax1.T4.2.2.2.2.2.2.m2.2b"><mo id="Ax1.T4.2.2.2.2.2.2.m2.1.1" rspace="0em">&lt;</mo><mo id="Ax1.T4.2.2.2.2.2.2.m2.2.2" lspace="0em">/</mo><mi id="Ax1.T4.2.2.2.2.2.2.m2.2.3">s</mi><mo id="Ax1.T4.2.2.2.2.2.2.m2.2.4">&gt;</mo></mrow><annotation encoding="application/x-tex" id="Ax1.T4.2.2.2.2.2.2.m2.2c">&lt;/s&gt;</annotation><annotation encoding="application/x-llamapun" id="Ax1.T4.2.2.2.2.2.2.m2.2d">&lt; / italic_s &gt;</annotation></semantics></math> Nur erlaubt bei Sockets für lokale Displays und den globalen Socket. = Permitted only on sockets of local displays and the global socket. <math alttext="&lt;/s&gt;" class="ltx_math_unparsed" display="inline" id="Ax1.T4.3.3.3.3.3.3.m3.2"><semantics id="Ax1.T4.3.3.3.3.3.3.m3.2a"><mrow id="Ax1.T4.3.3.3.3.3.3.m3.2b"><mo id="Ax1.T4.3.3.3.3.3.3.m3.1.1" rspace="0em">&lt;</mo><mo id="Ax1.T4.3.3.3.3.3.3.m3.2.2" lspace="0em">/</mo><mi id="Ax1.T4.3.3.3.3.3.3.m3.2.3">s</mi><mo id="Ax1.T4.3.3.3.3.3.3.m3.2.4">&gt;</mo></mrow><annotation encoding="application/x-tex" id="Ax1.T4.3.3.3.3.3.3.m3.2c">&lt;/s&gt;</annotation><annotation encoding="application/x-llamapun" id="Ax1.T4.3.3.3.3.3.3.m3.2d">&lt; / italic_s &gt;</annotation></semantics></math> Legt fest, ob Scandaten-Information, die in den MPEG2-Videoströmen enthalten sind, aktualisiert werden sollen. = This controls whether to update the scan data information contained in the MPEG-2 video streams. <math alttext="&lt;/s&gt;" class="ltx_math_unparsed" display="inline" id="Ax1.T4.4.4.4.4.4.4.m4.2"><semantics id="Ax1.T4.4.4.4.4.4.4.m4.2a"><mrow id="Ax1.T4.4.4.4.4.4.4.m4.2b"><mo id="Ax1.T4.4.4.4.4.4.4.m4.1.1" rspace="0em">&lt;</mo><mo id="Ax1.T4.4.4.4.4.4.4.m4.2.2" lspace="0em">/</mo><mi id="Ax1.T4.4.4.4.4.4.4.m4.2.3">s</mi><mo id="Ax1.T4.4.4.4.4.4.4.m4.2.4">&gt;</mo></mrow><annotation encoding="application/x-tex" id="Ax1.T4.4.4.4.4.4.4.m4.2c">&lt;/s&gt;</annotation><annotation encoding="application/x-llamapun" id="Ax1.T4.4.4.4.4.4.4.m4.2d">&lt; / italic_s &gt;</annotation></semantics></math> Die Sockets, die im write Array aufgelistet sind, werden daraufhin überwacht, ob ein Schreibvorgang den Socket blockiert. =</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Ax1.T4.4.4.6.2">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="2" id="Ax1.T4.4.4.6.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="Ax1.T4.4.4.6.2.1.1">Translation:</span></td>
</tr>
<tr class="ltx_tr" id="Ax1.T4.4.4.7.3">
<td class="ltx_td ltx_align_left" colspan="2" id="Ax1.T4.4.4.7.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">The sockets listed in the write array will be watched for whether a write operation blocks the socket.</td>
</tr>
<tr class="ltx_tr" id="Ax1.T4.4.4.8.4">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="2" id="Ax1.T4.4.4.8.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="Ax1.T4.4.4.8.4.1.1">Reference Label:</span></td>
</tr>
<tr class="ltx_tr" id="Ax1.T4.4.4.9.5">
<td class="ltx_td ltx_align_left" colspan="2" id="Ax1.T4.4.4.9.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">The sockets listed in the write array will be watched to see if a write will not block.</td>
</tr>
<tr class="ltx_tr" id="Ax1.T4.4.4.10.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="Ax1.T4.4.4.10.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_text ltx_font_bold" id="Ax1.T4.4.4.10.6.1.1">QE:</span> 67.59, <span class="ltx_text ltx_font_bold" id="Ax1.T4.4.4.10.6.1.2">BLEU score (using reference label):</span> 52.89</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="Ax1.T4.4.4.10.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="Ax1.T4.6.1">An example of selected ICEs</span> for a source text, its corresponding translation, reference label, and QE estimation compared to the BLEU score computed based on the reference label.</figcaption>
</figure>
<figure class="ltx_figure" id="Ax1.fig1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_listing ltx_figure_panel ltx_listing" id="Ax1.fig1.1">
<div class="ltx_listingline" id="algx1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span><span class="ltx_text ltx_font_bold" id="algx1.l1.1">function</span> <span class="ltx_text ltx_font_smallcaps" id="algx1.l1.2">Search</span>(…)

</div>
<div class="ltx_listingline" id="algx1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span>     <math alttext='\text{temp}\leftarrow[(``",0.0,``")]' class="ltx_Math" display="inline" id="algx1.l2.m1.2"><semantics id="algx1.l2.m1.2a"><mrow id="algx1.l2.m1.2.2" xref="algx1.l2.m1.2.2.cmml"><mtext id="algx1.l2.m1.2.2.3" xref="algx1.l2.m1.2.2.3a.cmml">temp</mtext><mo id="algx1.l2.m1.2.2.2" stretchy="false" xref="algx1.l2.m1.2.2.2.cmml">←</mo><mrow id="algx1.l2.m1.2.2.1.1" xref="algx1.l2.m1.2.2.1.2.cmml"><mo id="algx1.l2.m1.2.2.1.1.2" stretchy="false" xref="algx1.l2.m1.2.2.1.2.1.cmml">[</mo><mrow id="algx1.l2.m1.2.2.1.1.1.2" xref="algx1.l2.m1.2.2.1.1.1.3.cmml"><mo id="algx1.l2.m1.2.2.1.1.1.2.3" stretchy="false" xref="algx1.l2.m1.2.2.1.1.1.3.cmml">(</mo><mrow id="algx1.l2.m1.2.2.1.1.1.1.1" xref="algx1.l2.m1.2.2.1.1.1.1.1.cmml"><mi id="algx1.l2.m1.2.2.1.1.1.1.1.2" mathvariant="normal" xref="algx1.l2.m1.2.2.1.1.1.1.1.2.cmml">`</mi><mo id="algx1.l2.m1.2.2.1.1.1.1.1.1" xref="algx1.l2.m1.2.2.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l2.m1.2.2.1.1.1.1.1.3" mathvariant="normal" xref="algx1.l2.m1.2.2.1.1.1.1.1.3.cmml">`</mi><mo id="algx1.l2.m1.2.2.1.1.1.1.1.1a" xref="algx1.l2.m1.2.2.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l2.m1.2.2.1.1.1.1.1.4" mathvariant="normal" xref="algx1.l2.m1.2.2.1.1.1.1.1.4.cmml">"</mi></mrow><mo id="algx1.l2.m1.2.2.1.1.1.2.4" xref="algx1.l2.m1.2.2.1.1.1.3.cmml">,</mo><mn id="algx1.l2.m1.1.1" xref="algx1.l2.m1.1.1.cmml">0.0</mn><mo id="algx1.l2.m1.2.2.1.1.1.2.5" xref="algx1.l2.m1.2.2.1.1.1.3.cmml">,</mo><mrow id="algx1.l2.m1.2.2.1.1.1.2.2" xref="algx1.l2.m1.2.2.1.1.1.2.2.cmml"><mi id="algx1.l2.m1.2.2.1.1.1.2.2.2" mathvariant="normal" xref="algx1.l2.m1.2.2.1.1.1.2.2.2.cmml">`</mi><mo id="algx1.l2.m1.2.2.1.1.1.2.2.1" xref="algx1.l2.m1.2.2.1.1.1.2.2.1.cmml">⁢</mo><mi id="algx1.l2.m1.2.2.1.1.1.2.2.3" mathvariant="normal" xref="algx1.l2.m1.2.2.1.1.1.2.2.3.cmml">`</mi><mo id="algx1.l2.m1.2.2.1.1.1.2.2.1a" xref="algx1.l2.m1.2.2.1.1.1.2.2.1.cmml">⁢</mo><mi id="algx1.l2.m1.2.2.1.1.1.2.2.4" mathvariant="normal" xref="algx1.l2.m1.2.2.1.1.1.2.2.4.cmml">"</mi></mrow><mo id="algx1.l2.m1.2.2.1.1.1.2.6" stretchy="false" xref="algx1.l2.m1.2.2.1.1.1.3.cmml">)</mo></mrow><mo id="algx1.l2.m1.2.2.1.1.3" stretchy="false" xref="algx1.l2.m1.2.2.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l2.m1.2b"><apply id="algx1.l2.m1.2.2.cmml" xref="algx1.l2.m1.2.2"><ci id="algx1.l2.m1.2.2.2.cmml" xref="algx1.l2.m1.2.2.2">←</ci><ci id="algx1.l2.m1.2.2.3a.cmml" xref="algx1.l2.m1.2.2.3"><mtext id="algx1.l2.m1.2.2.3.cmml" xref="algx1.l2.m1.2.2.3">temp</mtext></ci><apply id="algx1.l2.m1.2.2.1.2.cmml" xref="algx1.l2.m1.2.2.1.1"><csymbol cd="latexml" id="algx1.l2.m1.2.2.1.2.1.cmml" xref="algx1.l2.m1.2.2.1.1.2">delimited-[]</csymbol><vector id="algx1.l2.m1.2.2.1.1.1.3.cmml" xref="algx1.l2.m1.2.2.1.1.1.2"><apply id="algx1.l2.m1.2.2.1.1.1.1.1.cmml" xref="algx1.l2.m1.2.2.1.1.1.1.1"><times id="algx1.l2.m1.2.2.1.1.1.1.1.1.cmml" xref="algx1.l2.m1.2.2.1.1.1.1.1.1"></times><ci id="algx1.l2.m1.2.2.1.1.1.1.1.2.cmml" xref="algx1.l2.m1.2.2.1.1.1.1.1.2">`</ci><ci id="algx1.l2.m1.2.2.1.1.1.1.1.3.cmml" xref="algx1.l2.m1.2.2.1.1.1.1.1.3">`</ci><ci id="algx1.l2.m1.2.2.1.1.1.1.1.4.cmml" xref="algx1.l2.m1.2.2.1.1.1.1.1.4">"</ci></apply><cn id="algx1.l2.m1.1.1.cmml" type="float" xref="algx1.l2.m1.1.1">0.0</cn><apply id="algx1.l2.m1.2.2.1.1.1.2.2.cmml" xref="algx1.l2.m1.2.2.1.1.1.2.2"><times id="algx1.l2.m1.2.2.1.1.1.2.2.1.cmml" xref="algx1.l2.m1.2.2.1.1.1.2.2.1"></times><ci id="algx1.l2.m1.2.2.1.1.1.2.2.2.cmml" xref="algx1.l2.m1.2.2.1.1.1.2.2.2">`</ci><ci id="algx1.l2.m1.2.2.1.1.1.2.2.3.cmml" xref="algx1.l2.m1.2.2.1.1.1.2.2.3">`</ci><ci id="algx1.l2.m1.2.2.1.1.1.2.2.4.cmml" xref="algx1.l2.m1.2.2.1.1.1.2.2.4">"</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.m1.2c">\text{temp}\leftarrow[(``",0.0,``")]</annotation><annotation encoding="application/x-llamapun" id="algx1.l2.m1.2d">temp ← [ ( ` ` " , 0.0 , ` ` " ) ]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span>     <math alttext='\text{prompt}\leftarrow``"' class="ltx_Math" display="inline" id="algx1.l3.m1.1"><semantics id="algx1.l3.m1.1a"><mrow id="algx1.l3.m1.1.1" xref="algx1.l3.m1.1.1.cmml"><mtext id="algx1.l3.m1.1.1.2" xref="algx1.l3.m1.1.1.2a.cmml">prompt</mtext><mo id="algx1.l3.m1.1.1.1" stretchy="false" xref="algx1.l3.m1.1.1.1.cmml">←</mo><mrow id="algx1.l3.m1.1.1.3" xref="algx1.l3.m1.1.1.3.cmml"><mi id="algx1.l3.m1.1.1.3.2" mathvariant="normal" xref="algx1.l3.m1.1.1.3.2.cmml">`</mi><mo id="algx1.l3.m1.1.1.3.1" xref="algx1.l3.m1.1.1.3.1.cmml">⁢</mo><mi id="algx1.l3.m1.1.1.3.3" mathvariant="normal" xref="algx1.l3.m1.1.1.3.3.cmml">`</mi><mo id="algx1.l3.m1.1.1.3.1a" xref="algx1.l3.m1.1.1.3.1.cmml">⁢</mo><mi id="algx1.l3.m1.1.1.3.4" mathvariant="normal" xref="algx1.l3.m1.1.1.3.4.cmml">"</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l3.m1.1b"><apply id="algx1.l3.m1.1.1.cmml" xref="algx1.l3.m1.1.1"><ci id="algx1.l3.m1.1.1.1.cmml" xref="algx1.l3.m1.1.1.1">←</ci><ci id="algx1.l3.m1.1.1.2a.cmml" xref="algx1.l3.m1.1.1.2"><mtext id="algx1.l3.m1.1.1.2.cmml" xref="algx1.l3.m1.1.1.2">prompt</mtext></ci><apply id="algx1.l3.m1.1.1.3.cmml" xref="algx1.l3.m1.1.1.3"><times id="algx1.l3.m1.1.1.3.1.cmml" xref="algx1.l3.m1.1.1.3.1"></times><ci id="algx1.l3.m1.1.1.3.2.cmml" xref="algx1.l3.m1.1.1.3.2">`</ci><ci id="algx1.l3.m1.1.1.3.3.cmml" xref="algx1.l3.m1.1.1.3.3">`</ci><ci id="algx1.l3.m1.1.1.3.4.cmml" xref="algx1.l3.m1.1.1.3.4">"</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l3.m1.1c">\text{prompt}\leftarrow``"</annotation><annotation encoding="application/x-llamapun" id="algx1.l3.m1.1d">prompt ← ` ` "</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>     <math alttext="\text{itr}\leftarrow 0" class="ltx_Math" display="inline" id="algx1.l4.m1.1"><semantics id="algx1.l4.m1.1a"><mrow id="algx1.l4.m1.1.1" xref="algx1.l4.m1.1.1.cmml"><mtext id="algx1.l4.m1.1.1.2" xref="algx1.l4.m1.1.1.2a.cmml">itr</mtext><mo id="algx1.l4.m1.1.1.1" stretchy="false" xref="algx1.l4.m1.1.1.1.cmml">←</mo><mn id="algx1.l4.m1.1.1.3" xref="algx1.l4.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="algx1.l4.m1.1b"><apply id="algx1.l4.m1.1.1.cmml" xref="algx1.l4.m1.1.1"><ci id="algx1.l4.m1.1.1.1.cmml" xref="algx1.l4.m1.1.1.1">←</ci><ci id="algx1.l4.m1.1.1.2a.cmml" xref="algx1.l4.m1.1.1.2"><mtext id="algx1.l4.m1.1.1.2.cmml" xref="algx1.l4.m1.1.1.2">itr</mtext></ci><cn id="algx1.l4.m1.1.1.3.cmml" type="integer" xref="algx1.l4.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l4.m1.1c">\text{itr}\leftarrow 0</annotation><annotation encoding="application/x-llamapun" id="algx1.l4.m1.1d">itr ← 0</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>     <math alttext="\text{best\_qe\_score}\leftarrow 0.0" class="ltx_Math" display="inline" id="algx1.l5.m1.1"><semantics id="algx1.l5.m1.1a"><mrow id="algx1.l5.m1.1.1" xref="algx1.l5.m1.1.1.cmml"><mtext id="algx1.l5.m1.1.1.2" xref="algx1.l5.m1.1.1.2a.cmml">best_qe_score</mtext><mo id="algx1.l5.m1.1.1.1" stretchy="false" xref="algx1.l5.m1.1.1.1.cmml">←</mo><mn id="algx1.l5.m1.1.1.3" xref="algx1.l5.m1.1.1.3.cmml">0.0</mn></mrow><annotation-xml encoding="MathML-Content" id="algx1.l5.m1.1b"><apply id="algx1.l5.m1.1.1.cmml" xref="algx1.l5.m1.1.1"><ci id="algx1.l5.m1.1.1.1.cmml" xref="algx1.l5.m1.1.1.1">←</ci><ci id="algx1.l5.m1.1.1.2a.cmml" xref="algx1.l5.m1.1.1.2"><mtext id="algx1.l5.m1.1.1.2.cmml" xref="algx1.l5.m1.1.1.2">best_qe_score</mtext></ci><cn id="algx1.l5.m1.1.1.3.cmml" type="float" xref="algx1.l5.m1.1.1.3">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l5.m1.1c">\text{best\_qe\_score}\leftarrow 0.0</annotation><annotation encoding="application/x-llamapun" id="algx1.l5.m1.1d">best_qe_score ← 0.0</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>     <math alttext="\text{patience\_counter}\leftarrow 0" class="ltx_Math" display="inline" id="algx1.l6.m1.1"><semantics id="algx1.l6.m1.1a"><mrow id="algx1.l6.m1.1.1" xref="algx1.l6.m1.1.1.cmml"><mtext id="algx1.l6.m1.1.1.2" xref="algx1.l6.m1.1.1.2a.cmml">patience_counter</mtext><mo id="algx1.l6.m1.1.1.1" stretchy="false" xref="algx1.l6.m1.1.1.1.cmml">←</mo><mn id="algx1.l6.m1.1.1.3" xref="algx1.l6.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="algx1.l6.m1.1b"><apply id="algx1.l6.m1.1.1.cmml" xref="algx1.l6.m1.1.1"><ci id="algx1.l6.m1.1.1.1.cmml" xref="algx1.l6.m1.1.1.1">←</ci><ci id="algx1.l6.m1.1.1.2a.cmml" xref="algx1.l6.m1.1.1.2"><mtext id="algx1.l6.m1.1.1.2.cmml" xref="algx1.l6.m1.1.1.2">patience_counter</mtext></ci><cn id="algx1.l6.m1.1.1.3.cmml" type="integer" xref="algx1.l6.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l6.m1.1c">\text{patience\_counter}\leftarrow 0</annotation><annotation encoding="application/x-llamapun" id="algx1.l6.m1.1d">patience_counter ← 0</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>     <span class="ltx_text ltx_font_bold" id="algx1.l7.1">while</span> <math alttext="\text{itr}&lt;\text{iteration}\text{ and }\text{patience\_counter}&lt;\text{early\_%
stop\_patience}" class="ltx_Math" display="inline" id="algx1.l7.m1.1"><semantics id="algx1.l7.m1.1a"><mrow id="algx1.l7.m1.1.1" xref="algx1.l7.m1.1.1.cmml"><mtext id="algx1.l7.m1.1.1.2" xref="algx1.l7.m1.1.1.2a.cmml">itr</mtext><mo id="algx1.l7.m1.1.1.3" xref="algx1.l7.m1.1.1.3.cmml">&lt;</mo><mrow id="algx1.l7.m1.1.1.4" xref="algx1.l7.m1.1.1.4d.cmml"><mtext id="algx1.l7.m1.1.1.4a" xref="algx1.l7.m1.1.1.4d.cmml">iteration</mtext><mtext id="algx1.l7.m1.1.1.4b" xref="algx1.l7.m1.1.1.4d.cmml"> and </mtext><mtext id="algx1.l7.m1.1.1.4c" xref="algx1.l7.m1.1.1.4d.cmml">patience_counter</mtext></mrow><mo id="algx1.l7.m1.1.1.5" xref="algx1.l7.m1.1.1.5.cmml">&lt;</mo><mtext id="algx1.l7.m1.1.1.6" xref="algx1.l7.m1.1.1.6a.cmml">early_stop_patience</mtext></mrow><annotation-xml encoding="MathML-Content" id="algx1.l7.m1.1b"><apply id="algx1.l7.m1.1.1.cmml" xref="algx1.l7.m1.1.1"><and id="algx1.l7.m1.1.1a.cmml" xref="algx1.l7.m1.1.1"></and><apply id="algx1.l7.m1.1.1b.cmml" xref="algx1.l7.m1.1.1"><lt id="algx1.l7.m1.1.1.3.cmml" xref="algx1.l7.m1.1.1.3"></lt><ci id="algx1.l7.m1.1.1.2a.cmml" xref="algx1.l7.m1.1.1.2"><mtext id="algx1.l7.m1.1.1.2.cmml" xref="algx1.l7.m1.1.1.2">itr</mtext></ci><ci id="algx1.l7.m1.1.1.4d.cmml" xref="algx1.l7.m1.1.1.4"><mrow id="algx1.l7.m1.1.1.4.cmml" xref="algx1.l7.m1.1.1.4"><mtext id="algx1.l7.m1.1.1.4a.cmml" xref="algx1.l7.m1.1.1.4">iteration</mtext><mtext id="algx1.l7.m1.1.1.4b.cmml" xref="algx1.l7.m1.1.1.4"> and </mtext><mtext id="algx1.l7.m1.1.1.4c.cmml" xref="algx1.l7.m1.1.1.4">patience_counter</mtext></mrow></ci></apply><apply id="algx1.l7.m1.1.1c.cmml" xref="algx1.l7.m1.1.1"><lt id="algx1.l7.m1.1.1.5.cmml" xref="algx1.l7.m1.1.1.5"></lt><share href="https://arxiv.org/html/2406.07970v3#algx1.l7.m1.1.1.4.cmml" id="algx1.l7.m1.1.1d.cmml" xref="algx1.l7.m1.1.1"></share><ci id="algx1.l7.m1.1.1.6a.cmml" xref="algx1.l7.m1.1.1.6"><mtext id="algx1.l7.m1.1.1.6.cmml" xref="algx1.l7.m1.1.1.6">early_stop_patience</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l7.m1.1c">\text{itr}&lt;\text{iteration}\text{ and }\text{patience\_counter}&lt;\text{early\_%
stop\_patience}</annotation><annotation encoding="application/x-llamapun" id="algx1.l7.m1.1d">itr &lt; roman_iteration and patience_counter &lt; early_stop_patience</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="algx1.l7.2">do</span>
</div>
<div class="ltx_listingline" id="algx1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>         <math alttext="\text{available\_Prompts}\leftarrow\textsc{GenerateAvailablePrompts}(...)" class="ltx_Math" display="inline" id="algx1.l8.m1.1"><semantics id="algx1.l8.m1.1a"><mrow id="algx1.l8.m1.1.2" xref="algx1.l8.m1.1.2.cmml"><mtext id="algx1.l8.m1.1.2.2" xref="algx1.l8.m1.1.2.2a.cmml">available_Prompts</mtext><mo id="algx1.l8.m1.1.2.1" stretchy="false" xref="algx1.l8.m1.1.2.1.cmml">←</mo><mrow id="algx1.l8.m1.1.2.3" xref="algx1.l8.m1.1.2.3.cmml"><mtext class="ltx_font_smallcaps" id="algx1.l8.m1.1.2.3.2" xref="algx1.l8.m1.1.2.3.2a.cmml">GenerateAvailablePrompts</mtext><mo id="algx1.l8.m1.1.2.3.1" xref="algx1.l8.m1.1.2.3.1.cmml">⁢</mo><mrow id="algx1.l8.m1.1.2.3.3.2" xref="algx1.l8.m1.1.2.3.cmml"><mo id="algx1.l8.m1.1.2.3.3.2.1" stretchy="false" xref="algx1.l8.m1.1.2.3.cmml">(</mo><mi id="algx1.l8.m1.1.1" mathvariant="normal" xref="algx1.l8.m1.1.1.cmml">…</mi><mo id="algx1.l8.m1.1.2.3.3.2.2" stretchy="false" xref="algx1.l8.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l8.m1.1b"><apply id="algx1.l8.m1.1.2.cmml" xref="algx1.l8.m1.1.2"><ci id="algx1.l8.m1.1.2.1.cmml" xref="algx1.l8.m1.1.2.1">←</ci><ci id="algx1.l8.m1.1.2.2a.cmml" xref="algx1.l8.m1.1.2.2"><mtext id="algx1.l8.m1.1.2.2.cmml" xref="algx1.l8.m1.1.2.2">available_Prompts</mtext></ci><apply id="algx1.l8.m1.1.2.3.cmml" xref="algx1.l8.m1.1.2.3"><times id="algx1.l8.m1.1.2.3.1.cmml" xref="algx1.l8.m1.1.2.3.1"></times><ci id="algx1.l8.m1.1.2.3.2a.cmml" xref="algx1.l8.m1.1.2.3.2"><mtext class="ltx_font_smallcaps" id="algx1.l8.m1.1.2.3.2.cmml" xref="algx1.l8.m1.1.2.3.2">GenerateAvailablePrompts</mtext></ci><ci id="algx1.l8.m1.1.1.cmml" xref="algx1.l8.m1.1.1">…</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l8.m1.1c">\text{available\_Prompts}\leftarrow\textsc{GenerateAvailablePrompts}(...)</annotation><annotation encoding="application/x-llamapun" id="algx1.l8.m1.1d">available_Prompts ← GenerateAvailablePrompts ( … )</annotation></semantics></math> <span class="ltx_text" id="algx1.l8.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx1.l8.1.m1.1"><semantics id="algx1.l8.1.m1.1a"><mo id="algx1.l8.1.m1.1.1" xref="algx1.l8.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx1.l8.1.m1.1b"><ci id="algx1.l8.1.m1.1.1.cmml" xref="algx1.l8.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l8.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx1.l8.1.m1.1d">▷</annotation></semantics></math> Initial ICEs
</span>
</div>
<div class="ltx_listingline" id="algx1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>         <span class="ltx_text ltx_font_bold" id="algx1.l9.1">if</span> <span class="ltx_text ltx_markedasmath" id="algx1.l9.2">available_prompts</span> is not empty <span class="ltx_text ltx_font_bold" id="algx1.l9.3">then</span>
</div>
<div class="ltx_listingline" id="algx1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>              <math alttext="\text{selected\_prompt\_index}\leftarrow\text{itr}\bmod k" class="ltx_Math" display="inline" id="algx1.l10.m1.1"><semantics id="algx1.l10.m1.1a"><mrow id="algx1.l10.m1.1.1" xref="algx1.l10.m1.1.1.cmml"><mtext id="algx1.l10.m1.1.1.2" xref="algx1.l10.m1.1.1.2a.cmml">selected_prompt_index</mtext><mo id="algx1.l10.m1.1.1.1" stretchy="false" xref="algx1.l10.m1.1.1.1.cmml">←</mo><mrow id="algx1.l10.m1.1.1.3" xref="algx1.l10.m1.1.1.3.cmml"><mtext id="algx1.l10.m1.1.1.3.2" xref="algx1.l10.m1.1.1.3.2a.cmml">itr</mtext><mo id="algx1.l10.m1.1.1.3.1" xref="algx1.l10.m1.1.1.3.1.cmml">mod</mo><mi id="algx1.l10.m1.1.1.3.3" xref="algx1.l10.m1.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l10.m1.1b"><apply id="algx1.l10.m1.1.1.cmml" xref="algx1.l10.m1.1.1"><ci id="algx1.l10.m1.1.1.1.cmml" xref="algx1.l10.m1.1.1.1">←</ci><ci id="algx1.l10.m1.1.1.2a.cmml" xref="algx1.l10.m1.1.1.2"><mtext id="algx1.l10.m1.1.1.2.cmml" xref="algx1.l10.m1.1.1.2">selected_prompt_index</mtext></ci><apply id="algx1.l10.m1.1.1.3.cmml" xref="algx1.l10.m1.1.1.3"><csymbol cd="latexml" id="algx1.l10.m1.1.1.3.1.cmml" xref="algx1.l10.m1.1.1.3.1">modulo</csymbol><ci id="algx1.l10.m1.1.1.3.2a.cmml" xref="algx1.l10.m1.1.1.3.2"><mtext id="algx1.l10.m1.1.1.3.2.cmml" xref="algx1.l10.m1.1.1.3.2">itr</mtext></ci><ci id="algx1.l10.m1.1.1.3.3.cmml" xref="algx1.l10.m1.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l10.m1.1c">\text{selected\_prompt\_index}\leftarrow\text{itr}\bmod k</annotation><annotation encoding="application/x-llamapun" id="algx1.l10.m1.1d">selected_prompt_index ← itr roman_mod italic_k</annotation></semantics></math> <span class="ltx_text" id="algx1.l10.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx1.l10.1.m1.1"><semantics id="algx1.l10.1.m1.1a"><mo id="algx1.l10.1.m1.1.1" xref="algx1.l10.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx1.l10.1.m1.1b"><ci id="algx1.l10.1.m1.1.1.cmml" xref="algx1.l10.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l10.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx1.l10.1.m1.1d">▷</annotation></semantics></math> Phase 1: Selection
</span>
</div>
<div class="ltx_listingline" id="algx1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>              <math alttext="\text{selected\_prompt}\leftarrow\text{available\_prompts}[\text{selected\_%
prompt\_index}]" class="ltx_Math" display="inline" id="algx1.l11.m1.1"><semantics id="algx1.l11.m1.1a"><mrow id="algx1.l11.m1.1.2" xref="algx1.l11.m1.1.2.cmml"><mtext id="algx1.l11.m1.1.2.2" xref="algx1.l11.m1.1.2.2a.cmml">selected_prompt</mtext><mo id="algx1.l11.m1.1.2.1" stretchy="false" xref="algx1.l11.m1.1.2.1.cmml">←</mo><mrow id="algx1.l11.m1.1.2.3" xref="algx1.l11.m1.1.2.3.cmml"><mtext id="algx1.l11.m1.1.2.3.2" xref="algx1.l11.m1.1.2.3.2a.cmml">available_prompts</mtext><mo id="algx1.l11.m1.1.2.3.1" xref="algx1.l11.m1.1.2.3.1.cmml">⁢</mo><mrow id="algx1.l11.m1.1.2.3.3.2" xref="algx1.l11.m1.1.2.3.3.1.cmml"><mo id="algx1.l11.m1.1.2.3.3.2.1" stretchy="false" xref="algx1.l11.m1.1.2.3.3.1.1.cmml">[</mo><mtext id="algx1.l11.m1.1.1" xref="algx1.l11.m1.1.1a.cmml">selected_prompt_index</mtext><mo id="algx1.l11.m1.1.2.3.3.2.2" stretchy="false" xref="algx1.l11.m1.1.2.3.3.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l11.m1.1b"><apply id="algx1.l11.m1.1.2.cmml" xref="algx1.l11.m1.1.2"><ci id="algx1.l11.m1.1.2.1.cmml" xref="algx1.l11.m1.1.2.1">←</ci><ci id="algx1.l11.m1.1.2.2a.cmml" xref="algx1.l11.m1.1.2.2"><mtext id="algx1.l11.m1.1.2.2.cmml" xref="algx1.l11.m1.1.2.2">selected_prompt</mtext></ci><apply id="algx1.l11.m1.1.2.3.cmml" xref="algx1.l11.m1.1.2.3"><times id="algx1.l11.m1.1.2.3.1.cmml" xref="algx1.l11.m1.1.2.3.1"></times><ci id="algx1.l11.m1.1.2.3.2a.cmml" xref="algx1.l11.m1.1.2.3.2"><mtext id="algx1.l11.m1.1.2.3.2.cmml" xref="algx1.l11.m1.1.2.3.2">available_prompts</mtext></ci><apply id="algx1.l11.m1.1.2.3.3.1.cmml" xref="algx1.l11.m1.1.2.3.3.2"><csymbol cd="latexml" id="algx1.l11.m1.1.2.3.3.1.1.cmml" xref="algx1.l11.m1.1.2.3.3.2.1">delimited-[]</csymbol><ci id="algx1.l11.m1.1.1a.cmml" xref="algx1.l11.m1.1.1"><mtext id="algx1.l11.m1.1.1.cmml" xref="algx1.l11.m1.1.1">selected_prompt_index</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l11.m1.1c">\text{selected\_prompt}\leftarrow\text{available\_prompts}[\text{selected\_%
prompt\_index}]</annotation><annotation encoding="application/x-llamapun" id="algx1.l11.m1.1d">selected_prompt ← available_prompts [ selected_prompt_index ]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>              <math alttext="\text{prompt}\leftarrow\textsc{ConstructFullPrompt}(...)(see~{}\ref{LLM})" class="ltx_Math" display="inline" id="algx1.l12.m1.2"><semantics id="algx1.l12.m1.2a"><mrow id="algx1.l12.m1.2.2" xref="algx1.l12.m1.2.2.cmml"><mtext id="algx1.l12.m1.2.2.3" xref="algx1.l12.m1.2.2.3a.cmml">prompt</mtext><mo id="algx1.l12.m1.2.2.2" stretchy="false" xref="algx1.l12.m1.2.2.2.cmml">←</mo><mrow id="algx1.l12.m1.2.2.1" xref="algx1.l12.m1.2.2.1.cmml"><mtext class="ltx_font_smallcaps" id="algx1.l12.m1.2.2.1.3" xref="algx1.l12.m1.2.2.1.3a.cmml">ConstructFullPrompt</mtext><mo id="algx1.l12.m1.2.2.1.2" xref="algx1.l12.m1.2.2.1.2.cmml">⁢</mo><mrow id="algx1.l12.m1.2.2.1.4.2" xref="algx1.l12.m1.2.2.1.cmml"><mo id="algx1.l12.m1.2.2.1.4.2.1" stretchy="false" xref="algx1.l12.m1.2.2.1.cmml">(</mo><mi id="algx1.l12.m1.1.1" mathvariant="normal" xref="algx1.l12.m1.1.1.cmml">…</mi><mo id="algx1.l12.m1.2.2.1.4.2.2" stretchy="false" xref="algx1.l12.m1.2.2.1.cmml">)</mo></mrow><mo id="algx1.l12.m1.2.2.1.2a" xref="algx1.l12.m1.2.2.1.2.cmml">⁢</mo><mrow id="algx1.l12.m1.2.2.1.1.1" xref="algx1.l12.m1.2.2.1.1.1.1.cmml"><mo id="algx1.l12.m1.2.2.1.1.1.2" stretchy="false" xref="algx1.l12.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="algx1.l12.m1.2.2.1.1.1.1" xref="algx1.l12.m1.2.2.1.1.1.1.cmml"><mi id="algx1.l12.m1.2.2.1.1.1.1.2" xref="algx1.l12.m1.2.2.1.1.1.1.2.cmml">s</mi><mo id="algx1.l12.m1.2.2.1.1.1.1.1" xref="algx1.l12.m1.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l12.m1.2.2.1.1.1.1.3" xref="algx1.l12.m1.2.2.1.1.1.1.3.cmml">e</mi><mo id="algx1.l12.m1.2.2.1.1.1.1.1a" xref="algx1.l12.m1.2.2.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l12.m1.2.2.1.1.1.1.4" xref="algx1.l12.m1.2.2.1.1.1.1.4.cmml">e</mi><mo id="algx1.l12.m1.2.2.1.1.1.1.1b" lspace="0.330em" xref="algx1.l12.m1.2.2.1.1.1.1.1.cmml">⁢</mo><mtext class="ltx_mathvariant_italic" id="algx1.l12.m1.2.2.1.1.1.1.5" xref="algx1.l12.m1.2.2.1.1.1.1.5c.cmml"><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2406.07970v3#S3.SS3" title="3.3 Multilingual Large Language Model ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a></mtext></mrow><mo id="algx1.l12.m1.2.2.1.1.1.3" stretchy="false" xref="algx1.l12.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l12.m1.2b"><apply id="algx1.l12.m1.2.2.cmml" xref="algx1.l12.m1.2.2"><ci id="algx1.l12.m1.2.2.2.cmml" xref="algx1.l12.m1.2.2.2">←</ci><ci id="algx1.l12.m1.2.2.3a.cmml" xref="algx1.l12.m1.2.2.3"><mtext id="algx1.l12.m1.2.2.3.cmml" xref="algx1.l12.m1.2.2.3">prompt</mtext></ci><apply id="algx1.l12.m1.2.2.1.cmml" xref="algx1.l12.m1.2.2.1"><times id="algx1.l12.m1.2.2.1.2.cmml" xref="algx1.l12.m1.2.2.1.2"></times><ci id="algx1.l12.m1.2.2.1.3a.cmml" xref="algx1.l12.m1.2.2.1.3"><mtext class="ltx_font_smallcaps" id="algx1.l12.m1.2.2.1.3.cmml" xref="algx1.l12.m1.2.2.1.3">ConstructFullPrompt</mtext></ci><ci id="algx1.l12.m1.1.1.cmml" xref="algx1.l12.m1.1.1">…</ci><apply id="algx1.l12.m1.2.2.1.1.1.1.cmml" xref="algx1.l12.m1.2.2.1.1.1"><times id="algx1.l12.m1.2.2.1.1.1.1.1.cmml" xref="algx1.l12.m1.2.2.1.1.1.1.1"></times><ci id="algx1.l12.m1.2.2.1.1.1.1.2.cmml" xref="algx1.l12.m1.2.2.1.1.1.1.2">𝑠</ci><ci id="algx1.l12.m1.2.2.1.1.1.1.3.cmml" xref="algx1.l12.m1.2.2.1.1.1.1.3">𝑒</ci><ci id="algx1.l12.m1.2.2.1.1.1.1.4.cmml" xref="algx1.l12.m1.2.2.1.1.1.1.4">𝑒</ci><ci id="algx1.l12.m1.2.2.1.1.1.1.5c.cmml" xref="algx1.l12.m1.2.2.1.1.1.1.5"><mtext class="ltx_mathvariant_italic" id="algx1.l12.m1.2.2.1.1.1.1.5.cmml" xref="algx1.l12.m1.2.2.1.1.1.1.5"><a class="ltx_ref ltx_font_italic" href="https://arxiv.org/html/2406.07970v3#S3.SS3" title="3.3 Multilingual Large Language Model ‣ 3 Experiments Setup ‣ Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a></mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l12.m1.2c">\text{prompt}\leftarrow\textsc{ConstructFullPrompt}(...)(see~{}\ref{LLM})</annotation><annotation encoding="application/x-llamapun" id="algx1.l12.m1.2d">prompt ← ConstructFullPrompt ( … ) ( italic_s italic_e italic_e )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span>              <math alttext="\text{input\_ids[0]}\leftarrow\textsc{EncodePrompt}(...)" class="ltx_Math" display="inline" id="algx1.l13.m1.1"><semantics id="algx1.l13.m1.1a"><mrow id="algx1.l13.m1.1.2" xref="algx1.l13.m1.1.2.cmml"><mtext id="algx1.l13.m1.1.2.2" xref="algx1.l13.m1.1.2.2a.cmml">input_ids[0]</mtext><mo id="algx1.l13.m1.1.2.1" stretchy="false" xref="algx1.l13.m1.1.2.1.cmml">←</mo><mrow id="algx1.l13.m1.1.2.3" xref="algx1.l13.m1.1.2.3.cmml"><mtext class="ltx_font_smallcaps" id="algx1.l13.m1.1.2.3.2" xref="algx1.l13.m1.1.2.3.2a.cmml">EncodePrompt</mtext><mo id="algx1.l13.m1.1.2.3.1" xref="algx1.l13.m1.1.2.3.1.cmml">⁢</mo><mrow id="algx1.l13.m1.1.2.3.3.2" xref="algx1.l13.m1.1.2.3.cmml"><mo id="algx1.l13.m1.1.2.3.3.2.1" stretchy="false" xref="algx1.l13.m1.1.2.3.cmml">(</mo><mi id="algx1.l13.m1.1.1" mathvariant="normal" xref="algx1.l13.m1.1.1.cmml">…</mi><mo id="algx1.l13.m1.1.2.3.3.2.2" stretchy="false" xref="algx1.l13.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l13.m1.1b"><apply id="algx1.l13.m1.1.2.cmml" xref="algx1.l13.m1.1.2"><ci id="algx1.l13.m1.1.2.1.cmml" xref="algx1.l13.m1.1.2.1">←</ci><ci id="algx1.l13.m1.1.2.2a.cmml" xref="algx1.l13.m1.1.2.2"><mtext id="algx1.l13.m1.1.2.2.cmml" xref="algx1.l13.m1.1.2.2">input_ids[0]</mtext></ci><apply id="algx1.l13.m1.1.2.3.cmml" xref="algx1.l13.m1.1.2.3"><times id="algx1.l13.m1.1.2.3.1.cmml" xref="algx1.l13.m1.1.2.3.1"></times><ci id="algx1.l13.m1.1.2.3.2a.cmml" xref="algx1.l13.m1.1.2.3.2"><mtext class="ltx_font_smallcaps" id="algx1.l13.m1.1.2.3.2.cmml" xref="algx1.l13.m1.1.2.3.2">EncodePrompt</mtext></ci><ci id="algx1.l13.m1.1.1.cmml" xref="algx1.l13.m1.1.1">…</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l13.m1.1c">\text{input\_ids[0]}\leftarrow\textsc{EncodePrompt}(...)</annotation><annotation encoding="application/x-llamapun" id="algx1.l13.m1.1d">input_ids[0] ← EncodePrompt ( … )</annotation></semantics></math> <span class="ltx_text" id="algx1.l13.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx1.l13.1.m1.1"><semantics id="algx1.l13.1.m1.1a"><mo id="algx1.l13.1.m1.1.1" xref="algx1.l13.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx1.l13.1.m1.1b"><ci id="algx1.l13.1.m1.1.1.cmml" xref="algx1.l13.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l13.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx1.l13.1.m1.1d">▷</annotation></semantics></math> Phase 2: Translation
</span>
</div>
<div class="ltx_listingline" id="algx1.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>              <span class="ltx_text ltx_font_bold" id="algx1.l14.1">if</span> <math alttext="\text{length}(input\_ids)&gt;\text{LLM\_max\_length}" class="ltx_Math" display="inline" id="algx1.l14.m1.1"><semantics id="algx1.l14.m1.1a"><mrow id="algx1.l14.m1.1.1" xref="algx1.l14.m1.1.1.cmml"><mrow id="algx1.l14.m1.1.1.1" xref="algx1.l14.m1.1.1.1.cmml"><mtext id="algx1.l14.m1.1.1.1.3" xref="algx1.l14.m1.1.1.1.3a.cmml">length</mtext><mo id="algx1.l14.m1.1.1.1.2" xref="algx1.l14.m1.1.1.1.2.cmml">⁢</mo><mrow id="algx1.l14.m1.1.1.1.1.1" xref="algx1.l14.m1.1.1.1.1.1.1.cmml"><mo id="algx1.l14.m1.1.1.1.1.1.2" stretchy="false" xref="algx1.l14.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="algx1.l14.m1.1.1.1.1.1.1" xref="algx1.l14.m1.1.1.1.1.1.1.cmml"><mi id="algx1.l14.m1.1.1.1.1.1.1.2" xref="algx1.l14.m1.1.1.1.1.1.1.2.cmml">i</mi><mo id="algx1.l14.m1.1.1.1.1.1.1.1" xref="algx1.l14.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l14.m1.1.1.1.1.1.1.3" xref="algx1.l14.m1.1.1.1.1.1.1.3.cmml">n</mi><mo id="algx1.l14.m1.1.1.1.1.1.1.1a" xref="algx1.l14.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l14.m1.1.1.1.1.1.1.4" xref="algx1.l14.m1.1.1.1.1.1.1.4.cmml">p</mi><mo id="algx1.l14.m1.1.1.1.1.1.1.1b" xref="algx1.l14.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l14.m1.1.1.1.1.1.1.5" xref="algx1.l14.m1.1.1.1.1.1.1.5.cmml">u</mi><mo id="algx1.l14.m1.1.1.1.1.1.1.1c" xref="algx1.l14.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l14.m1.1.1.1.1.1.1.6" xref="algx1.l14.m1.1.1.1.1.1.1.6.cmml">t</mi><mo id="algx1.l14.m1.1.1.1.1.1.1.1d" xref="algx1.l14.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l14.m1.1.1.1.1.1.1.7" mathvariant="normal" xref="algx1.l14.m1.1.1.1.1.1.1.7.cmml">_</mi><mo id="algx1.l14.m1.1.1.1.1.1.1.1e" xref="algx1.l14.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l14.m1.1.1.1.1.1.1.8" xref="algx1.l14.m1.1.1.1.1.1.1.8.cmml">i</mi><mo id="algx1.l14.m1.1.1.1.1.1.1.1f" xref="algx1.l14.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l14.m1.1.1.1.1.1.1.9" xref="algx1.l14.m1.1.1.1.1.1.1.9.cmml">d</mi><mo id="algx1.l14.m1.1.1.1.1.1.1.1g" xref="algx1.l14.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="algx1.l14.m1.1.1.1.1.1.1.10" xref="algx1.l14.m1.1.1.1.1.1.1.10.cmml">s</mi></mrow><mo id="algx1.l14.m1.1.1.1.1.1.3" stretchy="false" xref="algx1.l14.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="algx1.l14.m1.1.1.2" xref="algx1.l14.m1.1.1.2.cmml">&gt;</mo><mtext id="algx1.l14.m1.1.1.3" xref="algx1.l14.m1.1.1.3a.cmml">LLM_max_length</mtext></mrow><annotation-xml encoding="MathML-Content" id="algx1.l14.m1.1b"><apply id="algx1.l14.m1.1.1.cmml" xref="algx1.l14.m1.1.1"><gt id="algx1.l14.m1.1.1.2.cmml" xref="algx1.l14.m1.1.1.2"></gt><apply id="algx1.l14.m1.1.1.1.cmml" xref="algx1.l14.m1.1.1.1"><times id="algx1.l14.m1.1.1.1.2.cmml" xref="algx1.l14.m1.1.1.1.2"></times><ci id="algx1.l14.m1.1.1.1.3a.cmml" xref="algx1.l14.m1.1.1.1.3"><mtext id="algx1.l14.m1.1.1.1.3.cmml" xref="algx1.l14.m1.1.1.1.3">length</mtext></ci><apply id="algx1.l14.m1.1.1.1.1.1.1.cmml" xref="algx1.l14.m1.1.1.1.1.1"><times id="algx1.l14.m1.1.1.1.1.1.1.1.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.1"></times><ci id="algx1.l14.m1.1.1.1.1.1.1.2.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.2">𝑖</ci><ci id="algx1.l14.m1.1.1.1.1.1.1.3.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.3">𝑛</ci><ci id="algx1.l14.m1.1.1.1.1.1.1.4.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.4">𝑝</ci><ci id="algx1.l14.m1.1.1.1.1.1.1.5.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.5">𝑢</ci><ci id="algx1.l14.m1.1.1.1.1.1.1.6.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.6">𝑡</ci><ci id="algx1.l14.m1.1.1.1.1.1.1.7.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.7">_</ci><ci id="algx1.l14.m1.1.1.1.1.1.1.8.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.8">𝑖</ci><ci id="algx1.l14.m1.1.1.1.1.1.1.9.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.9">𝑑</ci><ci id="algx1.l14.m1.1.1.1.1.1.1.10.cmml" xref="algx1.l14.m1.1.1.1.1.1.1.10">𝑠</ci></apply></apply><ci id="algx1.l14.m1.1.1.3a.cmml" xref="algx1.l14.m1.1.1.3"><mtext id="algx1.l14.m1.1.1.3.cmml" xref="algx1.l14.m1.1.1.3">LLM_max_length</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l14.m1.1c">\text{length}(input\_ids)&gt;\text{LLM\_max\_length}</annotation><annotation encoding="application/x-llamapun" id="algx1.l14.m1.1d">length ( italic_i italic_n italic_p italic_u italic_t _ italic_i italic_d italic_s ) &gt; LLM_max_length</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="algx1.l14.2">then</span>
</div>
<div class="ltx_listingline" id="algx1.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span>                  <span class="ltx_text ltx_font_bold" id="algx1.l15.1">return</span> <span class="ltx_text ltx_markedasmath" id="algx1.l15.2">temp</span>
</div>
<div class="ltx_listingline" id="algx1.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span>              <span class="ltx_text ltx_font_bold" id="algx1.l16.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l16.2">if</span>
</div>
<div class="ltx_listingline" id="algx1.l17">
<span class="ltx_tag ltx_tag_listingline">17:</span>              <math alttext="\text{output}\leftarrow\textsc{GenerateOutput}(...)" class="ltx_Math" display="inline" id="algx1.l17.m1.1"><semantics id="algx1.l17.m1.1a"><mrow id="algx1.l17.m1.1.2" xref="algx1.l17.m1.1.2.cmml"><mtext id="algx1.l17.m1.1.2.2" xref="algx1.l17.m1.1.2.2a.cmml">output</mtext><mo id="algx1.l17.m1.1.2.1" stretchy="false" xref="algx1.l17.m1.1.2.1.cmml">←</mo><mrow id="algx1.l17.m1.1.2.3" xref="algx1.l17.m1.1.2.3.cmml"><mtext class="ltx_font_smallcaps" id="algx1.l17.m1.1.2.3.2" xref="algx1.l17.m1.1.2.3.2a.cmml">GenerateOutput</mtext><mo id="algx1.l17.m1.1.2.3.1" xref="algx1.l17.m1.1.2.3.1.cmml">⁢</mo><mrow id="algx1.l17.m1.1.2.3.3.2" xref="algx1.l17.m1.1.2.3.cmml"><mo id="algx1.l17.m1.1.2.3.3.2.1" stretchy="false" xref="algx1.l17.m1.1.2.3.cmml">(</mo><mi id="algx1.l17.m1.1.1" mathvariant="normal" xref="algx1.l17.m1.1.1.cmml">…</mi><mo id="algx1.l17.m1.1.2.3.3.2.2" stretchy="false" xref="algx1.l17.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l17.m1.1b"><apply id="algx1.l17.m1.1.2.cmml" xref="algx1.l17.m1.1.2"><ci id="algx1.l17.m1.1.2.1.cmml" xref="algx1.l17.m1.1.2.1">←</ci><ci id="algx1.l17.m1.1.2.2a.cmml" xref="algx1.l17.m1.1.2.2"><mtext id="algx1.l17.m1.1.2.2.cmml" xref="algx1.l17.m1.1.2.2">output</mtext></ci><apply id="algx1.l17.m1.1.2.3.cmml" xref="algx1.l17.m1.1.2.3"><times id="algx1.l17.m1.1.2.3.1.cmml" xref="algx1.l17.m1.1.2.3.1"></times><ci id="algx1.l17.m1.1.2.3.2a.cmml" xref="algx1.l17.m1.1.2.3.2"><mtext class="ltx_font_smallcaps" id="algx1.l17.m1.1.2.3.2.cmml" xref="algx1.l17.m1.1.2.3.2">GenerateOutput</mtext></ci><ci id="algx1.l17.m1.1.1.cmml" xref="algx1.l17.m1.1.1">…</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l17.m1.1c">\text{output}\leftarrow\textsc{GenerateOutput}(...)</annotation><annotation encoding="application/x-llamapun" id="algx1.l17.m1.1d">output ← GenerateOutput ( … )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l18">
<span class="ltx_tag ltx_tag_listingline">18:</span>              <math alttext="\text{final\_output}\leftarrow\textsc{DecodeOutput}(...)" class="ltx_Math" display="inline" id="algx1.l18.m1.1"><semantics id="algx1.l18.m1.1a"><mrow id="algx1.l18.m1.1.2" xref="algx1.l18.m1.1.2.cmml"><mtext id="algx1.l18.m1.1.2.2" xref="algx1.l18.m1.1.2.2a.cmml">final_output</mtext><mo id="algx1.l18.m1.1.2.1" stretchy="false" xref="algx1.l18.m1.1.2.1.cmml">←</mo><mrow id="algx1.l18.m1.1.2.3" xref="algx1.l18.m1.1.2.3.cmml"><mtext class="ltx_font_smallcaps" id="algx1.l18.m1.1.2.3.2" xref="algx1.l18.m1.1.2.3.2a.cmml">DecodeOutput</mtext><mo id="algx1.l18.m1.1.2.3.1" xref="algx1.l18.m1.1.2.3.1.cmml">⁢</mo><mrow id="algx1.l18.m1.1.2.3.3.2" xref="algx1.l18.m1.1.2.3.cmml"><mo id="algx1.l18.m1.1.2.3.3.2.1" stretchy="false" xref="algx1.l18.m1.1.2.3.cmml">(</mo><mi id="algx1.l18.m1.1.1" mathvariant="normal" xref="algx1.l18.m1.1.1.cmml">…</mi><mo id="algx1.l18.m1.1.2.3.3.2.2" stretchy="false" xref="algx1.l18.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l18.m1.1b"><apply id="algx1.l18.m1.1.2.cmml" xref="algx1.l18.m1.1.2"><ci id="algx1.l18.m1.1.2.1.cmml" xref="algx1.l18.m1.1.2.1">←</ci><ci id="algx1.l18.m1.1.2.2a.cmml" xref="algx1.l18.m1.1.2.2"><mtext id="algx1.l18.m1.1.2.2.cmml" xref="algx1.l18.m1.1.2.2">final_output</mtext></ci><apply id="algx1.l18.m1.1.2.3.cmml" xref="algx1.l18.m1.1.2.3"><times id="algx1.l18.m1.1.2.3.1.cmml" xref="algx1.l18.m1.1.2.3.1"></times><ci id="algx1.l18.m1.1.2.3.2a.cmml" xref="algx1.l18.m1.1.2.3.2"><mtext class="ltx_font_smallcaps" id="algx1.l18.m1.1.2.3.2.cmml" xref="algx1.l18.m1.1.2.3.2">DecodeOutput</mtext></ci><ci id="algx1.l18.m1.1.1.cmml" xref="algx1.l18.m1.1.1">…</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l18.m1.1c">\text{final\_output}\leftarrow\textsc{DecodeOutput}(...)</annotation><annotation encoding="application/x-llamapun" id="algx1.l18.m1.1d">final_output ← DecodeOutput ( … )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l19">
<span class="ltx_tag ltx_tag_listingline">19:</span>              <math alttext="\text{qe\_input}\leftarrow\textsc{PrepareQEInput}(\text{source},\text{final\_%
output})" class="ltx_Math" display="inline" id="algx1.l19.m1.2"><semantics id="algx1.l19.m1.2a"><mrow id="algx1.l19.m1.2.3" xref="algx1.l19.m1.2.3.cmml"><mtext id="algx1.l19.m1.2.3.2" xref="algx1.l19.m1.2.3.2a.cmml">qe_input</mtext><mo id="algx1.l19.m1.2.3.1" stretchy="false" xref="algx1.l19.m1.2.3.1.cmml">←</mo><mrow id="algx1.l19.m1.2.3.3" xref="algx1.l19.m1.2.3.3.cmml"><mtext class="ltx_font_smallcaps" id="algx1.l19.m1.2.3.3.2" xref="algx1.l19.m1.2.3.3.2a.cmml">PrepareQEInput</mtext><mo id="algx1.l19.m1.2.3.3.1" xref="algx1.l19.m1.2.3.3.1.cmml">⁢</mo><mrow id="algx1.l19.m1.2.3.3.3.2" xref="algx1.l19.m1.2.3.3.3.1.cmml"><mo id="algx1.l19.m1.2.3.3.3.2.1" stretchy="false" xref="algx1.l19.m1.2.3.3.3.1.cmml">(</mo><mtext id="algx1.l19.m1.1.1" xref="algx1.l19.m1.1.1a.cmml">source</mtext><mo id="algx1.l19.m1.2.3.3.3.2.2" xref="algx1.l19.m1.2.3.3.3.1.cmml">,</mo><mtext id="algx1.l19.m1.2.2" xref="algx1.l19.m1.2.2a.cmml">final_output</mtext><mo id="algx1.l19.m1.2.3.3.3.2.3" stretchy="false" xref="algx1.l19.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l19.m1.2b"><apply id="algx1.l19.m1.2.3.cmml" xref="algx1.l19.m1.2.3"><ci id="algx1.l19.m1.2.3.1.cmml" xref="algx1.l19.m1.2.3.1">←</ci><ci id="algx1.l19.m1.2.3.2a.cmml" xref="algx1.l19.m1.2.3.2"><mtext id="algx1.l19.m1.2.3.2.cmml" xref="algx1.l19.m1.2.3.2">qe_input</mtext></ci><apply id="algx1.l19.m1.2.3.3.cmml" xref="algx1.l19.m1.2.3.3"><times id="algx1.l19.m1.2.3.3.1.cmml" xref="algx1.l19.m1.2.3.3.1"></times><ci id="algx1.l19.m1.2.3.3.2a.cmml" xref="algx1.l19.m1.2.3.3.2"><mtext class="ltx_font_smallcaps" id="algx1.l19.m1.2.3.3.2.cmml" xref="algx1.l19.m1.2.3.3.2">PrepareQEInput</mtext></ci><interval closure="open" id="algx1.l19.m1.2.3.3.3.1.cmml" xref="algx1.l19.m1.2.3.3.3.2"><ci id="algx1.l19.m1.1.1a.cmml" xref="algx1.l19.m1.1.1"><mtext id="algx1.l19.m1.1.1.cmml" xref="algx1.l19.m1.1.1">source</mtext></ci><ci id="algx1.l19.m1.2.2a.cmml" xref="algx1.l19.m1.2.2"><mtext id="algx1.l19.m1.2.2.cmml" xref="algx1.l19.m1.2.2">final_output</mtext></ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l19.m1.2c">\text{qe\_input}\leftarrow\textsc{PrepareQEInput}(\text{source},\text{final\_%
output})</annotation><annotation encoding="application/x-llamapun" id="algx1.l19.m1.2d">qe_input ← PrepareQEInput ( source , final_output )</annotation></semantics></math> <span class="ltx_text" id="algx1.l19.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx1.l19.1.m1.1"><semantics id="algx1.l19.1.m1.1a"><mo id="algx1.l19.1.m1.1.1" xref="algx1.l19.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx1.l19.1.m1.1b"><ci id="algx1.l19.1.m1.1.1.cmml" xref="algx1.l19.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l19.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx1.l19.1.m1.1d">▷</annotation></semantics></math> Phase 3: Estimation
</span>
</div>
<div class="ltx_listingline" id="algx1.l20">
<span class="ltx_tag ltx_tag_listingline">20:</span>              <math alttext="\text{qe\_score}\leftarrow\textsc{EstimateQuality}(\text{qe\_input},\text{%
model\_QE})" class="ltx_Math" display="inline" id="algx1.l20.m1.2"><semantics id="algx1.l20.m1.2a"><mrow id="algx1.l20.m1.2.3" xref="algx1.l20.m1.2.3.cmml"><mtext id="algx1.l20.m1.2.3.2" xref="algx1.l20.m1.2.3.2a.cmml">qe_score</mtext><mo id="algx1.l20.m1.2.3.1" stretchy="false" xref="algx1.l20.m1.2.3.1.cmml">←</mo><mrow id="algx1.l20.m1.2.3.3" xref="algx1.l20.m1.2.3.3.cmml"><mtext class="ltx_font_smallcaps" id="algx1.l20.m1.2.3.3.2" xref="algx1.l20.m1.2.3.3.2a.cmml">EstimateQuality</mtext><mo id="algx1.l20.m1.2.3.3.1" xref="algx1.l20.m1.2.3.3.1.cmml">⁢</mo><mrow id="algx1.l20.m1.2.3.3.3.2" xref="algx1.l20.m1.2.3.3.3.1.cmml"><mo id="algx1.l20.m1.2.3.3.3.2.1" stretchy="false" xref="algx1.l20.m1.2.3.3.3.1.cmml">(</mo><mtext id="algx1.l20.m1.1.1" xref="algx1.l20.m1.1.1a.cmml">qe_input</mtext><mo id="algx1.l20.m1.2.3.3.3.2.2" xref="algx1.l20.m1.2.3.3.3.1.cmml">,</mo><mtext id="algx1.l20.m1.2.2" xref="algx1.l20.m1.2.2a.cmml">model_QE</mtext><mo id="algx1.l20.m1.2.3.3.3.2.3" stretchy="false" xref="algx1.l20.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l20.m1.2b"><apply id="algx1.l20.m1.2.3.cmml" xref="algx1.l20.m1.2.3"><ci id="algx1.l20.m1.2.3.1.cmml" xref="algx1.l20.m1.2.3.1">←</ci><ci id="algx1.l20.m1.2.3.2a.cmml" xref="algx1.l20.m1.2.3.2"><mtext id="algx1.l20.m1.2.3.2.cmml" xref="algx1.l20.m1.2.3.2">qe_score</mtext></ci><apply id="algx1.l20.m1.2.3.3.cmml" xref="algx1.l20.m1.2.3.3"><times id="algx1.l20.m1.2.3.3.1.cmml" xref="algx1.l20.m1.2.3.3.1"></times><ci id="algx1.l20.m1.2.3.3.2a.cmml" xref="algx1.l20.m1.2.3.3.2"><mtext class="ltx_font_smallcaps" id="algx1.l20.m1.2.3.3.2.cmml" xref="algx1.l20.m1.2.3.3.2">EstimateQuality</mtext></ci><interval closure="open" id="algx1.l20.m1.2.3.3.3.1.cmml" xref="algx1.l20.m1.2.3.3.3.2"><ci id="algx1.l20.m1.1.1a.cmml" xref="algx1.l20.m1.1.1"><mtext id="algx1.l20.m1.1.1.cmml" xref="algx1.l20.m1.1.1">qe_input</mtext></ci><ci id="algx1.l20.m1.2.2a.cmml" xref="algx1.l20.m1.2.2"><mtext id="algx1.l20.m1.2.2.cmml" xref="algx1.l20.m1.2.2">model_QE</mtext></ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l20.m1.2c">\text{qe\_score}\leftarrow\textsc{EstimateQuality}(\text{qe\_input},\text{%
model\_QE})</annotation><annotation encoding="application/x-llamapun" id="algx1.l20.m1.2d">qe_score ← EstimateQuality ( qe_input , model_QE )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l21">
<span class="ltx_tag ltx_tag_listingline">21:</span>              <math alttext="\text{temp}.\text{append}((\text{prompt},\text{current\_qe\_score},\text{final%
\_output}))" class="ltx_Math" display="inline" id="algx1.l21.m1.5"><semantics id="algx1.l21.m1.5a"><mrow id="algx1.l21.m1.5.5.1" xref="algx1.l21.m1.5.5.2.cmml"><mtext id="algx1.l21.m1.4.4" xref="algx1.l21.m1.4.4a.cmml">temp</mtext><mo id="algx1.l21.m1.5.5.1.2" lspace="0em" rspace="0.167em" xref="algx1.l21.m1.5.5.2a.cmml">.</mo><mrow id="algx1.l21.m1.5.5.1.1" xref="algx1.l21.m1.5.5.1.1.cmml"><mtext id="algx1.l21.m1.5.5.1.1.3" xref="algx1.l21.m1.5.5.1.1.3a.cmml">append</mtext><mo id="algx1.l21.m1.5.5.1.1.2" xref="algx1.l21.m1.5.5.1.1.2.cmml">⁢</mo><mrow id="algx1.l21.m1.5.5.1.1.1.1" xref="algx1.l21.m1.5.5.1.1.cmml"><mo id="algx1.l21.m1.5.5.1.1.1.1.2" stretchy="false" xref="algx1.l21.m1.5.5.1.1.cmml">(</mo><mrow id="algx1.l21.m1.5.5.1.1.1.1.1.2" xref="algx1.l21.m1.5.5.1.1.1.1.1.1.cmml"><mo id="algx1.l21.m1.5.5.1.1.1.1.1.2.1" stretchy="false" xref="algx1.l21.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mtext id="algx1.l21.m1.1.1" xref="algx1.l21.m1.1.1a.cmml">prompt</mtext><mo id="algx1.l21.m1.5.5.1.1.1.1.1.2.2" xref="algx1.l21.m1.5.5.1.1.1.1.1.1.cmml">,</mo><mtext id="algx1.l21.m1.2.2" xref="algx1.l21.m1.2.2a.cmml">current_qe_score</mtext><mo id="algx1.l21.m1.5.5.1.1.1.1.1.2.3" xref="algx1.l21.m1.5.5.1.1.1.1.1.1.cmml">,</mo><mtext id="algx1.l21.m1.3.3" xref="algx1.l21.m1.3.3a.cmml">final_output</mtext><mo id="algx1.l21.m1.5.5.1.1.1.1.1.2.4" stretchy="false" xref="algx1.l21.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="algx1.l21.m1.5.5.1.1.1.1.3" stretchy="false" xref="algx1.l21.m1.5.5.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l21.m1.5b"><apply id="algx1.l21.m1.5.5.2.cmml" xref="algx1.l21.m1.5.5.1"><csymbol cd="ambiguous" id="algx1.l21.m1.5.5.2a.cmml" xref="algx1.l21.m1.5.5.1.2">formulae-sequence</csymbol><ci id="algx1.l21.m1.4.4a.cmml" xref="algx1.l21.m1.4.4"><mtext id="algx1.l21.m1.4.4.cmml" xref="algx1.l21.m1.4.4">temp</mtext></ci><apply id="algx1.l21.m1.5.5.1.1.cmml" xref="algx1.l21.m1.5.5.1.1"><times id="algx1.l21.m1.5.5.1.1.2.cmml" xref="algx1.l21.m1.5.5.1.1.2"></times><ci id="algx1.l21.m1.5.5.1.1.3a.cmml" xref="algx1.l21.m1.5.5.1.1.3"><mtext id="algx1.l21.m1.5.5.1.1.3.cmml" xref="algx1.l21.m1.5.5.1.1.3">append</mtext></ci><vector id="algx1.l21.m1.5.5.1.1.1.1.1.1.cmml" xref="algx1.l21.m1.5.5.1.1.1.1.1.2"><ci id="algx1.l21.m1.1.1a.cmml" xref="algx1.l21.m1.1.1"><mtext id="algx1.l21.m1.1.1.cmml" xref="algx1.l21.m1.1.1">prompt</mtext></ci><ci id="algx1.l21.m1.2.2a.cmml" xref="algx1.l21.m1.2.2"><mtext id="algx1.l21.m1.2.2.cmml" xref="algx1.l21.m1.2.2">current_qe_score</mtext></ci><ci id="algx1.l21.m1.3.3a.cmml" xref="algx1.l21.m1.3.3"><mtext id="algx1.l21.m1.3.3.cmml" xref="algx1.l21.m1.3.3">final_output</mtext></ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l21.m1.5c">\text{temp}.\text{append}((\text{prompt},\text{current\_qe\_score},\text{final%
\_output}))</annotation><annotation encoding="application/x-llamapun" id="algx1.l21.m1.5d">temp . append ( ( prompt , current_qe_score , final_output ) )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l22">
<span class="ltx_tag ltx_tag_listingline">22:</span>              <span class="ltx_text ltx_font_bold" id="algx1.l22.1">if</span> <math alttext="\text{current\_bleu\_score}\geq 100" class="ltx_Math" display="inline" id="algx1.l22.m1.1"><semantics id="algx1.l22.m1.1a"><mrow id="algx1.l22.m1.1.1" xref="algx1.l22.m1.1.1.cmml"><mtext id="algx1.l22.m1.1.1.2" xref="algx1.l22.m1.1.1.2a.cmml">current_bleu_score</mtext><mo id="algx1.l22.m1.1.1.1" xref="algx1.l22.m1.1.1.1.cmml">≥</mo><mn id="algx1.l22.m1.1.1.3" xref="algx1.l22.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="algx1.l22.m1.1b"><apply id="algx1.l22.m1.1.1.cmml" xref="algx1.l22.m1.1.1"><geq id="algx1.l22.m1.1.1.1.cmml" xref="algx1.l22.m1.1.1.1"></geq><ci id="algx1.l22.m1.1.1.2a.cmml" xref="algx1.l22.m1.1.1.2"><mtext id="algx1.l22.m1.1.1.2.cmml" xref="algx1.l22.m1.1.1.2">current_bleu_score</mtext></ci><cn id="algx1.l22.m1.1.1.3.cmml" type="integer" xref="algx1.l22.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l22.m1.1c">\text{current\_bleu\_score}\geq 100</annotation><annotation encoding="application/x-llamapun" id="algx1.l22.m1.1d">current_bleu_score ≥ 100</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="algx1.l22.2">then</span>
</div>
<div class="ltx_listingline" id="algx1.l23">
<span class="ltx_tag ltx_tag_listingline">23:</span>                  <span class="ltx_text ltx_font_bold" id="algx1.l23.1">return</span> <span class="ltx_text ltx_markedasmath" id="algx1.l23.2">temp</span>
</div>
<div class="ltx_listingline" id="algx1.l24">
<span class="ltx_tag ltx_tag_listingline">24:</span>              <span class="ltx_text ltx_font_bold" id="algx1.l24.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l24.2">if</span>
</div>
<div class="ltx_listingline" id="algx1.l25">
<span class="ltx_tag ltx_tag_listingline">25:</span>              <math alttext="\text{temp}\leftarrow\textsc{SortTemp}(...)" class="ltx_Math" display="inline" id="algx1.l25.m1.1"><semantics id="algx1.l25.m1.1a"><mrow id="algx1.l25.m1.1.2" xref="algx1.l25.m1.1.2.cmml"><mtext id="algx1.l25.m1.1.2.2" xref="algx1.l25.m1.1.2.2a.cmml">temp</mtext><mo id="algx1.l25.m1.1.2.1" stretchy="false" xref="algx1.l25.m1.1.2.1.cmml">←</mo><mrow id="algx1.l25.m1.1.2.3" xref="algx1.l25.m1.1.2.3.cmml"><mtext class="ltx_font_smallcaps" id="algx1.l25.m1.1.2.3.2" xref="algx1.l25.m1.1.2.3.2a.cmml">SortTemp</mtext><mo id="algx1.l25.m1.1.2.3.1" xref="algx1.l25.m1.1.2.3.1.cmml">⁢</mo><mrow id="algx1.l25.m1.1.2.3.3.2" xref="algx1.l25.m1.1.2.3.cmml"><mo id="algx1.l25.m1.1.2.3.3.2.1" stretchy="false" xref="algx1.l25.m1.1.2.3.cmml">(</mo><mi id="algx1.l25.m1.1.1" mathvariant="normal" xref="algx1.l25.m1.1.1.cmml">…</mi><mo id="algx1.l25.m1.1.2.3.3.2.2" stretchy="false" xref="algx1.l25.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l25.m1.1b"><apply id="algx1.l25.m1.1.2.cmml" xref="algx1.l25.m1.1.2"><ci id="algx1.l25.m1.1.2.1.cmml" xref="algx1.l25.m1.1.2.1">←</ci><ci id="algx1.l25.m1.1.2.2a.cmml" xref="algx1.l25.m1.1.2.2"><mtext id="algx1.l25.m1.1.2.2.cmml" xref="algx1.l25.m1.1.2.2">temp</mtext></ci><apply id="algx1.l25.m1.1.2.3.cmml" xref="algx1.l25.m1.1.2.3"><times id="algx1.l25.m1.1.2.3.1.cmml" xref="algx1.l25.m1.1.2.3.1"></times><ci id="algx1.l25.m1.1.2.3.2a.cmml" xref="algx1.l25.m1.1.2.3.2"><mtext class="ltx_font_smallcaps" id="algx1.l25.m1.1.2.3.2.cmml" xref="algx1.l25.m1.1.2.3.2">SortTemp</mtext></ci><ci id="algx1.l25.m1.1.1.cmml" xref="algx1.l25.m1.1.1">…</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l25.m1.1c">\text{temp}\leftarrow\textsc{SortTemp}(...)</annotation><annotation encoding="application/x-llamapun" id="algx1.l25.m1.1d">temp ← SortTemp ( … )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l26">
<span class="ltx_tag ltx_tag_listingline">26:</span>              <span class="ltx_text ltx_font_bold" id="algx1.l26.1">if</span> <math alttext="\text{qe\_score}\leq\text{best\_qe\_score}" class="ltx_Math" display="inline" id="algx1.l26.m1.1"><semantics id="algx1.l26.m1.1a"><mrow id="algx1.l26.m1.1.1" xref="algx1.l26.m1.1.1.cmml"><mtext id="algx1.l26.m1.1.1.2" xref="algx1.l26.m1.1.1.2a.cmml">qe_score</mtext><mo id="algx1.l26.m1.1.1.1" xref="algx1.l26.m1.1.1.1.cmml">≤</mo><mtext id="algx1.l26.m1.1.1.3" xref="algx1.l26.m1.1.1.3a.cmml">best_qe_score</mtext></mrow><annotation-xml encoding="MathML-Content" id="algx1.l26.m1.1b"><apply id="algx1.l26.m1.1.1.cmml" xref="algx1.l26.m1.1.1"><leq id="algx1.l26.m1.1.1.1.cmml" xref="algx1.l26.m1.1.1.1"></leq><ci id="algx1.l26.m1.1.1.2a.cmml" xref="algx1.l26.m1.1.1.2"><mtext id="algx1.l26.m1.1.1.2.cmml" xref="algx1.l26.m1.1.1.2">qe_score</mtext></ci><ci id="algx1.l26.m1.1.1.3a.cmml" xref="algx1.l26.m1.1.1.3"><mtext id="algx1.l26.m1.1.1.3.cmml" xref="algx1.l26.m1.1.1.3">best_qe_score</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l26.m1.1c">\text{qe\_score}\leq\text{best\_qe\_score}</annotation><annotation encoding="application/x-llamapun" id="algx1.l26.m1.1d">qe_score ≤ best_qe_score</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="algx1.l26.2">then</span>
</div>
<div class="ltx_listingline" id="algx1.l27">
<span class="ltx_tag ltx_tag_listingline">27:</span>                  <math alttext="\text{patience\_counter}\leftarrow\text{patience\_counter}+1" class="ltx_Math" display="inline" id="algx1.l27.m1.1"><semantics id="algx1.l27.m1.1a"><mrow id="algx1.l27.m1.1.1" xref="algx1.l27.m1.1.1.cmml"><mtext id="algx1.l27.m1.1.1.2" xref="algx1.l27.m1.1.1.2a.cmml">patience_counter</mtext><mo id="algx1.l27.m1.1.1.1" stretchy="false" xref="algx1.l27.m1.1.1.1.cmml">←</mo><mrow id="algx1.l27.m1.1.1.3" xref="algx1.l27.m1.1.1.3.cmml"><mtext id="algx1.l27.m1.1.1.3.2" xref="algx1.l27.m1.1.1.3.2a.cmml">patience_counter</mtext><mo id="algx1.l27.m1.1.1.3.1" xref="algx1.l27.m1.1.1.3.1.cmml">+</mo><mn id="algx1.l27.m1.1.1.3.3" xref="algx1.l27.m1.1.1.3.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l27.m1.1b"><apply id="algx1.l27.m1.1.1.cmml" xref="algx1.l27.m1.1.1"><ci id="algx1.l27.m1.1.1.1.cmml" xref="algx1.l27.m1.1.1.1">←</ci><ci id="algx1.l27.m1.1.1.2a.cmml" xref="algx1.l27.m1.1.1.2"><mtext id="algx1.l27.m1.1.1.2.cmml" xref="algx1.l27.m1.1.1.2">patience_counter</mtext></ci><apply id="algx1.l27.m1.1.1.3.cmml" xref="algx1.l27.m1.1.1.3"><plus id="algx1.l27.m1.1.1.3.1.cmml" xref="algx1.l27.m1.1.1.3.1"></plus><ci id="algx1.l27.m1.1.1.3.2a.cmml" xref="algx1.l27.m1.1.1.3.2"><mtext id="algx1.l27.m1.1.1.3.2.cmml" xref="algx1.l27.m1.1.1.3.2">patience_counter</mtext></ci><cn id="algx1.l27.m1.1.1.3.3.cmml" type="integer" xref="algx1.l27.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l27.m1.1c">\text{patience\_counter}\leftarrow\text{patience\_counter}+1</annotation><annotation encoding="application/x-llamapun" id="algx1.l27.m1.1d">patience_counter ← patience_counter + 1</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l28">
<span class="ltx_tag ltx_tag_listingline">28:</span>              <span class="ltx_text ltx_font_bold" id="algx1.l28.1">else</span>
</div>
<div class="ltx_listingline" id="algx1.l29">
<span class="ltx_tag ltx_tag_listingline">29:</span>                  <math alttext="\text{patience\_counter}\leftarrow 0" class="ltx_Math" display="inline" id="algx1.l29.m1.1"><semantics id="algx1.l29.m1.1a"><mrow id="algx1.l29.m1.1.1" xref="algx1.l29.m1.1.1.cmml"><mtext id="algx1.l29.m1.1.1.2" xref="algx1.l29.m1.1.1.2a.cmml">patience_counter</mtext><mo id="algx1.l29.m1.1.1.1" stretchy="false" xref="algx1.l29.m1.1.1.1.cmml">←</mo><mn id="algx1.l29.m1.1.1.3" xref="algx1.l29.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="algx1.l29.m1.1b"><apply id="algx1.l29.m1.1.1.cmml" xref="algx1.l29.m1.1.1"><ci id="algx1.l29.m1.1.1.1.cmml" xref="algx1.l29.m1.1.1.1">←</ci><ci id="algx1.l29.m1.1.1.2a.cmml" xref="algx1.l29.m1.1.1.2"><mtext id="algx1.l29.m1.1.1.2.cmml" xref="algx1.l29.m1.1.1.2">patience_counter</mtext></ci><cn id="algx1.l29.m1.1.1.3.cmml" type="integer" xref="algx1.l29.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l29.m1.1c">\text{patience\_counter}\leftarrow 0</annotation><annotation encoding="application/x-llamapun" id="algx1.l29.m1.1d">patience_counter ← 0</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l30">
<span class="ltx_tag ltx_tag_listingline">30:</span>              <span class="ltx_text ltx_font_bold" id="algx1.l30.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l30.2">if</span>
</div>
<div class="ltx_listingline" id="algx1.l31">
<span class="ltx_tag ltx_tag_listingline">31:</span>              <math alttext="\text{best\_qe\_score}\leftarrow\text{temp}[0][1]" class="ltx_Math" display="inline" id="algx1.l31.m1.2"><semantics id="algx1.l31.m1.2a"><mrow id="algx1.l31.m1.2.3" xref="algx1.l31.m1.2.3.cmml"><mtext id="algx1.l31.m1.2.3.2" xref="algx1.l31.m1.2.3.2a.cmml">best_qe_score</mtext><mo id="algx1.l31.m1.2.3.1" stretchy="false" xref="algx1.l31.m1.2.3.1.cmml">←</mo><mrow id="algx1.l31.m1.2.3.3" xref="algx1.l31.m1.2.3.3.cmml"><mtext id="algx1.l31.m1.2.3.3.2" xref="algx1.l31.m1.2.3.3.2a.cmml">temp</mtext><mo id="algx1.l31.m1.2.3.3.1" xref="algx1.l31.m1.2.3.3.1.cmml">⁢</mo><mrow id="algx1.l31.m1.2.3.3.3.2" xref="algx1.l31.m1.2.3.3.3.1.cmml"><mo id="algx1.l31.m1.2.3.3.3.2.1" stretchy="false" xref="algx1.l31.m1.2.3.3.3.1.1.cmml">[</mo><mn id="algx1.l31.m1.1.1" xref="algx1.l31.m1.1.1.cmml">0</mn><mo id="algx1.l31.m1.2.3.3.3.2.2" stretchy="false" xref="algx1.l31.m1.2.3.3.3.1.1.cmml">]</mo></mrow><mo id="algx1.l31.m1.2.3.3.1a" xref="algx1.l31.m1.2.3.3.1.cmml">⁢</mo><mrow id="algx1.l31.m1.2.3.3.4.2" xref="algx1.l31.m1.2.3.3.4.1.cmml"><mo id="algx1.l31.m1.2.3.3.4.2.1" stretchy="false" xref="algx1.l31.m1.2.3.3.4.1.1.cmml">[</mo><mn id="algx1.l31.m1.2.2" xref="algx1.l31.m1.2.2.cmml">1</mn><mo id="algx1.l31.m1.2.3.3.4.2.2" stretchy="false" xref="algx1.l31.m1.2.3.3.4.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l31.m1.2b"><apply id="algx1.l31.m1.2.3.cmml" xref="algx1.l31.m1.2.3"><ci id="algx1.l31.m1.2.3.1.cmml" xref="algx1.l31.m1.2.3.1">←</ci><ci id="algx1.l31.m1.2.3.2a.cmml" xref="algx1.l31.m1.2.3.2"><mtext id="algx1.l31.m1.2.3.2.cmml" xref="algx1.l31.m1.2.3.2">best_qe_score</mtext></ci><apply id="algx1.l31.m1.2.3.3.cmml" xref="algx1.l31.m1.2.3.3"><times id="algx1.l31.m1.2.3.3.1.cmml" xref="algx1.l31.m1.2.3.3.1"></times><ci id="algx1.l31.m1.2.3.3.2a.cmml" xref="algx1.l31.m1.2.3.3.2"><mtext id="algx1.l31.m1.2.3.3.2.cmml" xref="algx1.l31.m1.2.3.3.2">temp</mtext></ci><apply id="algx1.l31.m1.2.3.3.3.1.cmml" xref="algx1.l31.m1.2.3.3.3.2"><csymbol cd="latexml" id="algx1.l31.m1.2.3.3.3.1.1.cmml" xref="algx1.l31.m1.2.3.3.3.2.1">delimited-[]</csymbol><cn id="algx1.l31.m1.1.1.cmml" type="integer" xref="algx1.l31.m1.1.1">0</cn></apply><apply id="algx1.l31.m1.2.3.3.4.1.cmml" xref="algx1.l31.m1.2.3.3.4.2"><csymbol cd="latexml" id="algx1.l31.m1.2.3.3.4.1.1.cmml" xref="algx1.l31.m1.2.3.3.4.2.1">delimited-[]</csymbol><cn id="algx1.l31.m1.2.2.cmml" type="integer" xref="algx1.l31.m1.2.2">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l31.m1.2c">\text{best\_qe\_score}\leftarrow\text{temp}[0][1]</annotation><annotation encoding="application/x-llamapun" id="algx1.l31.m1.2d">best_qe_score ← temp [ 0 ] [ 1 ]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l32">
<span class="ltx_tag ltx_tag_listingline">32:</span>         <span class="ltx_text ltx_font_bold" id="algx1.l32.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l32.2">if</span>
</div>
<div class="ltx_listingline" id="algx1.l33">
<span class="ltx_tag ltx_tag_listingline">33:</span>         <math alttext="\text{itr}\leftarrow\text{itr}+1" class="ltx_Math" display="inline" id="algx1.l33.m1.1"><semantics id="algx1.l33.m1.1a"><mrow id="algx1.l33.m1.1.1" xref="algx1.l33.m1.1.1.cmml"><mtext id="algx1.l33.m1.1.1.2" xref="algx1.l33.m1.1.1.2a.cmml">itr</mtext><mo id="algx1.l33.m1.1.1.1" stretchy="false" xref="algx1.l33.m1.1.1.1.cmml">←</mo><mrow id="algx1.l33.m1.1.1.3" xref="algx1.l33.m1.1.1.3.cmml"><mtext id="algx1.l33.m1.1.1.3.2" xref="algx1.l33.m1.1.1.3.2a.cmml">itr</mtext><mo id="algx1.l33.m1.1.1.3.1" xref="algx1.l33.m1.1.1.3.1.cmml">+</mo><mn id="algx1.l33.m1.1.1.3.3" xref="algx1.l33.m1.1.1.3.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l33.m1.1b"><apply id="algx1.l33.m1.1.1.cmml" xref="algx1.l33.m1.1.1"><ci id="algx1.l33.m1.1.1.1.cmml" xref="algx1.l33.m1.1.1.1">←</ci><ci id="algx1.l33.m1.1.1.2a.cmml" xref="algx1.l33.m1.1.1.2"><mtext id="algx1.l33.m1.1.1.2.cmml" xref="algx1.l33.m1.1.1.2">itr</mtext></ci><apply id="algx1.l33.m1.1.1.3.cmml" xref="algx1.l33.m1.1.1.3"><plus id="algx1.l33.m1.1.1.3.1.cmml" xref="algx1.l33.m1.1.1.3.1"></plus><ci id="algx1.l33.m1.1.1.3.2a.cmml" xref="algx1.l33.m1.1.1.3.2"><mtext id="algx1.l33.m1.1.1.3.2.cmml" xref="algx1.l33.m1.1.1.3.2">itr</mtext></ci><cn id="algx1.l33.m1.1.1.3.3.cmml" type="integer" xref="algx1.l33.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l33.m1.1c">\text{itr}\leftarrow\text{itr}+1</annotation><annotation encoding="application/x-llamapun" id="algx1.l33.m1.1d">itr ← itr + 1</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l34">
<span class="ltx_tag ltx_tag_listingline">34:</span>     <span class="ltx_text ltx_font_bold" id="algx1.l34.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l34.2">while</span>
</div>
<div class="ltx_listingline" id="algx1.l35">
<span class="ltx_tag ltx_tag_listingline">35:</span>     <span class="ltx_text ltx_font_bold" id="algx1.l35.1">return</span> <span class="ltx_text ltx_markedasmath" id="algx1.l35.2">temp</span>
</div>
<div class="ltx_listingline" id="algx1.l36">
<span class="ltx_tag ltx_tag_listingline">36:</span><span class="ltx_text ltx_font_bold" id="algx1.l36.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l36.2">function</span>
</div>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_float ltx_figure_panel ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.4.1.1">Algorithm 1</span> </span> <span class="ltx_text ltx_font_bold" id="alg1.5.2">Pseudocode outlining the proposed Search Algorithm.</span> Each phase of the methodology is annotated alongside the relevant code. Function arguments are omitted for simplicity. The first element of the returning list (<span class="ltx_text ltx_font_italic" id="alg1.6.3">temp</span>) includes the selected prompt, its associated QE score, and the translated text.</figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 18 07:05:37 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
