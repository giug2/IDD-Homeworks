<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Deep Learning-Based Approach for Mangrove Monitoring</title>
<!--Generated on Mon Oct  7 19:13:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Mangrove monitoring Benchmarking Deep learning Image segmentation Mamba architecture" lang="en" name="keywords"/>
<base href="/html/2410.05443v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S1" title="In A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S2" title="In A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S3" title="In A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Mangrove Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S4" title="In A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5" title="In A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments and Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.SS1" title="In 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Sampled MagSet-2 Dataset Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.SS2" title="In 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Complete MagSet-2 Dataset Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S6" title="In A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S7" title="In A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgments</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Deep Learning-Based Approach for Mangrove Monitoring</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lucas JosÃ© VelÃ´so de Souza
</span><span class="ltx_author_notes">1CentraleSupÃ©lec, Saclay Campus, Gif-sur-Yvette, 91190, France
<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_contact ltx_role_email"><a href="mailto:1%7Bingrid.valverde-zreik,%20lucasjose.velosodesouza,%20adrien.salemsermanet%7D@student-cs.fr">1{ingrid.valverde-zreik, lucasjose.velosodesouza, adrien.salemsermanet}@student-cs.fr</a>
</span>EverSea - <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.eversea.blue" title="">https://www.eversea.blue</a>
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1.id1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span> lionel.pourchier@eversea.blue </span></span></span>
<br class="ltx_break"/></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ingrid Valverde Reis Zreik
</span><span class="ltx_author_notes">1CentraleSupÃ©lec, Saclay Campus, Gif-sur-Yvette, 91190, France
<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_contact ltx_role_email"><a href="mailto:1%7Bingrid.valverde-zreik,%20lucasjose.velosodesouza,%20adrien.salemsermanet%7D@student-cs.fr">1{ingrid.valverde-zreik, lucasjose.velosodesouza, adrien.salemsermanet}@student-cs.fr</a>
</span>EverSea - <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.eversea.blue" title="">https://www.eversea.blue</a>
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1.id1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span> lionel.pourchier@eversea.blue </span></span></span>
<br class="ltx_break"/></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adrien Salem-Sermanet
</span><span class="ltx_author_notes">1CentraleSupÃ©lec, Saclay Campus, Gif-sur-Yvette, 91190, France
<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_contact ltx_role_email"><a href="mailto:1%7Bingrid.valverde-zreik,%20lucasjose.velosodesouza,%20adrien.salemsermanet%7D@student-cs.fr">1{ingrid.valverde-zreik, lucasjose.velosodesouza, adrien.salemsermanet}@student-cs.fr</a>
</span>EverSea - <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.eversea.blue" title="">https://www.eversea.blue</a>
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id3.1.id1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span> lionel.pourchier@eversea.blue </span></span></span>
<br class="ltx_break"/></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">NacÃ©ra Seghouani
</span><span class="ltx_author_notes">1CentraleSupÃ©lec, Saclay Campus, Gif-sur-Yvette, 91190, France
<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_contact ltx_role_email"><a href="mailto:1%7Bingrid.valverde-zreik,%20lucasjose.velosodesouza,%20adrien.salemsermanet%7D@student-cs.fr">1{ingrid.valverde-zreik, lucasjose.velosodesouza, adrien.salemsermanet}@student-cs.fr</a>
</span>2LISN Paris-Sacaly University, Orsay, 91400, France
<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_contact ltx_role_email"><a href="mailto:2nacera.seghouani@lisn.fr">2nacera.seghouani@lisn.fr</a>
</span>EverSea - <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.eversea.blue" title="">https://www.eversea.blue</a>
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id4.1.id1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span> lionel.pourchier@eversea.blue </span></span></span>
<br class="ltx_break"/></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lionel Pourchier

</span><span class="ltx_author_notes">EverSea - <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.eversea.blue" title="">https://www.eversea.blue</a>
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id5.1.id1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span> lionel.pourchier@eversea.blue </span></span></span>
<br class="ltx_break"/>33</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Mangroves are dynamic coastal ecosystems that are crucial to environmental health, economic stability, and climate resilience. The monitoring and preservation of mangroves are of global importance, with remote sensing technologies playing a pivotal role in these efforts. The integration of cutting-edge artificial intelligence with satellite data opens new avenues for ecological monitoring, potentially revolutionizing conservation strategies at a time when the protection of natural resources is more crucial than ever. The objective of this work is to provide a comprehensive evaluation of recent deep-learning models on the task of mangrove segmentation. We first introduce and make available a novel open-source dataset, MagSet-2, incorporating mangrove annotations from the Global Mangrove Watch and satellite images from Sentinel-2, from mangrove positions all over the world. We then benchmark three architectural groups, namely convolutional, transformer, and mamba models, using the created dataset. The experimental outcomes further validate the deep learning communityâ€™s interest in the Mamba model, which surpasses other architectures in all metrics.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Mangrove monitoring Benchmarking Deep learning Image segmentation Mamba architecture
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">At the nexus of land and water, mangroves are vital ecosystems with significant environmental, economic, and societal benefits. They support marine biodiversity and fisheries, essential for coastal communities, while protecting shorelines from erosion and climate change impacts. Additionally, mangroves contribute to carbon sequestration and water purification, underscoring their role in environmental sustainability. The increasing degradation of these ecosystems poses significant risks to both the socio-economic stability of coastal areas and global environmental health, emphasizing the need for focused conservation efforts.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Globally, initiatives and partnerships <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
<a class="ltx_ref ltx_href" href="https://www.worldwildlife.org/initiatives/mangroves-for-community-and-climate" title="">World Wildlife Fund (WWF), â€œMangroves for Community and Climateâ€.</a> </span></span></span> are advancing efforts to safeguard mangrove ecosystems, supported by legal and policy frameworks tailored to international conservation goals. These endeavors combine science, community action, and policy innovation to ensure the future of mangrove habitats.
Building upon this foundation, the collection and analysis of accurate information on areas affected by degradation and vegetation changes become indispensable. This data is crucial for effective decision-making in environmental management and conservation investment, serving as a cornerstone for the strategies and actions developed to protect these vital ecosystems. Remote sensing technologies allow both a large scale monitoring and precise quantification of changes in vegetation cover, facilitating informed decisions on resource allocation for conservation and restoration efforts.
Technological advancements in remote sensing and artificial intelligence offer promising avenues for monitoring and protecting these ecosystems. For instance, previous work have shown that the application of Convolutional Neural Networks (CNNs) in mangrove analysis supports detailed ecological assessments and strategic decision-making in mangrove managementÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib13" title="">13</a>]</cite>. However, with the recent advancements and enhancements in the field of Deep Learning, such as the development of Transformer models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib25" title="">25</a>]</cite> and recently Mamba techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib7" title="">7</a>]</cite>, the use of new and better performing methodologies becomes crucial for achieving more accurate outcomes and to better inform decision-makers.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Building upon these innovative approaches for improved mangrove segmentation and monitoring, the integration of geolocation data with updated satellite imagery is key to unlocking new potentials in ecosystem conservation. Despite the vast availability of data today, effectively accessing mangroves positions for the observation of this ecosystem remains a challenge. Although many organizations have detailed mangrove location data, it is often not integrated with satellite imagery, a crucial step for effective ecosystem management. This paper introduces a new dataset, <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">MagSet-2</span>, which combines mangrove geolocation data from <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">Global Mangrove Watch</span> Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib6" title="">6</a>]</cite> with <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">Sentinel-2</span> satellite images Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib22" title="">22</a>]</cite>. This integration catalyzes machine learning models that are vital for accurately monitoring and predicting mangrove locations. Furthermore, we concentrate our efforts in comparing various deep learning architectures to interpret satellite imagery, namely CNN-based architectures (U-Net<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib20" title="">20</a>]</cite>, MANet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib30" title="">30</a>]</cite>, PAN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib31" title="">31</a>]</cite>), Transformer-based architectures (BEiT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib3" title="">3</a>]</cite>, Segformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib29" title="">29</a>]</cite>) and a Mamba-based architecture (Swin-UMamba<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib15" title="">15</a>]</cite>). These architectures are chosen due to their recognized effectiveness in semantic segmentation tasks. By developing a comprehensive dataset and employing a novel Mamba-type model, this research aims to enhance the precision of mangrove ecosystem analysis, supporting more informed strategies.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the following, we delve into a review of related work in the field of mangrove detection and segmentation in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S2" title="2 Related Work â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">2</span></a>. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S3" title="3 Mangrove Data â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">3</span></a> offers insights into the dataset creation process, detailing how mangrove annotations were collected and how Sentinel-2 imagery was acquired, which are essential components for our research <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>GitHub with Code and Dataset: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/SVJLucas/MangroveAI" title="">https://github.com/SVJLucas/MangroveAI</a></span></span></span>. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S4" title="4 Methodology â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">4</span></a> presents our approach to mangrove segmentation, including the implementation of deep learning models and the utilization of advanced techniques such a the Mamba-type architecture. Continuing, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.SS1" title="5.1 Sampled MagSet-2 Dataset Evaluation â€£ 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">5.1</span></a> thoroughly evaluates the six leading deep learning architectures for mangrove segmentation, highlighting their performance and suitability for the task. Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S6" title="6 Conclusion â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes the evaluation results.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Historically, mangrove monitoring has evolved significantly, driven by advancements in remote sensing technology, computational methods, and machine learning techniques. Initially, mangrove monitoring relied on field surveys, which were labor-intensive and limited by the challenging and inaccessible nature of mangrove habitats <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib1" title="">1</a>]</cite>. With the advent of remote sensing, researchers gained the ability to observe mangroves over extensive areas with greater efficiency and reduced cost. Early remote sensing methods employed basic vegetation indices like NDVI, EVI, and VARI to assess the health and distribution of mangroves <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">As machine learning gained prominence, both unsupervised and supervised algorithms began to be integrated into remote sensing workflows. Techniques like K-nearest neighbors (KNN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib27" title="">27</a>]</cite>, support vector machines (SVMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib26" title="">26</a>]</cite>, and random forests (RF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib4" title="">4</a>]</cite> were applied to improve the accuracy of mangrove classification based on spectral data. However, these methods often struggled with spectral ambiguities, where different objects share similar spectral signatures, leading to misclassification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">To overcome the limitations of single-method approaches, researchers started combining traditional index methods with machine learning algorithms. This hybrid approach leveraged the strengths of both techniques, using indices to guide machine learning models, thereby enhancing the accuracy of mangrove extraction and analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The breakthrough of deep learning in image recognition, marked by the success of AlexNet in 2012 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib14" title="">14</a>]</cite>, paved the way for sophisticated architectures like CNNs, namely FCN , SegNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib2" title="">2</a>]</cite> , and U-NetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib20" title="">20</a>]</cite> for semantic segmentation of remote sensing images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib17" title="">17</a>]</cite>. These deep learning models have been particularly transformative in mangrove monitoring, offering significant improvements in segmentation accuracy through their ability to learn complex patterns and features from large datasets. However, a notable challenge persists as most of these advancements are constrained by geographical and ecological specificity, primarily due to the absence of a comprehensive global dataset that maps diverse mangrove species across various regions.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The complexity of mangrove ecosystems and the diverse challenges in their monitoring, thereby underscores the pressing need for a globally representative mangrove dataset to further enhance the effectiveness and scalability of deep learning applications in mangrove monitoring. By comparing the CNNs, Transformers and Mamba architectures, we aim to identify the most effective deep learning model for mangrove segmentation. This comparison is critical not only for enhancing the accuracy of current monitoring efforts but also for pushing the boundaries of what can be achieved in ecological monitoring and conservation with state-of-the-art technology.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Mangrove Data</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We aimed to build and release MagSet-2
of mangrove regions worldwide to perform benchmarking analyses and serving as the foundation for training the proposed models. This dataset consists of annotations of different species of mangroves from the <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">Global Mangrove Watch</span> datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib6" title="">6</a>]</cite> and corresponding <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">Sentinel-2</span> optical satellite imageryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib22" title="">22</a>]</cite>. The building of this dataset answers to the non-availability of a global heterogeneous mangrove annotated dataset.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.7">The <span class="ltx_text ltx_font_italic" id="S3.p2.7.1">Global Mangrove Watch</span> dataset of annotationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib6" title="">6</a>]</cite>, as well as a world map of mangrove distributionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib11" title="">11</a>]</cite>, were used to define <math alttext="10" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><cn id="S3.p2.1.m1.1.1.cmml" type="integer" xref="S3.p2.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">10</annotation></semantics></math> non-overlapping geographic zones: Central America, South America, West Africa, East Africa, Middle East, India, East Asia, Indonesia, Australia and Pacific Islands. These zones, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S3.F1" title="Figure 1 â€£ 3 Mangrove Data â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">1</span></a>, depict geographic regions with distinct mangrove characteristics to facilitate stratified sampling and improve model performance across diverse environments.
<span class="ltx_text ltx_font_italic" id="S3.p2.7.2">Sentinel-2</span> satellites provide multispectral imageryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib22" title="">22</a>]</cite> for various Earth observation applications. Since mangrove reflectance was observed to rise rapidly towards the red edgeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib23" title="">23</a>]</cite>, mangrove ecosystems can be observed using indices computed from spectral bands in the visible and infrared regions of optical remote sensing. That is why the Sentinel-2 bands used in training include <math alttext="B" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_B</annotation></semantics></math> (Blue), <math alttext="G" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_G</annotation></semantics></math> (Green), <math alttext="R" class="ltx_Math" display="inline" id="S3.p2.4.m4.1"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.1d">italic_R</annotation></semantics></math> (Red), <math alttext="NIR" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">N</mi><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">I</mi><mo id="S3.p2.5.m5.1.1.1a" xref="S3.p2.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.p2.5.m5.1.1.4" xref="S3.p2.5.m5.1.1.4.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><times id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"></times><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">ğ‘</ci><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">ğ¼</ci><ci id="S3.p2.5.m5.1.1.4.cmml" xref="S3.p2.5.m5.1.1.4">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">NIR</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">italic_N italic_I italic_R</annotation></semantics></math> (Near-Infrared), <math alttext="NIR_{vegetation}" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><mrow id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mi id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml">N</mi><mo id="S3.p2.6.m6.1.1.1" xref="S3.p2.6.m6.1.1.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">I</mi><mo id="S3.p2.6.m6.1.1.1a" xref="S3.p2.6.m6.1.1.1.cmml">â¢</mo><msub id="S3.p2.6.m6.1.1.4" xref="S3.p2.6.m6.1.1.4.cmml"><mi id="S3.p2.6.m6.1.1.4.2" xref="S3.p2.6.m6.1.1.4.2.cmml">R</mi><mrow id="S3.p2.6.m6.1.1.4.3" xref="S3.p2.6.m6.1.1.4.3.cmml"><mi id="S3.p2.6.m6.1.1.4.3.2" xref="S3.p2.6.m6.1.1.4.3.2.cmml">v</mi><mo id="S3.p2.6.m6.1.1.4.3.1" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.3" xref="S3.p2.6.m6.1.1.4.3.3.cmml">e</mi><mo id="S3.p2.6.m6.1.1.4.3.1a" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.4" xref="S3.p2.6.m6.1.1.4.3.4.cmml">g</mi><mo id="S3.p2.6.m6.1.1.4.3.1b" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.5" xref="S3.p2.6.m6.1.1.4.3.5.cmml">e</mi><mo id="S3.p2.6.m6.1.1.4.3.1c" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.6" xref="S3.p2.6.m6.1.1.4.3.6.cmml">t</mi><mo id="S3.p2.6.m6.1.1.4.3.1d" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.7" xref="S3.p2.6.m6.1.1.4.3.7.cmml">a</mi><mo id="S3.p2.6.m6.1.1.4.3.1e" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.8" xref="S3.p2.6.m6.1.1.4.3.8.cmml">t</mi><mo id="S3.p2.6.m6.1.1.4.3.1f" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.9" xref="S3.p2.6.m6.1.1.4.3.9.cmml">i</mi><mo id="S3.p2.6.m6.1.1.4.3.1g" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.10" xref="S3.p2.6.m6.1.1.4.3.10.cmml">o</mi><mo id="S3.p2.6.m6.1.1.4.3.1h" xref="S3.p2.6.m6.1.1.4.3.1.cmml">â¢</mo><mi id="S3.p2.6.m6.1.1.4.3.11" xref="S3.p2.6.m6.1.1.4.3.11.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><times id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1.1"></times><ci id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">ğ‘</ci><ci id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3">ğ¼</ci><apply id="S3.p2.6.m6.1.1.4.cmml" xref="S3.p2.6.m6.1.1.4"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.4.1.cmml" xref="S3.p2.6.m6.1.1.4">subscript</csymbol><ci id="S3.p2.6.m6.1.1.4.2.cmml" xref="S3.p2.6.m6.1.1.4.2">ğ‘…</ci><apply id="S3.p2.6.m6.1.1.4.3.cmml" xref="S3.p2.6.m6.1.1.4.3"><times id="S3.p2.6.m6.1.1.4.3.1.cmml" xref="S3.p2.6.m6.1.1.4.3.1"></times><ci id="S3.p2.6.m6.1.1.4.3.2.cmml" xref="S3.p2.6.m6.1.1.4.3.2">ğ‘£</ci><ci id="S3.p2.6.m6.1.1.4.3.3.cmml" xref="S3.p2.6.m6.1.1.4.3.3">ğ‘’</ci><ci id="S3.p2.6.m6.1.1.4.3.4.cmml" xref="S3.p2.6.m6.1.1.4.3.4">ğ‘”</ci><ci id="S3.p2.6.m6.1.1.4.3.5.cmml" xref="S3.p2.6.m6.1.1.4.3.5">ğ‘’</ci><ci id="S3.p2.6.m6.1.1.4.3.6.cmml" xref="S3.p2.6.m6.1.1.4.3.6">ğ‘¡</ci><ci id="S3.p2.6.m6.1.1.4.3.7.cmml" xref="S3.p2.6.m6.1.1.4.3.7">ğ‘</ci><ci id="S3.p2.6.m6.1.1.4.3.8.cmml" xref="S3.p2.6.m6.1.1.4.3.8">ğ‘¡</ci><ci id="S3.p2.6.m6.1.1.4.3.9.cmml" xref="S3.p2.6.m6.1.1.4.3.9">ğ‘–</ci><ci id="S3.p2.6.m6.1.1.4.3.10.cmml" xref="S3.p2.6.m6.1.1.4.3.10">ğ‘œ</ci><ci id="S3.p2.6.m6.1.1.4.3.11.cmml" xref="S3.p2.6.m6.1.1.4.3.11">ğ‘›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">NIR_{vegetation}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">italic_N italic_I italic_R start_POSTSUBSCRIPT italic_v italic_e italic_g italic_e italic_t italic_a italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT</annotation></semantics></math> (Vegetation Red Edge) and <math alttext="SWIR" class="ltx_Math" display="inline" id="S3.p2.7.m7.1"><semantics id="S3.p2.7.m7.1a"><mrow id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">S</mi><mo id="S3.p2.7.m7.1.1.1" xref="S3.p2.7.m7.1.1.1.cmml">â¢</mo><mi id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">W</mi><mo id="S3.p2.7.m7.1.1.1a" xref="S3.p2.7.m7.1.1.1.cmml">â¢</mo><mi id="S3.p2.7.m7.1.1.4" xref="S3.p2.7.m7.1.1.4.cmml">I</mi><mo id="S3.p2.7.m7.1.1.1b" xref="S3.p2.7.m7.1.1.1.cmml">â¢</mo><mi id="S3.p2.7.m7.1.1.5" xref="S3.p2.7.m7.1.1.5.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><times id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1"></times><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">ğ‘†</ci><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">ğ‘Š</ci><ci id="S3.p2.7.m7.1.1.4.cmml" xref="S3.p2.7.m7.1.1.4">ğ¼</ci><ci id="S3.p2.7.m7.1.1.5.cmml" xref="S3.p2.7.m7.1.1.5">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">SWIR</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m7.1d">italic_S italic_W italic_I italic_R</annotation></semantics></math> (Short-Wave Infrared), as can be observed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S3.F2" title="Figure 2 â€£ 3 Mangrove Data â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="116" id="S3.F1.g1" src="extracted/5893977/img/mangrove_world.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Mangrove Position (neon blue) and the different Mangrove Zones (green) Dataset based on the Global Mangrove Watch (GMW) v3.2020</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="286" id="S3.F2.g1" src="extracted/5893977/img/sample_from_magset2.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sentinel-2 Spectral Display and Vegetation Analysis: Starting from the top left with the RGB bands, followed by the NIR band, Vegetation NIR, and SWIR band in sequence. On the bottom row, from left to right, we have the estimated NDVI, NDWI, NDMI indices, and the targeted Mangrove locations for predictive modeling.</figcaption>
</figure>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">From these spectral bands, we were able to derive the following relevant spectral indices used in mangrove segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib23" title="">23</a>]</cite>: NDVI (Normalized Difference Vegetation Index), NDWI (Normalized Difference Water Index), and NDMI (Normalized Difference Moisture Index). These indices were computed using the formulas in Equations <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S3.E1" title="In 3 Mangrove Data â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S3.E2" title="In 3 Mangrove Data â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S3.E3" title="In 3 Mangrove Data â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">3</span></a> and used as additional input channels.</p>
</div>
<div class="ltx_para" id="S3.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\scriptsize NDVI=\frac{{NIR-R}}{{NIR+R}}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" mathsize="70%" xref="S3.E1.m1.1.1.2.2.cmml">N</mi><mo id="S3.E1.m1.1.1.2.1" xref="S3.E1.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.2.3" mathsize="70%" xref="S3.E1.m1.1.1.2.3.cmml">D</mi><mo id="S3.E1.m1.1.1.2.1a" xref="S3.E1.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.2.4" mathsize="70%" xref="S3.E1.m1.1.1.2.4.cmml">V</mi><mo id="S3.E1.m1.1.1.2.1b" xref="S3.E1.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.2.5" mathsize="70%" xref="S3.E1.m1.1.1.2.5.cmml">I</mi></mrow><mo id="S3.E1.m1.1.1.1" mathsize="70%" xref="S3.E1.m1.1.1.1.cmml">=</mo><mfrac id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mrow id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2" mathsize="70%" xref="S3.E1.m1.1.1.3.2.2.2.cmml">N</mi><mo id="S3.E1.m1.1.1.3.2.2.1" xref="S3.E1.m1.1.1.3.2.2.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.3.2.2.3" mathsize="70%" xref="S3.E1.m1.1.1.3.2.2.3.cmml">I</mi><mo id="S3.E1.m1.1.1.3.2.2.1a" xref="S3.E1.m1.1.1.3.2.2.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.3.2.2.4" mathsize="70%" xref="S3.E1.m1.1.1.3.2.2.4.cmml">R</mi></mrow><mo id="S3.E1.m1.1.1.3.2.1" mathsize="70%" xref="S3.E1.m1.1.1.3.2.1.cmml">âˆ’</mo><mi id="S3.E1.m1.1.1.3.2.3" mathsize="70%" xref="S3.E1.m1.1.1.3.2.3.cmml">R</mi></mrow><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mrow id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.3.3.2.2" mathsize="70%" xref="S3.E1.m1.1.1.3.3.2.2.cmml">N</mi><mo id="S3.E1.m1.1.1.3.3.2.1" xref="S3.E1.m1.1.1.3.3.2.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.3.3.2.3" mathsize="70%" xref="S3.E1.m1.1.1.3.3.2.3.cmml">I</mi><mo id="S3.E1.m1.1.1.3.3.2.1a" xref="S3.E1.m1.1.1.3.3.2.1.cmml">â¢</mo><mi id="S3.E1.m1.1.1.3.3.2.4" mathsize="70%" xref="S3.E1.m1.1.1.3.3.2.4.cmml">R</mi></mrow><mo id="S3.E1.m1.1.1.3.3.1" mathsize="70%" xref="S3.E1.m1.1.1.3.3.1.cmml">+</mo><mi id="S3.E1.m1.1.1.3.3.3" mathsize="70%" xref="S3.E1.m1.1.1.3.3.3.cmml">R</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><times id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2.1"></times><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">ğ‘</ci><ci id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">ğ·</ci><ci id="S3.E1.m1.1.1.2.4.cmml" xref="S3.E1.m1.1.1.2.4">ğ‘‰</ci><ci id="S3.E1.m1.1.1.2.5.cmml" xref="S3.E1.m1.1.1.2.5">ğ¼</ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><divide id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3"></divide><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><minus id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1"></minus><apply id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2"><times id="S3.E1.m1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2.1"></times><ci id="S3.E1.m1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2">ğ‘</ci><ci id="S3.E1.m1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3">ğ¼</ci><ci id="S3.E1.m1.1.1.3.2.2.4.cmml" xref="S3.E1.m1.1.1.3.2.2.4">ğ‘…</ci></apply><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">ğ‘…</ci></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><plus id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></plus><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2"><times id="S3.E1.m1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.1"></times><ci id="S3.E1.m1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.2">ğ‘</ci><ci id="S3.E1.m1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3">ğ¼</ci><ci id="S3.E1.m1.1.1.3.3.2.4.cmml" xref="S3.E1.m1.1.1.3.3.2.4">ğ‘…</ci></apply><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">ğ‘…</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\scriptsize NDVI=\frac{{NIR-R}}{{NIR+R}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_N italic_D italic_V italic_I = divide start_ARG italic_N italic_I italic_R - italic_R end_ARG start_ARG italic_N italic_I italic_R + italic_R end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.p5">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\scriptsize NDWI=\frac{{G-NIR}}{{G+NIR}}" class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mrow id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" mathsize="70%" xref="S3.E2.m1.1.1.2.2.cmml">N</mi><mo id="S3.E2.m1.1.1.2.1" xref="S3.E2.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.2.3" mathsize="70%" xref="S3.E2.m1.1.1.2.3.cmml">D</mi><mo id="S3.E2.m1.1.1.2.1a" xref="S3.E2.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.2.4" mathsize="70%" xref="S3.E2.m1.1.1.2.4.cmml">W</mi><mo id="S3.E2.m1.1.1.2.1b" xref="S3.E2.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.2.5" mathsize="70%" xref="S3.E2.m1.1.1.2.5.cmml">I</mi></mrow><mo id="S3.E2.m1.1.1.1" mathsize="70%" xref="S3.E2.m1.1.1.1.cmml">=</mo><mfrac id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mrow id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.3.2.2" mathsize="70%" xref="S3.E2.m1.1.1.3.2.2.cmml">G</mi><mo id="S3.E2.m1.1.1.3.2.1" mathsize="70%" xref="S3.E2.m1.1.1.3.2.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.1.1.3.2.3" xref="S3.E2.m1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.1.1.3.2.3.2" mathsize="70%" xref="S3.E2.m1.1.1.3.2.3.2.cmml">N</mi><mo id="S3.E2.m1.1.1.3.2.3.1" xref="S3.E2.m1.1.1.3.2.3.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.3.2.3.3" mathsize="70%" xref="S3.E2.m1.1.1.3.2.3.3.cmml">I</mi><mo id="S3.E2.m1.1.1.3.2.3.1a" xref="S3.E2.m1.1.1.3.2.3.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.3.2.3.4" mathsize="70%" xref="S3.E2.m1.1.1.3.2.3.4.cmml">R</mi></mrow></mrow><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" mathsize="70%" xref="S3.E2.m1.1.1.3.3.2.cmml">G</mi><mo id="S3.E2.m1.1.1.3.3.1" mathsize="70%" xref="S3.E2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.3.2" mathsize="70%" xref="S3.E2.m1.1.1.3.3.3.2.cmml">N</mi><mo id="S3.E2.m1.1.1.3.3.3.1" xref="S3.E2.m1.1.1.3.3.3.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.3.3.3.3" mathsize="70%" xref="S3.E2.m1.1.1.3.3.3.3.cmml">I</mi><mo id="S3.E2.m1.1.1.3.3.3.1a" xref="S3.E2.m1.1.1.3.3.3.1.cmml">â¢</mo><mi id="S3.E2.m1.1.1.3.3.3.4" mathsize="70%" xref="S3.E2.m1.1.1.3.3.3.4.cmml">R</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><times id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2.1"></times><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">ğ‘</ci><ci id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3">ğ·</ci><ci id="S3.E2.m1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.2.4">ğ‘Š</ci><ci id="S3.E2.m1.1.1.2.5.cmml" xref="S3.E2.m1.1.1.2.5">ğ¼</ci></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><divide id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3"></divide><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><minus id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2.1"></minus><ci id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2">ğº</ci><apply id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3"><times id="S3.E2.m1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.3.2.3.1"></times><ci id="S3.E2.m1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.3.2.3.2">ğ‘</ci><ci id="S3.E2.m1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.3.2.3.3">ğ¼</ci><ci id="S3.E2.m1.1.1.3.2.3.4.cmml" xref="S3.E2.m1.1.1.3.2.3.4">ğ‘…</ci></apply></apply><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><plus id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></plus><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">ğº</ci><apply id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3"><times id="S3.E2.m1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.3.2">ğ‘</ci><ci id="S3.E2.m1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3.3">ğ¼</ci><ci id="S3.E2.m1.1.1.3.3.3.4.cmml" xref="S3.E2.m1.1.1.3.3.3.4">ğ‘…</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\scriptsize NDWI=\frac{{G-NIR}}{{G+NIR}}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_N italic_D italic_W italic_I = divide start_ARG italic_G - italic_N italic_I italic_R end_ARG start_ARG italic_G + italic_N italic_I italic_R end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.p6">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\scriptsize NDMI=\frac{{NIR-SWIR}}{{NIR+SWIR}}" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mrow id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.2.2" mathsize="70%" xref="S3.E3.m1.1.1.2.2.cmml">N</mi><mo id="S3.E3.m1.1.1.2.1" xref="S3.E3.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.2.3" mathsize="70%" xref="S3.E3.m1.1.1.2.3.cmml">D</mi><mo id="S3.E3.m1.1.1.2.1a" xref="S3.E3.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.2.4" mathsize="70%" xref="S3.E3.m1.1.1.2.4.cmml">M</mi><mo id="S3.E3.m1.1.1.2.1b" xref="S3.E3.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.2.5" mathsize="70%" xref="S3.E3.m1.1.1.2.5.cmml">I</mi></mrow><mo id="S3.E3.m1.1.1.1" mathsize="70%" xref="S3.E3.m1.1.1.1.cmml">=</mo><mfrac id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mrow id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mrow id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2.2" mathsize="70%" xref="S3.E3.m1.1.1.3.2.2.2.cmml">N</mi><mo id="S3.E3.m1.1.1.3.2.2.1" xref="S3.E3.m1.1.1.3.2.2.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.2.2.3" mathsize="70%" xref="S3.E3.m1.1.1.3.2.2.3.cmml">I</mi><mo id="S3.E3.m1.1.1.3.2.2.1a" xref="S3.E3.m1.1.1.3.2.2.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.2.2.4" mathsize="70%" xref="S3.E3.m1.1.1.3.2.2.4.cmml">R</mi></mrow><mo id="S3.E3.m1.1.1.3.2.1" mathsize="70%" xref="S3.E3.m1.1.1.3.2.1.cmml">âˆ’</mo><mrow id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.1.1.3.2.3.2" mathsize="70%" xref="S3.E3.m1.1.1.3.2.3.2.cmml">S</mi><mo id="S3.E3.m1.1.1.3.2.3.1" xref="S3.E3.m1.1.1.3.2.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.2.3.3" mathsize="70%" xref="S3.E3.m1.1.1.3.2.3.3.cmml">W</mi><mo id="S3.E3.m1.1.1.3.2.3.1a" xref="S3.E3.m1.1.1.3.2.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.2.3.4" mathsize="70%" xref="S3.E3.m1.1.1.3.2.3.4.cmml">I</mi><mo id="S3.E3.m1.1.1.3.2.3.1b" xref="S3.E3.m1.1.1.3.2.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.2.3.5" mathsize="70%" xref="S3.E3.m1.1.1.3.2.3.5.cmml">R</mi></mrow></mrow><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mrow id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml"><mi id="S3.E3.m1.1.1.3.3.2.2" mathsize="70%" xref="S3.E3.m1.1.1.3.3.2.2.cmml">N</mi><mo id="S3.E3.m1.1.1.3.3.2.1" xref="S3.E3.m1.1.1.3.3.2.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.3.2.3" mathsize="70%" xref="S3.E3.m1.1.1.3.3.2.3.cmml">I</mi><mo id="S3.E3.m1.1.1.3.3.2.1a" xref="S3.E3.m1.1.1.3.3.2.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.3.2.4" mathsize="70%" xref="S3.E3.m1.1.1.3.3.2.4.cmml">R</mi></mrow><mo id="S3.E3.m1.1.1.3.3.1" mathsize="70%" xref="S3.E3.m1.1.1.3.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.3.2" mathsize="70%" xref="S3.E3.m1.1.1.3.3.3.2.cmml">S</mi><mo id="S3.E3.m1.1.1.3.3.3.1" xref="S3.E3.m1.1.1.3.3.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.3.3.3" mathsize="70%" xref="S3.E3.m1.1.1.3.3.3.3.cmml">W</mi><mo id="S3.E3.m1.1.1.3.3.3.1a" xref="S3.E3.m1.1.1.3.3.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.3.3.4" mathsize="70%" xref="S3.E3.m1.1.1.3.3.3.4.cmml">I</mi><mo id="S3.E3.m1.1.1.3.3.3.1b" xref="S3.E3.m1.1.1.3.3.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.3.3.3.5" mathsize="70%" xref="S3.E3.m1.1.1.3.3.3.5.cmml">R</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><times id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2.1"></times><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">ğ‘</ci><ci id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3">ğ·</ci><ci id="S3.E3.m1.1.1.2.4.cmml" xref="S3.E3.m1.1.1.2.4">ğ‘€</ci><ci id="S3.E3.m1.1.1.2.5.cmml" xref="S3.E3.m1.1.1.2.5">ğ¼</ci></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><divide id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3"></divide><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><minus id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2.1"></minus><apply id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2"><times id="S3.E3.m1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.3.2.2.1"></times><ci id="S3.E3.m1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2.2">ğ‘</ci><ci id="S3.E3.m1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.3.2.2.3">ğ¼</ci><ci id="S3.E3.m1.1.1.3.2.2.4.cmml" xref="S3.E3.m1.1.1.3.2.2.4">ğ‘…</ci></apply><apply id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3"><times id="S3.E3.m1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.3.1"></times><ci id="S3.E3.m1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.3.2">ğ‘†</ci><ci id="S3.E3.m1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.3.3">ğ‘Š</ci><ci id="S3.E3.m1.1.1.3.2.3.4.cmml" xref="S3.E3.m1.1.1.3.2.3.4">ğ¼</ci><ci id="S3.E3.m1.1.1.3.2.3.5.cmml" xref="S3.E3.m1.1.1.3.2.3.5">ğ‘…</ci></apply></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><plus id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1"></plus><apply id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2"><times id="S3.E3.m1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.3.3.2.1"></times><ci id="S3.E3.m1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2">ğ‘</ci><ci id="S3.E3.m1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3">ğ¼</ci><ci id="S3.E3.m1.1.1.3.3.2.4.cmml" xref="S3.E3.m1.1.1.3.3.2.4">ğ‘…</ci></apply><apply id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3"><times id="S3.E3.m1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.3.1"></times><ci id="S3.E3.m1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.3.2">ğ‘†</ci><ci id="S3.E3.m1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3.3">ğ‘Š</ci><ci id="S3.E3.m1.1.1.3.3.3.4.cmml" xref="S3.E3.m1.1.1.3.3.3.4">ğ¼</ci><ci id="S3.E3.m1.1.1.3.3.3.5.cmml" xref="S3.E3.m1.1.1.3.3.3.5">ğ‘…</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\scriptsize NDMI=\frac{{NIR-SWIR}}{{NIR+SWIR}}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_N italic_D italic_M italic_I = divide start_ARG italic_N italic_I italic_R - italic_S italic_W italic_I italic_R end_ARG start_ARG italic_N italic_I italic_R + italic_S italic_W italic_I italic_R end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">In the context of Sentinel-2 dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib22" title="">22</a>]</cite>, preprocessing levels indicate the amount of calibration and processing applied to the raw sensor data. The level used in our work is the Level-2A, which includes radiometric, geometric, and atmospheric corrections.</p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.2">Particular emphasis was placed on addressing the prevalent issue of cloud coverage. Globally, cloud cover averages approximately 70%, with even higher percentages over oceans and near the equator regions that host the majority of the worldâ€™s mangrove ecosystems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib5" title="">5</a>]</cite>. Given that optical sensors are inherently passive, clouds can obstruct the terrestrial view in satellite imagery, either partially or completely, thereby hindering precise segmentation. To mitigate this, we employed a technique involving the creation of "cloudless mosaics" at the time of obtaining <span class="ltx_text ltx_font_italic" id="S3.p8.2.1">Sentinel-2</span> images. This technique entails composing an image from the clearest pixels of multiple images captured at the same location over a predefined period <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib19" title="">19</a>]</cite>. We selected the time frame from March to June 2020. Ultimately, we set the standard image resolution <math alttext="(W\times H)" class="ltx_Math" display="inline" id="S3.p8.1.m1.1"><semantics id="S3.p8.1.m1.1a"><mrow id="S3.p8.1.m1.1.1.1" xref="S3.p8.1.m1.1.1.1.1.cmml"><mo id="S3.p8.1.m1.1.1.1.2" stretchy="false" xref="S3.p8.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.p8.1.m1.1.1.1.1" xref="S3.p8.1.m1.1.1.1.1.cmml"><mi id="S3.p8.1.m1.1.1.1.1.2" xref="S3.p8.1.m1.1.1.1.1.2.cmml">W</mi><mo id="S3.p8.1.m1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.p8.1.m1.1.1.1.1.1.cmml">Ã—</mo><mi id="S3.p8.1.m1.1.1.1.1.3" xref="S3.p8.1.m1.1.1.1.1.3.cmml">H</mi></mrow><mo id="S3.p8.1.m1.1.1.1.3" stretchy="false" xref="S3.p8.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p8.1.m1.1b"><apply id="S3.p8.1.m1.1.1.1.1.cmml" xref="S3.p8.1.m1.1.1.1"><times id="S3.p8.1.m1.1.1.1.1.1.cmml" xref="S3.p8.1.m1.1.1.1.1.1"></times><ci id="S3.p8.1.m1.1.1.1.1.2.cmml" xref="S3.p8.1.m1.1.1.1.1.2">ğ‘Š</ci><ci id="S3.p8.1.m1.1.1.1.1.3.cmml" xref="S3.p8.1.m1.1.1.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.1.m1.1c">(W\times H)</annotation><annotation encoding="application/x-llamapun" id="S3.p8.1.m1.1d">( italic_W Ã— italic_H )</annotation></semantics></math> at <math alttext="128\times 128" class="ltx_Math" display="inline" id="S3.p8.2.m2.1"><semantics id="S3.p8.2.m2.1a"><mrow id="S3.p8.2.m2.1.1" xref="S3.p8.2.m2.1.1.cmml"><mn id="S3.p8.2.m2.1.1.2" xref="S3.p8.2.m2.1.1.2.cmml">128</mn><mo id="S3.p8.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.p8.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.p8.2.m2.1.1.3" xref="S3.p8.2.m2.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p8.2.m2.1b"><apply id="S3.p8.2.m2.1.1.cmml" xref="S3.p8.2.m2.1.1"><times id="S3.p8.2.m2.1.1.1.cmml" xref="S3.p8.2.m2.1.1.1"></times><cn id="S3.p8.2.m2.1.1.2.cmml" type="integer" xref="S3.p8.2.m2.1.1.2">128</cn><cn id="S3.p8.2.m2.1.1.3.cmml" type="integer" xref="S3.p8.2.m2.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.2.m2.1c">128\times 128</annotation><annotation encoding="application/x-llamapun" id="S3.p8.2.m2.1d">128 Ã— 128</annotation></semantics></math>, subdividing larger acquisitions into several sub-images to maintain this resolution. The final dataset, MagSet-2, comprises 10,483 pairs of spectral bands and indices, along with mangrove location annotations.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Due to hardware limitations, our benchmarking process was conducted in two phases. In the first phase, we evaluated different models from distinct categoriesâ€”including convolutional, transformers, and mambaâ€”using a reduced sample of the MagSet-2 dataset. In the second phase, the top-performing models from each category were selected for a comprehensive evaluation, where they were trained and tested on the full dataset.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">For the initial evaluation stage, we began by selecting a 5% sample of the MagSet-2 dataset. This sample was equally divided into training and testing subsets, with each comprising 50% of the data. This training data is meticulously prepared for robust mangrove segmentation through a series of strategic preparation steps: <span class="ltx_text ltx_font_bold" id="S4.p2.1.1">(i)</span> Initiation involves geographic shuffling across zones to eliminate location-based biases and promote diverse sample representation, essential for the model to generalize across varying environments. <span class="ltx_text ltx_font_bold" id="S4.p2.1.2">(ii)</span> Preprocessing includes data augmentation (vertical/horizontal flips, 90-degree rotations, random resizing crops) to introduce variability, and <span class="ltx_text ltx_font_bold" id="S4.p2.1.3">(iii)</span> normalization of image pixels to a standardized range (0-1) to address disparities in brightness and contrast. <span class="ltx_text ltx_font_bold" id="S4.p2.1.4">(iv)</span> The introduction of Gaussian noise simulates real-world disturbances, enhancing the modelâ€™s generalization capabilities. These steps culminate in a well-structured training dataset, ready for effective training and evaluation within the deep learning paradigm.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Following the preprocessing of the sampled dataset, we assess the performance of six state-of-the-art deep learning architectures: U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib20" title="">20</a>]</cite>, MANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib30" title="">30</a>]</cite>, PAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib31" title="">31</a>]</cite>, BEiT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib3" title="">3</a>]</cite>, Segformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib29" title="">29</a>]</cite>, and Swin-UMamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib15" title="">15</a>]</cite>, within the context of mangrove segmentation. These architectures were chosen based on their prominence and demonstrated effectiveness in semantic segmentation tasks.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">After identifying the most promising model from each categoryâ€”convolutional, transformers, and mambaâ€”we utilized the complete MagSet-2 dataset for further evaluation. The full dataset was similarly divided into a training set (50%) and a testing set (50%), with the same preprocessing steps applied. Finally, we retrained the three best-performing models on this data.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">This comparative analysis serves as a pivotal step towards identifying the most suitable architecture for mangrove segmentation applications, thereby advancing the state-of-the-art techniques in environmental monitoring and conservation efforts.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments and Evaluation</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Sampled MagSet-2 Dataset Evaluation</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We utilized convolutional architectures such as U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib20" title="">20</a>]</cite>, MANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib30" title="">30</a>]</cite>, and PAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib31" title="">31</a>]</cite>, all integrated with a ResNet50 encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib9" title="">9</a>]</cite> pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib21" title="">21</a>]</cite>. These differ from BEiT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib3" title="">3</a>]</cite>, which employs transformer blocks for feature extraction, and SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib29" title="">29</a>]</cite>, which uses a transformer encoder paired with a lightweight decoder for semantic segmentation. Swin-UMamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib15" title="">15</a>]</cite> relies on the Swin-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib16" title="">16</a>]</cite> architecture as its encoder.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.2">All models employed a sigmoid activation function. Hyperparameters included a uniform batch size of 32 across models, with convolutional models set at a learning rate of <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mn id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">1</mn><mo id="S5.SS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p2.1.m1.1.1.1.cmml">Ã—</mo><msup id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml"><mn id="S5.SS1.p2.1.m1.1.1.3.2" xref="S5.SS1.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p2.1.m1.1.1.3.3" xref="S5.SS1.p2.1.m1.1.1.3.3.cmml"><mo id="S5.SS1.p2.1.m1.1.1.3.3a" xref="S5.SS1.p2.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S5.SS1.p2.1.m1.1.1.3.3.2" xref="S5.SS1.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><times id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"></times><cn id="S5.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.p2.1.m1.1.1.2">1</cn><apply id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.3.1.cmml" xref="S5.SS1.p2.1.m1.1.1.3">superscript</csymbol><cn id="S5.SS1.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS1.p2.1.m1.1.1.3.2">10</cn><apply id="S5.SS1.p2.1.m1.1.1.3.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3.3"><minus id="S5.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p2.1.m1.1.1.3.3"></minus><cn id="S5.SS1.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">1 Ã— 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and both transformer and the mamba models at <math alttext="5\times 10^{-4}" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mrow id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mn id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml">5</mn><mo id="S5.SS1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><msup id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml"><mn id="S5.SS1.p2.2.m2.1.1.3.2" xref="S5.SS1.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p2.2.m2.1.1.3.3" xref="S5.SS1.p2.2.m2.1.1.3.3.cmml"><mo id="S5.SS1.p2.2.m2.1.1.3.3a" xref="S5.SS1.p2.2.m2.1.1.3.3.cmml">âˆ’</mo><mn id="S5.SS1.p2.2.m2.1.1.3.3.2" xref="S5.SS1.p2.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><times id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1"></times><cn id="S5.SS1.p2.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.p2.2.m2.1.1.2">5</cn><apply id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.3.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3">superscript</csymbol><cn id="S5.SS1.p2.2.m2.1.1.3.2.cmml" type="integer" xref="S5.SS1.p2.2.m2.1.1.3.2">10</cn><apply id="S5.SS1.p2.2.m2.1.1.3.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3"><minus id="S5.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p2.2.m2.1.1.3.3"></minus><cn id="S5.SS1.p2.2.m2.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p2.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">5\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">5 Ã— 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>. Training extended over 100 epochs, using the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib18" title="">18</a>]</cite> with a learning rate scheduling mechanism that halves the rate after every 7 epochs without improvement. The loss function used was Binary Cross-Entropy.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Our experimental design aimed to ensure a fair comparison by maintaining a consistent number of parameters across models to control for computational complexity; each model had approximately <math alttext="33" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mn id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">33</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><cn id="S5.SS1.p3.1.m1.1.1.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1">33</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">33</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">33</annotation></semantics></math>M parameters. The analysis was conducted using a single NVIDIA A100 40 GB GPU. The output from each model was a segmentation map in a single channel, indicating the probability of each pixel belonging to the mangrove class.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">We assess the performance of each model using established image segmentation evaluation metrics, complemented by a qualitative analysis of the results. We focus on the Intersection over Union (IoU), also known as the Jaccard Index, which measures the extent of overlap between the predicted masks and the actual ground truth. This metric is crucial for evaluating the precision of the segmentation, as it quantifies the relative error by considering both the area of overlap and the total area covered by both the predicted and actual masks. Other straightforward metrics used include Accuracy, which gauges the proportion of correctly classified samples, the F1-score, balancing precision and recall, and Loss, Binary Cross-Entropy, which helps in optimizing the training process.
The performance of the selected deep learning models for mangrove segmentation on <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.1">Sentinel-2</span> satellite imagery is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.T1" title="Table 1 â€£ 5.1 Sampled MagSet-2 Dataset Evaluation â€£ 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">1</span></a>. The six models, categorized into three architectural groups, were evaluated:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Convolutional models</span>: U-Net, PAN, and MAnet achieved moderate performance, with U-Net having the lowest number of parameters (32.54 million) but also the lowest IoU score (61.76%). MAnet achieved the lowest Test Loss (0.34) among the convolutional ones, while PAN stood in the middle ground between the two.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Transformer models</span>: BEiT and SegFormer, both utilizing transformer architectures, showed improved performance compared to convolutional models. SegFormer achieved the highest IoU score (72.32%) within this group, suggesting its effectiveness in capturing complex spatial relationships in mangrove imagery.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Mamba model</span>: Swin-UMamba surpassed all other models in all metrics. It achieved the highest IoU (72.87%), Accuracy (86.64%), F1-score (84.27%), and the lowest Loss (0.31). This suggests Swin-UMambaâ€™s capability to learn efficient and accurate representations of mangroves in <span class="ltx_text ltx_font_italic" id="S5.I1.i3.p1.1.2">Sentinel-2</span> data.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.4.5.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T1.4.5.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.5.1.2"># Parameters (M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T1.4.5.1.3">Performance Metrics on MagSet-2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.4.5">Method</th>
<td class="ltx_td" id="S5.T1.4.4.6"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1">IoU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">â†‘</annotation></semantics></math> (%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.2">Acc<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.2.2.2.m1.1"><semantics id="S5.T1.2.2.2.m1.1a"><mo id="S5.T1.2.2.2.m1.1.1" stretchy="false" xref="S5.T1.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><ci id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.m1.1d">â†‘</annotation></semantics></math> (%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.3">F1-score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.3.3.3.m1.1"><semantics id="S5.T1.3.3.3.m1.1a"><mo id="S5.T1.3.3.3.m1.1.1" stretchy="false" xref="S5.T1.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><ci id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.m1.1d">â†‘</annotation></semantics></math> (%)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.4.4">Loss<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.4.4.4.m1.1"><semantics id="S5.T1.4.4.4.m1.1a"><mo id="S5.T1.4.4.4.m1.1.1" stretchy="false" xref="S5.T1.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.m1.1b"><ci id="S5.T1.4.4.4.m1.1.1.cmml" xref="S5.T1.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.4.6.2.1">U-Net</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.6.2.2">32.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.6.2.3">61.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.6.2.4">78.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.6.2.5">76.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.6.2.6">0.47</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.7.3.1">PAN</th>
<td class="ltx_td ltx_align_center" id="S5.T1.4.7.3.2">34.79</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.7.3.3">64.44</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.7.3.4">81.16</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.7.3.5">78.32</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.7.3.6">0.41</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.8.4.1">MAnet</th>
<td class="ltx_td ltx_align_center" id="S5.T1.4.8.4.2">33.38</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.8.4.3">71.75</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.8.4.4">85.80</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.8.4.5">83.51</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.8.4.6">0.34</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.4.9.5.1">BEiT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.9.5.2">33.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.9.5.3">70.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.9.5.4">85.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.9.5.5">82.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.9.5.6">0.48</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.10.6.1">SegFormer</th>
<td class="ltx_td ltx_align_center" id="S5.T1.4.10.6.2">34.63</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.10.6.3">72.32</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.10.6.4">86.13</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.10.6.5">83.91</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.10.6.6">0.42</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T1.4.11.7.1">Swin-UMamba</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.4.11.7.2">32.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.4.11.7.3"><span class="ltx_text ltx_font_bold" id="S5.T1.4.11.7.3.1">72.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.4.11.7.4"><span class="ltx_text ltx_font_bold" id="S5.T1.4.11.7.4.1">86.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.4.11.7.5"><span class="ltx_text ltx_font_bold" id="S5.T1.4.11.7.5.1">84.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.4.11.7.6"><span class="ltx_text ltx_font_bold" id="S5.T1.4.11.7.6.1">0.31</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance Comparison on Sampled MagSet-2 Dataset.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">The Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.F3" title="Figure 3 â€£ 5.1 Sampled MagSet-2 Dataset Evaluation â€£ 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">3</span></a> presents the comparative analysis across the loss, F1-score and IoU metrics tracked over a training period of 100 epochs. In this context, to evaluating model performance on unseen data, the F1-score is crucial for understanding precision and recall balance (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.F3" title="Figure 3 â€£ 5.1 Sampled MagSet-2 Dataset Evaluation â€£ 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">3</span></a>, center). Swin-UMambaâ€™s performance plateaued at the highest F1-score, reflecting consistent and superior segmentation accuracy. The remaining models displayed improvement over the training epochs, with SegFormer trailing closely behind Swin-UMamba, signifying the effectiveness of transformer-based architectures in handling the complexity of mangrove segmentation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">The Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.F4" title="Figure 4 â€£ 5.1 Sampled MagSet-2 Dataset Evaluation â€£ 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates a side-by-side comparison of segmentation results for mangrove areas from satellite imagery. The visual comparison suggests that the convolutional group generates predictions with limited granularity and detail, compared to the ground-truth, whereas transformer models produce excessively noisy Mangrove prediction masks with overly high levels of detail, leading to increased errors. However, Swin-UMamba offers a superior balance between detail and prediction accuracy, delivering predictions that are more detailed than those from convolutional models, without indicating overfitting. These findings suggest that while convolutional models like U-Net offer reasonable performance with fewer parameters, transformer models like SegFormer and the newly introduced Swin-UMamba demonstrate superior capabilities in capturing the intricate characteristics of mangroves in satellite imagery. Notably, Swin-UMambaâ€™s superior performance across all metrics indicates its potential as a promising approach for accurate and efficient mangrove segmentation tasks.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="191" id="S5.F3.g1" src="extracted/5893977/img/final_metrics_results_plot.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparative performance on sampled test of MagSet-2, using Training Set Loss (left), Test Set F1 Score (center), and Test Set Intersection over Union (IoU) (right). Each line represents a model: U-Net (neon blue), PAN (red), MANet (black), BEiT (green), SegFormer (yellow), and Swin-UMamba (dark blue) trained over 100 epochs. Lower loss values, higher F1 and IoU values indicate better performance. Swin-UMamba consistently shows superior performance over all metrics.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="S5.F4.g1" src="extracted/5893977/img/comparison_segmentation.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparative visual segmentation results of mangrove areas. The first column shows the original satellite images, the second column depicts the ground truth segmentation, and the subsequent columns display the segmentation results from U-Net, PAN, MANet, BEiT, SegFormer, and Swin-UMamba models, resp.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Complete MagSet-2 Dataset Evaluation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In the final evaluation, using the full version of the MagSet-2 dataset, we performed a comprehensive benchmark of the top-performing models across three categories: convolutional-based (MAnet), transformer-based (SegFormer), and Mamba-based (Swin-UMamba). The models were evaluated under the same training configurations as in the previous experiments, but with the complete dataset. The results, summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.T2" title="Table 2 â€£ 5.2 Complete MagSet-2 Dataset Evaluation â€£ 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">2</span></a>, indicate that Swin-UMamba was the top performer across all metrics, achieving an Intersection over Union (IoU) of 76.90% and an F1-score of 86.91%, while maintaining the lowest loss value of 0.28. These results further highlight Swin-UMambaâ€™s robustness and adaptability in handling MagSet-2 the dataset with high precision.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.4.5.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T2.4.5.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.5.1.2"># Parameters (M)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S5.T2.4.5.1.3">Performance Metrics on MagSet-2</th>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S5.T2.4.4.5">Method</th>
<th class="ltx_td ltx_th ltx_th_column" id="S5.T2.4.4.6"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1">IoU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">â†‘</annotation></semantics></math> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.2.2">Acc<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.m1.1"><semantics id="S5.T2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.m1.1.1" stretchy="false" xref="S5.T2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><ci id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.m1.1d">â†‘</annotation></semantics></math> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.3.3">F1-score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.m1.1.1" stretchy="false" xref="S5.T2.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.m1.1d">â†‘</annotation></semantics></math> (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4">Loss<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.4.4.4.m1.1"><semantics id="S5.T2.4.4.4.m1.1a"><mo id="S5.T2.4.4.4.m1.1.1" stretchy="false" xref="S5.T2.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.4.6.1.1">MAnet</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.6.1.2">33.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.6.1.3">76.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.6.1.4">87.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.6.1.5">86.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.6.1.6">0.29</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.7.2.1">SegFormer</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.7.2.2">34.63</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.7.2.3">75.37</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.7.2.4">86.61</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.7.2.5">85.92</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.7.2.6">0.35</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.4.8.3.1">Swin-UMamba</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.8.3.2">32.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.8.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.4.8.3.3.1">76.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.8.3.4"><span class="ltx_text ltx_font_bold" id="S5.T2.4.8.3.4.1">87.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.8.3.5"><span class="ltx_text ltx_font_bold" id="S5.T2.4.8.3.5.1">86.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.8.3.6"><span class="ltx_text ltx_font_bold" id="S5.T2.4.8.3.6.1">0.28</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance Comparison on Full MagSet-2 Dataset.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">The MAnet model also exhibited strong performance, closely matching Swin-UMamba with an IoU of 76.60% and an F1-score of 86.71%. MAnetâ€™s ability to efficiently manage a large dataset underscores the continued relevance of convolutional architectures in segmentation tasks. By contrast, SegFormer, which initially demonstrated superior performance during the subsample evaluations, experienced a modest decline when applied to the full dataset, achieving an IoU of 75.37% and an F1-score of 85.92%. This change in relative performance between SegFormer and MAnet suggests that SegFormer, despite the typical advantage of transformer-based models on larger datasets, may not have fully realized its potential in this scenario.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#S5.F5" title="Figure 5 â€£ 5.2 Complete MagSet-2 Dataset Evaluation â€£ 5 Experiments and Evaluation â€£ A Deep Learning-Based Approach for Mangrove Monitoring"><span class="ltx_text ltx_ref_tag">5</span></a>, both the central and right-hand charts, which represent the F1-score and IoU respectively, indicate that the SegFormer curve continues to rise. This suggests that the model did not fully converge within the 100 epochs established in the experiment. Transformer-based models, such as SegFormer, generally require more time to converge compared to convolutional models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05443v1#bib.bib28" title="">28</a>]</cite>. In this context, it is plausible that SegFormer required more than the 100 epochs used in this experiment to reach its optimal performance. This reinforces the notion that, although transformer models are often more suitable for larger datasets, SegFormer might have needed additional epochs or further hyperparameter tuning to improve generalization. Thus, while transformer-based models offer significant potential, they may require careful adjustment to prevent underperformance, particularly when working with larger and more complex datasets.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="191" id="S5.F5.g1" src="extracted/5893977/img/final_model_comparison_full.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparative performance on complete MagSet-2, using Training Set Loss (left), Test Set F1 Score (center), and Test Set Intersection over Union (IoU) (right). Each line represents a model: MANet (black), SegFormer (yellow), and Swin-UMamba (dark blue) trained over 100 epochs. Lower loss values, higher F1 and IoU values indicate better performance. Again, Swin-UMamba consistently shows superior performance over all metrics.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This study embarked on a comprehensive evaluation of various deep learning models for the segmentation of mangroves from satellite imagery, focusing on six architectures: U-Net, MANet, PAN, BEiT, SegFormer, and Swin-UMamba. This comparative analysis revealed that the Swin-UMamba model notably outperforms established convolutional and transformer architectures. Demonstrating superior performances and generalization in semantic segmentation, Swin-UMambaâ€™s advancements offer significant implications for ecological monitoring and the conservation of mangrove ecosystems. These results affirm the potential of leveraging state-of-the-art deep learning techniques in environmental remote sensing, providing a robust foundation for enhancing our understanding the dynamics of these critical habitats on a global scale.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgments</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We would like to express our sincere gratitude to <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">CÃ©line Hudelot</span> from the <span class="ltx_text ltx_font_italic" id="S7.p1.1.2">Laboratoire MathÃ©matiques et Informatique pour la ComplexitÃ© et les SystÃ¨mes (MICS)</span>, from <span class="ltx_text ltx_font_italic" id="S7.p1.1.3">Ã‰cole CentraleSupÃ©lec - Paris-Saclay University</span>, for her invaluable support and guidance throughout this work. Besides, this study was financed in part by the <span class="ltx_text ltx_font_italic" id="S7.p1.1.4">CoordenaÃ§Ã£o de AperfeiÃ§oamento de Pessoal de NÃ­vel Superior â€“ Brasil (CAPES)</span> â€“ Finance Code 001.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Anh, D.T.N., Tran, H.D., Ashley, M., Nguyen, A.T.: Monitoring landscape fragmentation and aboveground biomass estimation in can gio mangrove biosphere reserve over the past 20 years. Ecological Informatics <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">70</span>, 101743 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: A deep convolutional encoder-decoder architecture for image segmentation. arXiv:1511.00561 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv:2106.08254 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Behera, M.D., Barnwal, S., Paramanik, S., Das, P., Bhattyacharya, B.K., Jagadish, B., Roy, P.S., Ghosh, S.M., Behera, S.K.: Species-level classification and mapping of a mangrove forest using random forest utilisation of aviris-ng and sentinel data. Remote Sensing <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">13</span> (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Brun, P., Zimmermann, N.E., Hari, C., Pellissier, L., Karger, D.N.: Global climate-related predictors at kilometer resolution for the past and future. Earth System Science Data <span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">14</span>(12), 5573â€“5603 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bunting, P., Rosenqvist, A., Hilarides, L., Lucas, R.M., Thomas, N., Tadono, T., Worthington, T.A., Spalding, M., Murray, N.J., Rebelo, L.M.: Global mangrove extent change 1996â€“2020: Global mangrove watch version 3.0. Remote Sensing <span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">14</span>(15) (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Guo, M., Yu, Z., Xu, Y., Huang, Y., Li, C.: Me-net: A deep convolutional neural network for extracting mangrove using sentinel-2a data. Remote Sensing <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">13</span>, Â 1292 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE conference on computer vision and pattern recognition. pp. 770â€“778 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
He, S., Lu, X., Zhang, S., Li, S., Tang, H.T., Zheng, W., Lin, H., Luo, Q.: Research on classification algorithm of wetland land cover in the linhong estuary, jiangsu province. Marine Science <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">44</span>, 44â€“53 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hoff, R., Hensel, P., Proffitt, E., Delgado, P., Shigenaka, G., Yender, R., Mearns, A.: Oil spills in mangroves: Planning &amp; response considerations (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Houborg, R., Soegaard, H., Boegh, E.: Combining vegetation index and model inversion methods for the extraction of key vegetation biophysical parameters using terra and aqua modis reflectance data. Remote Sensing of Environment <span class="ltx_text ltx_font_bold" id="bib.bib12.1.1">106</span>, 39â€“58 (2007)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Iovan, C., Kulbicki, M., Mermet, E.: Deep convolutional neural network for mangrove mapping. In: IEEE International Geoscience and Remote Sensing Symposium (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. Communications of the ACM <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">60</span>, 84â€“90 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Liu, J., Yang, H., Zhou, H.Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang, S., Zheng, H., Wang, S.: Swin-umamba: Mamba-based unet with imagenet-based pretraining. arXiv:2402.03302 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint 2103.14030 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence <span class="ltx_text ltx_font_bold" id="bib.bib17.1.1">39</span>, 640â€“651 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
PitkÃ¤nen, T.P., Balazs, A., Tuominen, S.: Automatized sentinel-2 mosaicking for large area forest mapping. International Journal of Applied Earth Observation and Geoinformation <span class="ltx_text ltx_font_bold" id="bib.bib19.1.1">127</span>, 103659 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., etÂ al.: Imagenet large scale visual recognition challenge. Int. J. Comput. Vis. <span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">115</span>, 211â€“252 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Sinergise Solutions: Sentinel hub. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sentinel-hub.com" title="">https://www.sentinel-hub.com</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Tran, T.V., Reef, R., Zhu, X.: A review of spectral indices for mangrove remote sensing. Remote Sensing <span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">14</span>(19) (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Valderrama, L., Flores-Verdugo, F., RodrÃ­guez-Sobreyra, R., Kovacs, J., Flores-de Santiago, F.: Extrapolating canopy phenology information using sentinel-2 data and the google earth engine platform to identify the optimal dates for remotely sensed image acquisition of semiarid mangroves. Journal of Environmental Management <span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">279</span>, 111617 (11 2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Vidhya, R., Vijayasekaran, D., Farook, M.A., Jai, S., Rohini, M., Sinduja, A.: Improved classification of mangroves health status using hyperspectral remote sensing data. Photogrammetry, Remote Sensing and Spatial Information Sciences Journal <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">40</span>, 667â€“670 (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Woltz, V., Peneva-Reed, E., Zhu, Z., Bullock, E., Mackenzie, R., Apwong, M., Krauss, K., Gesch, D.: A comprehensive assessment of mangrove species and carbon stock on pohnpei, micronesia. PloS one <span class="ltx_text ltx_font_bold" id="bib.bib27.1.1">17</span> (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Xiao, T., Singh, M., Mintun, E., Darrell, T., DollÃ¡r, P., Girshick, R.: Early convolutions help transformers see better (2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2106.14881" title="">https://arxiv.org/abs/2106.14881</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer: Simple and efficient design for semantic segmentation with transformers. arXiv:2105.15203 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Zhang, H., Wu, C., Zhang, Z., Zhu, L.: Manet: A multi-attention network for semantic segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 5790â€“5798 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Zhao, H., Jiang, Y., Zhong, X., Li, L.: Pyramid attention network for semantic segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 10575â€“10584 (2018)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 19:13:03 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
