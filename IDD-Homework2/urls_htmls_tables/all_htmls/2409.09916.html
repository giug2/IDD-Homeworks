<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>\ourmodel: Towards Contextually Faithful LLMs</title>
<!--Generated on Mon Sep 16 01:06:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.09916v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S1" title="In \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S2" title="In \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_ERROR undefined">\ourmodel</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S2.SS1" title="In 2 \ourmodel ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span><span class="ltx_ERROR undefined">\ourmodel</span>Chat Template</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S2.SS2" title="In 2 \ourmodel ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>SFR-RAG Fine-tuning Process</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3" title="In \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.SS1" title="In 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Contextual Evaluation Suite - ContextualBench</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.SS1.SSS0.Px1" title="In 3.1 Contextual Evaluation Suite - ContextualBench ‣ 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title">Dataset Specific Settings.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.SS2" title="In 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Experimental Results on ContextualBench</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.SS3" title="In 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Resilience to Unanswerable, Conflicting and Counterfactual Contexts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.SS4" title="In 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Standard Benchmarks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S4" title="In \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#A1" title="In \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_ERROR undefined" id="id1.id1">\ourmodel</span>: Towards Contextually Faithful LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuan-Phi Nguyen  &amp;Shrey Pandit  &amp;Senthil Purushwalkam  &amp;Austin Xu   &amp;Hailin Chen  &amp;Yifei Ming  &amp;Zixuan Ke  &amp;Silvio Savarese  &amp;Caiming Xong &amp;Shafiq Joty 
<br class="ltx_break"/>
<br class="ltx_break"/>Salesforce AI Research
</span><span class="ltx_author_notes">Corresponding author: <a class="ltx_ref ltx_href" href="mailto:xnguyen@salesforce.com" title="">xnguyen@salesforce.com</a></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Retrieval Augmented Generation (RAG), a paradigm that integrates external contextual information with large language models (LLMs) to enhance factual accuracy and relevance, has emerged as a pivotal area in generative AI. The LLMs used in RAG applications are required to faithfully and completely comprehend the provided context and users’ questions, avoid hallucination, handle unanswerable, counterfactual or otherwise low-quality and irrelevant contexts, perform complex multi-hop reasoning and produce reliable citations. In this paper, we introduce <span class="ltx_ERROR undefined" id="id2.id1.1">\ourmodel</span>, a small LLM that is instruction-tuned with an emphasis on context-grounded generation and hallucination minimization. We also present ContextualBench, a new evaluation framework compiling multiple popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with consistent RAG settings to ensure reproducibility and consistency in model assessments. Experimental results demonstrate that our <span class="ltx_ERROR undefined" id="id2.id1.2">\ourmodel</span>-9B model outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with significantly fewer parameters. The model is also shown to be resilient to alteration in the contextual information and behave appropriately when relevant context is removed. Additionally, the <span class="ltx_ERROR undefined" id="id2.id1.3">\ourmodel</span>model maintains competitive performance in general instruction-following tasks and function-calling capabilities.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="467" id="S1.F1.g1" src="x1.png" width="415"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Our <span class="ltx_ERROR undefined" id="S1.F1.8.1">\ourmodel</span>-9B model exhibits strong overall performance on an ContextualBench, our comprehensive evaluation suite of seven contextual tasks under a standardized setup. Notably, <span class="ltx_ERROR undefined" id="S1.F1.9.2">\ourmodel</span>achieves state-of-the-art performance on three of seven tasks, with extremely competitive performance on the rest, despite having far fewer parameters than competitive baselines.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval Augmented Generation (RAG) has recently garnered significant attention as one of the most prominent areas of research in generative AI <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib54" title="">54</a>]</cite>, driven by the latest advancements in foundational large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib2" title="">2</a>]</cite>.
RAG frameworks are well-suited for solving knowledge-dependent problems or questions, where external contextual information is provided and the generated answer is expected to be <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">factually grounded</em> on the contextual cues.
In practice, the RAG setup is designed such that a generator LLM works in tandem with a knowledge retriever. The retriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib21" title="">21</a>]</cite> is tasked with retrieving passages relevant to a given query from a database of documents (potentially the entire internet). The LLM interacts with users, formulates queries for the retriever to gather knowledge, and finally answers users’ questions.
To retrieve the most accurate context information, the retriever typically relies on an embedding model <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib21" title="">21</a>]</cite>, and optionally employs a re-ranker to get a refined list of context documents <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib24" title="">24</a>]</cite>. Recent research has also led to the development of more sophisticated RAG frameworks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib45" title="">45</a>]</cite> that involve multiple inference steps to improve the reliability of answers.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this work, we focus our efforts on the generator LLM component of the RAG framework. Traditional general-purpose LLMs trained for chat often struggle when directly applied to the RAG framework. This can be attributed to several potential factors, including:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">The knowledge in the context obtained from the retriever might conflict with the training data used for the LLM.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The LLM is not trained to deal with conflicting or redundant facts from the retriever.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">In scenarios where the retrieved knowledge is insufficient, the LLMs revert to answering questions based on its training data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">It may also fail to provide adequate citations or to call appropriate functions and parameters to retrieve appropriate contexts in an agentic environment <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib50" title="">50</a>]</cite>, in which the model may use provided functions or tools to perform tasks.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S1.p2.2">Recent attempts have focused on training LLMs specifically tuned to succeed in the RAG framework, such as <a class="ltx_ref ltx_href" href="https://cohere.com/blog/command-r" title="">Command-R(+)</a> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib36" title="">36</a>]</cite> and <a class="ltx_ref ltx_href" href="https://contextual.ai/introducing-rag2" title="">RAG-2.0</a> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib37" title="">37</a>]</cite>.
Such RAG-specific LLMs not only serve as the foundation for generating up-to-date and factual AI responses, but also enable quick adoption in different domains, avoid the need to increase model capacity, context length or fine-tune an LLM on potentially proprietary data.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we introduce <span class="ltx_ERROR undefined" id="S1.p3.1.1">\ourmodel</span><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The model will be made available via API and later fully open-sourced.</span></span></span>, a 9-billion-parameter language model trained with a significant emphasis on reliable, precise and faithful contextual generation abilities specific to RAG and relevant agentic tasks. Beyond contextual tasks, <span class="ltx_ERROR undefined" id="S1.p3.1.2">\ourmodel</span> is also trained to serve as a competitive AI assistant in regular tasks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib7" title="">7</a>]</cite>. We develop a comprehensive recipe, on both data synthesis and training procedures, to train the base LLM so that it is familiar and adaptable to diverse real-life RAG use cases. This includes precise factual knowledge extraction, distinguishing relevant against distracting contexts, citing appropriate sources along with answers, producing complex and multi-hop reasoning over multiple contexts, consistent format following, as well as refraining from hallucination over unanswerable queries. <span class="ltx_ERROR undefined" id="S1.p3.1.3">\ourmodel</span>is also equipped with function calling and agentic abilities, which enable it to proactively search for knowledge from external tools, as well as conduct complex inference and reasoning strategies similar to Self-RAG <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib1" title="">1</a>]</cite>, ReAct <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib50" title="">50</a>]</cite> and alike <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib34" title="">34</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">There is limited well established evaluation standards for measuring progress in contextual comprehension qualities of LLMs. It is worth noting that Command-R(+) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib36" title="">36</a>]</cite> and RAG-2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib37" title="">37</a>]</cite> evaluated their proposed models on non-overlapping metrics <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib23" title="">23</a>]</cite> with inconsistent or undisclosed setups, which causes difficulties in aligning results and comparisons across different studies. To reliably evaluate our <span class="ltx_ERROR undefined" id="S1.p4.1.1">\ourmodel</span>model as well as other well-known baselines, in this work, we also introduce ContextualBench <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/Salesforce/ContextualBench" title="">https://huggingface.co/datasets/Salesforce/ContextualBench</a></span></span></span>, which is a compilation of many popular RAG and contextual benchmarks, such as HotpotQA and TriviaQA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib18" title="">18</a>]</cite>, that standardizes the evaluation setup leading to consistent and reproducible evaluation results. In experiments, we show that our <span class="ltx_ERROR undefined" id="S1.p4.1.2">\ourmodel</span>-9B model is both a well-rounded and high performing model, achieving state-of-the-art performance in three of the seven benchmarks in ContextualBench; see <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S1.F1" title="In 1 Introduction ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> for a preview of our results. <span class="ltx_ERROR undefined" id="S1.p4.1.3">\ourmodel</span>-9B outperforms or is competitive with GPT-4o <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib30" title="">30</a>]</cite> on all tasks in ContextualBench. It also outperforms powerful contextual models, such as Command-R+ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib36" title="">36</a>]</cite>, on a variety of tasks despite having 10 times fewer parameters. Compared to comparable baselines, our model is also shown to be resilient to factual alterations and unanswerability tests in the context. Lastly, despite being trained with a focus on RAG and contextual applications, our model is still competitive as a regular instruction-tuned LLM, with strong and comparable performances in standard benchmarks like MMLU or GSM8K <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib6" title="">6</a>]</cite>, as well as function-calling ability <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib48" title="">48</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_ERROR undefined" id="S2.1.1">\ourmodel</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we provide more insights into <span class="ltx_ERROR undefined" id="S2.p1.1.1">\ourmodel</span>. First, we introduce a novel chat template comprising two new chat roles with specific functions (<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S2.SS1" title="2.1 \ourmodelChat Template ‣ 2 \ourmodel ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>). Then, we briefly discuss the training process of <span class="ltx_ERROR undefined" id="S2.p1.1.2">\ourmodel</span>(<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S2.SS2" title="2.2 SFR-RAG Fine-tuning Process ‣ 2 \ourmodel ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>).</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span class="ltx_ERROR undefined" id="S2.SS1.1.1">\ourmodel</span>Chat Template</h3>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S2.F2.g1" src="extracted/5856846/img/thought_observation3.png" width="389"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.8.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.9.2" style="font-size:90%;">Example of the chat format used by <span class="ltx_ERROR undefined" id="S2.F2.9.2.1">\ourmodel</span>, with additional <span class="ltx_text ltx_font_typewriter" id="S2.F2.9.2.2">Thought</span> and <span class="ltx_text ltx_font_typewriter" id="S2.F2.9.2.3">Observation</span> turns (roles). The former indicates the model’s “inner” thought or reasoning, actions and tool use syntax that are not typically meant to be shown to users. The latter indicates all external information retrieved and returned by performing a search or function call. The <span class="ltx_text ltx_font_typewriter" id="S2.F2.9.2.4">Assistant</span> turn, therefore, is relieved to only be responsible to generate user-friendly responses. During training, <span class="ltx_text ltx_font_typewriter" id="S2.F2.9.2.5">Thought</span> and <span class="ltx_text ltx_font_typewriter" id="S2.F2.9.2.6">Assistant</span> turns are trained while the others are masked out.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Most instruction-tuned language models often feature a chat template that allows for three conversational roles: (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">i</span>) <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.2">System</span> role, typically specified once at the beginning, is used to define the general characteristics of the AI assistant with general instructions on how to respond to user inputs, (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">ii</span>) <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.4">User</span> role specifies where user messages reside, and (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.5">iii</span>) <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.6">Assistant</span> turn is where the model responds to the user’s query in accordance to the guidelines given by the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.7">System</span> turn.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">However, as more complex applications with (potentially multi-step) retrieval or function calling are being employed, such roles may have to handle increasingly complex and confusing data formats. For example, in retrieval tasks, external context information may be injected into the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.1">System</span> or <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.2">User</span> turn, or may even form a part of the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.3">Assistant</span> turn if the context is retrieved following a model’s function call <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib50" title="">50</a>]</cite>. This may cause confusion and distraction from the actual instruction or question queried by the user in the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.4">User</span> turn. In other words, there have been no generally agreed-upon position to store the contextual information in such tasks.
In another example of agentic function calling tasks, the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.5">Assistant</span> turn has to produce responses that use specific tool syntax and expects to receive the function call’s results following that <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib50" title="">50</a>]</cite>. This makes the fine-tuning process tricky as the function’s results, which are part of <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.6">Assistant</span> turn and typically contain answer clues, need to be masked out from the loss to prevent memorization <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib40" title="">40</a>]</cite>. Furthermore, for certain applications, practitioners may prefer to hide the model’s reasoning, or intermediate actions invoked by the model from the user and only show user-friendly responses. Having all data processing steps enclosed within the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.7">Assistant</span> turn may hinder such use cases.
Reliability is also an issue as the model may fail to produce specific key words consistently in inference strategies like ReAct <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib50" title="">50</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>ReAct uses arbitrarily literal phrases like “Thought:”, “Result:” and “Final Answer:” within the <span class="ltx_text ltx_font_typewriter" id="footnote3.1">Assistant</span> turn to parse reasoning, tool outputs and answers respectively, which the LLM may not always comply with.</span></span></span>
Furthermore, the ambiguities in roles, functions and privileges may lead to the model failing to comply with the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.8">System</span> prompt and surrendering to jailbreaks injected via the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p2.1.9">User</span> turn or tool outputs <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib44" title="">44</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">To overcome such complexities, we introduce two more optional roles (turns) in the conversational template for <span class="ltx_ERROR undefined" id="S2.SS1.p3.1.1">\ourmodel</span>: <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.2">Thought</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.3">Observation</span>. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S2.F2" title="In 2.1 \ourmodelChat Template ‣ 2 \ourmodel ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.4">Observation</span> role is designated to house any contextual information acquired from external sources, which can be either retrieved documents for retrieval use case or function call’s results in agentic tool use scenario. Meanwhile, the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.5">Thought</span> role is designed for the model to speak out any internal reasoning, or tool use syntax to invoke certain function calls. The benefits of this change are manifold. First, it allows easy masking during training. Specifically, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.6">System</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.7">User</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.8">Observation</span> turns may not be trainable as they are input information for the model to generate responses. Like <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.9">Assistant</span>, on the other hand, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.10">Thought</span> turns are to be included in the fine-tuning loss to train the model to produce such “thoughts”. Second, the separation and clarification of roles facilitate instruction hierarchy enforcement <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib44" title="">44</a>]</cite>, making LLMs safer by ensuring them to respect the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.11">System</span> prompt and refuse to follow malicious instructions injected in <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.12">User</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.13">Observation</span> turns. Third, the additional roles streamline the process of building reliable and secure RAG and agentic applications, by allowing developers to display or hide internal data processing steps, and avoid having to parse customized key words from the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.14">Assistant</span> output.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>SFR-RAG Fine-tuning Process</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">One of the most important goals of <span class="ltx_ERROR undefined" id="S2.SS2.p1.1.1">\ourmodel</span>is to make full use and complete comprehension of any provided contextual information in the real-world RAG scenarios. This trait includes many capabilities, among which are (<span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">i</span>) extracting relevant information from arbitrary long contexts, (<span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">ii</span>) recognizing the lack of relevant information and abstaining from hallucinated generation, (<span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.4">iii</span>) recognizing potential conflicting information in contextual passages, and (<span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.5">iv</span>) being resilient to distracting, counter-intuitive information or contents that are out-of-distribution from the pre-training process. We fine-tuned SFR-RAG via standard supervised fine-tuning and preference learning <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib31" title="">31</a>]</cite>, using extensive instruction-following data that mimic real-world retrieval question answering applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Contextual Evaluation Suite - ContextualBench</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">There are already several evaluation protocols available to measure performance of LLMs and RAG systems on contextual understanding across different domains and complexities <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib18" title="">18</a>]</cite>. However, prior studies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib50" title="">50</a>]</cite> have reported results on non-overlapping measures, datasets and inconsistent setups, especially on which contextual content to present to the LLMs and model hyper-parameters. This causes challenges in directly comparing results from different studies.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To offer a better common ground, we propose <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">ContextualBench</span>, which is primarily an aggregation of 7 popular contextual question answering tasks, namely HotpotQA, TriviaQA, TruthfulQA, PopQA, 2WikiHopQA, Musique and Natural Questions (NQ) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib18" title="">18</a>]</cite>. There are a few key contributions that make ContextualBench stand out as a comprehensive benchmarking framework for contextual LLMs:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The measures in ContextualBench are evaluated under the same instruction with contextual contents being consistently specified. The contextual contents include the original context documents of each benchmark if provided, or otherwise they are retrieved from a much larger Wikipedia database with an embedding model of choice.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">As it is not trivial to evaluate assistant-style LLMs with a wide range of verbosity in the generated output, ContextualBench offers multiple scoring methods to account for variations in answers compared to the ground truths. These scoring methods are (<span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">i</span>) Exact Match (EM) of the generated answer with the ground truth, (<span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.2">ii</span>) Easy match (EasyM) to check if the ground truth is in the generated answer, and (<span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.3">iii</span>) the F1 score, with room for further addition. The Appendix provides the complete results for all of these metrics.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">ContextualBench offers multiple setups common in the RAG scenarios, among which is whether to retrieve top-k chunks using consistent embedding models or feed the entire available contextual documents directly to the LLM (no retrieval needed).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">The variety of measures and tasks in ContextualBench enables both holistic and specific evaluation of contextual LLMs. By weighing each task and measure equally, ContextualBench allows for direct comparison of model performance in general. On the other hand, depending on practitioner use-case and domain specifications, certain measures or datasets may be prioritized, allowing for quick identification of the best task-specific models.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Dataset Specific Settings.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">For 2WikiHopQA, HotpotQA and Musique, the context documents are already provided for each question, so we use them directly as contextual sources. For TriviaQA, TruthfulQA, and NQ, the questions come with their respective Wikipedia article or source URL. We scraped the web content from these sources and used Cohere embedding <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib35" title="">35</a>]</cite> to retrieve top-10 chunks from the contextual sources where each chunk is 512 tokens long. Meanwhile, PopQA itself does not come with context documents, so we make use of the off-the-shelf context documents produced by the Self-RAG retriever <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib1" title="">1</a>]</cite>. For each task, we use the test set if they are complete with gold labels, otherwise we use the entire validation set to measure models’ performance. This is different from Command-R’s report <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib36" title="">36</a>]</cite>, where HotpotQA evaluation was conducted on a 100-sample subset of validation set, with no details about the context documents disclosed.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p2.1">Note that ContextualBench contains popular existing benchmarks, such as TriviaQA and TruthfulQA, where evaluation utilizes certain contexts to which models are expected to be faithful. That is, models are expected to utilize only the information found in such contexts, in contrast to traditional closed-book QA settings, where the parametric knowledge of LLMs are evaluated sans provided contexts. In other words, the presence of these contexts may cause the scores to differ significantly.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental Results on ContextualBench</h3>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Performances of <span class="ltx_ERROR undefined" id="S3.T1.4.2.1">\ourmodel</span>-9B and various open- and closed-source baselines across 7 contextual question answering tasks in ContextualBench. Bold numbers mean best of all, while underlined numbers mean best among open-source models. PopQA is measured in easy-match accuracy (EasyEM), while the rest are measured in exact-match accuracy (EM). The Appendix presents the full results in metrics.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.5" style="width:433.6pt;height:165.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.4pt,16.5pt) scale(0.833081854543261,0.833081854543261) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T1.5.1.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.2.1">TriviaQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.3.1">TruthfulQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.4.1">2WikiHopQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.1.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.5.1">MuSiQue</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.1.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.6.1">NQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.1.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.7.1">PopQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.5.1.1.1.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.8.1">HotpotQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.5.1.1.1.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.1.1.9.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.5.1.2.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4o</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.2.2.2.1">81.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">76.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">62.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.2.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.2.2.5.1">44.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.2.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.1.2.2.6.1">51.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.2.7" style="padding-left:5.0pt;padding-right:5.0pt;">42.24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.5.1.2.2.8" style="padding-left:5.0pt;padding-right:5.0pt;">57.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.2.2.9" style="padding-left:5.0pt;padding-right:5.0pt;">59.47</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.1.3.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT 4 Turbo</th>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">78.34</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">76.13</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.3.4" style="padding-left:5.0pt;padding-right:5.0pt;">59.90</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.3.5" style="padding-left:5.0pt;padding-right:5.0pt;">37.10</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">48.23</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.3.7" style="padding-left:5.0pt;padding-right:5.0pt;">53.82</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.3.3.8" style="padding-left:5.0pt;padding-right:5.0pt;">54.20</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.3.3.9" style="padding-left:5.0pt;padding-right:5.0pt;">58.25</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.1.4.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4o-mini</th>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">72.55</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.4.3" style="padding-left:5.0pt;padding-right:5.0pt;">61.02</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">46.60</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">32.10</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.4.6" style="padding-left:5.0pt;padding-right:5.0pt;">30.75</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">54.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.4.4.8" style="padding-left:5.0pt;padding-right:5.0pt;">49.80</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.4.4.9" style="padding-left:5.0pt;padding-right:5.0pt;">49.65</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.1.5.5.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT 3.5 Turbo</th>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">77.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.5.3" style="padding-left:5.0pt;padding-right:5.0pt;">47.00</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.5.4" style="padding-left:5.0pt;padding-right:5.0pt;">41.20</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.5.5" style="padding-left:5.0pt;padding-right:5.0pt;">22.60</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.5.6" style="padding-left:5.0pt;padding-right:5.0pt;">40.40</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.5.7" style="padding-left:5.0pt;padding-right:5.0pt;">53.39</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.5.5.8" style="padding-left:5.0pt;padding-right:5.0pt;">48.10</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.5.5.9" style="padding-left:5.0pt;padding-right:5.0pt;">47.16</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.5.1.6.6.1" style="padding-left:5.0pt;padding-right:5.0pt;">Command R (35B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.6.6.2" style="padding-left:5.0pt;padding-right:5.0pt;">71.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.6.6.3" style="padding-left:5.0pt;padding-right:5.0pt;">51.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.6.6.4" style="padding-left:5.0pt;padding-right:5.0pt;">41.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.6.6.5" style="padding-left:5.0pt;padding-right:5.0pt;">14.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.6.6.6" style="padding-left:5.0pt;padding-right:5.0pt;">35.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.6.6.7" style="padding-left:5.0pt;padding-right:5.0pt;">52.60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.5.1.6.6.8" style="padding-left:5.0pt;padding-right:5.0pt;">48.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.1.6.6.9" style="padding-left:5.0pt;padding-right:5.0pt;">45.15</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.1.7.7.1" style="padding-left:5.0pt;padding-right:5.0pt;">Command R+ (104B)</th>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.7.2" style="padding-left:5.0pt;padding-right:5.0pt;">76.34</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.7.3" style="padding-left:5.0pt;padding-right:5.0pt;">56.30</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.7.4" style="padding-left:5.0pt;padding-right:5.0pt;">47.50</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.7.5" style="padding-left:5.0pt;padding-right:5.0pt;">29.00</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.7.6" style="padding-left:5.0pt;padding-right:5.0pt;">47.44</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.7.7" style="padding-left:5.0pt;padding-right:5.0pt;">59.18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.7.7.8" style="padding-left:5.0pt;padding-right:5.0pt;">52.50</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.7.7.9" style="padding-left:5.0pt;padding-right:5.0pt;">52.61</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.1.8.8.1" style="padding-left:5.0pt;padding-right:5.0pt;">gemma-2-9b-it</th>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.8.2" style="padding-left:5.0pt;padding-right:5.0pt;">77.66</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.8.3" style="padding-left:5.0pt;padding-right:5.0pt;">59.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.8.4" style="padding-left:5.0pt;padding-right:5.0pt;">49.70</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.8.5" style="padding-left:5.0pt;padding-right:5.0pt;">30.50</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.8.6" style="padding-left:5.0pt;padding-right:5.0pt;">37.37</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.8.7" style="padding-left:5.0pt;padding-right:5.0pt;">53.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.8.8.8" style="padding-left:5.0pt;padding-right:5.0pt;">52.60</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.8.8.9" style="padding-left:5.0pt;padding-right:5.0pt;">51.57</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.1.9.9.1" style="padding-left:5.0pt;padding-right:5.0pt;">Llama3 8B Instruct</th>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.9.2" style="padding-left:5.0pt;padding-right:5.0pt;">72.28</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.9.3" style="padding-left:5.0pt;padding-right:5.0pt;">51.52</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.9.4" style="padding-left:5.0pt;padding-right:5.0pt;">27.90</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.9.5" style="padding-left:5.0pt;padding-right:5.0pt;">9.90</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.9.6" style="padding-left:5.0pt;padding-right:5.0pt;">31.01</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.9.7" style="padding-left:5.0pt;padding-right:5.0pt;">52.82</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.9.9.8" style="padding-left:5.0pt;padding-right:5.0pt;">45.90</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.9.9.9" style="padding-left:5.0pt;padding-right:5.0pt;">41.62</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.1.10.10.1" style="padding-left:5.0pt;padding-right:5.0pt;">Llama3.1 8B Instruct</th>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.10.2" style="padding-left:5.0pt;padding-right:5.0pt;">71.33</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.10.3" style="padding-left:5.0pt;padding-right:5.0pt;">59.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.10.4" style="padding-left:5.0pt;padding-right:5.0pt;">17.70</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.10.5" style="padding-left:5.0pt;padding-right:5.0pt;">9.40</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.10.6" style="padding-left:5.0pt;padding-right:5.0pt;">44.41</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.10.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.5.1.10.10.7.1">59.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.1.10.10.8" style="padding-left:5.0pt;padding-right:5.0pt;">46.70</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.1.10.10.9" style="padding-left:5.0pt;padding-right:5.0pt;">44.09</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T1.5.1.11.11.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_ERROR undefined" id="S3.T1.5.1.11.11.1.1">\ourmodel</span>-9B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.5.1.11.11.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.5.1.11.11.2.1">79.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.5.1.11.11.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.5.1.11.11.3.1">77.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.5.1.11.11.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.5.1.11.11.4.1">79.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.5.1.11.11.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.5.1.11.11.5.1">42.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.5.1.11.11.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.5.1.11.11.6.1">48.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.5.1.11.11.7" style="padding-left:5.0pt;padding-right:5.0pt;">53.97</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.5.1.11.11.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.5.1.11.11.8.1">65.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.5.1.11.11.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.5.1.11.11.9.1">63.44</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.T1" title="In 3.2 Experimental Results on ContextualBench ‣ 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> compares the performance of our 9B <span class="ltx_ERROR undefined" id="S3.SS2.p1.1.1">\ourmodel</span>model on ContextualBench against state-of-the-art large models as well as comparable ones across the 7 question answering tasks. PopQA scores are measured in easy matching, while the remaining are measured in exact matching. As shown, GPT-4o <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib30" title="">30</a>]</cite> unsurprisingly aces most of the benchmarks. However, given its small size, our <span class="ltx_ERROR undefined" id="S3.SS2.p1.1.2">\ourmodel</span>-9B model significantly outperforms strong open-source baselines such as Command-R and Command-R+ that have up to 10 times larger parameter counts. Remarkably, it achieves the state of the art in TruthfulQA, 2WikihopQA and HotpotQA in contextual settings. Overall, it also achieves the state of the art average performance, demonstrating our model’s strong ability across many contextual tasks. In particular, our model excels at 2WikiHopQA, with nearly a 25% increase in performance compared to GPT-4o.
Meanwhile, our 9B model consistently outperforms Llama-3.1 8B Instruct and gemma-2-9b-it across most benchmarks.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><svg class="ltx_picture ltx_centering" height="251.51" id="S3.F3.pic1" overflow="visible" version="1.1" width="589.91"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,251.51) matrix(1 0 0 -1 0 0) translate(51.9,0) translate(0,67.74) matrix(1.0 0.0 0.0 1.0 -51.9 -67.74)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(113.95,0) translate(0,88.25)"><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 0 -26.41 L 0 -20.51 M 137.88 -26.41 L 137.88 -20.51 M 275.76 -26.41 L 275.76 -20.51 M 413.65 -26.41 L 413.65 -20.51 M 0 163.13 L 0 157.22 M 137.88 163.13 L 137.88 157.22 M 275.76 163.13 L 275.76 157.22 M 413.65 163.13 L 413.65 157.22" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M -62.04 0 L -56.14 0 M -62.04 46.19 L -56.14 46.19 M -62.04 92.38 L -56.14 92.38 M -62.04 138.56 L -56.14 138.56 M 475.69 0 L 469.79 0 M 475.69 46.19 L 469.79 46.19 M 475.69 92.38 L 469.79 92.38 M 475.69 138.56 L 469.79 138.56" style="fill:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M -62.04 -20.51 L -62.04 157.22 L 475.69 157.22 L 475.69 -20.51 L -62.04 -20.51 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -43.91 -40.91)"><foreignobject height="10.76" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="87.83"><span class="ltx_text" id="S3.F3.pic1.17.17.17.17.17.1.1">Command-R+</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 113.52 -40.76)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="48.72"><span class="ltx_text" id="S3.F3.pic1.18.18.18.18.18.1.1">GPT-4o</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 231.95 -40.91)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="87.63"><span class="ltx_text" id="S3.F3.pic1.19.19.19.19.19.1.1">gemma-2-9b-it</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 371.7 -40.76)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="84.27"><span class="ltx_text" id="S3.F3.pic1.20.20.20.20.20.1.1">SFR-RAG-9B</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -80.77 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="40" class="ltx_Math" display="inline" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">40</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -80.77 41.73)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="60" class="ltx_Math" display="inline" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">60</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">60</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -80.77 87.92)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="80" class="ltx_Math" display="inline" id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">80</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">80</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -87.69 134.11)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="20.76"><math alttext="100" class="ltx_Math" display="inline" id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">100</annotation></semantics></math></foreignobject></g><clippath id="pgfcp9"><path d="M -62.04 -20.51 L 475.69 -20.51 L 475.69 157.22 L -62.04 157.22 Z"></path></clippath><g clip-path="url(#pgfcp9)"><g color="#0000FF" fill="#B3B3FF" stroke="#0000FF"><path d="M -33.9 -20.51 h 20.76 v 98.1 h -20.76 Z M 103.98 -20.51 h 20.76 v 37.13 h -20.76 Z M 241.86 -20.51 h 20.76 v 56.76 h -20.76 Z M 379.75 -20.51 h 20.76 v 126.28 h -20.76 Z"></path></g><g></g><g color="#FF0000" fill="#FFB3B3" stroke="#FF0000"><path d="M -10.38 -20.51 h 20.76 v 20.51 h -20.76 Z M 127.5 -20.51 h 20.76 v 66.46 h -20.76 Z M 265.39 -20.51 h 20.76 v 40.6 h -20.76 Z M 403.27 -20.51 h 20.76 v 91.17 h -20.76 Z"></path></g><g></g><g color="#734D26" fill="#ECD9C6" stroke="#734D26"><path d="M 13.15 -20.51 h 20.76 v 58.84 h -20.76 Z M 151.03 -20.51 h 20.76 v 143.37 h -20.76 Z M 288.91 -20.51 h 20.76 v 45.22 h -20.76 Z M 426.79 -20.51 h 20.76 v 157.22 h -20.76 Z"></path></g><g></g></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0 0.0 0.0 1.0 -35.75 82.48)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="73.6" class="ltx_Math" display="inline" id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">73.6</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">73.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">73.6</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">73.6</annotation></semantics></math></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0 0.0 0.0 1.0 102.14 21.52)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="47.2" class="ltx_Math" display="inline" id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">47.2</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">47.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">47.2</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">47.2</annotation></semantics></math></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0 0.0 0.0 1.0 240.02 41.15)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="55.7" class="ltx_Math" display="inline" id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">55.7</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">55.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">55.7</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">55.7</annotation></semantics></math></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" transform="matrix(1.0 0.0 0.0 1.0 377.9 110.66)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="85.8" class="ltx_Math" display="inline" id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">85.8</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">85.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">85.8</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">85.8</annotation></semantics></math></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1.0 0.0 0.0 1.0 -6.23 4.89)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="12.45"><math alttext="40" class="ltx_Math" display="inline" id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">40</annotation></semantics></math></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1.0 0.0 0.0 1.0 125.66 50.85)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="59.9" class="ltx_Math" display="inline" id="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">59.9</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">59.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">59.9</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">59.9</annotation></semantics></math></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1.0 0.0 0.0 1.0 263.54 24.98)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="48.7" class="ltx_Math" display="inline" id="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">48.7</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">48.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">48.7</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">48.7</annotation></semantics></math></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" transform="matrix(1.0 0.0 0.0 1.0 401.42 75.56)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="70.6" class="ltx_Math" display="inline" id="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" mathsize="90%" xref="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">70.6</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">70.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">70.6</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">70.6</annotation></semantics></math></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" transform="matrix(1.0 0.0 0.0 1.0 11.3 43.22)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="56.6" class="ltx_Math" display="inline" id="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" mathcolor="#734D26" mathsize="90%" xref="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">56.6</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">56.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">56.6</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">56.6</annotation></semantics></math></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" transform="matrix(1.0 0.0 0.0 1.0 149.18 127.75)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="93.2" class="ltx_Math" display="inline" id="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" mathcolor="#734D26" mathsize="90%" xref="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">93.2</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">93.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">93.2</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">93.2</annotation></semantics></math></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" transform="matrix(1.0 0.0 0.0 1.0 287.06 29.6)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="50.7" class="ltx_Math" display="inline" id="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" mathcolor="#734D26" mathsize="90%" xref="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50.7</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">50.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">50.7</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">50.7</annotation></semantics></math></foreignobject></g><g color="#734D26" fill="#734D26" stroke="#734D26" transform="matrix(1.0 0.0 0.0 1.0 424.95 141.61)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="24.45"><math alttext="99.2" class="ltx_Math" display="inline" id="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" mathcolor="#734D26" mathsize="90%" xref="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">99.2</mn><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">99.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">99.2</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">99.2</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(0.0 1.0 -1.0 0.0 -99.88 17.08)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="102.55"><span class="ltx_text" id="S3.F3.pic1.21.21.21.21.21.1.1">EasyM Accuracy</span></foreignobject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 77.21 -87.97 h 259.22 v 22.75 h -259.22 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 81.36 -85.2)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.61)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#0000FF" fill="#B3B3FF" stroke="#0000FF" transform="matrix(1 0 0 -1 0 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 13.01 0) translate(47.76,0) matrix(1.0 0.0 0.0 1.0 -44.99 -3.77)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="90.36"><span class="ltx_text" id="S3.F3.pic1.22.22.22.22.22.1.1.1.1.1">Counterfactual</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FF0000" fill="#FFB3B3" stroke="#FF0000" transform="matrix(1 0 0 -1 108.52 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 121.53 0) translate(31.4,0) matrix(1.0 0.0 0.0 1.0 -28.63 -3.77)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="57.65"><span class="ltx_text" id="S3.F3.pic1.23.23.23.23.23.2.2.2.1.1">Unknown</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#734D26" fill="#ECD9C6" stroke="#734D26" transform="matrix(1 0 0 -1 184.33 0) translate(2.35,0)"><path d="M -2.08 -2.77 h 4.15 v 11.07 h -4.15 Z M 6.23 -2.77 h 4.15 v 8.3 h -4.15 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 197.34 0) translate(26.79,0) matrix(1.0 0.0 0.0 1.0 -24.02 -3.77)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="48.05"><span class="ltx_text" id="S3.F3.pic1.24.24.24.24.24.3.3.3.1.1">Conflict</span></foreignobject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.4.2" style="font-size:90%;">FaithEval <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib27" title="">27</a>]</cite>: average easy match accuracy scores of different models when contextual facts are fabricated (Counterfactual), removed (Unknown) or when the facts are contradicting (Conflict). Small variations between those settings and overall high absolute scores indicate that <span class="ltx_ERROR undefined" id="S3.F3.4.2.1">\ourmodel</span>-9B is resilient to changes in contextual information.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Resilience to Unanswerable, Conflicting and Counterfactual Contexts</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Because most QA benchmarks are realistically based on real-world facts, understanding LLMs performance in contextual QA tasks may be ambiguous because high scores may be attributed to either (<span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">i</span>) the ability to seek accurate facts from the contextual documents and content, or (<span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">ii</span>) the intrinsic parametric knowledge of the model acquired during pre-training, of which large and state-of-the-art models like GPT-4o often have significant advantages.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Ming et al. [<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib27" title="">27</a>]</cite> recently proposed <span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">FaithEval</span>, an evaluation suite that measures how LLMs remain faithful to the context if the facts of the contexts are changed. The benchmark evaluate LLMs on three scenarios: (<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2">i</span>) “Unknown” where the relevant facts are removed and the original question becomes unanswerable; (<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.3">ii</span>) “Conflict” where multiple context documents are provided that contains conflicting or contradicting information and the model is expected to recognize that; and (<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.4">iii</span>) “Counterfactual” where certain commonsense facts are altered by introducing a falsely fabricated context document. For instance, “The Moon is Made of Marshmallows.” is considered a <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.5">counterfactual</em> context and the LLM under evaluation is expected to remain faithful to that “fact”, if prompted to do so. Following <cite class="ltx_cite ltx_citemacro_citet">Ming et al. [<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib27" title="">27</a>]</cite>, the “Unknown” and “Conflict” tasks are averaged over 10 benchmarks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib15" title="">15</a>]</cite>, while the “Counterfactual” task is evaluated using the ARC-C dataset <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.4.2" style="font-size:90%;">Standard LM-eval-harness benchmarks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib11" title="">11</a>]</cite>: <span class="ltx_ERROR undefined" id="S3.T2.4.2.1">\ourmodel</span>-9B maintains relative competitiveness in standard world knowledge and reasoning abilities against comparable baselines.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.5" style="width:433.6pt;height:108.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.5pt,-0.1pt) scale(1.00236261927291,1.00236261927291) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.2.1">MMLU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.3.1">GSM8K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.4.1">Winogrande</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.5.1">TruthfulQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.5.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.6.1">Hellaswag</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.5.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.7.1">ARC-C</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.5.1.2.1.1">Command-R (35B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.2.1.2">68.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.2.1.3">56.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.2.1.4">81.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.2.1.5">52.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.2.1.6">87.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.2.1.7">65.53</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.5.1.3.2.1">Llama-3-8b-instruct</th>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.3.2.2">67.07</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.3.2.3">68.69</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.3.2.4">74.51</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.3.2.5">51.65</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.3.2.6">78.55</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.3.2.7">60.75</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.5.1.4.3.1">Llama-3.1-8B-Instruct</th>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.3.2">68.22</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.3.3">71.04</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.3.4">78.06</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.3.5">54.58</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.3.6">80.47</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.3.7">60.92</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.5.1.5.4.1">gemma-2-9b-it</th>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.4.2">70.80</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.4.3">76.88</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.4.4">77.50</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.4.5">60.11</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.4.6">81.78</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.4.7">71.20</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T2.5.1.6.5.1">
<span class="ltx_ERROR undefined" id="S3.T2.5.1.6.5.1.1">\ourmodel</span>-9B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.5.1.6.5.2">70.15</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.5.1.6.5.3">82.56</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.5.1.6.5.4">78.46</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.5.1.6.5.5">56.49</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.5.1.6.5.6">81.58</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.5.1.6.5.7">69.12</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.4.2" style="font-size:90%;">Scores on Berkeley function calling benchmark <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib48" title="">48</a>]</cite>: <span class="ltx_ERROR undefined" id="S3.T3.4.2.1">\ourmodel</span>-9B exhibits competitive function calling abilities.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.5.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.1.2.1">Executable</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.5.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.1.3.1">AST</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.5.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.1.4.1">Relevance</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.5.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.2.1.1">GPT 4 Turbo</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.2.1.2">86.04</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.2.1.3">90.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.2.1.4">62.50</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.3.2.1">GPT 3.5 Turbo</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.3.2.2">81.38</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.3.2.3">75.23</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.3.2.4">87.80</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.4.3.1">Command R + (104B)</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.4.3.2">77.33</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.4.3.3">84.50</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.4.3.4">63.75</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.5.4.1">xLam 7B</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.4.2">87.12</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.4.3">89.46</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.4.4">85.00</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.6.5.1">Mistral Medium</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.6.5.2">73.47</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.6.5.3">84.48</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.6.5.4">88.33</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.7.6.1">gemma-2-9b-it</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.7.6.2">70.50</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.7.6.3">69.69</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.7.6.4">90.00</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.8.7.1">Meta-Llama-3-8B-Instruct</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.8.7.2">65.13</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.8.7.3">61.63</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.8.7.4">26.60</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.9.8.1">Meta-Llama-3.1-8B-Instruct</th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.9.8.2">75.17</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.9.8.3">77.56</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.9.8.4">40.00</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T3.5.10.9.1">
<span class="ltx_ERROR undefined" id="S3.T3.5.10.9.1.1">\ourmodel</span>-9B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.5.10.9.2">70.88</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.5.10.9.3">71.69</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.5.10.9.4">72.50</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.F3" title="In 3.2 Experimental Results on ContextualBench ‣ 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the average non-strict matching accuracy scores of different LLMs over 3 tasks under FaithEval suite.
As shown, other baselines such as GPT-4o exhibit high variations when the facts change in Counterfactual and Unknown settings. Particularly, GPT-4o scores low in Counterfactual setting perhaps because large models may have higher knowledge inertia and stronger resistance to factual changes.
Meanwhile, our <span class="ltx_ERROR undefined" id="S3.SS3.p3.1.1">\ourmodel</span>-9B scores consistently highly, even when the context information is altered. This demonstrates that our model is usefully resilient and faithful to unseen contextual information. It also means that the model is more adaptable to the ever-changing world. Plus, our model is more capable of identifying contradiction in the contexts, as well as resisting against its own parametric knowledge when contextual information presented is counter-intuitive. In other words, the model remains more faithful to the context even if the context contradicts its pre-trained knowledge.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Standard Benchmarks</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We also evaluate our <span class="ltx_ERROR undefined" id="S3.SS4.p1.1.1">\ourmodel</span>model in the traditional few-shot prompting benchmarks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib7" title="">7</a>]</cite> to measure its parametric knowledge as well as general instruction following and reasoning abilities. Using the similar setups in the Open LLM leaderboard <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib3" title="">3</a>]</cite>, we employ the standard evaluation harness <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib11" title="">11</a>]</cite> to evaluate our model in MMLU (5 shots), GSM8K (5 shots with strict matching), Winogrande (5 shots), TruthfulQA (0 shot MC2), Hellaswag (10 shots with normalized accuracy) and ARC-C (25 shots with normalized accuracy) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.T2" title="In 3.3 Resilience to Unanswerable, Conflicting and Counterfactual Contexts ‣ 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, our <span class="ltx_ERROR undefined" id="S3.SS4.p2.1.1">\ourmodel</span>model performs competitively in terms of world knowledge, common sense and reasoning abilities, despite the fact that it is optimized for contextual and retrieval use cases.
Particularly, our 9B model outperforms Command-R <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib36" title="">36</a>]</cite> with 35B parameters in MMLU, GSM8K, TruthfulQA as well as ARC-C. Meanwhile it remains competitive to Llama-3.1-Instruct <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib9" title="">9</a>]</cite> and gemma-2-9b-it <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib38" title="">38</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">Our model is also trained with function calling with a focus to support dynamic and multi-hop interactions with external tools to retrieve high-quality contextual information. As such, we compare our model with certain popular baselines in the Berkeley function calling task <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib48" title="">48</a>]</cite>. As reported in <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#S3.T3" title="In 3.3 Resilience to Unanswerable, Conflicting and Counterfactual Contexts ‣ 3 Evaluation ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>, our <span class="ltx_ERROR undefined" id="S3.SS4.p3.1.1">\ourmodel</span>performs competitively against comparable baselines such as Llama-3-8B-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#bib.bib9" title="">9</a>]</cite></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We present <span class="ltx_ERROR undefined" id="S4.p1.1.1">\ourmodel</span>, a LLM fine-tuned with an emphasis on faithful contextual comprehension and understanding for retrieval augmented generation applications. The <span class="ltx_ERROR undefined" id="S4.p1.1.2">\ourmodel</span>model is trained to minimize hallucination, effectively handle unanswerable, counterfactual or low-quality and irrelevant contexts. It is also capable of performing complex multi-hop reasoning and producing citations reliably and accurately. We also introduce ContextualBench, which is a compilation of various popular RAG benchmarks evaluated under consistent and appropriate settings. The experiments show that our 9B <span class="ltx_ERROR undefined" id="S4.p1.1.3">\ourmodel</span>model outperforms notable baselines, including Command-R+ and GPT-4o, and achieves the state of the art in 3 out of 7 benchmarks in ContextualBench. Our evaluation with FaithEval also shows that our model is resilient to changes in the context information and able to identify unanswerable questions. Lastly, the <span class="ltx_ERROR undefined" id="S4.p1.1.4">\ourmodel</span>model maintains competitive performance in general instruction-tuned tasks and function-calling capabilities, compared to baselines of similar sizes.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. [2023]</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2310.11511</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2023]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beeching et al. [2023]</span>
<span class="ltx_bibblock">
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.

</span>
<span class="ltx_bibblock">Open llm leaderboard.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard" title="">https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. [2020]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2024]</span>
<span class="ltx_bibblock">
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.

</span>
<span class="ltx_bibblock">Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. [2018]</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. [2021]</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et al. [2019]</span>
<span class="ltx_bibblock">
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.

</span>
<span class="ltx_bibblock">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:1903.00161</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. [2024]</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunn et al. [2017]</span>
<span class="ltx_bibblock">
Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho.

</span>
<span class="ltx_bibblock">Searchqa: A new q&amp;a dataset augmented with context from a search engine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1704.05179</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2024]</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, 07 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/12608602" title="">https://zenodo.org/records/12608602</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. [2021]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the International Conference on Learning Representations (ICLR)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. [2020]</span>
<span class="ltx_bibblock">
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.

</span>
<span class="ltx_bibblock">Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps.

</span>
<span class="ltx_bibblock">In Donia Scott, Nuria Bel, and Chengqing Zong, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 6609–6625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.coling-main.580</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.coling-main.580" title="">https://aclanthology.org/2020.coling-main.580</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023]</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. [2017]</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:1705.03551</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. [2024]</span>
<span class="ltx_bibblock">
Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky.

</span>
<span class="ltx_bibblock">Bridging the preference gap between retrievers and llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2401.06954</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kembhavi et al. [2017]</span>
<span class="ltx_bibblock">
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern recognition</em>, pages 4999–5007, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. [2019]</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. [2017]</span>
<span class="ltx_bibblock">
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.

</span>
<span class="ltx_bibblock">RACE: Large-scale ReAding comprehension dataset from examinations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, pages 785–794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D17-1082</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D17-1082" title="">https://aclanthology.org/D17-1082</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2024]</span>
<span class="ltx_bibblock">
Myeonghwa Lee, Seonho An, and Min-Soo Kim.

</span>
<span class="ltx_bibblock">Planrag: A plan-then-retrieval augmented generation for generative large language models as decision makers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2406.12430</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023a]</span>
<span class="ltx_bibblock">
Chaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao.

</span>
<span class="ltx_bibblock">Making large language models a better foundation for dense retrieval, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023b]</span>
<span class="ltx_bibblock">
Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing.

</span>
<span class="ltx_bibblock">Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2305.13269</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2021]</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2109.07958</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2023]</span>
<span class="ltx_bibblock">
Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.

</span>
<span class="ltx_bibblock">Large language model is not a good few-shot information extractor, but a good reranker for hard samples!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2303.08559</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et al. [2022]</span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2212.10511</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. [2024]</span>
<span class="ltx_bibblock">
Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz.

</span>
<span class="ltx_bibblock">Sfr-embedding-mistral:enhance text retrieval with transfer learning.

</span>
<span class="ltx_bibblock">Salesforce AI Research Blog, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.salesforceairesearch.com/sfr-embedded-mistral/" title="">https://blog.salesforceairesearch.com/sfr-embedded-mistral/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ming et al. [2024]</span>
<span class="ltx_bibblock">
Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Rayhan Joty.

</span>
<span class="ltx_bibblock">Faitheval: Can your language model stay faithful to context, even if "the moon is made of marshmallows"?

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. [2022]</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers.

</span>
<span class="ltx_bibblock">Mteb: Massive text embedding benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2210.07316</em>, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/ARXIV.2210.07316</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.07316" title="">https://arxiv.org/abs/2210.07316</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023a]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt (june 2023 version, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023b]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al. [2023]</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2305.18290</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. [2016]</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.

</span>
<span class="ltx_bibblock">Squad: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:1606.05250</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et al. [2021]</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Communications of the ACM</em>, 64(9):99–106, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2023]</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2301.12652</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team [2024a]</span>
<span class="ltx_bibblock">
Cohere Team.

</span>
<span class="ltx_bibblock">Introducing embed v3, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cohere.com/blog/introducing-embed-v3/" title="">https://cohere.com/blog/introducing-embed-v3/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team [2024b]</span>
<span class="ltx_bibblock">
Cohere Team.

</span>
<span class="ltx_bibblock">Command r: Retrieval-augmented generation at production scale, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cohere.com/blog/command-r/" title="">https://cohere.com/blog/command-r/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team [2024c]</span>
<span class="ltx_bibblock">
Contextual AI Team.

</span>
<span class="ltx_bibblock">Introducing rag 2.0, 2024c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://contextual.ai/introducing-rag2/" title="">https://contextual.ai/introducing-rag2/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. [2024]</span>
<span class="ltx_bibblock">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al.

</span>
<span class="ltx_bibblock">Gemma: Open models based on gemini research and technology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2403.08295</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et al. [2022]</span>
<span class="ltx_bibblock">
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2201.08239</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trischler et al. [2016]</span>
<span class="ltx_bibblock">
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman.

</span>
<span class="ltx_bibblock">Newsqa: A machine comprehension dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:1611.09830</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. [2022]</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Musique: Multihop questions via single-hop question composition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Transactions of the Association for Computational Linguistics</em>, 10:539–554, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsatsaronis et al. [2012]</span>
<span class="ltx_bibblock">
George Tsatsaronis, Michael Schroeder, Georgios Paliouras, Yannis Almirantis, Ion Androutsopoulos, Eric Gaussier, Patrick Gallinari, Thierry Artieres, Michael R Alvers, Matthias Zschunke, et al.

</span>
<span class="ltx_bibblock">Bioasq: A challenge on large-scale biomedical semantic indexing and question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">2012 AAAI Fall Symposium Series</em>, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallace et al. [2024]</span>
<span class="ltx_bibblock">
Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel.

</span>
<span class="ltx_bibblock">The instruction hierarchy: Training llms to prioritize privileged instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2404.13208</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weijia et al. [2023]</span>
<span class="ltx_bibblock">
Shi Weijia, Min Sewon, Yasunaga Michihiro, Seo Minjoon, James Rich, Lewis Mike, and Yih Wen-tau.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ArXiv: 2301.12652</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. [2023]</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.

</span>
<span class="ltx_bibblock">C-pack: Packaged resources to advance general chinese embedding, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023]</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi.

</span>
<span class="ltx_bibblock">Recomp: Improving retrieval-augmented lms with compression and selective augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2310.04408</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2024]</span>
<span class="ltx_bibblock">
Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez.

</span>
<span class="ltx_bibblock">Berkeley function calling leaderboard.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2018]</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">HotpotQA: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2022]</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2210.03629</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2024]</span>
<span class="ltx_bibblock">
Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Rankrag: Unifying context ranking with retrieval-augmented generation in llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2407.02485</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. [2019]</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:1905.07830</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2024]</span>
<span class="ltx_bibblock">
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for ai-generated content: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2402.19473</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2023]</span>
<span class="ltx_bibblock">
Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty.

</span>
<span class="ltx_bibblock">Retrieving multimodal information for augmented generation: A survey.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 4736–4756, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-emnlp.314</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-emnlp.314" title="">https://aclanthology.org/2023.findings-emnlp.314</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T4.2.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="A1.T4.3.2" style="font-size:90%;">ContextualBench scores for HotpotQA, 2WikihopQA and Musique, as measured in easy matching, strict matching accuracy and the F1 score.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T4.4" style="width:433.6pt;height:208.7pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.7pt,4.1pt) scale(0.961579905766406,0.961579905766406) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T4.4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A1.T4.4.1.1.1.1" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.4.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T4.4.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.4.1.1.1.2.1">HotpotQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T4.4.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.4.1.1.1.3.1">2WikiHopQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T4.4.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.4.1.1.1.4.1">MuSiQue</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">EasyM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">EasyM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.5" style="padding-left:5.0pt;padding-right:5.0pt;">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.6" style="padding-left:5.0pt;padding-right:5.0pt;">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.7" style="padding-left:5.0pt;padding-right:5.0pt;">EasyM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.8" style="padding-left:5.0pt;padding-right:5.0pt;">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.2.2.9" style="padding-left:5.0pt;padding-right:5.0pt;">F1</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.4.1.3.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT 4 Turbo</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">70.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">54.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.4" style="padding-left:5.0pt;padding-right:5.0pt;">69.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.5" style="padding-left:5.0pt;padding-right:5.0pt;">73.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">59.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.7" style="padding-left:5.0pt;padding-right:5.0pt;">69.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.8" style="padding-left:5.0pt;padding-right:5.0pt;">53.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.9" style="padding-left:5.0pt;padding-right:5.0pt;">37.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.3.3.10" style="padding-left:5.0pt;padding-right:5.0pt;">52.70</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.4.1.4.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT 4o</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">72.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.3" style="padding-left:5.0pt;padding-right:5.0pt;">57.2</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">71.40</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">74.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.6" style="padding-left:5.0pt;padding-right:5.0pt;">62.40</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">70.90</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.8" style="padding-left:5.0pt;padding-right:5.0pt;">60.20</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.9" style="padding-left:5.0pt;padding-right:5.0pt;">44.40</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.4.4.10" style="padding-left:5.0pt;padding-right:5.0pt;">58.60</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.4.1.5.5.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4o-mini</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">68.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.3" style="padding-left:5.0pt;padding-right:5.0pt;">49.80</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.4" style="padding-left:5.0pt;padding-right:5.0pt;">65.50</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.5" style="padding-left:5.0pt;padding-right:5.0pt;">57.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.6" style="padding-left:5.0pt;padding-right:5.0pt;">46.60</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.7" style="padding-left:5.0pt;padding-right:5.0pt;">56.50</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.8" style="padding-left:5.0pt;padding-right:5.0pt;">48.30</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.9" style="padding-left:5.0pt;padding-right:5.0pt;">32.10</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.5.5.10" style="padding-left:5.0pt;padding-right:5.0pt;">48.90</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.4.1.6.6.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT 3.5 Turbo</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.2" style="padding-left:5.0pt;padding-right:5.0pt;">63.10</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.3" style="padding-left:5.0pt;padding-right:5.0pt;">48.10</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.4" style="padding-left:5.0pt;padding-right:5.0pt;">61.80</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.5" style="padding-left:5.0pt;padding-right:5.0pt;">50.50</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.6" style="padding-left:5.0pt;padding-right:5.0pt;">41.20</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.7" style="padding-left:5.0pt;padding-right:5.0pt;">49.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.8" style="padding-left:5.0pt;padding-right:5.0pt;">34.90</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.9" style="padding-left:5.0pt;padding-right:5.0pt;">22.60</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.6.6.10" style="padding-left:5.0pt;padding-right:5.0pt;">35.90</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.4.1.7.7.1" style="padding-left:5.0pt;padding-right:5.0pt;">Command R (35B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.2" style="padding-left:5.0pt;padding-right:5.0pt;">63.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.3" style="padding-left:5.0pt;padding-right:5.0pt;">48.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.4" style="padding-left:5.0pt;padding-right:5.0pt;">63.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.5" style="padding-left:5.0pt;padding-right:5.0pt;">55.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.6" style="padding-left:5.0pt;padding-right:5.0pt;">41.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.7" style="padding-left:5.0pt;padding-right:5.0pt;">52.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.8" style="padding-left:5.0pt;padding-right:5.0pt;">36.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.9" style="padding-left:5.0pt;padding-right:5.0pt;">14.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.1.7.7.10" style="padding-left:5.0pt;padding-right:5.0pt;">34.10</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.4.1.8.8.1" style="padding-left:5.0pt;padding-right:5.0pt;">Command R + (104B)</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.2" style="padding-left:5.0pt;padding-right:5.0pt;">67.30</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.3" style="padding-left:5.0pt;padding-right:5.0pt;">52.50</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.4" style="padding-left:5.0pt;padding-right:5.0pt;">66.60</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.5" style="padding-left:5.0pt;padding-right:5.0pt;">60.80</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.6" style="padding-left:5.0pt;padding-right:5.0pt;">47.50</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.7" style="padding-left:5.0pt;padding-right:5.0pt;">57.30</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.8" style="padding-left:5.0pt;padding-right:5.0pt;">46.30</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.9" style="padding-left:5.0pt;padding-right:5.0pt;">29.00</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.8.8.10" style="padding-left:5.0pt;padding-right:5.0pt;">43.60</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.4.1.9.9.1" style="padding-left:5.0pt;padding-right:5.0pt;">LLama3 8B Instruct (8B)</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.2" style="padding-left:5.0pt;padding-right:5.0pt;">58.80</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.3" style="padding-left:5.0pt;padding-right:5.0pt;">45.90</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.4" style="padding-left:5.0pt;padding-right:5.0pt;">60.00</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.5" style="padding-left:5.0pt;padding-right:5.0pt;">46.50</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.6" style="padding-left:5.0pt;padding-right:5.0pt;">27.90</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.7" style="padding-left:5.0pt;padding-right:5.0pt;">42.20</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.8" style="padding-left:5.0pt;padding-right:5.0pt;">29.30</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.9" style="padding-left:5.0pt;padding-right:5.0pt;">9.90</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.9.9.10" style="padding-left:5.0pt;padding-right:5.0pt;">25.30</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.4.1.10.10.1" style="padding-left:5.0pt;padding-right:5.0pt;">LLama3.1 8B Instruct (8B)</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.2" style="padding-left:5.0pt;padding-right:5.0pt;">64.80</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.3" style="padding-left:5.0pt;padding-right:5.0pt;">46.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.4" style="padding-left:5.0pt;padding-right:5.0pt;">62.00</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.5" style="padding-left:5.0pt;padding-right:5.0pt;">51.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.6" style="padding-left:5.0pt;padding-right:5.0pt;">17.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.7" style="padding-left:5.0pt;padding-right:5.0pt;">38.30</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.8" style="padding-left:5.0pt;padding-right:5.0pt;">38.00</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.9" style="padding-left:5.0pt;padding-right:5.0pt;">9.40</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.10.10.10" style="padding-left:5.0pt;padding-right:5.0pt;">29.00</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.4.1.11.11.1" style="padding-left:5.0pt;padding-right:5.0pt;">gemma-2-9b-it</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.2" style="padding-left:5.0pt;padding-right:5.0pt;">67.20</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.3" style="padding-left:5.0pt;padding-right:5.0pt;">52.60</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.4" style="padding-left:5.0pt;padding-right:5.0pt;">66.10</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.5" style="padding-left:5.0pt;padding-right:5.0pt;">59.60</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.6" style="padding-left:5.0pt;padding-right:5.0pt;">49.70</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.7" style="padding-left:5.0pt;padding-right:5.0pt;">58.10</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.8" style="padding-left:5.0pt;padding-right:5.0pt;">47.00</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.9" style="padding-left:5.0pt;padding-right:5.0pt;">30.50</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.1.11.11.10" style="padding-left:5.0pt;padding-right:5.0pt;">43.50</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_ERROR undefined" id="A1.T4.4.1.12.12.1.1">\ourmodel</span>-9B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.2" style="padding-left:5.0pt;padding-right:5.0pt;">83.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.3" style="padding-left:5.0pt;padding-right:5.0pt;">65.70</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.4" style="padding-left:5.0pt;padding-right:5.0pt;">79.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.5" style="padding-left:5.0pt;padding-right:5.0pt;">86.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.6" style="padding-left:5.0pt;padding-right:5.0pt;">79.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.7" style="padding-left:5.0pt;padding-right:5.0pt;">84.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.8" style="padding-left:5.0pt;padding-right:5.0pt;">57.40</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.9" style="padding-left:5.0pt;padding-right:5.0pt;">42.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.4.1.12.12.10" style="padding-left:5.0pt;padding-right:5.0pt;">53.50</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T5.2.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="A1.T5.3.2" style="font-size:90%;">ContextualBench scores for TriviaQA, TruthfulQA, PopQA and NQ, as measured in easy matching, strict matching accuracy and the F1 score.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T5.4" style="width:433.6pt;height:251.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(30.6pt,-17.7pt) scale(1.16416497909968,1.16416497909968) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T5.4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A1.T5.4.1.1.1.1" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.4.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T5.4.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.4.1.1.1.2.1">TriviaQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.4.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.4.1.1.1.3.1">TruthfulQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.4.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.4.1.1.1.4.1">PopQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T5.4.1.1.1.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.4.1.1.1.5.1">NQ</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.2.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.2.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.2.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">EasyM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.2.2.5" style="padding-left:5.0pt;padding-right:5.0pt;">EM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.2.2.6" style="padding-left:5.0pt;padding-right:5.0pt;">F1</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.4.1.3.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT 4 Turbo</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.3.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">78.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.3.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">86.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.3.3.4" style="padding-left:5.0pt;padding-right:5.0pt;">76.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.3.3.5" style="padding-left:5.0pt;padding-right:5.0pt;">53.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.3.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">48.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.3.3.7" style="padding-left:5.0pt;padding-right:5.0pt;">67.20</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.4.1.4.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT 4o</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.4.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">81.73</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.4.4.3" style="padding-left:5.0pt;padding-right:5.0pt;">88.20</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.4.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">76.47</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.4.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">42.24</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.4.4.6" style="padding-left:5.0pt;padding-right:5.0pt;">51.85</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.4.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">69.75</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.4.1.5.5.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT-4o-mini</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.5.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">72.55</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.5.5.3" style="padding-left:5.0pt;padding-right:5.0pt;">81.77</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.5.5.4" style="padding-left:5.0pt;padding-right:5.0pt;">61.02</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.5.5.5" style="padding-left:5.0pt;padding-right:5.0pt;">54.75</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.5.5.6" style="padding-left:5.0pt;padding-right:5.0pt;">30.75</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.5.5.7" style="padding-left:5.0pt;padding-right:5.0pt;">54.90</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.4.1.6.6.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT 3.5 Turbo</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.6.6.2" style="padding-left:5.0pt;padding-right:5.0pt;">77.43</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.6.6.3" style="padding-left:5.0pt;padding-right:5.0pt;">84.70</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.6.6.4" style="padding-left:5.0pt;padding-right:5.0pt;">47.00</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.6.6.5" style="padding-left:5.0pt;padding-right:5.0pt;">53.39</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.6.6.6" style="padding-left:5.0pt;padding-right:5.0pt;">40.40</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.6.6.7" style="padding-left:5.0pt;padding-right:5.0pt;">60.69</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.4.1.7.7.1" style="padding-left:5.0pt;padding-right:5.0pt;">Command R (35B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.7.7.2" style="padding-left:5.0pt;padding-right:5.0pt;">71.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.7.7.3" style="padding-left:5.0pt;padding-right:5.0pt;">79.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.7.7.4" style="padding-left:5.0pt;padding-right:5.0pt;">51.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.7.7.5" style="padding-left:5.0pt;padding-right:5.0pt;">52.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.7.7.6" style="padding-left:5.0pt;padding-right:5.0pt;">35.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.4.1.7.7.7" style="padding-left:5.0pt;padding-right:5.0pt;">55.50</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.4.1.8.8.1" style="padding-left:5.0pt;padding-right:5.0pt;">Command R + (104B)</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.8.8.2" style="padding-left:5.0pt;padding-right:5.0pt;">76.34</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.8.8.3" style="padding-left:5.0pt;padding-right:5.0pt;">83.05</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.8.8.4" style="padding-left:5.0pt;padding-right:5.0pt;">56.30</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.8.8.5" style="padding-left:5.0pt;padding-right:5.0pt;">59.18</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.8.8.6" style="padding-left:5.0pt;padding-right:5.0pt;">47.44</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.8.8.7" style="padding-left:5.0pt;padding-right:5.0pt;">63.21</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.4.1.9.9.1" style="padding-left:5.0pt;padding-right:5.0pt;">LLama3 8B Instruct (8B)</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.9.9.2" style="padding-left:5.0pt;padding-right:5.0pt;">72.28</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.9.9.3" style="padding-left:5.0pt;padding-right:5.0pt;">79.07</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.9.9.4" style="padding-left:5.0pt;padding-right:5.0pt;">51.52</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.9.9.5" style="padding-left:5.0pt;padding-right:5.0pt;">52.82</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.9.9.6" style="padding-left:5.0pt;padding-right:5.0pt;">31.01</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.9.9.7" style="padding-left:5.0pt;padding-right:5.0pt;">48.22</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.4.1.10.10.1" style="padding-left:5.0pt;padding-right:5.0pt;">LLama3.1 8B Instruct (8B)</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.10.10.2" style="padding-left:5.0pt;padding-right:5.0pt;">71.33</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.10.10.3" style="padding-left:5.0pt;padding-right:5.0pt;">78.33</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.10.10.4" style="padding-left:5.0pt;padding-right:5.0pt;">59.43</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.10.10.5" style="padding-left:5.0pt;padding-right:5.0pt;">59.68</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.10.10.6" style="padding-left:5.0pt;padding-right:5.0pt;">44.41</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.10.10.7" style="padding-left:5.0pt;padding-right:5.0pt;">61.60</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.4.1.11.11.1" style="padding-left:5.0pt;padding-right:5.0pt;">gemma-2-9b-it</th>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.11.11.2" style="padding-left:5.0pt;padding-right:5.0pt;">77.66</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.11.11.3" style="padding-left:5.0pt;padding-right:5.0pt;">83.23</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.11.11.4" style="padding-left:5.0pt;padding-right:5.0pt;">59.43</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.11.11.5" style="padding-left:5.0pt;padding-right:5.0pt;">53.75</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.11.11.6" style="padding-left:5.0pt;padding-right:5.0pt;">37.37</td>
<td class="ltx_td ltx_align_center" id="A1.T5.4.1.11.11.7" style="padding-left:5.0pt;padding-right:5.0pt;">53.15</td>
</tr>
<tr class="ltx_tr" id="A1.T5.4.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A1.T5.4.1.12.12.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_ERROR undefined" id="A1.T5.4.1.12.12.1.1">\ourmodel</span>-9B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.4.1.12.12.2" style="padding-left:5.0pt;padding-right:5.0pt;">79.24</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.4.1.12.12.3" style="padding-left:5.0pt;padding-right:5.0pt;">84.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.4.1.12.12.4" style="padding-left:5.0pt;padding-right:5.0pt;">77.45</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.4.1.12.12.5" style="padding-left:5.0pt;padding-right:5.0pt;">53.97</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.4.1.12.12.6" style="padding-left:5.0pt;padding-right:5.0pt;">48.01</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.4.1.12.12.7" style="padding-left:5.0pt;padding-right:5.0pt;">59.66</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#A1.T4" title="In Appendix A Appendix ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.09916v1#A1.T5" title="Table 5 ‣ Appendix A Appendix ‣ \ourmodel: Towards Contextually Faithful LLMs"><span class="ltx_text ltx_ref_tag">5</span></a> present the complete results of ContextualBench across 7 benchmarks as measured in different metrics. As these metrics measure different aspects of the question-answering responses, the numbers provide more insights into the performance comparison between for models. For instance, a model that is too verbose may score low in EM and high in F1, and vice-versa.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 01:06:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
