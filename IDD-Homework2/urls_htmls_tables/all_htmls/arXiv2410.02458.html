<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation</title>
<!--Generated on Fri Oct  4 14:19:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02458v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S1" title="In MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S2" title="In MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S2.SS1" title="In 2 Related Works ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Vision Transformer for Medical Image Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S2.SS2" title="In 2 Related Works ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S2.SS3" title="In 2 Related Works ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>LLMs for Medical Image Segmentation tasks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S3" title="In MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S3.SS1" title="In 3 Methodology ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overall Framework Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S3.SS2" title="In 3 Methodology ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Vision Transformer Architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4" title="In MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS1" title="In 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset and Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS2" title="In 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Baselines and comparison metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS3" title="In 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Quantitative Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS3.SSS1" title="In 4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Comparative Analysis of ViT with MedVisionLlama</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS3.SSS2" title="In 4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Training Performance comparison between ViT and MedVisionLlama</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS4" title="In 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Ablation studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS4.SSS1" title="In 4.4 Ablation studies ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Model capacity and Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS4.SSS2" title="In 4.4 Ablation studies ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Comparison with several pre-trained LLMs</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S5" title="In MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_italic" id="id1.id1">MedVisionLlama</span>: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gurucharan Marthi Krishna Kumar
<br class="ltx_break"/>Montreal Neurological Institute, McGill University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1" style="font-size:90%;">gurucharan.marthikrishnakumar@mail.mcgill.ca</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aman Chadha
<br class="ltx_break"/>Amazon
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.1.id1" style="font-size:90%;">aman@amanchadha.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Janine Mendola
<br class="ltx_break"/>Dept. of Ophthalmology, McGill University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.1.id1" style="font-size:90%;">janine.mendola@mcgill.ca</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amir Shmuel
<br class="ltx_break"/>Montreal Neurological Institute, McGill University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.1.id1" style="font-size:90%;">amir.shmuel@mcgill.ca</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: <a class="ltx_ref ltx_href" href="https://bit.ly/3zf2CVs" title="">https://bit.ly/3zf2CVs</a></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Medical imaging, including MRI, X-rays, and microscopy, is vital in healthcare by providing critical visual data for diagnosing diseases and formulating treatment plans. However, interpreting these images is a challenging task requiring significant expertise. Issues like low resolution and noise in scanned images can lead to misdiagnosis, resulting in incorrect or delayed treatments that negatively impact patient health <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib33" title="">33</a>]</cite>. Recent advancements in deep learning, particularly with Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib11" title="">11</a>]</cite>, have shown potential in improving the accuracy of medical image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite their promise, ViTs in biomedical imaging face substantial challenges. Training these models requires extensive, meticulously labeled datasets, and creating such datasets in biomedical imaging is particularly burdensome due to the necessity of expert knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib25" title="">25</a>]</cite>. Optimizing ViTs is challenging due to the need for extensive parameter tuning, which requires deep architectural knowledge and significant computational resources, often surpassing practical limits in terms of time and cost.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This research explores methods to improve performance of ViTs in biomedical imaging while avoiding the need for larger datasets or excessive computational resources. LLMs trained on extensive textual data demonstrate notable versatility, even extending to computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib1" title="">1</a>]</cite>. They can interpret visual tokens and integrate them into structured formats within multimodal frameworks, using linear projections or cross-attention mechanisms to process these tokens.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our approach draws inspiration from previous studies involving the use of pre-trained LLMs for visual token processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib25" title="">25</a>]</cite>. Our method involves using a pre-trained LLM transformer block as a visual encoder layer, significantly deviating from traditional Vision-Language Model (VLM) designs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib26" title="">26</a>]</cite>. This method operates independently of language prompts, inputs, or outputs, allows for training from scratch without needing pre-trained backbones like CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib31" title="">31</a>]</cite>, and simplifies the use of LLMs by decoupling them into separate transformer blocks.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This method proves particularly advantageous for medical image segmentation tasks, enabling a wide range of applications. By employing a simple yet under-explored approach, we find that integrating a frozen transformer block from pre-trained LLMs as a visual encoder layer improves performance. Additionally, our examination of various LLMs and transformer blocks reveals that the effectiveness of frozen LLM transformers in visual encoding is a consistent phenomenon, offering significant potential for enhancing medical image analysis. In summary, our paper presents the following key contributions.</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce a frozen transformer block from a pre-trained LLM network into the encoder of a ViT-based medical image segmentation model, significantly improving both performance and accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose an innovative Hybrid Attention Mechanism that combines efficient and channel attention to balance global and local feature learning, along with a Multi-Scale Fusion Block that aggregates features across various scales to enhance segmentation precision.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We evaluate the effect of incorporating a frozen LLM transformer block on segmentation performance across 10 different medical imaging modalities, supported by ablation studies to further assess its effectiveness.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vision Transformer for Medical Image Segmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">CNN-based techniques like U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib32" title="">32</a>]</cite> have notably advanced medical image segmentation by replacing per-image optimization with global function optimization, thus speeding up performance after training. However, CNN-based architectures have limitations in modeling long-range spatial relations due to the intrinsic locality of convolution operations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib37" title="">37</a>]</cite>. Although U-Net employs down-sampling and up-sampling techniques, its effective receptive field remains constrained, limiting its capacity to capture long-range dependencies and accommodate significant deformations in medical images. Various enhancements, such as nested U-Net (U-Net++) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib50" title="">50</a>]</cite>, dilated convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib48" title="">48</a>]</cite> and attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib29" title="">29</a>]</cite>, have been proposed to address these limitations, but advanced CNN-based network architectures for medical image segmentation remain underexplored.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib38" title="">38</a>]</cite>, initially developed for natural language processing tasks, have shown great potential in computer vision. By utilizing the self-attention mechanism, Transformers can capture long-range spatial information, overcoming the limitations of convolutional operations. The ViT was the first purely self-attention-based network applied to images, achieving state-of-the-art performance in image recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib11" title="">11</a>]</cite>. Unlike convolutional operations, which are constrained by the size of their kernels, transformers possess large receptive fields that enable them to identify correlations among patches, or tokens, across an entire image. The significant success of ViTs in image segmentation has spurred the development of various transformer variants, such as the Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib28" title="">28</a>]</cite>, Pyramid Vision Transformer (PVT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib41" title="">41</a>]</cite>, and Convolutional Vision Transformer (CvT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib45" title="">45</a>]</cite>, each tailored to enhance segmentation performance further. Transformer-based methods have recently gained traction in medical imaging, particularly for image segmentation tasks, showcasing their ability to effectively model long-range spatial relationships and improve segmentation performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib13" title="">13</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Large Language Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The evolution of LLMs began with the pre-training of transformers using masked token prediction, a method that greatly enhances their adaptability across different tasks and modalities, as prominently demonstrated. Building upon this foundation, several models such as GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib5" title="">5</a>]</cite>, Llama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib12" title="">12</a>]</cite>, and PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib8" title="">8</a>]</cite>, with their billions of parameters, have exhibited remarkable in-context learning abilities and exceptional performance in zero-shot tasks, including text classification and text infilling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib10" title="">10</a>]</cite>. As evidenced by the results in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS3" title="4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.3</span></a> that show improved performance in medical image segmentation and registration, transformer blocks within these large-scale models possess a unique capability to engage with biomedical data, thereby enhancing a broad range of computer vision tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>LLMs for Medical Image Segmentation tasks</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">In the biomedical imaging field, LLMs have sparked a surge of innovative applications due to their generative potential. Traditional approaches either project visual features directly onto the LLM input layer or use latent bottlenecks to further encode visual tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib40" title="">40</a>]</cite>. Research has explored the effects of integrating LLM decoders into visual tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib39" title="">39</a>]</cite>, as well as how utilizing LLMs to interpret label features can improve medical image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib24" title="">24</a>]</cite>. Recent studies have demonstrated that incorporating a text-pretrained, frozen LLM block into the encoder of a ViT not only enhances training performance but also boosts accuracy in biomedical image classification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib25" title="">25</a>]</cite>. This study underscores the potential of leveraging transformer blocks in LLMs as adaptable encoders for visual data, extending their utility beyond their traditional roles as text embeddings or tokenized output decoders.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overall Framework Design</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The main framework utilizing a frozen layer of a pretrained LLM for medical image segmentation tasks is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S3.F1" title="Figure 1 ‣ 3.1 Overall Framework Design ‣ 3 Methodology ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="S3.F1.g1" src="extracted/5901749/Figures/LLM_Architecture.png" width="361"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F1.4.2" style="font-size:90%;">Overall Framework of the LLM enhanced ViT Model for Medical Image Segmentation.<span class="ltx_text ltx_font_medium" id="S3.F1.4.2.1"> The architecture integrates a frozen pre-trained LLM transformer block into a Vision Transformer, enhancing feature representation for 3D medical image segmentation with residual connections to improve performance.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.6">Consider a ViT network with an encoder part, denoted as <math alttext="V_{E}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">V</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑉</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">V_{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_V start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT</annotation></semantics></math>, which processes the input image <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">bold_x</annotation></semantics></math>. This encoder splits the image into patches and applies a patch embedding module. The encoder includes a hybrid attention block, which plays a crucial role in capturing both global and local contextual information from the image patches. The encoder transforms the input image into a latent representation <math alttext="\mathbf{z}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathbf{z}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">bold_z</annotation></semantics></math>. The decoder part, denoted as <math alttext="V_{D}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">V</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑉</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">V_{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_V start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT</annotation></semantics></math>, reconstructs the latent representation <math alttext="\mathbf{z}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\mathbf{z}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">bold_z</annotation></semantics></math> back into the original spatial dimensions to produce the final segmentation map <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><mi id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">bold_y</annotation></semantics></math>. Thus, the transformation process from the input image to the segmentation output is represented as:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle V_{E}(\mathbf{x})\rightarrow\mathbf{z},\quad V_{D}(\mathbf{z})%
\rightarrow\mathbf{y}" class="ltx_Math" display="inline" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.3.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><msub id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml">V</mi><mi id="S3.E1.m1.3.3.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml">E</mi></msub><mo id="S3.E1.m1.3.3.1.1.2.1" xref="S3.E1.m1.3.3.1.1.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.2.3.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.2.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝐱</mi><mo id="S3.E1.m1.3.3.1.1.2.3.2.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.1" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.cmml">→</mo><mi id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">𝐳</mi></mrow><mo id="S3.E1.m1.4.4.2.3" rspace="1.167em" xref="S3.E1.m1.4.4.3a.cmml">,</mo><mrow id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml"><mrow id="S3.E1.m1.4.4.2.2.2" xref="S3.E1.m1.4.4.2.2.2.cmml"><msub id="S3.E1.m1.4.4.2.2.2.2" xref="S3.E1.m1.4.4.2.2.2.2.cmml"><mi id="S3.E1.m1.4.4.2.2.2.2.2" xref="S3.E1.m1.4.4.2.2.2.2.2.cmml">V</mi><mi id="S3.E1.m1.4.4.2.2.2.2.3" xref="S3.E1.m1.4.4.2.2.2.2.3.cmml">D</mi></msub><mo id="S3.E1.m1.4.4.2.2.2.1" xref="S3.E1.m1.4.4.2.2.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.2.2.2.3.2" xref="S3.E1.m1.4.4.2.2.2.cmml"><mo id="S3.E1.m1.4.4.2.2.2.3.2.1" stretchy="false" xref="S3.E1.m1.4.4.2.2.2.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">𝐳</mi><mo id="S3.E1.m1.4.4.2.2.2.3.2.2" stretchy="false" xref="S3.E1.m1.4.4.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.2.2.1" stretchy="false" xref="S3.E1.m1.4.4.2.2.1.cmml">→</mo><mi id="S3.E1.m1.4.4.2.2.3" xref="S3.E1.m1.4.4.2.2.3.cmml">𝐲</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.3a.cmml" xref="S3.E1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><ci id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1">→</ci><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><times id="S3.E1.m1.3.3.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1"></times><apply id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2">𝑉</ci><ci id="S3.E1.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3">𝐸</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐱</ci></apply><ci id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3">𝐳</ci></apply><apply id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2"><ci id="S3.E1.m1.4.4.2.2.1.cmml" xref="S3.E1.m1.4.4.2.2.1">→</ci><apply id="S3.E1.m1.4.4.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2"><times id="S3.E1.m1.4.4.2.2.2.1.cmml" xref="S3.E1.m1.4.4.2.2.2.1"></times><apply id="S3.E1.m1.4.4.2.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.2.2.2.1.cmml" xref="S3.E1.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.4.4.2.2.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2.2">𝑉</ci><ci id="S3.E1.m1.4.4.2.2.2.2.3.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3">𝐷</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐳</ci></apply><ci id="S3.E1.m1.4.4.2.2.3.cmml" xref="S3.E1.m1.4.4.2.2.3">𝐲</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\displaystyle V_{E}(\mathbf{x})\rightarrow\mathbf{z},\quad V_{D}(\mathbf{z})%
\rightarrow\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">italic_V start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ( bold_x ) → bold_z , italic_V start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( bold_z ) → bold_y</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.14">Building upon the foundational ViT network, we enhance its architecture by integrating a frozen transformer block from a pre-trained LLM model, referred to as <math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">V</mi><mtext id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝑉</ci><ci id="S3.SS1.p4.1.m1.1.1.3a.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><mtext id="S3.SS1.p4.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p4.1.m1.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math>. In this study, we utilize the pre-trained weights from several publicly available LLM models, such as Meta-Llama-3.1-8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib12" title="">12</a>]</cite>, Gemma-2-9B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib35" title="">35</a>]</cite>, Mistral-7B-v0.1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib21" title="">21</a>]</cite>, Qwen2-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib46" title="">46</a>]</cite>, and Yi-1.5-9B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib47" title="">47</a>]</cite>. These advanced blocks, trained on several billion parameters, are strategically inserted between the encoder <math alttext="V_{E}" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">V</mi><mi id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">𝑉</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">V_{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_V start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT</annotation></semantics></math> and the decoder <math alttext="V_{D}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><msub id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">V</mi><mi id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝑉</ci><ci id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">V_{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">italic_V start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT</annotation></semantics></math>. To align the feature dimensions between <math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m4.1"><semantics id="S3.SS1.p4.4.m4.1a"><msub id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">V</mi><mtext id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">𝑉</ci><ci id="S3.SS1.p4.4.m4.1.1.3a.cmml" xref="S3.SS1.p4.4.m4.1.1.3"><mtext id="S3.SS1.p4.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p4.4.m4.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.4.m4.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="V_{E}" class="ltx_Math" display="inline" id="S3.SS1.p4.5.m5.1"><semantics id="S3.SS1.p4.5.m5.1a"><msub id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml">V</mi><mi id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2">𝑉</ci><ci id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">V_{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.5.m5.1d">italic_V start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT</annotation></semantics></math>, we introduce two linear layers, <math alttext="V_{L_{1}}" class="ltx_Math" display="inline" id="S3.SS1.p4.6.m6.1"><semantics id="S3.SS1.p4.6.m6.1a"><msub id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml"><mi id="S3.SS1.p4.6.m6.1.1.2" xref="S3.SS1.p4.6.m6.1.1.2.cmml">V</mi><msub id="S3.SS1.p4.6.m6.1.1.3" xref="S3.SS1.p4.6.m6.1.1.3.cmml"><mi id="S3.SS1.p4.6.m6.1.1.3.2" xref="S3.SS1.p4.6.m6.1.1.3.2.cmml">L</mi><mn id="S3.SS1.p4.6.m6.1.1.3.3" xref="S3.SS1.p4.6.m6.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><apply id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m6.1.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p4.6.m6.1.1.2.cmml" xref="S3.SS1.p4.6.m6.1.1.2">𝑉</ci><apply id="S3.SS1.p4.6.m6.1.1.3.cmml" xref="S3.SS1.p4.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m6.1.1.3.1.cmml" xref="S3.SS1.p4.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.6.m6.1.1.3.2.cmml" xref="S3.SS1.p4.6.m6.1.1.3.2">𝐿</ci><cn id="S3.SS1.p4.6.m6.1.1.3.3.cmml" type="integer" xref="S3.SS1.p4.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">V_{L_{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.6.m6.1d">italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="V_{L_{2}}" class="ltx_Math" display="inline" id="S3.SS1.p4.7.m7.1"><semantics id="S3.SS1.p4.7.m7.1a"><msub id="S3.SS1.p4.7.m7.1.1" xref="S3.SS1.p4.7.m7.1.1.cmml"><mi id="S3.SS1.p4.7.m7.1.1.2" xref="S3.SS1.p4.7.m7.1.1.2.cmml">V</mi><msub id="S3.SS1.p4.7.m7.1.1.3" xref="S3.SS1.p4.7.m7.1.1.3.cmml"><mi id="S3.SS1.p4.7.m7.1.1.3.2" xref="S3.SS1.p4.7.m7.1.1.3.2.cmml">L</mi><mn id="S3.SS1.p4.7.m7.1.1.3.3" xref="S3.SS1.p4.7.m7.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.1b"><apply id="S3.SS1.p4.7.m7.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.1.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p4.7.m7.1.1.2.cmml" xref="S3.SS1.p4.7.m7.1.1.2">𝑉</ci><apply id="S3.SS1.p4.7.m7.1.1.3.cmml" xref="S3.SS1.p4.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.1.1.3.1.cmml" xref="S3.SS1.p4.7.m7.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.7.m7.1.1.3.2.cmml" xref="S3.SS1.p4.7.m7.1.1.3.2">𝐿</ci><cn id="S3.SS1.p4.7.m7.1.1.3.3.cmml" type="integer" xref="S3.SS1.p4.7.m7.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">V_{L_{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.7.m7.1d">italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, to adapt the dimensions appropriately. Specifically, <math alttext="V_{L_{1}}" class="ltx_Math" display="inline" id="S3.SS1.p4.8.m8.1"><semantics id="S3.SS1.p4.8.m8.1a"><msub id="S3.SS1.p4.8.m8.1.1" xref="S3.SS1.p4.8.m8.1.1.cmml"><mi id="S3.SS1.p4.8.m8.1.1.2" xref="S3.SS1.p4.8.m8.1.1.2.cmml">V</mi><msub id="S3.SS1.p4.8.m8.1.1.3" xref="S3.SS1.p4.8.m8.1.1.3.cmml"><mi id="S3.SS1.p4.8.m8.1.1.3.2" xref="S3.SS1.p4.8.m8.1.1.3.2.cmml">L</mi><mn id="S3.SS1.p4.8.m8.1.1.3.3" xref="S3.SS1.p4.8.m8.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m8.1b"><apply id="S3.SS1.p4.8.m8.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.8.m8.1.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p4.8.m8.1.1.2.cmml" xref="S3.SS1.p4.8.m8.1.1.2">𝑉</ci><apply id="S3.SS1.p4.8.m8.1.1.3.cmml" xref="S3.SS1.p4.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.8.m8.1.1.3.1.cmml" xref="S3.SS1.p4.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.8.m8.1.1.3.2.cmml" xref="S3.SS1.p4.8.m8.1.1.3.2">𝐿</ci><cn id="S3.SS1.p4.8.m8.1.1.3.3.cmml" type="integer" xref="S3.SS1.p4.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m8.1c">V_{L_{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.8.m8.1d">italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is applied just before <math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S3.SS1.p4.9.m9.1"><semantics id="S3.SS1.p4.9.m9.1a"><msub id="S3.SS1.p4.9.m9.1.1" xref="S3.SS1.p4.9.m9.1.1.cmml"><mi id="S3.SS1.p4.9.m9.1.1.2" xref="S3.SS1.p4.9.m9.1.1.2.cmml">V</mi><mtext id="S3.SS1.p4.9.m9.1.1.3" xref="S3.SS1.p4.9.m9.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.9.m9.1b"><apply id="S3.SS1.p4.9.m9.1.1.cmml" xref="S3.SS1.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.9.m9.1.1.1.cmml" xref="S3.SS1.p4.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p4.9.m9.1.1.2.cmml" xref="S3.SS1.p4.9.m9.1.1.2">𝑉</ci><ci id="S3.SS1.p4.9.m9.1.1.3a.cmml" xref="S3.SS1.p4.9.m9.1.1.3"><mtext id="S3.SS1.p4.9.m9.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p4.9.m9.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.9.m9.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.9.m9.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math> to map the patch embedding output from <math alttext="V_{E}" class="ltx_Math" display="inline" id="S3.SS1.p4.10.m10.1"><semantics id="S3.SS1.p4.10.m10.1a"><msub id="S3.SS1.p4.10.m10.1.1" xref="S3.SS1.p4.10.m10.1.1.cmml"><mi id="S3.SS1.p4.10.m10.1.1.2" xref="S3.SS1.p4.10.m10.1.1.2.cmml">V</mi><mi id="S3.SS1.p4.10.m10.1.1.3" xref="S3.SS1.p4.10.m10.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.10.m10.1b"><apply id="S3.SS1.p4.10.m10.1.1.cmml" xref="S3.SS1.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.10.m10.1.1.1.cmml" xref="S3.SS1.p4.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p4.10.m10.1.1.2.cmml" xref="S3.SS1.p4.10.m10.1.1.2">𝑉</ci><ci id="S3.SS1.p4.10.m10.1.1.3.cmml" xref="S3.SS1.p4.10.m10.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.10.m10.1c">V_{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.10.m10.1d">italic_V start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT</annotation></semantics></math> to the dimensions required by <math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S3.SS1.p4.11.m11.1"><semantics id="S3.SS1.p4.11.m11.1a"><msub id="S3.SS1.p4.11.m11.1.1" xref="S3.SS1.p4.11.m11.1.1.cmml"><mi id="S3.SS1.p4.11.m11.1.1.2" xref="S3.SS1.p4.11.m11.1.1.2.cmml">V</mi><mtext id="S3.SS1.p4.11.m11.1.1.3" xref="S3.SS1.p4.11.m11.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.11.m11.1b"><apply id="S3.SS1.p4.11.m11.1.1.cmml" xref="S3.SS1.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.11.m11.1.1.1.cmml" xref="S3.SS1.p4.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p4.11.m11.1.1.2.cmml" xref="S3.SS1.p4.11.m11.1.1.2">𝑉</ci><ci id="S3.SS1.p4.11.m11.1.1.3a.cmml" xref="S3.SS1.p4.11.m11.1.1.3"><mtext id="S3.SS1.p4.11.m11.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p4.11.m11.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.11.m11.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.11.m11.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math>. Following <math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S3.SS1.p4.12.m12.1"><semantics id="S3.SS1.p4.12.m12.1a"><msub id="S3.SS1.p4.12.m12.1.1" xref="S3.SS1.p4.12.m12.1.1.cmml"><mi id="S3.SS1.p4.12.m12.1.1.2" xref="S3.SS1.p4.12.m12.1.1.2.cmml">V</mi><mtext id="S3.SS1.p4.12.m12.1.1.3" xref="S3.SS1.p4.12.m12.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.12.m12.1b"><apply id="S3.SS1.p4.12.m12.1.1.cmml" xref="S3.SS1.p4.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.12.m12.1.1.1.cmml" xref="S3.SS1.p4.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.p4.12.m12.1.1.2.cmml" xref="S3.SS1.p4.12.m12.1.1.2">𝑉</ci><ci id="S3.SS1.p4.12.m12.1.1.3a.cmml" xref="S3.SS1.p4.12.m12.1.1.3"><mtext id="S3.SS1.p4.12.m12.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p4.12.m12.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.12.m12.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.12.m12.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="V_{L_{2}}" class="ltx_Math" display="inline" id="S3.SS1.p4.13.m13.1"><semantics id="S3.SS1.p4.13.m13.1a"><msub id="S3.SS1.p4.13.m13.1.1" xref="S3.SS1.p4.13.m13.1.1.cmml"><mi id="S3.SS1.p4.13.m13.1.1.2" xref="S3.SS1.p4.13.m13.1.1.2.cmml">V</mi><msub id="S3.SS1.p4.13.m13.1.1.3" xref="S3.SS1.p4.13.m13.1.1.3.cmml"><mi id="S3.SS1.p4.13.m13.1.1.3.2" xref="S3.SS1.p4.13.m13.1.1.3.2.cmml">L</mi><mn id="S3.SS1.p4.13.m13.1.1.3.3" xref="S3.SS1.p4.13.m13.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.13.m13.1b"><apply id="S3.SS1.p4.13.m13.1.1.cmml" xref="S3.SS1.p4.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.13.m13.1.1.1.cmml" xref="S3.SS1.p4.13.m13.1.1">subscript</csymbol><ci id="S3.SS1.p4.13.m13.1.1.2.cmml" xref="S3.SS1.p4.13.m13.1.1.2">𝑉</ci><apply id="S3.SS1.p4.13.m13.1.1.3.cmml" xref="S3.SS1.p4.13.m13.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.13.m13.1.1.3.1.cmml" xref="S3.SS1.p4.13.m13.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.13.m13.1.1.3.2.cmml" xref="S3.SS1.p4.13.m13.1.1.3.2">𝐿</ci><cn id="S3.SS1.p4.13.m13.1.1.3.3.cmml" type="integer" xref="S3.SS1.p4.13.m13.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.13.m13.1c">V_{L_{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.13.m13.1d">italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> re-maps the output from the frozen LLM Transformer block back to the dimensions of the original ViT, allowing the embedded patches to be converted back to the original spatial dimensions by <math alttext="V_{D}" class="ltx_Math" display="inline" id="S3.SS1.p4.14.m14.1"><semantics id="S3.SS1.p4.14.m14.1a"><msub id="S3.SS1.p4.14.m14.1.1" xref="S3.SS1.p4.14.m14.1.1.cmml"><mi id="S3.SS1.p4.14.m14.1.1.2" xref="S3.SS1.p4.14.m14.1.1.2.cmml">V</mi><mi id="S3.SS1.p4.14.m14.1.1.3" xref="S3.SS1.p4.14.m14.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.14.m14.1b"><apply id="S3.SS1.p4.14.m14.1.1.cmml" xref="S3.SS1.p4.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.14.m14.1.1.1.cmml" xref="S3.SS1.p4.14.m14.1.1">subscript</csymbol><ci id="S3.SS1.p4.14.m14.1.1.2.cmml" xref="S3.SS1.p4.14.m14.1.1.2">𝑉</ci><ci id="S3.SS1.p4.14.m14.1.1.3.cmml" xref="S3.SS1.p4.14.m14.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.14.m14.1c">V_{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.14.m14.1d">italic_V start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT</annotation></semantics></math>. The resulting network can be represented as:</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx2">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle V_{E}(\mathbf{x})\rightarrow\mathbf{w},V_{L_{1}}V_{\text{LLM}}(%
\mathbf{w})V_{L_{2}}\rightarrow\mathbf{z},V_{D}(\mathbf{z})\rightarrow\mathbf{y}" class="ltx_Math" display="inline" id="S3.E2.m1.5"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5.2" xref="S3.E2.m1.5.5.3.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml"><msub id="S3.E2.m1.4.4.1.1.2.2" xref="S3.E2.m1.4.4.1.1.2.2.cmml"><mi id="S3.E2.m1.4.4.1.1.2.2.2" xref="S3.E2.m1.4.4.1.1.2.2.2.cmml">V</mi><mi id="S3.E2.m1.4.4.1.1.2.2.3" xref="S3.E2.m1.4.4.1.1.2.2.3.cmml">E</mi></msub><mo id="S3.E2.m1.4.4.1.1.2.1" xref="S3.E2.m1.4.4.1.1.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.2.3.2" xref="S3.E2.m1.4.4.1.1.2.cmml"><mo id="S3.E2.m1.4.4.1.1.2.3.2.1" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">𝐱</mi><mo id="S3.E2.m1.4.4.1.1.2.3.2.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.1" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.cmml">→</mo><mi id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml">𝐰</mi></mrow><mo id="S3.E2.m1.5.5.2.3" xref="S3.E2.m1.5.5.3a.cmml">,</mo><mrow id="S3.E2.m1.5.5.2.2.2" xref="S3.E2.m1.5.5.2.2.3.cmml"><mrow id="S3.E2.m1.5.5.2.2.1.1" xref="S3.E2.m1.5.5.2.2.1.1.cmml"><mrow id="S3.E2.m1.5.5.2.2.1.1.2" xref="S3.E2.m1.5.5.2.2.1.1.2.cmml"><msub id="S3.E2.m1.5.5.2.2.1.1.2.2" xref="S3.E2.m1.5.5.2.2.1.1.2.2.cmml"><mi id="S3.E2.m1.5.5.2.2.1.1.2.2.2" xref="S3.E2.m1.5.5.2.2.1.1.2.2.2.cmml">V</mi><msub id="S3.E2.m1.5.5.2.2.1.1.2.2.3" xref="S3.E2.m1.5.5.2.2.1.1.2.2.3.cmml"><mi id="S3.E2.m1.5.5.2.2.1.1.2.2.3.2" xref="S3.E2.m1.5.5.2.2.1.1.2.2.3.2.cmml">L</mi><mn id="S3.E2.m1.5.5.2.2.1.1.2.2.3.3" xref="S3.E2.m1.5.5.2.2.1.1.2.2.3.3.cmml">1</mn></msub></msub><mo id="S3.E2.m1.5.5.2.2.1.1.2.1" xref="S3.E2.m1.5.5.2.2.1.1.2.1.cmml">⁢</mo><msub id="S3.E2.m1.5.5.2.2.1.1.2.3" xref="S3.E2.m1.5.5.2.2.1.1.2.3.cmml"><mi id="S3.E2.m1.5.5.2.2.1.1.2.3.2" xref="S3.E2.m1.5.5.2.2.1.1.2.3.2.cmml">V</mi><mtext id="S3.E2.m1.5.5.2.2.1.1.2.3.3" xref="S3.E2.m1.5.5.2.2.1.1.2.3.3a.cmml">LLM</mtext></msub><mo id="S3.E2.m1.5.5.2.2.1.1.2.1a" xref="S3.E2.m1.5.5.2.2.1.1.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.5.5.2.2.1.1.2.4.2" xref="S3.E2.m1.5.5.2.2.1.1.2.cmml"><mo id="S3.E2.m1.5.5.2.2.1.1.2.4.2.1" stretchy="false" xref="S3.E2.m1.5.5.2.2.1.1.2.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">𝐰</mi><mo id="S3.E2.m1.5.5.2.2.1.1.2.4.2.2" stretchy="false" xref="S3.E2.m1.5.5.2.2.1.1.2.cmml">)</mo></mrow><mo id="S3.E2.m1.5.5.2.2.1.1.2.1b" xref="S3.E2.m1.5.5.2.2.1.1.2.1.cmml">⁢</mo><msub id="S3.E2.m1.5.5.2.2.1.1.2.5" xref="S3.E2.m1.5.5.2.2.1.1.2.5.cmml"><mi id="S3.E2.m1.5.5.2.2.1.1.2.5.2" xref="S3.E2.m1.5.5.2.2.1.1.2.5.2.cmml">V</mi><msub id="S3.E2.m1.5.5.2.2.1.1.2.5.3" xref="S3.E2.m1.5.5.2.2.1.1.2.5.3.cmml"><mi id="S3.E2.m1.5.5.2.2.1.1.2.5.3.2" xref="S3.E2.m1.5.5.2.2.1.1.2.5.3.2.cmml">L</mi><mn id="S3.E2.m1.5.5.2.2.1.1.2.5.3.3" xref="S3.E2.m1.5.5.2.2.1.1.2.5.3.3.cmml">2</mn></msub></msub></mrow><mo id="S3.E2.m1.5.5.2.2.1.1.1" stretchy="false" xref="S3.E2.m1.5.5.2.2.1.1.1.cmml">→</mo><mi id="S3.E2.m1.5.5.2.2.1.1.3" xref="S3.E2.m1.5.5.2.2.1.1.3.cmml">𝐳</mi></mrow><mo id="S3.E2.m1.5.5.2.2.2.3" xref="S3.E2.m1.5.5.2.2.3a.cmml">,</mo><mrow id="S3.E2.m1.5.5.2.2.2.2" xref="S3.E2.m1.5.5.2.2.2.2.cmml"><mrow id="S3.E2.m1.5.5.2.2.2.2.2" xref="S3.E2.m1.5.5.2.2.2.2.2.cmml"><msub id="S3.E2.m1.5.5.2.2.2.2.2.2" xref="S3.E2.m1.5.5.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.5.5.2.2.2.2.2.2.2" xref="S3.E2.m1.5.5.2.2.2.2.2.2.2.cmml">V</mi><mi id="S3.E2.m1.5.5.2.2.2.2.2.2.3" xref="S3.E2.m1.5.5.2.2.2.2.2.2.3.cmml">D</mi></msub><mo id="S3.E2.m1.5.5.2.2.2.2.2.1" xref="S3.E2.m1.5.5.2.2.2.2.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.5.5.2.2.2.2.2.3.2" xref="S3.E2.m1.5.5.2.2.2.2.2.cmml"><mo id="S3.E2.m1.5.5.2.2.2.2.2.3.2.1" stretchy="false" xref="S3.E2.m1.5.5.2.2.2.2.2.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">𝐳</mi><mo id="S3.E2.m1.5.5.2.2.2.2.2.3.2.2" stretchy="false" xref="S3.E2.m1.5.5.2.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.5.5.2.2.2.2.1" stretchy="false" xref="S3.E2.m1.5.5.2.2.2.2.1.cmml">→</mo><mi id="S3.E2.m1.5.5.2.2.2.2.3" xref="S3.E2.m1.5.5.2.2.2.2.3.cmml">𝐲</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.3.cmml" xref="S3.E2.m1.5.5.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.3a.cmml" xref="S3.E2.m1.5.5.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1.1"><ci id="S3.E2.m1.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1">→</ci><apply id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2"><times id="S3.E2.m1.4.4.1.1.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1"></times><apply id="S3.E2.m1.4.4.1.1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2.2">𝑉</ci><ci id="S3.E2.m1.4.4.1.1.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.2.3">𝐸</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐱</ci></apply><ci id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3">𝐰</ci></apply><apply id="S3.E2.m1.5.5.2.2.3.cmml" xref="S3.E2.m1.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.3a.cmml" xref="S3.E2.m1.5.5.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.5.5.2.2.1.1.cmml" xref="S3.E2.m1.5.5.2.2.1.1"><ci id="S3.E2.m1.5.5.2.2.1.1.1.cmml" xref="S3.E2.m1.5.5.2.2.1.1.1">→</ci><apply id="S3.E2.m1.5.5.2.2.1.1.2.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2"><times id="S3.E2.m1.5.5.2.2.1.1.2.1.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.1"></times><apply id="S3.E2.m1.5.5.2.2.1.1.2.2.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.1.1.2.2.1.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.2.2.1.1.2.2.2.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.2.2">𝑉</ci><apply id="S3.E2.m1.5.5.2.2.1.1.2.2.3.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.1.1.2.2.3.1.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.2.3">subscript</csymbol><ci id="S3.E2.m1.5.5.2.2.1.1.2.2.3.2.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.2.3.2">𝐿</ci><cn id="S3.E2.m1.5.5.2.2.1.1.2.2.3.3.cmml" type="integer" xref="S3.E2.m1.5.5.2.2.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.5.5.2.2.1.1.2.3.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.1.1.2.3.1.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.3">subscript</csymbol><ci id="S3.E2.m1.5.5.2.2.1.1.2.3.2.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.3.2">𝑉</ci><ci id="S3.E2.m1.5.5.2.2.1.1.2.3.3a.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.3.3"><mtext id="S3.E2.m1.5.5.2.2.1.1.2.3.3.cmml" mathsize="70%" xref="S3.E2.m1.5.5.2.2.1.1.2.3.3">LLM</mtext></ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝐰</ci><apply id="S3.E2.m1.5.5.2.2.1.1.2.5.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.5"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.1.1.2.5.1.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.5">subscript</csymbol><ci id="S3.E2.m1.5.5.2.2.1.1.2.5.2.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.5.2">𝑉</ci><apply id="S3.E2.m1.5.5.2.2.1.1.2.5.3.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.5.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.1.1.2.5.3.1.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.5.3">subscript</csymbol><ci id="S3.E2.m1.5.5.2.2.1.1.2.5.3.2.cmml" xref="S3.E2.m1.5.5.2.2.1.1.2.5.3.2">𝐿</ci><cn id="S3.E2.m1.5.5.2.2.1.1.2.5.3.3.cmml" type="integer" xref="S3.E2.m1.5.5.2.2.1.1.2.5.3.3">2</cn></apply></apply></apply><ci id="S3.E2.m1.5.5.2.2.1.1.3.cmml" xref="S3.E2.m1.5.5.2.2.1.1.3">𝐳</ci></apply><apply id="S3.E2.m1.5.5.2.2.2.2.cmml" xref="S3.E2.m1.5.5.2.2.2.2"><ci id="S3.E2.m1.5.5.2.2.2.2.1.cmml" xref="S3.E2.m1.5.5.2.2.2.2.1">→</ci><apply id="S3.E2.m1.5.5.2.2.2.2.2.cmml" xref="S3.E2.m1.5.5.2.2.2.2.2"><times id="S3.E2.m1.5.5.2.2.2.2.2.1.cmml" xref="S3.E2.m1.5.5.2.2.2.2.2.1"></times><apply id="S3.E2.m1.5.5.2.2.2.2.2.2.cmml" xref="S3.E2.m1.5.5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.5.5.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.5.5.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.5.5.2.2.2.2.2.2.2">𝑉</ci><ci id="S3.E2.m1.5.5.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.5.5.2.2.2.2.2.2.3">𝐷</ci></apply><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝐳</ci></apply><ci id="S3.E2.m1.5.5.2.2.2.2.3.cmml" xref="S3.E2.m1.5.5.2.2.2.2.3">𝐲</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">\displaystyle V_{E}(\mathbf{x})\rightarrow\mathbf{w},V_{L_{1}}V_{\text{LLM}}(%
\mathbf{w})V_{L_{2}}\rightarrow\mathbf{z},V_{D}(\mathbf{z})\rightarrow\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.5d">italic_V start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ( bold_x ) → bold_w , italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT ( bold_w ) italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT → bold_z , italic_V start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( bold_z ) → bold_y</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.4">During training, the module <math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1.1"><semantics id="S3.SS1.p6.1.m1.1a"><msub id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml"><mi id="S3.SS1.p6.1.m1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.cmml">V</mi><mtext id="S3.SS1.p6.1.m1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.2">𝑉</ci><ci id="S3.SS1.p6.1.m1.1.1.3a.cmml" xref="S3.SS1.p6.1.m1.1.1.3"><mtext id="S3.SS1.p6.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p6.1.m1.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math> is kept frozen and is not updated, whereas all other components of the framework, including the linear layers <math alttext="V_{L_{1}}" class="ltx_Math" display="inline" id="S3.SS1.p6.2.m2.1"><semantics id="S3.SS1.p6.2.m2.1a"><msub id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml"><mi id="S3.SS1.p6.2.m2.1.1.2" xref="S3.SS1.p6.2.m2.1.1.2.cmml">V</mi><msub id="S3.SS1.p6.2.m2.1.1.3" xref="S3.SS1.p6.2.m2.1.1.3.cmml"><mi id="S3.SS1.p6.2.m2.1.1.3.2" xref="S3.SS1.p6.2.m2.1.1.3.2.cmml">L</mi><mn id="S3.SS1.p6.2.m2.1.1.3.3" xref="S3.SS1.p6.2.m2.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><apply id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p6.2.m2.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p6.2.m2.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.2">𝑉</ci><apply id="S3.SS1.p6.2.m2.1.1.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p6.2.m2.1.1.3.1.cmml" xref="S3.SS1.p6.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.p6.2.m2.1.1.3.2.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2">𝐿</ci><cn id="S3.SS1.p6.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS1.p6.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">V_{L_{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.2.m2.1d">italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="V_{L_{2}}" class="ltx_Math" display="inline" id="S3.SS1.p6.3.m3.1"><semantics id="S3.SS1.p6.3.m3.1a"><msub id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml"><mi id="S3.SS1.p6.3.m3.1.1.2" xref="S3.SS1.p6.3.m3.1.1.2.cmml">V</mi><msub id="S3.SS1.p6.3.m3.1.1.3" xref="S3.SS1.p6.3.m3.1.1.3.cmml"><mi id="S3.SS1.p6.3.m3.1.1.3.2" xref="S3.SS1.p6.3.m3.1.1.3.2.cmml">L</mi><mn id="S3.SS1.p6.3.m3.1.1.3.3" xref="S3.SS1.p6.3.m3.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><apply id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p6.3.m3.1.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p6.3.m3.1.1.2.cmml" xref="S3.SS1.p6.3.m3.1.1.2">𝑉</ci><apply id="S3.SS1.p6.3.m3.1.1.3.cmml" xref="S3.SS1.p6.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p6.3.m3.1.1.3.1.cmml" xref="S3.SS1.p6.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p6.3.m3.1.1.3.2.cmml" xref="S3.SS1.p6.3.m3.1.1.3.2">𝐿</ci><cn id="S3.SS1.p6.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS1.p6.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">V_{L_{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.3.m3.1d">italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, are trained normally. Additionally, to ensure compatibility with traditional visual backbones, the rotary positional embeddings and attention masks have been removed from <math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S3.SS1.p6.4.m4.1"><semantics id="S3.SS1.p6.4.m4.1a"><msub id="S3.SS1.p6.4.m4.1.1" xref="S3.SS1.p6.4.m4.1.1.cmml"><mi id="S3.SS1.p6.4.m4.1.1.2" xref="S3.SS1.p6.4.m4.1.1.2.cmml">V</mi><mtext id="S3.SS1.p6.4.m4.1.1.3" xref="S3.SS1.p6.4.m4.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.4.m4.1b"><apply id="S3.SS1.p6.4.m4.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p6.4.m4.1.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p6.4.m4.1.1.2.cmml" xref="S3.SS1.p6.4.m4.1.1.2">𝑉</ci><ci id="S3.SS1.p6.4.m4.1.1.3a.cmml" xref="S3.SS1.p6.4.m4.1.1.3"><mtext id="S3.SS1.p6.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p6.4.m4.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.4.m4.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.4.m4.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math> as discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib30" title="">30</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Vision Transformer Architecture</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.4">In this study, we employed a traditional ViT network specifically designed for image segmentation tasks. The process begins with an input image of dimensions <math alttext="H\times W\times D" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">H</mi><mo id="S3.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">W</mi><mo id="S3.SS2.p1.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.4" xref="S3.SS2.p1.1.m1.1.1.4.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝐻</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝑊</ci><ci id="S3.SS2.p1.1.m1.1.1.4.cmml" xref="S3.SS2.p1.1.m1.1.1.4">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">H\times W\times D</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_H × italic_W × italic_D</annotation></semantics></math>, where <math alttext="H" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_H</annotation></semantics></math>, <math alttext="W" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">W</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_W</annotation></semantics></math>, and <math alttext="D" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_D</annotation></semantics></math> denote the height, width, and depth of the image, respectively.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.5">Initially, the input image is divided into non-overlapping patches of size <math alttext="P\times P\times P" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">P</mi><mo id="S3.SS2.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">P</mi><mo id="S3.SS2.p2.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS2.p2.1.m1.1.1.4" xref="S3.SS2.p2.1.m1.1.1.4.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑃</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑃</ci><ci id="S3.SS2.p2.1.m1.1.1.4.cmml" xref="S3.SS2.p2.1.m1.1.1.4">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">P\times P\times P</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_P × italic_P × italic_P</annotation></semantics></math>. Each patch is then transformed into a 1-dimensional vector through a linear embedding layer. This transformation produces a tensor with shape <math alttext="(B,T,E)" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.3"><semantics id="S3.SS2.p2.2.m2.3a"><mrow id="S3.SS2.p2.2.m2.3.4.2" xref="S3.SS2.p2.2.m2.3.4.1.cmml"><mo id="S3.SS2.p2.2.m2.3.4.2.1" stretchy="false" xref="S3.SS2.p2.2.m2.3.4.1.cmml">(</mo><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">B</mi><mo id="S3.SS2.p2.2.m2.3.4.2.2" xref="S3.SS2.p2.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS2.p2.2.m2.2.2" xref="S3.SS2.p2.2.m2.2.2.cmml">T</mi><mo id="S3.SS2.p2.2.m2.3.4.2.3" xref="S3.SS2.p2.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS2.p2.2.m2.3.3" xref="S3.SS2.p2.2.m2.3.3.cmml">E</mi><mo id="S3.SS2.p2.2.m2.3.4.2.4" stretchy="false" xref="S3.SS2.p2.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.3b"><vector id="S3.SS2.p2.2.m2.3.4.1.cmml" xref="S3.SS2.p2.2.m2.3.4.2"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝐵</ci><ci id="S3.SS2.p2.2.m2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2">𝑇</ci><ci id="S3.SS2.p2.2.m2.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3">𝐸</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.3c">(B,T,E)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.3d">( italic_B , italic_T , italic_E )</annotation></semantics></math>, where <math alttext="B" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_B</annotation></semantics></math> is the batch size, <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">italic_T</annotation></semantics></math> is the number of patches, and <math alttext="E" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">E</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_E</annotation></semantics></math> is the embedding size of each patch.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{x}\in\mathbb{R}^{H\times W\times D}\rightarrow\text{Patches}%
\rightarrow\mathbf{x}\in\mathbb{R}^{B\times T\times E}" class="ltx_Math" display="block" id="S3.Ex1.m1.1"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">𝐱</mi><mo id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml">∈</mo><msup id="S3.Ex1.m1.1.1.4" xref="S3.Ex1.m1.1.1.4.cmml"><mi id="S3.Ex1.m1.1.1.4.2" xref="S3.Ex1.m1.1.1.4.2.cmml">ℝ</mi><mrow id="S3.Ex1.m1.1.1.4.3" xref="S3.Ex1.m1.1.1.4.3.cmml"><mi id="S3.Ex1.m1.1.1.4.3.2" xref="S3.Ex1.m1.1.1.4.3.2.cmml">H</mi><mo id="S3.Ex1.m1.1.1.4.3.1" lspace="0.222em" rspace="0.222em" xref="S3.Ex1.m1.1.1.4.3.1.cmml">×</mo><mi id="S3.Ex1.m1.1.1.4.3.3" xref="S3.Ex1.m1.1.1.4.3.3.cmml">W</mi><mo id="S3.Ex1.m1.1.1.4.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.Ex1.m1.1.1.4.3.1.cmml">×</mo><mi id="S3.Ex1.m1.1.1.4.3.4" xref="S3.Ex1.m1.1.1.4.3.4.cmml">D</mi></mrow></msup><mo id="S3.Ex1.m1.1.1.5" stretchy="false" xref="S3.Ex1.m1.1.1.5.cmml">→</mo><mtext id="S3.Ex1.m1.1.1.6" xref="S3.Ex1.m1.1.1.6a.cmml">Patches</mtext><mo id="S3.Ex1.m1.1.1.7" stretchy="false" xref="S3.Ex1.m1.1.1.7.cmml">→</mo><mi id="S3.Ex1.m1.1.1.8" xref="S3.Ex1.m1.1.1.8.cmml">𝐱</mi><mo id="S3.Ex1.m1.1.1.9" xref="S3.Ex1.m1.1.1.9.cmml">∈</mo><msup id="S3.Ex1.m1.1.1.10" xref="S3.Ex1.m1.1.1.10.cmml"><mi id="S3.Ex1.m1.1.1.10.2" xref="S3.Ex1.m1.1.1.10.2.cmml">ℝ</mi><mrow id="S3.Ex1.m1.1.1.10.3" xref="S3.Ex1.m1.1.1.10.3.cmml"><mi id="S3.Ex1.m1.1.1.10.3.2" xref="S3.Ex1.m1.1.1.10.3.2.cmml">B</mi><mo id="S3.Ex1.m1.1.1.10.3.1" lspace="0.222em" rspace="0.222em" xref="S3.Ex1.m1.1.1.10.3.1.cmml">×</mo><mi id="S3.Ex1.m1.1.1.10.3.3" xref="S3.Ex1.m1.1.1.10.3.3.cmml">T</mi><mo id="S3.Ex1.m1.1.1.10.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.Ex1.m1.1.1.10.3.1.cmml">×</mo><mi id="S3.Ex1.m1.1.1.10.3.4" xref="S3.Ex1.m1.1.1.10.3.4.cmml">E</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><and id="S3.Ex1.m1.1.1a.cmml" xref="S3.Ex1.m1.1.1"></and><apply id="S3.Ex1.m1.1.1b.cmml" xref="S3.Ex1.m1.1.1"><in id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"></in><ci id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2">𝐱</ci><apply id="S3.Ex1.m1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.4.1.cmml" xref="S3.Ex1.m1.1.1.4">superscript</csymbol><ci id="S3.Ex1.m1.1.1.4.2.cmml" xref="S3.Ex1.m1.1.1.4.2">ℝ</ci><apply id="S3.Ex1.m1.1.1.4.3.cmml" xref="S3.Ex1.m1.1.1.4.3"><times id="S3.Ex1.m1.1.1.4.3.1.cmml" xref="S3.Ex1.m1.1.1.4.3.1"></times><ci id="S3.Ex1.m1.1.1.4.3.2.cmml" xref="S3.Ex1.m1.1.1.4.3.2">𝐻</ci><ci id="S3.Ex1.m1.1.1.4.3.3.cmml" xref="S3.Ex1.m1.1.1.4.3.3">𝑊</ci><ci id="S3.Ex1.m1.1.1.4.3.4.cmml" xref="S3.Ex1.m1.1.1.4.3.4">𝐷</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1c.cmml" xref="S3.Ex1.m1.1.1"><ci id="S3.Ex1.m1.1.1.5.cmml" xref="S3.Ex1.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2410.02458v2#S3.Ex1.m1.1.1.4.cmml" id="S3.Ex1.m1.1.1d.cmml" xref="S3.Ex1.m1.1.1"></share><ci id="S3.Ex1.m1.1.1.6a.cmml" xref="S3.Ex1.m1.1.1.6"><mtext id="S3.Ex1.m1.1.1.6.cmml" xref="S3.Ex1.m1.1.1.6">Patches</mtext></ci></apply><apply id="S3.Ex1.m1.1.1e.cmml" xref="S3.Ex1.m1.1.1"><ci id="S3.Ex1.m1.1.1.7.cmml" xref="S3.Ex1.m1.1.1.7">→</ci><share href="https://arxiv.org/html/2410.02458v2#S3.Ex1.m1.1.1.6.cmml" id="S3.Ex1.m1.1.1f.cmml" xref="S3.Ex1.m1.1.1"></share><ci id="S3.Ex1.m1.1.1.8.cmml" xref="S3.Ex1.m1.1.1.8">𝐱</ci></apply><apply id="S3.Ex1.m1.1.1g.cmml" xref="S3.Ex1.m1.1.1"><in id="S3.Ex1.m1.1.1.9.cmml" xref="S3.Ex1.m1.1.1.9"></in><share href="https://arxiv.org/html/2410.02458v2#S3.Ex1.m1.1.1.8.cmml" id="S3.Ex1.m1.1.1h.cmml" xref="S3.Ex1.m1.1.1"></share><apply id="S3.Ex1.m1.1.1.10.cmml" xref="S3.Ex1.m1.1.1.10"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.10.1.cmml" xref="S3.Ex1.m1.1.1.10">superscript</csymbol><ci id="S3.Ex1.m1.1.1.10.2.cmml" xref="S3.Ex1.m1.1.1.10.2">ℝ</ci><apply id="S3.Ex1.m1.1.1.10.3.cmml" xref="S3.Ex1.m1.1.1.10.3"><times id="S3.Ex1.m1.1.1.10.3.1.cmml" xref="S3.Ex1.m1.1.1.10.3.1"></times><ci id="S3.Ex1.m1.1.1.10.3.2.cmml" xref="S3.Ex1.m1.1.1.10.3.2">𝐵</ci><ci id="S3.Ex1.m1.1.1.10.3.3.cmml" xref="S3.Ex1.m1.1.1.10.3.3">𝑇</ci><ci id="S3.Ex1.m1.1.1.10.3.4.cmml" xref="S3.Ex1.m1.1.1.10.3.4">𝐸</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\mathbf{x}\in\mathbb{R}^{H\times W\times D}\rightarrow\text{Patches}%
\rightarrow\mathbf{x}\in\mathbb{R}^{B\times T\times E}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.1d">bold_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W × italic_D end_POSTSUPERSCRIPT → Patches → bold_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_B × italic_T × italic_E end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Next, the tensor is processed through a Multiscale Fusion Block with scales of 3, 5 and 7. This block aggregates features from multiple scales, enriching the representation with multi-scale contextual information. Subsequently, the sequence of these multi-scale fused patches is processed through the hybrid attention block within the Transformer encoder. This block integrates Efficient Attention and Channel Attention mechanisms. Efficient Attention captures global dependencies and long-range relationships across patches, while Channel Attention refines feature representations by focusing on the importance of different channels. These combined attention mechanisms enhance the feature extraction process, significantly enriching the tensor’s representations.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="266" id="S3.F2.g1" src="extracted/5901749/Figures/Encoder.png" width="182"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.4.2" style="font-size:90%;">ViT Encoder Block.<span class="ltx_text ltx_font_medium" id="S3.F2.4.2.1"> The encoder consists of linear projections, followed by layer normalization, efficient attention, channel attention, and an MLP block to produce the output embeddings.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.2">The tensor continues to retain its shape of <math alttext="(B,T,E)" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.3"><semantics id="S3.SS2.p5.1.m1.3a"><mrow id="S3.SS2.p5.1.m1.3.4.2" xref="S3.SS2.p5.1.m1.3.4.1.cmml"><mo id="S3.SS2.p5.1.m1.3.4.2.1" stretchy="false" xref="S3.SS2.p5.1.m1.3.4.1.cmml">(</mo><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">B</mi><mo id="S3.SS2.p5.1.m1.3.4.2.2" xref="S3.SS2.p5.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS2.p5.1.m1.2.2" xref="S3.SS2.p5.1.m1.2.2.cmml">T</mi><mo id="S3.SS2.p5.1.m1.3.4.2.3" xref="S3.SS2.p5.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS2.p5.1.m1.3.3" xref="S3.SS2.p5.1.m1.3.3.cmml">E</mi><mo id="S3.SS2.p5.1.m1.3.4.2.4" stretchy="false" xref="S3.SS2.p5.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.3b"><vector id="S3.SS2.p5.1.m1.3.4.1.cmml" xref="S3.SS2.p5.1.m1.3.4.2"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">𝐵</ci><ci id="S3.SS2.p5.1.m1.2.2.cmml" xref="S3.SS2.p5.1.m1.2.2">𝑇</ci><ci id="S3.SS2.p5.1.m1.3.3.cmml" xref="S3.SS2.p5.1.m1.3.3">𝐸</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.3c">(B,T,E)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.3d">( italic_B , italic_T , italic_E )</annotation></semantics></math>, but the feature representations are considerably enhanced by the hybrid attention block. Residual connections are incorporated before and after the hybrid attention block to improve gradient flow and allow embeddings to pass through shortcut paths, enhancing gradient exchange and embedding propagation efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib25" title="">25</a>]</cite>. Despite the shape remaining <math alttext="(B,T,E)" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.3"><semantics id="S3.SS2.p5.2.m2.3a"><mrow id="S3.SS2.p5.2.m2.3.4.2" xref="S3.SS2.p5.2.m2.3.4.1.cmml"><mo id="S3.SS2.p5.2.m2.3.4.2.1" stretchy="false" xref="S3.SS2.p5.2.m2.3.4.1.cmml">(</mo><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">B</mi><mo id="S3.SS2.p5.2.m2.3.4.2.2" xref="S3.SS2.p5.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS2.p5.2.m2.2.2" xref="S3.SS2.p5.2.m2.2.2.cmml">T</mi><mo id="S3.SS2.p5.2.m2.3.4.2.3" xref="S3.SS2.p5.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS2.p5.2.m2.3.3" xref="S3.SS2.p5.2.m2.3.3.cmml">E</mi><mo id="S3.SS2.p5.2.m2.3.4.2.4" stretchy="false" xref="S3.SS2.p5.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.3b"><vector id="S3.SS2.p5.2.m2.3.4.1.cmml" xref="S3.SS2.p5.2.m2.3.4.2"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝐵</ci><ci id="S3.SS2.p5.2.m2.2.2.cmml" xref="S3.SS2.p5.2.m2.2.2">𝑇</ci><ci id="S3.SS2.p5.2.m2.3.3.cmml" xref="S3.SS2.p5.2.m2.3.3">𝐸</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.3c">(B,T,E)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.3d">( italic_B , italic_T , italic_E )</annotation></semantics></math>, the features are now significantly enriched with advanced contextual information.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="V_{\text{LLM}}(\mathbf{w})\rightarrow\mathbf{z},\mathbf{z}\in\mathbb{R}^{B%
\times T\times E}" class="ltx_Math" display="block" id="S3.Ex2.m1.3"><semantics id="S3.Ex2.m1.3a"><mrow id="S3.Ex2.m1.3.3.2" xref="S3.Ex2.m1.3.3.3.cmml"><mrow id="S3.Ex2.m1.2.2.1.1" xref="S3.Ex2.m1.2.2.1.1.cmml"><mrow id="S3.Ex2.m1.2.2.1.1.2" xref="S3.Ex2.m1.2.2.1.1.2.cmml"><msub id="S3.Ex2.m1.2.2.1.1.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.2.cmml">V</mi><mtext id="S3.Ex2.m1.2.2.1.1.2.2.3" xref="S3.Ex2.m1.2.2.1.1.2.2.3a.cmml">LLM</mtext></msub><mo id="S3.Ex2.m1.2.2.1.1.2.1" xref="S3.Ex2.m1.2.2.1.1.2.1.cmml">⁢</mo><mrow id="S3.Ex2.m1.2.2.1.1.2.3.2" xref="S3.Ex2.m1.2.2.1.1.2.cmml"><mo id="S3.Ex2.m1.2.2.1.1.2.3.2.1" stretchy="false" xref="S3.Ex2.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">𝐰</mi><mo id="S3.Ex2.m1.2.2.1.1.2.3.2.2" stretchy="false" xref="S3.Ex2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.2.2.1.1.1" stretchy="false" xref="S3.Ex2.m1.2.2.1.1.1.cmml">→</mo><mi id="S3.Ex2.m1.2.2.1.1.3" xref="S3.Ex2.m1.2.2.1.1.3.cmml">𝐳</mi></mrow><mo id="S3.Ex2.m1.3.3.2.3" xref="S3.Ex2.m1.3.3.3a.cmml">,</mo><mrow id="S3.Ex2.m1.3.3.2.2" xref="S3.Ex2.m1.3.3.2.2.cmml"><mi id="S3.Ex2.m1.3.3.2.2.2" xref="S3.Ex2.m1.3.3.2.2.2.cmml">𝐳</mi><mo id="S3.Ex2.m1.3.3.2.2.1" xref="S3.Ex2.m1.3.3.2.2.1.cmml">∈</mo><msup id="S3.Ex2.m1.3.3.2.2.3" xref="S3.Ex2.m1.3.3.2.2.3.cmml"><mi id="S3.Ex2.m1.3.3.2.2.3.2" xref="S3.Ex2.m1.3.3.2.2.3.2.cmml">ℝ</mi><mrow id="S3.Ex2.m1.3.3.2.2.3.3" xref="S3.Ex2.m1.3.3.2.2.3.3.cmml"><mi id="S3.Ex2.m1.3.3.2.2.3.3.2" xref="S3.Ex2.m1.3.3.2.2.3.3.2.cmml">B</mi><mo id="S3.Ex2.m1.3.3.2.2.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.Ex2.m1.3.3.2.2.3.3.1.cmml">×</mo><mi id="S3.Ex2.m1.3.3.2.2.3.3.3" xref="S3.Ex2.m1.3.3.2.2.3.3.3.cmml">T</mi><mo id="S3.Ex2.m1.3.3.2.2.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.Ex2.m1.3.3.2.2.3.3.1.cmml">×</mo><mi id="S3.Ex2.m1.3.3.2.2.3.3.4" xref="S3.Ex2.m1.3.3.2.2.3.3.4.cmml">E</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.3b"><apply id="S3.Ex2.m1.3.3.3.cmml" xref="S3.Ex2.m1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.3a.cmml" xref="S3.Ex2.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.Ex2.m1.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1"><ci id="S3.Ex2.m1.2.2.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1">→</ci><apply id="S3.Ex2.m1.2.2.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2"><times id="S3.Ex2.m1.2.2.1.1.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.1"></times><apply id="S3.Ex2.m1.2.2.1.1.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2">𝑉</ci><ci id="S3.Ex2.m1.2.2.1.1.2.2.3a.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3"><mtext id="S3.Ex2.m1.2.2.1.1.2.2.3.cmml" mathsize="70%" xref="S3.Ex2.m1.2.2.1.1.2.2.3">LLM</mtext></ci></apply><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">𝐰</ci></apply><ci id="S3.Ex2.m1.2.2.1.1.3.cmml" xref="S3.Ex2.m1.2.2.1.1.3">𝐳</ci></apply><apply id="S3.Ex2.m1.3.3.2.2.cmml" xref="S3.Ex2.m1.3.3.2.2"><in id="S3.Ex2.m1.3.3.2.2.1.cmml" xref="S3.Ex2.m1.3.3.2.2.1"></in><ci id="S3.Ex2.m1.3.3.2.2.2.cmml" xref="S3.Ex2.m1.3.3.2.2.2">𝐳</ci><apply id="S3.Ex2.m1.3.3.2.2.3.cmml" xref="S3.Ex2.m1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.2.2.3.1.cmml" xref="S3.Ex2.m1.3.3.2.2.3">superscript</csymbol><ci id="S3.Ex2.m1.3.3.2.2.3.2.cmml" xref="S3.Ex2.m1.3.3.2.2.3.2">ℝ</ci><apply id="S3.Ex2.m1.3.3.2.2.3.3.cmml" xref="S3.Ex2.m1.3.3.2.2.3.3"><times id="S3.Ex2.m1.3.3.2.2.3.3.1.cmml" xref="S3.Ex2.m1.3.3.2.2.3.3.1"></times><ci id="S3.Ex2.m1.3.3.2.2.3.3.2.cmml" xref="S3.Ex2.m1.3.3.2.2.3.3.2">𝐵</ci><ci id="S3.Ex2.m1.3.3.2.2.3.3.3.cmml" xref="S3.Ex2.m1.3.3.2.2.3.3.3">𝑇</ci><ci id="S3.Ex2.m1.3.3.2.2.3.3.4.cmml" xref="S3.Ex2.m1.3.3.2.2.3.3.4">𝐸</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.3c">V_{\text{LLM}}(\mathbf{w})\rightarrow\mathbf{z},\mathbf{z}\in\mathbb{R}^{B%
\times T\times E}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.3d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT ( bold_w ) → bold_z , bold_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_B × italic_T × italic_E end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.5">After processing through the LLM’s Transformer block, the tensor is projected back into the spatial dimensions of the original image. This projection is achieved using a series of up-sampling operations and convolutional layers. The output of this projection stage is a tensor with shape <math alttext="(B,C,H^{\prime},W^{\prime},D^{\prime})" class="ltx_Math" display="inline" id="S3.SS2.p7.1.m1.5"><semantics id="S3.SS2.p7.1.m1.5a"><mrow id="S3.SS2.p7.1.m1.5.5.3" xref="S3.SS2.p7.1.m1.5.5.4.cmml"><mo id="S3.SS2.p7.1.m1.5.5.3.4" stretchy="false" xref="S3.SS2.p7.1.m1.5.5.4.cmml">(</mo><mi id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">B</mi><mo id="S3.SS2.p7.1.m1.5.5.3.5" xref="S3.SS2.p7.1.m1.5.5.4.cmml">,</mo><mi id="S3.SS2.p7.1.m1.2.2" xref="S3.SS2.p7.1.m1.2.2.cmml">C</mi><mo id="S3.SS2.p7.1.m1.5.5.3.6" xref="S3.SS2.p7.1.m1.5.5.4.cmml">,</mo><msup id="S3.SS2.p7.1.m1.3.3.1.1" xref="S3.SS2.p7.1.m1.3.3.1.1.cmml"><mi id="S3.SS2.p7.1.m1.3.3.1.1.2" xref="S3.SS2.p7.1.m1.3.3.1.1.2.cmml">H</mi><mo id="S3.SS2.p7.1.m1.3.3.1.1.3" xref="S3.SS2.p7.1.m1.3.3.1.1.3.cmml">′</mo></msup><mo id="S3.SS2.p7.1.m1.5.5.3.7" xref="S3.SS2.p7.1.m1.5.5.4.cmml">,</mo><msup id="S3.SS2.p7.1.m1.4.4.2.2" xref="S3.SS2.p7.1.m1.4.4.2.2.cmml"><mi id="S3.SS2.p7.1.m1.4.4.2.2.2" xref="S3.SS2.p7.1.m1.4.4.2.2.2.cmml">W</mi><mo id="S3.SS2.p7.1.m1.4.4.2.2.3" xref="S3.SS2.p7.1.m1.4.4.2.2.3.cmml">′</mo></msup><mo id="S3.SS2.p7.1.m1.5.5.3.8" xref="S3.SS2.p7.1.m1.5.5.4.cmml">,</mo><msup id="S3.SS2.p7.1.m1.5.5.3.3" xref="S3.SS2.p7.1.m1.5.5.3.3.cmml"><mi id="S3.SS2.p7.1.m1.5.5.3.3.2" xref="S3.SS2.p7.1.m1.5.5.3.3.2.cmml">D</mi><mo id="S3.SS2.p7.1.m1.5.5.3.3.3" xref="S3.SS2.p7.1.m1.5.5.3.3.3.cmml">′</mo></msup><mo id="S3.SS2.p7.1.m1.5.5.3.9" stretchy="false" xref="S3.SS2.p7.1.m1.5.5.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.5b"><vector id="S3.SS2.p7.1.m1.5.5.4.cmml" xref="S3.SS2.p7.1.m1.5.5.3"><ci id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">𝐵</ci><ci id="S3.SS2.p7.1.m1.2.2.cmml" xref="S3.SS2.p7.1.m1.2.2">𝐶</ci><apply id="S3.SS2.p7.1.m1.3.3.1.1.cmml" xref="S3.SS2.p7.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.3.3.1.1.1.cmml" xref="S3.SS2.p7.1.m1.3.3.1.1">superscript</csymbol><ci id="S3.SS2.p7.1.m1.3.3.1.1.2.cmml" xref="S3.SS2.p7.1.m1.3.3.1.1.2">𝐻</ci><ci id="S3.SS2.p7.1.m1.3.3.1.1.3.cmml" xref="S3.SS2.p7.1.m1.3.3.1.1.3">′</ci></apply><apply id="S3.SS2.p7.1.m1.4.4.2.2.cmml" xref="S3.SS2.p7.1.m1.4.4.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.4.4.2.2.1.cmml" xref="S3.SS2.p7.1.m1.4.4.2.2">superscript</csymbol><ci id="S3.SS2.p7.1.m1.4.4.2.2.2.cmml" xref="S3.SS2.p7.1.m1.4.4.2.2.2">𝑊</ci><ci id="S3.SS2.p7.1.m1.4.4.2.2.3.cmml" xref="S3.SS2.p7.1.m1.4.4.2.2.3">′</ci></apply><apply id="S3.SS2.p7.1.m1.5.5.3.3.cmml" xref="S3.SS2.p7.1.m1.5.5.3.3"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.5.5.3.3.1.cmml" xref="S3.SS2.p7.1.m1.5.5.3.3">superscript</csymbol><ci id="S3.SS2.p7.1.m1.5.5.3.3.2.cmml" xref="S3.SS2.p7.1.m1.5.5.3.3.2">𝐷</ci><ci id="S3.SS2.p7.1.m1.5.5.3.3.3.cmml" xref="S3.SS2.p7.1.m1.5.5.3.3.3">′</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.5c">(B,C,H^{\prime},W^{\prime},D^{\prime})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.1.m1.5d">( italic_B , italic_C , italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_W start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT )</annotation></semantics></math>, where <math alttext="H^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p7.2.m2.1"><semantics id="S3.SS2.p7.2.m2.1a"><msup id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml"><mi id="S3.SS2.p7.2.m2.1.1.2" xref="S3.SS2.p7.2.m2.1.1.2.cmml">H</mi><mo id="S3.SS2.p7.2.m2.1.1.3" xref="S3.SS2.p7.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><apply id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.2.m2.1.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p7.2.m2.1.1.2.cmml" xref="S3.SS2.p7.2.m2.1.1.2">𝐻</ci><ci id="S3.SS2.p7.2.m2.1.1.3.cmml" xref="S3.SS2.p7.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">H^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.2.m2.1d">italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="W^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p7.3.m3.1"><semantics id="S3.SS2.p7.3.m3.1a"><msup id="S3.SS2.p7.3.m3.1.1" xref="S3.SS2.p7.3.m3.1.1.cmml"><mi id="S3.SS2.p7.3.m3.1.1.2" xref="S3.SS2.p7.3.m3.1.1.2.cmml">W</mi><mo id="S3.SS2.p7.3.m3.1.1.3" xref="S3.SS2.p7.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.1b"><apply id="S3.SS2.p7.3.m3.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.1.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p7.3.m3.1.1.2.cmml" xref="S3.SS2.p7.3.m3.1.1.2">𝑊</ci><ci id="S3.SS2.p7.3.m3.1.1.3.cmml" xref="S3.SS2.p7.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.1c">W^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.3.m3.1d">italic_W start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p7.4.m4.1"><semantics id="S3.SS2.p7.4.m4.1a"><msup id="S3.SS2.p7.4.m4.1.1" xref="S3.SS2.p7.4.m4.1.1.cmml"><mi id="S3.SS2.p7.4.m4.1.1.2" xref="S3.SS2.p7.4.m4.1.1.2.cmml">D</mi><mo id="S3.SS2.p7.4.m4.1.1.3" xref="S3.SS2.p7.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.4.m4.1b"><apply id="S3.SS2.p7.4.m4.1.1.cmml" xref="S3.SS2.p7.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.4.m4.1.1.1.cmml" xref="S3.SS2.p7.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p7.4.m4.1.1.2.cmml" xref="S3.SS2.p7.4.m4.1.1.2">𝐷</ci><ci id="S3.SS2.p7.4.m4.1.1.3.cmml" xref="S3.SS2.p7.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.4.m4.1c">D^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.4.m4.1d">italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> denote the height, width, and depth of the segmentation map, respectively. Here, <math alttext="C" class="ltx_Math" display="inline" id="S3.SS2.p7.5.m5.1"><semantics id="S3.SS2.p7.5.m5.1a"><mi id="S3.SS2.p7.5.m5.1.1" xref="S3.SS2.p7.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.5.m5.1b"><ci id="S3.SS2.p7.5.m5.1.1.cmml" xref="S3.SS2.p7.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.5.m5.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.5.m5.1d">italic_C</annotation></semantics></math> represents the number of output channels, corresponding to the number of classes or segmentation labels.</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="V_{D}(\mathbf{z})\rightarrow\mathbf{y},\mathbf{y}\in\mathbb{R}^{B\times C%
\times H^{\prime}\times W^{\prime}\times D^{\prime}}" class="ltx_Math" display="block" id="S3.Ex3.m1.3"><semantics id="S3.Ex3.m1.3a"><mrow id="S3.Ex3.m1.3.3.2" xref="S3.Ex3.m1.3.3.3.cmml"><mrow id="S3.Ex3.m1.2.2.1.1" xref="S3.Ex3.m1.2.2.1.1.cmml"><mrow id="S3.Ex3.m1.2.2.1.1.2" xref="S3.Ex3.m1.2.2.1.1.2.cmml"><msub id="S3.Ex3.m1.2.2.1.1.2.2" xref="S3.Ex3.m1.2.2.1.1.2.2.cmml"><mi id="S3.Ex3.m1.2.2.1.1.2.2.2" xref="S3.Ex3.m1.2.2.1.1.2.2.2.cmml">V</mi><mi id="S3.Ex3.m1.2.2.1.1.2.2.3" xref="S3.Ex3.m1.2.2.1.1.2.2.3.cmml">D</mi></msub><mo id="S3.Ex3.m1.2.2.1.1.2.1" xref="S3.Ex3.m1.2.2.1.1.2.1.cmml">⁢</mo><mrow id="S3.Ex3.m1.2.2.1.1.2.3.2" xref="S3.Ex3.m1.2.2.1.1.2.cmml"><mo id="S3.Ex3.m1.2.2.1.1.2.3.2.1" stretchy="false" xref="S3.Ex3.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml">𝐳</mi><mo id="S3.Ex3.m1.2.2.1.1.2.3.2.2" stretchy="false" xref="S3.Ex3.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.Ex3.m1.2.2.1.1.1" stretchy="false" xref="S3.Ex3.m1.2.2.1.1.1.cmml">→</mo><mi id="S3.Ex3.m1.2.2.1.1.3" xref="S3.Ex3.m1.2.2.1.1.3.cmml">𝐲</mi></mrow><mo id="S3.Ex3.m1.3.3.2.3" xref="S3.Ex3.m1.3.3.3a.cmml">,</mo><mrow id="S3.Ex3.m1.3.3.2.2" xref="S3.Ex3.m1.3.3.2.2.cmml"><mi id="S3.Ex3.m1.3.3.2.2.2" xref="S3.Ex3.m1.3.3.2.2.2.cmml">𝐲</mi><mo id="S3.Ex3.m1.3.3.2.2.1" xref="S3.Ex3.m1.3.3.2.2.1.cmml">∈</mo><msup id="S3.Ex3.m1.3.3.2.2.3" xref="S3.Ex3.m1.3.3.2.2.3.cmml"><mi id="S3.Ex3.m1.3.3.2.2.3.2" xref="S3.Ex3.m1.3.3.2.2.3.2.cmml">ℝ</mi><mrow id="S3.Ex3.m1.3.3.2.2.3.3" xref="S3.Ex3.m1.3.3.2.2.3.3.cmml"><mi id="S3.Ex3.m1.3.3.2.2.3.3.2" xref="S3.Ex3.m1.3.3.2.2.3.3.2.cmml">B</mi><mo id="S3.Ex3.m1.3.3.2.2.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.Ex3.m1.3.3.2.2.3.3.1.cmml">×</mo><mi id="S3.Ex3.m1.3.3.2.2.3.3.3" xref="S3.Ex3.m1.3.3.2.2.3.3.3.cmml">C</mi><mo id="S3.Ex3.m1.3.3.2.2.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.Ex3.m1.3.3.2.2.3.3.1.cmml">×</mo><msup id="S3.Ex3.m1.3.3.2.2.3.3.4" xref="S3.Ex3.m1.3.3.2.2.3.3.4.cmml"><mi id="S3.Ex3.m1.3.3.2.2.3.3.4.2" xref="S3.Ex3.m1.3.3.2.2.3.3.4.2.cmml">H</mi><mo id="S3.Ex3.m1.3.3.2.2.3.3.4.3" xref="S3.Ex3.m1.3.3.2.2.3.3.4.3.cmml">′</mo></msup><mo id="S3.Ex3.m1.3.3.2.2.3.3.1b" lspace="0.222em" rspace="0.222em" xref="S3.Ex3.m1.3.3.2.2.3.3.1.cmml">×</mo><msup id="S3.Ex3.m1.3.3.2.2.3.3.5" xref="S3.Ex3.m1.3.3.2.2.3.3.5.cmml"><mi id="S3.Ex3.m1.3.3.2.2.3.3.5.2" xref="S3.Ex3.m1.3.3.2.2.3.3.5.2.cmml">W</mi><mo id="S3.Ex3.m1.3.3.2.2.3.3.5.3" xref="S3.Ex3.m1.3.3.2.2.3.3.5.3.cmml">′</mo></msup><mo id="S3.Ex3.m1.3.3.2.2.3.3.1c" lspace="0.222em" rspace="0.222em" xref="S3.Ex3.m1.3.3.2.2.3.3.1.cmml">×</mo><msup id="S3.Ex3.m1.3.3.2.2.3.3.6" xref="S3.Ex3.m1.3.3.2.2.3.3.6.cmml"><mi id="S3.Ex3.m1.3.3.2.2.3.3.6.2" xref="S3.Ex3.m1.3.3.2.2.3.3.6.2.cmml">D</mi><mo id="S3.Ex3.m1.3.3.2.2.3.3.6.3" xref="S3.Ex3.m1.3.3.2.2.3.3.6.3.cmml">′</mo></msup></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.3b"><apply id="S3.Ex3.m1.3.3.3.cmml" xref="S3.Ex3.m1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.3.3.3a.cmml" xref="S3.Ex3.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.Ex3.m1.2.2.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1"><ci id="S3.Ex3.m1.2.2.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1">→</ci><apply id="S3.Ex3.m1.2.2.1.1.2.cmml" xref="S3.Ex3.m1.2.2.1.1.2"><times id="S3.Ex3.m1.2.2.1.1.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1"></times><apply id="S3.Ex3.m1.2.2.1.1.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.2.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.2">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.2.2.2">𝑉</ci><ci id="S3.Ex3.m1.2.2.1.1.2.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.2.2.3">𝐷</ci></apply><ci id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.1">𝐳</ci></apply><ci id="S3.Ex3.m1.2.2.1.1.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3">𝐲</ci></apply><apply id="S3.Ex3.m1.3.3.2.2.cmml" xref="S3.Ex3.m1.3.3.2.2"><in id="S3.Ex3.m1.3.3.2.2.1.cmml" xref="S3.Ex3.m1.3.3.2.2.1"></in><ci id="S3.Ex3.m1.3.3.2.2.2.cmml" xref="S3.Ex3.m1.3.3.2.2.2">𝐲</ci><apply id="S3.Ex3.m1.3.3.2.2.3.cmml" xref="S3.Ex3.m1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.3.3.2.2.3.1.cmml" xref="S3.Ex3.m1.3.3.2.2.3">superscript</csymbol><ci id="S3.Ex3.m1.3.3.2.2.3.2.cmml" xref="S3.Ex3.m1.3.3.2.2.3.2">ℝ</ci><apply id="S3.Ex3.m1.3.3.2.2.3.3.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3"><times id="S3.Ex3.m1.3.3.2.2.3.3.1.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.1"></times><ci id="S3.Ex3.m1.3.3.2.2.3.3.2.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.2">𝐵</ci><ci id="S3.Ex3.m1.3.3.2.2.3.3.3.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.3">𝐶</ci><apply id="S3.Ex3.m1.3.3.2.2.3.3.4.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.3.3.2.2.3.3.4.1.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.4">superscript</csymbol><ci id="S3.Ex3.m1.3.3.2.2.3.3.4.2.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.4.2">𝐻</ci><ci id="S3.Ex3.m1.3.3.2.2.3.3.4.3.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.4.3">′</ci></apply><apply id="S3.Ex3.m1.3.3.2.2.3.3.5.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.5"><csymbol cd="ambiguous" id="S3.Ex3.m1.3.3.2.2.3.3.5.1.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.5">superscript</csymbol><ci id="S3.Ex3.m1.3.3.2.2.3.3.5.2.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.5.2">𝑊</ci><ci id="S3.Ex3.m1.3.3.2.2.3.3.5.3.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.5.3">′</ci></apply><apply id="S3.Ex3.m1.3.3.2.2.3.3.6.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.6"><csymbol cd="ambiguous" id="S3.Ex3.m1.3.3.2.2.3.3.6.1.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.6">superscript</csymbol><ci id="S3.Ex3.m1.3.3.2.2.3.3.6.2.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.6.2">𝐷</ci><ci id="S3.Ex3.m1.3.3.2.2.3.3.6.3.cmml" xref="S3.Ex3.m1.3.3.2.2.3.3.6.3">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.3c">V_{D}(\mathbf{z})\rightarrow\mathbf{y},\mathbf{y}\in\mathbb{R}^{B\times C%
\times H^{\prime}\times W^{\prime}\times D^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex3.m1.3d">italic_V start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( bold_z ) → bold_y , bold_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_B × italic_C × italic_H start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT × italic_W start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT × italic_D start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">In summary, the pipeline involves segmenting the image into patches, embedding these patches into a high-dimensional space, passing them through an LLM Transformer block to capture contextual information, and finally reconstructing the tensor to produce the segmentation map. This segmentation map has spatial dimensions that match the original input image but contains predictions across multiple channels.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset and Preprocessing</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To evaluate our results, we used all ten datasets from the Medical Segmentation Decathlon (MSD) challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib3" title="">3</a>]</cite>. The MSD dataset provides a comprehensive benchmark with 10 diverse medical imaging segmentation tasks, each focusing on different anatomical structures and conditions. It includes annotated images of varying detail and complexity, making it ideal for assessing segmentation performance across multiple scenarios. A summary of the tasks is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.T1" title="Table 1 ‣ 4.1 Dataset and Preprocessing ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.1" style="font-size:90%;">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.2.1" style="font-size:90%;">Modality</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.3.1" style="font-size:90%;">Number of Images</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.2.1.1"><span class="ltx_text" id="S4.T1.2.2.1.1.1" style="font-size:90%;">Task01_BrainTumour</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.2"><span class="ltx_text" id="S4.T1.2.2.1.2.1" style="font-size:90%;">MRI</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.2.2.1.3"><span class="ltx_text" id="S4.T1.2.2.1.3.1" style="font-size:90%;">484</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3.2">
<td class="ltx_td ltx_align_left" id="S4.T1.2.3.2.1"><span class="ltx_text" id="S4.T1.2.3.2.1.1" style="font-size:90%;">Task02_Heart</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.2"><span class="ltx_text" id="S4.T1.2.3.2.2.1" style="font-size:90%;">MRI</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.3.2.3"><span class="ltx_text" id="S4.T1.2.3.2.3.1" style="font-size:90%;">20</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4.3">
<td class="ltx_td ltx_align_left" id="S4.T1.2.4.3.1"><span class="ltx_text" id="S4.T1.2.4.3.1.1" style="font-size:90%;">Task03_Liver</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.3.2"><span class="ltx_text" id="S4.T1.2.4.3.2.1" style="font-size:90%;">CT</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.4.3.3"><span class="ltx_text" id="S4.T1.2.4.3.3.1" style="font-size:90%;">100</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5.4">
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.4.1"><span class="ltx_text" id="S4.T1.2.5.4.1.1" style="font-size:90%;">Task04_Hippocampus</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.4.2"><span class="ltx_text" id="S4.T1.2.5.4.2.1" style="font-size:90%;">MRI</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.5.4.3"><span class="ltx_text" id="S4.T1.2.5.4.3.1" style="font-size:90%;">260</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6.5">
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.5.1"><span class="ltx_text" id="S4.T1.2.6.5.1.1" style="font-size:90%;">Task05_Prostate</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.5.2"><span class="ltx_text" id="S4.T1.2.6.5.2.1" style="font-size:90%;">MRI</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.6.5.3"><span class="ltx_text" id="S4.T1.2.6.5.3.1" style="font-size:90%;">32</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7.6">
<td class="ltx_td ltx_align_left" id="S4.T1.2.7.6.1"><span class="ltx_text" id="S4.T1.2.7.6.1.1" style="font-size:90%;">Task06_Lung</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.7.6.2"><span class="ltx_text" id="S4.T1.2.7.6.2.1" style="font-size:90%;">CT</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.7.6.3"><span class="ltx_text" id="S4.T1.2.7.6.3.1" style="font-size:90%;">53</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.8.7">
<td class="ltx_td ltx_align_left" id="S4.T1.2.8.7.1"><span class="ltx_text" id="S4.T1.2.8.7.1.1" style="font-size:90%;">Task07_Pancreas</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.8.7.2"><span class="ltx_text" id="S4.T1.2.8.7.2.1" style="font-size:90%;">CT</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.8.7.3"><span class="ltx_text" id="S4.T1.2.8.7.3.1" style="font-size:90%;">281</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.9.8">
<td class="ltx_td ltx_align_left" id="S4.T1.2.9.8.1"><span class="ltx_text" id="S4.T1.2.9.8.1.1" style="font-size:90%;">Task08_HepaticVessel</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.8.2"><span class="ltx_text" id="S4.T1.2.9.8.2.1" style="font-size:90%;">CT</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.9.8.3"><span class="ltx_text" id="S4.T1.2.9.8.3.1" style="font-size:90%;">303</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.10.9">
<td class="ltx_td ltx_align_left" id="S4.T1.2.10.9.1"><span class="ltx_text" id="S4.T1.2.10.9.1.1" style="font-size:90%;">Task09_Spleen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.9.2"><span class="ltx_text" id="S4.T1.2.10.9.2.1" style="font-size:90%;">CT</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.10.9.3"><span class="ltx_text" id="S4.T1.2.10.9.3.1" style="font-size:90%;">41</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.11.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.2.11.10.1"><span class="ltx_text" id="S4.T1.2.11.10.1.1" style="font-size:90%;">Task10_Colon</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.11.10.2"><span class="ltx_text" id="S4.T1.2.11.10.2.1" style="font-size:90%;">CT</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.2.11.10.3"><span class="ltx_text" id="S4.T1.2.11.10.3.1" style="font-size:90%;">126</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Summary of tasks, modalities, and number of images in the MSD dataset.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">For each dataset, the data was partitioned into training (70%), validation (20%), and test (approximately 10%) subsets. Data augmentation techniques were applied in cases where the number of images was substantially low to enhance model robustness. The model was implemented in Python using the PyTorch framework and trained on an NVIDIA A100 GPU with 40 GB of memory. We trained the model for 100 epochs with an initial learning rate of <math alttext="1\times 10^{-5}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p2.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mn id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.p2.1.m1.1.1.3.3a" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p2.1.m1.1.1.3.3.2" xref="S4.SS1.p2.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></times><cn id="S4.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.2">1</cn><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3">superscript</csymbol><cn id="S4.SS1.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"><minus id="S4.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3"></minus><cn id="S4.SS1.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">1\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">1 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, using the Adam optimizer with a batch size of 4. The cross-entropy loss function was employed to train the model for segmentation performance evaluation.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baselines and comparison metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Distinct from current VLMs, our approach operates independently of pre-trained visual encoders and does not use text-based inputs. Unlike traditional VLM models such as SimVLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib43" title="">43</a>]</cite>, ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib23" title="">23</a>]</cite>, FLAVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib34" title="">34</a>]</cite>, and CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib31" title="">31</a>]</cite>, which rely on pre-trained visual encoders for aligning visual features with language, our method is designed to operate effectively without such dependencies.
This allows our model to be trained from scratch, focusing exclusively on visual representation learning without the need for language-based inputs, making it versatile for a wider range of visual tasks. A key innovation is our Hybrid Attention Mechanism, combining efficient and channel attention for balanced global and local feature learning, alongside a Multi-Scale Fusion Block that refines precision across different scales <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib27" title="">27</a>]</cite>. These enhancements enable our model to capture complex visual information and perform detailed segmentation, distinguishing it from traditional VLMs and broadening its applicability beyond vision-language tasks.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Hence, in this study, we compare the training and segmentation performances of the base ViT against the ViT enhanced with different pre-trained LLM transformer blocks across various datasets. To evaluate segmentation performance, we use the Dice Coefficient <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib9" title="">9</a>]</cite> for overlap accuracy, the 95th percentile of the Hausdorff Distance (HD95) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib19" title="">19</a>]</cite> for boundary precision, and the Jaccard Index <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib20" title="">20</a>]</cite> for similarity measurement. Additionally, we analyze the dice and loss curves to determine how quickly each model stabilizes during training. These metrics collectively provide a comprehensive assessment of the effectiveness and efficiency of the enhanced ViT model.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Quantitative Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">This study evaluates the impact of integrating the pre-trained weights of the Llama 3.1 transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib12" title="">12</a>]</cite> as a frozen layer into the existing ViT architecture for medical image segmentation, resulting in the MedVisionLlama model. We compare MedVisionLlama with the baseline ViT model across key metrics, including Dice score, accuracy, precision, recall, Jaccard Index, and HD95 in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS3.SSS1" title="4.3.1 Comparative Analysis of ViT with MedVisionLlama ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>. We also examine the effects on activation maps and training dynamics to assess improvements in feature representation and model performance in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS3.SSS2" title="4.3.2 Training Performance comparison between ViT and MedVisionLlama ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Comparative Analysis of ViT with MedVisionLlama</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">The first experiment in this study involved comparing the performance of the base ViT to that of the Llama 3.1-enhanced model across different datasets. The aim was to evaluate the effectiveness of adding a frozen Llama transformer block to training for various medical imaging modalities.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:682.2pt;height:253pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S4.T2.2.1"><span class="ltx_text" id="S4.T2.2.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S4.T2.2.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.2.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.1.1">Metric</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.2.1">Task01</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.3.1">Task02</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.4.1">Task03</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.5.1">Task04</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.6.1">Task05</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.7.1">Task06</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.8.1">Task07</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.9.1">Task08</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.10.1">Task09</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S4.T2.2.1.1.1.1.1.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.11.1">Task10</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.1.1">+Llama</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.2.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.3.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.4.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.5.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.6.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.7.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.8"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.8.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.9.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.10"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.10.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.11.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.12"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.12.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.13.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.14"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.14.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.15.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.16"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.16.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.17.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.18"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.18.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2.2.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.19.1">✓</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.20"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.20.1">×</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1.2.2.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.2.21.1">✓</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S4.T2.2.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.1.1">Dice</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.2">0.72</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.3.1">0.84</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.4">0.75</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.5.1">0.78</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.6">0.68</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.7.1">0.72</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.8">0.71</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.9.1">0.72</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.10">0.64</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.11.1">0.77</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.12">0.76</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.13.1">0.82</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.14">0.82</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.15.1">0.89</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.16">0.76</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.17.1">0.78</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.18">0.79</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.3.1.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.19.1">0.81</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.20">0.75</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.3.1.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1.21.1">0.77</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.4.2">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.1.1.4.2.1"></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.2">± 0.14</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.3.1">± 0.11</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.4">± 0.14</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.5.1">± 0.13</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.6">± 0.09</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.7.1">± 0.08</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.8">± 0.12</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.9.1">± 0.15</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.10">± 0.07</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.11.1">± 0.08</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.12">± 0.12</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.13.1">± 0.10</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.14">± 0.11</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.15.1">± 0.11</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.16">± 0.14</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.17.1">± 0.09</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.18">± 0.14</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.4.2.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.19.1">± 0.13</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.4.2.20">± 0.10</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.1.1.1.4.2.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.4.2.21.1">± 0.12</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.5.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.1.1">Accuracy</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.2">0.94</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.3.1">0.95</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.4">0.88</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.5.1">0.93</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.6">0.92</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.7.1">0.94</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.8">0.94</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.9.1">0.95</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.10">0.95</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.11.1">0.98</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.12">0.92</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.13.1">0.95</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.14">0.97</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.15.1">0.99</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.16">0.91</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.17.1">0.93</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.18">0.92</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.5.3.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.19.1">0.97</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.20">0.96</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.5.3.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.5.3.21.1">0.98</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.6.4">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.1.1.6.4.1"></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.2">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.3.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.4">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.5.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.6">± 0.02</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.7.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.8">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.9.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.10">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.11.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.12">± 0.02</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.13.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.14">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.15.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.16">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.17.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.18">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.6.4.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.19.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.6.4.20">± 0.02</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.1.1.1.6.4.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.6.4.21.1">± 0.01</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.7.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.1.1">Precision</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.2">0.70</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.3.1">0.78</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.4">0.68</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.5.1">0.74</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.6">0.72</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.7.1">0.77</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.8">0.68</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.9.1">0.72</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.10">0.66</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.11.1">0.79</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.12">0.73</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.13.1">0.76</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.14">0.69</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.15.1">0.80</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.16">0.69</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.17.1">0.74</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.18">0.65</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.7.5.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.19.1">0.77</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.20">0.63</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.7.5.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.7.5.21.1">0.73</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.8.6">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.1.1.8.6.1"></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.2">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.3.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.4">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.5.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.6">± 0.05</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.7.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.8">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.9.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.10">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.11.1">± 0.03</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.12">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.13.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.14">± 0.05</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.15.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.16">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.17.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.18">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.8.6.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.19.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.8.6.20">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.1.1.1.8.6.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.8.6.21.1">± 0.02</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.9.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.1.1">Recall</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.2">0.90</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.3.1">0.91</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.4">0.86</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.5.1">0.89</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.6">0.88</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.7.1">0.92</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.8">0.85</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.9.1">0.88</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.10">0.92</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.11.1">0.94</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.12">0.87</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.13.1">0.89</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.14">0.94</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.15.1">0.96</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.16">0.89</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.17.1">0.92</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.18">0.90</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.9.7.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.19.1">0.94</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.20">0.94</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.9.7.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.9.7.21.1">0.95</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.10.8">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.1.1.10.8.1"></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.2">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.3.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.4">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.5.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.6">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.7.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.8">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.9.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.10">± 0.05</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.11.1">± 0.03</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.12">± 0.02</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.13.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.14">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.15.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.16">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.17.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.18">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.10.8.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.19.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.10.8.20">± 0.03</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.1.1.1.10.8.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.10.8.21.1">± 0.02</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.11.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.1.1">Jaccard</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.2">0.54</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.3.1">0.62</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.4">0.58</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.5.1">0.64</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.6">0.51</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.7.1">0.59</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.8">0.61</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.9.1">0.65</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.10">0.55</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.11.1">0.62</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.12">0.62</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.13.1">0.65</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.14">0.66</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.15.1">0.72</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.16">0.60</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.17.1">0.71</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.18">0.58</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.11.9.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.19.1">0.68</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.20">0.63</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.11.9.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.11.9.21.1">0.68</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.12.10">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.1.1.1.12.10.1"></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.2">± 0.05</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.3.1">± 0.02</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.4">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.5.1">± 0.03</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.6">± 0.05</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.7.1">± 0.01</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.8">± 0.04</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.9.1">± 0.08</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.10">± 0.06</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.11.1">± 0.09</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.12">± 0.05</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.13.1">± 0.11</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.14">± 0.09</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.15.1">± 0.11</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.16">± 0.13</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.17.1">± 0.10</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.18">± 0.09</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r" id="S4.T2.2.1.1.1.12.10.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.19.1">± 0.13</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center" id="S4.T2.2.1.1.1.12.10.20">± 0.08</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.1.1.1.12.10.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.12.10.21.1">± 0.12</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.13.11">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.1.1">HD95</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.2">14.7</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.3.1">11.2</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.4">15.4</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.5.1">12.1</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.6">16.5</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.7.1">10.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.8">15.9</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.9.1">11.6</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.10">14.8</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.11.1">12.3</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.12">16.2</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.13.1">11.0</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.14">15.7</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.15.1">12.0</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.16">14.9</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.17.1">11.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.18">15.6</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.13.11.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.19.1">11.5</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.20">14.4</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.1.1.1.13.11.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.13.11.21.1">12.2</span></span></span>
<span class="ltx_tr" id="S4.T2.2.1.1.1.14.12">
<span class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.1"></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.2">± 3.5</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.3.1">± 2.3</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.4">± 4.2</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.5.1">± 3.1</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.6">± 5.1</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.7.1">± 2.5</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.8">± 4.3</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.9"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.9.1">± 2.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.10">± 4.6</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.11"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.11.1">± 2.6</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.12">± 5.2</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.13"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.13.1">± 2.4</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.14">± 4.8</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.15"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.15.1">± 2.7</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.16">± 4.4</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.17"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.17.1">± 2.5</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.18">± 5.0</span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.1.1.1.14.12.19"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.19.1">± 2.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.20">± 4.7</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.1.1.1.14.12.21"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.14.12.21.1">± 2.6</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.4.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.5.2" style="font-size:90%;">Comparison of various metric scores for the ViT (<span class="ltx_text" id="S4.T2.5.2.1">×</span>) and MedVisionLlama (✓) models across Tasks 01 to 10. MedVisionLlama generally outperforms the ViT model in terms of Dice, Accuracy, Precision, Recall, Jaccard, and HD95 scores, indicating improved overall performance and accuracy across different tasks.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">From Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.T2" title="Table 2 ‣ 4.3.1 Comparative Analysis of ViT with MedVisionLlama ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, it is evident that integrating the Llama transformer block into the ViT significantly boosts performance across all tasks. The average Dice score improves from 0.74 with ViT to 0.79 with MedVisionLlama, with notable increases of about 0.12 and 0.13 points in Task01 and Task05, respectively. Accuracy rises from 0.93 to 0.96 on average, with Task02 and Task09 showing enhancements of 0.05 points. Precision also improves from 0.68 to 0.76, reflecting better prediction balance, particularly in Task05 and Task09, which see increases of 0.13 and 0.12 points. Recall shows a slight improvement from 0.90 to 0.92 on average, with Task03 and Task09 demonstrating increases of 0.04 points. The Jaccard Index enhances from an average of 0.59 to 0.66, with Task08 and Task09 showing improvements of 0.11 and 0.10 points. Additionally, the Hausdorff Distance at the 95th percentile (HD95) decreases from 15.4 to 11.7, indicating improved boundary alignment, with Task03 and Task06 showing reductions of 5.7 and 5.2 units, respectively. These results collectively highlight the substantial effectiveness of the Llama transformer block in enhancing the ViT’s performance across various medical imaging tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Training Performance comparison between ViT and MedVisionLlama</h4>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="675" id="S4.F3.g1" src="extracted/5901749/Figures/Combined_Metrics.png" width="945"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F3.4.2" style="font-size:90%;">Training dice and loss curves for ViT and MedVisionLlama.<span class="ltx_text ltx_font_medium" id="S4.F3.4.2.1"> MedVisionLlama shows higher Dice scores and lower loss compared to the original ViT during training, indicating better segmentation accuracy and faster convergence.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">We conducted a critical comparison of training performance between the vanilla ViT and MedVisionLlama models, highlighting the importance of both segmentation accuracy and training curves. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.F3" title="Figure 3 ‣ 4.3.2 Training Performance comparison between ViT and MedVisionLlama ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrates that the Llama-enhanced ViT outperforms the original ViT in both Dice coefficient and loss metrics across training and validation datasets. MedVisionLlama consistently shows higher Dice scores and lower loss values, indicating improved segmentation accuracy, stability, and faster convergence. This underscores Llama’s effectiveness in enhancing feature representation and overall model performance.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="383" id="S4.F4.g1" src="extracted/5901749/Figures/Activation.png" width="202"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F4.4.2" style="font-size:90%;">Comparison of feature activation maps between the ViT and MedVisionLlama models.<span class="ltx_text ltx_font_medium" id="S4.F4.4.2.1"> MedVisionLlama shows more concentrated and precise feature activations compared to the ViT model, indicating enhanced focus on relevant features and improved segmentation accuracy.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.3">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.F4" title="Figure 4 ‣ 4.3.2 Training Performance comparison between ViT and MedVisionLlama ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> displays the feature activation maps of the ViT and MedVisionLlama models. The top image represents an input MRI slice used for segmentation. The maps show how different features are activated across the input volume at various levels: <math alttext="V_{E}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.1.m1.1"><semantics id="S4.SS3.SSS2.p2.1.m1.1a"><msub id="S4.SS3.SSS2.p2.1.m1.1.1" xref="S4.SS3.SSS2.p2.1.m1.1.1.cmml"><mi id="S4.SS3.SSS2.p2.1.m1.1.1.2" xref="S4.SS3.SSS2.p2.1.m1.1.1.2.cmml">V</mi><mi id="S4.SS3.SSS2.p2.1.m1.1.1.3" xref="S4.SS3.SSS2.p2.1.m1.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.1.m1.1b"><apply id="S4.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1.2">𝑉</ci><ci id="S4.SS3.SSS2.p2.1.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.1.m1.1c">V_{E}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.1.m1.1d">italic_V start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="V_{LLM}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.2.m2.1"><semantics id="S4.SS3.SSS2.p2.2.m2.1a"><msub id="S4.SS3.SSS2.p2.2.m2.1.1" xref="S4.SS3.SSS2.p2.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p2.2.m2.1.1.2" xref="S4.SS3.SSS2.p2.2.m2.1.1.2.cmml">V</mi><mrow id="S4.SS3.SSS2.p2.2.m2.1.1.3" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.cmml"><mi id="S4.SS3.SSS2.p2.2.m2.1.1.3.2" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.2.cmml">L</mi><mo id="S4.SS3.SSS2.p2.2.m2.1.1.3.1" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS2.p2.2.m2.1.1.3.3" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.3.cmml">L</mi><mo id="S4.SS3.SSS2.p2.2.m2.1.1.3.1a" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.SSS2.p2.2.m2.1.1.3.4" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.4.cmml">M</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.2.m2.1b"><apply id="S4.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1.2">𝑉</ci><apply id="S4.SS3.SSS2.p2.2.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1.3"><times id="S4.SS3.SSS2.p2.2.m2.1.1.3.1.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.1"></times><ci id="S4.SS3.SSS2.p2.2.m2.1.1.3.2.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.2">𝐿</ci><ci id="S4.SS3.SSS2.p2.2.m2.1.1.3.3.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.3">𝐿</ci><ci id="S4.SS3.SSS2.p2.2.m2.1.1.3.4.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1.3.4">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.2.m2.1c">V_{LLM}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.2.m2.1d">italic_V start_POSTSUBSCRIPT italic_L italic_L italic_M end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="V_{D}" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p2.3.m3.1"><semantics id="S4.SS3.SSS2.p2.3.m3.1a"><msub id="S4.SS3.SSS2.p2.3.m3.1.1" xref="S4.SS3.SSS2.p2.3.m3.1.1.cmml"><mi id="S4.SS3.SSS2.p2.3.m3.1.1.2" xref="S4.SS3.SSS2.p2.3.m3.1.1.2.cmml">V</mi><mi id="S4.SS3.SSS2.p2.3.m3.1.1.3" xref="S4.SS3.SSS2.p2.3.m3.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.3.m3.1b"><apply id="S4.SS3.SSS2.p2.3.m3.1.1.cmml" xref="S4.SS3.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p2.3.m3.1.1.1.cmml" xref="S4.SS3.SSS2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p2.3.m3.1.1.2.cmml" xref="S4.SS3.SSS2.p2.3.m3.1.1.2">𝑉</ci><ci id="S4.SS3.SSS2.p2.3.m3.1.1.3.cmml" xref="S4.SS3.SSS2.p2.3.m3.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.3.m3.1c">V_{D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p2.3.m3.1d">italic_V start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT</annotation></semantics></math>. The ViT model’s activation maps (left column) are more diffuse, indicating broader and less specific feature activations. In contrast, the MedVisionLlama model’s activation maps (right column) exhibit more concentrated and precise activations, particularly around the region of interest. This suggests that incorporating the Llama transformer module enhances the model’s ability to focus on relevant features, thereby improving the accuracy of the segmentation. The improved localization seen in the MedVisionLlama model underscores the effectiveness of LLM-based feature representations in refining segmentation performance.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation studies</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">We conducted an ablation study to assess the impact of integrating frozen LLM transformer blocks (<math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><msub id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">V</mi><mtext id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">𝑉</ci><ci id="S4.SS4.p1.1.m1.1.1.3a.cmml" xref="S4.SS4.p1.1.m1.1.1.3"><mtext id="S4.SS4.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS4.p1.1.m1.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math>), like Llama, into ViT-based segmentation models. In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS4.SSS1" title="4.4.1 Model capacity and Fine-tuning ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.4.1</span></a>, we compare Hybrid Attention (HA) with Spatial Attention (SA) and evaluate Llama against an MLP alternative. In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.SS3.SSS2" title="4.3.2 Training Performance comparison between ViT and MedVisionLlama ‣ 4.3 Quantitative Results ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>, we extend the analysis to include LLMs like Gemma <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib35" title="">35</a>]</cite>, Mistral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib21" title="">21</a>]</cite>, Qwen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib46" title="">46</a>]</cite>, and Yi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#bib.bib47" title="">47</a>]</cite>, examining their effects on segmentation accuracy, stability, and the trade-offs between efficiency and complexity.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Model capacity and Fine-tuning</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.3">In our initial study, we compare the performance of the ViT enhanced with the Llama model under varying capacities and fine-tuning conditions. Specifically, we evaluate the hybrid attention network against the spatial attention network. Additionally, we introduce a variant model, ViT + MLP, which replaces the Llama layer in the MedVisionLlama model. In the ViT + MLP model, the <math alttext="V_{\text{LLM}}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.1.m1.1"><semantics id="S4.SS4.SSS1.p1.1.m1.1a"><msub id="S4.SS4.SSS1.p1.1.m1.1.1" xref="S4.SS4.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS1.p1.1.m1.1.1.2" xref="S4.SS4.SSS1.p1.1.m1.1.1.2.cmml">V</mi><mtext id="S4.SS4.SSS1.p1.1.m1.1.1.3" xref="S4.SS4.SSS1.p1.1.m1.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.1.m1.1b"><apply id="S4.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1.2">𝑉</ci><ci id="S4.SS4.SSS1.p1.1.m1.1.1.3a.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1.3"><mtext id="S4.SS4.SSS1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS4.SSS1.p1.1.m1.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.1.m1.1c">V_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.1.m1.1d">italic_V start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math> layer between <math alttext="V_{L_{1}}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.2.m2.1"><semantics id="S4.SS4.SSS1.p1.2.m2.1a"><msub id="S4.SS4.SSS1.p1.2.m2.1.1" xref="S4.SS4.SSS1.p1.2.m2.1.1.cmml"><mi id="S4.SS4.SSS1.p1.2.m2.1.1.2" xref="S4.SS4.SSS1.p1.2.m2.1.1.2.cmml">V</mi><msub id="S4.SS4.SSS1.p1.2.m2.1.1.3" xref="S4.SS4.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S4.SS4.SSS1.p1.2.m2.1.1.3.2" xref="S4.SS4.SSS1.p1.2.m2.1.1.3.2.cmml">L</mi><mn id="S4.SS4.SSS1.p1.2.m2.1.1.3.3" xref="S4.SS4.SSS1.p1.2.m2.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.2.m2.1b"><apply id="S4.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.2">𝑉</ci><apply id="S4.SS4.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.3.2">𝐿</ci><cn id="S4.SS4.SSS1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S4.SS4.SSS1.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.2.m2.1c">V_{L_{1}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.2.m2.1d">italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="V_{L_{2}}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.3.m3.1"><semantics id="S4.SS4.SSS1.p1.3.m3.1a"><msub id="S4.SS4.SSS1.p1.3.m3.1.1" xref="S4.SS4.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS4.SSS1.p1.3.m3.1.1.2" xref="S4.SS4.SSS1.p1.3.m3.1.1.2.cmml">V</mi><msub id="S4.SS4.SSS1.p1.3.m3.1.1.3" xref="S4.SS4.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S4.SS4.SSS1.p1.3.m3.1.1.3.2" xref="S4.SS4.SSS1.p1.3.m3.1.1.3.2.cmml">L</mi><mn id="S4.SS4.SSS1.p1.3.m3.1.1.3.3" xref="S4.SS4.SSS1.p1.3.m3.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.3.m3.1b"><apply id="S4.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.2">𝑉</ci><apply id="S4.SS4.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.3.2">𝐿</ci><cn id="S4.SS4.SSS1.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S4.SS4.SSS1.p1.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.3.m3.1c">V_{L_{2}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.3.m3.1d">italic_V start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> is removed and substituted with a GeLU activation function followed by a layer normalization. We performed this study for two datasets, Task01 and Task03.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.2.1">Params (M)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.3.1">HA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.4.1">SA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.5.1">Task01</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.2.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.6.1">Task03</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.2.2.1">MedVisionLlama</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.2.2">204.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.3.1">×</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.2.5">0.79</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.2.2.2.6">0.71</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3.3">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.2.3.3.1"></th>
<td class="ltx_td" id="S4.T3.2.3.3.2"></td>
<td class="ltx_td" id="S4.T3.2.3.3.3"></td>
<td class="ltx_td" id="S4.T3.2.3.3.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.3.3.5">± 0.04</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.3.3.6">± 0.07</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.4.4.1">MedVisionLlama</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.4.2">218.12</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.4.4.3.1">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.4.4.4.1">×</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.4.4.5.1">0.84</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.4.4.6.1">0.72</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5.5">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.2.5.5.1"></th>
<td class="ltx_td" id="S4.T3.2.5.5.2"></td>
<td class="ltx_td" id="S4.T3.2.5.5.3"></td>
<td class="ltx_td" id="S4.T3.2.5.5.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.5.5.5.1">± 0.05</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.5.5.6.1">± 0.07</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.6.6.1">ViT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.6.2">0.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.6.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.6.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.6.4.1">×</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.6.6.5">0.72</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.2.6.6.6">0.68</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7.7">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.2.7.7.1"></th>
<td class="ltx_td" id="S4.T3.2.7.7.2"></td>
<td class="ltx_td" id="S4.T3.2.7.7.3"></td>
<td class="ltx_td" id="S4.T3.2.7.7.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.7.7.5">± 0.08</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.7.7.6">± 0.05</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.8.8.1">ViT + MLP</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.8.2">0.96</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.8.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.8.8.3.1">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.8.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.8.8.4.1">×</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.8.8.5">0.74</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.8.8.6">0.65</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9.9">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.2.9.9.1"></th>
<td class="ltx_td" id="S4.T3.2.9.9.2"></td>
<td class="ltx_td" id="S4.T3.2.9.9.3"></td>
<td class="ltx_td" id="S4.T3.2.9.9.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.9.9.5">± 0.06</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.9.9.6">± 0.11</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.10.10.1">MedVisionLlama</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.10.2">218.12</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.10.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.10.10.3.1">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.10.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.10.10.4.1">×</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.10.10.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.10.10.5.1">0.84</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.2.10.10.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.10.10.6.1">0.72</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.11.11">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="S4.T3.2.11.11.1"></th>
<td class="ltx_td ltx_border_bb" id="S4.T3.2.11.11.2"></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.2.11.11.3"></td>
<td class="ltx_td ltx_border_bb" id="S4.T3.2.11.11.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.11.11.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.11.5.1">± 0.05</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.2.11.11.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.11.11.6.1">± 0.07</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">Comparison of different models across various metrics. HA: Hybrid Attention, SA: Spatial Attention. The MedVisionLlama model with HA and SA shows superior performance in Task01 and Task03 Dice scores compared to other configurations, emphasizing its effectiveness in enhancing segmentation accuracy.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">Evaluating models with HA versus SA reveals that HA consistently provides superior performance and stability. For example, in Task01, MedVisionLlama with HA shows a notable improvement in Dice score compared to SA, and a similar trend is observed in Task03. This suggests that HA offers higher accuracy and reduced variability in performance. Additionally, the Llama block outperforms both the MLP and Vanilla ViT models, achieving the highest Dice scores across tasks. Specifically, MedVisionLlama shows superior performance, surpassing both the ViT and ViT + MLP models. This indicates that the Llama block enhances model performance and consistency compared to the other approaches.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Comparison with several pre-trained LLMs</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">In the second study, we assessed the impact of integrating several publicly available pre-trained frozen LLM transformer blocks into the ViT model. This evaluation was conducted by comparing these enhanced models against a baseline vanilla ViT. For this analysis, the Dice score was used as the sole metric to assess segmentation performance. We examined the impact of five different LLMs across multiple datasets. The outcomes of this comparison are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.T4" title="Table 4 ‣ 4.4.2 Comparison with several pre-trained LLMs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>, highlighting the relative performance improvements brought about by each LLM when integrated into the ViT framework.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1">From Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.T4" title="Table 4 ‣ 4.4.2 Comparison with several pre-trained LLMs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>, it can be seen that the models Qwen and Yi consistently deliver strong performance across all 10 tasks, often achieving some of the highest Dice scores. For instance, in Task01, Yi outperforms with a Dice score of 0.86, while Qwen follows closely with 0.82. Similarly, in Task07, Qwen achieves a score of 0.76, surpassing Yi’s 0.73. These results demonstrate the superior performance of Qwen and Yi across various medical imaging tasks. Conversely, the base ViT model shows the lowest performance across most tasks, with significantly lower scores compared to the other models. For example, in Task06, the ViT model only achieves a score of 0.76, while Qwen and Yi reach higher scores of 0.83 and 0.86, respectively. Gemma, Llama, and Mistral exhibit moderate performance, generally outperforming the ViT model but not quite reaching the levels of Qwen and Yi. For instance, in Task09, Gemma scores 0.77, Llama scores 0.81, and Mistral scores 0.85, which are higher than ViT’s 0.79 but slightly lower than Yi’s 0.86.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="138" id="S4.F5.g1" src="extracted/5901749/Figures/Task01_03.png" width="486"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F5.4.2" style="font-size:90%;">Segmentation results of different pre-trained LLM transformer blocks on Task01 and Task03.<span class="ltx_text ltx_font_medium" id="S4.F5.4.2.1"> The Dice scores displayed above the segmentation results highlight that lighter LLMs like Yi and Qwen achieve higher scores compared to other heavier models.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T4.2.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.3.1">ViT +</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.4.1">ViT +</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.5.1">ViT +</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.6.1">ViT +</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.7.1">ViT +</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T4.2.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1.1">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.2.1">ViT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.2.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.3.1">Gemma</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.4.1">Llama</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.5.1">Mistral</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.6.1">Qwen</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S4.T4.2.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.7.1">Yi</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T4.2.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.3.1.1">Params (M)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.3.3.2">0.84</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.3.3.3">264.25</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.3.3.4">218.12</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.3.3.5">218.12</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.3.3.6">233.06</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.3.3.7">202.39</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.2.4.1.1">Task01</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.1.2">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.1.3">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.1.4">0.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.1.5">0.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.4.1.6">0.82</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.2.4.1.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.4.1.7.1">0.86</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.5.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.5.2.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.2.2">± 0.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.2.3">± 0.12</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.2.4">± 0.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.2.5">± 0.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.2.6">± 0.13</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.5.2.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.5.2.7.1">± 0.12</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.6.3.1">Task02</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.3.2">0.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.3.3">0.73</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.3.4">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.6.3.5.1">0.79</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.3.6">± 0.76</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.6.3.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.6.3.7.1">0.79</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.7.4">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.7.4.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.4.2">± 0.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.4.3">± 0.08</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.4.4">± 0.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.7.4.5.1">± 0.14</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.7.4.6">± 0.13</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.7.4.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.7.4.7.1">± 0.12</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.8.5.1">Task03</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.5.2">0.68</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.5.3">0.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.5.4">0.72</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.5.5">0.65</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.5.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.8.5.6.1">0.76</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.8.5.7">0.73</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.9.6">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.9.6.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.6.2">± 0.09</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.6.3">± 0.08</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.6.4">± 0.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.6.5">± 0.12</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.6.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.9.6.6.1">± 0.13</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.9.6.7">± 0.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.10.7.1">Task04</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.10.7.2">0.71</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.10.7.3">0.72</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.10.7.4">0.72</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.10.7.5">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.10.7.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.10.7.6.1">0.78</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.10.7.7">0.77</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.11.8">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.11.8.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.11.8.2">± 0.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.11.8.3">± 0.12</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.11.8.4">± 0.10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.11.8.5">± 0.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.11.8.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.11.8.6.1">± 0.13</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.11.8.7">± 0.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.12.9.1">Task05</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.12.9.2">0.64</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.12.9.3">0.73</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.12.9.4">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.12.9.5">0.81</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.12.9.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.12.9.6.1">0.83</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.12.9.7">0.79</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.13.10">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.13.10.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.13.10.2">± 0.07</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.13.10.3">± 0.08</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.13.10.4">± 0.08</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.13.10.5">± 0.09</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.13.10.6"><span class="ltx_text ltx_font_bold" id="S4.T4.2.13.10.6.1">± 0.07</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.13.10.7">± 0.09</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.14.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.14.11.1">Task06</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.14.11.2">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.14.11.3">0.79</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.14.11.4">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.14.11.5">0.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.14.11.6">0.83</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.14.11.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.14.11.7.1">0.86</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.15.12">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.15.12.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.15.12.2">± 0.12</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.15.12.3">± 0.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.15.12.4">± 0.10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.15.12.5">± 0.10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.15.12.6">± 0.13</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.15.12.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.15.12.7.1">± 0.12</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.16.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.16.13.1">Task07</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.16.13.2">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.16.13.3">0.83</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.16.13.4"><span class="ltx_text ltx_font_bold" id="S4.T4.2.16.13.4.1">0.89</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.16.13.5">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.16.13.6">0.87</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.16.13.7">0.88</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.17.14">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.17.14.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.17.14.2">± 0.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.17.14.3">± 0.08</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.17.14.4"><span class="ltx_text ltx_font_bold" id="S4.T4.2.17.14.4.1">± 0.11</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.17.14.5">± 0.05</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.17.14.6">± 0.07</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.17.14.7">± 0.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.18.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.18.15.1">Task08</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.18.15.2">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.18.15.3">0.80</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.18.15.4">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.18.15.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.18.15.5.1">0.81</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.18.15.6">0.73</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.18.15.7">0.77</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.19.16">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.19.16.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.19.16.2">± 0.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.19.16.3">± 0.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.19.16.4">± 0.09</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.19.16.5"><span class="ltx_text ltx_font_bold" id="S4.T4.2.19.16.5.1">± 0.08</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.19.16.6">± 0.10</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.19.16.7">± 0.12</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.20.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.20.17.1">Task09</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.20.17.2">0.79</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.20.17.3">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.20.17.4">0.81</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.20.17.5">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.20.17.6">0.83</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.20.17.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.20.17.7.1">0.86</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.21.18">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.2.21.18.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.21.18.2">± 0.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.21.18.3">± 0.12</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.21.18.4">± 0.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.21.18.5">± 0.10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.21.18.6">± 0.13</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.21.18.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.21.18.7.1">± 0.11</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.22.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.22.19.1">Task10</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.22.19.2">0.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.22.19.3">0.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.22.19.4">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.22.19.5">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.22.19.6">0.80</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.2.22.19.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.22.19.7.1">0.83</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.23.20">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="S4.T4.2.23.20.1"></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.23.20.2">± 0.12</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.23.20.3">± 0.10</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.23.20.4">± 0.12</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.23.20.5">± 0.13</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.23.20.6">± 0.13</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.2.23.20.7"><span class="ltx_text ltx_font_bold" id="S4.T4.2.23.20.7.1">± 0.09</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">Comparison of segmentation performance across different LLM models for each task based on Dice score. The ViT + Yi model consistently achieves high Dice scores across various tasks compared to other models, demonstrating its superior effectiveness in improving segmentation accuracy.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.SSS2.p3">
<p class="ltx_p" id="S4.SS4.SSS2.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.02458v2#S4.F5" title="Figure 5 ‣ 4.4.2 Comparison with several pre-trained LLMs ‣ 4.4 Ablation studies ‣ 4 Experiments ‣ MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates the segmentation performance of different pre-trained LLM transformer blocks, specifically for Task01 and Task03. Each model’s Dice coefficient, a metric that quantifies the similarity between predicted and ground truth masks, is displayed. From the results, it is evident that ViT + YI and ViT + Qwen configurations consistently achieve high Dice scores across both tasks, indicating superior segmentation accuracy. On the other hand, the vanilla ViT model without additional LLM enhancements shows the lowest performance, emphasizing the importance of integrating these advanced LLM-based blocks for improved segmentation.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p4">
<p class="ltx_p" id="S4.SS4.SSS2.p4.1">This comparison underscores the advantage of incorporating advanced LLMs which significantly enhance segmentation performance in various medical imaging tasks. Lighter models such as Qwen and Yi often benefit from faster training times and lower inference costs, making them ideal for tasks that do not require the extensive representational capacity of larger models. In contrast, while heavier models like Gemma, Llama, and Mistral offer greater capacity and may perform exceptionally well in more complex scenarios, they may be excessive for simpler segmentation tasks. This suggests that for specific medical imaging applications, the efficiency and speed of lighter models can be more advantageous than the increased complexity of larger models, highlighting the importance of choosing the right model size based on the task’s demands.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we explored the novel integration of a frozen transformer block from pre-trained LLMs into the encoder of a ViT-based medical image segmentation model. This approach led to significant enhancements in both performance and accuracy across various medical imaging modalities. Additionally, we proposed an innovative Hybrid Attention Mechanism that effectively balances global and local feature learning by combining efficient and channel attention, alongside a Multi-Scale Fusion Block designed to aggregate features across multiple scales, further refining segmentation precision. Our extensive evaluation, including ablation studies, demonstrated the efficacy of incorporating LLM transformer blocks, highlighting their potential to substantially elevate the accuracy and robustness of medical image segmentation models.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Integrating the pre-trained weights of the Llama 3.1 transformer block into the ViT model significantly enhances segmentation accuracy and stability across medical imaging tasks. This integration leads to improved key metrics like Dice score, precision, and Jaccard Index, with more precise activations and better feature representation. Ablation studies confirm that frozen LLM transformer blocks, such as Llama, combined with the hybrid attention block, consistently outperform alternative configurations, offering superior segmentation performance.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Moreover, advanced LLMs like Qwen and Yi demonstrate substantial performance gains, often surpassing heavier models like Llama and Mistral, emphasizing the importance of balancing model efficiency with task complexity. A possible hypothesis for this trend is that Qwen and Yi incorporate unique architectural enhancements and optimizations that effectively strike a balance between computational efficiency and representation power. Unlike the heavier models, which offer high feature representation capabilities at the cost of increased computational load and potential overfitting, Qwen and Yi likely utilize advanced mechanisms that are better aligned with the specific needs of medical imaging tasks. Through this study, we demonstrate that although LLMs are primarily trained on textual data, they can act as powerful enhancers for image segmentation tasks, providing significant boosts to performance and enabling more accurate and stable outcomes in medical imaging applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Abdelrahman Abdelhamed, Mahmoud Afifi, and Alec Go.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">What do you see? enhancing zero-shot image classification with multimodal large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">arXiv preprint arXiv:2405.15668</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Flamingo: a visual language model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib2.4.2" style="font-size:90%;">, 35:23716–23736, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">The medical segmentation decathlon.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">Nature communications</span><span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">, 13(1):4128, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Reza Azad, Amirhossein Kazerouni, Moein Heidari, Ehsan Khodapanah Aghdam, Amirali Molaei, Yiwei Jia, Abin Jose, Rijo Roy, and Dorit Merhof.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Advances in medical image analysis with vision transformers: a comprehensive review.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">Medical Image Analysis</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, page 103000, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">, 33:1877–1901, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Swin-unet: Unet-like pure transformer for medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">European conference on computer vision</span><span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">, pages 205–218. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Transunet: Transformers make strong encoders for medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">arXiv preprint arXiv:2102.04306</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Palm: Scaling language modeling with pathways.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">Journal of Machine Learning Research</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, 24(240):1–113, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
L. R. Dice.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Measures of the amount of ecologic association between species.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">Ecology</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 26(3):297–302, 1945.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Chris Donahue, Mina Lee, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Enabling language models to fill in the blanks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.3.1" style="font-size:90%;">arXiv preprint arXiv:2005.05339</span><span class="ltx_text" id="bib.bib10.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">The llama 3 herd of models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">arXiv preprint arXiv:2407.21783</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Sina Ghorbani Kolahi, Seyed Kamal Chaharsooghi, Toktam Khatibi, Afshin Bozorgpour, Reza Azad, Moein Heidari, Ilker Hacihaliloglu, and Dorit Merhof.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Msa2net: Multi-scale adaptive attention-guided network for medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">arXiv e-prints</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, pages arXiv–2407, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Unetr: Transformers for 3d medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF winter conference on applications of computer vision</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, pages 574–584, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Kelei He, Chen Gan, Zhuoyuan Li, Islem Rekik, Zihao Yin, Wen Ji, Yang Gao, Qian Wang, Junfeng Zhang, and Dinggang Shen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Transformers in medical image analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.3.1" style="font-size:90%;">Intelligent Medicine</span><span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">, 3(1):59–78, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Moein Heidari, Amirhossein Kazerouni, Milad Soltany, Reza Azad, Ehsan Khodapanah Aghdam, Julien Cohen-Adad, and Dorit Merhof.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Hiformer: Hierarchical multi-scale representations using transformers for medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF winter conference on applications of computer vision</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 6202–6212, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Xiaohong Huang, Zhifang Deng, Dandan Li, and Xueguang Yuan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Missformer: An effective medical image segmentation transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">arXiv preprint arXiv:2109.07162</span><span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Daniel P. Huttenlocher, Gary A. Klanderman, and W. J. Rucklidge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Comparing images using the hausdorff distance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">, 15(9):850–863, 1993.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Paul Jaccard.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Étude comparative de la distribution florale dans une portion des alpes et des pyrénées.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1" style="font-size:90%;">Bulletin de la Société Vaudoise des Sciences Naturelles</span><span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">, 37:547–579, 1901.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Mistral 7b.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">arXiv preprint arXiv:2310.06825</span><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Pravneet Kaur, Gautam Siddharth Kashyap, Ankit Kumar, Md Tabrez Nafis, Sandeep Kumar, and Vikrant Shokeen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">From text to transformation: A comprehensive review of large language models’ versatility.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.3.1" style="font-size:90%;">arXiv preprint arXiv:2402.16142</span><span class="ltx_text" id="bib.bib22.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Wonjae Kim, Bokyung Son, and Ildoo Kim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Vilt: Vision-and-language transformer without convolution or region supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, pages 5583–5594. PMLR, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Suruchi Kumari, Aryan Das, Swalpa Kumar Roy, Indu Joshi, and Pravendra Singh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Leveraging task-specific knowledge from llm for semi-supervised 3d medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">arXiv preprint arXiv:2407.05088</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, and Naira Hovakimyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Residual-based language models are free boosters for biomedical imaging tasks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib25.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib25.5.3" style="font-size:90%;">, pages 5086–5096, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Zihan Li, Yunxiang Li, Qingde Li, Puyang Wang, Dazhou Guo, Le Lu, Dakai Jin, You Zhang, and Qingqi Hong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Lvit: language meets vision transformer in medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib27.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib27.5.3" style="font-size:90%;">, pages 2117–2125, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted windows.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib28.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib28.5.3" style="font-size:90%;">, pages 10012–10022, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Attention u-net: Learning where to look for the pancreas.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">arXiv preprint arXiv:1804.03999</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Ziqi Pang, Ziyang Xie, Yunze Man, and Yu-Xiong Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Frozen transformers in language models are effective visual encoder layers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">arXiv preprint arXiv:2310.12973</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib31.4.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib31.5.3" style="font-size:90%;">, pages 8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib32.4.2" style="font-size:90%;">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</span><span class="ltx_text" id="bib.bib32.5.3" style="font-size:90%;">, pages 234–241. Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Gordon D Schiff, Seijeoung Kim, Richard Abrams, Karen Cosby, Bruce Lambert, Arthur S Elstein, Scott Hasler, Nela Krosnjar, Richard Odwazny, Mary F Wisniewski, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Diagnosing diagnosis errors: lessons from a multi-institutional collaborative project.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.3.1" style="font-size:90%;">Advances in patient safety: from research to implementation (volume 2: concepts and methodology)</span><span class="ltx_text" id="bib.bib33.4.2" style="font-size:90%;">, 2005.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Flava: A foundational language and vision alignment model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, pages 15638–15650, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Gemma 2: Improving open language models at a practical size.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">arXiv preprint arXiv:2408.00118</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Llama: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.3.1" style="font-size:90%;">arXiv preprint arXiv:2302.13971</span><span class="ltx_text" id="bib.bib36.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Medical transformer: Gated axial-attention for medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib37.4.2" style="font-size:90%;">Medical image computing and computer assisted intervention–MICCAI 2021: 24th international conference, Strasbourg, France, September 27–October 1, 2021, proceedings, part I 24</span><span class="ltx_text" id="bib.bib37.5.3" style="font-size:90%;">, pages 36–46. Springer, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib38.4.2" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Jiahao Wang, Wenqi Shao, Mengzhao Chen, Chengyue Wu, Yong Liu, Kaipeng Zhang, Songyang Zhang, Kai Chen, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Adapting llama decoder to vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.3.1" style="font-size:90%;">arXiv preprint arXiv:2404.06773</span><span class="ltx_text" id="bib.bib39.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib40.4.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib41.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib41.5.3" style="font-size:90%;">, pages 568–578, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Zhiqiang Wang, Yiran Pang, and Yanbin Lin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Large language models are zero-shot text classifiers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.3.1" style="font-size:90%;">arXiv preprint arXiv:2312.01044</span><span class="ltx_text" id="bib.bib42.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Simvlm: Simple visual language model pretraining with weak supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.3.1" style="font-size:90%;">arXiv preprint arXiv:2108.10904</span><span class="ltx_text" id="bib.bib43.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">Cbam: Convolutional block attention module.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib44.4.2" style="font-size:90%;">Proceedings of the European conference on computer vision (ECCV)</span><span class="ltx_text" id="bib.bib44.5.3" style="font-size:90%;">, pages 3–19, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">Cvt: Introducing convolutions to vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib45.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib45.5.3" style="font-size:90%;">, pages 22–31, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">Qwen2 technical report.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.3.1" style="font-size:90%;">arXiv preprint arXiv:2407.10671</span><span class="ltx_text" id="bib.bib46.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">Yi: Open foundation models by 01. ai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.3.1" style="font-size:90%;">arXiv preprint arXiv:2403.04652</span><span class="ltx_text" id="bib.bib47.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
Fisher Yu and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">Multi-scale context aggregation by dilated convolutions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.3.1" style="font-size:90%;">arXiv preprint arXiv:1511.07122</span><span class="ltx_text" id="bib.bib48.4.2" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
Sukmin Yun, Seong Hyeon Park, Paul Hongsuck Seo, and Jinwoo Shin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">Ifseg: Image-free semantic segmentation via vision-language model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib49.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib49.5.3" style="font-size:90%;">, pages 2967–2977, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">Unet++: A nested u-net architecture for medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib50.4.2" style="font-size:90%;">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4</span><span class="ltx_text" id="bib.bib50.5.3" style="font-size:90%;">, pages 3–11. Springer, 2018.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 14:19:53 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
