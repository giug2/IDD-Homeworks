<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.03537] Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation</title><meta property="og:description" content="Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics. Traditional approaches of accumulating data and periodically retraining models are outpaced. We propose a novel p…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.03537">

<!--Generated on Wed Jun  5 15:33:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
web phishing,  continual learning,  federated learning, 
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jesher Joshua M
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">School of Computer Science and Engineering</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Vellore Institute of Technology
<br class="ltx_break"></span>Chennai, India 
<br class="ltx_break">jesherjoshua.m2021@vitstudent.ac.in
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adhithya R
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">School of Computer Science and Engineering</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">Vellore Institute of Technology
<br class="ltx_break"></span>Chennai, India 
<br class="ltx_break">adhithya.r2021@vitstudent.ac.in
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sree Dananjay S
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_font_italic">School of Computer Science and Engineering</span>
<br class="ltx_break"><span id="id6.2.id2" class="ltx_text ltx_font_italic">Vellore Institute of Technology
<br class="ltx_break"></span>Chennai, India 
<br class="ltx_break">sreedananjay.s2021@vitstudent.ac.in
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">M Revathi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_font_italic">School of Computer Science and Engineering</span>
<br class="ltx_break"><span id="id8.2.id2" class="ltx_text ltx_font_italic">Vellore Institute of Technology
<br class="ltx_break"></span>Chennai, India 
<br class="ltx_break">m.revathi@vit.ac.in
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Web phishing poses a dynamic threat, requiring detection systems to quickly adapt to the latest tactics. Traditional approaches of accumulating data and periodically retraining models are outpaced. We propose a novel paradigm combining federated learning and continual learning, enabling distributed nodes to continually update models on streams of new phishing data, without accumulating data. These locally adapted models are then aggregated at a central server via federated learning. To enhance detection, we introduce a custom attention-based classifier model with residual connections, tailored for web phishing, leveraging attention mechanisms to capture intricate phishing patterns. We evaluate our hybrid learning paradigm across continual learning strategies (cumulative, replay, MIR, LwF) and model architectures through an empirical investigation. Our main contributions are: (1) a new hybrid federated-continual learning paradigm for robust web phishing detection, and (2) a novel attention + residual connections based model explicitly designed for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93 f1-score with the LwF strategy, outperforming traditional approaches in detecting emerging phishing threats while retaining past knowledge.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
web phishing, continual learning, federated learning,

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Phishing attacks continue to pose a significant threat to online security, exploiting human vulnerabilities through deceptive tactics to gain unauthorized access to sensitive information or systems. These attacks often involve the creation of fraudulent websites that mimic legitimate ones, luring unsuspecting users into divulging personal data or credentials. As phishing techniques become increasingly sophisticated, the need for robust and adaptable detection mechanisms becomes paramount.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Traditional approaches to phishing website detection have relied on static blacklists or heuristic-based methods, which often lag behind the rapidly evolving nature of these attacks. Machine learning (ML) techniques have emerged as a promising solution, offering the capability to learn and identify complex patterns indicative of phishing websites. However, most existing ML-based approaches suffer from several limitations, such as the inability to adapt to new phishing strategies, reliance on outdated datasets, and performance degradation over time.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address these challenges, we propose a novel hybrid learning paradigm that combines the strengths of federated learning and continual learning for robust web phishing detection. Federated learning enables distributed learning nodes to collaboratively train a shared model without centralizing data, preserving privacy and data sovereignty. Continual learning, on the other hand, allows these nodes to continually adapt their models to the most recent phishing data streams, ensuring timely detection of emerging threats.
Our approach leverages the power of attention mechanisms to develop a tailored attention-based classifier model explicitly designed for web phishing detection. By capturing intricate patterns and contextual cues indicative of phishing websites, this model enhances the accuracy and robustness of the detection process. Furthermore, we incorporate adaptive feature selection mechanisms to identify the most relevant features dynamically, further improving the model’s performance and interpretability.
The integration of federated and continual learning paradigms addresses the dynamic nature of phishing attacks, enabling distributed nodes to continually update their models based on the latest phishing data streams. This approach eliminates the need for data accumulation and periodic retraining, ensuring that the detection system remains up-to-date and effective against evolving threats.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Through an extensive empirical investigation, we evaluate the efficacy of our proposed hybrid learning paradigm across various continual learning strategies, model architectures, and datasets. We compare our approach with traditional ML techniques and state-of-the-art methods, demonstrating its superior performance in detecting the latest phishing threats while preserving knowledge from past data distributions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contributions of this research are twofold: (1) a novel hybrid learning paradigm that combines federated and continual learning for robust web phishing detection in dynamic environments, and (2) a tailored attention-based classifier model designed explicitly for web phishing detection, leveraging attention mechanisms and adaptive feature selection.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">By addressing the limitations of existing approaches and offering a comprehensive solution for robust and adaptable phishing detection, our work contributes significantly to the ongoing efforts in mitigating this persistent cyber threat, ultimately enhancing online security and protecting users from falling victim to these deceptive attacks.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Phishing website detection remains a critical challenge in cybersecurity, prompting extensive research efforts to develop effective countermeasures. Numerous studies have focused on employing machine learning (ML) techniques for phishing detection. Zamir et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposed a stacking model approach combining multiple ML algorithms, while Salloum et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> compared various classifiers, including XGBoost, Random Forest, and ANN. Similarly, Alnemari and Alshammari <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> evaluated ANNs, SVMs, DTs, and RF for detecting phishing domains. However, these traditional ML techniques often struggle to adapt to the dynamic nature of phishing attacks and may suffer from performance deterioration over time.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">To address this limitation, our work introduces a novel hybrid learning paradigm that combines federated learning and continual learning. By enabling distributed learning nodes to continuously adapt their models to the most recent phishing data streams, our approach ensures robust phishing detection in dynamic environments without the need for data accumulation. This approach aligns with the work of Ejaz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, who advocated for continual learning as a sustainable solution for phishing attack detection over time.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Several studies have explored deep learning architectures for phishing detection. Dutta <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> employed a recurrent neural network, while Maci et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> utilized the double deep Q-Network (DDQN) for unbalanced web phishing classification. Nagy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> developed a parallel computing ML model using CNN and LSTM for real-time detection. In contrast, our work introduces a novel attention-based classifier model tailored specifically for web phishing detection, leveraging attention mechanisms to capture intricate patterns and contextual cues indicative of phishing websites.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Feature engineering and selection have been crucial aspects in phishing detection research. Sánchez-Paniagua et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> highlighted the limitations of existing datasets and proposed a new dataset containing essential elements like login samples. Das Guptta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> considered URL and hyperlink-based hybrid features, while Abdul Samad et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> investigated the impact of feature selection and hyperparameter optimization. Our approach complements these efforts by incorporating attention-based feature selection mechanisms into our novel classifier model, adaptively identifying the most relevant features for accurate phishing detection.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Several studies have explored hybrid and ensemble approaches. Jovanovic et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> introduced a hybrid two-level framework for feature selection and XGBoost tuning, while Sakhare et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> combined multiple algorithms, including XGBoost, LightGBM, and Graph Neural Networks. Prabakaran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> integrated Variational Autoencoders (VAE) with deep neural networks. Our hybrid learning paradigm leverages the strengths of federated and continual learning, presenting a unique and effective solution for robust phishing detection in dynamic environments.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Researchers have also investigated alternative techniques, such as Bozkir et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> proposing the GramBeddings model using n-gram embeddings, Ozcan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> combining character embedding-based and NLP features in a hybrid DNN-LSTM model, and Kumar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> employing swarm intelligence optimization for network design. While these approaches contribute valuable insights, our work focuses on a comprehensive solution that addresses the key challenges of adaptability, feature selection, and model robustness in phishing detection.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">In summary, our proposed approach stands out by introducing a novel hybrid learning paradigm that combines the benefits of federated and continual learning, enabling distributed nodes to continually adapt their models to the most recent phishing data streams. Additionally, our work introduces a tailored attention-based classifier model designed explicitly for web phishing detection, leveraging attention mechanisms and adaptive feature selection to capture intricate phishing patterns effectively. By addressing the limitations of traditional ML techniques and offering a comprehensive solution for robust and adaptable phishing detection, our approach contributes significantly to the ongoing efforts in mitigating this persistent cyber threat.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">About the dataset</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The dataset used in this study is the result of merging two publicly available datasets: the ”Web page phishing detection” dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and the ”Phishing Websites Dataset” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. A subset of the most relevant features was selectively included in the merged dataset to avoid redundancy and focus on the shared characteristics between the two original datasets. The resulting dataset provides a comprehensive view of the shared features while maintaining a streamlined and focused set of attributes.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Initially, the raw dataset exhibited class imbalance, with a disproportionate number of samples in normal class compared to the phishing. To address this issue, undersampling was performed, where the majority class was downsampled to match the size of the minority class, resulting in a balanced dataset.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The dataset with each row representing a website and each column representing a feature. The last column contains the binary label indicating whether the website is phishing (1) or legitimate (0). The dataset consists of 20 features related to the characteristics of the URL. These features include the length of the URL, the count of various special characters (such as dots, hyphens, underscores, slashes, question marks, equal signs, at symbols, ampersands, exclamation marks, spaces, tildes, commas, plus signs, asterisks, hashtags, dollar signs, and percent signs), and the count of redirections within the URL. Fig. <a href="#S3.F1" title="Figure 1 ‣ III About the dataset ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, illustrates the correlation of the feature columns with the target.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2405.03537/assets/feature_correlation.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="358" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Features correlation with target.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Proposed Solution</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To address the limitations of traditional machine learning approaches in detecting phishing websites and adapt to the dynamic nature of these attacks, we propose a novel hybrid learning paradigm that seamlessly integrates federated learning and continual learning. This integrative approach leverages the strengths of both learning paradigms, enabling distributed nodes to continually update their models based on the most recent phishing data streams while preserving user privacy and data sovereignty. Fig. <a href="#S4.F2" title="Figure 2 ‣ IV Proposed Solution ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, provides an overview of our proposed solution.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2405.03537/assets/workflow.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Schematic of proposed solution.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Federated Learning for Collaborative Model Training</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Federated learning is a decentralized approach to machine learning that enables multiple parties to collaboratively train a shared model without exchanging raw data. In our proposed solution, distributed learning nodes (e.g., client devices, organizations, or edge nodes) train local models on their respective phishing data streams. These local models are then aggregated at a central server through a secure federated averaging protocol, which updates the global model without exposing individual data points.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The federated learning framework offers several advantages in the context of phishing website detection. First, it preserves user privacy by keeping sensitive data on local devices, eliminating the need for centralized data storage. Second, it enables collaborative learning from diverse data sources, enhancing the model’s ability to capture a wide range of phishing patterns and strategies. Third, it fosters data sovereignty, allowing organizations or individuals to retain control over their data while benefiting from the collective knowledge of the federated network.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Continual Learning for Adapting to Evolving Threats
</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">While federated learning facilitates collaborative model training, the dynamic nature of phishing attacks demands a continuous learning approach to ensure the detection system remains effective against emerging threats. Continual learning, also known as lifelong learning, enables machine learning models to adapt to new data distributions without catastrophically forgetting previously learned knowledge.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In our proposed solution, the distributed learning nodes employ continual learning strategies to continually update their local models based on the most recent phishing data streams. We investigate various continual learning techniques, including cumulative learning, replay-based methods (e.g., experience replay), regularization-based approaches (e.g., MIR, LwF), and others, to identify the most effective strategy for phishing website detection.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>Naive Continual Learning</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Naive continual learning, also known as incremental learning, is a straightforward approach where a model is trained continuously on new data without considering the previously learned knowledge. While this strategy is simple to implement, it often leads to catastrophic forgetting, where the model loses performance on previously learned tasks as it focuses solely on the most recent data. Naive continual learning lacks mechanisms to mitigate forgetting and adapt to new tasks without compromising performance on older ones.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>Cumulative Continual Learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Cumulative continual learning aims to address the issue of catastrophic forgetting by preserving previously learned knowledge while incorporating new information. This strategy involves updating the model parameters in a way that minimizes interference with existing knowledge when learning new tasks. Techniques such as parameter regularization, rehearsal, and task-specific constraints are employed to ensure that the model retains its performance on previous tasks while adapting to new ones. Cumulative continual learning is effective in maintaining a balance between old and new knowledge, thereby enabling the model to learn sequentially without significant performance degradation.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS3.4.1.1" class="ltx_text">IV-B</span>3 </span>Replay Continual Learning</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Replay continual learning mitigates catastrophic forgetting by periodically replaying samples from previous tasks during training. These replayed samples serve as a form of rehearsal, allowing the model to reinforce its understanding of earlier tasks while learning new ones. Replay can be implemented using various techniques, such as storing a buffer of past experiences or generating synthetic data resembling previous tasks. By exposing the model to a diverse range of past experiences, replay continual learning helps to preserve and consolidate previously acquired knowledge, thus improving overall performance across multiple tasks over time.</p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS4.4.1.1" class="ltx_text">IV-B</span>4 </span>Learning without Forgetting (LwF)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">Learning without Forgetting (LwF) is a continual learning strategy that aims to retain performance on previous tasks while learning new ones without explicit rehearsal or replay. LwF achieves this by leveraging the knowledge distillation technique, where the model is trained to mimic its own predictions from previous iterations when learning new tasks. By distilling the knowledge from the old model into the new one, LwF encourages the model to preserve its understanding of previous tasks while adapting to new ones. This approach effectively reduces the risk of catastrophic forgetting by ensuring that the model’s knowledge is retained and transferred across tasks throughout the learning process.</p>
</div>
</section>
<section id="S4.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS5.4.1.1" class="ltx_text">IV-B</span>5 </span>Maximally Interfered Replay (MIR)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</h4>

<div id="S4.SS2.SSS5.p1" class="ltx_para">
<p id="S4.SS2.SSS5.p1.1" class="ltx_p">Maximally Interfered Replay (MIR) is a continual learning strategy that draws inspiration from human memory mechanisms to mitigate forgetting. MIR maintains a memory buffer containing representations of past experiences, which are selectively sampled and replayed alongside new data during training. By integrating past experiences into the learning process, MIR enables the model to continually update its knowledge while retaining information about previous tasks. The selective sampling mechanism ensures that the memory buffer remains relevant to the current learning task, optimizing the balance between old and new information. MIR’s ability to leverage past experiences for continual learning makes it effective in preserving and consolidating knowledge across multiple tasks over time.</p>
</div>
<div id="S4.SS2.SSS5.p2" class="ltx_para">
<p id="S4.SS2.SSS5.p2.1" class="ltx_p">By integrating continual learning into the federated learning framework, our approach ensures that the global model remains up-to-date and adaptable to evolving phishing strategies. As new phishing data becomes available at the local nodes, their continually updated models are aggregated through federated averaging, continuously refining the global model’s knowledge and detection capabilities.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Attention-Based Classifier Model</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">At the core of our proposed solution is a novel attention-based classifier model tailored explicitly for web phishing detection. In Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C Attention-Based Classifier Model ‣ IV Proposed Solution ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our proposed attention-based classifier model is designed to effectively capture the intricate patterns and contextual cues indicative of phishing websites, leveraging the power of attention mechanisms. The input data is first projected into a higher-dimensional space through a fully connected linear layer. This initial encoding step aims to capture a rich representation of the input data.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The encoded representation is then passed through a multi-layer encoder, which consists of a sequence of linear transformations interleaved with residual connections and multi-head attention mechanisms. Specifically, the encoder is implemented as a module list of linear layers, where each layer performs a linear transformation by projecting the input into the hidden dimension. The transformed input is then combined with the original input through a residual connection, allowing for better gradient flow and mitigating the vanishing gradient problem. Subsequently, the residual output is passed through a multi-head attention mechanism, which computes attention weights over the input sequence, enabling the model to focus on the most relevant features or patterns.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">The multi-head attention mechanism is a crucial component of our model, as it allows the model to attend to different representations of the input data by projecting the queries, keys, and values into multiple subspaces and computing attention weights separately in each subspace. This mechanism is particularly beneficial for phishing website detection, as it can help the model identify and prioritize the discriminative features and patterns that distinguish phishing websites from legitimate ones. After the multi-layer encoder, the encoded representation is passed through a dropout layer to regularize the model and prevent overfitting.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2405.03537/assets/model.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Custom neural network architecture.</figcaption>
</figure>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Finally, the encoded representation is mapped to the output classes through a fully connected linear layer, producing logits that represent the model’s predictions for the binary classification task (phishing or legitimate website). During the forward pass, the input data is first encoded and then propagated through the multi-layer encoder. At each layer, the residual connection and multi-head attention mechanism allow the model to capture and attend to the most relevant features and patterns. The encoded representation is then processed by the dropout layer and finally passed through the classification layer to obtain the output logits.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Proposed Algorithm</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Our proposed algorithm Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-D Proposed Algorithm ‣ IV Proposed Solution ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, seamlessly integrates federated learning, continual learning, and the attention-based classifier model to provide a robust and adaptable solution for phishing website detection.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div id="S4.F4.1" class="ltx_listing ltx_listing">
<div id="algx1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="algx1.l1.2" class="ltx_text ltx_font_bold">procedure</span> <span id="algx1.l1.3" class="ltx_text ltx_font_smallcaps">Hybrid-learning</span>(Initial global model <math id="algx1.l1.m1.1" class="ltx_Math" alttext="\mathbf{w}_{0}" display="inline"><semantics id="algx1.l1.m1.1a"><msub id="algx1.l1.m1.1.1" xref="algx1.l1.m1.1.1.cmml"><mi id="algx1.l1.m1.1.1.2" xref="algx1.l1.m1.1.1.2.cmml">𝐰</mi><mn id="algx1.l1.m1.1.1.3" xref="algx1.l1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algx1.l1.m1.1b"><apply id="algx1.l1.m1.1.1.cmml" xref="algx1.l1.m1.1.1"><csymbol cd="ambiguous" id="algx1.l1.m1.1.1.1.cmml" xref="algx1.l1.m1.1.1">subscript</csymbol><ci id="algx1.l1.m1.1.1.2.cmml" xref="algx1.l1.m1.1.1.2">𝐰</ci><cn type="integer" id="algx1.l1.m1.1.1.3.cmml" xref="algx1.l1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m1.1c">\mathbf{w}_{0}</annotation></semantics></math>, number of nodes <math id="algx1.l1.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="algx1.l1.m2.1a"><mi id="algx1.l1.m2.1.1" xref="algx1.l1.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="algx1.l1.m2.1b"><ci id="algx1.l1.m2.1.1.cmml" xref="algx1.l1.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m2.1c">N</annotation></semantics></math>, learning rates <math id="algx1.l1.m3.1" class="ltx_Math" alttext="\{\eta_{k}\}_{k=1}^{N}" display="inline"><semantics id="algx1.l1.m3.1a"><msubsup id="algx1.l1.m3.1.1" xref="algx1.l1.m3.1.1.cmml"><mrow id="algx1.l1.m3.1.1.1.1.1" xref="algx1.l1.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="algx1.l1.m3.1.1.1.1.1.2" xref="algx1.l1.m3.1.1.1.1.2.cmml">{</mo><msub id="algx1.l1.m3.1.1.1.1.1.1" xref="algx1.l1.m3.1.1.1.1.1.1.cmml"><mi id="algx1.l1.m3.1.1.1.1.1.1.2" xref="algx1.l1.m3.1.1.1.1.1.1.2.cmml">η</mi><mi id="algx1.l1.m3.1.1.1.1.1.1.3" xref="algx1.l1.m3.1.1.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="algx1.l1.m3.1.1.1.1.1.3" xref="algx1.l1.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="algx1.l1.m3.1.1.1.3" xref="algx1.l1.m3.1.1.1.3.cmml"><mi id="algx1.l1.m3.1.1.1.3.2" xref="algx1.l1.m3.1.1.1.3.2.cmml">k</mi><mo id="algx1.l1.m3.1.1.1.3.1" xref="algx1.l1.m3.1.1.1.3.1.cmml">=</mo><mn id="algx1.l1.m3.1.1.1.3.3" xref="algx1.l1.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="algx1.l1.m3.1.1.3" xref="algx1.l1.m3.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="algx1.l1.m3.1b"><apply id="algx1.l1.m3.1.1.cmml" xref="algx1.l1.m3.1.1"><csymbol cd="ambiguous" id="algx1.l1.m3.1.1.2.cmml" xref="algx1.l1.m3.1.1">superscript</csymbol><apply id="algx1.l1.m3.1.1.1.cmml" xref="algx1.l1.m3.1.1"><csymbol cd="ambiguous" id="algx1.l1.m3.1.1.1.2.cmml" xref="algx1.l1.m3.1.1">subscript</csymbol><set id="algx1.l1.m3.1.1.1.1.2.cmml" xref="algx1.l1.m3.1.1.1.1.1"><apply id="algx1.l1.m3.1.1.1.1.1.1.cmml" xref="algx1.l1.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algx1.l1.m3.1.1.1.1.1.1.1.cmml" xref="algx1.l1.m3.1.1.1.1.1.1">subscript</csymbol><ci id="algx1.l1.m3.1.1.1.1.1.1.2.cmml" xref="algx1.l1.m3.1.1.1.1.1.1.2">𝜂</ci><ci id="algx1.l1.m3.1.1.1.1.1.1.3.cmml" xref="algx1.l1.m3.1.1.1.1.1.1.3">𝑘</ci></apply></set><apply id="algx1.l1.m3.1.1.1.3.cmml" xref="algx1.l1.m3.1.1.1.3"><eq id="algx1.l1.m3.1.1.1.3.1.cmml" xref="algx1.l1.m3.1.1.1.3.1"></eq><ci id="algx1.l1.m3.1.1.1.3.2.cmml" xref="algx1.l1.m3.1.1.1.3.2">𝑘</ci><cn type="integer" id="algx1.l1.m3.1.1.1.3.3.cmml" xref="algx1.l1.m3.1.1.1.3.3">1</cn></apply></apply><ci id="algx1.l1.m3.1.1.3.cmml" xref="algx1.l1.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m3.1c">\{\eta_{k}\}_{k=1}^{N}</annotation></semantics></math>)

</div>
<div id="algx1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span>     Initialize local models <math id="algx1.l2.m1.1" class="ltx_Math" alttext="\mathbf{w}_{k}^{0}=\mathbf{w}_{0}" display="inline"><semantics id="algx1.l2.m1.1a"><mrow id="algx1.l2.m1.1.1" xref="algx1.l2.m1.1.1.cmml"><msubsup id="algx1.l2.m1.1.1.2" xref="algx1.l2.m1.1.1.2.cmml"><mi id="algx1.l2.m1.1.1.2.2.2" xref="algx1.l2.m1.1.1.2.2.2.cmml">𝐰</mi><mi id="algx1.l2.m1.1.1.2.2.3" xref="algx1.l2.m1.1.1.2.2.3.cmml">k</mi><mn id="algx1.l2.m1.1.1.2.3" xref="algx1.l2.m1.1.1.2.3.cmml">0</mn></msubsup><mo id="algx1.l2.m1.1.1.1" xref="algx1.l2.m1.1.1.1.cmml">=</mo><msub id="algx1.l2.m1.1.1.3" xref="algx1.l2.m1.1.1.3.cmml"><mi id="algx1.l2.m1.1.1.3.2" xref="algx1.l2.m1.1.1.3.2.cmml">𝐰</mi><mn id="algx1.l2.m1.1.1.3.3" xref="algx1.l2.m1.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algx1.l2.m1.1b"><apply id="algx1.l2.m1.1.1.cmml" xref="algx1.l2.m1.1.1"><eq id="algx1.l2.m1.1.1.1.cmml" xref="algx1.l2.m1.1.1.1"></eq><apply id="algx1.l2.m1.1.1.2.cmml" xref="algx1.l2.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l2.m1.1.1.2.1.cmml" xref="algx1.l2.m1.1.1.2">superscript</csymbol><apply id="algx1.l2.m1.1.1.2.2.cmml" xref="algx1.l2.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l2.m1.1.1.2.2.1.cmml" xref="algx1.l2.m1.1.1.2">subscript</csymbol><ci id="algx1.l2.m1.1.1.2.2.2.cmml" xref="algx1.l2.m1.1.1.2.2.2">𝐰</ci><ci id="algx1.l2.m1.1.1.2.2.3.cmml" xref="algx1.l2.m1.1.1.2.2.3">𝑘</ci></apply><cn type="integer" id="algx1.l2.m1.1.1.2.3.cmml" xref="algx1.l2.m1.1.1.2.3">0</cn></apply><apply id="algx1.l2.m1.1.1.3.cmml" xref="algx1.l2.m1.1.1.3"><csymbol cd="ambiguous" id="algx1.l2.m1.1.1.3.1.cmml" xref="algx1.l2.m1.1.1.3">subscript</csymbol><ci id="algx1.l2.m1.1.1.3.2.cmml" xref="algx1.l2.m1.1.1.3.2">𝐰</ci><cn type="integer" id="algx1.l2.m1.1.1.3.3.cmml" xref="algx1.l2.m1.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.m1.1c">\mathbf{w}_{k}^{0}=\mathbf{w}_{0}</annotation></semantics></math> for all nodes <math id="algx1.l2.m2.3" class="ltx_Math" alttext="k=1,\ldots,N" display="inline"><semantics id="algx1.l2.m2.3a"><mrow id="algx1.l2.m2.3.4" xref="algx1.l2.m2.3.4.cmml"><mi id="algx1.l2.m2.3.4.2" xref="algx1.l2.m2.3.4.2.cmml">k</mi><mo id="algx1.l2.m2.3.4.1" xref="algx1.l2.m2.3.4.1.cmml">=</mo><mrow id="algx1.l2.m2.3.4.3.2" xref="algx1.l2.m2.3.4.3.1.cmml"><mn id="algx1.l2.m2.1.1" xref="algx1.l2.m2.1.1.cmml">1</mn><mo id="algx1.l2.m2.3.4.3.2.1" xref="algx1.l2.m2.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="algx1.l2.m2.2.2" xref="algx1.l2.m2.2.2.cmml">…</mi><mo id="algx1.l2.m2.3.4.3.2.2" xref="algx1.l2.m2.3.4.3.1.cmml">,</mo><mi id="algx1.l2.m2.3.3" xref="algx1.l2.m2.3.3.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l2.m2.3b"><apply id="algx1.l2.m2.3.4.cmml" xref="algx1.l2.m2.3.4"><eq id="algx1.l2.m2.3.4.1.cmml" xref="algx1.l2.m2.3.4.1"></eq><ci id="algx1.l2.m2.3.4.2.cmml" xref="algx1.l2.m2.3.4.2">𝑘</ci><list id="algx1.l2.m2.3.4.3.1.cmml" xref="algx1.l2.m2.3.4.3.2"><cn type="integer" id="algx1.l2.m2.1.1.cmml" xref="algx1.l2.m2.1.1">1</cn><ci id="algx1.l2.m2.2.2.cmml" xref="algx1.l2.m2.2.2">…</ci><ci id="algx1.l2.m2.3.3.cmml" xref="algx1.l2.m2.3.3">𝑁</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.m2.3c">k=1,\ldots,N</annotation></semantics></math>

</div>
<div id="algx1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span>     Initialize cumulative weight <math id="algx1.l3.m1.1" class="ltx_Math" alttext="\mathbf{w}_{\text{cum}}=\mathbf{0}" display="inline"><semantics id="algx1.l3.m1.1a"><mrow id="algx1.l3.m1.1.1" xref="algx1.l3.m1.1.1.cmml"><msub id="algx1.l3.m1.1.1.2" xref="algx1.l3.m1.1.1.2.cmml"><mi id="algx1.l3.m1.1.1.2.2" xref="algx1.l3.m1.1.1.2.2.cmml">𝐰</mi><mtext id="algx1.l3.m1.1.1.2.3" xref="algx1.l3.m1.1.1.2.3a.cmml">cum</mtext></msub><mo id="algx1.l3.m1.1.1.1" xref="algx1.l3.m1.1.1.1.cmml">=</mo><mn id="algx1.l3.m1.1.1.3" xref="algx1.l3.m1.1.1.3.cmml">𝟎</mn></mrow><annotation-xml encoding="MathML-Content" id="algx1.l3.m1.1b"><apply id="algx1.l3.m1.1.1.cmml" xref="algx1.l3.m1.1.1"><eq id="algx1.l3.m1.1.1.1.cmml" xref="algx1.l3.m1.1.1.1"></eq><apply id="algx1.l3.m1.1.1.2.cmml" xref="algx1.l3.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l3.m1.1.1.2.1.cmml" xref="algx1.l3.m1.1.1.2">subscript</csymbol><ci id="algx1.l3.m1.1.1.2.2.cmml" xref="algx1.l3.m1.1.1.2.2">𝐰</ci><ci id="algx1.l3.m1.1.1.2.3a.cmml" xref="algx1.l3.m1.1.1.2.3"><mtext mathsize="70%" id="algx1.l3.m1.1.1.2.3.cmml" xref="algx1.l3.m1.1.1.2.3">cum</mtext></ci></apply><cn type="integer" id="algx1.l3.m1.1.1.3.cmml" xref="algx1.l3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l3.m1.1c">\mathbf{w}_{\text{cum}}=\mathbf{0}</annotation></semantics></math>

</div>
<div id="algx1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>     Initialize total sample size <math id="algx1.l4.m1.1" class="ltx_Math" alttext="n_{\text{total}}=0" display="inline"><semantics id="algx1.l4.m1.1a"><mrow id="algx1.l4.m1.1.1" xref="algx1.l4.m1.1.1.cmml"><msub id="algx1.l4.m1.1.1.2" xref="algx1.l4.m1.1.1.2.cmml"><mi id="algx1.l4.m1.1.1.2.2" xref="algx1.l4.m1.1.1.2.2.cmml">n</mi><mtext id="algx1.l4.m1.1.1.2.3" xref="algx1.l4.m1.1.1.2.3a.cmml">total</mtext></msub><mo id="algx1.l4.m1.1.1.1" xref="algx1.l4.m1.1.1.1.cmml">=</mo><mn id="algx1.l4.m1.1.1.3" xref="algx1.l4.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="algx1.l4.m1.1b"><apply id="algx1.l4.m1.1.1.cmml" xref="algx1.l4.m1.1.1"><eq id="algx1.l4.m1.1.1.1.cmml" xref="algx1.l4.m1.1.1.1"></eq><apply id="algx1.l4.m1.1.1.2.cmml" xref="algx1.l4.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l4.m1.1.1.2.1.cmml" xref="algx1.l4.m1.1.1.2">subscript</csymbol><ci id="algx1.l4.m1.1.1.2.2.cmml" xref="algx1.l4.m1.1.1.2.2">𝑛</ci><ci id="algx1.l4.m1.1.1.2.3a.cmml" xref="algx1.l4.m1.1.1.2.3"><mtext mathsize="70%" id="algx1.l4.m1.1.1.2.3.cmml" xref="algx1.l4.m1.1.1.2.3">total</mtext></ci></apply><cn type="integer" id="algx1.l4.m1.1.1.3.cmml" xref="algx1.l4.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l4.m1.1c">n_{\text{total}}=0</annotation></semantics></math>

</div>
<div id="algx1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>     <span id="algx1.l5.2" class="ltx_text ltx_font_bold">for</span> <math id="algx1.l5.m1.3" class="ltx_Math" alttext="t=1,2,\ldots" display="inline"><semantics id="algx1.l5.m1.3a"><mrow id="algx1.l5.m1.3.4" xref="algx1.l5.m1.3.4.cmml"><mi id="algx1.l5.m1.3.4.2" xref="algx1.l5.m1.3.4.2.cmml">t</mi><mo id="algx1.l5.m1.3.4.1" xref="algx1.l5.m1.3.4.1.cmml">=</mo><mrow id="algx1.l5.m1.3.4.3.2" xref="algx1.l5.m1.3.4.3.1.cmml"><mn id="algx1.l5.m1.1.1" xref="algx1.l5.m1.1.1.cmml">1</mn><mo id="algx1.l5.m1.3.4.3.2.1" xref="algx1.l5.m1.3.4.3.1.cmml">,</mo><mn id="algx1.l5.m1.2.2" xref="algx1.l5.m1.2.2.cmml">2</mn><mo id="algx1.l5.m1.3.4.3.2.2" xref="algx1.l5.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="algx1.l5.m1.3.3" xref="algx1.l5.m1.3.3.cmml">…</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l5.m1.3b"><apply id="algx1.l5.m1.3.4.cmml" xref="algx1.l5.m1.3.4"><eq id="algx1.l5.m1.3.4.1.cmml" xref="algx1.l5.m1.3.4.1"></eq><ci id="algx1.l5.m1.3.4.2.cmml" xref="algx1.l5.m1.3.4.2">𝑡</ci><list id="algx1.l5.m1.3.4.3.1.cmml" xref="algx1.l5.m1.3.4.3.2"><cn type="integer" id="algx1.l5.m1.1.1.cmml" xref="algx1.l5.m1.1.1">1</cn><cn type="integer" id="algx1.l5.m1.2.2.cmml" xref="algx1.l5.m1.2.2">2</cn><ci id="algx1.l5.m1.3.3.cmml" xref="algx1.l5.m1.3.3">…</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l5.m1.3c">t=1,2,\ldots</annotation></semantics></math> <span id="algx1.l5.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="algx1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>         Wait for a node <math id="algx1.l6.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="algx1.l6.m1.1a"><mi id="algx1.l6.m1.1.1" xref="algx1.l6.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="algx1.l6.m1.1b"><ci id="algx1.l6.m1.1.1.cmml" xref="algx1.l6.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l6.m1.1c">k</annotation></semantics></math> to send its updated model <math id="algx1.l6.m2.1" class="ltx_Math" alttext="\mathbf{w}_{k}^{t}" display="inline"><semantics id="algx1.l6.m2.1a"><msubsup id="algx1.l6.m2.1.1" xref="algx1.l6.m2.1.1.cmml"><mi id="algx1.l6.m2.1.1.2.2" xref="algx1.l6.m2.1.1.2.2.cmml">𝐰</mi><mi id="algx1.l6.m2.1.1.2.3" xref="algx1.l6.m2.1.1.2.3.cmml">k</mi><mi id="algx1.l6.m2.1.1.3" xref="algx1.l6.m2.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="algx1.l6.m2.1b"><apply id="algx1.l6.m2.1.1.cmml" xref="algx1.l6.m2.1.1"><csymbol cd="ambiguous" id="algx1.l6.m2.1.1.1.cmml" xref="algx1.l6.m2.1.1">superscript</csymbol><apply id="algx1.l6.m2.1.1.2.cmml" xref="algx1.l6.m2.1.1"><csymbol cd="ambiguous" id="algx1.l6.m2.1.1.2.1.cmml" xref="algx1.l6.m2.1.1">subscript</csymbol><ci id="algx1.l6.m2.1.1.2.2.cmml" xref="algx1.l6.m2.1.1.2.2">𝐰</ci><ci id="algx1.l6.m2.1.1.2.3.cmml" xref="algx1.l6.m2.1.1.2.3">𝑘</ci></apply><ci id="algx1.l6.m2.1.1.3.cmml" xref="algx1.l6.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l6.m2.1c">\mathbf{w}_{k}^{t}</annotation></semantics></math> and local sample size <math id="algx1.l6.m3.1" class="ltx_Math" alttext="n_{k}^{t}" display="inline"><semantics id="algx1.l6.m3.1a"><msubsup id="algx1.l6.m3.1.1" xref="algx1.l6.m3.1.1.cmml"><mi id="algx1.l6.m3.1.1.2.2" xref="algx1.l6.m3.1.1.2.2.cmml">n</mi><mi id="algx1.l6.m3.1.1.2.3" xref="algx1.l6.m3.1.1.2.3.cmml">k</mi><mi id="algx1.l6.m3.1.1.3" xref="algx1.l6.m3.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="algx1.l6.m3.1b"><apply id="algx1.l6.m3.1.1.cmml" xref="algx1.l6.m3.1.1"><csymbol cd="ambiguous" id="algx1.l6.m3.1.1.1.cmml" xref="algx1.l6.m3.1.1">superscript</csymbol><apply id="algx1.l6.m3.1.1.2.cmml" xref="algx1.l6.m3.1.1"><csymbol cd="ambiguous" id="algx1.l6.m3.1.1.2.1.cmml" xref="algx1.l6.m3.1.1">subscript</csymbol><ci id="algx1.l6.m3.1.1.2.2.cmml" xref="algx1.l6.m3.1.1.2.2">𝑛</ci><ci id="algx1.l6.m3.1.1.2.3.cmml" xref="algx1.l6.m3.1.1.2.3">𝑘</ci></apply><ci id="algx1.l6.m3.1.1.3.cmml" xref="algx1.l6.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l6.m3.1c">n_{k}^{t}</annotation></semantics></math>

</div>
<div id="algx1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>         <math id="algx1.l7.m1.1" class="ltx_Math" alttext="n_{\text{total}}\leftarrow n_{\text{total}}+n_{k}^{t}" display="inline"><semantics id="algx1.l7.m1.1a"><mrow id="algx1.l7.m1.1.1" xref="algx1.l7.m1.1.1.cmml"><msub id="algx1.l7.m1.1.1.2" xref="algx1.l7.m1.1.1.2.cmml"><mi id="algx1.l7.m1.1.1.2.2" xref="algx1.l7.m1.1.1.2.2.cmml">n</mi><mtext id="algx1.l7.m1.1.1.2.3" xref="algx1.l7.m1.1.1.2.3a.cmml">total</mtext></msub><mo stretchy="false" id="algx1.l7.m1.1.1.1" xref="algx1.l7.m1.1.1.1.cmml">←</mo><mrow id="algx1.l7.m1.1.1.3" xref="algx1.l7.m1.1.1.3.cmml"><msub id="algx1.l7.m1.1.1.3.2" xref="algx1.l7.m1.1.1.3.2.cmml"><mi id="algx1.l7.m1.1.1.3.2.2" xref="algx1.l7.m1.1.1.3.2.2.cmml">n</mi><mtext id="algx1.l7.m1.1.1.3.2.3" xref="algx1.l7.m1.1.1.3.2.3a.cmml">total</mtext></msub><mo id="algx1.l7.m1.1.1.3.1" xref="algx1.l7.m1.1.1.3.1.cmml">+</mo><msubsup id="algx1.l7.m1.1.1.3.3" xref="algx1.l7.m1.1.1.3.3.cmml"><mi id="algx1.l7.m1.1.1.3.3.2.2" xref="algx1.l7.m1.1.1.3.3.2.2.cmml">n</mi><mi id="algx1.l7.m1.1.1.3.3.2.3" xref="algx1.l7.m1.1.1.3.3.2.3.cmml">k</mi><mi id="algx1.l7.m1.1.1.3.3.3" xref="algx1.l7.m1.1.1.3.3.3.cmml">t</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l7.m1.1b"><apply id="algx1.l7.m1.1.1.cmml" xref="algx1.l7.m1.1.1"><ci id="algx1.l7.m1.1.1.1.cmml" xref="algx1.l7.m1.1.1.1">←</ci><apply id="algx1.l7.m1.1.1.2.cmml" xref="algx1.l7.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l7.m1.1.1.2.1.cmml" xref="algx1.l7.m1.1.1.2">subscript</csymbol><ci id="algx1.l7.m1.1.1.2.2.cmml" xref="algx1.l7.m1.1.1.2.2">𝑛</ci><ci id="algx1.l7.m1.1.1.2.3a.cmml" xref="algx1.l7.m1.1.1.2.3"><mtext mathsize="70%" id="algx1.l7.m1.1.1.2.3.cmml" xref="algx1.l7.m1.1.1.2.3">total</mtext></ci></apply><apply id="algx1.l7.m1.1.1.3.cmml" xref="algx1.l7.m1.1.1.3"><plus id="algx1.l7.m1.1.1.3.1.cmml" xref="algx1.l7.m1.1.1.3.1"></plus><apply id="algx1.l7.m1.1.1.3.2.cmml" xref="algx1.l7.m1.1.1.3.2"><csymbol cd="ambiguous" id="algx1.l7.m1.1.1.3.2.1.cmml" xref="algx1.l7.m1.1.1.3.2">subscript</csymbol><ci id="algx1.l7.m1.1.1.3.2.2.cmml" xref="algx1.l7.m1.1.1.3.2.2">𝑛</ci><ci id="algx1.l7.m1.1.1.3.2.3a.cmml" xref="algx1.l7.m1.1.1.3.2.3"><mtext mathsize="70%" id="algx1.l7.m1.1.1.3.2.3.cmml" xref="algx1.l7.m1.1.1.3.2.3">total</mtext></ci></apply><apply id="algx1.l7.m1.1.1.3.3.cmml" xref="algx1.l7.m1.1.1.3.3"><csymbol cd="ambiguous" id="algx1.l7.m1.1.1.3.3.1.cmml" xref="algx1.l7.m1.1.1.3.3">superscript</csymbol><apply id="algx1.l7.m1.1.1.3.3.2.cmml" xref="algx1.l7.m1.1.1.3.3"><csymbol cd="ambiguous" id="algx1.l7.m1.1.1.3.3.2.1.cmml" xref="algx1.l7.m1.1.1.3.3">subscript</csymbol><ci id="algx1.l7.m1.1.1.3.3.2.2.cmml" xref="algx1.l7.m1.1.1.3.3.2.2">𝑛</ci><ci id="algx1.l7.m1.1.1.3.3.2.3.cmml" xref="algx1.l7.m1.1.1.3.3.2.3">𝑘</ci></apply><ci id="algx1.l7.m1.1.1.3.3.3.cmml" xref="algx1.l7.m1.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l7.m1.1c">n_{\text{total}}\leftarrow n_{\text{total}}+n_{k}^{t}</annotation></semantics></math>

</div>
<div id="algx1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span>         <math id="algx1.l8.m1.1" class="ltx_Math" alttext="\mathbf{w}_{\text{cum}}\leftarrow\mathbf{w}_{\text{cum}}+n_{k}^{t}\mathbf{w}_{k}^{t}" display="inline"><semantics id="algx1.l8.m1.1a"><mrow id="algx1.l8.m1.1.1" xref="algx1.l8.m1.1.1.cmml"><msub id="algx1.l8.m1.1.1.2" xref="algx1.l8.m1.1.1.2.cmml"><mi id="algx1.l8.m1.1.1.2.2" xref="algx1.l8.m1.1.1.2.2.cmml">𝐰</mi><mtext id="algx1.l8.m1.1.1.2.3" xref="algx1.l8.m1.1.1.2.3a.cmml">cum</mtext></msub><mo stretchy="false" id="algx1.l8.m1.1.1.1" xref="algx1.l8.m1.1.1.1.cmml">←</mo><mrow id="algx1.l8.m1.1.1.3" xref="algx1.l8.m1.1.1.3.cmml"><msub id="algx1.l8.m1.1.1.3.2" xref="algx1.l8.m1.1.1.3.2.cmml"><mi id="algx1.l8.m1.1.1.3.2.2" xref="algx1.l8.m1.1.1.3.2.2.cmml">𝐰</mi><mtext id="algx1.l8.m1.1.1.3.2.3" xref="algx1.l8.m1.1.1.3.2.3a.cmml">cum</mtext></msub><mo id="algx1.l8.m1.1.1.3.1" xref="algx1.l8.m1.1.1.3.1.cmml">+</mo><mrow id="algx1.l8.m1.1.1.3.3" xref="algx1.l8.m1.1.1.3.3.cmml"><msubsup id="algx1.l8.m1.1.1.3.3.2" xref="algx1.l8.m1.1.1.3.3.2.cmml"><mi id="algx1.l8.m1.1.1.3.3.2.2.2" xref="algx1.l8.m1.1.1.3.3.2.2.2.cmml">n</mi><mi id="algx1.l8.m1.1.1.3.3.2.2.3" xref="algx1.l8.m1.1.1.3.3.2.2.3.cmml">k</mi><mi id="algx1.l8.m1.1.1.3.3.2.3" xref="algx1.l8.m1.1.1.3.3.2.3.cmml">t</mi></msubsup><mo lspace="0em" rspace="0em" id="algx1.l8.m1.1.1.3.3.1" xref="algx1.l8.m1.1.1.3.3.1.cmml">​</mo><msubsup id="algx1.l8.m1.1.1.3.3.3" xref="algx1.l8.m1.1.1.3.3.3.cmml"><mi id="algx1.l8.m1.1.1.3.3.3.2.2" xref="algx1.l8.m1.1.1.3.3.3.2.2.cmml">𝐰</mi><mi id="algx1.l8.m1.1.1.3.3.3.2.3" xref="algx1.l8.m1.1.1.3.3.3.2.3.cmml">k</mi><mi id="algx1.l8.m1.1.1.3.3.3.3" xref="algx1.l8.m1.1.1.3.3.3.3.cmml">t</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l8.m1.1b"><apply id="algx1.l8.m1.1.1.cmml" xref="algx1.l8.m1.1.1"><ci id="algx1.l8.m1.1.1.1.cmml" xref="algx1.l8.m1.1.1.1">←</ci><apply id="algx1.l8.m1.1.1.2.cmml" xref="algx1.l8.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.2.1.cmml" xref="algx1.l8.m1.1.1.2">subscript</csymbol><ci id="algx1.l8.m1.1.1.2.2.cmml" xref="algx1.l8.m1.1.1.2.2">𝐰</ci><ci id="algx1.l8.m1.1.1.2.3a.cmml" xref="algx1.l8.m1.1.1.2.3"><mtext mathsize="70%" id="algx1.l8.m1.1.1.2.3.cmml" xref="algx1.l8.m1.1.1.2.3">cum</mtext></ci></apply><apply id="algx1.l8.m1.1.1.3.cmml" xref="algx1.l8.m1.1.1.3"><plus id="algx1.l8.m1.1.1.3.1.cmml" xref="algx1.l8.m1.1.1.3.1"></plus><apply id="algx1.l8.m1.1.1.3.2.cmml" xref="algx1.l8.m1.1.1.3.2"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.3.2.1.cmml" xref="algx1.l8.m1.1.1.3.2">subscript</csymbol><ci id="algx1.l8.m1.1.1.3.2.2.cmml" xref="algx1.l8.m1.1.1.3.2.2">𝐰</ci><ci id="algx1.l8.m1.1.1.3.2.3a.cmml" xref="algx1.l8.m1.1.1.3.2.3"><mtext mathsize="70%" id="algx1.l8.m1.1.1.3.2.3.cmml" xref="algx1.l8.m1.1.1.3.2.3">cum</mtext></ci></apply><apply id="algx1.l8.m1.1.1.3.3.cmml" xref="algx1.l8.m1.1.1.3.3"><times id="algx1.l8.m1.1.1.3.3.1.cmml" xref="algx1.l8.m1.1.1.3.3.1"></times><apply id="algx1.l8.m1.1.1.3.3.2.cmml" xref="algx1.l8.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.3.3.2.1.cmml" xref="algx1.l8.m1.1.1.3.3.2">superscript</csymbol><apply id="algx1.l8.m1.1.1.3.3.2.2.cmml" xref="algx1.l8.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.3.3.2.2.1.cmml" xref="algx1.l8.m1.1.1.3.3.2">subscript</csymbol><ci id="algx1.l8.m1.1.1.3.3.2.2.2.cmml" xref="algx1.l8.m1.1.1.3.3.2.2.2">𝑛</ci><ci id="algx1.l8.m1.1.1.3.3.2.2.3.cmml" xref="algx1.l8.m1.1.1.3.3.2.2.3">𝑘</ci></apply><ci id="algx1.l8.m1.1.1.3.3.2.3.cmml" xref="algx1.l8.m1.1.1.3.3.2.3">𝑡</ci></apply><apply id="algx1.l8.m1.1.1.3.3.3.cmml" xref="algx1.l8.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.3.3.3.1.cmml" xref="algx1.l8.m1.1.1.3.3.3">superscript</csymbol><apply id="algx1.l8.m1.1.1.3.3.3.2.cmml" xref="algx1.l8.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.3.3.3.2.1.cmml" xref="algx1.l8.m1.1.1.3.3.3">subscript</csymbol><ci id="algx1.l8.m1.1.1.3.3.3.2.2.cmml" xref="algx1.l8.m1.1.1.3.3.3.2.2">𝐰</ci><ci id="algx1.l8.m1.1.1.3.3.3.2.3.cmml" xref="algx1.l8.m1.1.1.3.3.3.2.3">𝑘</ci></apply><ci id="algx1.l8.m1.1.1.3.3.3.3.cmml" xref="algx1.l8.m1.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l8.m1.1c">\mathbf{w}_{\text{cum}}\leftarrow\mathbf{w}_{\text{cum}}+n_{k}^{t}\mathbf{w}_{k}^{t}</annotation></semantics></math>

</div>
<div id="algx1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span>         <math id="algx1.l9.m1.1" class="ltx_Math" alttext="\mathbf{w}_{0}\leftarrow\frac{1}{n_{\text{total}}}\mathbf{w}_{\text{cum}}" display="inline"><semantics id="algx1.l9.m1.1a"><mrow id="algx1.l9.m1.1.1" xref="algx1.l9.m1.1.1.cmml"><msub id="algx1.l9.m1.1.1.2" xref="algx1.l9.m1.1.1.2.cmml"><mi id="algx1.l9.m1.1.1.2.2" xref="algx1.l9.m1.1.1.2.2.cmml">𝐰</mi><mn id="algx1.l9.m1.1.1.2.3" xref="algx1.l9.m1.1.1.2.3.cmml">0</mn></msub><mo stretchy="false" id="algx1.l9.m1.1.1.1" xref="algx1.l9.m1.1.1.1.cmml">←</mo><mrow id="algx1.l9.m1.1.1.3" xref="algx1.l9.m1.1.1.3.cmml"><mfrac id="algx1.l9.m1.1.1.3.2" xref="algx1.l9.m1.1.1.3.2.cmml"><mn id="algx1.l9.m1.1.1.3.2.2" xref="algx1.l9.m1.1.1.3.2.2.cmml">1</mn><msub id="algx1.l9.m1.1.1.3.2.3" xref="algx1.l9.m1.1.1.3.2.3.cmml"><mi id="algx1.l9.m1.1.1.3.2.3.2" xref="algx1.l9.m1.1.1.3.2.3.2.cmml">n</mi><mtext id="algx1.l9.m1.1.1.3.2.3.3" xref="algx1.l9.m1.1.1.3.2.3.3a.cmml">total</mtext></msub></mfrac><mo lspace="0em" rspace="0em" id="algx1.l9.m1.1.1.3.1" xref="algx1.l9.m1.1.1.3.1.cmml">​</mo><msub id="algx1.l9.m1.1.1.3.3" xref="algx1.l9.m1.1.1.3.3.cmml"><mi id="algx1.l9.m1.1.1.3.3.2" xref="algx1.l9.m1.1.1.3.3.2.cmml">𝐰</mi><mtext id="algx1.l9.m1.1.1.3.3.3" xref="algx1.l9.m1.1.1.3.3.3a.cmml">cum</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l9.m1.1b"><apply id="algx1.l9.m1.1.1.cmml" xref="algx1.l9.m1.1.1"><ci id="algx1.l9.m1.1.1.1.cmml" xref="algx1.l9.m1.1.1.1">←</ci><apply id="algx1.l9.m1.1.1.2.cmml" xref="algx1.l9.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l9.m1.1.1.2.1.cmml" xref="algx1.l9.m1.1.1.2">subscript</csymbol><ci id="algx1.l9.m1.1.1.2.2.cmml" xref="algx1.l9.m1.1.1.2.2">𝐰</ci><cn type="integer" id="algx1.l9.m1.1.1.2.3.cmml" xref="algx1.l9.m1.1.1.2.3">0</cn></apply><apply id="algx1.l9.m1.1.1.3.cmml" xref="algx1.l9.m1.1.1.3"><times id="algx1.l9.m1.1.1.3.1.cmml" xref="algx1.l9.m1.1.1.3.1"></times><apply id="algx1.l9.m1.1.1.3.2.cmml" xref="algx1.l9.m1.1.1.3.2"><divide id="algx1.l9.m1.1.1.3.2.1.cmml" xref="algx1.l9.m1.1.1.3.2"></divide><cn type="integer" id="algx1.l9.m1.1.1.3.2.2.cmml" xref="algx1.l9.m1.1.1.3.2.2">1</cn><apply id="algx1.l9.m1.1.1.3.2.3.cmml" xref="algx1.l9.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="algx1.l9.m1.1.1.3.2.3.1.cmml" xref="algx1.l9.m1.1.1.3.2.3">subscript</csymbol><ci id="algx1.l9.m1.1.1.3.2.3.2.cmml" xref="algx1.l9.m1.1.1.3.2.3.2">𝑛</ci><ci id="algx1.l9.m1.1.1.3.2.3.3a.cmml" xref="algx1.l9.m1.1.1.3.2.3.3"><mtext mathsize="50%" id="algx1.l9.m1.1.1.3.2.3.3.cmml" xref="algx1.l9.m1.1.1.3.2.3.3">total</mtext></ci></apply></apply><apply id="algx1.l9.m1.1.1.3.3.cmml" xref="algx1.l9.m1.1.1.3.3"><csymbol cd="ambiguous" id="algx1.l9.m1.1.1.3.3.1.cmml" xref="algx1.l9.m1.1.1.3.3">subscript</csymbol><ci id="algx1.l9.m1.1.1.3.3.2.cmml" xref="algx1.l9.m1.1.1.3.3.2">𝐰</ci><ci id="algx1.l9.m1.1.1.3.3.3a.cmml" xref="algx1.l9.m1.1.1.3.3.3"><mtext mathsize="70%" id="algx1.l9.m1.1.1.3.3.3.cmml" xref="algx1.l9.m1.1.1.3.3.3">cum</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l9.m1.1c">\mathbf{w}_{0}\leftarrow\frac{1}{n_{\text{total}}}\mathbf{w}_{\text{cum}}</annotation></semantics></math>

</div>
<div id="algx1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span>         Broadcast the updated global model <math id="algx1.l10.m1.1" class="ltx_Math" alttext="\mathbf{w}_{0}" display="inline"><semantics id="algx1.l10.m1.1a"><msub id="algx1.l10.m1.1.1" xref="algx1.l10.m1.1.1.cmml"><mi id="algx1.l10.m1.1.1.2" xref="algx1.l10.m1.1.1.2.cmml">𝐰</mi><mn id="algx1.l10.m1.1.1.3" xref="algx1.l10.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algx1.l10.m1.1b"><apply id="algx1.l10.m1.1.1.cmml" xref="algx1.l10.m1.1.1"><csymbol cd="ambiguous" id="algx1.l10.m1.1.1.1.cmml" xref="algx1.l10.m1.1.1">subscript</csymbol><ci id="algx1.l10.m1.1.1.2.cmml" xref="algx1.l10.m1.1.1.2">𝐰</ci><cn type="integer" id="algx1.l10.m1.1.1.3.cmml" xref="algx1.l10.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l10.m1.1c">\mathbf{w}_{0}</annotation></semantics></math> to all nodes

</div>
<div id="algx1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>     <span id="algx1.l11.2" class="ltx_text ltx_font_bold">end</span> <span id="algx1.l11.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="algx1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span><span id="algx1.l12.2" class="ltx_text ltx_font_bold">end</span> <span id="algx1.l12.3" class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Proposed hybrid-learning algorithm</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The proposed algorithm leverages dynamic averaging(7,8) to aggregate the local model updates from the participating nodes, ensuring that the global model remains up-to-date and adaptable to emerging phishing threats. The attention-based classifier model is optimized during the local model updates, capturing the most relevant features and patterns for phishing detection through the attention mechanism.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">By combining federated learning, continual learning, and the attention-based classifier model, our algorithm facilitates collaborative training, continuous adaptation to evolving threats, and effective pattern recognition through attention mechanisms. This approach ensures that the global model remains up-to-date and adaptable to emerging phishing strategies, while preserving user privacy and data sovereignty.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Result Analysis</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To evaluate the efficacy of our proposed hybrid learning paradigm and attention-based classifier model, we conduct an extensive empirical investigation. We compare our approach to pre-existing neural network architectures and investigate the best continual learning strategy for this scenario.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our experimental setup involves multiple datasets, to assess the performance and adaptability of our solution under various scenarios. We evaluate our approach across different continual learning strategies and model architectures, aiming to identify the optimal setup for robust and accurate phishing detection.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Experimental Setup Hyper-Parameters</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Hyper-Parameter</span></th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Federated Learning Rounds</td>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">20</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Local Nodes</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Global Server</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Epochs in Local Node</td>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">10</td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<td id="S5.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Learning Rate</td>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.001</td>
</tr>
<tr id="S5.T1.1.7.6" class="ltx_tr">
<td id="S5.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Data Streams</td>
<td id="S5.T1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">4</td>
</tr>
<tr id="S5.T1.1.8.7" class="ltx_tr">
<td id="S5.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Batch Size</td>
<td id="S5.T1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">16</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The evaluation metrics considered in our analysis include accuracy, precision, recall and F1-score across various continual learning strategies, namely Naive, Replay, Cumulative, Learning without Forgetting (LwF), and Maximally Interfered Replay (MIR). Each strategy is compared against traditional machine learning models including Simple Multilayer Perceptron (MLP), Deep MLP, and Simple Recurrent Neural Network (RNN).</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In Table <a href="#S5.T2" title="TABLE II ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> the Naive continual learning strategy, our attention-based classifier model achieved an accuracy of 0.70, outperforming Simple MLP, Deep MLP, and Simple RNN models with accuracies of 0.52, 0.50, and 0.64, respectively. The precision, recall, and F1-score for our model were 0.75, 0.60, and 0.67, indicating its effectiveness in capturing phishing website patterns compared to baseline models.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Results under Naive Strategy</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S5.T2.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">F1-Score</span></td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<td id="S5.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple MLP</td>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.52</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.25</td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.50</td>
<td id="S5.T2.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.33</td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<td id="S5.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep MLP</td>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.50</td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.43</td>
<td id="S5.T2.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.75</td>
<td id="S5.T2.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.55</td>
</tr>
<tr id="S5.T2.1.4.4" class="ltx_tr">
<td id="S5.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple RNN</td>
<td id="S5.T2.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S5.T2.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.86</td>
<td id="S5.T2.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.50</td>
<td id="S5.T2.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.62</td>
</tr>
<tr id="S5.T2.1.5.5" class="ltx_tr">
<td id="S5.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ours</td>
<td id="S5.T2.1.5.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.70</td>
<td id="S5.T2.1.5.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.75</td>
<td id="S5.T2.1.5.5.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.60</td>
<td id="S5.T2.1.5.5.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.67</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">In Table <a href="#S5.T3" title="TABLE III ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> under the Replay continual learning strategy, our model demonstrated further improvement with an accuracy of 0.73, surpassing Simple MLP, Deep MLP, and Simple RNN models with accuracies of 0.60, 0.65, and 0.67, respectively. Notably, our model achieved the highest precision and F1-score of 0.91 and 0.75, respectively, highlighting its superior ability in distinguishing phishing websites.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Results under Replay Strategy</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S5.T3.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">F1-Score</span></td>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<td id="S5.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple MLP</td>
<td id="S5.T3.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.60</td>
<td id="S5.T3.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.70</td>
<td id="S5.T3.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.57</td>
<td id="S5.T3.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
</tr>
<tr id="S5.T3.1.3.3" class="ltx_tr">
<td id="S5.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep MLP</td>
<td id="S5.T3.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.65</td>
<td id="S5.T3.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.78</td>
<td id="S5.T3.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.60</td>
<td id="S5.T3.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.67</td>
</tr>
<tr id="S5.T3.1.4.4" class="ltx_tr">
<td id="S5.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple RNN</td>
<td id="S5.T3.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.67</td>
<td id="S5.T3.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.80</td>
<td id="S5.T3.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.63</td>
<td id="S5.T3.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.71</td>
</tr>
<tr id="S5.T3.1.5.5" class="ltx_tr">
<td id="S5.T3.1.5.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ours</td>
<td id="S5.T3.1.5.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.73</td>
<td id="S5.T3.1.5.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.91</td>
<td id="S5.T3.1.5.5.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.63</td>
<td id="S5.T3.1.5.5.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.75</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">In Table <a href="#S5.T4" title="TABLE IV ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> our attention-based classifier model excelled in the Cumulative continual learning scenario, achieving an accuracy of 0.86, substantially outperforming Simple MLP, Deep MLP, and Simple RNN models with accuracies of 0.72, 0.76, and 0.76, respectively. Additionally, our model exhibited superior precision, recall, and F1-score of 0.87, 0.84, and 0.84, underscoring its robustness and adaptability in cumulative learning settings.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Results under Cumulative Strategy</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.5.1" class="ltx_text ltx_font_bold">F1-Score</span></td>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<td id="S5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple MLP</td>
<td id="S5.T4.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.72</td>
<td id="S5.T4.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.73</td>
<td id="S5.T4.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.75</td>
<td id="S5.T4.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.73</td>
</tr>
<tr id="S5.T4.1.3.3" class="ltx_tr">
<td id="S5.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep MLP</td>
<td id="S5.T4.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.76</td>
<td id="S5.T4.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.78</td>
<td id="S5.T4.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.75</td>
<td id="S5.T4.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.77</td>
</tr>
<tr id="S5.T4.1.4.4" class="ltx_tr">
<td id="S5.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple RNN</td>
<td id="S5.T4.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.76</td>
<td id="S5.T4.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.77</td>
<td id="S5.T4.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.77</td>
<td id="S5.T4.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.77</td>
</tr>
<tr id="S5.T4.1.5.5" class="ltx_tr">
<td id="S5.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ours</td>
<td id="S5.T4.1.5.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.86</td>
<td id="S5.T4.1.5.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.87</td>
<td id="S5.T4.1.5.5.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.84</td>
<td id="S5.T4.1.5.5.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.84</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">In Table <a href="#S5.T5" title="TABLE V ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> the LwF continual learning approach, our model demonstrated remarkable performance, achieving an accuracy of 0.93, significantly outperforming Simple MLP, Deep MLP, and Simple RNN models with accuracies of 0.77, 0.73, and 0.88, respectively. With precision, recall, and F1-score values of 0.90, 0.96, and 0.93, our model exhibited superior capability in preserving previously learned knowledge while accommodating new information.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Results under LwF Strategy</figcaption>
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S5.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.4.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S5.T5.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.5.1" class="ltx_text ltx_font_bold">F1-Score</span></td>
</tr>
<tr id="S5.T5.1.2.2" class="ltx_tr">
<td id="S5.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple MLP</td>
<td id="S5.T5.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.77</td>
<td id="S5.T5.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.69</td>
<td id="S5.T5.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.75</td>
<td id="S5.T5.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.71</td>
</tr>
<tr id="S5.T5.1.3.3" class="ltx_tr">
<td id="S5.T5.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep MLP</td>
<td id="S5.T5.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.73</td>
<td id="S5.T5.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.67</td>
<td id="S5.T5.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.71</td>
<td id="S5.T5.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.69</td>
</tr>
<tr id="S5.T5.1.4.4" class="ltx_tr">
<td id="S5.T5.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple RNN</td>
<td id="S5.T5.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.88</td>
<td id="S5.T5.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.87</td>
<td id="S5.T5.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.87</td>
<td id="S5.T5.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.87</td>
</tr>
<tr id="S5.T5.1.5.5" class="ltx_tr">
<td id="S5.T5.1.5.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ours</td>
<td id="S5.T5.1.5.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.93</td>
<td id="S5.T5.1.5.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.90</td>
<td id="S5.T5.1.5.5.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.96</td>
<td id="S5.T5.1.5.5.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.93</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p">In Table <a href="#S5.T6" title="TABLE VI ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> under the MIR continual learning strategy, our attention-based classifier model achieved an accuracy of 0.59, surpassing Simple MLP, Deep MLP, and Simple RNN models with accuracies of 0.53, 0.55, and 0.54, respectively. Despite the lower performance compared to other strategies, our model maintained competitive precision, recall, and F1-score values of 0.58 each, showcasing its potential in adapting to evolving data distributions.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Results under MIR Strategy</figcaption>
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S5.T6.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S5.T6.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T6.1.1.1.5.1" class="ltx_text ltx_font_bold">F1-Score</span></td>
</tr>
<tr id="S5.T6.1.2.2" class="ltx_tr">
<td id="S5.T6.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple MLP</td>
<td id="S5.T6.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.53</td>
<td id="S5.T6.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.56</td>
<td id="S5.T6.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.53</td>
<td id="S5.T6.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.53</td>
</tr>
<tr id="S5.T6.1.3.3" class="ltx_tr">
<td id="S5.T6.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Deep MLP</td>
<td id="S5.T6.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.55</td>
<td id="S5.T6.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.57</td>
<td id="S5.T6.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.53</td>
<td id="S5.T6.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.55</td>
</tr>
<tr id="S5.T6.1.4.4" class="ltx_tr">
<td id="S5.T6.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Simple RNN</td>
<td id="S5.T6.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.54</td>
<td id="S5.T6.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.56</td>
<td id="S5.T6.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.53</td>
<td id="S5.T6.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.55</td>
</tr>
<tr id="S5.T6.1.5.5" class="ltx_tr">
<td id="S5.T6.1.5.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ours</td>
<td id="S5.T6.1.5.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.59</td>
<td id="S5.T6.1.5.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.58</td>
<td id="S5.T6.1.5.5.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.58</td>
<td id="S5.T6.1.5.5.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.58</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p9" class="ltx_para">
<p id="S5.p9.1" class="ltx_p">In Fig. <a href="#S5.F5" title="Figure 5 ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, Fig. <a href="#S5.F6" title="Figure 6 ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, Fig. <a href="#S5.F7" title="Figure 7 ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Fig. <a href="#S5.F8" title="Figure 8 ‣ V Result Analysis ‣ Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, through our comprehensive empirical evaluation and comparative analysis, we demonstrate the superior performance of our proposed hybrid learning paradigm and attention-based classifier model in detecting the latest phishing threats while preserving knowledge from past data distributions. Our results highlight the advantages of integrating federated and continual learning, leveraging attention mechanisms, and incorporating adaptive feature selection for robust and adaptable phishing website detection.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2405.03537/assets/Accuracy_comparison.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="479" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy comparison.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2405.03537/assets/F1-Score_comparison.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="479" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>F1-Score comparison.</figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2405.03537/assets/Precision_comparison.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="479" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Precision comparison.</figcaption>
</figure>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2405.03537/assets/Recall_comparison.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="479" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Recall comparison.</figcaption>
</figure>
<div id="S5.p10" class="ltx_para">
<p id="S5.p10.1" class="ltx_p">In summary, our proposed solution addresses the limitations of existing approaches by introducing a novel hybrid learning paradigm that combines federated and continual learning, enabling distributed nodes to collaboratively train and continually adapt a shared model to emerging phishing threats. The attention-based classifier model, tailored explicitly for web phishing detection, leverages attention mechanisms and adaptive feature selection to capture intricate patterns and distinguish between legitimate and malicious websites effectively. Through our empirical evaluation and comparative analysis, we demonstrate the efficacy and robustness of our approach, contributing to the ongoing efforts in mitigating the persistent threat of phishing attacks.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Future Scope</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">While our proposed approach has demonstrated promising results in addressing the challenges of robust and adaptable phishing website detection, several avenues for future research and enhancement can be explored:</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Expanded Continual Learning Strategies</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Our current work investigates several continual learning strategies, including cumulative learning, replay-based methods, and regularization-based approaches like MIR and LwF. However, the field of continual learning is rapidly evolving, and new strategies, such as meta-learning-based methods, generative replay, and hybrid approaches, can be explored to further improve the model’s ability to adapt to emerging phishing threats while mitigating catastrophic forgetting.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic">Multimodal Phishing Detection</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">While our focus has been on leveraging textual and structural features for phishing detection, future research can explore the integration of multimodal data sources, such as visual elements (e.g., website screenshots, logos, images) and behavioral patterns (e.g., user interactions, mouse movements). By fusing these diverse modalities, our attention-based classifier model could potentially capture more sophisticated phishing patterns and improve detection accuracy.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.4.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.5.2" class="ltx_text ltx_font_italic">Adversarial Robustness</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">As phishing attacks continue to evolve, adversaries may employ techniques to evade detection systems, such as adversarial examples or evasion attacks. Investigating methods to enhance the robustness of our model against such adversarial attacks is crucial for ensuring its long-term effectiveness and security.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS4.4.1.1" class="ltx_text">VI-D</span> </span><span id="S6.SS4.5.2" class="ltx_text ltx_font_italic">Interpretability and Explainability</span>
</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">While our attention mechanism provides insights into the most relevant features for phishing detection, further research can be conducted to improve the interpretability and explainability of our model’s decisions. This could involve developing techniques for visualizing and explaining the attention weights, as well as integrating human-in-the-loop approaches for interactive model refinement and knowledge extraction.</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS5.4.1.1" class="ltx_text">VI-E</span> </span><span id="S6.SS5.5.2" class="ltx_text ltx_font_italic">Real-World Deployment and Continuous Monitoring</span>
</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">To fully realize the potential of our approach, future work should focus on deploying and continuously monitoring our system in real-world scenarios. This would involve addressing practical challenges such as scalability, system integration, and data privacy concerns, as well as establishing mechanisms for continuous model updates and performance monitoring.</p>
</div>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS6.4.1.1" class="ltx_text">VI-F</span> </span><span id="S6.SS6.5.2" class="ltx_text ltx_font_italic">Cross-Domain Transfer Learning</span>
</h3>

<div id="S6.SS6.p1" class="ltx_para">
<p id="S6.SS6.p1.1" class="ltx_p">Exploring the applicability of our approach to other domains beyond phishing website detection could open up new research avenues. By leveraging transfer learning techniques, our attention-based classifier model could potentially be adapted and fine-tuned for detecting other types of cyber threats or malicious activities, such as malware detection, spam filtering, or fraudulent activity identification.</p>
</div>
<div id="S6.SS6.p2" class="ltx_para">
<p id="S6.SS6.p2.1" class="ltx_p">By pursuing these future research directions, we can further advance the field of phishing website detection and contribute to the development of more robust, adaptable, and secure systems for protecting users and organizations against the ever-evolving landscape of cyber threats.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this research, we have proposed a novel hybrid learning paradigm that seamlessly integrates federated learning and continual learning to address the challenges of robust and adaptable phishing website detection. By enabling distributed nodes to collaboratively train and continually adapt a shared model, our approach ensures timely detection of emerging phishing threats while preserving user privacy and data sovereignty.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">At the core of our solution is a tailored attention-based classifier model designed explicitly for web phishing detection. This model leverages attention mechanisms to capture intricate patterns and contextual cues indicative of phishing websites, enhancing the accuracy and robustness of the detection process. Furthermore, we incorporate adaptive feature selection mechanisms to dynamically identify the most relevant features, improving the model’s performance and interpretability.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Through an extensive empirical evaluation, we have demonstrated the superior performance of our proposed approach in detecting the latest phishing threats while preserving knowledge from past data distributions. By comparing our approach with traditional machine learning techniques and state-of-the-art methods across various continual learning strategies, model architectures, and datasets, we have shown that our model combined with the Learning without Forgetting (LwF) continual learning strategy yields the most robust results.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">By addressing the limitations of existing approaches and offering a comprehensive solution for robust and adaptable phishing detection, our work contributes significantly to the ongoing efforts in mitigating the persistent threat of phishing attacks, ultimately enhancing online security and protecting users from falling victim to these deceptive attacks.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">In conclusion, our proposed hybrid learning paradigm and attention-based classifier model represent a significant step forward in the battle against phishing attacks, offering a robust and adaptable solution that can effectively detect and mitigate these persistent threats, safeguarding online users and promoting a more secure digital ecosystem.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Saleem Raja Abdul Samad, Sundarvadivazhagan Balasubaramanian, Amna Salim Al-Kaabi, Bhisham Sharma, Subrata Chowdhury, Abolfazl Mehbodniya, Julian L Webber, and Ali Bostani.

</span>
<span class="ltx_bibblock">Analysis of the performance impact of fine-tuned machine learning model for phishing url detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Electronics</span>, 12(7):1642, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R Alazaidah, A Al-Shaikh, MR AL-Mousa, H Khafajah, G Samara, M Alzyoud, N Al-Shanableh, and S Almatarneh.

</span>
<span class="ltx_bibblock">Website phishing detection using machine learning techniques.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Journal of Statistics Applications &amp; Probability</span>, 13(1):119–129, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Rahaf Aljundi et al.

</span>
<span class="ltx_bibblock">Online continual learning with maximal interfered retrieval.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, volume 32, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Shouq Alnemari and Majid Alshammari.

</span>
<span class="ltx_bibblock">Detecting phishing domains using machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 13(8):4649, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ahmet Selman Bozkir, Firat Coskun Dalgic, and Murat Aydos.

</span>
<span class="ltx_bibblock">Grambeddings: a new neural network for url based identification of phishing web pages through n-gram embeddings.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Computers &amp; Security</span>, 124:102964, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Sumitra Das Guptta, Khandaker Tayef Shahriar, Hamed Alqahtani, Dheyaaldin Alsalman, and Iqbal H Sarker.

</span>
<span class="ltx_bibblock">Modeling hybrid feature-based phishing websites detection using machine learning techniques.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Annals of Data Science</span>, 11(1):217–242, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Ashit Kumar Dutta.

</span>
<span class="ltx_bibblock">Detecting phishing websites using machine learning technique.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">PloS one</span>, 16(10):e0258361, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Asif Ejaz, Adnan Noor Mian, and Sanaullah Manzoor.

</span>
<span class="ltx_bibblock">Life-long phishing attack detection using continual learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Scientific Reports</span>, 13(1):11488, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Abdelhakim Hannousse and Salima Yahiouche.

</span>
<span class="ltx_bibblock">Towards benchmark datasets for machine learning based website phishing detection: An experimental study.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Engineering Applications of Artificial Intelligence</span>, 104:104347, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Luka Jovanovic, Dijana Jovanovic, Milos Antonijevic, Bosko Nikolic, Nebojsa Bacanin, Miodrag Zivkovic, and Ivana Strumberger.

</span>
<span class="ltx_bibblock">Improving phishing website detection using a hybrid two-level framework for feature selection and xgboost tuning.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Journal of Web Engineering</span>, 22(3):543–574, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Parvathapuram Pavan Kumar, T Jaya, and V Rajendran.

</span>
<span class="ltx_bibblock">Si-bba–a novel phishing website detection based on swarm intelligence with deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Materials Today: Proceedings</span>, 80:3129–3139, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Zhizhong Li and Derek Hoiem.

</span>
<span class="ltx_bibblock">Learning without forgetting.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, 40(12):2935–2947, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Antonio Maci, Alessandro Santorsola, Antonio Coscia, and Andrea Iannacone.

</span>
<span class="ltx_bibblock">Unbalanced web phishing classification through deep reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Computers</span>, 12(6):118, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Naya Nagy, Malak Aljabri, Afrah Shaahid, Amnah Albin Ahmed, Fatima Alnasser, Linda Almakramy, Manar Alhadab, and Shahad Alfaddagh.

</span>
<span class="ltx_bibblock">Phishing urls detection using sequential and parallel ml techniques: comparative analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Sensors</span>, 23(7):3467, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Naya Nagy, Malak Aljabri, Afrah Shaahid, Amnah Albin Ahmed, Fatima Alnasser, Linda Almakramy, Manar Alhadab, and Shahad Alfaddagh.

</span>
<span class="ltx_bibblock">Phishing urls detection using sequential and parallel ml techniques: comparative analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Sensors</span>, 23(7):3467, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Alper Ozcan, Cagatay Catal, Emrah Donmez, and Behcet Senturk.

</span>
<span class="ltx_bibblock">A hybrid dnn–lstm model for detecting phishing urls.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Neural Computing and Applications</span>, pages 1–17, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Manoj Kumar Prabakaran, Parvathy Meenakshi Sundaram, and Abinaya Devi Chandrasekar.

</span>
<span class="ltx_bibblock">An enhanced deep learning-based phishing detection mechanism to effectively identify malicious urls using variational autoencoders.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IET Information Security</span>, 17(3):423–440, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Saba Hussein Rashid and Wisam Dawood Abdullah.

</span>
<span class="ltx_bibblock">Enhanced website phishing detection based on the cyber kill chain and cloud computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Indonesian Journal of Electrical Engineering and Computer Science</span>, 32(1):517–529, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Nitin N Sakhare, Jyoti L Bangare, Radhika G Purandare, Disha S Wankhede, and Pooja Dehankar.

</span>
<span class="ltx_bibblock">Phishing website detection using advanced machine learning techniques.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">International Journal of Intelligent Systems and Applications in Engineering</span>, 12(12s):329–346, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Said Salloum, Tarek Gaber, Sunil Vadera, and Khaled Shaalan.

</span>
<span class="ltx_bibblock">Phishing website detection from urls using classical machine learning ann model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">International Conference on Security and Privacy in Communication Systems</span>, pages 509–523. Springer, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sanjeev Shukla, Manoj Misra, and Gaurav Varshney.

</span>
<span class="ltx_bibblock">Http header based phishing attack detection using machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Transactions on Emerging Telecommunications Technologies</span>, 35(1):e4872, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Fu Song, Yusi Lei, Sen Chen, Lingling Fan, and Yang Liu.

</span>
<span class="ltx_bibblock">Advanced evasion attacks and mitigations on practical ml-based phishing website classifiers.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">International Journal of Intelligent Systems</span>, 36(9):5210–5240, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K. Thórisson et al.

</span>
<span class="ltx_bibblock">Cumulative learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial General Intelligence</span>, volume 7, pages 198–208. Springer, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Grega Vrbančič.

</span>
<span class="ltx_bibblock">Phishing websites dataset, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ammara Zamir, Hikmat Ullah Khan, Tassawar Iqbal, Nazish Yousaf, Farah Aslam, Almas Anjum, and Maryam Hamdani.

</span>
<span class="ltx_bibblock">Phishing website detection using diverse machine learning algorithms.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">The Electronic Library</span>, 38(1):65–80, 2020.

</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.03536" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.03537" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.03537">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.03537" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.03538" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 15:33:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
