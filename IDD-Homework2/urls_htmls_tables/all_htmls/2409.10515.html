<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems</title>
<!--Generated on Mon Sep 16 16:42:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.10515v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S1" title="In An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S2" title="In An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background &amp; Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S2.SS1" title="In 2 Background &amp; Related Work ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Supervised Context Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S2.SS2" title="In 2 Background &amp; Related Work ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Unsupervised Learning From Dialogue Contexts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S2.SS3" title="In 2 Background &amp; Related Work ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Model Distillation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3" title="In An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Self-Learning for Dialogue ASR</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3.SS1" title="In 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Teacher Model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3.SS1.SSS1" title="In 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Learning from Explicit Context</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3.SS1.SSS1.Px1" title="In 3.1.1 Learning from Explicit Context ‣ 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title">Audio context modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3.SS1.SSS1.Px2" title="In 3.1.1 Learning from Explicit Context ‣ 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title">Text context modeling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3.SS1.SSS2" title="In 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Learning from Implicit Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3.SS1.SSS3" title="In 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Online Hard Negative Mining (Ohm)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3.SS1.SSS4" title="In 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.4 </span>Reformulation Up-Sampling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S3.SS2" title="In 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Student-Teacher Distillation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S4" title="In An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S4.SS1" title="In 4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S4.SS1.SSS1" title="In 4.1 Datasets ‣ 4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Closed Source Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S4.SS1.SSS1.Px1" title="In 4.1.1 Closed Source Data ‣ 4.1 Datasets ‣ 4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title">Mining dialogues with reformulations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S4.SS1.SSS2" title="In 4.1 Datasets ‣ 4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>OD3</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S4.SS2" title="In 4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Model Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S4.SS3" title="In 4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Training details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5" title="In An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5.SS1" title="In 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Teacher Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5.SS1.SSS0.Px1" title="In 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title">Impact of audio context:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5.SS1.SSS0.Px2" title="In 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title">Impact of text context:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5.SS1.SSS0.Px3" title="In 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title">Combining Context Types:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5.SS1.SSS0.Px4" title="In 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title">Causal vs. Non-Causal Context:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5.SS1.SSS0.Px5" title="In 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title">Implicit Context Learning:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5.SS2" title="In 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Distilling knowledge to student model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S5.SS3" title="In 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Tail-Distribution Performance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#S6" title="In An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#A1" title="In An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Insertions/Deletions/Substitutions in CLC/Ohm</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hitesh Tulsiani
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David M. Chan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shalini Ghosh
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Garima Lalwani
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Prabhat Pandey
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ankish Bansal
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sri Garimella
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ariya Rastrow
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Björn Hoffmeister
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Dialog systems, such as voice assistants, are expected to engage with users in complex, evolving conversations. Unfortunately, traditional automatic speech recognition (ASR) systems deployed in such applications are usually trained to recognize each turn independently and lack the ability to adapt to the conversational context or incorporate user feedback. In this work, we introduce a general framework for ASR in dialog systems that can go beyond learning from single-turn utterances and learn over time how to adapt to both explicit supervision and implicit user feedback present in multi-turn conversations. We accomplish that by leveraging advances in student-teacher learning and context-aware dialog processing, and designing contrastive self-supervision approaches with Ohm, a new online hard-negative mining approach. We show that leveraging our new framework compared to traditional training leads to relative WER reductions of close to 10% in real-world dialog systems, and up to 26% on public synthetic data.</p>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break"/>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Automatic speech recognition (ASR) for dialog systems has traditionally been a focused field, where the primary goal is to produce a text transcript for an utterance given the acoustic signal corresponding to that utterance <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib50" title="">2023</a>; Baevski et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib2" title="">2020</a>; Hsu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib25" title="">2021</a>; Mitra et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib42" title="">2023</a>)</cite>. While such systems have been largely successful, particularly in the domain of dialog systems and voice assistants (leading to word error rates below 2% on the Librispeech benchmark <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib49" title="">2022</a>)</cite>), in real-world applications such single-utterance systems have been shown to struggle with a long-tailed distribution of rare words, proper nouns, etc., leading to decreased user satisfaction with such systems <cite class="ltx_cite ltx_citemacro_citep">(Schwarz et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib54" title="">2023</a>; Kim &amp; Metze, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib32" title="">2018</a>; Chang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib9" title="">2021</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib12" title="">2019</a>; Sathyendra et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib53" title="">2022</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib62" title="">2021</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S1.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Traditional dialog systems learn to perform ASR using only supervised feedback with large-scale unsupervised/semi-supervised pre-training on single isolated utterances. This work introduces a novel general framework leveraging student-teacher distillation, contrastive learning, and online hard-negative mining, allowing ASR systems to learn from contextual clues and implicit feedback present in full conversational transcripts. Our two stage system naturally allows us to distill contextual signals from a context-aware teacher model to a context unaware student model.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Such a struggle with long-tailed distributions has led to several promising directions of research aimed at specializing large-scale general models to handle rare words. These approaches generally center around fine-tuning where models are tuned on rare words as they are discovered <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib37" title="">2022</a>; Ku et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib34" title="">2024</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib18" title="">2022</a>; Chang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib10" title="">2022</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib65" title="">2023b</a>; Hung et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib27" title="">2023</a>)</cite>, “ASR model personalization” where model parameters are locally adapted with user-specific context <cite class="ltx_cite ltx_citemacro_citep">(Gourav et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib19" title="">2021</a>; Biadsy et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib3" title="">2022</a>; Shor et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib56" title="">2019</a>)</cite>, or “Contextual biasing” where model inputs include additional user-specific context as part of the input to the model <cite class="ltx_cite ltx_citemacro_citep">(Jayanthi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib30" title="">2023</a>; Kim &amp; Metze, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib32" title="">2018</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib58" title="">2024</a>; Sathyendra et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib53" title="">2022</a>; Chang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib9" title="">2021</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib12" title="">2019</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib62" title="">2021</a>; Dingliwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib15" title="">2023</a>)</cite>. While these approaches have shown promising results, they often require additional compute during training or additional storage and retrieval for model parameter adapters, leading to significant compromises in terms of critical latency factors. These methods also often must rely on additional supervised training data during the training stage, leading to increased real-world system costs (such as data labeling) that are often not justifiable by marginal performance increases.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we aim to address the two key challenges – run-time performance and increased data costs – by introducing a two-stage framework for context-aware automatic speech recognition. To reduce run-time performance costs, we explore the use of model distillation in a student-teacher framework — we leverage context signals during training of the teacher model, but do not use context signals during run-time inference of the student model for efficiency. To reduce data costs, we leverage recent advances in self-supervised learning from intrinsic contextual signals, augmented with a novel algorithm for online hard-negative mining — this enables the teacher model to learn context signals in a self-supervised fashion, eliminating the need for additional supervised data during training.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Evaluating the real-world performance of such a system is challenging, as there is little publicly available dialog data. To evaluate our approach, we run experiments on a large dataset of over 200K hours of real-world de-identified data from a popular conversational assistant system and show that leveraging context can help to significantly improve teacher model performance.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our key contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce a multi-stage teacher model for automatic speech recognition in contextual dialog systems which is capable of leveraging both explicit context signals (through audio and text context) and implicit feedback signals (through contrastive learning combined with a novel online hard-negative mining algorithm) present in sequential task-oriented dialogues <cite class="ltx_cite ltx_citemacro_citep">(Chan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We leverage our teacher model in a distillation framework, and demonstrate that context signals can be distilled into a student model requiring no additional run-time compute compared to conventional systems.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We demonstrate close to a 10% relative WER improvement in real-world dialog systems applications for the teacher model, and up to 24.4% WERR on the public OD3 dataset. Further, we demonstrate close to a 4% relative WER improvement when our teacher model is distilled to the student model; providing strong evidence that learning from context at training time can be effective at test time (even when such context is unavailable). We additionally show our approach does extremely well in lower-resource domains, demonstrating up to a 22.8% WERR on the tail distribution of real-world data.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Terminology</span> We refer to our system as ”self-learning” to convey the system’s ability to iteratively improve its performance by learning from dialogue contexts and user feedback. This goes beyond traditional SSL (self-supervised learning) techniques by integrating an ”interactive” (though offline) component where the system learns from its environment rather than solely relying on pre-existing unlabeled data.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background &amp; Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Methods for modeling context for automatic speech recognition (ASR) systems can be generally categorized into two main categories: supervised methods, which rely on additional data and labels to infer context which is useful for speech recognition, and unsupervised methods, which learn context cues directly from the utterance, and any associated prior/future utterances. In this section, we discuss our proposed approach in context with prior approaches for context-aware ASR.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Supervised Context Modeling</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">As discussed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S1" title="1 Introduction ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">section 1</span></a>, supervised context modeling for automatic speech recognition largely falls into three categories:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Fine-tuning</span>: where models are fine-tuned on specific datasets to increase global context awareness.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Model Personalization</span>: where model <span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.2">parameters</span> are updated on a per-user basis using a small set of user-specific samples.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Contextual biasing</span>: where models take additional context as input during the training and inference stages.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Each of these approaches has benefits and drawbacks. Perhaps the most common approach for context modeling is fine-tuning, which includes context by training on specialized datasets <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib37" title="">2022</a>; Ku et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib34" title="">2024</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib18" title="">2022</a>; Chang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib10" title="">2022</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib65" title="">2023b</a>; Hung et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib27" title="">2023</a>)</cite>. Such an approach can be quite effective, as it turns a long-tail distribution problem into an in-domain problem. However, it requires the collection of explicit data for the target problem, and the scope of the context that a model can learn is limited to the collected data. Further, this data collection process is often expensive – thus fine-tuning is often employed largely as an augmentation to an existing pre-trained model to fix specific errors, rather than as a good method for improving context awareness in general.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">While fine-tuning adjusts the model globally to incorporate context (such as rare words), recently some approaches have been explored that focus on adjusting the model parameters locally to account for context. <cite class="ltx_cite ltx_citemacro_citet">Gourav et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib19" title="">2021</a>)</cite> show that small personalized models can be effective at incorporating information from user contexts, and <cite class="ltx_cite ltx_citemacro_citet">Biadsy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib3" title="">2022</a>)</cite> show that small model adapters consisting of only a few thousand parameters can be locally fine-tuned for each user to improve ASR recognition performance. <cite class="ltx_cite ltx_citemacro_citet">Shor et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib56" title="">2019</a>)</cite> show that such models can be trained using as little as five minutes of personalized speech. These approaches represent good ways of fine-tuning models to focus on users’ individual needs, however, training model adapters for each user can be expensive and comes with storage requirements, inference performance questions, and data privacy concerns (as speech needs to be processed in the cloud, often with batches of other user data).</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Instead of adjusting the model parameters, contextual biasing moves the inclusion of context to the input domain. Several types of context are effective including user information (such as contact names) <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib58" title="">2024</a>; Sathyendra et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib53" title="">2022</a>)</cite>, prior utterances <cite class="ltx_cite ltx_citemacro_citep">(Chang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib9" title="">2021</a>)</cite>, visual clues <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib25" title="">2021</a>; Chan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib6" title="">2022</a>)</cite>, text catalogs <cite class="ltx_cite ltx_citemacro_citep">(Dingliwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib15" title="">2023</a>; Chan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib7" title="">2023</a>)</cite>. Outside of ASR, contextual biasing has long been shown to be effective in NLP applications <cite class="ltx_cite ltx_citemacro_citep">(Novotney et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib45" title="">2022</a>; Shenoy et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib55" title="">2021</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib69" title="">2019</a>; Liu &amp; Lane, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib39" title="">2017</a>; Jaech &amp; Ostendorf, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib28" title="">2018</a>; Kim &amp; Metze, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib32" title="">2018</a>; Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib38" title="">2015</a>; Williams et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib63" title="">2018</a>; Munkhdalai et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib44" title="">2022</a>; Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib57" title="">2023</a>)</cite>. While contextual biasing represents an important component of context modeling, it is often limited by the requirement to collect supervised data during the training phase (i.e. contexts need to be explicitly collected and stored), as well as the requirement to have contexts during the inference phase, which can lead to significantly degraded performance in context-free scenarios. Further, contextual biasing often suffers from increased model complexity during inference, leading to slower response times and decreased user satisfaction.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Unsupervised Learning From Dialogue Contexts</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Instead of learning context explicitly, unsupervised learning of context clues is a largely under-explored area in automatic speech recognition. Recent work has started to explore how we can learn contextual information from audio context alone. <cite class="ltx_cite ltx_citemacro_citet">Hori et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib24" title="">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Hori et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib23" title="">2020</a>)</cite> take in several utterances at once, and use this joint context to perform automatic speech recognition on the final target utterance (demonstrating up to 15% improvements in WER). Unfortunately, these methods require previous utterances to be available at test time and suffer when no previous context is available. Using only the target utterance, <cite class="ltx_cite ltx_citemacro_citet">Chan &amp; Ghosh (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib5" title="">2022</a>)</cite> show that other unrelated utterances within a batch can be used to filter noise from automated speech recognition models, however, they do not show that such methods help beyond global and local noise removal.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Instead of using audio, <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib33" title="">2019b</a>)</cite> apply BERT to the partial ASR transcript generated so far and use those BERT embeddings to inform the generation of the next token (effectively fusing the language model with the speech model). Similarly, both <cite class="ltx_cite ltx_citemacro_citet">Chang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib11" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Duarte-Torres et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib16" title="">2024</a>)</cite> show that taking in related text context from past utterances can improve ASR performance. These approaches, while interesting, focus primarily on text embeddings of prior context, and do not show that such ASR performance can persist in a context-free scenario (as is often the case in on-device learning) or discuss the inclusion of future context (available at train, but not inference time).</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The approach of unsupervised learning from dialog contexts is closely inspired by <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib7" title="">2023</a>)</cite> who introduce a family of methods (CLC) for learning from both past and future dialog contexts, using contrastive learning between the latent representations of past/future dialogues and the latent representation of the target utterance. The motivation behind this work is that audio that shares similar dialog contexts should have more similar latent representations, and thus, is more likely to contain relevant acoustic information. While our current approach borrows the PF-CLC objective from <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib7" title="">2023</a>)</cite> as an additional pre-training objective on top of our fully supervised and self-supervised fine-tuning process, we also found that alone, PF-CLC led to only minor improvements in overall performance due to the small per-gpu batch sizes used during training (in our case, each GPU has a maximum batch size of 16). Thus, to improve the performance of PF-CLC in our real-world training scenario, we introduce a novel scheme for online hard-negative mining, allowing for improved efficiency when applying the CLC losses during fine-tuning. Further, <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib7" title="">2023</a>)</cite> does not study in detail how to embed contexts during training, the impact of training on both past and future contexts, or if this context training persists under distillation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Model Distillation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Model distillation <cite class="ltx_cite ltx_citemacro_citep">(Buciluǎ et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib4" title="">2006</a>; Hinton et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib22" title="">2015</a>)</cite> has long been an effective tool when used to improve the performance of models during inference time. Not only are student models often more efficient than teacher models, but surprisingly, such models are often more effective on downstream test data <cite class="ltx_cite ltx_citemacro_citep">(Radosavovic et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib51" title="">2018</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib67" title="">2019</a>; Pham et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib48" title="">2022</a>)</cite>. These trends have held in ASR as well, where <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib26" title="">2018</a>); Kim et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib31" title="">2019a</a>); Mun’im et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib43" title="">2019</a>)</cite> all show that large teacher ASR models can be distilled to resource-efficient but performant student models.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Beyond model compression, however, model distillation has more recently also been used effectively to bridge streaming models and non-streaming models, as both <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib66" title="">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Kurata &amp; Saon (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib36" title="">2020</a>)</cite> have shown that using distillation between model architectures can lead to overcoming fundamental architecture limitations at inference time. Recently, <cite class="ltx_cite ltx_citemacro_citet">Futami et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib17" title="">2022</a>)</cite> showed that ASR can be improved by distilling in language models such as BERT, however, they did so using a vector-based representation, unlike our proposed approach that leverages model distillation and self-supervision <cite class="ltx_cite ltx_citemacro_citep">(Pham et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib48" title="">2022</a>)</cite>. Closest to our framework, <cite class="ltx_cite ltx_citemacro_citet">Masumura et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib40" title="">2021</a>)</cite> use distillation to bridge models that have long audio input contexts (such as several input utterances) to single utterance models, however their approach is limited to using text models in context, and they do not explore using audio context or learning the contextual hints from dialog.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Self-Learning for Dialogue ASR</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">An overview of our approach is given in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, and consists of two key components: a context-aware teacher model, leveraging both explicit context signals and implicit user feedback in the dialogue, and a single-utterance student model distilled from the context-aware teacher.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Teacher Model</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of our approach. During training our teacher model ingests context from past/future audio and text along with the current utterance, and learns both implicitly and explicitly using CLC <cite class="ltx_cite ltx_citemacro_citep">(Chan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite> for implicit context learning and supervised joint loss for explicit learning from supervised data. <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.F2.4.m1.1"><semantics id="S3.F2.4.m1.1b"><mo id="S3.F2.4.m1.1.1" mathcolor="#000000" stretchy="false" xref="S3.F2.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.F2.4.m1.1c"><ci id="S3.F2.4.m1.1.1.cmml" xref="S3.F2.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.m1.1e">↑</annotation></semantics></math> show data flow in forward-pass, and <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.F2.5.m2.1"><semantics id="S3.F2.5.m2.1b"><mo id="S3.F2.5.m2.1.1" mathcolor="#FF8000" stretchy="false" xref="S3.F2.5.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.F2.5.m2.1c"><ci id="S3.F2.5.m2.1.1.cmml" xref="S3.F2.5.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m2.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.F2.5.m2.1e">↓</annotation></semantics></math> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.F2.6.m3.1"><semantics id="S3.F2.6.m3.1b"><mo id="S3.F2.6.m3.1.1" mathcolor="#0000FF" stretchy="false" xref="S3.F2.6.m3.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.F2.6.m3.1c"><ci id="S3.F2.6.m3.1.1.cmml" xref="S3.F2.6.m3.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m3.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.F2.6.m3.1e">↓</annotation></semantics></math> show loss propagation from each of the components.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Following best practices, our teacher model is composed of a Conformer-based transducer network <cite class="ltx_cite ltx_citemacro_citep">(Graves, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib20" title="">2012</a>)</cite> - a nonstreaming model which can attend to all frames in an utterance. In addition, our teacher model also leverages both past and future contexts as explained in the following sections.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Learning from Explicit Context</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.6">In this work, we leverage several explicit context sources drawn from the dialogues themselves. The first is the audio context, formed by the sequence of user input queries in a given dialogue (preceding and succeeding audio context is represented as <math alttext="X^{P}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.1.m1.1"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><msup id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">X</mi><mi id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml">P</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">𝑋</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">X^{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.1.m1.1d">italic_X start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="X^{F}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.2.m2.1"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><msup id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p1.2.m2.1.1.2" xref="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml">X</mi><mi id="S3.SS1.SSS1.p1.2.m2.1.1.3" xref="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><apply id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.2">𝑋</ci><ci id="S3.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">X^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.2.m2.1d">italic_X start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math> respectively in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S3.F2" title="Figure 2 ‣ 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>). The second is text context, the ASR one-best hypothesis (indicated as <math alttext="\hat{Y}^{P}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.3.m3.1"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><msup id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml"><mover accent="true" id="S3.SS1.SSS1.p1.3.m3.1.1.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.SSS1.p1.3.m3.1.1.2.2" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.2.cmml">Y</mi><mo id="S3.SS1.SSS1.p1.3.m3.1.1.2.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS1.SSS1.p1.3.m3.1.1.3" xref="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml">P</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><apply id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.2"><ci id="S3.SS1.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.1">^</ci><ci id="S3.SS1.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.2.2">𝑌</ci></apply><ci id="S3.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">\hat{Y}^{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.3.m3.1d">over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\hat{Y}^{F}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.4.m4.1"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><msup id="S3.SS1.SSS1.p1.4.m4.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.cmml"><mover accent="true" id="S3.SS1.SSS1.p1.4.m4.1.1.2" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS1.SSS1.p1.4.m4.1.1.2.2" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.2.cmml">Y</mi><mo id="S3.SS1.SSS1.p1.4.m4.1.1.2.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS1.SSS1.p1.4.m4.1.1.3" xref="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m4.1b"><apply id="S3.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.2"><ci id="S3.SS1.SSS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.1">^</ci><ci id="S3.SS1.SSS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.2.2">𝑌</ci></apply><ci id="S3.SS1.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m4.1c">\hat{Y}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.4.m4.1d">over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>) corresponding to the sequence of user input queries in a dialogue along with the response generated by assistant encoded in text form (indicated as <math alttext="A^{P}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.5.m5.1"><semantics id="S3.SS1.SSS1.p1.5.m5.1a"><msup id="S3.SS1.SSS1.p1.5.m5.1.1" xref="S3.SS1.SSS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS1.p1.5.m5.1.1.2" xref="S3.SS1.SSS1.p1.5.m5.1.1.2.cmml">A</mi><mi id="S3.SS1.SSS1.p1.5.m5.1.1.3" xref="S3.SS1.SSS1.p1.5.m5.1.1.3.cmml">P</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.5.m5.1b"><apply id="S3.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1.2">𝐴</ci><ci id="S3.SS1.SSS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.5.m5.1c">A^{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.5.m5.1d">italic_A start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="A^{F}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p1.6.m6.1"><semantics id="S3.SS1.SSS1.p1.6.m6.1a"><msup id="S3.SS1.SSS1.p1.6.m6.1.1" xref="S3.SS1.SSS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS1.p1.6.m6.1.1.2" xref="S3.SS1.SSS1.p1.6.m6.1.1.2.cmml">A</mi><mi id="S3.SS1.SSS1.p1.6.m6.1.1.3" xref="S3.SS1.SSS1.p1.6.m6.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.6.m6.1b"><apply id="S3.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1.2">𝐴</ci><ci id="S3.SS1.SSS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.6.m6.1c">A^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p1.6.m6.1d">italic_A start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.10">Traditionally, a transducer-based system, at each time step, outputs a probability distribution over its vocabulary (word-pieces) conditioned on the acoustic observations <math alttext="X=x_{1},x_{2},...,x_{T}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.1.m1.4"><semantics id="S3.SS1.SSS1.p2.1.m1.4a"><mrow id="S3.SS1.SSS1.p2.1.m1.4.4" xref="S3.SS1.SSS1.p2.1.m1.4.4.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.4.4.5" xref="S3.SS1.SSS1.p2.1.m1.4.4.5.cmml">X</mi><mo id="S3.SS1.SSS1.p2.1.m1.4.4.4" xref="S3.SS1.SSS1.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS1.SSS1.p2.1.m1.4.4.3.3" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.4.cmml"><msub id="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1" xref="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.4" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2" xref="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.2" xref="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.3" xref="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.5" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.4.cmml">,</mo><mi id="S3.SS1.SSS1.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml">…</mi><mo id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.6" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.2" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.3" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.3.cmml">T</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.4b"><apply id="S3.SS1.SSS1.p2.1.m1.4.4.cmml" xref="S3.SS1.SSS1.p2.1.m1.4.4"><eq id="S3.SS1.SSS1.p2.1.m1.4.4.4.cmml" xref="S3.SS1.SSS1.p2.1.m1.4.4.4"></eq><ci id="S3.SS1.SSS1.p2.1.m1.4.4.5.cmml" xref="S3.SS1.SSS1.p2.1.m1.4.4.5">𝑋</ci><list id="S3.SS1.SSS1.p2.1.m1.4.4.3.4.cmml" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.3"><apply id="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.2">𝑥</ci><cn id="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">…</ci><apply id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.2">𝑥</ci><ci id="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.4.4.3.3.3.3">𝑇</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.4c">X=x_{1},x_{2},...,x_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.1.m1.4d">italic_X = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> and previously observed word-piece tokens <math alttext="y_{1},y_{2},...,y_{u-1}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.2.m2.4"><semantics id="S3.SS1.SSS1.p2.2.m2.4a"><mrow id="S3.SS1.SSS1.p2.2.m2.4.4.3" xref="S3.SS1.SSS1.p2.2.m2.4.4.4.cmml"><msub id="S3.SS1.SSS1.p2.2.m2.2.2.1.1" xref="S3.SS1.SSS1.p2.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.SSS1.p2.2.m2.2.2.1.1.2" xref="S3.SS1.SSS1.p2.2.m2.2.2.1.1.2.cmml">y</mi><mn id="S3.SS1.SSS1.p2.2.m2.2.2.1.1.3" xref="S3.SS1.SSS1.p2.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS1.p2.2.m2.4.4.3.4" xref="S3.SS1.SSS1.p2.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.2.m2.3.3.2.2" xref="S3.SS1.SSS1.p2.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.SSS1.p2.2.m2.3.3.2.2.2" xref="S3.SS1.SSS1.p2.2.m2.3.3.2.2.2.cmml">y</mi><mn id="S3.SS1.SSS1.p2.2.m2.3.3.2.2.3" xref="S3.SS1.SSS1.p2.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS1.p2.2.m2.4.4.3.5" xref="S3.SS1.SSS1.p2.2.m2.4.4.4.cmml">,</mo><mi id="S3.SS1.SSS1.p2.2.m2.1.1" mathvariant="normal" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.SSS1.p2.2.m2.4.4.3.6" xref="S3.SS1.SSS1.p2.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.2.m2.4.4.3.3" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.cmml"><mi id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.2" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.2.cmml">y</mi><mrow id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.2" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.2.cmml">u</mi><mo id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.1" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.1.cmml">−</mo><mn id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.3" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.4b"><list id="S3.SS1.SSS1.p2.2.m2.4.4.4.cmml" xref="S3.SS1.SSS1.p2.2.m2.4.4.3"><apply id="S3.SS1.SSS1.p2.2.m2.2.2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.2.2.1.1.2">𝑦</ci><cn id="S3.SS1.SSS1.p2.2.m2.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.SSS1.p2.2.m2.3.3.2.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.p2.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.3.3.2.2.2">𝑦</ci><cn id="S3.SS1.SSS1.p2.2.m2.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">…</ci><apply id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.cmml" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.2">𝑦</ci><apply id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3"><minus id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.1"></minus><ci id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.2">𝑢</ci><cn id="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.2.m2.4.4.3.3.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.4c">y_{1},y_{2},...,y_{u-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.2.m2.4d">italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_u - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, which could be expressed as <math alttext="P(y_{u}|X,y_{1},y_{2},...,y_{u-1})" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.3.m3.3"><semantics id="S3.SS1.SSS1.p2.3.m3.3a"><mrow id="S3.SS1.SSS1.p2.3.m3.3.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.3.3.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.3.cmml">P</mi><mo id="S3.SS1.SSS1.p2.3.m3.3.3.2" xref="S3.SS1.SSS1.p2.3.m3.3.3.2.cmml">⁢</mo><mrow id="S3.SS1.SSS1.p2.3.m3.3.3.1.1" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.cmml"><mo id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.2" stretchy="false" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.cmml"><msub id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.2" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.2.cmml">y</mi><mi id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.3.cmml">u</mi></msub><mo fence="false" id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.4" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.4.cmml">|</mo><mrow id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.4.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml">X</mi><mo id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.4" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.2" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.2.cmml">y</mi><mn id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.5" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.2" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.2.cmml">y</mi><mn id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.6" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.4.cmml">,</mo><mi id="S3.SS1.SSS1.p2.3.m3.2.2" mathvariant="normal" xref="S3.SS1.SSS1.p2.3.m3.2.2.cmml">…</mi><mo id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.7" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.2" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.2.cmml">y</mi><mrow id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.2" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.2.cmml">u</mi><mo id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.1" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.3" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.3" stretchy="false" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.3b"><apply id="S3.SS1.SSS1.p2.3.m3.3.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3"><times id="S3.SS1.SSS1.p2.3.m3.3.3.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.2"></times><ci id="S3.SS1.SSS1.p2.3.m3.3.3.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.3">𝑃</ci><apply id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.4.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.4">conditional</csymbol><apply id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5">subscript</csymbol><ci id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.2">𝑦</ci><ci id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.5.3">𝑢</ci></apply><list id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.4.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3"><ci id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1">𝑋</ci><apply id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.2">𝑦</ci><cn id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.2">𝑦</ci><cn id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS1.p2.3.m3.2.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.2.2">…</ci><apply id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.2">𝑦</ci><apply id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3"><minus id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.1"></minus><ci id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.2">𝑢</ci><cn id="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.3.m3.3.3.1.1.1.3.3.3.3.3">1</cn></apply></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.3c">P(y_{u}|X,y_{1},y_{2},...,y_{u-1})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.3.m3.3d">italic_P ( italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT | italic_X , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_u - 1 end_POSTSUBSCRIPT )</annotation></semantics></math>. To model the explicit long-term context in the teacher model, we extend the above equation by further conditioning on the set of context signals, <math alttext="Z=\{\hat{Y}^{P}" class="ltx_math_unparsed" display="inline" id="S3.SS1.SSS1.p2.4.m4.1"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><mrow id="S3.SS1.SSS1.p2.4.m4.1b"><mi id="S3.SS1.SSS1.p2.4.m4.1.1">Z</mi><mo id="S3.SS1.SSS1.p2.4.m4.1.2">=</mo><mrow id="S3.SS1.SSS1.p2.4.m4.1.3"><mo id="S3.SS1.SSS1.p2.4.m4.1.3.1" stretchy="false">{</mo><msup id="S3.SS1.SSS1.p2.4.m4.1.3.2"><mover accent="true" id="S3.SS1.SSS1.p2.4.m4.1.3.2.2"><mi id="S3.SS1.SSS1.p2.4.m4.1.3.2.2.2">Y</mi><mo id="S3.SS1.SSS1.p2.4.m4.1.3.2.2.1">^</mo></mover><mi id="S3.SS1.SSS1.p2.4.m4.1.3.2.3">P</mi></msup></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.4.m4.1c">Z=\{\hat{Y}^{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.4.m4.1d">italic_Z = { over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\hat{Y}^{F}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.5.m5.1"><semantics id="S3.SS1.SSS1.p2.5.m5.1a"><msup id="S3.SS1.SSS1.p2.5.m5.1.1" xref="S3.SS1.SSS1.p2.5.m5.1.1.cmml"><mover accent="true" id="S3.SS1.SSS1.p2.5.m5.1.1.2" xref="S3.SS1.SSS1.p2.5.m5.1.1.2.cmml"><mi id="S3.SS1.SSS1.p2.5.m5.1.1.2.2" xref="S3.SS1.SSS1.p2.5.m5.1.1.2.2.cmml">Y</mi><mo id="S3.SS1.SSS1.p2.5.m5.1.1.2.1" xref="S3.SS1.SSS1.p2.5.m5.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS1.SSS1.p2.5.m5.1.1.3" xref="S3.SS1.SSS1.p2.5.m5.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.5.m5.1b"><apply id="S3.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1">superscript</csymbol><apply id="S3.SS1.SSS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1.2"><ci id="S3.SS1.SSS1.p2.5.m5.1.1.2.1.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1.2.1">^</ci><ci id="S3.SS1.SSS1.p2.5.m5.1.1.2.2.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1.2.2">𝑌</ci></apply><ci id="S3.SS1.SSS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.5.m5.1c">\hat{Y}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.5.m5.1d">over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>,
<math alttext="{A}^{P}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.6.m6.1"><semantics id="S3.SS1.SSS1.p2.6.m6.1a"><msup id="S3.SS1.SSS1.p2.6.m6.1.1" xref="S3.SS1.SSS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.SSS1.p2.6.m6.1.1.2" xref="S3.SS1.SSS1.p2.6.m6.1.1.2.cmml">A</mi><mi id="S3.SS1.SSS1.p2.6.m6.1.1.3" xref="S3.SS1.SSS1.p2.6.m6.1.1.3.cmml">P</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.6.m6.1b"><apply id="S3.SS1.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.SSS1.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.SSS1.p2.6.m6.1.1.2">𝐴</ci><ci id="S3.SS1.SSS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.SSS1.p2.6.m6.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.6.m6.1c">{A}^{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.6.m6.1d">italic_A start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="{A}^{F}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.7.m7.1"><semantics id="S3.SS1.SSS1.p2.7.m7.1a"><msup id="S3.SS1.SSS1.p2.7.m7.1.1" xref="S3.SS1.SSS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.SSS1.p2.7.m7.1.1.2" xref="S3.SS1.SSS1.p2.7.m7.1.1.2.cmml">A</mi><mi id="S3.SS1.SSS1.p2.7.m7.1.1.3" xref="S3.SS1.SSS1.p2.7.m7.1.1.3.cmml">F</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.7.m7.1b"><apply id="S3.SS1.SSS1.p2.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.SSS1.p2.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.SSS1.p2.7.m7.1.1.2">𝐴</ci><ci id="S3.SS1.SSS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.SSS1.p2.7.m7.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.7.m7.1c">{A}^{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.7.m7.1d">italic_A start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="{X}^{P}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.8.m8.1"><semantics id="S3.SS1.SSS1.p2.8.m8.1a"><msup id="S3.SS1.SSS1.p2.8.m8.1.1" xref="S3.SS1.SSS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.SSS1.p2.8.m8.1.1.2" xref="S3.SS1.SSS1.p2.8.m8.1.1.2.cmml">X</mi><mi id="S3.SS1.SSS1.p2.8.m8.1.1.3" xref="S3.SS1.SSS1.p2.8.m8.1.1.3.cmml">P</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.8.m8.1b"><apply id="S3.SS1.SSS1.p2.8.m8.1.1.cmml" xref="S3.SS1.SSS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.SSS1.p2.8.m8.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.SSS1.p2.8.m8.1.1.2">𝑋</ci><ci id="S3.SS1.SSS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.SSS1.p2.8.m8.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.8.m8.1c">{X}^{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.8.m8.1d">italic_X start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="{X}^{F}\}" class="ltx_math_unparsed" display="inline" id="S3.SS1.SSS1.p2.9.m9.1"><semantics id="S3.SS1.SSS1.p2.9.m9.1a"><mrow id="S3.SS1.SSS1.p2.9.m9.1b"><msup id="S3.SS1.SSS1.p2.9.m9.1.1"><mi id="S3.SS1.SSS1.p2.9.m9.1.1.2">X</mi><mi id="S3.SS1.SSS1.p2.9.m9.1.1.3">F</mi></msup><mo id="S3.SS1.SSS1.p2.9.m9.1.2" stretchy="false">}</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.9.m9.1c">{X}^{F}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.9.m9.1d">italic_X start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT }</annotation></semantics></math>, to get <math alttext="P(y_{u}|X,y_{1},y_{2},...,y_{u-1},Z)" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.10.m10.4"><semantics id="S3.SS1.SSS1.p2.10.m10.4a"><mrow id="S3.SS1.SSS1.p2.10.m10.4.4" xref="S3.SS1.SSS1.p2.10.m10.4.4.cmml"><mi id="S3.SS1.SSS1.p2.10.m10.4.4.3" xref="S3.SS1.SSS1.p2.10.m10.4.4.3.cmml">P</mi><mo id="S3.SS1.SSS1.p2.10.m10.4.4.2" xref="S3.SS1.SSS1.p2.10.m10.4.4.2.cmml">⁢</mo><mrow id="S3.SS1.SSS1.p2.10.m10.4.4.1.1" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.cmml"><mo id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.2" stretchy="false" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.cmml"><msub id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.cmml"><mi id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.2" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.2.cmml">y</mi><mi id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.3" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.3.cmml">u</mi></msub><mo fence="false" id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.4" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.4.cmml">|</mo><mrow id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.4.cmml"><mi id="S3.SS1.SSS1.p2.10.m10.1.1" xref="S3.SS1.SSS1.p2.10.m10.1.1.cmml">X</mi><mo id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.4" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.2" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.2.cmml">y</mi><mn id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.3" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.5" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.cmml"><mi id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.2" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.2.cmml">y</mi><mn id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.3" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.6" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.4.cmml">,</mo><mi id="S3.SS1.SSS1.p2.10.m10.2.2" mathvariant="normal" xref="S3.SS1.SSS1.p2.10.m10.2.2.cmml">…</mi><mo id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.7" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.4.cmml">,</mo><msub id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.2" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.2.cmml">y</mi><mrow id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.cmml"><mi id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.2" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.2.cmml">u</mi><mo id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.1" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.3" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.8" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.4.cmml">,</mo><mi id="S3.SS1.SSS1.p2.10.m10.3.3" xref="S3.SS1.SSS1.p2.10.m10.3.3.cmml">Z</mi></mrow></mrow><mo id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.3" stretchy="false" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.10.m10.4b"><apply id="S3.SS1.SSS1.p2.10.m10.4.4.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4"><times id="S3.SS1.SSS1.p2.10.m10.4.4.2.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.2"></times><ci id="S3.SS1.SSS1.p2.10.m10.4.4.3.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.3">𝑃</ci><apply id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.4.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.4">conditional</csymbol><apply id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5">subscript</csymbol><ci id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.2.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.2">𝑦</ci><ci id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.3.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.5.3">𝑢</ci></apply><list id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.4.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3"><ci id="S3.SS1.SSS1.p2.10.m10.1.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.1.1">𝑋</ci><apply id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.2">𝑦</ci><cn id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.2.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.2">𝑦</ci><cn id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS1.p2.10.m10.2.2.cmml" xref="S3.SS1.SSS1.p2.10.m10.2.2">…</ci><apply id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.2">𝑦</ci><apply id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3"><minus id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.1.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.1"></minus><ci id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.2.cmml" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.2">𝑢</ci><cn id="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S3.SS1.SSS1.p2.10.m10.4.4.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S3.SS1.SSS1.p2.10.m10.3.3.cmml" xref="S3.SS1.SSS1.p2.10.m10.3.3">𝑍</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.10.m10.4c">P(y_{u}|X,y_{1},y_{2},...,y_{u-1},Z)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.10.m10.4d">italic_P ( italic_y start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT | italic_X , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_u - 1 end_POSTSUBSCRIPT , italic_Z )</annotation></semantics></math>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Audio context modeling</h5>
<div class="ltx_para" id="S3.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px1.p1.1">Like <cite class="ltx_cite ltx_citemacro_citet">Hori et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib24" title="">2021</a>)</cite> we explore two methods for adding audio context from dialogues:</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.Px1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.Px1.p2.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.Px1.p2.1.1">Feature Concatenation:</span> In feature concatenation, we concatenate features of past and future utterances along with the seed utterance and pass it through the audio encoder. Encoder outputs are then segmented to extract embeddings corresponding to seed utterance and are combined with prediction network output to compute transducer loss. The presence of a self-attention network in the audio encoder allows us to learn the dependency on context streams.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.Px1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.Px1.p3.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.Px1.p3.1.1">Audio Embeddings:</span> For audio embeddings, past and future context is encoded via a separate encoder (called “context encoder”). We use either a HuBERT pre-trained conformer encoder or the audio encoder of the transducer network as the context encoder. These audio embeddings are passed through a multi-headed self-attentive pooling layer <cite class="ltx_cite ltx_citemacro_citep">(Chang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib11" title="">2023</a>)</cite> and concatenated in time dimension with keys and values of self-attention module (MHSA) in the audio encoder (represented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S3.F2" title="Figure 2 ‣ 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. Thus the inputs to MHSA (query - q, key - k, value - v) can be represented as <math alttext="q=X;k=[{X}^{P},X,{X}^{F}];v=[{X}^{P},X,{X}^{F}]" class="ltx_Math" display="inline" id="S3.SS1.SSS1.Px1.p3.1.m1.4"><semantics id="S3.SS1.SSS1.Px1.p3.1.m1.4a"><mrow id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.3.cmml"><mrow id="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1" xref="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.cmml"><mi id="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.2.cmml">q</mi><mo id="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.1" xref="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.1.cmml">=</mo><mi id="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.3.cmml">X</mi></mrow><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.3a.cmml">;</mo><mrow id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.3.cmml"><mrow id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.cmml"><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.4" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.4.cmml">k</mi><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.3.cmml">=</mo><mrow id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.3.cmml"><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.3" stretchy="false" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.3.cmml">[</mo><msup id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.2.cmml">X</mi><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.3.cmml">P</mi></msup><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.4" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.3.cmml">,</mo><mi id="S3.SS1.SSS1.Px1.p3.1.m1.1.1" xref="S3.SS1.SSS1.Px1.p3.1.m1.1.1.cmml">X</mi><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.5" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.3.cmml">,</mo><msup id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.cmml"><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.2.cmml">X</mi><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.3.cmml">F</mi></msup><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.6" stretchy="false" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.3.cmml">]</mo></mrow></mrow><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.3a.cmml">;</mo><mrow id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.cmml"><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.4" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.4.cmml">v</mi><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.3.cmml">=</mo><mrow id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.3.cmml"><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.3" stretchy="false" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.3.cmml">[</mo><msup id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.2.cmml">X</mi><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.3.cmml">P</mi></msup><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.4" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.3.cmml">,</mo><mi id="S3.SS1.SSS1.Px1.p3.1.m1.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.2.2.cmml">X</mi><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.5" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.3.cmml">,</mo><msup id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.2" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.2.cmml">X</mi><mi id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.3" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.3.cmml">F</mi></msup><mo id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.6" stretchy="false" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.3.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.Px1.p3.1.m1.4b"><apply id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.3a.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1"><eq id="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.1"></eq><ci id="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.2">𝑞</ci><ci id="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.3.3.1.1.3">𝑋</ci></apply><apply id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.3a.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1"><eq id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.3"></eq><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.4.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.4">𝑘</ci><list id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2"><apply id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.2">𝑋</ci><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.1.1.1.3">𝑃</ci></apply><ci id="S3.SS1.SSS1.Px1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.1.1">𝑋</ci><apply id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2">superscript</csymbol><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.2">𝑋</ci><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.1.1.2.2.2.3">𝐹</ci></apply></list></apply><apply id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2"><eq id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.3"></eq><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.4.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.4">𝑣</ci><list id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2"><apply id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.2">𝑋</ci><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.1.1.1.3">𝑃</ci></apply><ci id="S3.SS1.SSS1.Px1.p3.1.m1.2.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.2.2">𝑋</ci><apply id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2">superscript</csymbol><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.2">𝑋</ci><ci id="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS1.Px1.p3.1.m1.4.4.2.2.2.2.2.2.2.3">𝐹</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.Px1.p3.1.m1.4c">q=X;k=[{X}^{P},X,{X}^{F}];v=[{X}^{P},X,{X}^{F}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.Px1.p3.1.m1.4d">italic_q = italic_X ; italic_k = [ italic_X start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT , italic_X , italic_X start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ] ; italic_v = [ italic_X start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT , italic_X , italic_X start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ]</annotation></semantics></math>. This allows us to attend to the contextual signal on a per-query basis. Note here that the output and input of the MHSA module still have the same number of time frames. This ensures that no other component in the model needs to be modified. Another distinct advantage of re-purposing MHSA in this manner as opposed to introducing a separate cross-attention layer (to attend to context) is that it allows us to easily extend conventional single utterance models to be context aware.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Text context modeling</h5>
<div class="ltx_para" id="S3.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px2.p1.1">In addition to audio context, following <cite class="ltx_cite ltx_citemacro_citet">Duarte-Torres et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib16" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib31" title="">2019a</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Chang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib11" title="">2023</a>)</cite>, we explore two variants for encoding text context from prior utterances in a dialog:</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.Px2.p2">
<p class="ltx_p" id="S3.SS1.SSS1.Px2.p2.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.Px2.p2.1.1">BERT Embeddings: </span> In the BERT embedding case, we leverage a pre-trained text embedding model <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib14" title="">2019</a>)</cite> with 5M parameters based on BERT to generate a summary vector for the past/future text representations.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.Px2.p3">
<p class="ltx_p" id="S3.SS1.SSS1.Px2.p3.1"><span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.Px2.p3.1.1">Learned Embeddings:</span> In the learned embedding case, text context is tokenized using a sentence piece model <cite class="ltx_cite ltx_citemacro_citep">(Kudo &amp; Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib35" title="">2018</a>)</cite> and each token is represented as a one-hot vector over vocabulary size. This is then converted to an embedding and combined in the self-attention layer of the audio encoder (similar to the audio embeddings described above).</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Learning from Implicit Context</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">In a multi-turn interaction with a voice assistant, the user may repeat or rephrase their query (to correct the system) following an unexpected response by the assistant. To empirically establish that such user reformulations (implicit interactions) are correlated with ASR,
we conducted a simple experiment. We prepared two datasets: (i) Natural sampling: data is uniformly sampled to form our test set; and (ii) Reformulation sampling: we sample utterances that cause the user to repeat or rephrase their query. We then evaluate both our existing teacher and student models on these datasets. In this experiment, we observed that both the teacher and student models have significantly higher word error rates (11% and 15% respectively) on the reformulation sampling dataset compared to uniform sampling. This observation, combined with the fact that approximately 15% of analyzed interactions had user reformulations, shows that user-provided implicit feedback can correct ASR errors. Please note that such feedback is not directly solicited through the dialogue but inferred from user corrections and follow-up queries, hence we refer to it as ”implicit”.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">Thus, while it is possible to learn to leverage context signals from the explicit context, it is also important to learn from implicit signals in the data. Recently, <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite> showed that implicit context present in the dialogues can be used to further augment the training process through contrastive learning. Drawing on their work, in this work, we leverage the past-future CLC objective from <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite> (which we refer to as PF-CLC), to incorporate implicit context in addition to the explicit context discussed in the previous section. In this PF-CLC approach, the positive pairs contain past/future/current utterances from the same utterance, while negative pairs are formed by past/future pairs from other utterances in the batches, further encouraging the model to organize the latent space semantically in addition to phonetically during pre-training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Online Hard Negative Mining (Ohm)</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">When training our self-learning based system, we found a significant correlation between the local GPU batch size and the performance of the pre-training <cite class="ltx_cite ltx_citemacro_citep">(Jain et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib29" title="">2024</a>)</cite>. We hypothesize that this correlation is caused by the PF-CLC learning introduced in <cite class="ltx_cite ltx_citemacro_citep">(Chan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite>, as with smaller local batch sizes, there are fewer “hard negatives” in each batch, leading to reduced efficiency when training with CLC-based losses. Unfortunately, scaling the local GPU batch size can be practically difficult, without introducing complex optimizers and training procedures for model sharding. This presents a challenging issue: how can we train contrastive models under restricted local batch sizes?</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">While several technical methods have been developed for contrastive learning with small batch sizes including BASIC <cite class="ltx_cite ltx_citemacro_citep">(Pham et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib47" title="">2023</a>)</cite>, which leverage tools from gradient checkpoint and model parallelism to improve the “effective” batch size, such methods have significant compute bottlenecks, and still rely on some form of all-reduce to compute the global contrastive loss. These all-reduces are technically complex, and on many GPU clusters can lead to significant communication overhead when machines must communicate with non-local devices.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p3">
<p class="ltx_p" id="S3.SS1.SSS3.p3.1">Instead, of such a complex all-reduce based approach, we target the root of the problem by aiming to build more effective local batches (i.e. batches that will induce high contrastive loss by leveraging hard negative mining, introduced by <cite class="ltx_cite ltx_citemacro_citet">Robinson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib52" title="">2021</a>)</cite>. Traditionally, such methods for hard-negative mining rely on a pre-labeling step, where batches are pre-constructed in an offline-scan, and then consumed during training. Unfortunately, such a pre-labeling approach does not scale well as the size of the training data increases. To remedy this, we introduce Ohm, a simple online hard-negative mining procedure that can run in line with traditional streaming data pipelines. An overview of the Ohm approach is given in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#alg1" title="Algorithm 1 ‣ 3.1.3 Online Hard Negative Mining (Ohm) ‣ 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Algorithm 1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p4">
<p class="ltx_p" id="S3.SS1.SSS3.p4.9">Ohm consists of several key stages each augmenting a data pipeline. In the first stage, samples are collected into an initial buffer <math alttext="B" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.1.m1.1"><semantics id="S3.SS1.SSS3.p4.1.m1.1a"><mi id="S3.SS1.SSS3.p4.1.m1.1.1" xref="S3.SS1.SSS3.p4.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.1.m1.1b"><ci id="S3.SS1.SSS3.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p4.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.1.m1.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.1.m1.1d">italic_B</annotation></semantics></math> using a stateful map. When the size of the initial buffer exceeds the update window size, then a parametric clustering method <math alttext="C_{\phi}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.2.m2.1"><semantics id="S3.SS1.SSS3.p4.2.m2.1a"><msub id="S3.SS1.SSS3.p4.2.m2.1.1" xref="S3.SS1.SSS3.p4.2.m2.1.1.cmml"><mi id="S3.SS1.SSS3.p4.2.m2.1.1.2" xref="S3.SS1.SSS3.p4.2.m2.1.1.2.cmml">C</mi><mi id="S3.SS1.SSS3.p4.2.m2.1.1.3" xref="S3.SS1.SSS3.p4.2.m2.1.1.3.cmml">ϕ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.2.m2.1b"><apply id="S3.SS1.SSS3.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p4.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p4.2.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p4.2.m2.1.1.2">𝐶</ci><ci id="S3.SS1.SSS3.p4.2.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p4.2.m2.1.1.3">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.2.m2.1c">C_{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.2.m2.1d">italic_C start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT</annotation></semantics></math> is fit on the samples from <math alttext="B" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.3.m3.1"><semantics id="S3.SS1.SSS3.p4.3.m3.1a"><mi id="S3.SS1.SSS3.p4.3.m3.1.1" xref="S3.SS1.SSS3.p4.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.3.m3.1b"><ci id="S3.SS1.SSS3.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p4.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.3.m3.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.3.m3.1d">italic_B</annotation></semantics></math>. Note that this process happens per-device, and only with the samples yielded to each device, leading to non-blocking, and non-communicative behavior. Future version of Ohm could , however, communicate the <math alttext="C_{\phi}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.4.m4.1"><semantics id="S3.SS1.SSS3.p4.4.m4.1a"><msub id="S3.SS1.SSS3.p4.4.m4.1.1" xref="S3.SS1.SSS3.p4.4.m4.1.1.cmml"><mi id="S3.SS1.SSS3.p4.4.m4.1.1.2" xref="S3.SS1.SSS3.p4.4.m4.1.1.2.cmml">C</mi><mi id="S3.SS1.SSS3.p4.4.m4.1.1.3" xref="S3.SS1.SSS3.p4.4.m4.1.1.3.cmml">ϕ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.4.m4.1b"><apply id="S3.SS1.SSS3.p4.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p4.4.m4.1.1.1.cmml" xref="S3.SS1.SSS3.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p4.4.m4.1.1.2.cmml" xref="S3.SS1.SSS3.p4.4.m4.1.1.2">𝐶</ci><ci id="S3.SS1.SSS3.p4.4.m4.1.1.3.cmml" xref="S3.SS1.SSS3.p4.4.m4.1.1.3">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.4.m4.1c">C_{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.4.m4.1d">italic_C start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT</annotation></semantics></math> model, leading to all-reduce like behavior with reduced overhead. Once <math alttext="C_{\phi}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.5.m5.1"><semantics id="S3.SS1.SSS3.p4.5.m5.1a"><msub id="S3.SS1.SSS3.p4.5.m5.1.1" xref="S3.SS1.SSS3.p4.5.m5.1.1.cmml"><mi id="S3.SS1.SSS3.p4.5.m5.1.1.2" xref="S3.SS1.SSS3.p4.5.m5.1.1.2.cmml">C</mi><mi id="S3.SS1.SSS3.p4.5.m5.1.1.3" xref="S3.SS1.SSS3.p4.5.m5.1.1.3.cmml">ϕ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.5.m5.1b"><apply id="S3.SS1.SSS3.p4.5.m5.1.1.cmml" xref="S3.SS1.SSS3.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p4.5.m5.1.1.1.cmml" xref="S3.SS1.SSS3.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p4.5.m5.1.1.2.cmml" xref="S3.SS1.SSS3.p4.5.m5.1.1.2">𝐶</ci><ci id="S3.SS1.SSS3.p4.5.m5.1.1.3.cmml" xref="S3.SS1.SSS3.p4.5.m5.1.1.3">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.5.m5.1c">C_{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.5.m5.1d">italic_C start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT</annotation></semantics></math> had been trained, <math alttext="C_{\phi}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.6.m6.1"><semantics id="S3.SS1.SSS3.p4.6.m6.1a"><msub id="S3.SS1.SSS3.p4.6.m6.1.1" xref="S3.SS1.SSS3.p4.6.m6.1.1.cmml"><mi id="S3.SS1.SSS3.p4.6.m6.1.1.2" xref="S3.SS1.SSS3.p4.6.m6.1.1.2.cmml">C</mi><mi id="S3.SS1.SSS3.p4.6.m6.1.1.3" xref="S3.SS1.SSS3.p4.6.m6.1.1.3.cmml">ϕ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.6.m6.1b"><apply id="S3.SS1.SSS3.p4.6.m6.1.1.cmml" xref="S3.SS1.SSS3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p4.6.m6.1.1.1.cmml" xref="S3.SS1.SSS3.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p4.6.m6.1.1.2.cmml" xref="S3.SS1.SSS3.p4.6.m6.1.1.2">𝐶</ci><ci id="S3.SS1.SSS3.p4.6.m6.1.1.3.cmml" xref="S3.SS1.SSS3.p4.6.m6.1.1.3">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.6.m6.1c">C_{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.6.m6.1d">italic_C start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT</annotation></semantics></math> is used in a streaming fashion to assign labels to each sample. Finally, reservoir sampling <cite class="ltx_cite ltx_citemacro_citep">(Vitter, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib61" title="">1985</a>)</cite> is used to sample batches of samples from each cluster group as they become available. This leads to batches which are generally closer semantically, even in the presence of a poor clustering algorithm <math alttext="C_{\phi}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.7.m7.1"><semantics id="S3.SS1.SSS3.p4.7.m7.1a"><msub id="S3.SS1.SSS3.p4.7.m7.1.1" xref="S3.SS1.SSS3.p4.7.m7.1.1.cmml"><mi id="S3.SS1.SSS3.p4.7.m7.1.1.2" xref="S3.SS1.SSS3.p4.7.m7.1.1.2.cmml">C</mi><mi id="S3.SS1.SSS3.p4.7.m7.1.1.3" xref="S3.SS1.SSS3.p4.7.m7.1.1.3.cmml">ϕ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.7.m7.1b"><apply id="S3.SS1.SSS3.p4.7.m7.1.1.cmml" xref="S3.SS1.SSS3.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p4.7.m7.1.1.1.cmml" xref="S3.SS1.SSS3.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p4.7.m7.1.1.2.cmml" xref="S3.SS1.SSS3.p4.7.m7.1.1.2">𝐶</ci><ci id="S3.SS1.SSS3.p4.7.m7.1.1.3.cmml" xref="S3.SS1.SSS3.p4.7.m7.1.1.3">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.7.m7.1c">C_{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.7.m7.1d">italic_C start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT</annotation></semantics></math>. Since the representations that we are using change, we periodically update <math alttext="C_{\phi}" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.8.m8.1"><semantics id="S3.SS1.SSS3.p4.8.m8.1a"><msub id="S3.SS1.SSS3.p4.8.m8.1.1" xref="S3.SS1.SSS3.p4.8.m8.1.1.cmml"><mi id="S3.SS1.SSS3.p4.8.m8.1.1.2" xref="S3.SS1.SSS3.p4.8.m8.1.1.2.cmml">C</mi><mi id="S3.SS1.SSS3.p4.8.m8.1.1.3" xref="S3.SS1.SSS3.p4.8.m8.1.1.3.cmml">ϕ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.8.m8.1b"><apply id="S3.SS1.SSS3.p4.8.m8.1.1.cmml" xref="S3.SS1.SSS3.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p4.8.m8.1.1.1.cmml" xref="S3.SS1.SSS3.p4.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p4.8.m8.1.1.2.cmml" xref="S3.SS1.SSS3.p4.8.m8.1.1.2">𝐶</ci><ci id="S3.SS1.SSS3.p4.8.m8.1.1.3.cmml" xref="S3.SS1.SSS3.p4.8.m8.1.1.3">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.8.m8.1c">C_{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.8.m8.1d">italic_C start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT</annotation></semantics></math> (every 10,000 steps) using the buffer <math alttext="B" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p4.9.m9.1"><semantics id="S3.SS1.SSS3.p4.9.m9.1a"><mi id="S3.SS1.SSS3.p4.9.m9.1.1" xref="S3.SS1.SSS3.p4.9.m9.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p4.9.m9.1b"><ci id="S3.SS1.SSS3.p4.9.m9.1.1.cmml" xref="S3.SS1.SSS3.p4.9.m9.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p4.9.m9.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p4.9.m9.1d">italic_B</annotation></semantics></math>. Ohm thus solves a practical problem: training on GPUs with less VRAM precludes the use of larger (more effective) batch sizes, and leveraging Ohm claws back some of that performance loss.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Ohm: Online Hard Negative Mining</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l0">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l0.1.1.1" style="font-size:80%;">0:</span></span>  stream

</div>
<div class="ltx_listingline" id="alg1.l0a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l0a.1.1.1" style="font-size:80%;">0:</span></span>  Reordered stream

</div>
<div class="ltx_listingline" id="alg1.l1">  Initialize state with zero index and an empty feature array

</div>
<div class="ltx_listingline" id="alg1.l2">  <math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l2.m1.1"><semantics id="alg1.l2.m1.1a"><mo id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l2.m1.1d">▷</annotation></semantics></math> Step 1: Update clusters with streaming scan (stateful map)

</div>
<div class="ltx_listingline" id="alg1.l3">  stream <math alttext="\leftarrow" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mo id="alg1.l3.m1.1.1" stretchy="false" xref="alg1.l3.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">←</annotation></semantics></math> stream.scan(initial_state, update_clusters)

</div>
<div class="ltx_listingline" id="alg1.l4">  <math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l4.m1.1"><semantics id="alg1.l4.m1.1a"><mo id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m1.1d">▷</annotation></semantics></math> Step 2: Generate cluster labels for each sample

</div>
<div class="ltx_listingline" id="alg1.l5">  stream <math alttext="\leftarrow" class="ltx_Math" display="inline" id="alg1.l5.m1.1"><semantics id="alg1.l5.m1.1a"><mo id="alg1.l5.m1.1.1" stretchy="false" xref="alg1.l5.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.1d">←</annotation></semantics></math> stream.map(generate_labels)

</div>
<div class="ltx_listingline" id="alg1.l6">  <math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l6.m1.1"><semantics id="alg1.l6.m1.1a"><mo id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><ci id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">▷</annotation></semantics></math> Step 3: Group samples by cluster label

</div>
<div class="ltx_listingline" id="alg1.l7">  stream <math alttext="\leftarrow" class="ltx_Math" display="inline" id="alg1.l7.m1.1"><semantics id="alg1.l7.m1.1a"><mo id="alg1.l7.m1.1.1" stretchy="false" xref="alg1.l7.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><ci id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m1.1d">←</annotation></semantics></math> reservoir_sample(dataset)

</div>
<div class="ltx_listingline" id="alg1.l8">  <span class="ltx_text ltx_font_bold" id="alg1.l8.1">return</span> batch_processed dataset

</div>
<div class="ltx_listingline" id="alg1.l9">  <span class="ltx_text ltx_font_bold" id="alg1.l9.1">procedure</span> update_clusters(buffer, sample)

</div>
<div class="ltx_listingline" id="alg1.l10">  Add sample to buffer, and trim to update_window_size

</div>
<div class="ltx_listingline" id="alg1.l11">  <span class="ltx_text ltx_font_bold" id="alg1.l11.1">if</span> buffer.size() <math alttext="\geq" class="ltx_Math" display="inline" id="alg1.l11.m1.1"><semantics id="alg1.l11.m1.1a"><mo id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><geq id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m1.1d">≥</annotation></semantics></math> update_window_size <span class="ltx_text ltx_font_bold" id="alg1.l11.2">then</span>
</div>
<div class="ltx_listingline" id="alg1.l12">     Partially fit clusters to buffer

</div>
<div class="ltx_listingline" id="alg1.l13">  <span class="ltx_text ltx_font_bold" id="alg1.l13.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l13.2">if</span>
</div>
<div class="ltx_listingline" id="alg1.l14">  <span class="ltx_text ltx_font_bold" id="alg1.l14.1">return</span> buffer

</div>
<div class="ltx_listingline" id="alg1.l15">  <span class="ltx_text ltx_font_bold" id="alg1.l15.1">procedure</span> generate_labels(sample)

</div>
<div class="ltx_listingline" id="alg1.l16">  Use clusters to assign sample a cluster label

</div>
<div class="ltx_listingline" id="alg1.l17">  <span class="ltx_text ltx_font_bold" id="alg1.l17.1">return</span> {…sample, cluster_label}

</div>
<div class="ltx_listingline" id="alg1.l18">  <span class="ltx_text ltx_font_bold" id="alg1.l18.1">procedure</span> reservoir_sample(stream)

</div>
<div class="ltx_listingline" id="alg1.l19">  reservoir <math alttext="\leftarrow" class="ltx_Math" display="inline" id="alg1.l19.m1.1"><semantics id="alg1.l19.m1.1a"><mo id="alg1.l19.m1.1.1" stretchy="false" xref="alg1.l19.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l19.m1.1b"><ci id="alg1.l19.m1.1.1.cmml" xref="alg1.l19.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l19.m1.1c">\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg1.l19.m1.1d">←</annotation></semantics></math> empty list for each cluster

</div>
<div class="ltx_listingline" id="alg1.l20">  <span class="ltx_text ltx_font_bold" id="alg1.l20.1">while</span> stream.size() <math alttext="&gt;" class="ltx_Math" display="inline" id="alg1.l20.m1.1"><semantics id="alg1.l20.m1.1a"><mo id="alg1.l20.m1.1.1" xref="alg1.l20.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="alg1.l20.m1.1b"><gt id="alg1.l20.m1.1.1.cmml" xref="alg1.l20.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="alg1.l20.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="alg1.l20.m1.1d">&gt;</annotation></semantics></math> 0 <span class="ltx_text ltx_font_bold" id="alg1.l20.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l21">     sample <math alttext="\leftarrow" class="ltx_Math" display="inline" id="alg1.l21.m1.1"><semantics id="alg1.l21.m1.1a"><mo id="alg1.l21.m1.1.1" stretchy="false" xref="alg1.l21.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l21.m1.1b"><ci id="alg1.l21.m1.1.1.cmml" xref="alg1.l21.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m1.1c">\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg1.l21.m1.1d">←</annotation></semantics></math> stream.take()

</div>
<div class="ltx_listingline" id="alg1.l22">     <span class="ltx_text ltx_font_bold" id="alg1.l22.1">if</span> any reservoir[cluster].size() <math alttext="&gt;" class="ltx_Math" display="inline" id="alg1.l22.m1.1"><semantics id="alg1.l22.m1.1a"><mo id="alg1.l22.m1.1.1" xref="alg1.l22.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="alg1.l22.m1.1b"><gt id="alg1.l22.m1.1.1.cmml" xref="alg1.l22.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="alg1.l22.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="alg1.l22.m1.1d">&gt;</annotation></semantics></math> batch_size <span class="ltx_text ltx_font_bold" id="alg1.l22.2">then</span>
</div>
<div class="ltx_listingline" id="alg1.l23">        <span class="ltx_text ltx_font_bold" id="alg1.l23.1">yield from</span> reservoir[cluster]

</div>
<div class="ltx_listingline" id="alg1.l24">     <span class="ltx_text ltx_font_bold" id="alg1.l24.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l24.2">if</span>
</div>
<div class="ltx_listingline" id="alg1.l25">  <span class="ltx_text ltx_font_bold" id="alg1.l25.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l25.2">while</span>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Reformulation Up-Sampling</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">In addition to contrastive learning we further over-sample interactions containing reformulations during training of both the transducer and re-scorer. This approach can help to emphasize loss from reformulation samples, without introducing additional overhead. In our experiments, we empirically find an over-sampling ratio of 1:5 (1 reformulation to every 5 standard samples) to be effective (See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S3.T1" title="Table 1 ‣ 3.1.4 Reformulation Up-Sampling ‣ 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 1</span></a>).</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span> Word error rate reduction (WERR) on the <span class="ltx_text ltx_font_smallcaps" id="S3.T1.5.1">all</span> dataset when using different reformulation up-sampling rates.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.6.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_tt" id="S3.T1.6.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.6.1.1.1.1">
<span class="ltx_p" id="S3.T1.6.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.6.1.1.1.1.1.1" style="font-size:70%;">Rate</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.6.1.1.2.1" style="font-size:70%;">None</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.6.1.1.3.1" style="font-size:70%;">1:10</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.6.1.1.4.1" style="font-size:70%;">1:5</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.6.1.1.5.1" style="font-size:70%;">1:4</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T1.6.1.1.6.1" style="font-size:70%;">1:3</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T1.6.1.1.7.1" style="font-size:70%;">1:2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.1.1.8"><span class="ltx_text ltx_font_bold" id="S3.T1.6.1.1.8.1" style="font-size:70%;">1:1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.2.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T1.6.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.6.2.2.1.1">
<span class="ltx_p" id="S3.T1.6.2.2.1.1.1"><span class="ltx_text" id="S3.T1.6.2.2.1.1.1.1" style="font-size:70%;">WERR</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.6.2.2.2"><span class="ltx_text" id="S3.T1.6.2.2.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.6.2.2.3"><span class="ltx_text" id="S3.T1.6.2.2.3.1" style="font-size:70%;">0.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.6.2.2.4"><span class="ltx_text" id="S3.T1.6.2.2.4.1" style="font-size:70%;">0.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.6.2.2.5"><span class="ltx_text" id="S3.T1.6.2.2.5.1" style="font-size:70%;">0.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.6.2.2.6"><span class="ltx_text" id="S3.T1.6.2.2.6.1" style="font-size:70%;">0.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.6.2.2.7"><span class="ltx_text" id="S3.T1.6.2.2.7.1" style="font-size:70%;">0.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.6.2.2.8"><span class="ltx_text" id="S3.T1.6.2.2.8.1" style="font-size:70%;">0.24</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Student-Teacher Distillation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our overall system comprises both a student and a teacher ASR system. Our student model is a Conformer-based transducer network <cite class="ltx_cite ltx_citemacro_citep">(Gulati et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib21" title="">2020</a>)</cite> - a conventional streaming model i.e. it operates on single-utterance and attends to only past frames in the utterance. During run-time, governed by latency constraints, we use the student model to recognize user queries. These interactions (including reformulations and repeats) are captured – details on how to determine reformulations are described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S4.SS1" title="4.1 Datasets ‣ 4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">subsection 4.1</span></a>. Such interactions are then decoded using a context-aware teacher (discussed in the previous section) to get a recognition hypothesis, which acts as a label for semi-supervised training of student model <cite class="ltx_cite ltx_citemacro_citep">(Parthasarathi &amp; Strom, <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib46" title="">2019</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Design</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we discuss the details used when evaluating our approach on both real-world conversational data, and open semi-synthetic data.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">All our transducer models are first pre-trained on 200k+ hours of de-identified ASR data (<span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p1.1.1">pretrain</span>) using transducer loss without incorporating any contextual information. We then fine-tune and evaluate our approach on one of the two sources of data below: a closed-source real-world dataset from a conversational assistant, and the recently introduced OD3 dataset <cite class="ltx_cite ltx_citemacro_citep">(Chan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Closed Source Data</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">This dataset consists of de-identified dialogues constructed from real-world interactions with a voice assistant. These dialogues are each constructed around a seed utterance, which is human transcribed.
Given the seed utterance, the method pulls in all related conversations occurring 90 seconds before and after it. This step is iteratively applied, amassing additional conversational exchanges as they appear. If this approach yields over five utterances, the interval for gathering conversations is cut down to 45 seconds, and the process is repeated. This shortening of the interval persists until the number of utterances falls below five or the interval narrows to a 15-second minimum. These restrictions are enforced to ensure that utterances in a dialog are a semantically coherent interaction around the same request.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Mining dialogues with reformulations</h5>
<div class="ltx_para" id="S4.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.Px1.p1.1">To train and evaluate our system, we additionally detect a subset of dialogues consisting of reformulations. To detect reformulations, we use a text similarity-based approach. To be precise, we use cosine similarity and edit distance between the ASR hypothesis of the seed and context utterances (generated during the user’s interaction with the assistant). The dialog is said to contain a reformulation if any {seed, context} pair has a similarity greater than the threshold.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.Px1.p2.1">We <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.Px1.p2.1.1">train</span> our context encoder teacher models on 10M de-identified dialogues. Additionally, we upsample dialogues containing user reformulations, by 20%, during training of the transducer i.e. one in every five dialogues has reformulation. For <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.Px1.p2.1.2">evaluation</span>, we only select dialogues containing reformulations and ensure that all utterances in the dialog are human-transcribed. By focusing on dialogues where the user reformulated his query, we ensure that the selected dialog has significant ASR errors (as discussed in section <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S3.SS1.SSS2" title="3.1.2 Learning from Implicit Context ‣ 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">subsubsection 3.1.2</span></a>) and where contextual signals are expected to be meaningfully related to user queries. We create two datasets for evaluation (1) <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.Px1.p2.1.3">all</span>: All transcribed utterances across all validation dialogues (60K utterances) and (2) <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.Px1.p2.1.4">ref</span>: A subset of <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.Px1.p2.1.5">all</span> containing only utterances that lead to user reformulations of the query (8.5K utterances).</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.Px1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.Px1.p3.1">To <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.Px1.p3.1.1">distill</span> the knowledge of the teacher model to the student model, we use closed-source dialogues containing reformulations. Dialogues used for distillation are distinct from those used for training teacher models and we don’t require seed utterances to be human transcribed. Such dialogues are fed to a context-aware teacher model to obtain transcription for seed utterances. This single utterance audio-text pair (approximately 25,000 hours) is then used for training the student model. We refer to this as <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.Px1.p3.1.2">s</span>emi-<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.Px1.p3.1.3">s</span>upervised single utterance <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.Px1.p3.1.4">r</span>eformulation <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.Px1.p3.1.5">d</span>ataset (<span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.Px1.p3.1.6">ssrd</span>).</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>OD3</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Following <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite>, we further evaluate our models on the open directed dialogue dataset (OD3). The OD3 dataset is a semi-synthetic dataset, where human-generated task-oriented dialogues from several popular data sets are augmented with LLM-generated conversational errors and computer-generated TTS audio. OD3 contains 620K turns of audio (approximately 1,172 hours).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Details</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For our experiments, we use transducer architecture, with Conformer <cite class="ltx_cite ltx_citemacro_citep">(Gulati et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib21" title="">2020</a>)</cite> as the audio encoder. We experiment with two different teacher architectures: (i) 200M parameters: 17 conformer blocks and attention size of 1024 (ii) 1B parameters: 18 conformer blocks and attention size of 1536. Each conformer block is composed of four modules - multi-head attention and convolutional modules are sandwiched between two feed-forward modules. The convolutional module has a kernel size of 30 frames. Before conformer blocks, we use a pre-processing block consisting of two convolution layers, which takes in features at a 30ms frame rate, and has a kernel size of 5 and stride of 3.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Our student model has 1B parameters and consists of 18 conformer blocks with an attention size of 1536.
However, for student models we restrict the attention module to only attend to past frames - this ensures user-perceived latency is minimal. For the prediction network, we use a two-layer LSTM network with 1024 hidden dimensions and a vocabulary of 4,000 tokens.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Training details</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Our teacher model is pre-trained using the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.1">pretrain</span> dataset for 500K iterations, using a per-gpu batch size ranging from 32 to 1, depending on the length of the sequence (sequences are batched to maximally use GPU memory) across either 64 P100 GPUs (for 200M model) or 64 A100 GPUs (for 1B model). We pre-train using an Adam optimizer - we linearly increase the learning rate for 5000 steps and thereafter decrease it proportionally to the inverse square root of the step (as per schedule described in <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib60" title="">2017</a>)</cite>), and use magnitude-based gradient clipping with a value of <math alttext="10" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn id="S4.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">10</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.9">We then fine-tune our teacher models for 150k steps, using an Adam optimizer with gradient clipping, featuring a learning rate decay schedule that starts at <math alttext="1e^{-8}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><msup id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml"><mi id="S4.SS3.p2.1.m1.1.1.3.2" xref="S4.SS3.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.SS3.p2.1.m1.1.1.3.3" xref="S4.SS3.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS3.p2.1.m1.1.1.3.3a" xref="S4.SS3.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.1.m1.1.1.3.3.2" xref="S4.SS3.p2.1.m1.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn id="S4.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.2">1</cn><apply id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.3.1.cmml" xref="S4.SS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.3.2.cmml" xref="S4.SS3.p2.1.m1.1.1.3.2">𝑒</ci><apply id="S4.SS3.p2.1.m1.1.1.3.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3"><minus id="S4.SS3.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3"></minus><cn id="S4.SS3.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">1e^{-8}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">1 italic_e start_POSTSUPERSCRIPT - 8 end_POSTSUPERSCRIPT</annotation></semantics></math>, holds at <math alttext="1e^{-5}" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">1</mn><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">⁢</mo><msup id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml"><mi id="S4.SS3.p2.2.m2.1.1.3.2" xref="S4.SS3.p2.2.m2.1.1.3.2.cmml">e</mi><mrow id="S4.SS3.p2.2.m2.1.1.3.3" xref="S4.SS3.p2.2.m2.1.1.3.3.cmml"><mo id="S4.SS3.p2.2.m2.1.1.3.3a" xref="S4.SS3.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.2.m2.1.1.3.3.2" xref="S4.SS3.p2.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><times id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></times><cn id="S4.SS3.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.p2.2.m2.1.1.2">1</cn><apply id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.3.1.cmml" xref="S4.SS3.p2.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.3.2.cmml" xref="S4.SS3.p2.2.m2.1.1.3.2">𝑒</ci><apply id="S4.SS3.p2.2.m2.1.1.3.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3.3"><minus id="S4.SS3.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS3.p2.2.m2.1.1.3.3"></minus><cn id="S4.SS3.p2.2.m2.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p2.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">1e^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">1 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, and decays to <math alttext="1e^{-6}" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">1</mn><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">⁢</mo><msup id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml"><mi id="S4.SS3.p2.3.m3.1.1.3.2" xref="S4.SS3.p2.3.m3.1.1.3.2.cmml">e</mi><mrow id="S4.SS3.p2.3.m3.1.1.3.3" xref="S4.SS3.p2.3.m3.1.1.3.3.cmml"><mo id="S4.SS3.p2.3.m3.1.1.3.3a" xref="S4.SS3.p2.3.m3.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.3.m3.1.1.3.3.2" xref="S4.SS3.p2.3.m3.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><times id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"></times><cn id="S4.SS3.p2.3.m3.1.1.2.cmml" type="integer" xref="S4.SS3.p2.3.m3.1.1.2">1</cn><apply id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.3.m3.1.1.3.1.cmml" xref="S4.SS3.p2.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS3.p2.3.m3.1.1.3.2.cmml" xref="S4.SS3.p2.3.m3.1.1.3.2">𝑒</ci><apply id="S4.SS3.p2.3.m3.1.1.3.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3.3"><minus id="S4.SS3.p2.3.m3.1.1.3.3.1.cmml" xref="S4.SS3.p2.3.m3.1.1.3.3"></minus><cn id="S4.SS3.p2.3.m3.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p2.3.m3.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">1e^{-6}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">1 italic_e start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math>, with the clipping norm set to 0.3, and a schedule policy that adjusts the learning rate at 20K, 80K, and 600K training steps. In addition, we apply dynamic L2 regularization to the Multi-head self-attention layers of the conformer using a PiecewiseConstantDecay scheduler that increases the regularization factor at training steps 15K and 30K (calculated as <math alttext="2" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><cn id="S4.SS3.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p2.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">2</annotation></semantics></math> <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mo id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><times id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">×</annotation></semantics></math> number of conformer blocks <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS3.p2.6.m6.1"><semantics id="S4.SS3.p2.6.m6.1a"><mo id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><times id="S4.SS3.p2.6.m6.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.6.m6.1d">×</annotation></semantics></math> warmup steps), with the regularization values set to <math alttext="1e^{-6}" class="ltx_Math" display="inline" id="S4.SS3.p2.7.m7.1"><semantics id="S4.SS3.p2.7.m7.1a"><mrow id="S4.SS3.p2.7.m7.1.1" xref="S4.SS3.p2.7.m7.1.1.cmml"><mn id="S4.SS3.p2.7.m7.1.1.2" xref="S4.SS3.p2.7.m7.1.1.2.cmml">1</mn><mo id="S4.SS3.p2.7.m7.1.1.1" xref="S4.SS3.p2.7.m7.1.1.1.cmml">⁢</mo><msup id="S4.SS3.p2.7.m7.1.1.3" xref="S4.SS3.p2.7.m7.1.1.3.cmml"><mi id="S4.SS3.p2.7.m7.1.1.3.2" xref="S4.SS3.p2.7.m7.1.1.3.2.cmml">e</mi><mrow id="S4.SS3.p2.7.m7.1.1.3.3" xref="S4.SS3.p2.7.m7.1.1.3.3.cmml"><mo id="S4.SS3.p2.7.m7.1.1.3.3a" xref="S4.SS3.p2.7.m7.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.7.m7.1.1.3.3.2" xref="S4.SS3.p2.7.m7.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.m7.1b"><apply id="S4.SS3.p2.7.m7.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1"><times id="S4.SS3.p2.7.m7.1.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1.1"></times><cn id="S4.SS3.p2.7.m7.1.1.2.cmml" type="integer" xref="S4.SS3.p2.7.m7.1.1.2">1</cn><apply id="S4.SS3.p2.7.m7.1.1.3.cmml" xref="S4.SS3.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.7.m7.1.1.3.1.cmml" xref="S4.SS3.p2.7.m7.1.1.3">superscript</csymbol><ci id="S4.SS3.p2.7.m7.1.1.3.2.cmml" xref="S4.SS3.p2.7.m7.1.1.3.2">𝑒</ci><apply id="S4.SS3.p2.7.m7.1.1.3.3.cmml" xref="S4.SS3.p2.7.m7.1.1.3.3"><minus id="S4.SS3.p2.7.m7.1.1.3.3.1.cmml" xref="S4.SS3.p2.7.m7.1.1.3.3"></minus><cn id="S4.SS3.p2.7.m7.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p2.7.m7.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.m7.1c">1e^{-6}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.7.m7.1d">1 italic_e start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="5e^{-6}" class="ltx_Math" display="inline" id="S4.SS3.p2.8.m8.1"><semantics id="S4.SS3.p2.8.m8.1a"><mrow id="S4.SS3.p2.8.m8.1.1" xref="S4.SS3.p2.8.m8.1.1.cmml"><mn id="S4.SS3.p2.8.m8.1.1.2" xref="S4.SS3.p2.8.m8.1.1.2.cmml">5</mn><mo id="S4.SS3.p2.8.m8.1.1.1" xref="S4.SS3.p2.8.m8.1.1.1.cmml">⁢</mo><msup id="S4.SS3.p2.8.m8.1.1.3" xref="S4.SS3.p2.8.m8.1.1.3.cmml"><mi id="S4.SS3.p2.8.m8.1.1.3.2" xref="S4.SS3.p2.8.m8.1.1.3.2.cmml">e</mi><mrow id="S4.SS3.p2.8.m8.1.1.3.3" xref="S4.SS3.p2.8.m8.1.1.3.3.cmml"><mo id="S4.SS3.p2.8.m8.1.1.3.3a" xref="S4.SS3.p2.8.m8.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.8.m8.1.1.3.3.2" xref="S4.SS3.p2.8.m8.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.8.m8.1b"><apply id="S4.SS3.p2.8.m8.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1"><times id="S4.SS3.p2.8.m8.1.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1.1"></times><cn id="S4.SS3.p2.8.m8.1.1.2.cmml" type="integer" xref="S4.SS3.p2.8.m8.1.1.2">5</cn><apply id="S4.SS3.p2.8.m8.1.1.3.cmml" xref="S4.SS3.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.8.m8.1.1.3.1.cmml" xref="S4.SS3.p2.8.m8.1.1.3">superscript</csymbol><ci id="S4.SS3.p2.8.m8.1.1.3.2.cmml" xref="S4.SS3.p2.8.m8.1.1.3.2">𝑒</ci><apply id="S4.SS3.p2.8.m8.1.1.3.3.cmml" xref="S4.SS3.p2.8.m8.1.1.3.3"><minus id="S4.SS3.p2.8.m8.1.1.3.3.1.cmml" xref="S4.SS3.p2.8.m8.1.1.3.3"></minus><cn id="S4.SS3.p2.8.m8.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p2.8.m8.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.8.m8.1c">5e^{-6}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.8.m8.1d">5 italic_e start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="1e^{-5}" class="ltx_Math" display="inline" id="S4.SS3.p2.9.m9.1"><semantics id="S4.SS3.p2.9.m9.1a"><mrow id="S4.SS3.p2.9.m9.1.1" xref="S4.SS3.p2.9.m9.1.1.cmml"><mn id="S4.SS3.p2.9.m9.1.1.2" xref="S4.SS3.p2.9.m9.1.1.2.cmml">1</mn><mo id="S4.SS3.p2.9.m9.1.1.1" xref="S4.SS3.p2.9.m9.1.1.1.cmml">⁢</mo><msup id="S4.SS3.p2.9.m9.1.1.3" xref="S4.SS3.p2.9.m9.1.1.3.cmml"><mi id="S4.SS3.p2.9.m9.1.1.3.2" xref="S4.SS3.p2.9.m9.1.1.3.2.cmml">e</mi><mrow id="S4.SS3.p2.9.m9.1.1.3.3" xref="S4.SS3.p2.9.m9.1.1.3.3.cmml"><mo id="S4.SS3.p2.9.m9.1.1.3.3a" xref="S4.SS3.p2.9.m9.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.9.m9.1.1.3.3.2" xref="S4.SS3.p2.9.m9.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.9.m9.1b"><apply id="S4.SS3.p2.9.m9.1.1.cmml" xref="S4.SS3.p2.9.m9.1.1"><times id="S4.SS3.p2.9.m9.1.1.1.cmml" xref="S4.SS3.p2.9.m9.1.1.1"></times><cn id="S4.SS3.p2.9.m9.1.1.2.cmml" type="integer" xref="S4.SS3.p2.9.m9.1.1.2">1</cn><apply id="S4.SS3.p2.9.m9.1.1.3.cmml" xref="S4.SS3.p2.9.m9.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.9.m9.1.1.3.1.cmml" xref="S4.SS3.p2.9.m9.1.1.3">superscript</csymbol><ci id="S4.SS3.p2.9.m9.1.1.3.2.cmml" xref="S4.SS3.p2.9.m9.1.1.3.2">𝑒</ci><apply id="S4.SS3.p2.9.m9.1.1.3.3.cmml" xref="S4.SS3.p2.9.m9.1.1.3.3"><minus id="S4.SS3.p2.9.m9.1.1.3.3.1.cmml" xref="S4.SS3.p2.9.m9.1.1.3.3"></minus><cn id="S4.SS3.p2.9.m9.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p2.9.m9.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.9.m9.1c">1e^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.9.m9.1d">1 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> at these intervals.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.3">For models trained with contrastive learning, we use a set of <math alttext="32" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mn id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><cn id="S4.SS3.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p3.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">32</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">32</annotation></semantics></math> learned clusters for hard-negative mining, with a buffer size of <math alttext="4096" class="ltx_Math" display="inline" id="S4.SS3.p3.2.m2.1"><semantics id="S4.SS3.p3.2.m2.1a"><mn id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><cn id="S4.SS3.p3.2.m2.1.1.cmml" type="integer" xref="S4.SS3.p3.2.m2.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">4096</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.2.m2.1d">4096</annotation></semantics></math> for the online reservoir sampling. We leverage the BIRCH <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib68" title="">1997</a>)</cite> clustering algorithm, over the embeddings of <math alttext="X^{p}" class="ltx_Math" display="inline" id="S4.SS3.p3.3.m3.1"><semantics id="S4.SS3.p3.3.m3.1a"><msup id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mi id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml">X</mi><mi id="S4.SS3.p3.3.m3.1.1.3" xref="S4.SS3.p3.3.m3.1.1.3.cmml">p</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">superscript</csymbol><ci id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2">𝑋</ci><ci id="S4.SS3.p3.3.m3.1.1.3.cmml" xref="S4.SS3.p3.3.m3.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">X^{p}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.3.m3.1d">italic_X start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>. In the future, we intend to explore additional clustering algorithms and leverage better distance functions for the Ohm mining approach. For the hyper-parameters of the PF-CLC loss, we follow the parameters in <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite>.
Our student model is pre-trained using <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p3.3.1">pretrain</span> dataset for 400K iterations with 64 A100 GPUs and a per-gpu batch size ranging from 128 to 1. For model distillation, we use both <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p3.3.2">ssrd</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p3.3.3">pretrain</span> data, sampled at differing ratios, and standard ASR transducer loss.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.2">As discussed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S4" title="4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">section 4</span></a>, we evaluate our system on both closed-source data and the OD3 dataset. In general, we use both standard word error rate (WER, <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" stretchy="false" xref="S5.p1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">↓</annotation></semantics></math>) and relative word error rate improvement (WERR, <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mo id="S5.p1.2.m2.1.1" stretchy="false" xref="S5.p1.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">↑</annotation></semantics></math>) to evaluate our system.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Teacher Performance</h3>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2: </span> WER improvements on the <span class="ltx_text ltx_font_smallcaps" id="S5.T2.8.1">all</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.T2.9.2">ref</span> datasets for the teacher model. LE: Learned Embeddings, AE: Audio Embeddings, FC: Feature Concatenation, WERR: Word Error Rate Reduction. </figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.1" style="font-size:70%;">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.3.1" style="font-size:70%;">Audio</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.1" style="font-size:70%;">Text</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S5.T2.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.5.1">
<span class="ltx_p" id="S5.T2.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.5.1.1.1" style="font-size:70%;">CLC +</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1" style="font-size:70%;">WERR (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_left" id="S5.T2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.1.1" style="font-size:70%;">Context</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.2.1" style="font-size:70%;">Context</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.1.3.1">
<span class="ltx_p" id="S5.T2.1.2.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.3.1.1.1" style="font-size:70%;">Ohm</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.2.1.4"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.2.1.4.1" style="font-size:70%;">all</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.2.1.5"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.2.1.5.1" style="font-size:70%;">ref</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.3.2.1"><span class="ltx_text" id="S5.T2.1.3.2.1.1" style="font-size:70%;">Baseline (200M)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.3.2.2"><span class="ltx_text" id="S5.T2.1.3.2.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.3.2.3"><span class="ltx_text" id="S5.T2.1.3.2.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.2.4.1">
<span class="ltx_p" id="S5.T2.1.3.2.4.1.1"><span class="ltx_text" id="S5.T2.1.3.2.4.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.5"><span class="ltx_text" id="S5.T2.1.3.2.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.6"><span class="ltx_text" id="S5.T2.1.3.2.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.1" rowspan="8"><span class="ltx_text" id="S5.T2.1.4.3.1.1" style="font-size:70%;">Teacher (200M)</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.2"><span class="ltx_text" id="S5.T2.1.4.3.2.1" style="font-size:70%;">FC</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.3"><span class="ltx_text" id="S5.T2.1.4.3.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.3.4.1">
<span class="ltx_p" id="S5.T2.1.4.3.4.1.1"><span class="ltx_text" id="S5.T2.1.4.3.4.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.5"><span class="ltx_text" id="S5.T2.1.4.3.5.1" style="font-size:70%;">3.73</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.6"><span class="ltx_text" id="S5.T2.1.4.3.6.1" style="font-size:70%;">7.04</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="S5.T2.1.5.4.1"><span class="ltx_text" id="S5.T2.1.5.4.1.1" style="font-size:70%;">AE/HuBERT</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.5.4.2"><span class="ltx_text" id="S5.T2.1.5.4.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.4.3.1">
<span class="ltx_p" id="S5.T2.1.5.4.3.1.1"><span class="ltx_text" id="S5.T2.1.5.4.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.4"><span class="ltx_text" id="S5.T2.1.5.4.4.1" style="font-size:70%;">1.94</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.4.5"><span class="ltx_text" id="S5.T2.1.5.4.5.1" style="font-size:70%;">4.04</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<td class="ltx_td ltx_align_left" id="S5.T2.1.6.5.1"><span class="ltx_text" id="S5.T2.1.6.5.1.1" style="font-size:70%;">AE/Transducer</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.6.5.2"><span class="ltx_text" id="S5.T2.1.6.5.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.5.3.1">
<span class="ltx_p" id="S5.T2.1.6.5.3.1.1"><span class="ltx_text" id="S5.T2.1.6.5.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.4"><span class="ltx_text" id="S5.T2.1.6.5.4.1" style="font-size:70%;">2.77</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.5"><span class="ltx_text" id="S5.T2.1.6.5.5.1" style="font-size:70%;">4.97</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.6">
<td class="ltx_td ltx_align_left" id="S5.T2.1.7.6.1"><span class="ltx_text" id="S5.T2.1.7.6.1.1" style="font-size:70%;">AE/Transducer</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.7.6.2"><span class="ltx_text" id="S5.T2.1.7.6.2.1" style="font-size:70%;">LE</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.6.3.1">
<span class="ltx_p" id="S5.T2.1.7.6.3.1.1"><span class="ltx_text" id="S5.T2.1.7.6.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.6.4"><span class="ltx_text" id="S5.T2.1.7.6.4.1" style="font-size:70%;">3.32</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.6.5"><span class="ltx_text" id="S5.T2.1.7.6.5.1" style="font-size:70%;">6.70</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.7">
<td class="ltx_td ltx_align_left" id="S5.T2.1.8.7.1"><span class="ltx_text" id="S5.T2.1.8.7.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.8.7.2"><span class="ltx_text" id="S5.T2.1.8.7.2.1" style="font-size:70%;">BERT</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.8.7.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.7.3.1">
<span class="ltx_p" id="S5.T2.1.8.7.3.1.1"><span class="ltx_text" id="S5.T2.1.8.7.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.8.7.4"><span class="ltx_text" id="S5.T2.1.8.7.4.1" style="font-size:70%;">0.69</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.8.7.5"><span class="ltx_text" id="S5.T2.1.8.7.5.1" style="font-size:70%;">2.66</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="S5.T2.1.9.8.1"><span class="ltx_text" id="S5.T2.1.9.8.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.9.8.2"><span class="ltx_text" id="S5.T2.1.9.8.2.1" style="font-size:70%;">LE</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.9.8.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.8.3.1">
<span class="ltx_p" id="S5.T2.1.9.8.3.1.1"><span class="ltx_text" id="S5.T2.1.9.8.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.8.4"><span class="ltx_text" id="S5.T2.1.9.8.4.1" style="font-size:70%;">2.63</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.8.5"><span class="ltx_text" id="S5.T2.1.9.8.5.1" style="font-size:70%;">5.66</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10.9">
<td class="ltx_td ltx_align_left" id="S5.T2.1.10.9.1"><span class="ltx_text" id="S5.T2.1.10.9.1.1" style="font-size:70%;">FC</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.10.9.2"><span class="ltx_text" id="S5.T2.1.10.9.2.1" style="font-size:70%;">LE</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.10.9.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.9.3.1">
<span class="ltx_p" id="S5.T2.1.10.9.3.1.1"><span class="ltx_text" id="S5.T2.1.10.9.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.10.9.4"><span class="ltx_text" id="S5.T2.1.10.9.4.1" style="font-size:70%;">4.70</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.10.9.5"><span class="ltx_text" id="S5.T2.1.10.9.5.1" style="font-size:70%;">8.08</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.11.10">
<td class="ltx_td ltx_align_left" id="S5.T2.1.11.10.1"><span class="ltx_text" id="S5.T2.1.11.10.1.1" style="font-size:70%;">FC</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.11.10.2"><span class="ltx_text" id="S5.T2.1.11.10.2.1" style="font-size:70%;">LE</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.11.10.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.10.3.1">
<span class="ltx_p" id="S5.T2.1.11.10.3.1.1"><span class="ltx_text" id="S5.T2.1.11.10.3.1.1.1" style="font-size:50%;">✓</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.11.10.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.11.10.4.1" style="font-size:70%;">6.91</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.11.10.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.11.10.5.1" style="font-size:70%;">9.58</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T2.1.12.11.1" rowspan="2"><span class="ltx_text" id="S5.T2.1.12.11.1.1" style="font-size:70%;">Teacher (1B)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.12.11.2"><span class="ltx_text" id="S5.T2.1.12.11.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.12.11.3"><span class="ltx_text" id="S5.T2.1.12.11.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.12.11.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.11.4.1">
<span class="ltx_p" id="S5.T2.1.12.11.4.1.1"><span class="ltx_text" id="S5.T2.1.12.11.4.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.12.11.5"><span class="ltx_text" id="S5.T2.1.12.11.5.1" style="font-size:70%;">0.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.12.11.6"><span class="ltx_text" id="S5.T2.1.12.11.6.1" style="font-size:70%;">2.89</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.13.12.1"><span class="ltx_text" id="S5.T2.1.13.12.1.1" style="font-size:70%;">FC</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.13.12.2"><span class="ltx_text" id="S5.T2.1.13.12.2.1" style="font-size:70%;">LE</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T2.1.13.12.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.12.3.1">
<span class="ltx_p" id="S5.T2.1.13.12.3.1.1"><span class="ltx_text" id="S5.T2.1.13.12.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.13.12.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.12.4.1" style="font-size:70%;">5.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.13.12.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.12.5.1" style="font-size:70%;">9.24</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 3: </span> Results on OD3 (overall and repeat/rephrase inducing) using the 200M model. WER (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.3.m1.1"><semantics id="S5.T3.3.m1.1b"><mo id="S5.T3.3.m1.1.1" stretchy="false" xref="S5.T3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.m1.1c"><ci id="S5.T3.3.m1.1.1.cmml" xref="S5.T3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.m1.1e">↓</annotation></semantics></math>): Word Error Rate, BERT-S (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.4.m2.1"><semantics id="S5.T3.4.m2.1b"><mo id="S5.T3.4.m2.1.1" stretchy="false" xref="S5.T3.4.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.m2.1c"><ci id="S5.T3.4.m2.1.1.cmml" xref="S5.T3.4.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.m2.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.m2.1e">↑</annotation></semantics></math>): Bert Score. B: Basline, CX: Context, C: CLC Loss, O: Ohm.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.9.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.9.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.9.1.1.1.1">
<span class="ltx_p" id="S5.T3.9.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.9.1.1.1.1.1.1" style="font-size:70%;">Model</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T3.9.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.9.1.1.2.1" style="font-size:70%;">Overall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T3.9.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.9.1.1.3.1" style="font-size:70%;">Repeat/Rephrase</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.9.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S5.T3.9.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.9.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.9.2.2.2.1" style="font-size:70%;">WER (WERR)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.9.2.2.3"><span class="ltx_text ltx_font_bold" id="S5.T3.9.2.2.3.1" style="font-size:70%;">BERT-S</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.9.2.2.4"><span class="ltx_text ltx_font_bold" id="S5.T3.9.2.2.4.1" style="font-size:70%;">WER (WERR)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.9.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.9.2.2.5.1" style="font-size:70%;">BERT-S</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.9.3.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" id="S5.T3.9.3.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.9.3.1.1.1">
<span class="ltx_p" id="S5.T3.9.3.1.1.1.1"><span class="ltx_text" id="S5.T3.9.3.1.1.1.1.1" style="font-size:70%;">B</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.9.3.1.2"><span class="ltx_text" id="S5.T3.9.3.1.2.1" style="font-size:70%;">11.90 (-)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.9.3.1.3"><span class="ltx_text" id="S5.T3.9.3.1.3.1" style="font-size:70%;">0.9711</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.9.3.1.4"><span class="ltx_text" id="S5.T3.9.3.1.4.1" style="font-size:70%;">19.09 (-)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.9.3.1.5"><span class="ltx_text" id="S5.T3.9.3.1.5.1" style="font-size:70%;">0.9402</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.9.4.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="S5.T3.9.4.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.9.4.2.1.1">
<span class="ltx_p" id="S5.T3.9.4.2.1.1.1"><span class="ltx_text" id="S5.T3.9.4.2.1.1.1.1" style="font-size:70%;">B/CX</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.9.4.2.2"><span class="ltx_text" id="S5.T3.9.4.2.2.1" style="font-size:70%;">11.13 (6.47%)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.4.2.3"><span class="ltx_text" id="S5.T3.9.4.2.3.1" style="font-size:70%;">0.9762</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.4.2.4"><span class="ltx_text" id="S5.T3.9.4.2.4.1" style="font-size:70%;">16.17 (15.29%)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.4.2.5"><span class="ltx_text" id="S5.T3.9.4.2.5.1" style="font-size:70%;">0.9690</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.9.5.3">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="S5.T3.9.5.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.9.5.3.1.1">
<span class="ltx_p" id="S5.T3.9.5.3.1.1.1"><span class="ltx_text" id="S5.T3.9.5.3.1.1.1.1" style="font-size:70%;">B/CX/C</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.9.5.3.2"><span class="ltx_text" id="S5.T3.9.5.3.2.1" style="font-size:70%;">8.99 (24.4%)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.5.3.3"><span class="ltx_text" id="S5.T3.9.5.3.3.1" style="font-size:70%;">0.9812</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.5.3.4"><span class="ltx_text" id="S5.T3.9.5.3.4.1" style="font-size:70%;">13.81 (27.65%)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.5.3.5"><span class="ltx_text" id="S5.T3.9.5.3.5.1" style="font-size:70%;">0.9737</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.9.6.4">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb" id="S5.T3.9.6.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.9.6.4.1.1">
<span class="ltx_p" id="S5.T3.9.6.4.1.1.1"><span class="ltx_text" id="S5.T3.9.6.4.1.1.1.1" style="font-size:70%;">B/CX/C/O</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.9.6.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.9.6.4.2.1" style="font-size:70%;">8.73 (26.2%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.9.6.4.3"><span class="ltx_text ltx_font_bold" id="S5.T3.9.6.4.3.1" style="font-size:70%;">0.9817</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.9.6.4.4"><span class="ltx_text ltx_font_bold" id="S5.T3.9.6.4.4.1" style="font-size:70%;">13.21 (30.8%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.9.6.4.5"><span class="ltx_text ltx_font_bold" id="S5.T3.9.6.4.5.1" style="font-size:70%;">0.9771</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 4: </span> Zero-shot results on OD3 for several open-source models - Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib50" title="">2023</a>)</cite>, Conformer <cite class="ltx_cite ltx_citemacro_citep">(Gulati et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib21" title="">2020</a>)</cite>, Wav2Vec 2 <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib2" title="">2020</a>)</cite>, Streaming Conformer <cite class="ltx_cite ltx_citemacro_citep">(Tsunoo et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib59" title="">2021</a>)</cite>, CLC <cite class="ltx_cite ltx_citemacro_citep">(Chan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib8" title="">2024</a>)</cite>. Models in this table are not directly comparable (trained on differing data, setups, hyperparameters, optimizers etc.), but serve as a benchmark for performance on OD3 under several varying setups. WER (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.3.m1.1"><semantics id="S5.T4.3.m1.1b"><mo id="S5.T4.3.m1.1.1" stretchy="false" xref="S5.T4.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.m1.1c"><ci id="S5.T4.3.m1.1.1.cmml" xref="S5.T4.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.m1.1e">↓</annotation></semantics></math>): Word Error Rate, BERT-S (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.4.m2.1"><semantics id="S5.T4.4.m2.1b"><mo id="S5.T4.4.m2.1.1" stretchy="false" xref="S5.T4.4.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.m2.1c"><ci id="S5.T4.4.m2.1.1.cmml" xref="S5.T4.4.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.m2.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.m2.1e">↑</annotation></semantics></math>): Bert Score</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.29">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.29.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.29.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.29.1.1.1.1" style="font-size:70%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.29.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.29.1.1.2.1" style="font-size:70%;">Overall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.29.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.29.1.1.3.1" style="font-size:70%;">Repeat/Rephrase</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.29.2.1">
<td class="ltx_td" id="S5.T4.29.2.1.1"></td>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column" id="S5.T4.29.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.2.1.2.1">
<span class="ltx_p" id="S5.T4.29.2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.29.2.1.2.1.1.1" style="font-size:70%;">WER</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.29.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.29.2.1.3.1" style="font-size:70%;">BERT-S</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column" id="S5.T4.29.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.2.1.4.1">
<span class="ltx_p" id="S5.T4.29.2.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.29.2.1.4.1.1.1" style="font-size:70%;">WER</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.29.2.1.5"><span class="ltx_text ltx_font_bold" id="S5.T4.29.2.1.5.1" style="font-size:70%;">BERT-S</span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.29.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.29.3.2.1"><span class="ltx_text" id="S5.T4.29.3.2.1.1" style="font-size:70%;">Whisper S (200M)</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T4.29.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.3.2.2.1">
<span class="ltx_p" id="S5.T4.29.3.2.2.1.1"><span class="ltx_text" id="S5.T4.29.3.2.2.1.1.1" style="font-size:70%;">11.24</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.29.3.2.3"><span class="ltx_text" id="S5.T4.29.3.2.3.1" style="font-size:70%;">0.9775</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T4.29.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.3.2.4.1">
<span class="ltx_p" id="S5.T4.29.3.2.4.1.1"><span class="ltx_text" id="S5.T4.29.3.2.4.1.1.1" style="font-size:70%;">14.17</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.29.3.2.5"><span class="ltx_text" id="S5.T4.29.3.2.5.1" style="font-size:70%;">0.9727</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.29.4.3">
<td class="ltx_td ltx_align_left" id="S5.T4.29.4.3.1"><span class="ltx_text" id="S5.T4.29.4.3.1.1" style="font-size:70%;">Whisper L (1.3B)</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.4.3.2.1">
<span class="ltx_p" id="S5.T4.29.4.3.2.1.1"><span class="ltx_text" id="S5.T4.29.4.3.2.1.1.1" style="font-size:70%;">8.51</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.4.3.3"><span class="ltx_text" id="S5.T4.29.4.3.3.1" style="font-size:70%;">0.9852</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.4.3.4.1">
<span class="ltx_p" id="S5.T4.29.4.3.4.1.1"><span class="ltx_text" id="S5.T4.29.4.3.4.1.1.1" style="font-size:70%;">12.37</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.4.3.5"><span class="ltx_text" id="S5.T4.29.4.3.5.1" style="font-size:70%;">0.9792</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.29.5.4">
<td class="ltx_td ltx_align_left" id="S5.T4.29.5.4.1"><span class="ltx_text" id="S5.T4.29.5.4.1.1" style="font-size:70%;">Conformer (100M, Librispeech)</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.5.4.2.1">
<span class="ltx_p" id="S5.T4.29.5.4.2.1.1"><span class="ltx_text" id="S5.T4.29.5.4.2.1.1.1" style="font-size:70%;">19.26</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.5.4.3"><span class="ltx_text" id="S5.T4.29.5.4.3.1" style="font-size:70%;">0.9612</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.5.4.4.1">
<span class="ltx_p" id="S5.T4.29.5.4.4.1.1"><span class="ltx_text" id="S5.T4.29.5.4.4.1.1.1" style="font-size:70%;">22.19</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.5.4.5"><span class="ltx_text" id="S5.T4.29.5.4.5.1" style="font-size:70%;">0.9571</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.29.6.5">
<td class="ltx_td ltx_align_left" id="S5.T4.29.6.5.1"><span class="ltx_text" id="S5.T4.29.6.5.1.1" style="font-size:70%;">Wav2Vec 2 (433M, Librispeech)</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.6.5.2.1">
<span class="ltx_p" id="S5.T4.29.6.5.2.1.1"><span class="ltx_text" id="S5.T4.29.6.5.2.1.1.1" style="font-size:70%;">19.41</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.6.5.3"><span class="ltx_text" id="S5.T4.29.6.5.3.1" style="font-size:70%;">0.9582</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.6.5.4.1">
<span class="ltx_p" id="S5.T4.29.6.5.4.1.1"><span class="ltx_text" id="S5.T4.29.6.5.4.1.1.1" style="font-size:70%;">22.03</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.6.5.5"><span class="ltx_text" id="S5.T4.29.6.5.5.1" style="font-size:70%;">0.9544</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.29.7.6">
<td class="ltx_td ltx_align_left" id="S5.T4.29.7.6.1"><span class="ltx_text" id="S5.T4.29.7.6.1.1" style="font-size:70%;">Streaming Conformer (45M)</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.7.6.2.1">
<span class="ltx_p" id="S5.T4.29.7.6.2.1.1"><span class="ltx_text" id="S5.T4.29.7.6.2.1.1.1" style="font-size:70%;">14.38</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.7.6.3"><span class="ltx_text" id="S5.T4.29.7.6.3.1" style="font-size:70%;">0.9701</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.7.6.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.7.6.4.1">
<span class="ltx_p" id="S5.T4.29.7.6.4.1.1"><span class="ltx_text" id="S5.T4.29.7.6.4.1.1.1" style="font-size:70%;">16.70</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.7.6.5"><span class="ltx_text" id="S5.T4.29.7.6.5.1" style="font-size:70%;">0.9665</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.29.8.7">
<td class="ltx_td ltx_align_left" id="S5.T4.29.8.7.1"><span class="ltx_text" id="S5.T4.29.8.7.1.1" style="font-size:70%;">CLC (200M)</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.8.7.2.1">
<span class="ltx_p" id="S5.T4.29.8.7.2.1.1"><span class="ltx_text" id="S5.T4.29.8.7.2.1.1.1" style="font-size:70%;">8.99</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.8.7.3"><span class="ltx_text" id="S5.T4.29.8.7.3.1" style="font-size:70%;">0.9812</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T4.29.8.7.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.8.7.4.1">
<span class="ltx_p" id="S5.T4.29.8.7.4.1.1"><span class="ltx_text" id="S5.T4.29.8.7.4.1.1.1" style="font-size:70%;">13.81</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T4.29.8.7.5"><span class="ltx_text" id="S5.T4.29.8.7.5.1" style="font-size:70%;">0.9737</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.29.9.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.29.9.8.1"><span class="ltx_text" id="S5.T4.29.9.8.1.1" style="font-size:70%;">Our model (200M)</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T4.29.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.9.8.2.1">
<span class="ltx_p" id="S5.T4.29.9.8.2.1.1"><span class="ltx_text" id="S5.T4.29.9.8.2.1.1.1" style="font-size:70%;">8.73</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.29.9.8.3"><span class="ltx_text" id="S5.T4.29.9.8.3.1" style="font-size:70%;">0.9817</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T4.29.9.8.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.29.9.8.4.1">
<span class="ltx_p" id="S5.T4.29.9.8.4.1.1"><span class="ltx_text" id="S5.T4.29.9.8.4.1.1.1" style="font-size:70%;">13.21</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.29.9.8.5"><span class="ltx_text" id="S5.T4.29.9.8.5.1" style="font-size:70%;">0.9771</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Our overall results for the teacher model on the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.1.1">all</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.1.2">ref</span> are given in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T2" title="Table 2 ‣ 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 2</span></a>. We can see that our system, combining the feature-concatenation audio context (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S3.SS1" title="3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">subsection 3.1</span></a>), learned text context (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S3.SS1" title="3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">subsection 3.1</span></a>), and CLC/Ohm losses (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S3.SS1.SSS2" title="3.1.2 Learning from Implicit Context ‣ 3.1 Teacher Model ‣ 3 Self-Learning for Dialogue ASR ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">subsubsection 3.1.2</span></a>), outperforms the baseline model by up to 9.58% on the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.1.3">ref</span> and up to 6.91% on the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.1.4">all</span> dataset. These trends hold across model sizes, as our context-enabled model has similar improvement in both the 200M and 1B cases, implying such improvements are model-size independent. Note that for ASR models, 1B parameters is generally considered quite large, given the challenging latency and run-time requirements for ASR applications.</p>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Impact of audio context:</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.2">Both the approaches of incorporating audio context (feature concatenation and audio embeddings) improve over a baseline system that doesn’t use contextual signals. Improvements due to feature concatenation are larger (<math alttext="&gt;7\%" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml"></mi><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">&gt;</mo><mrow id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">7</mn><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1"><gt id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2">absent</csymbol><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.1">percent</csymbol><cn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.2">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">&gt;7\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.1.m1.1d">&gt; 7 %</annotation></semantics></math> on the reformulation test set), which is not unexpected; by concatenating features we allow the model to attend to all context frames as necessary, as opposed to attending to summary vectors (<math alttext="N=8" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S5.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">N</mi><mo id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1"><eq id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1"></eq><ci id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝑁</ci><cn id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.2.m2.1c">N=8</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.2.m2.1d">italic_N = 8</annotation></semantics></math>, in our experiments) coming from multi-headed attentive pooling. Among the two approaches of extracting audio embeddings, we find using a transducer encoder as a context encoder is marginally better, likely as the embeddings are “on-policy” for the trained model, as opposed to coming from an external embedding model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Impact of text context:</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">In general, we find that encoding text via learned embeddings as opposed to summary vector by BERT encoder is more beneficial. This is aligned with the observation made above for audio context embeddings - again, likely because the latent space is “on-policy” and trained specifically on in-domain data. We also see that textual context under-performs audio context, likely due to incorrect text content from the teacher model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Combining Context Types:</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">We get the best performance when both audio and text contexts are combined (compared to adding two modalities individually). Interestingly, the 200M model with context is significantly better than the 1B model without contextual signals, highlighting the efficacy of our proposed approach in modeling implicit context signals. On OD3, we can see that adding both context types leads to a 6.47% performance improvement, tracking similarly to the performance improvements seen in the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px3.p1.1.1">all</span> dataset.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 5: </span> Ablation experiments on the teacher model (200M). WERR: Word Error Rate Reduction.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T5.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.2.1" style="font-size:70%;">Context Type</span></td>
<td class="ltx_td ltx_border_tt" id="S5.T5.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T5.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1" style="font-size:70%;">WERR (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.1">
<td class="ltx_td ltx_align_center" id="S5.T5.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.1.1.1" style="font-size:70%;">Past</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.1.2.1" style="font-size:70%;">Future</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.1.3.1" style="font-size:70%;">Audio</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.2.1.4.1" style="font-size:70%;">Text</span></td>
<td class="ltx_td" id="S5.T5.1.2.1.5"></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.2.1.6"><span class="ltx_text ltx_font_smallcaps" id="S5.T5.1.2.1.6.1" style="font-size:70%;">all</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.2.1.7"><span class="ltx_text ltx_font_smallcaps" id="S5.T5.1.2.1.7.1" style="font-size:70%;">ref</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.2.1"><span class="ltx_text" id="S5.T5.1.3.2.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.2.2"><span class="ltx_text" id="S5.T5.1.3.2.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.2.3"><span class="ltx_text" id="S5.T5.1.3.2.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.3.2.4"><span class="ltx_text" id="S5.T5.1.3.2.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_border_t" id="S5.T5.1.3.2.5"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.3.2.6"><span class="ltx_text" id="S5.T5.1.3.2.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.3.2.7"><span class="ltx_text" id="S5.T5.1.3.2.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.3">
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.3.1"><span class="ltx_text" id="S5.T5.1.4.3.1.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.3.2"><span class="ltx_text" id="S5.T5.1.4.3.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.3.3"><span class="ltx_text" id="S5.T5.1.4.3.3.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.3.4"><span class="ltx_text" id="S5.T5.1.4.3.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td" id="S5.T5.1.4.3.5"></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.4.3.6"><span class="ltx_text" id="S5.T5.1.4.3.6.1" style="font-size:70%;">1.52</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.4.3.7"><span class="ltx_text" id="S5.T5.1.4.3.7.1" style="font-size:70%;">3.35</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.4">
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.4.1"><span class="ltx_text" id="S5.T5.1.5.4.1.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.4.2"><span class="ltx_text" id="S5.T5.1.5.4.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.4.3"><span class="ltx_text" id="S5.T5.1.5.4.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.4.4"><span class="ltx_text" id="S5.T5.1.5.4.4.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td" id="S5.T5.1.5.4.5"></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.5.4.6"><span class="ltx_text" id="S5.T5.1.5.4.6.1" style="font-size:70%;">1.24</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.5.4.7"><span class="ltx_text" id="S5.T5.1.5.4.7.1" style="font-size:70%;">1.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.5">
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.5.1"><span class="ltx_text" id="S5.T5.1.6.5.1.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.5.2"><span class="ltx_text" id="S5.T5.1.6.5.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.5.3"><span class="ltx_text" id="S5.T5.1.6.5.3.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.5.4"><span class="ltx_text" id="S5.T5.1.6.5.4.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td" id="S5.T5.1.6.5.5"></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.6.5.6"><span class="ltx_text" id="S5.T5.1.6.5.6.1" style="font-size:70%;">2.90</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.6.5.7"><span class="ltx_text" id="S5.T5.1.6.5.7.1" style="font-size:70%;">5.08</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.6">
<td class="ltx_td ltx_align_center" id="S5.T5.1.7.6.1"><span class="ltx_text" id="S5.T5.1.7.6.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.7.6.2"><span class="ltx_text" id="S5.T5.1.7.6.2.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.7.6.3"><span class="ltx_text" id="S5.T5.1.7.6.3.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.7.6.4"><span class="ltx_text" id="S5.T5.1.7.6.4.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td" id="S5.T5.1.7.6.5"></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.7.6.6"><span class="ltx_text" id="S5.T5.1.7.6.6.1" style="font-size:70%;">3.60</span></td>
<td class="ltx_td ltx_align_right" id="S5.T5.1.7.6.7"><span class="ltx_text" id="S5.T5.1.7.6.7.1" style="font-size:70%;">7.39</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.8.7.1"><span class="ltx_text" id="S5.T5.1.8.7.1.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.8.7.2"><span class="ltx_text" id="S5.T5.1.8.7.2.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.8.7.3"><span class="ltx_text" id="S5.T5.1.8.7.3.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.8.7.4"><span class="ltx_text" id="S5.T5.1.8.7.4.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_border_bb" id="S5.T5.1.8.7.5"></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T5.1.8.7.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.8.7.6.1" style="font-size:70%;">4.70</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T5.1.8.7.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.8.7.7.1" style="font-size:70%;">8.08</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 6: </span> WER improvements on the <span class="ltx_text ltx_font_smallcaps" id="S5.T6.10.1">all</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.T6.11.2">ref</span> datasets for the teacher model with CLC/Ohm and <span class="ltx_text ltx_font_bold" id="S5.T6.12.3">context-aware teacher model as baseline</span>. Note: Baseline is different from Table 1 to ensure comparable training setup. WERR: Word Error Rate Reduction.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T6.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.2.1" style="font-size:70%;">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T6.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.3.1" style="font-size:70%;">Context</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T6.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.4.1" style="font-size:70%;">CLC</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T6.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.5.1">
<span class="ltx_p" id="S5.T6.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.1.1.1" style="font-size:70%;">Ohm</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T6.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.1" style="font-size:70%;">WERR (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T6.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.m1.1b"><ci id="S5.T6.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</span></th>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2.1">
<th class="ltx_td ltx_th ltx_th_column" id="S5.T6.1.2.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S5.T6.1.2.1.2"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S5.T6.1.2.1.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.4"><span class="ltx_text ltx_font_smallcaps" id="S5.T6.1.2.1.4.1" style="font-size:70%;">all</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.5"><span class="ltx_text ltx_font_smallcaps" id="S5.T6.1.2.1.5.1" style="font-size:70%;">ref</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T6.1.3.1.1" rowspan="3"><span class="ltx_text" id="S5.T6.1.3.1.1.1" style="font-size:70%;">Teacher (200M)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.1.3.1.2"><span class="ltx_text" id="S5.T6.1.3.1.2.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T6.1.3.1.3"><span class="ltx_text" id="S5.T6.1.3.1.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T6.1.3.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.1.4.1">
<span class="ltx_p" id="S5.T6.1.3.1.4.1.1"><span class="ltx_text" id="S5.T6.1.3.1.4.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.5"><span class="ltx_text" id="S5.T6.1.3.1.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.6"><span class="ltx_text" id="S5.T6.1.3.1.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.2">
<td class="ltx_td ltx_align_left" id="S5.T6.1.4.2.1"><span class="ltx_text" id="S5.T6.1.4.2.1.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_left" id="S5.T6.1.4.2.2"><span class="ltx_text" id="S5.T6.1.4.2.2.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T6.1.4.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.2.3.1">
<span class="ltx_p" id="S5.T6.1.4.2.3.1.1"><span class="ltx_text" id="S5.T6.1.4.2.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.4"><span class="ltx_text" id="S5.T6.1.4.2.4.1" style="font-size:70%;">6.09</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.5"><span class="ltx_text" id="S5.T6.1.4.2.5.1" style="font-size:70%;">8.68</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T6.1.5.3.1"><span class="ltx_text" id="S5.T6.1.5.3.1.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T6.1.5.3.2"><span class="ltx_text" id="S5.T6.1.5.3.2.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T6.1.5.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.3.3.1">
<span class="ltx_p" id="S5.T6.1.5.3.3.1.1"><span class="ltx_text" id="S5.T6.1.5.3.3.1.1.1" style="font-size:50%;">✓</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S5.T6.1.5.3.4.1" style="font-size:70%;">6.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S5.T6.1.5.3.5.1" style="font-size:70%;">10.60</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Causal vs. Non-Causal Context:</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px4.p1.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T5" title="Table 5 ‣ Combining Context Types: ‣ 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 5</span></a>, we ablate the types of context that we show to the model. We observe that injecting non-causal (“future”) context <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px4.p1.1.1">during training</span> provides relative WERR of 7.39% as opposed to 5.08% on the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px4.p1.1.2">ref</span> dataset (as well as improvements on the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px4.p1.1.3">all</span> dataset), indicating that future context is significantly more important when correcting user reformulations. This is likely due to the fact that user reformulation is a “future signal” i.e. it follows the utterance that caused the error.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Implicit Context Learning:</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px5.p1.1">We can see that learning from the implicit context in the model is important for understanding and correcting dialog errors. As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T6" title="Table 6 ‣ Combining Context Types: ‣ 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 6</span></a>, Adding CLC and Ohm to the baseline model leads to significant improvement in the overall performance, particularly on the <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px5.p1.1.1">ref</span> dataset (so much that it enables a 200M parameter model to outperform a 1B parameter model without such losses). On the OD3 dataset (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T3" title="Table 3 ‣ 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 3</span></a>), the performance is even more distinct, with learning from implicit context leading to up to a 26.6% relative improvement over a baseline non-context model. In addition, zero shot comparison with other open source benchmark models is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T4" title="Table 4 ‣ 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Distilling knowledge to student model</h3>
<figure class="ltx_table" id="S5.T7">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 7: </span> Table showing the impact of distillation from a teacher model trained with implicit/explicit context. WERR: Word Error Rate Reduction. DE: Distillation Efficiency. </figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T7.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T7.2.2.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T7.2.2.3.1" style="font-size:70%;">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.2.2.4" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.T7.2.2.4.1" style="font-size:70%;">ssrd<span class="ltx_text ltx_font_upright" id="S5.T7.2.2.4.1.1"> weight</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T7.2.2.5.1" style="font-size:70%;">% Params</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T7.2.2.2">
<span class="ltx_text ltx_font_bold" id="S5.T7.2.2.2.2" style="font-size:70%;">WERR (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T7.1.1.1.1.m1.1"><semantics id="S5.T7.1.1.1.1.m1.1a"><mo id="S5.T7.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T7.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.1.m1.1b"><ci id="S5.T7.1.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T7.1.1.1.1.m1.1d">↑</annotation></semantics></math>) / DE (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T7.2.2.2.2.m2.1"><semantics id="S5.T7.2.2.2.2.m2.1a"><mo id="S5.T7.2.2.2.2.m2.1.1" stretchy="false" xref="S5.T7.2.2.2.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.2.2.m2.1b"><ci id="S5.T7.2.2.2.2.m2.1.1.cmml" xref="S5.T7.2.2.2.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.2.2.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T7.2.2.2.2.m2.1d">↑</annotation></semantics></math></span><span class="ltx_text" id="S5.T7.2.2.2.3" style="font-size:70%;">)</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.3.1">
<td class="ltx_td ltx_align_center" id="S5.T7.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T7.2.3.1.1.1" style="font-size:70%;">Adapted</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.3.1.2"><span class="ltx_text ltx_font_smallcaps" id="S5.T7.2.3.1.2.1" style="font-size:70%;">all</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.3.1.3"><span class="ltx_text ltx_font_smallcaps" id="S5.T7.2.3.1.3.1" style="font-size:70%;">ref</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T7.2.4.2.1"><span class="ltx_text" id="S5.T7.2.4.2.1.1" style="font-size:70%;">Student (1B)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.2.2"><span class="ltx_text" id="S5.T7.2.4.2.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.2.3"><span class="ltx_text" id="S5.T7.2.4.2.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.2.4"><span class="ltx_text" id="S5.T7.2.4.2.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.2.5"><span class="ltx_text" id="S5.T7.2.4.2.5.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.5.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T7.2.5.3.1" rowspan="3"><span class="ltx_text" id="S5.T7.2.5.3.1.1" style="font-size:70%;">+Distillation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.5.3.2"><span class="ltx_text" id="S5.T7.2.5.3.2.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.5.3.3"><span class="ltx_text" id="S5.T7.2.5.3.3.1" style="font-size:70%;">20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.5.3.4"><span class="ltx_text" id="S5.T7.2.5.3.4.1" style="font-size:70%;">1.38 / 19.97%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.5.3.5"><span class="ltx_text ltx_font_bold" id="S5.T7.2.5.3.5.1" style="font-size:70%;">3.06 / 31.94%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.6.4">
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.4.1"><span class="ltx_text" id="S5.T7.2.6.4.1.1" style="font-size:70%;">20</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.4.2"><span class="ltx_text" id="S5.T7.2.6.4.2.1" style="font-size:70%;">20</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.4.3"><span class="ltx_text ltx_font_bold" id="S5.T7.2.6.4.3.1" style="font-size:70%;">1.76 / 25.47%</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.4.4"><span class="ltx_text" id="S5.T7.2.6.4.4.1" style="font-size:70%;">1.2 / 12.52%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.2.7.5.1"><span class="ltx_text" id="S5.T7.2.7.5.1.1" style="font-size:70%;">50</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.2.7.5.2"><span class="ltx_text" id="S5.T7.2.7.5.2.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.2.7.5.3"><span class="ltx_text" id="S5.T7.2.7.5.3.1" style="font-size:70%;">1.51 / 21.85%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.2.7.5.4"><span class="ltx_text" id="S5.T7.2.7.5.4.1" style="font-size:70%;">2.95 / 30.79%</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T7" title="Table 7 ‣ 5.2 Distilling knowledge to student model ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 7</span></a> shows the performance of our model when distilled to a context-free student model. We can see that in all cases, the student model distilled from a context-trained model achieves superior performance. We also evaluate the distillation efficiency (DE) of the models – how much of the WER gains of the teacher model were retained during distillation. It is interesting to note that when leveraging the <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.1">ssrd</span> dataset, only 20% of the parameters in the model are necessary during the distillation process to achieve the same WERR, compared to when less reformulation data is used (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S4.SS3" title="4.3 Training details ‣ 4 Experimental Design ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">subsection 4.3</span></a>), indicating that the using the pre-trained teacher model with context not only is more accurate, but can be more efficient as well.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Tail-Distribution Performance</h3>
<figure class="ltx_table" id="S5.T8">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 8: </span> WERR when normalized by the domain (instead of by-utterance) on the <span class="ltx_text ltx_font_smallcaps" id="S5.T8.11.1">all</span> dataset. WERR (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T8.3.m1.1"><semantics id="S5.T8.3.m1.1b"><mo id="S5.T8.3.m1.1.1" stretchy="false" xref="S5.T8.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T8.3.m1.1c"><ci id="S5.T8.3.m1.1.1.cmml" xref="S5.T8.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.3.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T8.3.m1.1e">↑</annotation></semantics></math>): Word Error Rate Reduction. SERR (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T8.4.m2.1"><semantics id="S5.T8.4.m2.1b"><mo id="S5.T8.4.m2.1.1" stretchy="false" xref="S5.T8.4.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T8.4.m2.1c"><ci id="S5.T8.4.m2.1.1.cmml" xref="S5.T8.4.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.4.m2.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T8.4.m2.1e">↑</annotation></semantics></math>): Sentence Error Rate Improvement.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T8.12">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T8.12.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T8.12.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T8.12.1.1.1.1" style="font-size:70%;">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T8.12.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T8.12.1.1.2.1" style="font-size:70%;">Context</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T8.12.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T8.12.1.1.3.1" style="font-size:70%;">CLC</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T8.12.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.12.1.1.4.1">
<span class="ltx_p" id="S5.T8.12.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T8.12.1.1.4.1.1.1" style="font-size:70%;">Ohm</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T8.12.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T8.12.1.1.5.1" style="font-size:70%;">WERR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T8.12.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T8.12.1.1.6.1" style="font-size:70%;">SERR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T8.12.2.1">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T8.12.2.1.1" rowspan="3"><span class="ltx_text" id="S5.T8.12.2.1.1.1" style="font-size:70%;">Teacher (200M)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T8.12.2.1.2"><span class="ltx_text" id="S5.T8.12.2.1.2.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T8.12.2.1.3"><span class="ltx_text" id="S5.T8.12.2.1.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T8.12.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.12.2.1.4.1">
<span class="ltx_p" id="S5.T8.12.2.1.4.1.1"><span class="ltx_text" id="S5.T8.12.2.1.4.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.12.2.1.5"><span class="ltx_text" id="S5.T8.12.2.1.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.12.2.1.6"><span class="ltx_text" id="S5.T8.12.2.1.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.12.3.2">
<td class="ltx_td ltx_align_left" id="S5.T8.12.3.2.1"><span class="ltx_text" id="S5.T8.12.3.2.1.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_left" id="S5.T8.12.3.2.2"><span class="ltx_text" id="S5.T8.12.3.2.2.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_justify" id="S5.T8.12.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.12.3.2.3.1">
<span class="ltx_p" id="S5.T8.12.3.2.3.1.1"><span class="ltx_text" id="S5.T8.12.3.2.3.1.1.1" style="font-size:70%;">-</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T8.12.3.2.4"><span class="ltx_text" id="S5.T8.12.3.2.4.1" style="font-size:70%;">3.75</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.12.3.2.5"><span class="ltx_text" id="S5.T8.12.3.2.5.1" style="font-size:70%;">2.40</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.12.4.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T8.12.4.3.1"><span class="ltx_text" id="S5.T8.12.4.3.1.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T8.12.4.3.2"><span class="ltx_text" id="S5.T8.12.4.3.2.1" style="font-size:50%;">✓</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T8.12.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.12.4.3.3.1">
<span class="ltx_p" id="S5.T8.12.4.3.3.1.1"><span class="ltx_text" id="S5.T8.12.4.3.3.1.1.1" style="font-size:50%;">✓</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.12.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T8.12.4.3.4.1" style="font-size:70%;">6.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.12.4.3.5"><span class="ltx_text ltx_font_bold" id="S5.T8.12.4.3.5.1" style="font-size:70%;">7.79</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">While overall WER is an important measure, many times, a strong indicator of user satisfaction is performance on a wide range of queries on different topics (Such as home automation, calling/messaging and shopping). In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T8" title="Table 8 ‣ 5.3 Tail-Distribution Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 8</span></a>, we present WERR and SERR (Sentence Error Rate Improvement) when the WER is computed on each topic independently, and then averaged instead of being averaged over all utterances (independent of domain, i.e. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T8" title="Table 8 ‣ 5.3 Tail-Distribution Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 8</span></a> makes the assumption that all domains are equally likely). From this, we can see that while our non-context models perform well on the most common utterances, the contrastive models lead to significant improvements in less-common domains in our <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p1.1.1">all</span> dataset, including queries categorized into shopping (82.86% WERR), calling/messaging tasks (73.7% WERR), and music request tasks (36.8% WERR), all of which often need contextual disambiguation. On the other hand, while still in the long tail of the dataset, our approach performs worse than the baseline on home automation tasks (-22.68% WERR), one of the less diverse tasks that requires less contextual disambiguation. In such cases, our model may be relying more on the context, than the target utterance: leading to decreased performance. It remains interesting for future work to explore how we can dynamically trade off between context clues (for challenging utterances), and non-context learning (for utterances that don’t require contextual disambiguation).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This work introduces a framework that improves ASR in dialog systems through a dual-model approach to contextual learning: a context-aware teacher model that improves learning through explicit and implicit context signals, and a distilled student model that maintains efficiency during inference without context reliance. We achieve significant WER reductions, up to 9.58% on real-world datasets and 26.6% on the OD3 dataset, with the student model maintaining up to 33% of the reduction without context across the distillation process. The enhancements observed, particularly for rare words and diverse user queries, indicate a path toward more robust and satisfying conversational experiences, notably, the pronounced gains for tail queries suggests that our approach can significantly improve performance on less common tasks. Future directions for this work involve exploring the dynamic adjustment of the relative importance of context versus the target utterance based on their predicted utility, error correction <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib64" title="">2023a</a>)</cite> and safety <cite class="ltx_cite ltx_citemacro_citep">(Mehrabi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib41" title="">2023</a>)</cite>. This could potentially unlock even greater improvements in ASR performance, paving the way for more intelligent and adaptable conversational AI systems.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Impact Statement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work. Automatic speech recognition technology enhances accessibility, education, healthcare, legal processes, customer service, workplace productivity, language preservation, global connectivity, media accessibility, and safety across various societal sectors. While such impact is largely positive, it is important to recognize the impact of self-learning systems for automatic speech recognition on greater discussions in privacy and security, which are well discussed in related work <cite class="ltx_cite ltx_citemacro_citep">(Chennupati et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib13" title="">2022</a>; Aloufi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.10515v1#bib.bib1" title="">2021</a>)</cite>.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aloufi et al. (2021)</span>
<span class="ltx_bibblock">
Aloufi, R., Haddadi, H., and Boyle, D.

</span>
<span class="ltx_bibblock">Configurable privacy-preserving automatic speech recognition.

</span>
<span class="ltx_bibblock">In Hermansky, H., Cernocký, H., Burget, L., Lamel, L., Scharenborg, O., and Motlícek, P. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021</em>, pp.  861–865. ISCA, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2021-1783</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2021-1783" title="">https://doi.org/10.21437/Interspeech.2021-1783</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock">In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biadsy et al. (2022)</span>
<span class="ltx_bibblock">
Biadsy, F., Chen, Y., Zhang, X., Rybakov, O., Rosenberg, A., and Moreno, P. J.

</span>
<span class="ltx_bibblock">A scalable model specialization framework for training and inference using submodels and its application to speech model personalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ArXiv preprint</em>, abs/2203.12559, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.12559" title="">https://arxiv.org/abs/2203.12559</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buciluǎ et al. (2006)</span>
<span class="ltx_bibblock">
Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A.

</span>
<span class="ltx_bibblock">Model compression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, pp.  535–541, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan &amp; Ghosh (2022)</span>
<span class="ltx_bibblock">
Chan, D. M. and Ghosh, S.

</span>
<span class="ltx_bibblock">Content-context factorized representations for automated speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Interspeech</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al. (2022)</span>
<span class="ltx_bibblock">
Chan, D. M., Ghosh, S., Chakrabarty, D., and Hoffmeister, B.

</span>
<span class="ltx_bibblock">Multi-modal pre-training for automated speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ICASSP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al. (2023)</span>
<span class="ltx_bibblock">
Chan, D. M., Ghosh, S., Rastrow, A., and Hoffmeister, B.

</span>
<span class="ltx_bibblock">Domain adaptation with external off-policy acoustic catalogs for scalable contextual end-to-end automated speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ICASSP 2023-2023</em>, pp.  1–5. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al. (2024)</span>
<span class="ltx_bibblock">
Chan, D. M., Ghosh, S., Tulsiani, H., Rastrow, A., and Hoffmeister, B.

</span>
<span class="ltx_bibblock">Task oriented dialogue as a catalyst for self-supervised automatic speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  11806–11810, 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP48485.2024.10447164</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2021)</span>
<span class="ltx_bibblock">
Chang, F.-J., Liu, J., Radfar, M., Mouchtaris, A., Omologo, M., Rastrow, A., and Kunzmann, S.

</span>
<span class="ltx_bibblock">Context-aware transformer transducer for speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">2021 ASRU</em>, pp.  503–510. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2022)</span>
<span class="ltx_bibblock">
Chang, K.-W., Tseng, W.-C., Li, S.-W., and Lee, H.-y.

</span>
<span class="ltx_bibblock">Speechprompt: An exploration of prompt tuning on generative spoken language model for speech processing tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">ArXiv preprint</em>, abs/2203.16773, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.16773" title="">https://arxiv.org/abs/2203.16773</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2023)</span>
<span class="ltx_bibblock">
Chang, S.-Y. et al.

</span>
<span class="ltx_bibblock">Context-aware end-to-end asr using self-attentive embedding and tensor fusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">ICASSP 2023-2023</em>, pp.  1–5. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Chen, Z., Jain, M., Wang, Y., Seltzer, M. L., and Fuegen, C.

</span>
<span class="ltx_bibblock">Joint grapheme and phoneme embeddings for contextual end-to-end ASR.

</span>
<span class="ltx_bibblock">In Kubin, G. and Kacic, Z. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019</em>, pp.  3490–3494. ISCA, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2019-1434</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2019-1434" title="">https://doi.org/10.21437/Interspeech.2019-1434</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chennupati et al. (2022)</span>
<span class="ltx_bibblock">
Chennupati, G., Rao, M., Chadha, G., Eakin, A., Raju, A., Tiwari, G., Sahu, A. K., Rastrow, A., Droppo, J., Oberlin, A., Nandanoor, B., Venkataramanan, P., Wu, Z., and Sitpure, P.

</span>
<span class="ltx_bibblock">Ilasr: privacy-preserving incremental learning for automatic speech recognition at production scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">KDD 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.  4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1423</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1423" title="">https://aclanthology.org/N19-1423</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dingliwal et al. (2023)</span>
<span class="ltx_bibblock">
Dingliwal, S., Sunkara, M., Ronanki, S., Farris, J., Kirchhoff, K., and Bodapati, S.

</span>
<span class="ltx_bibblock">Personalization of ctc speech recognition models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">2022 IEEE Spoken Language Technology Workshop (SLT)</em>, pp.  302–309. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duarte-Torres et al. (2024)</span>
<span class="ltx_bibblock">
Duarte-Torres, S., Sen, A., Rana, A., Drude, L., Gomez-Alanis, A., Schwarz, A., Rädel, L., and Leutnant, V.

</span>
<span class="ltx_bibblock">Promptformer: Prompted conformer transducer for asr.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">ArXiv preprint</em>, abs/2401.07360, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.07360" title="">https://arxiv.org/abs/2401.07360</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Futami et al. (2022)</span>
<span class="ltx_bibblock">
Futami, H., Inaguma, H., Mimura, M., Sakai, S., and Kawahara, T.

</span>
<span class="ltx_bibblock">Distilling the knowledge of bert for ctc-based asr.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ArXiv preprint</em>, abs/2209.02030, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2209.02030" title="">https://arxiv.org/abs/2209.02030</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2022)</span>
<span class="ltx_bibblock">
Gao, H., Ni, J., Qian, K., Zhang, Y., Chang, S., and Hasegawa-Johnson, M.

</span>
<span class="ltx_bibblock">Wavprompt: Towards few-shot spoken language understanding with frozen language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ArXiv preprint</em>, abs/2203.15863, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.15863" title="">https://arxiv.org/abs/2203.15863</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gourav et al. (2021)</span>
<span class="ltx_bibblock">
Gourav, A., Liu, L., Gandhe, A., Gu, Y., Lan, G., Huang, X., Kalmane, S., Tiwari, G., Filimonov, D., Rastrow, A., et al.

</span>
<span class="ltx_bibblock">Personalization strategies for end-to-end speech recognition systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  7348–7352. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves (2012)</span>
<span class="ltx_bibblock">
Graves, A.

</span>
<span class="ltx_bibblock">Sequence transduction with recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1211.3711</em>, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulati et al. (2020)</span>
<span class="ltx_bibblock">
Gulati, A., Qin, J., Chiu, C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang, R.

</span>
<span class="ltx_bibblock">Conformer: Convolution-augmented transformer for speech recognition.

</span>
<span class="ltx_bibblock">In Meng, H., Xu, B., and Zheng, T. F. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020</em>, pp.  5036–5040. ISCA, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2020-3015</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2020-3015" title="">https://doi.org/10.21437/Interspeech.2020-3015</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al. (2015)</span>
<span class="ltx_bibblock">
Hinton, G., Vinyals, O., and Dean, J.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">stat</em>, 1050:9, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hori et al. (2020)</span>
<span class="ltx_bibblock">
Hori, T., Moritz, N., Hori, C., and Roux, J. L.

</span>
<span class="ltx_bibblock">Transformer-based long-context end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In Meng, H., Xu, B., and Zheng, T. F. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020</em>, pp.  5011–5015. ISCA, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2020-2928</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2020-2928" title="">https://doi.org/10.21437/Interspeech.2020-2928</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hori et al. (2021)</span>
<span class="ltx_bibblock">
Hori, T., Moritz, N., Hori, C., and Roux, J. L.

</span>
<span class="ltx_bibblock">Advanced long-context end-to-end speech recognition using context-expanded transformers.

</span>
<span class="ltx_bibblock">In Hermansky, H., Cernocký, H., Burget, L., Lamel, L., Scharenborg, O., and Motlícek, P. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021</em>, pp.  2097–2101. ISCA, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2021-1643</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2021-1643" title="">https://doi.org/10.21437/Interspeech.2021-1643</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Hsu, W.-N., Tsai, Y.-H. H., Bolte, B., Salakhutdinov, R., and Mohamed, A.

</span>
<span class="ltx_bibblock">Hubert: How much can a bad teacher benefit asr pre-training?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ICASSP</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2018)</span>
<span class="ltx_bibblock">
Huang, M., You, Y., Chen, Z., Qian, Y., and Yu, K.

</span>
<span class="ltx_bibblock">Knowledge distillation for sequence model.

</span>
<span class="ltx_bibblock">In Yegnanarayana, B. (ed.), <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad, India, 2-6 September 2018</em>, pp.  3703–3707. ISCA, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2018-1589</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2018-1589" title="">https://doi.org/10.21437/Interspeech.2018-1589</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hung et al. (2023)</span>
<span class="ltx_bibblock">
Hung, Y.-N., Yang, C.-H. H., Chen, P.-Y., and Lerch, A.

</span>
<span class="ltx_bibblock">Low-resource music genre classification with cross-modal neural model reprogramming.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1–5. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaech &amp; Ostendorf (2018)</span>
<span class="ltx_bibblock">
Jaech, A. and Ostendorf, M.

</span>
<span class="ltx_bibblock">Personalized language model for query auto-completion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pp.  700–705, Melbourne, Australia, 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P18-2111</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P18-2111" title="">https://aclanthology.org/P18-2111</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2024)</span>
<span class="ltx_bibblock">
Jain, Y., Chan, D. M., Dheram, P., Khare, A., Shonibare, O., Ravichandran, V., and Ghosh, S.

</span>
<span class="ltx_bibblock">Multi-stage multi-modal pre-training for automatic speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">LREC-COLING</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jayanthi et al. (2023)</span>
<span class="ltx_bibblock">
Jayanthi, S. M., Kulshreshtha, D., Dingliwal, S., Ronanki, S., and Bodapati, S.

</span>
<span class="ltx_bibblock">Retrieve and copy: Scaling asr personalization to large catalogs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">EMNLP 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2019a)</span>
<span class="ltx_bibblock">
Kim, H., Na, H., Lee, H., Lee, J., Kang, T. G., Lee, M., and Choi, Y. S.

</span>
<span class="ltx_bibblock">Knowledge distillation using output errors for self-attention end-to-end models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019</em>, pp.  6181–6185. IEEE, 2019a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP.2019.8682775</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICASSP.2019.8682775" title="">https://doi.org/10.1109/ICASSP.2019.8682775</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim &amp; Metze (2018)</span>
<span class="ltx_bibblock">
Kim, S. and Metze, F.

</span>
<span class="ltx_bibblock">Dialog-context aware end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">SLT</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2019b)</span>
<span class="ltx_bibblock">
Kim, S., Dalmia, S., and Metze, F.

</span>
<span class="ltx_bibblock">Gated embeddings in end-to-end speech recognition for conversational-context fusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pp.  1131–1141, Florence, Italy, 2019b. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1107</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1107" title="">https://aclanthology.org/P19-1107</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ku et al. (2024)</span>
<span class="ltx_bibblock">
Ku, P.-J., Chen, I.-F., Yang, H., Raju, A., Dheram, P., Ghahremani, P., King, B., Liu, J., Ren, R., and Nidadavolu, P.

</span>
<span class="ltx_bibblock">Hot-fixing wake word recognition for end-to-end asr via neural model reprogramming.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">ICASSP 2024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo &amp; Richardson (2018)</span>
<span class="ltx_bibblock">
Kudo, T. and Richardson, J.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pp.  66–71, Brussels, Belgium, 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D18-2012</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-2012" title="">https://aclanthology.org/D18-2012</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurata &amp; Saon (2020)</span>
<span class="ltx_bibblock">
Kurata, G. and Saon, G.

</span>
<span class="ltx_bibblock">Knowledge distillation from offline to streaming RNN transducer for end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In Meng, H., Xu, B., and Zheng, T. F. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020</em>, pp.  2117–2121. ISCA, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2020-2442</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2020-2442" title="">https://doi.org/10.21437/Interspeech.2020-2442</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Li, B., Pang, R., Zhang, Y., Sainath, T. N., Strohman, T., Haghani, P., Zhu, Y., Farris, B., Gaur, N., and Prasad, M.

</span>
<span class="ltx_bibblock">Massively multilingual asr: A lifelong learning solution.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  6397–6401. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2015)</span>
<span class="ltx_bibblock">
Lin, R., Liu, S., Yang, M., Li, M., Zhou, M., and Li, S.

</span>
<span class="ltx_bibblock">Hierarchical recurrent neural network for document modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>, pp.  899–907, Lisbon, Portugal, 2015. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D15-1106</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D15-1106" title="">https://aclanthology.org/D15-1106</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu &amp; Lane (2017)</span>
<span class="ltx_bibblock">
Liu, B. and Lane, I.

</span>
<span class="ltx_bibblock">Dialog context language modeling with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">2017 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2017, New Orleans, LA, USA, March 5-9, 2017</em>, pp.  5715–5719. IEEE, 2017.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP.2017.7953251</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICASSP.2017.7953251" title="">https://doi.org/10.1109/ICASSP.2017.7953251</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masumura et al. (2021)</span>
<span class="ltx_bibblock">
Masumura, R., Makishima, N., Ihori, M., Takashima, A., Tanaka, T., and Orihashi, S.

</span>
<span class="ltx_bibblock">Hierarchical transformer-based large-context end-to-end asr with large-context knowledge distillation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  5879–5883. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehrabi et al. (2023)</span>
<span class="ltx_bibblock">
Mehrabi, N., Goyal, P., Dupuy, C., Hu, Q., Ghosh, S., Zemel, R., Chang, K.-W., Galstyan, A., and Gupta, R.

</span>
<span class="ltx_bibblock">Flirt: Feedback loop in-context red teaming.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.04265" title="">https://arxiv.org/abs/2308.04265</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra et al. (2023)</span>
<span class="ltx_bibblock">
Mitra, S. et al.

</span>
<span class="ltx_bibblock">Unified modeling of multi-domain multi-device ASR systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">TSD</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mun’im et al. (2019)</span>
<span class="ltx_bibblock">
Mun’im, R. M., Inoue, N., and Shinoda, K.

</span>
<span class="ltx_bibblock">Sequence-level knowledge distillation for model compression of attention-based sequence-to-sequence speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019</em>, pp.  6151–6155. IEEE, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP.2019.8683171</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICASSP.2019.8683171" title="">https://doi.org/10.1109/ICASSP.2019.8683171</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munkhdalai et al. (2022)</span>
<span class="ltx_bibblock">
Munkhdalai, T., Sim, K. C., Chandorkar, A., Gao, F., Chua, M., Strohman, T., and Beaufays, F.

</span>
<span class="ltx_bibblock">Fast contextual adaptation with neural associative memory for on-device personalized speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">ICASSP</em>, pp.  6632–6636. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Novotney et al. (2022)</span>
<span class="ltx_bibblock">
Novotney, S., Mukherjee, S., Ahmed, Z., and Stolcke, A.

</span>
<span class="ltx_bibblock">CUE vectors: Modular training of language models conditioned on diverse contextual signals.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Findings of the Association for Computational Linguistics: ACL 2022</em>, pp.  3368–3379, Dublin, Ireland, 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.findings-acl.265</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.findings-acl.265" title="">https://aclanthology.org/2022.findings-acl.265</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parthasarathi &amp; Strom (2019)</span>
<span class="ltx_bibblock">
Parthasarathi, S. H. K. and Strom, N.

</span>
<span class="ltx_bibblock">Lessons from building acoustic models with a million hours of speech.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019</em>, pp.  6670–6674. IEEE, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP.2019.8683690</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICASSP.2019.8683690" title="">https://doi.org/10.1109/ICASSP.2019.8683690</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et al. (2023)</span>
<span class="ltx_bibblock">
Pham, H. et al.

</span>
<span class="ltx_bibblock">Combined scaling for zero-shot transfer learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Neurocomputing</em>, pp.  126658, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham et al. (2022)</span>
<span class="ltx_bibblock">
Pham, M., Cho, M., Joshi, A., and Hegde, C.

</span>
<span class="ltx_bibblock">Revisiting self-distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">ArXiv preprint</em>, abs/2206.08491, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2206.08491" title="">https://arxiv.org/abs/2206.08491</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2022)</span>
<span class="ltx_bibblock">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">ArXiv preprint</em>, abs/2212.04356, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2212.04356" title="">https://arxiv.org/abs/2212.04356</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">ICML</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radosavovic et al. (2018)</span>
<span class="ltx_bibblock">
Radosavovic, I., Dollár, P., Girshick, R. B., Gkioxari, G., and He, K.

</span>
<span class="ltx_bibblock">Data distillation: Towards omni-supervised learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018</em>, pp.  4119–4128. IEEE Computer Society, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2018.00433</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.html" title="">http://openaccess.thecvf.com/content_cvpr_2018/html/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson et al. (2021)</span>
<span class="ltx_bibblock">
Robinson, J. D., Chuang, C., Sra, S., and Jegelka, S.

</span>
<span class="ltx_bibblock">Contrastive learning with hard negative samples.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=CR1XOQ0UTh-" title="">https://openreview.net/forum?id=CR1XOQ0UTh-</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sathyendra et al. (2022)</span>
<span class="ltx_bibblock">
Sathyendra, K. M., Muniyappa, T., Chang, F.-J., Liu, J., Su, J., Strimel, G. P., Mouchtaris, A., and Kunzmann, S.

</span>
<span class="ltx_bibblock">Contextual adapters for personalized speech recognition in neural transducers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ICASSP 2022-2022</em>, pp.  8537–8541. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwarz et al. (2023)</span>
<span class="ltx_bibblock">
Schwarz, A., He, D., Van Segbroeck, M., Hethnawi, M., and Rastrow, A.

</span>
<span class="ltx_bibblock">Personalized predictive asr for latency reduction in voice assistants.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">ArXiv preprint</em>, abs/2305.13794, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.13794" title="">https://arxiv.org/abs/2305.13794</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shenoy et al. (2021)</span>
<span class="ltx_bibblock">
Shenoy, A., Bodapati, S., and Kirchhoff, K.

</span>
<span class="ltx_bibblock">Contextual biasing of language models for speech recognition in goal-oriented conversational agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ArXiv preprint</em>, abs/2103.10325, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2103.10325" title="">https://arxiv.org/abs/2103.10325</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shor et al. (2019)</span>
<span class="ltx_bibblock">
Shor, J., Emanuel, D., Lang, O., Tuval, O., Brenner, M. P., Cattiau, J., Vieira, F., McNally, M., Charbonneau, T., Nollstadt, M., Hassidim, A., and Matias, Y.

</span>
<span class="ltx_bibblock">Personalizing ASR for dysarthric and accented speech with limited data.

</span>
<span class="ltx_bibblock">In Kubin, G. and Kacic, Z. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019</em>, pp.  784–788. ISCA, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2019-1427</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2019-1427" title="">https://doi.org/10.21437/Interspeech.2019-1427</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Sun, C., Ahmed, Z., Ma, Y., Liu, Z., Pang, Y., and Kalinli, O.

</span>
<span class="ltx_bibblock">Contextual biasing of named-entities with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">ArXiv preprint</em>, abs/2309.00723, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.00723" title="">https://arxiv.org/abs/2309.00723</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2024)</span>
<span class="ltx_bibblock">
Tang, J., Kim, K., Shon, S., Wu, F., Sridhar, P., and Watanabe, S.

</span>
<span class="ltx_bibblock">Improving asr contextual biasing with guided attention.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ArXiv preprint</em>, abs/2401.08835, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.08835" title="">https://arxiv.org/abs/2401.08835</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsunoo et al. (2021)</span>
<span class="ltx_bibblock">
Tsunoo, E., Kashiwagi, Y., and Watanabe, S.

</span>
<span class="ltx_bibblock">Streaming transformer asr with blockwise synchronous beam search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">2021 IEEE Spoken Language Technology Workshop (SLT)</em>, pp.  22–29. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pp.  5998–6008, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vitter (1985)</span>
<span class="ltx_bibblock">
Vitter, J. S.

</span>
<span class="ltx_bibblock">Random sampling with a reservoir.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">ACM Transactions on Mathematical Software (TOMS)</em>, 11(1):37–57, 1985.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Wei, K. et al.

</span>
<span class="ltx_bibblock">Attentive contextual carryover for multi-turn end-to-end spoken language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">2021 ASRU</em>, pp.  837–844. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams et al. (2018)</span>
<span class="ltx_bibblock">
Williams, I., Kannan, A., Aleksic, P. S., Rybach, D., and Sainath, T. N.

</span>
<span class="ltx_bibblock">Contextual speech recognition in end-to-end neural network systems using beam search.

</span>
<span class="ltx_bibblock">In Yegnanarayana, B. (ed.), <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad, India, 2-6 September 2018</em>, pp.  2227–2231. ISCA, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2018-2416</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2018-2416" title="">https://doi.org/10.21437/Interspeech.2018-2416</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023a)</span>
<span class="ltx_bibblock">
Yang, C.-H. H., Gu, Y., Liu, Y.-C., Ghosh, S., Bulyko, I., and Stolcke, A.

</span>
<span class="ltx_bibblock">Generative speech recognition error correction with large language models and task-activating prompting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, December 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023b)</span>
<span class="ltx_bibblock">
Yang, C.-H. H., Li, B., Zhang, Y., Chen, N., Prabhavalkar, R., Sainath, T. N., and Strohman, T.

</span>
<span class="ltx_bibblock">From english to more languages: Parameter-efficient model reprogramming for cross-lingual speech recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1–5. IEEE, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2021)</span>
<span class="ltx_bibblock">
Yu, J., Han, W., Gulati, A., Chiu, C., Li, B., Sainath, T. N., Wu, Y., and Pang, R.

</span>
<span class="ltx_bibblock">Dual-mode ASR: unify and improve streaming ASR with full-context modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Pz_dcqfcKW8" title="">https://openreview.net/forum?id=Pz_dcqfcKW8</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Zhang, L., Song, J., Gao, A., Chen, J., Bao, C., and Ma, K.

</span>
<span class="ltx_bibblock">Be your own teacher: Improve the performance of convolutional neural networks via self distillation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</em>, pp.  3712–3721. IEEE, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICCV.2019.00381</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICCV.2019.00381" title="">https://doi.org/10.1109/ICCV.2019.00381</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (1997)</span>
<span class="ltx_bibblock">
Zhang, T., Ramakrishnan, R., and Livny, M.

</span>
<span class="ltx_bibblock">Birch: A new data clustering algorithm and its applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Data mining and knowledge discovery</em>, 1:141–182, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2019)</span>
<span class="ltx_bibblock">
Zhao, D., Sainath, T. N., Rybach, D., Rondon, P., Bhatia, D., Li, B., and Pang, R.

</span>
<span class="ltx_bibblock">Shallow-fusion end-to-end contextual biasing.

</span>
<span class="ltx_bibblock">In Kubin, G. and Kacic, Z. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019</em>, pp.  1418–1422. ISCA, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2019-1209</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21437/Interspeech.2019-1209" title="">https://doi.org/10.21437/Interspeech.2019-1209</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Insertions/Deletions/Substitutions in CLC/Ohm</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We can further break down the performance in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#S5.T6" title="Table 6 ‣ Combining Context Types: ‣ 5.1 Teacher Performance ‣ 5 Results and Discussion ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table 6</span></a> in terms of insertions, deletions and substitutions, which is given in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.10515v1#A1.T1" title="Table A.1 ‣ Appendix A Insertions/Deletions/Substitutions in CLC/Ohm ‣ An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems"><span class="ltx_text ltx_ref_tag">Table A.1</span></a>. We can see that adding CLC loss significantly improves the rate of deletion compared to baseline models. Unfortunately, this comes at the cost of improvement in substitution and insertion. CLC, instead of doing the best job of disambiguating generated tokens, focuses on recall as opposed to precision. Ohm improves the disambiguation, as at the cost of deletions: more tokens are dropped, but the tokens that are preserved are more accurate.</p>
</div>
<figure class="ltx_table" id="A1.T1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table A.1: </span> WERR when normalized by the domain (instead of by-utterance) on the <span class="ltx_text ltx_font_smallcaps" id="A1.T1.11.1">all</span> dataset. WERR (<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T1.3.m1.1"><semantics id="A1.T1.3.m1.1b"><mo id="A1.T1.3.m1.1.1" stretchy="false" xref="A1.T1.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T1.3.m1.1c"><ci id="A1.T1.3.m1.1.1.cmml" xref="A1.T1.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.3.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T1.3.m1.1e">↑</annotation></semantics></math>): Word Error Rate Reduction. SERR (<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T1.4.m2.1"><semantics id="A1.T1.4.m2.1b"><mo id="A1.T1.4.m2.1.1" stretchy="false" xref="A1.T1.4.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T1.4.m2.1c"><ci id="A1.T1.4.m2.1.1.cmml" xref="A1.T1.4.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.4.m2.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T1.4.m2.1e">↑</annotation></semantics></math>): Sentence Error Rate Improvement. INSR: Relative Insertion rate. SUBR: Relative Substitution Rate. DELR: Relative Deletion Rate</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T1.12">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T1.12.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T1.12.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.1.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T1.12.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.2.1" style="font-size:90%;">Context</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T1.12.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.3.1" style="font-size:90%;">CLC</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T1.12.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="A1.T1.12.1.1.4.1">
<span class="ltx_p" id="A1.T1.12.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.4.1.1.1" style="font-size:90%;">Ohm</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T1.12.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.5.1" style="font-size:90%;">WERR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T1.12.1.1.6"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.6.1" style="font-size:90%;">SERR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T1.12.1.1.7"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.7.1" style="font-size:90%;">SUBR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T1.12.1.1.8"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.8.1" style="font-size:90%;">INSR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T1.12.1.1.9"><span class="ltx_text ltx_font_bold" id="A1.T1.12.1.1.9.1" style="font-size:90%;">DELR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T1.12.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A1.T1.12.2.1.1" rowspan="3"><span class="ltx_text" id="A1.T1.12.2.1.1.1" style="font-size:90%;">Teacher (200M)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T1.12.2.1.2"><span class="ltx_text" id="A1.T1.12.2.1.2.1" style="font-size:50%;">✓</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T1.12.2.1.3"><span class="ltx_text" id="A1.T1.12.2.1.3.1" style="font-size:90%;">-</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" id="A1.T1.12.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="A1.T1.12.2.1.4.1">
<span class="ltx_p" id="A1.T1.12.2.1.4.1.1"><span class="ltx_text" id="A1.T1.12.2.1.4.1.1.1" style="font-size:90%;">-</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T1.12.2.1.5"><span class="ltx_text" id="A1.T1.12.2.1.5.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T1.12.2.1.6"><span class="ltx_text" id="A1.T1.12.2.1.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T1.12.2.1.7"><span class="ltx_text" id="A1.T1.12.2.1.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T1.12.2.1.8"><span class="ltx_text" id="A1.T1.12.2.1.8.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T1.12.2.1.9"><span class="ltx_text" id="A1.T1.12.2.1.9.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.12.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T1.12.3.2.1"><span class="ltx_text" id="A1.T1.12.3.2.1.1" style="font-size:50%;">✓</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T1.12.3.2.2"><span class="ltx_text" id="A1.T1.12.3.2.2.1" style="font-size:50%;">✓</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="A1.T1.12.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T1.12.3.2.3.1">
<span class="ltx_p" id="A1.T1.12.3.2.3.1.1"><span class="ltx_text" id="A1.T1.12.3.2.3.1.1.1" style="font-size:90%;">-</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center" id="A1.T1.12.3.2.4"><span class="ltx_text" id="A1.T1.12.3.2.4.1" style="font-size:90%;">3.75</span></td>
<td class="ltx_td ltx_align_center" id="A1.T1.12.3.2.5"><span class="ltx_text" id="A1.T1.12.3.2.5.1" style="font-size:90%;">2.40</span></td>
<td class="ltx_td ltx_align_center" id="A1.T1.12.3.2.6"><span class="ltx_text" id="A1.T1.12.3.2.6.1" style="font-size:90%;">1.56</span></td>
<td class="ltx_td ltx_align_center" id="A1.T1.12.3.2.7"><span class="ltx_text" id="A1.T1.12.3.2.7.1" style="font-size:90%;">0.82</span></td>
<td class="ltx_td ltx_align_center" id="A1.T1.12.3.2.8"><span class="ltx_text" id="A1.T1.12.3.2.8.1" style="font-size:90%;">13.65</span></td>
</tr>
<tr class="ltx_tr" id="A1.T1.12.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T1.12.4.3.1"><span class="ltx_text" id="A1.T1.12.4.3.1.1" style="font-size:50%;">✓</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T1.12.4.3.2"><span class="ltx_text" id="A1.T1.12.4.3.2.1" style="font-size:50%;">✓</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb" id="A1.T1.12.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T1.12.4.3.3.1">
<span class="ltx_p" id="A1.T1.12.4.3.3.1.1"><span class="ltx_text" id="A1.T1.12.4.3.3.1.1.1" style="font-size:50%;">✓</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T1.12.4.3.4"><span class="ltx_text ltx_font_bold" id="A1.T1.12.4.3.4.1" style="font-size:90%;">6.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T1.12.4.3.5"><span class="ltx_text ltx_font_bold" id="A1.T1.12.4.3.5.1" style="font-size:90%;">7.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T1.12.4.3.6"><span class="ltx_text ltx_font_bold" id="A1.T1.12.4.3.6.1" style="font-size:90%;">3.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T1.12.4.3.7"><span class="ltx_text ltx_font_bold" id="A1.T1.12.4.3.7.1" style="font-size:90%;">1.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T1.12.4.3.8"><span class="ltx_text ltx_font_bold" id="A1.T1.12.4.3.8.1" style="font-size:90%;">6.18</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 16:42:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
