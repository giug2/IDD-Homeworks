<article class="ltx_document">
 <h1 class="ltx_title ltx_title_document">
  Counterfactual Debating with Preset Stances for Hallucination
  <br class="ltx_break"/>
  Elimination of LLMs
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Yi Fang
    <sup class="ltx_sup" id="id1.1.id1">
     1
    </sup>
    ,
Moxin Li
    <sup class="ltx_sup" id="id2.2.id2">
     2
    </sup>
    ,
Wenjie Wang
    <sup class="ltx_sup" id="id3.3.id3">
     2
    </sup>
    ,
Hui Lin
    <sup class="ltx_sup" id="id4.4.id4">
     3
    </sup>
    ,
Fuli Feng
    <sup class="ltx_sup" id="id5.5.id5">
     1
    </sup>
    <span class="ltx_note ltx_role_footnotemark" id="footnotex1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_note_type">
        footnotemark:
       </span>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
      </span>
     </span>
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id6.6.id6">
     1
    </sup>
    University of Science and Technology of China,
    <sup class="ltx_sup" id="id7.7.id7">
     2
    </sup>
    National University of Singapore,
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id8.8.id8">
     3
    </sup>
    Electronic Science Research Institute of China Electronics
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id9.9.id9">
     peterfang@mail.ustc.edu.cn
    </span>
    ,
    <span class="ltx_text ltx_font_typewriter" id="id10.10.id10">
     limoxin@u.nus.edu
    </span>
    ,
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id11.11.id11">
     {wenjiewang96,fulifeng93}@gamil.com
    </span>
    ,
    <span class="ltx_text ltx_font_typewriter" id="id12.12.id12">
     linhui@whu.edu.cn
    </span>
   </span>
   <span class="ltx_author_notes">
    Corresponding author.
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id13.id1">
   Large Language Models (LLMs) excel in various natural language processing tasks but struggle with hallucination issues.
Existing solutions have considered utilizing LLMs’ inherent reasoning abilities to alleviate hallucination, such as self-correction and diverse sampling methods.
However, these methods often overtrust LLMs’ initial answers due to inherent biases.
The key to alleviating this issue lies in overriding LLMs’ inherent biases for answer inspection.
To this end, we propose a
   <span class="ltx_text ltx_font_bold" id="id13.id1.1">
    C
   </span>
   ounter
   <span class="ltx_text ltx_font_bold" id="id13.id1.2">
    F
   </span>
   actual
   <span class="ltx_text ltx_font_bold" id="id13.id1.3">
    M
   </span>
   ulti-
   <span class="ltx_text ltx_font_bold" id="id13.id1.4">
    A
   </span>
   gent
   <span class="ltx_text ltx_font_bold" id="id13.id1.5">
    D
   </span>
   ebate (CFMAD) framework.
CFMAD presets the stances of LLMs to override their inherent biases by compelling LLMs to generate justifications for a predetermined answer’s correctness.
The LLMs with different predetermined stances are engaged with a skeptical critic for counterfactual debate on the rationality of generated justifications.
Finally, the debate process is evaluated by a third-party judge to determine the final answer.
Extensive experiments on four datasets of three tasks demonstrate the superiority of CFMAD over existing methods.
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <div class="ltx_block ltx_align_bottom" id="p1.1">
   <p class="ltx_p" id="p1.1.1">
    <span class="ltx_text ltx_font_bold" id="p1.1.1.1">
     Counterfactual Debating with Preset Stances for Hallucination
     <br class="ltx_break"/>
     Elimination of LLMs
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
   <p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;">
    <span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
     <span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
      <span class="ltx_tbody">
       <span class="ltx_tr" id="p1.1.2.1.1.1.1">
        <span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">
          Yi Fang
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">
           1
          </sup>
          ,
Moxin Li
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">
           2
          </sup>
          ,
Wenjie Wang
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">
           2
          </sup>
          <span class="ltx_note ltx_role_thanks" id="p1.1.2.1.1.1.1.1.1.4">
           <sup class="ltx_note_mark">
            †
           </sup>
           <span class="ltx_note_outer">
            <span class="ltx_note_content">
             <sup class="ltx_note_mark">
              †
             </sup>
             <span class="ltx_note_type">
              thanks:
             </span>
             Corresponding author.
            </span>
           </span>
          </span>
          ,
Hui Lin
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.5">
           3
          </sup>
          ,
Fuli Feng
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.6">
           1
          </sup>
          <span class="ltx_note ltx_role_footnotemark" id="footnotex2">
           <sup class="ltx_note_mark">
            1
           </sup>
           <span class="ltx_note_outer">
            <span class="ltx_note_content">
             <sup class="ltx_note_mark">
              1
             </sup>
             <span class="ltx_note_type">
              footnotemark:
             </span>
             <span class="ltx_tag ltx_tag_note">
              <span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">
               1
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.1.2.1.1.2.2">
        <span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">
         <sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1">
          1
         </sup>
         University of Science and Technology of China,
         <sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.2">
          2
         </sup>
         National University of Singapore,
        </span>
       </span>
       <span class="ltx_tr" id="p1.1.2.1.1.3.3">
        <span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1">
         <sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">
          3
         </sup>
         Electronic Science Research Institute of China Electronics
        </span>
       </span>
       <span class="ltx_tr" id="p1.1.2.1.1.4.4">
        <span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1">
         <span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.4.4.1.1">
          peterfang@mail.ustc.edu.cn
         </span>
         ,
         <span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.4.4.1.2">
          limoxin@u.nus.edu
         </span>
         ,
        </span>
       </span>
       <span class="ltx_tr" id="p1.1.2.1.1.5.5">
        <span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.5.1">
         <span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.5.5.1.1">
          {wenjiewang96,fulifeng93}@gamil.com
         </span>
         ,
         <span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.5.5.1.2">
          linhui@whu.edu.cn
         </span>
        </span>
       </span>
      </span>
     </span>
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
  </div>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Large Language Models, especially closed-source ones such as GPT-4
    <cite class="ltx_cite ltx_citemacro_citep">
     (Achiam et al.,
     <a class="ltx_ref" href="#bib.bib1" title="">
      2023
     </a>
     )
    </cite>
    and Gemini
    <cite class="ltx_cite ltx_citemacro_citep">
     (Team et al.,
     <a class="ltx_ref" href="#bib.bib30" title="">
      2023
     </a>
     )
    </cite>
    , have demonstrated state-of-the-art performance across various natural language processing tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     Bubeck et al. (
     <a class="ltx_ref" href="#bib.bib5" title="">
      2023
     </a>
     ); Zhao et al. (
     <a class="ltx_ref" href="#bib.bib48" title="">
      2023
     </a>
     )
    </cite>
    .
However, LLMs still struggle with the hallucination problem,
    <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">
     i.e.,
    </em>
    occasionally generating unfaithful content
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhang et al.,
     <a class="ltx_ref" href="#bib.bib47" title="">
      2023
     </a>
     ; Bang et al.,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2023
     </a>
     ; Zheng et al.,
     <a class="ltx_ref" href="#bib.bib49" title="">
      2023
     </a>
     )
    </cite>
    .
Due to the black-box nature of closed-source LLMs, it is difficult for users to directly intervene in or optimize their internal mechanisms to address the hallucination problems.
Currently, extensive research is investigating how to use LLMs’ inherent reasoning abilities to alleviate hallucinations without model intervention
    <cite class="ltx_cite ltx_citemacro_citep">
     (Shinn et al.,
     <a class="ltx_ref" href="#bib.bib26" title="">
      2024
     </a>
     ; Liang et al.,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="461" id="S1.F1.g1" src="/html/2406.11514/assets/x1.png" width="461"/>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Comparison of CFMAD with self-correction and diverse sampling methods. CFMAD presets stances for LLMs to override their inherent biases.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Related work of using LLMs’ own abilities for hallucination elimination can be categorized into self-correction and diverse sampling methods, which imitate human deep reasoning and broad reasoning to enhance LLMs’ reasoning capabilities, respectively
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhang et al.,
     <a class="ltx_ref" href="#bib.bib46" title="">
      2024b
     </a>
     )
    </cite>
    .
Self-correction methods
    <cite class="ltx_cite ltx_citemacro_citep">
     (Shinn et al.,
     <a class="ltx_ref" href="#bib.bib26" title="">
      2024
     </a>
     ; Madaan et al.,
     <a class="ltx_ref" href="#bib.bib21" title="">
      2024
     </a>
     )
    </cite>
    guide LLMs to reflect on and refine their previous answers iteratively.
Diverse sampling methods
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhang et al.,
     <a class="ltx_ref" href="#bib.bib46" title="">
      2024b
     </a>
     ; Du et al.,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2023
     </a>
     ; Wang et al.,
     <a class="ltx_ref" href="#bib.bib36" title="">
      2023
     </a>
     ; Mielke et al.,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2022
     </a>
     ; Xiong et al.,
     <a class="ltx_ref" href="#bib.bib41" title="">
      2023
     </a>
     )
    </cite>
    first sample multiple initial answers, and then compare or deliberate on the differences among these answers to reach a consistent answer.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    While self-correction and diverse sampling methods show potential for improving the output reliability of LLMs, they still have the overconfidence issue
    <cite class="ltx_cite ltx_citemacro_citep">
     (Mielke et al.,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2022
     </a>
     ; Xiong et al.,
     <a class="ltx_ref" href="#bib.bib41" title="">
      2023
     </a>
     )
    </cite>
    as illustrated in Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    .
Self-correction methods may overtrust LLMs’ initially generated answers, making it difficult to effectively recognize errors
    <cite class="ltx_cite ltx_citemacro_citep">
     (Huang et al.,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2024b
     </a>
     ; Stechly et al.,
     <a class="ltx_ref" href="#bib.bib28" title="">
      2023
     </a>
     ; Valmeekam et al.,
     <a class="ltx_ref" href="#bib.bib32" title="">
      2023
     </a>
     )
    </cite>
    .
By contrast, diverse sampling methods may repeatedly generate the same incorrect answers due to LLMs’ inherent biases and beliefs
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wang et al.,
     <a class="ltx_ref" href="#bib.bib35" title="">
      2024b
     </a>
     )
    </cite>
    , limiting LLMs to contrast and deliberate on other possible answers.
We believe that a key reason for the above overconfidence issue is that these methods do not intervene in the LLMs’ answer-generation process, allowing LLMs to refine or sample diverse answers according to their own biases and beliefs.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    The main challenge in addressing the overconfidence issue is to override LLMs’ inherent biases and beliefs, compelling them to inspect answers they would not normally consider.
To achieve this, we consider presetting different stances for LLMs, allowing LLMs to imagine each answer as correct in each round of reasoning, and then generate the reasons why the answer is valid.
By overriding the LLM’s original beliefs with this new mindset, we can regulate LLMs to assess the possibility of each answer being correct.
Thereafter, we can eliminate the incorrect answers by reflecting the generated reasons for all answers.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    To this end, we propose a
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">
     C
    </span>
    ounter
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">
     F
    </span>
    actual
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">
     M
    </span>
    ulti-
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.4">
     A
    </span>
    gent
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.5">
     D
    </span>
    ebate (CFMAD) framework comprising two key stages: abduction generation and counterfactual debate. In the abduction generation stage, LLMs are tasked with producing potential correct reasons for a predetermined answer. Subsequently, in the counterfactual debate stage, a structured debate method is employed to assess these abductions and ascertain the sole correct response. Specifically, we introduce a critic who questions the validity of each generated abduction, and prompt the LLM to defend its position in a debate with the critic. The deliberation is then presented to an impartial third-party judge for final adjudication. Extensive experiments spanning fact-checking, reading comprehension, and commonsense reasoning tasks validate the effectiveness of CFMAD over existing benchmarks across four datasets.
We release our code and data at
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://anonymous.4open.science/r/CFMAD-468D/" target="_blank" title="">
     https://anonymous.4open.science/r/CFMAD-468D/
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    The contributions of this work are threefold:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We propose to preset various stances for LLMs, overriding their inherent biases and beliefs to address the overconfidence issue of LLMs.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We propose a CFMAD framework, which instructs LLMs to generate abduction with preset stances and then conduct counterfactual debate to eliminate incorrect answers.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       We conduct extensive experiments on three generative tasks with four datasets, validating the effectiveness of CFMAD.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Preliminary Experiments
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    We formulate methods for self-correction and diverse sampling, and subsequently conduct a quantitative experiment to expose the overconfidence issue prevalent in both approaches.
   </p>
  </div>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Problem Definition
   </h3>
   <section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
    <h5 class="ltx_title ltx_title_paragraph">
     Self-correction.
    </h5>
    <div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.7">
      Self-correction methods involve two steps: reflection and refinement
      <cite class="ltx_cite ltx_citemacro_citep">
       (Shinn et al.,
       <a class="ltx_ref" href="#bib.bib26" title="">
        2024
       </a>
       )
      </cite>
      .
Given a question
      <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.1.m1.1">
       <semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a">
        <mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">
         q
        </mi>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.1.m1.1b">
         <ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">
          𝑞
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">
         q
        </annotation>
       </semantics>
      </math>
      and
      <math alttext="R_{0}=LLM(q)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.2.m2.1">
       <semantics id="S2.SS1.SSS0.Px1.p1.2.m2.1a">
        <mrow id="S2.SS1.SSS0.Px1.p1.2.m2.1.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.cmml">
         <msub id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.cmml">
          <mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.2.cmml">
           R
          </mi>
          <mn id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.3" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.3.cmml">
           0
          </mn>
         </msub>
         <mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.1.cmml">
          =
         </mo>
         <mrow id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.cmml">
          <mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.2.cmml">
           L
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.1" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.1.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.3" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.3.cmml">
           L
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.1a" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.1.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.4" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.4.cmml">
           M
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.1b" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.1.cmml">
           ​
          </mo>
          <mrow id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.5.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.cmml">
           <mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.5.2.1" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.cmml">
            (
           </mo>
           <mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">
            q
           </mi>
           <mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.5.2.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.cmml">
            )
           </mo>
          </mrow>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.2.m2.1b">
         <apply id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2">
          <eq id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.1">
          </eq>
          <apply id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2">
            subscript
           </csymbol>
           <ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.2">
            𝑅
           </ci>
           <cn id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.3">
            0
           </cn>
          </apply>
          <apply id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3">
           <times id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.1">
           </times>
           <ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.2">
            𝐿
           </ci>
           <ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.3">
            𝐿
           </ci>
           <ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.4.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.4">
            𝑀
           </ci>
           <ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">
            𝑞
           </ci>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.1c">
         R_{0}=LLM(q)
        </annotation>
       </semantics>
      </math>
      representing the initial response of an LLM, self-correction methods further instruct the LLM to reflect on the initial response
      <math alttext="R_{0}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.3.m3.1">
       <semantics id="S2.SS1.SSS0.Px1.p1.3.m3.1a">
        <msub id="S2.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">
         <mi id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml">
          R
         </mi>
         <mn id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml">
          0
         </mn>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.3.m3.1b">
         <apply id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">
          <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.2">
           𝑅
          </ci>
          <cn id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.3">
           0
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.3.m3.1c">
         R_{0}
        </annotation>
       </semantics>
      </math>
      and generate feedback by
      <math alttext="F=LLM(q,R_{0})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.4.m4.2">
       <semantics id="S2.SS1.SSS0.Px1.p1.4.m4.2a">
        <mrow id="S2.SS1.SSS0.Px1.p1.4.m4.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml">
         <mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.3.cmml">
          F
         </mi>
         <mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.2.cmml">
          =
         </mo>
         <mrow id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.cmml">
          <mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.3.cmml">
           L
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.2" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.4" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.4.cmml">
           L
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.2a" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.5" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.5.cmml">
           M
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.2b" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.2.cmml">
           ​
          </mo>
          <mrow id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.2.cmml">
           <mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.2.cmml">
            (
           </mo>
           <mi id="S2.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">
            q
           </mi>
           <mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.2.cmml">
            ,
           </mo>
           <msub id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.cmml">
            <mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.2.cmml">
             R
            </mi>
            <mn id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.3.cmml">
             0
            </mn>
           </msub>
           <mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.4" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.2.cmml">
            )
           </mo>
          </mrow>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.4.m4.2b">
         <apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2">
          <eq id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.2">
          </eq>
          <ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.3">
           𝐹
          </ci>
          <apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1">
           <times id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.2">
           </times>
           <ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.3">
            𝐿
           </ci>
           <ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.4.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.4">
            𝐿
           </ci>
           <ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.5.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.5">
            𝑀
           </ci>
           <interval closure="open" id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1">
            <ci id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1">
             𝑞
            </ci>
            <apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1">
             <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1">
              subscript
             </csymbol>
             <ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.2">
              𝑅
             </ci>
             <cn id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.3">
              0
             </cn>
            </apply>
           </interval>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.4.m4.2c">
         F=LLM(q,R_{0})
        </annotation>
       </semantics>
      </math>
      .
Given
      <math alttext="R_{0}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.5.m5.1">
       <semantics id="S2.SS1.SSS0.Px1.p1.5.m5.1a">
        <msub id="S2.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">
         <mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml">
          R
         </mi>
         <mn id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml">
          0
         </mn>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.5.m5.1b">
         <apply id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1">
          <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2">
           𝑅
          </ci>
          <cn id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3">
           0
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.5.m5.1c">
         R_{0}
        </annotation>
       </semantics>
      </math>
      and
      <math alttext="F" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.6.m6.1">
       <semantics id="S2.SS1.SSS0.Px1.p1.6.m6.1a">
        <mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">
         F
        </mi>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.6.m6.1b">
         <ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1">
          𝐹
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.6.m6.1c">
         F
        </annotation>
       </semantics>
      </math>
      , the LLM then generates a revised answer in the refinement stage, denoted as
      <math alttext="R_{1}=LLM(q,R_{0},F)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.7.m7.3">
       <semantics id="S2.SS1.SSS0.Px1.p1.7.m7.3a">
        <mrow id="S2.SS1.SSS0.Px1.p1.7.m7.3.3" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.cmml">
         <msub id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.cmml">
          <mi id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.2" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.2.cmml">
           R
          </mi>
          <mn id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.3" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.3.cmml">
           1
          </mn>
         </msub>
         <mo id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.2" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.2.cmml">
          =
         </mo>
         <mrow id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.cmml">
          <mi id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.3" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.3.cmml">
           L
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.2" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.2.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.4" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.4.cmml">
           L
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.2a" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.2.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.5" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.5.cmml">
           M
          </mi>
          <mo id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.2b" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.2.cmml">
           ​
          </mo>
          <mrow id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.2.cmml">
           <mo id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.2.cmml">
            (
           </mo>
           <mi id="S2.SS1.SSS0.Px1.p1.7.m7.1.1" xref="S2.SS1.SSS0.Px1.p1.7.m7.1.1.cmml">
            q
           </mi>
           <mo id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.2.cmml">
            ,
           </mo>
           <msub id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.cmml">
            <mi id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.2" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.2.cmml">
             R
            </mi>
            <mn id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.3.cmml">
             0
            </mn>
           </msub>
           <mo id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.4" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.2.cmml">
            ,
           </mo>
           <mi id="S2.SS1.SSS0.Px1.p1.7.m7.2.2" xref="S2.SS1.SSS0.Px1.p1.7.m7.2.2.cmml">
            F
           </mi>
           <mo id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.5" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.2.cmml">
            )
           </mo>
          </mrow>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.7.m7.3b">
         <apply id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3">
          <eq id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.2">
          </eq>
          <apply id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3">
            subscript
           </csymbol>
           <ci id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.2">
            𝑅
           </ci>
           <cn id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.3.3">
            1
           </cn>
          </apply>
          <apply id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1">
           <times id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.2">
           </times>
           <ci id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.3.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.3">
            𝐿
           </ci>
           <ci id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.4.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.4">
            𝐿
           </ci>
           <ci id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.5.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.5">
            𝑀
           </ci>
           <vector id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1">
            <ci id="S2.SS1.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.1.1">
             𝑞
            </ci>
            <apply id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1">
             <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1">
              subscript
             </csymbol>
             <ci id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.2">
              𝑅
             </ci>
             <cn id="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.7.m7.3.3.1.1.1.1.3">
              0
             </cn>
            </apply>
            <ci id="S2.SS1.SSS0.Px1.p1.7.m7.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.7.m7.2.2">
             𝐹
            </ci>
           </vector>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.7.m7.3c">
         R_{1}=LLM(q,R_{0},F)
        </annotation>
       </semantics>
      </math>
      .
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
    <h5 class="ltx_title ltx_title_paragraph">
     Diverse Sampling.
    </h5>
    <div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.5">
      Diverse Sampling methods usually involve three steps: sampling, deliberation, and judging
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zhang et al.,
       <a class="ltx_ref" href="#bib.bib46" title="">
        2024b
       </a>
       )
      </cite>
      .
First,
      <math alttext="N" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.1.m1.1">
       <semantics id="S2.SS1.SSS0.Px2.p1.1.m1.1a">
        <mi id="S2.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">
         N
        </mi>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.1.m1.1b">
         <ci id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1">
          𝑁
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.1.m1.1c">
         N
        </annotation>
       </semantics>
      </math>
      initial responses are sampled by:
      <math alttext="R_{0}^{i}=LLM(q,\theta_{i}),i\in[1,N]" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.2.m2.5">
       <semantics id="S2.SS1.SSS0.Px2.p1.2.m2.5a">
        <mrow id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.3.cmml">
         <mrow id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.cmml">
          <msubsup id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.cmml">
           <mi id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.2.cmml">
            R
           </mi>
           <mn id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.3.cmml">
            0
           </mn>
           <mi id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.3.cmml">
            i
           </mi>
          </msubsup>
          <mo id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.2.cmml">
           =
          </mo>
          <mrow id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.cmml">
           <mi id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.3.cmml">
            L
           </mi>
           <mo id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.2" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.2.cmml">
            ​
           </mo>
           <mi id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.4" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.4.cmml">
            L
           </mi>
           <mo id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.2a" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.2.cmml">
            ​
           </mo>
           <mi id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.5" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.5.cmml">
            M
           </mi>
           <mo id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.2b" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.2.cmml">
            ​
           </mo>
           <mrow id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.2.cmml">
            <mo id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.2" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.2.cmml">
             (
            </mo>
            <mi id="S2.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">
             q
            </mi>
            <mo id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.2.cmml">
             ,
            </mo>
            <msub id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.cmml">
             <mi id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.2.cmml">
              θ
             </mi>
             <mi id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.3.cmml">
              i
             </mi>
            </msub>
            <mo id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.4" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.2.cmml">
             )
            </mo>
           </mrow>
          </mrow>
         </mrow>
         <mo id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.3a.cmml">
          ,
         </mo>
         <mrow id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.cmml">
          <mi id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.2.cmml">
           i
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.1" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.1.cmml">
           ∈
          </mo>
          <mrow id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.1.cmml">
           <mo id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.2.1" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.1.cmml">
            [
           </mo>
           <mn id="S2.SS1.SSS0.Px2.p1.2.m2.2.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.2.2.cmml">
            1
           </mn>
           <mo id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.2.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.1.cmml">
            ,
           </mo>
           <mi id="S2.SS1.SSS0.Px2.p1.2.m2.3.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.3.3.cmml">
            N
           </mi>
           <mo id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.2.3" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.1.cmml">
            ]
           </mo>
          </mrow>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.2.m2.5b">
         <apply id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.3.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2">
          <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.3a.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.3">
           formulae-sequence
          </csymbol>
          <apply id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1">
           <eq id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.2">
           </eq>
           <apply id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3">
            <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3">
             superscript
            </csymbol>
            <apply id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3">
             <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3">
              subscript
             </csymbol>
             <ci id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.2">
              𝑅
             </ci>
             <cn id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.2.3">
              0
             </cn>
            </apply>
            <ci id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.3.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.3.3">
             𝑖
            </ci>
           </apply>
           <apply id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1">
            <times id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.2">
            </times>
            <ci id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.3">
             𝐿
            </ci>
            <ci id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.4.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.4">
             𝐿
            </ci>
            <ci id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.5.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.5">
             𝑀
            </ci>
            <interval closure="open" id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1">
             <ci id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1">
              𝑞
             </ci>
             <apply id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1">
              <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1">
               subscript
              </csymbol>
              <ci id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.2">
               𝜃
              </ci>
              <ci id="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.4.4.1.1.1.1.1.1.3">
               𝑖
              </ci>
             </apply>
            </interval>
           </apply>
          </apply>
          <apply id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2">
           <in id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.1">
           </in>
           <ci id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.2">
            𝑖
           </ci>
           <interval closure="closed" id="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.5.5.2.2.3.2">
            <cn id="S2.SS1.SSS0.Px2.p1.2.m2.2.2.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.2.m2.2.2">
             1
            </cn>
            <ci id="S2.SS1.SSS0.Px2.p1.2.m2.3.3.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.3.3">
             𝑁
            </ci>
           </interval>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.2.m2.5c">
         R_{0}^{i}=LLM(q,\theta_{i}),i\in[1,N]
        </annotation>
       </semantics>
      </math>
      .
Here
      <math alttext="\theta_{i}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.3.m3.1">
       <semantics id="S2.SS1.SSS0.Px2.p1.3.m3.1a">
        <msub id="S2.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1.cmml">
         <mi id="S2.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">
          θ
         </mi>
         <mi id="S2.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.3.m3.1b">
         <apply id="S2.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1">
          <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1.2">
           𝜃
          </ci>
          <ci id="S2.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.3.m3.1c">
         \theta_{i}
        </annotation>
       </semantics>
      </math>
      represents settings such as improving temperature or using different prompts, which are widely used in diverse sampling to enhance the diversity of responses.
In the following deliberation stage, each response is refined by contrasting with other responses, thereby improving the initial responses:
      <math alttext="R_{1}^{i}=LLM(q,R_{0}^{i},\{R_{0}^{j\neq i}\})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.4.m4.3">
       <semantics id="S2.SS1.SSS0.Px2.p1.4.m4.3a">
        <mrow id="S2.SS1.SSS0.Px2.p1.4.m4.3.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.cmml">
         <msubsup id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.cmml">
          <mi id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.2" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.2.cmml">
           R
          </mi>
          <mn id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.3.cmml">
           1
          </mn>
          <mi id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.3.cmml">
           i
          </mi>
         </msubsup>
         <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.3.cmml">
          =
         </mo>
         <mrow id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.cmml">
          <mi id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.4" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.4.cmml">
           L
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.3" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.3.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.5" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.5.cmml">
           L
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.3a" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.3.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.6" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.6.cmml">
           M
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.3b" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.3.cmml">
           ​
          </mo>
          <mrow id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.3.cmml">
           <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.3" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.3.cmml">
            (
           </mo>
           <mi id="S2.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1.cmml">
            q
           </mi>
           <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.4" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.3.cmml">
            ,
           </mo>
           <msubsup id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.cmml">
            <mi id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.2" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.2.cmml">
             R
            </mi>
            <mn id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.3.cmml">
             0
            </mn>
            <mi id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.3.cmml">
             i
            </mi>
           </msubsup>
           <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.5" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.3.cmml">
            ,
           </mo>
           <mrow id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.2.cmml">
            <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.2" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.2.cmml">
             {
            </mo>
            <msubsup id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.cmml">
             <mi id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.2" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.2.cmml">
              R
             </mi>
             <mn id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.3.cmml">
              0
             </mn>
             <mrow id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.cmml">
              <mi id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.2" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.2.cmml">
               j
              </mi>
              <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.1" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.1.cmml">
               ≠
              </mo>
              <mi id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.3.cmml">
               i
              </mi>
             </mrow>
            </msubsup>
            <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.3" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.2.cmml">
             }
            </mo>
           </mrow>
           <mo id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.6" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.3.cmml">
            )
           </mo>
          </mrow>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.4.m4.3b">
         <apply id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3">
          <eq id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.3">
          </eq>
          <apply id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4">
            superscript
           </csymbol>
           <apply id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4">
            <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4">
             subscript
            </csymbol>
            <ci id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.2">
             𝑅
            </ci>
            <cn id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.2.3">
             1
            </cn>
           </apply>
           <ci id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.4.3">
            𝑖
           </ci>
          </apply>
          <apply id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2">
           <times id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.3">
           </times>
           <ci id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.4.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.4">
            𝐿
           </ci>
           <ci id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.5.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.5">
            𝐿
           </ci>
           <ci id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.6.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.6">
            𝑀
           </ci>
           <vector id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2">
            <ci id="S2.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1">
             𝑞
            </ci>
            <apply id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1">
             <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1">
              superscript
             </csymbol>
             <apply id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1">
              <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1">
               subscript
              </csymbol>
              <ci id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.2">
               𝑅
              </ci>
              <cn id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.2.3">
               0
              </cn>
             </apply>
             <ci id="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.2.2.1.1.1.1.3">
              𝑖
             </ci>
            </apply>
            <set id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1">
             <apply id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1">
              <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1">
               superscript
              </csymbol>
              <apply id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1">
               <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1">
                subscript
               </csymbol>
               <ci id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.2">
                𝑅
               </ci>
               <cn id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.2.3">
                0
               </cn>
              </apply>
              <apply id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3">
               <neq id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.1">
               </neq>
               <ci id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.2">
                𝑗
               </ci>
               <ci id="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.3.3.2.2.2.2.1.1.3.3">
                𝑖
               </ci>
              </apply>
             </apply>
            </set>
           </vector>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.4.m4.3c">
         R_{1}^{i}=LLM(q,R_{0}^{i},\{R_{0}^{j\neq i}\})
        </annotation>
       </semantics>
      </math>
      .
Finally, a judging process is employed to determine the final answer
      <math alttext="R_{f}=judge(R_{1}^{1},R_{1}^{2},...,R_{1}^{N})" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.5.m5.4">
       <semantics id="S2.SS1.SSS0.Px2.p1.5.m5.4a">
        <mrow id="S2.SS1.SSS0.Px2.p1.5.m5.4.4" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.cmml">
         <msub id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.cmml">
          <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.2" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.2.cmml">
           R
          </mi>
          <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.3.cmml">
           f
          </mi>
         </msub>
         <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.4" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.4.cmml">
          =
         </mo>
         <mrow id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.cmml">
          <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.5" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.5.cmml">
           j
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.6" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.6.cmml">
           u
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4a" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.7" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.7.cmml">
           d
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4b" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.8" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.8.cmml">
           g
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4c" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4.cmml">
           ​
          </mo>
          <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.9" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.9.cmml">
           e
          </mi>
          <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4d" lspace="0em" rspace="0em" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4.cmml">
           ​
          </mo>
          <mrow id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.4.cmml">
           <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.4" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.4.cmml">
            (
           </mo>
           <msubsup id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.cmml">
            <mi id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.2" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.2.cmml">
             R
            </mi>
            <mn id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.3.cmml">
             1
            </mn>
            <mn id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.3.cmml">
             1
            </mn>
           </msubsup>
           <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.5" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.4.cmml">
            ,
           </mo>
           <msubsup id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.cmml">
            <mi id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.2" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.2.cmml">
             R
            </mi>
            <mn id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.3.cmml">
             1
            </mn>
            <mn id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.3.cmml">
             2
            </mn>
           </msubsup>
           <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.6" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.4.cmml">
            ,
           </mo>
           <mi id="S2.SS1.SSS0.Px2.p1.5.m5.1.1" mathvariant="normal" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.cmml">
            …
           </mi>
           <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.7" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.4.cmml">
            ,
           </mo>
           <msubsup id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.cmml">
            <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.2" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.2.cmml">
             R
            </mi>
            <mn id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.3.cmml">
             1
            </mn>
            <mi id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.3.cmml">
             N
            </mi>
           </msubsup>
           <mo id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.8" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.4.cmml">
            )
           </mo>
          </mrow>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.5.m5.4b">
         <apply id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4">
          <eq id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.4.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.4">
          </eq>
          <apply id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5">
            subscript
           </csymbol>
           <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.2">
            𝑅
           </ci>
           <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.3.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.5.3">
            𝑓
           </ci>
          </apply>
          <apply id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3">
           <times id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.4">
           </times>
           <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.5.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.5">
            𝑗
           </ci>
           <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.6.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.6">
            𝑢
           </ci>
           <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.7.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.7">
            𝑑
           </ci>
           <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.8.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.8">
            𝑔
           </ci>
           <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.9.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.9">
            𝑒
           </ci>
           <vector id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.4.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3">
            <apply id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1">
             <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1">
              superscript
             </csymbol>
             <apply id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1">
              <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1">
               subscript
              </csymbol>
              <ci id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.2">
               𝑅
              </ci>
              <cn id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.2.3">
               1
              </cn>
             </apply>
             <cn id="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.1.3">
              1
             </cn>
            </apply>
            <apply id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2">
             <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2">
              superscript
             </csymbol>
             <apply id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2">
              <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2">
               subscript
              </csymbol>
              <ci id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.2">
               𝑅
              </ci>
              <cn id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.2.3">
               1
              </cn>
             </apply>
             <cn id="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.2.3">
              2
             </cn>
            </apply>
            <ci id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1">
             …
            </ci>
            <apply id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3">
             <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3">
              superscript
             </csymbol>
             <apply id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3">
              <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3">
               subscript
              </csymbol>
              <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.2">
               𝑅
              </ci>
              <cn id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.2.3">
               1
              </cn>
             </apply>
             <ci id="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.3.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.3.3">
              𝑁
             </ci>
            </apply>
           </vector>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.5.m5.4c">
         R_{f}=judge(R_{1}^{1},R_{1}^{2},...,R_{1}^{N})
        </annotation>
       </semantics>
      </math>
      .
     </p>
    </div>
    <div class="ltx_para" id="S2.SS1.SSS0.Px2.p2">
     <p class="ltx_p" id="S2.SS1.SSS0.Px2.p2.4">
      However, the LLM with self-correction or diverse sampling face the issue of overconfidence. Formally, the LLM with self-correction tends to overtrust the initial response
      <math alttext="R_{0}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.1.m1.1">
       <semantics id="S2.SS1.SSS0.Px2.p2.1.m1.1a">
        <msub id="S2.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1.cmml">
         <mi id="S2.SS1.SSS0.Px2.p2.1.m1.1.1.2" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml">
          R
         </mi>
         <mn id="S2.SS1.SSS0.Px2.p2.1.m1.1.1.3" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml">
          0
         </mn>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.1.m1.1b">
         <apply id="S2.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1">
          <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1.2">
           𝑅
          </ci>
          <cn id="S2.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1.3">
           0
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.1.m1.1c">
         R_{0}
        </annotation>
       </semantics>
      </math>
      , resulting in
      <math alttext="R_{1}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.2.m2.1">
       <semantics id="S2.SS1.SSS0.Px2.p2.2.m2.1a">
        <msub id="S2.SS1.SSS0.Px2.p2.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1.cmml">
         <mi id="S2.SS1.SSS0.Px2.p2.2.m2.1.1.2" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1.2.cmml">
          R
         </mi>
         <mn id="S2.SS1.SSS0.Px2.p2.2.m2.1.1.3" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1.3.cmml">
          1
         </mn>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.2.m2.1b">
         <apply id="S2.SS1.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1">
          <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS1.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1.2">
           𝑅
          </ci>
          <cn id="S2.SS1.SSS0.Px2.p2.2.m2.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1.3">
           1
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.2.m2.1c">
         R_{1}
        </annotation>
       </semantics>
      </math>
      having the same error as
      <math alttext="R_{0}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.3.m3.1">
       <semantics id="S2.SS1.SSS0.Px2.p2.3.m3.1a">
        <msub id="S2.SS1.SSS0.Px2.p2.3.m3.1.1" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1.cmml">
         <mi id="S2.SS1.SSS0.Px2.p2.3.m3.1.1.2" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1.2.cmml">
          R
         </mi>
         <mn id="S2.SS1.SSS0.Px2.p2.3.m3.1.1.3" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1.3.cmml">
          0
         </mn>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.3.m3.1b">
         <apply id="S2.SS1.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1">
          <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS1.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1.2">
           𝑅
          </ci>
          <cn id="S2.SS1.SSS0.Px2.p2.3.m3.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1.3">
           0
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.3.m3.1c">
         R_{0}
        </annotation>
       </semantics>
      </math>
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zhang et al.,
       <a class="ltx_ref" href="#bib.bib46" title="">
        2024b
       </a>
       )
      </cite>
      . Meanwhile, for diverse sampling, the incorrect answer might repeat in
      <math alttext="\{R_{0}^{1},R_{0}^{2},...,R_{0}^{N}\}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p2.4.m4.4">
       <semantics id="S2.SS1.SSS0.Px2.p2.4.m4.4a">
        <mrow id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.4.cmml">
         <mo id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.4" stretchy="false" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.4.cmml">
          {
         </mo>
         <msubsup id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.cmml">
          <mi id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.2" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.2.cmml">
           R
          </mi>
          <mn id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.3" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.3.cmml">
           0
          </mn>
          <mn id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.3" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.3.cmml">
           1
          </mn>
         </msubsup>
         <mo id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.5" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.4.cmml">
          ,
         </mo>
         <msubsup id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.cmml">
          <mi id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.2" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.2.cmml">
           R
          </mi>
          <mn id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.3" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.3.cmml">
           0
          </mn>
          <mn id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.3" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.3.cmml">
           2
          </mn>
         </msubsup>
         <mo id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.6" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.4.cmml">
          ,
         </mo>
         <mi id="S2.SS1.SSS0.Px2.p2.4.m4.1.1" mathvariant="normal" xref="S2.SS1.SSS0.Px2.p2.4.m4.1.1.cmml">
          …
         </mi>
         <mo id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.7" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.4.cmml">
          ,
         </mo>
         <msubsup id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.cmml">
          <mi id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.2" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.2.cmml">
           R
          </mi>
          <mn id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.3" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.3.cmml">
           0
          </mn>
          <mi id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.3" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.3.cmml">
           N
          </mi>
         </msubsup>
         <mo id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.8" stretchy="false" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.4.cmml">
          }
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.4.m4.4b">
         <set id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.4.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3">
          <apply id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1">
            superscript
           </csymbol>
           <apply id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1">
            <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1">
             subscript
            </csymbol>
            <ci id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.2">
             𝑅
            </ci>
            <cn id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.2.3">
             0
            </cn>
           </apply>
           <cn id="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p2.4.m4.2.2.1.1.3">
            1
           </cn>
          </apply>
          <apply id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2">
            superscript
           </csymbol>
           <apply id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2">
            <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2">
             subscript
            </csymbol>
            <ci id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.2">
             𝑅
            </ci>
            <cn id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.2.3">
             0
            </cn>
           </apply>
           <cn id="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p2.4.m4.3.3.2.2.3">
            2
           </cn>
          </apply>
          <ci id="S2.SS1.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.1.1">
           …
          </ci>
          <apply id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3">
           <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3">
            superscript
           </csymbol>
           <apply id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3">
            <csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3">
             subscript
            </csymbol>
            <ci id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.2">
             𝑅
            </ci>
            <cn id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.2.3">
             0
            </cn>
           </apply>
           <ci id="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.3.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.4.4.3.3.3">
            𝑁
           </ci>
          </apply>
         </set>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.4.m4.4c">
         \{R_{0}^{1},R_{0}^{2},...,R_{0}^{N}\}
        </annotation>
       </semantics>
      </math>
      , resulting in the deliberation and judging stages potentially accepting such an incorrect answer.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Investigation of Overconfidence
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     To investigate the overconfidence issue, we conduct some preliminary experiments on the representative self-correction and diverse sampling methods.
    </p>
   </div>
   <section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
    <h5 class="ltx_title ltx_title_paragraph">
     Testing Methods.
    </h5>
    <div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
     <p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">
      We evaluate four representative methods and count the number of testing samples exhibiting the overconfidence issue as follows:
     </p>
     <ul class="ltx_itemize" id="S2.I1">
      <li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S2.I1.i1.p1">
        <p class="ltx_p" id="S2.I1.i1.p1.1">
         <span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">
          Self-reflection
         </span>
         <cite class="ltx_cite ltx_citemacro_citep">
          (Shinn et al.,
          <a class="ltx_ref" href="#bib.bib26" title="">
           2024
          </a>
          )
         </cite>
         : This method instructs the LLMs to reflect on an initial answer and subsequently provide feedback, asking the LLM to refine and generate a revised response based on this feedback.
If the revised answer for a testing sample remains the same as the initial incorrect response, we treat it as a sample with the overconfidence issue.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S2.I1.i2.p1">
        <p class="ltx_p" id="S2.I1.i2.p1.1">
         <span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">
          Self-consistency
         </span>
         <cite class="ltx_cite ltx_citemacro_citep">
          (Wang et al.,
          <a class="ltx_ref" href="#bib.bib36" title="">
           2023
          </a>
          )
         </cite>
         : This approach samples multiple initial answers using the same prompts, followed by a voting process to determine the final answer.
We implement it by sampling seven initial answers and consider a test sample as an overconfidence sample if six out of the seven responses are identically incorrect.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S2.I1.i3.p1">
        <p class="ltx_p" id="S2.I1.i3.p1.1">
         <span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">
          Self-contrast
         </span>
         <cite class="ltx_cite ltx_citemacro_citep">
          (Zhang et al.,
          <a class="ltx_ref" href="#bib.bib46" title="">
           2024b
          </a>
          )
         </cite>
         : In this method, three initial answers are generated by the LLMs using self-generated, varying prompts.
These answers are then contrasted to derive the final answer.
If all three initial responses are the same incorrect answers for a given testing sample, it is regarded as an overconfidence sample.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S2.I1.i4.p1">
        <p class="ltx_p" id="S2.I1.i4.p1.1">
         <span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">
          MAD
         </span>
         <cite class="ltx_cite ltx_citemacro_citep">
          (Du et al.,
          <a class="ltx_ref" href="#bib.bib7" title="">
           2023
          </a>
          )
         </cite>
         : This strategy involves sampling multiple initial answers from different agents and using a debate process to decide on the final answer.
Similarly, an overconfidence sample is defined as that three initial responses are the same and incorrect.
        </p>
       </div>
      </li>
     </ul>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
    <h5 class="ltx_title ltx_title_paragraph">
     Results.
    </h5>
    <div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
     <p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">
      We assess the overconfidence issue by applying these methods to a representative LLM, GPT-3.5-turbo
      <cite class="ltx_cite ltx_citemacro_citep">
       (Ouyang et al.,
       <a class="ltx_ref" href="#bib.bib24" title="">
        2022
       </a>
       )
      </cite>
      , on the CommonsenseQA
      <cite class="ltx_cite ltx_citemacro_citep">
       (Talmor et al.,
       <a class="ltx_ref" href="#bib.bib29" title="">
        2019
       </a>
       )
      </cite>
      and Hover
      <cite class="ltx_cite ltx_citemacro_citep">
       (Jiang et al.,
       <a class="ltx_ref" href="#bib.bib13" title="">
        2020
       </a>
       )
      </cite>
      datasets.
We calculate the proportion of incorrect answers attributable to overconfidence among all incorrect cases.
As shown in Figure
      <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ Results. ‣ 2.2 Investigation of Overconfidence ‣ 2 Preliminary Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      , for self-reflection, MAD, and Self-Contrast, more than half of the errors are caused by overconfidence.
For Self-consistency, although the overconfidence issue is alleviated due to the increase in sample number and temperature, approximately 40% of the errors are still caused by the overconfidence of LLMs. This validates the severity of the overconfidence issue in existing self-correction and diverse sampling methods.
     </p>
    </div>
    <div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
     <p class="ltx_p" id="S2.SS2.SSS0.Px2.p2.1">
      A key reason for the overconfidence issue of LLMs might be that self-correction and diverse sampling methods do not intervene in the LLM’s answer-generation process, permitting LLMs to refine and sample diverse answers based on their inherent biases and beliefs.
Consequently, LLMs tend to trust the initial incorrect answer, hindering the consideration of alternative potential answers.
     </p>
    </div>
    <figure class="ltx_figure" id="S2.F2">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="346" id="S2.F2.g1" src="/html/2406.11514/assets/x2.png" width="461"/>
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_figure">
       Figure 2:
      </span>
      Proportion of the overconfident answers among all incorrect answers.
     </figcaption>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Counterfactual Multi-agent Debate
  </h2>
  <figure class="ltx_figure" id="S3.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="345" id="S3.F3.g1" src="/html/2406.11514/assets/x3.png" width="350"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 3:
    </span>
    Illustration of CFMAD framework with two stages. In the abduction generation stage, we initialize multiple LLM agents, each configured to assume a specific answer is correct and to generate supporting abductions. In the subsequent counterfactual debate stage, each agent is challenged by a critical evaluator for debating. The debating processes are assessed by a third-party judge for final adjudication.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    To address the overconfidence issue, the key lies in overriding the inherent biases and beliefs of LLMs for answer generation.
To achieve this,
we consider initially configuring the LLMs with various stances, allowing them to hypothesize the correctness of each possible answer and uncover the underlying rationale of each answer. This approach compels LLMs to inspect all potential answers, liberating them from inherent biases and beliefs. Subsequently, we can critically assess the potential rationales to identify the correct answer.
   </p>
  </div>
  <div class="ltx_para" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    To this end, we propose a CFMAD framework comprising two sequential stages: abduction generation and counterfactual debate, depicted in Figure
    <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3 Counterfactual Multi-agent Debate ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    .
In the abduction generation phase, we initialize multiple LLM agents and configure each one to adopt a predetermined stance, assuming a specified answer is correct. Subsequently, these agents are instructed to generate abductions,
    <em class="ltx_emph ltx_font_italic" id="S3.p2.1.1">
     i.e.,
    </em>
    potential correct reasons for the given answer.
In the counterfactual debate phase, we create an adversarial debate scenario. Each abducting agent, adopting a predetermined answer as correct, faces a critical evaluator tasked with challenging the validity of the abductions generated by the agent. Meanwhile, the abducting agent is directed to defend its position on the abduction correctness.
Eventually, the deliberations between each agent-critic pair are presented to a third-party judge to deliver the final adjudication.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Abduction Generation
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.2">
     Prior studies have illustrated that LLMs exhibit proficiency in counterfactual reasoning
     <cite class="ltx_cite ltx_citemacro_cite">
      Nguyen et al. (
      <a class="ltx_ref" href="#bib.bib23" title="">
       2024
      </a>
      ); Bhattacharjee et al. (
      <a class="ltx_ref" href="#bib.bib4" title="">
       2024
      </a>
      )
     </cite>
     , thereby allowing them to engage in reasoning with predetermined stances.
Specifically, given a possible answer
     <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1">
      <semantics id="S3.SS1.p1.1.m1.1a">
       <msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">
        <mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">
         a
        </mi>
        <mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b">
        <apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">
          𝑎
         </ci>
         <ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">
        a_{i}
       </annotation>
      </semantics>
     </math>
     , we preset the LLMs’ stance with
     <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1">
      <semantics id="S3.SS1.p1.2.m2.1a">
       <msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">
        <mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">
         a
        </mi>
        <mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b">
        <apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">
          𝑎
         </ci>
         <ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">
        a_{i}
       </annotation>
      </semantics>
     </math>
     by the following prompt:
    </p>
    <p class="ltx_p ltx_align_center" id="S3.SS1.p1.4.2">
     <span class="ltx_text ltx_framed ltx_framed_rectangle" id="S3.SS1.p1.4.2.2" style="border-color: #000000;">
      <span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S3.SS1.p1.4.2.2.2" style="width:424.9pt;background-color:#F7F7F7;">
       <span class="ltx_p" id="S3.SS1.p1.4.2.2.2.2">
        Why is
        <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.1.1.1.1.m1.1" style="background-color:#F7F7F7;">
         <semantics id="S3.SS1.p1.3.1.1.1.1.m1.1a">
          <msub id="S3.SS1.p1.3.1.1.1.1.m1.1.1" xref="S3.SS1.p1.3.1.1.1.1.m1.1.1.cmml">
           <mi id="S3.SS1.p1.3.1.1.1.1.m1.1.1.2" mathbackground="#F7F7F7" xref="S3.SS1.p1.3.1.1.1.1.m1.1.1.2.cmml">
            a
           </mi>
           <mi id="S3.SS1.p1.3.1.1.1.1.m1.1.1.3" mathbackground="#F7F7F7" xref="S3.SS1.p1.3.1.1.1.1.m1.1.1.3.cmml">
            i
           </mi>
          </msub>
          <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.1.1.1.1.m1.1b">
           <apply id="S3.SS1.p1.3.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.p1.3.1.1.1.1.m1.1.1">
            <csymbol cd="ambiguous" id="S3.SS1.p1.3.1.1.1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.3.1.1.1.1.m1.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS1.p1.3.1.1.1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.3.1.1.1.1.m1.1.1.2">
             𝑎
            </ci>
            <ci id="S3.SS1.p1.3.1.1.1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.3.1.1.1.1.m1.1.1.3">
             𝑖
            </ci>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.SS1.p1.3.1.1.1.1.m1.1c">
           a_{i}
          </annotation>
         </semantics>
        </math>
        the correct answer? Your answer should look like this: The answer is
        <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.2.2.2.2.m2.1" style="background-color:#F7F7F7;">
         <semantics id="S3.SS1.p1.4.2.2.2.2.m2.1a">
          <msub id="S3.SS1.p1.4.2.2.2.2.m2.1.1" xref="S3.SS1.p1.4.2.2.2.2.m2.1.1.cmml">
           <mi id="S3.SS1.p1.4.2.2.2.2.m2.1.1.2" mathbackground="#F7F7F7" xref="S3.SS1.p1.4.2.2.2.2.m2.1.1.2.cmml">
            a
           </mi>
           <mi id="S3.SS1.p1.4.2.2.2.2.m2.1.1.3" mathbackground="#F7F7F7" xref="S3.SS1.p1.4.2.2.2.2.m2.1.1.3.cmml">
            i
           </mi>
          </msub>
          <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.2.2.2.2.m2.1b">
           <apply id="S3.SS1.p1.4.2.2.2.2.m2.1.1.cmml" xref="S3.SS1.p1.4.2.2.2.2.m2.1.1">
            <csymbol cd="ambiguous" id="S3.SS1.p1.4.2.2.2.2.m2.1.1.1.cmml" xref="S3.SS1.p1.4.2.2.2.2.m2.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS1.p1.4.2.2.2.2.m2.1.1.2.cmml" xref="S3.SS1.p1.4.2.2.2.2.m2.1.1.2">
             𝑎
            </ci>
            <ci id="S3.SS1.p1.4.2.2.2.2.m2.1.1.3.cmml" xref="S3.SS1.p1.4.2.2.2.2.m2.1.1.3">
             𝑖
            </ci>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.SS1.p1.4.2.2.2.2.m2.1c">
           a_{i}
          </annotation>
         </semantics>
        </math>
        because …
       </span>
      </span>
     </span>
    </p>
    <p class="ltx_p" id="S3.SS1.p1.5">
     Even if
     <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m1.1">
      <semantics id="S3.SS1.p1.5.m1.1a">
       <msub id="S3.SS1.p1.5.m1.1.1" xref="S3.SS1.p1.5.m1.1.1.cmml">
        <mi id="S3.SS1.p1.5.m1.1.1.2" xref="S3.SS1.p1.5.m1.1.1.2.cmml">
         a
        </mi>
        <mi id="S3.SS1.p1.5.m1.1.1.3" xref="S3.SS1.p1.5.m1.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m1.1b">
        <apply id="S3.SS1.p1.5.m1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p1.5.m1.1.1.1.cmml" xref="S3.SS1.p1.5.m1.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p1.5.m1.1.1.2.cmml" xref="S3.SS1.p1.5.m1.1.1.2">
          𝑎
         </ci>
         <ci id="S3.SS1.p1.5.m1.1.1.3.cmml" xref="S3.SS1.p1.5.m1.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.5.m1.1c">
        a_{i}
       </annotation>
      </semantics>
     </math>
     is incorrect, the LLM can still follow our instructions to perform counterfactual reasoning to generate plausible justifications.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.3">
     Drawing from this insight, CFMAD assigns the LLM agents the task of generating abductions for each potential answer. Concretely, as depicted in Figure
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3 Counterfactual Multi-agent Debate ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     , when presented with a set of possible answers
     <math alttext="\{a_{1},a_{2},...,a_{M}\}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.4">
      <semantics id="S3.SS1.p2.1.m1.4a">
       <mrow id="S3.SS1.p2.1.m1.4.4.3" xref="S3.SS1.p2.1.m1.4.4.4.cmml">
        <mo id="S3.SS1.p2.1.m1.4.4.3.4" stretchy="false" xref="S3.SS1.p2.1.m1.4.4.4.cmml">
         {
        </mo>
        <msub id="S3.SS1.p2.1.m1.2.2.1.1" xref="S3.SS1.p2.1.m1.2.2.1.1.cmml">
         <mi id="S3.SS1.p2.1.m1.2.2.1.1.2" xref="S3.SS1.p2.1.m1.2.2.1.1.2.cmml">
          a
         </mi>
         <mn id="S3.SS1.p2.1.m1.2.2.1.1.3" xref="S3.SS1.p2.1.m1.2.2.1.1.3.cmml">
          1
         </mn>
        </msub>
        <mo id="S3.SS1.p2.1.m1.4.4.3.5" xref="S3.SS1.p2.1.m1.4.4.4.cmml">
         ,
        </mo>
        <msub id="S3.SS1.p2.1.m1.3.3.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.cmml">
         <mi id="S3.SS1.p2.1.m1.3.3.2.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.2.cmml">
          a
         </mi>
         <mn id="S3.SS1.p2.1.m1.3.3.2.2.3" xref="S3.SS1.p2.1.m1.3.3.2.2.3.cmml">
          2
         </mn>
        </msub>
        <mo id="S3.SS1.p2.1.m1.4.4.3.6" xref="S3.SS1.p2.1.m1.4.4.4.cmml">
         ,
        </mo>
        <mi id="S3.SS1.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p2.1.m1.1.1.cmml">
         …
        </mi>
        <mo id="S3.SS1.p2.1.m1.4.4.3.7" xref="S3.SS1.p2.1.m1.4.4.4.cmml">
         ,
        </mo>
        <msub id="S3.SS1.p2.1.m1.4.4.3.3" xref="S3.SS1.p2.1.m1.4.4.3.3.cmml">
         <mi id="S3.SS1.p2.1.m1.4.4.3.3.2" xref="S3.SS1.p2.1.m1.4.4.3.3.2.cmml">
          a
         </mi>
         <mi id="S3.SS1.p2.1.m1.4.4.3.3.3" xref="S3.SS1.p2.1.m1.4.4.3.3.3.cmml">
          M
         </mi>
        </msub>
        <mo id="S3.SS1.p2.1.m1.4.4.3.8" stretchy="false" xref="S3.SS1.p2.1.m1.4.4.4.cmml">
         }
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.4b">
        <set id="S3.SS1.p2.1.m1.4.4.4.cmml" xref="S3.SS1.p2.1.m1.4.4.3">
         <apply id="S3.SS1.p2.1.m1.2.2.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1">
          <csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.2">
           𝑎
          </ci>
          <cn id="S3.SS1.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.2.2.1.1.3">
           1
          </cn>
         </apply>
         <apply id="S3.SS1.p2.1.m1.3.3.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2">
          <csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2">
           subscript
          </csymbol>
          <ci id="S3.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2.2">
           𝑎
          </ci>
          <cn id="S3.SS1.p2.1.m1.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.3.3.2.2.3">
           2
          </cn>
         </apply>
         <ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">
          …
         </ci>
         <apply id="S3.SS1.p2.1.m1.4.4.3.3.cmml" xref="S3.SS1.p2.1.m1.4.4.3.3">
          <csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.4.4.3.3.1.cmml" xref="S3.SS1.p2.1.m1.4.4.3.3">
           subscript
          </csymbol>
          <ci id="S3.SS1.p2.1.m1.4.4.3.3.2.cmml" xref="S3.SS1.p2.1.m1.4.4.3.3.2">
           𝑎
          </ci>
          <ci id="S3.SS1.p2.1.m1.4.4.3.3.3.cmml" xref="S3.SS1.p2.1.m1.4.4.3.3.3">
           𝑀
          </ci>
         </apply>
        </set>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.4c">
        \{a_{1},a_{2},...,a_{M}\}
       </annotation>
      </semantics>
     </math>
     , we activate multiple abducting agents. Each agent is tasked with assuming that a specific answer
     <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1">
      <semantics id="S3.SS1.p2.2.m2.1a">
       <msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">
        <mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">
         a
        </mi>
        <mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b">
        <apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">
          𝑎
         </ci>
         <ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">
        a_{i}
       </annotation>
      </semantics>
     </math>
     is correct and then generating the corresponding abduction
     <math alttext="r_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1">
      <semantics id="S3.SS1.p2.3.m3.1a">
       <msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">
        <mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">
         r
        </mi>
        <mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b">
        <apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">
          𝑟
         </ci>
         <ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">
        r_{i}
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Counterfactual Debate
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.6">
     Among the generated abductions
     <math alttext="\{r_{1},r_{2},...,r_{M}\}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.4">
      <semantics id="S3.SS2.p1.1.m1.4a">
       <mrow id="S3.SS2.p1.1.m1.4.4.3" xref="S3.SS2.p1.1.m1.4.4.4.cmml">
        <mo id="S3.SS2.p1.1.m1.4.4.3.4" stretchy="false" xref="S3.SS2.p1.1.m1.4.4.4.cmml">
         {
        </mo>
        <msub id="S3.SS2.p1.1.m1.2.2.1.1" xref="S3.SS2.p1.1.m1.2.2.1.1.cmml">
         <mi id="S3.SS2.p1.1.m1.2.2.1.1.2" xref="S3.SS2.p1.1.m1.2.2.1.1.2.cmml">
          r
         </mi>
         <mn id="S3.SS2.p1.1.m1.2.2.1.1.3" xref="S3.SS2.p1.1.m1.2.2.1.1.3.cmml">
          1
         </mn>
        </msub>
        <mo id="S3.SS2.p1.1.m1.4.4.3.5" xref="S3.SS2.p1.1.m1.4.4.4.cmml">
         ,
        </mo>
        <msub id="S3.SS2.p1.1.m1.3.3.2.2" xref="S3.SS2.p1.1.m1.3.3.2.2.cmml">
         <mi id="S3.SS2.p1.1.m1.3.3.2.2.2" xref="S3.SS2.p1.1.m1.3.3.2.2.2.cmml">
          r
         </mi>
         <mn id="S3.SS2.p1.1.m1.3.3.2.2.3" xref="S3.SS2.p1.1.m1.3.3.2.2.3.cmml">
          2
         </mn>
        </msub>
        <mo id="S3.SS2.p1.1.m1.4.4.3.6" xref="S3.SS2.p1.1.m1.4.4.4.cmml">
         ,
        </mo>
        <mi id="S3.SS2.p1.1.m1.1.1" mathvariant="normal" xref="S3.SS2.p1.1.m1.1.1.cmml">
         …
        </mi>
        <mo id="S3.SS2.p1.1.m1.4.4.3.7" xref="S3.SS2.p1.1.m1.4.4.4.cmml">
         ,
        </mo>
        <msub id="S3.SS2.p1.1.m1.4.4.3.3" xref="S3.SS2.p1.1.m1.4.4.3.3.cmml">
         <mi id="S3.SS2.p1.1.m1.4.4.3.3.2" xref="S3.SS2.p1.1.m1.4.4.3.3.2.cmml">
          r
         </mi>
         <mi id="S3.SS2.p1.1.m1.4.4.3.3.3" xref="S3.SS2.p1.1.m1.4.4.3.3.3.cmml">
          M
         </mi>
        </msub>
        <mo id="S3.SS2.p1.1.m1.4.4.3.8" stretchy="false" xref="S3.SS2.p1.1.m1.4.4.4.cmml">
         }
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.4b">
        <set id="S3.SS2.p1.1.m1.4.4.4.cmml" xref="S3.SS2.p1.1.m1.4.4.3">
         <apply id="S3.SS2.p1.1.m1.2.2.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1">
          <csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS2.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.2">
           𝑟
          </ci>
          <cn id="S3.SS2.p1.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.2.2.1.1.3">
           1
          </cn>
         </apply>
         <apply id="S3.SS2.p1.1.m1.3.3.2.2.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2">
          <csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.3.3.2.2.1.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2">
           subscript
          </csymbol>
          <ci id="S3.SS2.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2">
           𝑟
          </ci>
          <cn id="S3.SS2.p1.1.m1.3.3.2.2.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.3.3.2.2.3">
           2
          </cn>
         </apply>
         <ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">
          …
         </ci>
         <apply id="S3.SS2.p1.1.m1.4.4.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3">
          <csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.4.4.3.3.1.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3">
           subscript
          </csymbol>
          <ci id="S3.SS2.p1.1.m1.4.4.3.3.2.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.2">
           𝑟
          </ci>
          <ci id="S3.SS2.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3">
           𝑀
          </ci>
         </apply>
        </set>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.4c">
        \{r_{1},r_{2},...,r_{M}\}
       </annotation>
      </semantics>
     </math>
     , only one is factual, while the remainder are incorrect justifications. Hence, we introduce a counterfactual debate mechanism to discern the correct answer from the pool of abductions.
Specifically, for each abducting agent
     <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1">
      <semantics id="S3.SS2.p1.2.m2.1a">
       <msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">
        <mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">
         g
        </mi>
        <mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b">
        <apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">
          𝑔
         </ci>
         <ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">
        g_{i}
       </annotation>
      </semantics>
     </math>
     , who is preset with the position that
     <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1">
      <semantics id="S3.SS2.p1.3.m3.1a">
       <msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">
        <mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">
         a
        </mi>
        <mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b">
        <apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">
          𝑎
         </ci>
         <ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">
        a_{i}
       </annotation>
      </semantics>
     </math>
     is correct, we introduce a critic evaluator to challenge the correctness of
     <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1">
      <semantics id="S3.SS2.p1.4.m4.1a">
       <msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">
        <mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">
         a
        </mi>
        <mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b">
        <apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">
          𝑎
         </ci>
         <ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">
        a_{i}
       </annotation>
      </semantics>
     </math>
     .
By showing the agent’s abduction
     <math alttext="r_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1">
      <semantics id="S3.SS2.p1.5.m5.1a">
       <msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">
        <mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">
         r
        </mi>
        <mi id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b">
        <apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">
          𝑟
         </ci>
         <ci id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">
        r_{i}
       </annotation>
      </semantics>
     </math>
     to critic
     <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1">
      <semantics id="S3.SS2.p1.6.m6.1a">
       <msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">
        <mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">
         c
        </mi>
        <mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b">
        <apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">
          𝑐
         </ci>
         <ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">
        c_{i}
       </annotation>
      </semantics>
     </math>
     and instructing the critic with a prompt like:
    </p>
    <p class="ltx_p ltx_align_center" id="S3.SS2.p1.9">
     <span class="ltx_text ltx_framed ltx_framed_rectangle" id="S3.SS2.p1.9.1" style="border-color: #000000;">
      <span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S3.SS2.p1.9.1.1" style="width:424.9pt;background-color:#F7F7F7;">
       <span class="ltx_p" id="S3.SS2.p1.9.1.1.1">
        The agent’s answer may be wrong. Please persuade the agent that the answer is incorrect.
       </span>
      </span>
     </span>
    </p>
    <p class="ltx_p" id="S3.SS2.p1.8">
     Simultaneously, we preset the stance of
     <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m1.1">
      <semantics id="S3.SS2.p1.7.m1.1a">
       <msub id="S3.SS2.p1.7.m1.1.1" xref="S3.SS2.p1.7.m1.1.1.cmml">
        <mi id="S3.SS2.p1.7.m1.1.1.2" xref="S3.SS2.p1.7.m1.1.1.2.cmml">
         g
        </mi>
        <mi id="S3.SS2.p1.7.m1.1.1.3" xref="S3.SS2.p1.7.m1.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m1.1b">
        <apply id="S3.SS2.p1.7.m1.1.1.cmml" xref="S3.SS2.p1.7.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.7.m1.1.1.1.cmml" xref="S3.SS2.p1.7.m1.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.7.m1.1.1.2.cmml" xref="S3.SS2.p1.7.m1.1.1.2">
          𝑔
         </ci>
         <ci id="S3.SS2.p1.7.m1.1.1.3.cmml" xref="S3.SS2.p1.7.m1.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.7.m1.1c">
        g_{i}
       </annotation>
      </semantics>
     </math>
     , ensuring it firmly believes in the correctness of its answer and addresses challenges from the critic. For instance, we provide
     <math alttext="g_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m2.1">
      <semantics id="S3.SS2.p1.8.m2.1a">
       <msub id="S3.SS2.p1.8.m2.1.1" xref="S3.SS2.p1.8.m2.1.1.cmml">
        <mi id="S3.SS2.p1.8.m2.1.1.2" xref="S3.SS2.p1.8.m2.1.1.2.cmml">
         g
        </mi>
        <mi id="S3.SS2.p1.8.m2.1.1.3" xref="S3.SS2.p1.8.m2.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m2.1b">
        <apply id="S3.SS2.p1.8.m2.1.1.cmml" xref="S3.SS2.p1.8.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.8.m2.1.1.1.cmml" xref="S3.SS2.p1.8.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.8.m2.1.1.2.cmml" xref="S3.SS2.p1.8.m2.1.1.2">
          𝑔
         </ci>
         <ci id="S3.SS2.p1.8.m2.1.1.3.cmml" xref="S3.SS2.p1.8.m2.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.8.m2.1c">
        g_{i}
       </annotation>
      </semantics>
     </math>
     with a prompt such as:
    </p>
    <p class="ltx_p ltx_align_center" id="S3.SS2.p1.10">
     <span class="ltx_text ltx_framed ltx_framed_rectangle" id="S3.SS2.p1.10.1" style="border-color: #000000;">
      <span class="ltx_inline-block ltx_parbox ltx_align_middle" id="S3.SS2.p1.10.1.1" style="width:424.9pt;background-color:#F7F7F7;">
       <span class="ltx_p" id="S3.SS2.p1.10.1.1.1">
        Please refute the critic’s answer and persuade the critic that your answer is correct.
       </span>
      </span>
     </span>
    </p>
    <p class="ltx_p" id="S3.SS2.p1.11">
     With the aforementioned configuration, we orchestrate an adversarial debate scenario for each agent-critic pair.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.3">
     The abduction
     <math alttext="r_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1">
      <semantics id="S3.SS2.p2.1.m1.1a">
       <msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">
        <mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">
         r
        </mi>
        <mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b">
        <apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">
          𝑟
         </ci>
         <ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">
        r_{i}
       </annotation>
      </semantics>
     </math>
     for an incorrect answer
     <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1">
      <semantics id="S3.SS2.p2.2.m2.1a">
       <msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">
        <mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">
         a
        </mi>
        <mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b">
        <apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">
          𝑎
         </ci>
         <ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">
        a_{i}
       </annotation>
      </semantics>
     </math>
     inevitably incorporates numerous fabricated reasoning processes and factually incorrect elements. The adversarial debate process will help to unveil the errors or unreasonable justifications in
     <math alttext="r_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1">
      <semantics id="S3.SS2.p2.3.m3.1a">
       <msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">
        <mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">
         r
        </mi>
        <mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">
         i
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b">
        <apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">
          𝑟
         </ci>
         <ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">
          𝑖
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">
        r_{i}
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     After multi-round debating, we present the debate process of all agent-critic pairs to a third-party judge, enabling them to meticulously analyze and juxtapose the varied debate trajectories, thereby discerning the final answer.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiments
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    In this section, we conduct extensive experiments on the widely studied fact-checking, reading comprehension, and commonsense reasoning tasks.
   </p>
  </div>
  <figure class="ltx_table" id="S4.T1">
   <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
    <thead class="ltx_thead">
     <tr class="ltx_tr" id="S4.T1.1.1.1">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.1.1">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">
        Method
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.2">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">
        Hover 3-hop
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.3">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">
        Hover 4-hop
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.4">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">
        BoolQ
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.5">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1">
        CosmosQA
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.6">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.6.1">
        CommenseQA
       </span>
      </th>
     </tr>
    </thead>
    <tbody class="ltx_tbody">
     <tr class="ltx_tr" id="S4.T1.1.2.1">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">
       CoT
      </th>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">
       0.6108
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">
       0.5886
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">
       0.7767
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">
       0.7833
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.6">
       0.7467
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.3.2">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">
       Self-Reflection
      </th>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">
       0.5986
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">
       0.5813
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">
       0.7728
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.5">
       0.7867
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.6">
       0.7567
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.4.3">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.3.1">
       Self-Consistency
      </th>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2">
       0.6342
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3">
       0.6044
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4">
       0.8033
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.5">
       0.8067
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.6">
       <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.1.4.3.6.1">
        0.7733
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.5.4">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1">
       MAD
      </th>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.2">
       <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.1.5.4.2.1">
        0.6476
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3">
       0.6069
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.4">
       0.8020
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.5">
       0.7933
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.6">
       0.7700
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.6.5">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.5.1">
       Self-Contrast
      </th>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.2">
       0.6359
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.3">
       <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.1.6.5.3.1">
        0.6178
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.4">
       <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.1.6.5.4.1">
        0.8267
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.5">
       <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.1.6.5.5.1">
        0.8133
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.6">
       0.7633
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.1.7.6">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T1.1.7.6.1">
       CFMAD (Ours)
      </th>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.7.6.2">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.7.6.2.1">
        0.6757
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.7.6.3">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.7.6.3.1">
        0.6361
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.7.6.4">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.7.6.4.1">
        0.8366
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.7.6.5">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.7.6.5.1">
        0.8267
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.7.6.6">
       <span class="ltx_text ltx_font_bold" id="S4.T1.1.7.6.6.1">
        0.7933
       </span>
      </td>
     </tr>
    </tbody>
   </table>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 1:
    </span>
    Overall performance comparison on all experiment datasets. Bold font and underline indicate the best and second-best performance, respectively.
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Experimental Setup
   </h3>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
    <h5 class="ltx_title ltx_title_paragraph">
     Datasets.
    </h5>
    <div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">
      We conduct experiments on four datasets: Hover
      <cite class="ltx_cite ltx_citemacro_citep">
       (Jiang et al.,
       <a class="ltx_ref" href="#bib.bib13" title="">
        2020
       </a>
       )
      </cite>
      , BoolQ
      <cite class="ltx_cite ltx_citemacro_citep">
       (Clark et al.,
       <a class="ltx_ref" href="#bib.bib6" title="">
        2019
       </a>
       )
      </cite>
      , CosmosQA
      <cite class="ltx_cite ltx_citemacro_citep">
       (Huang et al.,
       <a class="ltx_ref" href="#bib.bib11" title="">
        2019
       </a>
       )
      </cite>
      , and CommonsenseQA
      <cite class="ltx_cite ltx_citemacro_citep">
       (Talmor et al.,
       <a class="ltx_ref" href="#bib.bib29" title="">
        2019
       </a>
       )
      </cite>
      . Hover and BoolQ are binary prediction tasks with only true or false answers. CosmosQA and CommenseQA are multi-choice tasks with 4 and 5 options, respectively.
Note that we split Hover into two subsets named Hover 3-hop and Hover 4-hop with questions requiring 3 and 4 steps of reasoning, respectively. Comparison between Hover 3-hop and Hover 4-hop might reveal the influence of problem difficulty on method effectiveness since more complex questions typically require more reasoning hops.
More details about these datasets can be found in the Appendix
      <a class="ltx_ref" href="#A1.SS1" title="A.1 Dataset Details ‣ Appendix A Experiments Details ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        A.1
       </span>
      </a>
      .
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
    <h5 class="ltx_title ltx_title_paragraph">
     Baselines.
    </h5>
    <div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">
      As introduced in Sections
      <a class="ltx_ref" href="#S2.SS2" title="2.2 Investigation of Overconfidence ‣ 2 Preliminary Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        2.2
       </span>
      </a>
      , we compare CFMAD with the four baselines: Chain-of-thought (CoT) prompting
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wei et al.,
       <a class="ltx_ref" href="#bib.bib39" title="">
        2022
       </a>
       )
      </cite>
      , Self-Reflection
      <cite class="ltx_cite ltx_citemacro_citep">
       (Shinn et al.,
       <a class="ltx_ref" href="#bib.bib26" title="">
        2024
       </a>
       )
      </cite>
      , Self-Consistency
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wang et al.,
       <a class="ltx_ref" href="#bib.bib36" title="">
        2023
       </a>
       )
      </cite>
      , Self-Contrast
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zhang et al.,
       <a class="ltx_ref" href="#bib.bib46" title="">
        2024b
       </a>
       )
      </cite>
      , MAD
      <cite class="ltx_cite ltx_citemacro_citep">
       (Du et al.,
       <a class="ltx_ref" href="#bib.bib7" title="">
        2023
       </a>
       )
      </cite>
      .
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
    <h5 class="ltx_title ltx_title_paragraph">
     Implementation Details.
    </h5>
    <div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">
      The implementation detail involves three key factors: backbone, prompt, and inference temperature. For all compared methods, we use GPT-3.5-turbo-0613
      <span class="ltx_note ltx_role_footnote" id="footnote1">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          1
         </sup>
         <span class="ltx_tag ltx_tag_note">
          1
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chatgpt.com/" target="_blank" title="">
          https://chatgpt.com/
         </a>
         .
        </span>
       </span>
      </span>
      as our backbone LLM and present their prompts in Appendix
      <a class="ltx_ref" href="#A2" title="Appendix B Basline Prompts ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        B
       </span>
      </a>
      and
      <a class="ltx_ref" href="#A3" title="Appendix C Our Prompts ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        C
       </span>
      </a>
      . As to the inference temperature, we set it to 0.2 in most methods for the sake of fair comparison. The only exception is Self-Consistency, where we follow the original paper and set the temperature to 1 since the method requires high diversity of samples
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wang et al.,
       <a class="ltx_ref" href="#bib.bib36" title="">
        2023
       </a>
       )
      </cite>
      .
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
    <h5 class="ltx_title ltx_title_paragraph">
     Evaluation Metrics.
    </h5>
    <div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.1">
      For binary prediction datasets,
      <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.1">
       i.e.,
      </em>
      Hover and BoolQ, we follow the previous work
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wang and Shu,
       <a class="ltx_ref" href="#bib.bib33" title="">
        2023
       </a>
       )
      </cite>
      and adopt the macro-F1 score as the evaluation metric. As to multi-choice datasets,
      <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px4.p1.1.2">
       i.e.,
      </em>
      CosmosQA and CommenseQA, we report accuracy following previous work
      <cite class="ltx_cite ltx_citemacro_citep">
       (Wang and Zhao,
       <a class="ltx_ref" href="#bib.bib37" title="">
        2023
       </a>
       )
      </cite>
      .
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Performance Comparison
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     shows the performance of the compared methods on all datasets. From the table, we have the following observations:
    </p>
    <ul class="ltx_itemize" id="S4.I1">
     <li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i1.p1">
       <p class="ltx_p" id="S4.I1.i1.p1.1">
        In all cases, CFMAD outperforms all baselines, showing stronger reasoning capabilities. Such performance gain indicates the effectiveness of the abduction generation and counterfactual debate mechanism.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i2.p1">
       <p class="ltx_p" id="S4.I1.i2.p1.1">
        Among all self-correction and diverse sampling methods, Self-Reflection performs the worst in all cases, sometimes even worse than CoT. Given that Self-Reflection encounters the most severe overconfidence issue (as shown in Figure
        <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ Results. ‣ 2.2 Investigation of Overconfidence ‣ 2 Preliminary Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
         <span class="ltx_text ltx_ref_tag">
          2
         </span>
        </a>
        ), we postulate that such inferior performance is due to overconfidence.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i3.p1">
       <p class="ltx_p" id="S4.I1.i3.p1.1">
        While Self-Consistency exhibits lower levels of overconfidence than Self-Contrast by providing answers within a wider scope, it does not consistently outperform Self-Contrast across all tasks. This suggests that incorporating diverse perspectives alone does not guarantee superior reasoning outcomes; the effective utilization of these varied viewpoints is crucial for optimal performance.
       </p>
      </div>
     </li>
    </ul>
   </div>
   <figure class="ltx_figure" id="S4.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="S4.F4.g1" src="/html/2406.11514/assets/x4.png" width="461"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     <span class="ltx_text ltx_font_bold" id="S4.F4.2.1">
      Proportion of changes in initial stances.
     </span>
     “Valid” means the stances changed from incorrect to correct. “Invalid” represents the stances changed from correct to incorrect.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    In-depth Analysis
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     We proceed to analyze the performance enhancement of CFMAD. We posit that the efficacy of CFMAD stems from two key factors: 1) Agents instructed to generate abductions for incorrect answers are more likely to waver and change their stance during the debate process due to the contradictions with factual information. 2) Engaging in counterfactual debates aids judges in distinctly discerning between accurate and inaccurate answers. Subsequently, we undertake experimental investigations to delve into these aspects.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S4.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.1
     </span>
     Counterfactual Answers are More Prone to Change
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS1.p1">
     <p class="ltx_p" id="S4.SS3.SSS1.p1.1">
      As to stance change, we first analyze whether the agents would change their stance even when instructed to maintain their original position. For simplicity, we conduct our analysis using the Hover dataset with binary answers. Specifically, we first ask two agents to generate abductions for both “True” and “False” answers, respectively. Given that there are only two possible answers, one of these abductions is necessarily factual while the other is counterfactual.
Given these abductions, we conduct a single round of counterfactual debate. For both agents with factual and counterfactual abductions, we instruct the critic to persuade the agent that their claim is actually incorrect. After that, we present the critic’s argument to the corresponding agents and instruct them to maintain their original stance by pointing out the errors in the critic’s answer and reiterating your point.
Finally, we observe whether these factual and counterfactual agents would change their stance.
     </p>
    </div>
    <figure class="ltx_figure" id="S4.F5">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="S4.F5.g1" src="/html/2406.11514/assets/x5.png" width="461"/>
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_figure">
       Figure 5:
      </span>
      <span class="ltx_text ltx_font_bold" id="S4.F5.2.1">
       The final judgment on inconsistent stances.
      </span>
      “Valid” means that the judge makes a correct judgment while “Invalid” denotes making an incorrect judgment.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S4.SS3.SSS1.p2">
     <p class="ltx_p" id="S4.SS3.SSS1.p2.1">
      The results on Hover 3-hop and Hover 4-hop are shown in Figure
      <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2 Performance Comparison ‣ 4 Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      . From Figure
      <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.2 Performance Comparison ‣ 4 Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      , we find that over 50% of factual and counterfactual agents reached a consensus after one round of counterfactual debate. It means that a significant number of agents were persuaded by the critic, while we instruct these agents to maintain their original stance. Specifically, more than 34% of the stance changes came from counterfactual agents, which is 10% higher than the changes from factual agents. We believe this is because counterfactual answers inherently contradict the facts, making it easier for the critic to point out issues and for the agents to realize the problems and subsequently change their stance.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S4.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.3.2
     </span>
     Counterfactual Debates Contain Additional Clues
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS2.p1">
     <p class="ltx_p" id="S4.SS3.SSS2.p1.1">
      We first analyze the contribution of the counterfactual debate by continuing the previous experiment. For those agents that do not reach a consensus, we present the entire debate process between the critic and the factual and counterfactual agents to a third-party judge.
The judge then makes a final decision on which stance is more factual.
As shown in Figure
      <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.3.1 Counterfactual Answers are More Prone to Change ‣ 4.3 In-depth Analysis ‣ 4 Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      , the number of correct judgments was twice that of incorrect judgments, indicating that even if a consensus is not ultimately reached, leveraging the judge to evaluate the counterfactual debate process can still significantly improve the accuracy of the final decision.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS2.p2">
     <p class="ltx_p" id="S4.SS3.SSS2.p2.1">
      To further investigate the effectiveness of the counterfactual debate, we evaluate several variants of CFMAD, including:
     </p>
     <ul class="ltx_itemize" id="S4.I2">
      <li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S4.I2.i1.p1">
        <p class="ltx_p" id="S4.I2.i1.p1.1">
         <span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">
          Direct Judge:
         </span>
         Removing the counterfactual debate and directly presenting the generated abductions to the judge for final decision.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S4.I2.i2.p1">
        <p class="ltx_p" id="S4.I2.i2.p1.1">
         <span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">
          Replace with Self-Reflection:
         </span>
         Replacing the counterfactual debate with self-reflection, where the LLM reflects on each generated abduction. Both the original answer and the reflection process were shown to the judge for final decision.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S4.I2.i3.p1">
        <p class="ltx_p" id="S4.I2.i3.p1.1">
         <span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">
          Replace with MAD:
         </span>
         Replacing the counterfactual debate with three rounds of MAD
         <cite class="ltx_cite ltx_citemacro_citep">
          (Du et al.,
          <a class="ltx_ref" href="#bib.bib7" title="">
           2023
          </a>
          )
         </cite>
         , then presenting the MAD debate process to the judge for the final decision.
        </p>
       </div>
      </li>
     </ul>
     <p class="ltx_p" id="S4.SS3.SSS2.p2.2">
      We conduct the ablation experiments on three datasets. For CosmosQA and CommenseQA, we used the same 300 data as in Table
      <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        1
       </span>
      </a>
      . For Hover 3-hop, we randomly sampled 300 data points due to cost limitations.
The results are shown in Table
      <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.3.2 Counterfactual Debates Contain Additional Clues ‣ 4.3 In-depth Analysis ‣ 4 Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      . We can see that our proposed counterfactual debate component outperforms the other control group across all tasks. This demonstrates that the counterfactual debate component helps the judge more effectively determine the correct final answer.
     </p>
    </div>
    <figure class="ltx_table" id="S4.T2">
     <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:152.7pt;vertical-align:-0.0pt;">
      <span class="ltx_transformed_inner" style="transform:translate(89.0pt,-31.3pt) scale(1.69630109241124,1.69630109241124) ;">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S4.T2.1.1.1.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.1.1.1.1.1" style="padding-left:2.3pt;padding-right:2.3pt;">
           Method
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.2" style="padding-left:2.3pt;padding-right:2.3pt;">
           Hover 3-hop
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.3" style="padding-left:2.3pt;padding-right:2.3pt;">
           CosmosQA
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.4" style="padding-left:2.3pt;padding-right:2.3pt;">
           CommenseQA
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S4.T2.1.1.2.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.2.1.1" style="padding-left:2.3pt;padding-right:2.3pt;">
           CFMAD
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.2" style="padding-left:2.3pt;padding-right:2.3pt;">
           <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.2.1">
            0.6815
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.3" style="padding-left:2.3pt;padding-right:2.3pt;">
           <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.3.1">
            0.8267
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.4" style="padding-left:2.3pt;padding-right:2.3pt;">
           <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.4.1">
            0.7933
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.3.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.3.2.1" style="padding-left:2.3pt;padding-right:2.3pt;">
           Direct Judge
          </th>
          <td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.2" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.6027
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.3" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.7633
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.4" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.7500
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.4.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.4.3.1" style="padding-left:2.3pt;padding-right:2.3pt;">
           Repl. w/ SR
          </th>
          <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.2" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.6063
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.3" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.7800
          </td>
          <td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.4" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.7600
          </td>
         </tr>
         <tr class="ltx_tr" id="S4.T2.1.1.5.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.1.1.5.4.1" style="padding-left:2.3pt;padding-right:2.3pt;">
           Repl. w/ MAD
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.5.4.2" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.6224
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.5.4.3" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.6767
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.5.4.4" style="padding-left:2.3pt;padding-right:2.3pt;">
           0.7200
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 2:
      </span>
      Ablation studies on the effectiveness of our counterfactual debate component.
     </figcaption>
    </figure>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4
    </span>
    Impact of Hyperparameters
   </h3>
   <div class="ltx_para" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     We then investigate the influence of hyperparameters on the effectiveness of CFMAD, including the number of initial counterfactual answers and debate rounds.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F6">
    <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="198" id="S4.F6.g1" src="/html/2406.11514/assets/x6.png" width="461"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     Comparison of different numbers of (A) initial counterfactual answers and (B) debate rounds.
    </figcaption>
   </figure>
   <section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
    <h5 class="ltx_title ltx_title_paragraph">
     Number of Initial Counterfactual Answers.
    </h5>
    <div class="ltx_para" id="S4.SS4.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS4.SSS0.Px1.p1.1">
      Considering that datasets like CosmosQA and CommenseQA have multiple potential answers, we explore the influence of initial counterfactual answers by increasing the number of sampled stances. Note that directly sampling a few stances from many options may fail to include the correct answer when the initial number of stances is small (
      <em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS0.Px1.p1.1.1">
       e.g.,
      </em>
      2 out of 5 choices). We thus need to conduct the comparison under the condition that the correct answer is included.
To this end, we use a CoT prompt to generate three answers and select the most frequently occurring answer as the most potential stance. We then randomly sample the remaining stances to complete the initial settings.
Considering the expensive time and monetary costs, we randomly sampled 100 data from each dataset.
The final result is shown in Figure
      <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ 4.4 Impact of Hyperparameters ‣ 4 Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      (A), where the accuracy of the final judgment decreases as the number of initial counterfactual responses increases. We believe this is because the presence of too many incorrect stances can confuse the LLMs. Notably, only two initial counterfactual answers are needed to achieve good results, which also saves time and cost.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
    <h5 class="ltx_title ltx_title_paragraph">
     Number of Debate Rounds.
    </h5>
    <div class="ltx_para" id="S4.SS4.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS4.SSS0.Px2.p1.1">
      We also test the impact of conducting multiple rounds of counterfactual debate.
As shown in Figure
      <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ 4.4 Impact of Hyperparameters ‣ 4 Experiments ‣ Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      (B), the accuracy decreases with the increase of debate rounds.
We speculate that through multiple rounds of debate, LLM-based agents and critics may veer away from our predetermined stances to adhere to the biases in the LLM itself, thereby influencing the efficacy of the debate. As such, we conduct only one-round debate between the agent and critic by default.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Related Work
  </h2>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
   <h5 class="ltx_title ltx_title_paragraph">
    Prompting LLM for Better Reasoning.
   </h5>
   <div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">
     Researchers have made significant progress in improving the reasoning abilities of LLMs through designing better prompting methods. These methods often enhance the LLM’s reasoning capabilities in either reasoning depth or breadth.
CoT prompting
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wei et al.,
      <a class="ltx_ref" href="#bib.bib38" title="">
       2023
      </a>
      )
     </cite>
     guides the model to generate intermediate reasoning steps before arriving at a final answer, thus improving the reasoning depth. Self-correction methods
     <cite class="ltx_cite ltx_citemacro_citep">
      (Madaan et al.,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2024
      </a>
      ; Shinn et al.,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2024
      </a>
      ; Paul et al.,
      <a class="ltx_ref" href="#bib.bib25" title="">
       2023
      </a>
      ; Xi et al.,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2024
      </a>
      )
     </cite>
     are also typical examples of enhancing LLM reasoning depth.
They leverage the LLM’s self-correction ability, generating feedback by LLM itself to iteratively refine its answers, thereby enhancing its accuracy and reliability.
Breadth reasoning approaches, on the other hand, involve sampling diverse responses with temperature larger than 0
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wang et al.,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2023
      </a>
      ; Yoran et al.,
      <a class="ltx_ref" href="#bib.bib44" title="">
       2023
      </a>
      )
     </cite>
     or guiding the LLM to generate responses from different perspectives
     <cite class="ltx_cite ltx_citemacro_citep">
      (Huang et al.,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2024a
      </a>
      ; Zhang et al.,
      <a class="ltx_ref" href="#bib.bib46" title="">
       2024b
      </a>
      )
     </cite>
     , gathering more diverse insights for the answer.
This helps to derive the correct answer from the collection of a wider range of potential responses to improve the overall reliability.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
   <h5 class="ltx_title ltx_title_paragraph">
    Multi-agent Debate.
   </h5>
   <div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">
     Recent research has explored how to engage multiple agents of the same model or different models in debates to jointly improve decision-making and reasoning processes
     <cite class="ltx_cite ltx_citemacro_citep">
      (Du et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      ; Liang et al.,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      ; Wang et al.,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2024b
      </a>
      )
     </cite>
     , which can be divided into two modes: collaborative and adversarial.
In the collaborative mode, each agent provides its own answer to the same question and then refines its answer with reference to the responses of other agents
     <cite class="ltx_cite ltx_citemacro_citep">
      (Du et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      )
     </cite>
     .
This mode may encounter overconfidence issues that the initial responses of most agents arrive at the same incorrect answer. In the adversarial mode, for a given answer, two agents are initialized: one believing the answer is correct, and the other believing the answer is incorrect, and they are instructed to debate and challenge each other’s response to reach a more precise conclusion
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liang et al.,
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      ; Wang et al.,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2024b
      </a>
      )
     </cite>
     .
The difference between our counterfactual debate and the adversarial debate lies in that they first have the LLM generate a single answer and then conduct a debate about that answer, while we first have the LLM explore multiple answers as thoroughly as possible, and then conduct debates for each of these answers.
Additionally, existing work also leverages multiple side rationales in LLM reasoning
     <cite class="ltx_cite ltx_citemacro_cite">
      Jung et al. (
      <a class="ltx_ref" href="#bib.bib14" title="">
       2022
      </a>
      ); Liu et al. (
      <a class="ltx_ref" href="#bib.bib20" title="">
       2023
      </a>
      ); Balepur et al. (
      <a class="ltx_ref" href="#bib.bib2" title="">
       2023
      </a>
      )
     </cite>
     which is similar to our abduction, yet not all of them has shown promising results. We incorporate them in the counterfactual debate process and achieve enhanced reasoning.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
   <h5 class="ltx_title ltx_title_paragraph">
    Confidence Calibration.
   </h5>
   <div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">
     Recently, confidence calibration for LLMs has gained significant attention
     <cite class="ltx_cite ltx_citemacro_citep">
      (Lin et al.,
      <a class="ltx_ref" href="#bib.bib19" title="">
       2022
      </a>
      ; Kuhn et al.,
      <a class="ltx_ref" href="#bib.bib16" title="">
       2023
      </a>
      ; Huang et al.,
      <a class="ltx_ref" href="#bib.bib12" title="">
       2023
      </a>
      ; Tian et al.,
      <a class="ltx_ref" href="#bib.bib31" title="">
       2023
      </a>
      )
     </cite>
     .
The goal of confidence calibration is to obtain LLM’s confidence score on its own answer which aligns with the actual answer accuracy.
However, some studies found that LLMs sometimes
generate confidence scores that are poorly calibrated and often assign high confidence scores to incorrect answers
     <cite class="ltx_cite ltx_citemacro_citep">
      (Shrivastava et al.,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2023
      </a>
      ; Yang et al.,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2024
      </a>
      ; Xiong et al.,
      <a class="ltx_ref" href="#bib.bib42" title="">
       2024
      </a>
      )
     </cite>
     .
Some methods attempt to calibrate the confidence for LLMs through estimating response consistency across multiple perspectives
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhang et al.,
      <a class="ltx_ref" href="#bib.bib45" title="">
       2024a
      </a>
      ; Wang et al.,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2024a
      </a>
      )
     </cite>
     , and various prompting strategies for LLM to self-estimate the confidence
     <cite class="ltx_cite ltx_citemacro_cite">
      Tian et al. (
      <a class="ltx_ref" href="#bib.bib31" title="">
       2023
      </a>
      ); Kadavath et al. (
      <a class="ltx_ref" href="#bib.bib15" title="">
       2022
      </a>
      ); Li et al. (
      <a class="ltx_ref" href="#bib.bib17" title="">
       2024
      </a>
      )
     </cite>
     , where some work also leverages explanation and rationales
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib17" title="">
       2024
      </a>
      ); Feng et al. (
      <a class="ltx_ref" href="#bib.bib8" title="">
       2024
      </a>
      )
     </cite>
     .
However, these works mainly aim at improving calibration errors or identifying incorrect answers instead of directly improving the answer accuracy.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this paper, we addressed the overconfidence issue presented in existing self-correction and diverse sampling methods for hallucination elimination in LLM reasoning.
We revealed the overconfidence issues of these two methods through experiments, and pointed out that the overconfidence issue mainly stems from the LLM’s inherent biases towards overly favoring a particular answer while lacking sufficient exploration of other potential answers.
To address this, we proposed the CFMAD framework, which first presets the stance for the LLM, encouraging it to explore as many answers as possible, and then uses counterfactual debate to expose and correct the errors in the incorrect answers.
Empirical results validate the superiority of CFMAD over baselines in mitigating hallucinations.
In this work, we mainly test CFMAD on binary and multiple-choice questions. In the future, we intend to extend CFMAD to more scenarios with open-ended questions.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="Sx1">
  <h2 class="ltx_title ltx_title_section">
   Limitations
  </h2>
  <div class="ltx_para" id="Sx1.p1">
   <p class="ltx_p" id="Sx1.p1.1">
    Our work has the following limitations: First, we require the LLM to generate reasons for each possible answer and conduct debates for each answer, which results in additional computational overhead.
Secondly, since it is necessary to preset the stance for the LLM, we must identify potential answers. We address this by initially using CoT prompts sampling to generate three possible answers. However, it is worth exploring superior methods to improve the recall rate of correct answers.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="Sx2">
  <h2 class="ltx_title ltx_title_section">
   Ethics Statement
  </h2>
  <div class="ltx_para" id="Sx2.p1">
   <p class="ltx_p" id="Sx2.p1.1">
    Our ethical concerns include the following points. First, although we can mitigate LLM hallucinations using CFMAD, the LLM may still produce some inaccurate answers, which could potentially cause harm. Secondly, our experiments are conducted exclusively on English datasets, meaning the applicability of our findings to other languages has not been comprehensively evaluated.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Achiam et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Gpt-4 technical report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      arXiv preprint arXiv:2303.08774
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Balepur et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Nishant Balepur, Shramay Palta, and Rachel Rudinger. 2023.
    </span>
    <span class="ltx_bibblock">
     It’s not easy being wrong: Evaluating process of elimination reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      arXiv preprint arXiv:2311.07532
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      arXiv preprint arXiv:2302.04023
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bhattacharjee et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. 2024.
    </span>
    <span class="ltx_bibblock">
     Zero-shot llm-guided counterfactual generation for text.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      arXiv preprint arXiv:2405.04793
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bubeck et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Sparks of artificial general intelligence: Early experiments with gpt-4.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2303.12712
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Clark et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.
    </span>
    <span class="ltx_bibblock">
     Boolq: Exploring the surprising difficulty of natural yes/no questions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:1905.10044
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Du et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023.
    </span>
    <span class="ltx_bibblock">
     Improving factuality and reasoning in language models through multiagent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      arXiv preprint arXiv:2305.14325
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Feng et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024.
    </span>
    <span class="ltx_bibblock">
     Don’t hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      arXiv preprint arXiv:2402.00367
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. 2024a.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.17272" target="_blank" title="">
      Enhancing large language models in coding through multi-perspective self-consistency
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      Preprint
     </em>
     , arXiv:2309.17272.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.01798" target="_blank" title="">
      Large language models cannot self-correct reasoning yet
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      Preprint
     </em>
     , arXiv:2310.01798.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.
    </span>
    <span class="ltx_bibblock">
     Cosmos qa: Machine reading comprehension with contextual commonsense reasoning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      arXiv preprint arXiv:1909.00277
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, and Lei Ma. 2023.
    </span>
    <span class="ltx_bibblock">
     Look before you leap: An exploratory study of uncertainty measurement for large language models.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.findings-emnlp.309" target="_blank" title="">
      Hover: A dataset for many-hop fact extraction and claim verification
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      Findings of the Association for Computational Linguistics: EMNLP 2020
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jung et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022.
    </span>
    <span class="ltx_bibblock">
     Maieutic prompting: Logically consistent reasoning with recursive explanations.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 1266–1279.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kadavath et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Language models (mostly) know what they know.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2207.05221
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kuhn et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.
    </span>
    <span class="ltx_bibblock">
     Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. 2024.
    </span>
    <span class="ltx_bibblock">
     Think twice before assure: Confidence estimation for large language models through reflection on multiple answers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      arXiv preprint arXiv:2403.09972
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023.
    </span>
    <span class="ltx_bibblock">
     Encouraging divergent thinking in large language models through multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      arXiv preprint arXiv:2305.19118
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lin et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
    </span>
    <span class="ltx_bibblock">
     Teaching models to express their uncertainty in words.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      arXiv preprint arXiv:2205.14334
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, and Jieyu Zhao. 2023.
    </span>
    <span class="ltx_bibblock">
     Score: A framework for self-contradictory reasoning evaluation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      arXiv preprint arXiv:2311.09603
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Madaan et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Self-refine: Iterative refinement with self-feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mielke et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. 2022.
    </span>
    <span class="ltx_bibblock">
     Reducing conversational agents’ overconfidence through linguistic calibration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      Transactions of the Association for Computational Linguistics
     </em>
     , 10:857–872.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nguyen et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Van Bach Nguyen, Paul Youssef, Jörg Schlötterer, and Christin Seifert. 2024.
    </span>
    <span class="ltx_bibblock">
     Llms for generating and evaluating counterfactuals: A comprehensive study.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2405.00722
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ouyang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf" target="_blank" title="">
      Training language models to follow instructions with human feedback
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , volume 35, pages 27730–27744. Curran Associates, Inc.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Paul et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023.
    </span>
    <span class="ltx_bibblock">
     Refiner: Reasoning feedback on intermediate representations.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024.
    </span>
    <span class="ltx_bibblock">
     Reflexion: Language agents with verbal reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shrivastava et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.08877" target="_blank" title="">
      Llamas know what gpts don’t show: Surrogate models for confidence estimation
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      Preprint
     </em>
     , arXiv:2311.08877.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stechly et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023.
    </span>
    <span class="ltx_bibblock">
     Gpt-4 doesn’t know it’s wrong: An analysis of iterative prompting for reasoning problems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:2310.12397
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Talmor et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1811.00937" target="_blank" title="">
      Commonsenseqa: A question answering challenge targeting commonsense knowledge
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      Preprint
     </em>
     , arXiv:1811.00937.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Team et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Gemini: a family of highly capable multimodal models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      arXiv preprint arXiv:2312.11805
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tian et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.14975" target="_blank" title="">
      Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      Preprint
     </em>
     , arXiv:2305.14975.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Valmeekam et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023.
    </span>
    <span class="ltx_bibblock">
     Can large language models really improve by self-critiquing their own plans?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      arXiv preprint arXiv:2310.08118
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang and Shu (2023)
    </span>
    <span class="ltx_bibblock">
     Haoran Wang and Kai Shu. 2023.
    </span>
    <span class="ltx_bibblock">
     Explainable claim verification via knowledge-grounded reasoning with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      arXiv preprint arXiv:2310.05253
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Pei Wang, Yejie Wang, Muxi Diao, Keqing He, Guanting Dong, and Weiran Xu. 2024a.
    </span>
    <span class="ltx_bibblock">
     Multi-perspective consistency enhances confidence estimation in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      arXiv preprint arXiv:2402.11279
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024b.
    </span>
    <span class="ltx_bibblock">
     Rethinking the bounds of llm reasoning: Are multi-agent discussions the key?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      arXiv preprint arXiv:2402.18272
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2203.11171" target="_blank" title="">
      Self-consistency improves chain of thought reasoning in language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      Preprint
     </em>
     , arXiv:2203.11171.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang and Zhao (2023)
    </span>
    <span class="ltx_bibblock">
     Yuqing Wang and Yun Zhao. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.17661" target="_blank" title="">
      Gemini in reasoning: Unveiling commonsense in multimodal large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      Preprint
     </em>
     , arXiv:2312.17661.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2201.11903" target="_blank" title="">
      Chain-of-thought prompting elicits reasoning in large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      Preprint
     </em>
     , arXiv:2201.11903.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      Advances in neural information processing systems
     </em>
     , 35:24824–24837.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xi et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.14497" target="_blank" title="">
      Self-polish: Enhance reasoning in large language models via problem refinement
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      Preprint
     </em>
     , arXiv:2305.14497.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xiong et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023.
    </span>
    <span class="ltx_bibblock">
     Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      arXiv preprint arXiv:2306.13063
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xiong et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.13063" target="_blank" title="">
      Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      Preprint
     </em>
     , arXiv:2306.13063.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Ruixin Yang, Dheeraj Rajagopa, Shirley Anugrah Hayati, Bin Hu, and Dongyeop Kang. 2024.
    </span>
    <span class="ltx_bibblock">
     Confidence calibration and rationalization for llms via multi-agent deliberation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      arXiv preprint arXiv:2404.09127
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yoran et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023.
    </span>
    <span class="ltx_bibblock">
     Answering questions by meta-reasoning over multiple chains of thought.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      arXiv preprint arXiv:2304.13007
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, and Xipeng Qiu. 2024a.
    </span>
    <span class="ltx_bibblock">
     Calibrating the confidence of large language models by eliciting fidelity.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      arXiv preprint arXiv:2404.02655
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. 2024b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2401.02009" target="_blank" title="">
      Self-contrast: Better reflection through inconsistent solving perspectives
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      Preprint
     </em>
     , arXiv:2401.02009.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Siren’s song in the ai ocean: a survey on hallucination in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      arXiv preprint arXiv:2309.01219
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     A survey of large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      arXiv preprint arXiv:2303.18223
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023.
    </span>
    <span class="ltx_bibblock">
     Why does chatgpt fall short in answering questions faithfully?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      arXiv preprint arXiv:2304.10513
     </em>
     .
    </span>
   </li>
  </ul>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Experiments Details
  </h2>
  <section class="ltx_subsection" id="A1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.1
    </span>
    Dataset Details
   </h3>
   <div class="ltx_para" id="A1.SS1.p1">
    <p class="ltx_p" id="A1.SS1.p1.1">
     We performed experiments using four datasets: Hover, BoolQ, CosmosQA, and CommonsenseQA. The details of these datasets are as follows:
    </p>
   </div>
   <div class="ltx_para" id="A1.SS1.p2">
    <ul class="ltx_itemize" id="A1.I1">
     <li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A1.I1.i1.p1">
       <p class="ltx_p" id="A1.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">
         Hover:
        </span>
        Hover is a fact-checking task dataset. Each instance in the Hover dataset consists of a claim and supporting evidence. The task requires multi-hop reasoning based on the supporting evidence to determine whether the evidence supports the claim or not.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A1.I1.i2.p1">
       <p class="ltx_p" id="A1.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">
         BoolQ:
        </span>
        BoolQ is a reading comprehension task dataset that consist of questions that can be answered with a simple “yes” or “no”. And each question is paired with a paragraph from Wikipedia that contains the answer.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A1.I1.i3.p1">
       <p class="ltx_p" id="A1.I1.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="A1.I1.i3.p1.1.1">
         CosmosQA:
        </span>
        CosmosQA is a dataset focused on reading comprehension and commonsense reasoning. Each instance consists of a context and a question with four answer options that require inference beyond the text, using commonsense knowledge to determine the correct answer.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A1.I1.i4.p1">
       <p class="ltx_p" id="A1.I1.i4.p1.1">
        <span class="ltx_text ltx_font_bold" id="A1.I1.i4.p1.1.1">
         CommonsenseQA
        </span>
        : CommonsenseQA is a challenging dataset that tests a model’s ability to use commonsense knowledge to answer multiple-choice questions. Each question has one correct answer and four distractors.
       </p>
      </div>
     </li>
    </ul>
   </div>
   <div class="ltx_para" id="A1.SS1.p3">
    <p class="ltx_p" id="A1.SS1.p3.1">
     In this work, we first tested all 3-hop and 4-hop instances in the validation set of Hover, with 1,835 instances for 3-hop and 1,039 instances for 4-hop to demonstrate our method’s effectiveness. Next, due to budget constraints, we randomly selected 300 instances from the validation set of each of the remaining three datasets to conduct our experiments.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.2
    </span>
    Method Implementation Details
   </h3>
   <div class="ltx_para" id="A1.SS2.p1">
    <p class="ltx_p" id="A1.SS2.p1.1">
     For MAD, we initialized 3 agents and conducted 3 rounds of debate. For Self-Contrast, we had the LLM initially generate answers from 3 perspectives for subsequent contrast. For Self-Consistency, we initially generated 7 answers, voting for the final answer. For our CFMAD framework, we initially preset two predetermined answers to instruct the LLMs to generate abduction. For datasets like CosmosQA and CommonsenseQA, which have multiple potential answers, we first use 3 rounds of CoT prompting to obtain one potentially correct answer as a predetermined answer. Then, we randomly select another predetermined answer from the remaining options.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Basline Prompts
  </h2>
  <section class="ltx_subsection" id="A2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.1
    </span>
    CoT Prompt
   </h3>
   <div class="ltx_para" id="A2.SS1.p1">
    <ul class="ltx_itemize" id="A2.I1">
     <li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I1.i1.p1">
       <p class="ltx_p" id="A2.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I1.i1.p1.1.1">
         Fact Check Task
        </span>
        <br class="ltx_break"/>
        Evidence: {evidence}
        <br class="ltx_break"/>
        Claim: {claim}
        <br class="ltx_break"/>
        You are a fact checker. Please fully understand the evidence and claim, and answer is the claim true or false? Let us verify step by step.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I1.i2.p1">
       <p class="ltx_p" id="A2.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I1.i2.p1.1.1">
         Commonsense Resoning
        </span>
        <br class="ltx_break"/>
        {Question and option here}
       </p>
      </div>
      <div class="ltx_para" id="A2.I1.i2.p2">
       <p class="ltx_p" id="A2.I1.i2.p2.1">
        Play the role of a common sense reasoning expert. Choose the most appropriate answer for the question. You are expected to explain your reasoning process step-by-step before providing the final answer.
       </p>
      </div>
      <div class="ltx_para" id="A2.I1.i2.p3">
       <p class="ltx_p" id="A2.I1.i2.p3.1">
        Output format:
        <br class="ltx_break"/>
        Reasoning steps: [Your precise reasoning steps here]
        <br class="ltx_break"/>
        Judgement: The correct answer is Option [X].
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_subsection" id="A2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.2
    </span>
    Relfection Prompt
   </h3>
   <div class="ltx_para" id="A2.SS2.p1">
    <ul class="ltx_itemize" id="A2.I2">
     <li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I2.i1.p1">
       <p class="ltx_p" id="A2.I2.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I2.i1.p1.1.1">
         Reflection Prompt
        </span>
        <br class="ltx_break"/>
        As a critic, review the assistant’s response. Identify any incorrect or missing information, and provide feedback.
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Assistant’s reply: {CoT_reply}
        <br class="ltx_break"/>
        Output format:
        <br class="ltx_break"/>
        Judgement: [Critically evaluate the assistant’s response.]
        <br class="ltx_break"/>
        Potential Improvements: [Suggest ways to enhance the accuracy or clarity of the assistant’s response.]
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I2.i2.p1">
       <p class="ltx_p" id="A2.I2.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I2.i2.p1.1.1">
         Revision Prompt
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Assistant’s reply: {CoT_reply}
        <br class="ltx_break"/>
        Feeback: {reflection_reply}
        <br class="ltx_break"/>
        Based on the feedback provided, revise your response to the question.
        <br class="ltx_break"/>
        Output format:
        <br class="ltx_break"/>
        The correct answer is Option [X].
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_subsection" id="A2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.3
    </span>
    MAD
   </h3>
   <div class="ltx_para" id="A2.SS3.p1">
    <p class="ltx_p" id="A2.SS3.p1.1">
     Here we show the prompt for CommonsenseQA. The prompt structure is similar for other tasks, and the specific prompts for other tasks can be found in our code.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS3.p2">
    <ul class="ltx_itemize" id="A2.I3">
     <li class="ltx_item" id="A2.I3.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I3.i1.p1">
       <p class="ltx_p" id="A2.I3.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I3.i1.p1.1.1">
         Initial Prompt 1
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Play the role of a common sense reasoning expert. Choose the most appropriate answer for the question. You are expected to explain your reasoning process step-by-step before providing the final answer.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I3.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I3.i2.p1">
       <p class="ltx_p" id="A2.I3.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I3.i2.p1.1.1">
         Initial Prompt 2
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Which option is the most appropriate answer based on the common sense?
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I3.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I3.i3.p1">
       <p class="ltx_p" id="A2.I3.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I3.i3.p1.1.1">
         Initial Prompt 3
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Let us think step by step and find the most appropriate answer based on the common sense.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I3.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I3.i4.p1">
       <p class="ltx_p" id="A2.I3.i4.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I3.i4.p1.1.1">
         Debate Prompt
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Let us think step by step and find the most appropriate answer based on the common sense.
        <br class="ltx_break"/>
        Assistant: {Your previous response}
        <br class="ltx_break"/>
        Other agent1: {Other agents’ previous responses1}
        <br class="ltx_break"/>
        Other agent2: {Other agents’ previous responses2}
        <br class="ltx_break"/>
        Using the judgements from other agents as additional information, can you give an updated response.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I3.i5" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I3.i5.p1">
       <p class="ltx_p" id="A2.I3.i5.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I3.i5.p1.1.1">
         Judge Prompt
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Let us think step by step and find the most appropriate answer based on the common sense.
        <br class="ltx_break"/>
        Agent1: {last response of agent 1}
        <br class="ltx_break"/>
        Agent2: {last response of agent 2}
        <br class="ltx_break"/>
        Agent3: {last response of agent 3}
        <br class="ltx_break"/>
        Three agents have given their answers.
        <br class="ltx_break"/>
        According to the majority of the answers, what is the most appropriate answer? Your answer should look like this: “The correct answer is Option [X]”
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_subsection" id="A2.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.4
    </span>
    Self-contrast
   </h3>
   <div class="ltx_para" id="A2.SS4.p1">
    <p class="ltx_p" id="A2.SS4.p1.1">
     Here we show the prompt for CommonsenseQA. The prompt structure is similar for other tasks, and the specific prompts for other tasks can be found in our code.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS4.p2">
    <ul class="ltx_itemize" id="A2.I4">
     <li class="ltx_item" id="A2.I4.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I4.i1.p1">
       <p class="ltx_p" id="A2.I4.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I4.i1.p1.1.1">
         Self-Curate Prompt
        </span>
        <br class="ltx_break"/>
        You are a commonsense reasoning specialist. You need to complete multiple choice questions related to commonsense reasoning. Given a question, you need to carefully analyze the question and dynamically generate several useful prompt instructions. These prompt instructions should be diverse and also useful for commonsense reasoning. These prompt instructions are used to guide the language model to think in different ways, attention to different emphases, and reason from different perspectives for more accurate commonsense reasoning.
        <br class="ltx_break"/>
        For instance, you can adopt multi-faceted thinking (logical thinking, lateral thinking, analogical thinking, etc .), different reasoning perspectives( e.g., top-down, bottom-up , step-by-step), and different emphases of concern, (entity words, numbers, time, etc ) for input question in prompt instruction.
        <br class="ltx_break"/>
        Here are some guidance rules for Prompt Generation:
        <br class="ltx_break"/>
        1. Tone Requirement: Please generate prompt instructions in the third person.
        <br class="ltx_break"/>
        2. Content Requirement: Each prompt instruction should adopt a different way of thinking, or focus on a different perspective, or different emphases to solve the question.
        <br class="ltx_break"/>
        3. Number Requirement: Dynamically generate the most valuable 3 prompt instructions based on the input math question.
        <br class="ltx_break"/>
        4. Format Requirement: Each prompt instruction should start with ### and end with @@@
        <br class="ltx_break"/>
        5. Others: Prompt instructions should focus on commonsense reasoning. So don’t ask any other irrelevant questions in the prompt.
        <br class="ltx_break"/>
        Here is an example : The question is: Who is the first president of the United States?
        <br class="ltx_break"/>
        Output:
        <br class="ltx_break"/>
        bottom - up perspective : ### As a specialist in commonsense reasoning, you have to judge the given question from a bottom-up perspective. Breaking the question down into smaller components or details. What specific pieces of information are provided in the question, and how do they contribute to understanding the problem? @@@
        <br class="ltx_break"/>
        The input question is: {question}. Please generate the most suitable three prompts:
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I4.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I4.i2.p1">
       <p class="ltx_p" id="A2.I4.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I4.i2.p1.1.1">
         Contrast Prompt
        </span>
        <br class="ltx_break"/>
        You are a specialist in commonsense reasoning. Given some candidate judgements for a question, you should carefully compare the difference for each two judgements in their reasoning steps.
        <br class="ltx_break"/>
        When you compare, you need to consider the following questions:
        <br class="ltx_break"/>
        1: Are the two judgements have different final judge and judge reasons?
        <br class="ltx_break"/>
        2: Where are the differences in their reason steps and judge reasons?
        <br class="ltx_break"/>
        3. Why are the answers of the two judgements different?
        <br class="ltx_break"/>
        After contrasting , you should generate a checklist based on these differences between candidate judgements . You should carefully consider each discrepancy and the reasons behind it, summarizing them into a few checking instructions in the checklist. This checklist can guide others to re-examine the input question and these candidate judgements to eliminate these discrepancies .
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Judgements:
        <br class="ltx_break"/>
        Judgement1: {reply1},
        <br class="ltx_break"/>
        Judgement2: {reply2},
        <br class="ltx_break"/>
        Judgement3: {reply3}
        <br class="ltx_break"/>
        Output Format:
        <br class="ltx_break"/>
        For Judgement1 and Judgement2 : [Give the difference between Judgement1 and Judgement2 here]
        <br class="ltx_break"/>
        For Judgement1 and Judgement3 : [Give the difference between Judgement1 and Judgement3 here]
        <br class="ltx_break"/>
        For Judgement2 and Judgement3 : [Give the difference between Judgement2 and Judgement3 here]
        <br class="ltx_break"/>
        Checklist : [Give the directives for checking here]
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I4.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I4.i3.p1">
       <p class="ltx_p" id="A2.I4.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="A2.I4.i3.p1.1.1">
         Reflection Prompt
        </span>
        <br class="ltx_break"/>
        Given a question, multiple inconsistent judgements, their differences in their reasoning processes and a checklist. You should revise the inconsistent reasoning step for each judgements, eliminate the differences, and output a new judgement.
        <br class="ltx_break"/>
        Guidance Rules for Reflection:
        <br class="ltx_break"/>
        1. Please check carefully according to the requirements on the checklist. It helps you to resolve conflicts between different judgements.
        <br class="ltx_break"/>
        2. When you finish revising inconsistent judgements, please ensure all revised judgements should have the same answer . If not , please revise again until all inconsistencies are removed , and all candidates are consistent.
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        The candidate judgements and their discrepancy are as follows:
        <br class="ltx_break"/>
        {
        <br class="ltx_break"/>
        “Candidate”: {
        <br class="ltx_break"/>
        “Judgement”: “{reply1}”,
        <br class="ltx_break"/>
        “Judgement”: “{reply2}”,
        <br class="ltx_break"/>
        “Judgement3”: “{reply3}”
        <br class="ltx_break"/>
        },
        <br class="ltx_break"/>
        “Discrepancy”: {
        <br class="ltx_break"/>
        “difference_1_2”: {
        <br class="ltx_break"/>
        “source”: “Judgement1”,
        <br class="ltx_break"/>
        “target”: “Judgement2”,
        <br class="ltx_break"/>
        “relation”: {difference_1_2}
        <br class="ltx_break"/>
        },
        <br class="ltx_break"/>
        “difference_1_3”: {
        <br class="ltx_break"/>
        “source”: “Judgement1”,
        <br class="ltx_break"/>
        “target”: “Judgement3”,
        <br class="ltx_break"/>
        “relation”: {difference_1_3}
        <br class="ltx_break"/>
        },
        <br class="ltx_break"/>
        “difference_2_3”: {
        <br class="ltx_break"/>
        “source”: “Judgement2”,
        <br class="ltx_break"/>
        “target”: “Judgement3”,
        <br class="ltx_break"/>
        “relation”: {difference_2_3}
        <br class="ltx_break"/>
        } } }
        <br class="ltx_break"/>
        Checklist: {checklist}
        <br class="ltx_break"/>
        Please revise each inconsistent judgement and give your final judgement.
        <br class="ltx_break"/>
        Output Format:
        <br class="ltx_break"/>
        The answer is Option [X].
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Our Prompts
  </h2>
  <section class="ltx_subsection" id="A3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.1
    </span>
    Fact Check Task
   </h3>
   <div class="ltx_para" id="A3.SS1.p1">
    <ul class="ltx_itemize" id="A3.I1">
     <li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A3.I1.i1.p1">
       <p class="ltx_p" id="A3.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="A3.I1.i1.p1.1.1">
         Abduction Generation
        </span>
        <br class="ltx_break"/>
        Evidence: {evidence}
        <br class="ltx_break"/>
        Claim: {claim}
        <br class="ltx_break"/>
        Please fully understand the evidence and claim, and answer why the claim is {true/false}?
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A3.I1.i2.p1">
       <p class="ltx_p" id="A3.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="A3.I1.i2.p1.1.1">
         Counterfactual Debate for Critic
        </span>
        <br class="ltx_break"/>
        Evidence: {evidence}
        <br class="ltx_break"/>
        Claim: {claim}
        <br class="ltx_break"/>
        Assistant: {reply of assistant}
        <br class="ltx_break"/>
        The Assistant’s answer maybe wrong. Please persuade the assistant that the claim is actually incorrect based on the evidence.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A3.I1.i3.p1">
       <p class="ltx_p" id="A3.I1.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="A3.I1.i3.p1.1.1">
         Counterfactual Deabte for Assistant
        </span>
        <br class="ltx_break"/>
        Evidence: {evidence}
        <br class="ltx_break"/>
        Claim: {claim}
        <br class="ltx_break"/>
        Please fully understand the evidence and claim, and answer why the claim is true?
        <br class="ltx_break"/>
        Fact checker: {reply of assistant}
        <br class="ltx_break"/>
        Critic: {reply of crtic}
        <br class="ltx_break"/>
        Play the role of fact checker. Please point out the errors in critic’s answer and reiterate your point.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A3.I1.i4.p1">
       <p class="ltx_p" id="A3.I1.i4.p1.1">
        <span class="ltx_text ltx_font_bold" id="A3.I1.i4.p1.1.1">
         Judge
        </span>
        <br class="ltx_break"/>
        Evidence: {evidence}
        <br class="ltx_break"/>
        Claim: {claim}
        <br class="ltx_break"/>
        {Debate Process for each stance}
        <br class="ltx_break"/>
        After hearing the positive and negative sides, do you think the claim is true or false? [True/False]
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_subsection" id="A3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.2
    </span>
    Commonsense Reasoning
   </h3>
   <div class="ltx_para" id="A3.SS2.p1">
    <ul class="ltx_itemize" id="A3.I2">
     <li class="ltx_item" id="A3.I2.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A3.I2.i1.p1">
       <p class="ltx_p" id="A3.I2.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="A3.I2.i1.p1.1.1">
         Abduction Generation
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Try to explain why the question’s answer might be option {predetermined answer}.
        <br class="ltx_break"/>
        Output Format:
        <br class="ltx_break"/>
        Judgement: The answer is option {predetermined answer}.
        <br class="ltx_break"/>
        Reasoning: [Your reasoning here]
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A3.I2.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A3.I2.i2.p1">
       <p class="ltx_p" id="A3.I2.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="A3.I2.i2.p1.1.1">
         Counterfactual Debate for Critic
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Assistant: {reply of assistant}
        <br class="ltx_break"/>
        The Assistant’s answer maybe wrong. Please persuade the assistant that his answer maybe wrong.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A3.I2.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A3.I2.i3.p1">
       <p class="ltx_p" id="A3.I2.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="A3.I2.i3.p1.1.1">
         Counterfactual Deabte for Assistant
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Assistant: {reply of assistant}
        <br class="ltx_break"/>
        Critic: {reply of critic}
        <br class="ltx_break"/>
        As assistant, please refute the critic’s answer and persuade the critic that your answer is correct.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A3.I2.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A3.I2.i4.p1">
       <p class="ltx_p" id="A3.I2.i4.p1.1">
        <span class="ltx_text ltx_font_bold" id="A3.I2.i4.p1.1.1">
         Judge
        </span>
        <br class="ltx_break"/>
        {Question Content Here}
        <br class="ltx_break"/>
        Which option is the answer of the question? The results of the analysis for each of the possible options are as follows:
        <br class="ltx_break"/>
        {Debate Process for each stance}
        <br class="ltx_break"/>
        After seeing the debate process above, do you think which option is the most appropriate answer for the question? Please only give a correct answer and no other replies.
        <br class="ltx_break"/>
        Output format:
        <br class="ltx_break"/>
        Judgement: The correct answer is Option [X].
        <br class="ltx_break"/>
        Reasoning steps: [Your precise reasoning steps here]
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
 </section>
</article>
