<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2102.03462] Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication</title><meta property="og:description" content="How do adults understand children's speech?
Children's productions over the course of language development often bear little resemblance to typical adult pronunciations, yet caregivers nonetheless reliably recover mean…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2102.03462">

<!--Generated on Wed Feb 28 05:49:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Child-directed Listening:
<br class="ltx_break">How Caregiver Inference Enables Children's Early Verbal Communication</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_text" style="font-size:90%;">Stephan C. Meylan</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text" style="font-size:80%;">Department of Brain and Cognitive Sciences, MIT ({smeylan, rplevy}@mit.edu)</span>
</span>
<span class="ltx_contact ltx_role_affiliation">Department of Psychology and Neuroscience, Duke University (elika.bergelson@duke.edu)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_text" style="font-size:90%;">Ruthe Foushee</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Psychology, University of Chicago (foushee@uchicago.edu)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id4.1.id1" class="ltx_text" style="font-size:90%;">Elika Bergelson</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Psychology and Neuroscience, Duke University (elika.bergelson@duke.edu)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_text" style="font-size:90%;">Roger P. Levy</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.2.id1" class="ltx_text" style="font-size:80%;">Department of Brain and Cognitive Sciences, MIT ({smeylan, rplevy}@mit.edu)</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p"><span id="id7.id1.1" class="ltx_text" style="font-size:80%;">How do adults understand children's speech?
Children's productions over the course of language development often bear little resemblance to typical adult pronunciations, yet caregivers nonetheless reliably recover meaning from them.
Here, we employ a suite of Bayesian models of spoken word recognition to understand how adults overcome the noisiness of child language, showing that communicative success between children and adults relies heavily on adult inferential processes. By evaluating competing models on phonetically-annotated corpora, we show that adults' recovered meanings are best predicted by prior expectations fitted specifically to the child language environment, rather than to typical adult-adult language.
After quantifying the contribution of this ``child-directed listening'' over developmental time, we discuss the consequences for theories of language acquisition, as well as the implications for commonly-used methods for assessing children's linguistic proficiency.
</span></p>
<p id="id8.id2" class="ltx_p"><span id="id8.id2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Keywords: language development, child-directed speech, noisy channel communication, spoken word recognition, Bayesian inference</span><span id="id8.id2.2" class="ltx_text" style="font-size:80%;"></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:80%;">The past five decades have seen extensive research dedicated to characterizing how adults speak to infants and young children </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.p1.1.2.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Snow </span><span class="ltx_ERROR undefined">\BBA</span><span class="ltx_text" style="font-size:80%;"> Ferguson</span><span id="S1.p1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">1977</span></a>; <span class="ltx_text" style="font-size:80%;">Soderstrom</span><span id="S1.p1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2007</span></a><span id="S1.p1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S1.p1.1.5" class="ltx_text" style="font-size:80%;">, and to investigating the degree to which adults’ </span><span id="S1.p1.1.6" class="ltx_text ltx_font_italic" style="font-size:80%;">child-directed speech</span><span id="S1.p1.1.7" class="ltx_text" style="font-size:80%;"> directly supports language learning </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.p1.1.8.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Golinkoff </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S1.p1.1.9.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2015</span></a><span id="S1.p1.1.10.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S1.p1.1.11" class="ltx_text" style="font-size:80%;">.
By contrast, how caregivers understand the communicative acts of young children — </span><span id="S1.p1.1.12" class="ltx_text ltx_font_italic" style="font-size:80%;">child-directed listening</span><span id="S1.p1.1.13" class="ltx_text" style="font-size:80%;"> (</span><span id="S1.p1.1.14" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">cdl</span><span id="S1.p1.1.15" class="ltx_text" style="font-size:80%;">) — has received far less attention.
In this paper, we investigate how English-speaking adults interpret English-learning children's verbal productions, making meaning out of vocalizations that are often perceptually distant from targets in the adult language (</span><span id="S1.p1.1.16" class="ltx_text ltx_font_italic" style="font-size:80%;">e.g., </span><span id="S1.p1.1.17" class="ltx_text" style="font-size:80%;">/wid/ for </span><span id="S1.p1.1.18" class="ltx_text ltx_font_italic" style="font-size:80%;">read</span><span id="S1.p1.1.19" class="ltx_text" style="font-size:80%;">; see Table </span><a href="#S2.T1" title="Table 1 ‣ 2 Task and Modeling Setup ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p1.1.20" class="ltx_text" style="font-size:80%;">A).</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:80%;">This characterization of adults' role in conversations with young learners dovetails with ``noisy-channel'' accounts of spoken language interpretation, which provide a framework for describing how listeners overcome imperfect acoustic information, verbal ambiguity, distractions, and speaker variability present in everyday conversation </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.p2.1.2.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Levy</span><span id="S1.p2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2008</span></a>; <span class="ltx_text" style="font-size:80%;">Shannon</span><span id="S1.p2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">1951</span></a>; <span class="ltx_text" style="font-size:80%;">Gibson </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S1.p2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2013</span></a><span id="S1.p2.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S1.p2.1.5" class="ltx_text" style="font-size:80%;">. To recover meanings from highly noisy input, adult listeners rely on their expectations about what speakers are likely to say, combined with the perceptual similarity between what the listener heard and guesses as to what the speaker might intend.
We argue that child language represents a ``noisier-than-usual'' channel, where adults must use expectations fitted to the child language environment to recover meaning from child productions. That is, while hearing /wid/ might typically suggest </span><span id="S1.p2.1.6" class="ltx_text ltx_font_italic" style="font-size:80%;">weed</span><span id="S1.p2.1.7" class="ltx_text" style="font-size:80%;"> or </span><span id="S1.p2.1.8" class="ltx_text ltx_font_italic" style="font-size:80%;">wheat</span><span id="S1.p2.1.9" class="ltx_text" style="font-size:80%;"> as a speaker's intended word (based solely on acoustic information), an adult caregiver might instead recover </span><span id="S1.p2.1.10" class="ltx_text ltx_font_italic" style="font-size:80%;">read</span><span id="S1.p2.1.11" class="ltx_text" style="font-size:80%;"> as the intended word from a child speaker.</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>One intriguing deviation from the classic noisy-channel setup is that adults may “recover” messages when children do not intend to communicate anything at all (<span id="footnote1.1" class="ltx_text ltx_font_italic">i.e., </span>drawing from a noise distribution).</span></span></span><span id="S1.p2.1.12" class="ltx_text" style="font-size:80%;"></span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="font-size:80%;">In what follows, we seek evidence for the role of child-directed listening in language development.
We present a computational framework to predict what adults are likely to recover from children's imperfect speech, and compare it to what adults </span><span id="S1.p3.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">actually</span><span id="S1.p3.1.3" class="ltx_text" style="font-size:80%;"> recovered.
As a proxy for caregivers' realtime interpretations, we use the orthographic annotations made by trained in-lab transcribers of spontaneous at-home child language recordings. This approach allows us to characterize the utility of adult listeners' expectations, versus the acoustic/phonetic signal produced by the child.
To capture the degree to which listening is truly </span><span id="S1.p3.1.4" class="ltx_text ltx_font_italic" style="font-size:80%;">child</span><span id="S1.p3.1.5" class="ltx_text" style="font-size:80%;">-directed (</span><span id="S1.p3.1.6" class="ltx_text ltx_font_italic" style="font-size:80%;">i.e., </span><span id="S1.p3.1.7" class="ltx_text" style="font-size:80%;">distinct from adult-directed listening), we compare the utility of expectations tuned on large-scale adult corpora, versus expectations tailored to reflect the child language environment.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">2 </span>Task and Modeling Setup</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.3" class="ltx_p"><span id="S2.p1.3.1" class="ltx_text" style="font-size:80%;">We focus here on the adult listener's task of recovering meaning from noisy child productions.
Specifically, we look at a large set of phonetically-transcribed productions (</span><span id="S2.p1.3.2" class="ltx_text ltx_font_italic" style="font-size:80%;">e.g., </span><span id="S2.p1.3.3" class="ltx_text" style="font-size:80%;">/A@ wAn d@ wid/ in Table </span><a href="#S2.T1" title="Table 1 ‣ 2 Task and Modeling Setup ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.p1.3.4" class="ltx_text" style="font-size:80%;">A) from the Providence corpus </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.p1.3.5.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Demuth </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S2.p1.3.6.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2006</span></a><span id="S2.p1.3.7.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.p1.3.8" class="ltx_text" style="font-size:80%;">, and treat the challenge of inferring a word identity in context (here, an orthographic word like </span><span id="S2.p1.3.9" class="ltx_text ltx_font_italic" style="font-size:80%;">read</span><span id="S2.p1.3.10" class="ltx_text" style="font-size:80%;">) as a </span><span id="S2.p1.3.11" class="ltx_text ltx_font_italic" style="font-size:80%;">masked word prediction</span><span id="S2.p1.3.12" class="ltx_text" style="font-size:80%;"> task (</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:80%;">Devlin </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S2.p1.3.13.1.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2019</span></a></cite><span id="S2.p1.3.14" class="ltx_text" style="font-size:80%;">).
To combine the contributions of caregiver expectations given the linguistic context with the specific sequence of phonemes produced by the child, we employ a Bayesian model of spoken word recognition in the vein of </span><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:80%;">Norris </span><span class="ltx_ERROR undefined">\BBA</span><span class="ltx_text" style="font-size:80%;"> McQueen</span> <span id="S2.p1.3.15.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2008</span></a><span id="S2.p1.3.16.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.p1.3.17" class="ltx_text" style="font-size:80%;">, which assigns a probability to a candidate word identity </span><math id="S2.p1.1.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.p1.1.m1.1a"><mi mathsize="80%" id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">w</annotation></semantics></math><span id="S2.p1.3.18" class="ltx_text" style="font-size:80%;"> given corresponding perceptual input </span><math id="S2.p1.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.p1.2.m2.1a"><mi mathsize="80%" id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">d</annotation></semantics></math><span id="S2.p1.3.19" class="ltx_text" style="font-size:80%;"> in context </span><math id="S2.p1.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.p1.3.m3.1a"><mi mathsize="80%" id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">c</annotation></semantics></math><span id="S2.p1.3.20" class="ltx_text" style="font-size:80%;">:</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.10" class="ltx_Math" alttext="\displaystyle P(w|~{}d,c)=\frac{P(d|w,c)P(w|c)}{\sum_{w^{\prime}\in V}{P(d|w^{\prime},c)P(w^{\prime}|c)}}" display="inline"><semantics id="S2.E1.m1.10a"><mrow id="S2.E1.m1.10.10" xref="S2.E1.m1.10.10.cmml"><mrow id="S2.E1.m1.10.10.1" xref="S2.E1.m1.10.10.1.cmml"><mi mathsize="80%" id="S2.E1.m1.10.10.1.3" xref="S2.E1.m1.10.10.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.10.10.1.2" xref="S2.E1.m1.10.10.1.2.cmml">​</mo><mrow id="S2.E1.m1.10.10.1.1.1" xref="S2.E1.m1.10.10.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.E1.m1.10.10.1.1.1.2" xref="S2.E1.m1.10.10.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.10.10.1.1.1.1" xref="S2.E1.m1.10.10.1.1.1.1.cmml"><mi mathsize="80%" id="S2.E1.m1.10.10.1.1.1.1.2" xref="S2.E1.m1.10.10.1.1.1.1.2.cmml">w</mi><mo fence="false" mathsize="80%" rspace="0.548em" id="S2.E1.m1.10.10.1.1.1.1.1" xref="S2.E1.m1.10.10.1.1.1.1.1.cmml">|</mo><mrow id="S2.E1.m1.10.10.1.1.1.1.3.2" xref="S2.E1.m1.10.10.1.1.1.1.3.1.cmml"><mi mathsize="80%" id="S2.E1.m1.8.8" xref="S2.E1.m1.8.8.cmml">d</mi><mo mathsize="80%" id="S2.E1.m1.10.10.1.1.1.1.3.2.1" xref="S2.E1.m1.10.10.1.1.1.1.3.1.cmml">,</mo><mi mathsize="80%" id="S2.E1.m1.9.9" xref="S2.E1.m1.9.9.cmml">c</mi></mrow></mrow><mo maxsize="80%" minsize="80%" id="S2.E1.m1.10.10.1.1.1.3" xref="S2.E1.m1.10.10.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="80%" id="S2.E1.m1.10.10.2" xref="S2.E1.m1.10.10.2.cmml">=</mo><mstyle displaystyle="true" id="S2.E1.m1.7.7" xref="S2.E1.m1.7.7.cmml"><mfrac id="S2.E1.m1.7.7a" xref="S2.E1.m1.7.7.cmml"><mrow id="S2.E1.m1.4.4.4" xref="S2.E1.m1.4.4.4.cmml"><mi mathsize="80%" id="S2.E1.m1.4.4.4.6" xref="S2.E1.m1.4.4.4.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.4.5" xref="S2.E1.m1.4.4.4.5.cmml">​</mo><mrow id="S2.E1.m1.3.3.3.3.1" xref="S2.E1.m1.3.3.3.3.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.E1.m1.3.3.3.3.1.2" xref="S2.E1.m1.3.3.3.3.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.3.3.1.1" xref="S2.E1.m1.3.3.3.3.1.1.cmml"><mi mathsize="80%" id="S2.E1.m1.3.3.3.3.1.1.2" xref="S2.E1.m1.3.3.3.3.1.1.2.cmml">d</mi><mo fence="false" mathsize="80%" id="S2.E1.m1.3.3.3.3.1.1.1" xref="S2.E1.m1.3.3.3.3.1.1.1.cmml">|</mo><mrow id="S2.E1.m1.3.3.3.3.1.1.3.2" xref="S2.E1.m1.3.3.3.3.1.1.3.1.cmml"><mi mathsize="80%" id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">w</mi><mo mathsize="80%" id="S2.E1.m1.3.3.3.3.1.1.3.2.1" xref="S2.E1.m1.3.3.3.3.1.1.3.1.cmml">,</mo><mi mathsize="80%" id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.2.cmml">c</mi></mrow></mrow><mo maxsize="80%" minsize="80%" id="S2.E1.m1.3.3.3.3.1.3" xref="S2.E1.m1.3.3.3.3.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.4.5a" xref="S2.E1.m1.4.4.4.5.cmml">​</mo><mi mathsize="80%" id="S2.E1.m1.4.4.4.7" xref="S2.E1.m1.4.4.4.7.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.4.5b" xref="S2.E1.m1.4.4.4.5.cmml">​</mo><mrow id="S2.E1.m1.4.4.4.4.1" xref="S2.E1.m1.4.4.4.4.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.E1.m1.4.4.4.4.1.2" xref="S2.E1.m1.4.4.4.4.1.1.cmml">(</mo><mrow id="S2.E1.m1.4.4.4.4.1.1" xref="S2.E1.m1.4.4.4.4.1.1.cmml"><mi mathsize="80%" id="S2.E1.m1.4.4.4.4.1.1.2" xref="S2.E1.m1.4.4.4.4.1.1.2.cmml">w</mi><mo fence="false" mathsize="80%" id="S2.E1.m1.4.4.4.4.1.1.1" xref="S2.E1.m1.4.4.4.4.1.1.1.cmml">|</mo><mi mathsize="80%" id="S2.E1.m1.4.4.4.4.1.1.3" xref="S2.E1.m1.4.4.4.4.1.1.3.cmml">c</mi></mrow><mo maxsize="80%" minsize="80%" id="S2.E1.m1.4.4.4.4.1.3" xref="S2.E1.m1.4.4.4.4.1.1.cmml">)</mo></mrow></mrow><mrow id="S2.E1.m1.7.7.7" xref="S2.E1.m1.7.7.7.cmml"><msub id="S2.E1.m1.7.7.7.4" xref="S2.E1.m1.7.7.7.4.cmml"><mo maxsize="80%" minsize="80%" stretchy="true" id="S2.E1.m1.7.7.7.4.2" xref="S2.E1.m1.7.7.7.4.2.cmml">∑</mo><mrow id="S2.E1.m1.7.7.7.4.3" xref="S2.E1.m1.7.7.7.4.3.cmml"><msup id="S2.E1.m1.7.7.7.4.3.2" xref="S2.E1.m1.7.7.7.4.3.2.cmml"><mi mathsize="80%" id="S2.E1.m1.7.7.7.4.3.2.2" xref="S2.E1.m1.7.7.7.4.3.2.2.cmml">w</mi><mo mathsize="80%" id="S2.E1.m1.7.7.7.4.3.2.3" xref="S2.E1.m1.7.7.7.4.3.2.3.cmml">′</mo></msup><mo mathsize="80%" id="S2.E1.m1.7.7.7.4.3.1" xref="S2.E1.m1.7.7.7.4.3.1.cmml">∈</mo><mi mathsize="80%" id="S2.E1.m1.7.7.7.4.3.3" xref="S2.E1.m1.7.7.7.4.3.3.cmml">V</mi></mrow></msub><mrow id="S2.E1.m1.7.7.7.3" xref="S2.E1.m1.7.7.7.3.cmml"><mi mathsize="80%" id="S2.E1.m1.7.7.7.3.4" xref="S2.E1.m1.7.7.7.3.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.7.3.3" xref="S2.E1.m1.7.7.7.3.3.cmml">​</mo><mrow id="S2.E1.m1.6.6.6.2.1.1" xref="S2.E1.m1.6.6.6.2.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.E1.m1.6.6.6.2.1.1.2" xref="S2.E1.m1.6.6.6.2.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.6.6.6.2.1.1.1" xref="S2.E1.m1.6.6.6.2.1.1.1.cmml"><mi mathsize="80%" id="S2.E1.m1.6.6.6.2.1.1.1.3" xref="S2.E1.m1.6.6.6.2.1.1.1.3.cmml">d</mi><mo fence="false" mathsize="80%" id="S2.E1.m1.6.6.6.2.1.1.1.2" xref="S2.E1.m1.6.6.6.2.1.1.1.2.cmml">|</mo><mrow id="S2.E1.m1.6.6.6.2.1.1.1.1.1" xref="S2.E1.m1.6.6.6.2.1.1.1.1.2.cmml"><msup id="S2.E1.m1.6.6.6.2.1.1.1.1.1.1" xref="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.cmml"><mi mathsize="80%" id="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.2" xref="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.2.cmml">w</mi><mo mathsize="80%" id="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.3" xref="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.3.cmml">′</mo></msup><mo mathsize="80%" id="S2.E1.m1.6.6.6.2.1.1.1.1.1.2" xref="S2.E1.m1.6.6.6.2.1.1.1.1.2.cmml">,</mo><mi mathsize="80%" id="S2.E1.m1.5.5.5.1" xref="S2.E1.m1.5.5.5.1.cmml">c</mi></mrow></mrow><mo maxsize="80%" minsize="80%" id="S2.E1.m1.6.6.6.2.1.1.3" xref="S2.E1.m1.6.6.6.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.7.3.3a" xref="S2.E1.m1.7.7.7.3.3.cmml">​</mo><mi mathsize="80%" id="S2.E1.m1.7.7.7.3.5" xref="S2.E1.m1.7.7.7.3.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.7.3.3b" xref="S2.E1.m1.7.7.7.3.3.cmml">​</mo><mrow id="S2.E1.m1.7.7.7.3.2.1" xref="S2.E1.m1.7.7.7.3.2.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.E1.m1.7.7.7.3.2.1.2" xref="S2.E1.m1.7.7.7.3.2.1.1.cmml">(</mo><mrow id="S2.E1.m1.7.7.7.3.2.1.1" xref="S2.E1.m1.7.7.7.3.2.1.1.cmml"><msup id="S2.E1.m1.7.7.7.3.2.1.1.2" xref="S2.E1.m1.7.7.7.3.2.1.1.2.cmml"><mi mathsize="80%" id="S2.E1.m1.7.7.7.3.2.1.1.2.2" xref="S2.E1.m1.7.7.7.3.2.1.1.2.2.cmml">w</mi><mo mathsize="80%" id="S2.E1.m1.7.7.7.3.2.1.1.2.3" xref="S2.E1.m1.7.7.7.3.2.1.1.2.3.cmml">′</mo></msup><mo fence="false" mathsize="80%" id="S2.E1.m1.7.7.7.3.2.1.1.1" xref="S2.E1.m1.7.7.7.3.2.1.1.1.cmml">|</mo><mi mathsize="80%" id="S2.E1.m1.7.7.7.3.2.1.1.3" xref="S2.E1.m1.7.7.7.3.2.1.1.3.cmml">c</mi></mrow><mo maxsize="80%" minsize="80%" id="S2.E1.m1.7.7.7.3.2.1.3" xref="S2.E1.m1.7.7.7.3.2.1.1.cmml">)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.10b"><apply id="S2.E1.m1.10.10.cmml" xref="S2.E1.m1.10.10"><eq id="S2.E1.m1.10.10.2.cmml" xref="S2.E1.m1.10.10.2"></eq><apply id="S2.E1.m1.10.10.1.cmml" xref="S2.E1.m1.10.10.1"><times id="S2.E1.m1.10.10.1.2.cmml" xref="S2.E1.m1.10.10.1.2"></times><ci id="S2.E1.m1.10.10.1.3.cmml" xref="S2.E1.m1.10.10.1.3">𝑃</ci><apply id="S2.E1.m1.10.10.1.1.1.1.cmml" xref="S2.E1.m1.10.10.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.10.10.1.1.1.1.1.cmml" xref="S2.E1.m1.10.10.1.1.1.1.1">conditional</csymbol><ci id="S2.E1.m1.10.10.1.1.1.1.2.cmml" xref="S2.E1.m1.10.10.1.1.1.1.2">𝑤</ci><list id="S2.E1.m1.10.10.1.1.1.1.3.1.cmml" xref="S2.E1.m1.10.10.1.1.1.1.3.2"><ci id="S2.E1.m1.8.8.cmml" xref="S2.E1.m1.8.8">𝑑</ci><ci id="S2.E1.m1.9.9.cmml" xref="S2.E1.m1.9.9">𝑐</ci></list></apply></apply><apply id="S2.E1.m1.7.7.cmml" xref="S2.E1.m1.7.7"><divide id="S2.E1.m1.7.7.8.cmml" xref="S2.E1.m1.7.7"></divide><apply id="S2.E1.m1.4.4.4.cmml" xref="S2.E1.m1.4.4.4"><times id="S2.E1.m1.4.4.4.5.cmml" xref="S2.E1.m1.4.4.4.5"></times><ci id="S2.E1.m1.4.4.4.6.cmml" xref="S2.E1.m1.4.4.4.6">𝑃</ci><apply id="S2.E1.m1.3.3.3.3.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1">conditional</csymbol><ci id="S2.E1.m1.3.3.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.2">𝑑</ci><list id="S2.E1.m1.3.3.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.3.2"><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">𝑤</ci><ci id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2">𝑐</ci></list></apply><ci id="S2.E1.m1.4.4.4.7.cmml" xref="S2.E1.m1.4.4.4.7">𝑃</ci><apply id="S2.E1.m1.4.4.4.4.1.1.cmml" xref="S2.E1.m1.4.4.4.4.1"><csymbol cd="latexml" id="S2.E1.m1.4.4.4.4.1.1.1.cmml" xref="S2.E1.m1.4.4.4.4.1.1.1">conditional</csymbol><ci id="S2.E1.m1.4.4.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.4.4.1.1.2">𝑤</ci><ci id="S2.E1.m1.4.4.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.4.4.1.1.3">𝑐</ci></apply></apply><apply id="S2.E1.m1.7.7.7.cmml" xref="S2.E1.m1.7.7.7"><apply id="S2.E1.m1.7.7.7.4.cmml" xref="S2.E1.m1.7.7.7.4"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.7.4.1.cmml" xref="S2.E1.m1.7.7.7.4">subscript</csymbol><sum id="S2.E1.m1.7.7.7.4.2.cmml" xref="S2.E1.m1.7.7.7.4.2"></sum><apply id="S2.E1.m1.7.7.7.4.3.cmml" xref="S2.E1.m1.7.7.7.4.3"><in id="S2.E1.m1.7.7.7.4.3.1.cmml" xref="S2.E1.m1.7.7.7.4.3.1"></in><apply id="S2.E1.m1.7.7.7.4.3.2.cmml" xref="S2.E1.m1.7.7.7.4.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.7.4.3.2.1.cmml" xref="S2.E1.m1.7.7.7.4.3.2">superscript</csymbol><ci id="S2.E1.m1.7.7.7.4.3.2.2.cmml" xref="S2.E1.m1.7.7.7.4.3.2.2">𝑤</ci><ci id="S2.E1.m1.7.7.7.4.3.2.3.cmml" xref="S2.E1.m1.7.7.7.4.3.2.3">′</ci></apply><ci id="S2.E1.m1.7.7.7.4.3.3.cmml" xref="S2.E1.m1.7.7.7.4.3.3">𝑉</ci></apply></apply><apply id="S2.E1.m1.7.7.7.3.cmml" xref="S2.E1.m1.7.7.7.3"><times id="S2.E1.m1.7.7.7.3.3.cmml" xref="S2.E1.m1.7.7.7.3.3"></times><ci id="S2.E1.m1.7.7.7.3.4.cmml" xref="S2.E1.m1.7.7.7.3.4">𝑃</ci><apply id="S2.E1.m1.6.6.6.2.1.1.1.cmml" xref="S2.E1.m1.6.6.6.2.1.1"><csymbol cd="latexml" id="S2.E1.m1.6.6.6.2.1.1.1.2.cmml" xref="S2.E1.m1.6.6.6.2.1.1.1.2">conditional</csymbol><ci id="S2.E1.m1.6.6.6.2.1.1.1.3.cmml" xref="S2.E1.m1.6.6.6.2.1.1.1.3">𝑑</ci><list id="S2.E1.m1.6.6.6.2.1.1.1.1.2.cmml" xref="S2.E1.m1.6.6.6.2.1.1.1.1.1"><apply id="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.cmml" xref="S2.E1.m1.6.6.6.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.6.6.6.2.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.2">𝑤</ci><ci id="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.6.6.6.2.1.1.1.1.1.1.3">′</ci></apply><ci id="S2.E1.m1.5.5.5.1.cmml" xref="S2.E1.m1.5.5.5.1">𝑐</ci></list></apply><ci id="S2.E1.m1.7.7.7.3.5.cmml" xref="S2.E1.m1.7.7.7.3.5">𝑃</ci><apply id="S2.E1.m1.7.7.7.3.2.1.1.cmml" xref="S2.E1.m1.7.7.7.3.2.1"><csymbol cd="latexml" id="S2.E1.m1.7.7.7.3.2.1.1.1.cmml" xref="S2.E1.m1.7.7.7.3.2.1.1.1">conditional</csymbol><apply id="S2.E1.m1.7.7.7.3.2.1.1.2.cmml" xref="S2.E1.m1.7.7.7.3.2.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.7.3.2.1.1.2.1.cmml" xref="S2.E1.m1.7.7.7.3.2.1.1.2">superscript</csymbol><ci id="S2.E1.m1.7.7.7.3.2.1.1.2.2.cmml" xref="S2.E1.m1.7.7.7.3.2.1.1.2.2">𝑤</ci><ci id="S2.E1.m1.7.7.7.3.2.1.1.2.3.cmml" xref="S2.E1.m1.7.7.7.3.2.1.1.2.3">′</ci></apply><ci id="S2.E1.m1.7.7.7.3.2.1.1.3.cmml" xref="S2.E1.m1.7.7.7.3.2.1.1.3">𝑐</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.10c">\displaystyle P(w|~{}d,c)=\frac{P(d|w,c)P(w|c)}{\sum_{w^{\prime}\in V}{P(d|w^{\prime},c)P(w^{\prime}|c)}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.9" class="ltx_p"><span id="S2.p3.9.1" class="ltx_text" style="font-size:80%;">This cashes out the intuition that the probability assigned to a candidate word </span><math id="S2.p3.1.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.p3.1.m1.1a"><mi mathsize="80%" id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">w</annotation></semantics></math><span id="S2.p3.9.2" class="ltx_text" style="font-size:80%;"> in spoken word recognition reflects the combination of
</span>
<span id="S2.I1" class="ltx_inline-enumerate">
<span id="S2.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(a)</span> <span id="S2.I1.i1.1" class="ltx_text" style="font-size:80%;">fit to perceptual data and
</span></span>
<span id="S2.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(b)</span> <span id="S2.I1.i2.1" class="ltx_text" style="font-size:80%;">linguistic expectations.
</span></span>
</span><span id="S2.p3.9.3" class="ltx_text" style="font-size:80%;">
Fit to perceptual data is evaluated via a likelihood function, </span><math id="S2.p3.2.m2.3" class="ltx_Math" alttext="P(d|w,c)" display="inline"><semantics id="S2.p3.2.m2.3a"><mrow id="S2.p3.2.m2.3.3" xref="S2.p3.2.m2.3.3.cmml"><mi mathsize="80%" id="S2.p3.2.m2.3.3.3" xref="S2.p3.2.m2.3.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.p3.2.m2.3.3.2" xref="S2.p3.2.m2.3.3.2.cmml">​</mo><mrow id="S2.p3.2.m2.3.3.1.1" xref="S2.p3.2.m2.3.3.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.p3.2.m2.3.3.1.1.2" xref="S2.p3.2.m2.3.3.1.1.1.cmml">(</mo><mrow id="S2.p3.2.m2.3.3.1.1.1" xref="S2.p3.2.m2.3.3.1.1.1.cmml"><mi mathsize="80%" id="S2.p3.2.m2.3.3.1.1.1.2" xref="S2.p3.2.m2.3.3.1.1.1.2.cmml">d</mi><mo fence="false" mathsize="80%" id="S2.p3.2.m2.3.3.1.1.1.1" xref="S2.p3.2.m2.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.p3.2.m2.3.3.1.1.1.3.2" xref="S2.p3.2.m2.3.3.1.1.1.3.1.cmml"><mi mathsize="80%" id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">w</mi><mo mathsize="80%" id="S2.p3.2.m2.3.3.1.1.1.3.2.1" xref="S2.p3.2.m2.3.3.1.1.1.3.1.cmml">,</mo><mi mathsize="80%" id="S2.p3.2.m2.2.2" xref="S2.p3.2.m2.2.2.cmml">c</mi></mrow></mrow><mo maxsize="80%" minsize="80%" id="S2.p3.2.m2.3.3.1.1.3" xref="S2.p3.2.m2.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.3b"><apply id="S2.p3.2.m2.3.3.cmml" xref="S2.p3.2.m2.3.3"><times id="S2.p3.2.m2.3.3.2.cmml" xref="S2.p3.2.m2.3.3.2"></times><ci id="S2.p3.2.m2.3.3.3.cmml" xref="S2.p3.2.m2.3.3.3">𝑃</ci><apply id="S2.p3.2.m2.3.3.1.1.1.cmml" xref="S2.p3.2.m2.3.3.1.1"><csymbol cd="latexml" id="S2.p3.2.m2.3.3.1.1.1.1.cmml" xref="S2.p3.2.m2.3.3.1.1.1.1">conditional</csymbol><ci id="S2.p3.2.m2.3.3.1.1.1.2.cmml" xref="S2.p3.2.m2.3.3.1.1.1.2">𝑑</ci><list id="S2.p3.2.m2.3.3.1.1.1.3.1.cmml" xref="S2.p3.2.m2.3.3.1.1.1.3.2"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">𝑤</ci><ci id="S2.p3.2.m2.2.2.cmml" xref="S2.p3.2.m2.2.2">𝑐</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.3c">P(d|w,c)</annotation></semantics></math><span id="S2.p3.9.4" class="ltx_text" style="font-size:80%;">, which reflects the probability that the word </span><math id="S2.p3.3.m3.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.p3.3.m3.1a"><mi mathsize="80%" id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">w</annotation></semantics></math><span id="S2.p3.9.5" class="ltx_text" style="font-size:80%;"> would generate the observed data </span><math id="S2.p3.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.p3.4.m4.1a"><mi mathsize="80%" id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">d</annotation></semantics></math><span id="S2.p3.9.6" class="ltx_text" style="font-size:80%;"> in context </span><math id="S2.p3.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.p3.5.m5.1a"><mi mathsize="80%" id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><ci id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">c</annotation></semantics></math><span id="S2.p3.9.7" class="ltx_text" style="font-size:80%;">.
Linguistic expectations are captured in the prior, </span><math id="S2.p3.6.m6.1" class="ltx_Math" alttext="P(w|c)" display="inline"><semantics id="S2.p3.6.m6.1a"><mrow id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mi mathsize="80%" id="S2.p3.6.m6.1.1.3" xref="S2.p3.6.m6.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.p3.6.m6.1.1.2" xref="S2.p3.6.m6.1.1.2.cmml">​</mo><mrow id="S2.p3.6.m6.1.1.1.1" xref="S2.p3.6.m6.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.p3.6.m6.1.1.1.1.2" xref="S2.p3.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S2.p3.6.m6.1.1.1.1.1" xref="S2.p3.6.m6.1.1.1.1.1.cmml"><mi mathsize="80%" id="S2.p3.6.m6.1.1.1.1.1.2" xref="S2.p3.6.m6.1.1.1.1.1.2.cmml">w</mi><mo fence="false" mathsize="80%" id="S2.p3.6.m6.1.1.1.1.1.1" xref="S2.p3.6.m6.1.1.1.1.1.1.cmml">|</mo><mi mathsize="80%" id="S2.p3.6.m6.1.1.1.1.1.3" xref="S2.p3.6.m6.1.1.1.1.1.3.cmml">c</mi></mrow><mo maxsize="80%" minsize="80%" id="S2.p3.6.m6.1.1.1.1.3" xref="S2.p3.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><times id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2"></times><ci id="S2.p3.6.m6.1.1.3.cmml" xref="S2.p3.6.m6.1.1.3">𝑃</ci><apply id="S2.p3.6.m6.1.1.1.1.1.cmml" xref="S2.p3.6.m6.1.1.1.1"><csymbol cd="latexml" id="S2.p3.6.m6.1.1.1.1.1.1.cmml" xref="S2.p3.6.m6.1.1.1.1.1.1">conditional</csymbol><ci id="S2.p3.6.m6.1.1.1.1.1.2.cmml" xref="S2.p3.6.m6.1.1.1.1.1.2">𝑤</ci><ci id="S2.p3.6.m6.1.1.1.1.1.3.cmml" xref="S2.p3.6.m6.1.1.1.1.1.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">P(w|c)</annotation></semantics></math><span id="S2.p3.9.8" class="ltx_text" style="font-size:80%;">, or the anticipated probability of the word in context </span><math id="S2.p3.7.m7.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.p3.7.m7.1a"><mi mathsize="80%" id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><ci id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">c</annotation></semantics></math><span id="S2.p3.9.9" class="ltx_text" style="font-size:80%;">, absent any perceptual data.
The denominator in Equation </span><a href="#S2.E1" title="In 2 Task and Modeling Setup ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.p3.9.10" class="ltx_text" style="font-size:80%;"> reflects the summed strength of </span><span id="S2.p3.9.11" class="ltx_text ltx_font_italic" style="font-size:80%;">all</span><span id="S2.p3.9.12" class="ltx_text" style="font-size:80%;"> competitor words </span><math id="S2.p3.8.m8.1" class="ltx_Math" alttext="w^{\prime}" display="inline"><semantics id="S2.p3.8.m8.1a"><msup id="S2.p3.8.m8.1.1" xref="S2.p3.8.m8.1.1.cmml"><mi mathsize="80%" id="S2.p3.8.m8.1.1.2" xref="S2.p3.8.m8.1.1.2.cmml">w</mi><mo mathsize="80%" id="S2.p3.8.m8.1.1.3" xref="S2.p3.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.p3.8.m8.1b"><apply id="S2.p3.8.m8.1.1.cmml" xref="S2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.1.cmml" xref="S2.p3.8.m8.1.1">superscript</csymbol><ci id="S2.p3.8.m8.1.1.2.cmml" xref="S2.p3.8.m8.1.1.2">𝑤</ci><ci id="S2.p3.8.m8.1.1.3.cmml" xref="S2.p3.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.8.m8.1c">w^{\prime}</annotation></semantics></math><span id="S2.p3.9.13" class="ltx_text" style="font-size:80%;"> in the candidate vocabulary </span><math id="S2.p3.9.m9.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.p3.9.m9.1a"><mi mathsize="80%" id="S2.p3.9.m9.1.1" xref="S2.p3.9.m9.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.p3.9.m9.1b"><ci id="S2.p3.9.m9.1.1.cmml" xref="S2.p3.9.m9.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.9.m9.1c">V</annotation></semantics></math><span id="S2.p3.9.14" class="ltx_text" style="font-size:80%;">.
Thus, the predictions derived from the model (a </span><span id="S2.p3.9.15" class="ltx_text ltx_font_italic" style="font-size:80%;">posterior</span><span id="S2.p3.9.16" class="ltx_text" style="font-size:80%;">) constitute a probability distribution over candidate words, with highly favored interpretations receiving more of the probability mass than disfavored ones.</span></p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text" style="font-size:80%;">Our principal goal is to find a model that best simulates how adults understand children. We discuss the likelihood and prior of the set of models under consideration in turn.
All models used the same likelihood, derived from measures of pairwise string similarity a phonemic transcription of the child's production and phonemic forms of all candidate words (translated into the International Phonetic Alphabet, IPA, via a dictionary of conventional English pronunciations). To illustrate, given the transcribed production /wid/, the likelihood term for the candidate word </span><span id="S2.p4.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">weed</span><span id="S2.p4.1.3" class="ltx_text" style="font-size:80%;"> (citation phonetic form /wid/) will be higher than the likelihood term for the candidate word </span><span id="S2.p4.1.4" class="ltx_text ltx_font_italic" style="font-size:80%;">read</span><span id="S2.p4.1.5" class="ltx_text" style="font-size:80%;"> (where the citation phonetic form /⁢rid/ differs by one phoneme).</span></p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text" style="font-size:80%;">However, the inferential process sketched in Equation </span><a href="#S3.E3" title="In 3.4 Likelihood ‣ 3 Methods and Model Details ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S2.p5.1.2" class="ltx_text" style="font-size:80%;"> foreshadows the inadequacy of the acoustic signal alone: if children often produce noisy, idiosyncratic phoneme sequences, the prior must do more ``work.''
The priors we evaluate take the form of probabilistic language models: computational models that return a probability distribution over word guesses, based on the surrounding linguistic context (Table </span><a href="#S2.T1" title="Table 1 ‣ 2 Task and Modeling Setup ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.p5.1.3" class="ltx_text" style="font-size:80%;">C).
When priors from each model are combined with the likelihood, they yield posterior distributions (Table </span><a href="#S2.T1" title="Table 1 ‣ 2 Task and Modeling Setup ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.p5.1.4" class="ltx_text" style="font-size:80%;">D).</span></p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text" style="font-size:80%;">Here, we take advantage of a distinction within the transcripts of caregiver-child speech in the PhonBank database </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S2.p6.1.2.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Rose </span><span class="ltx_ERROR undefined">\BBA</span><span class="ltx_text" style="font-size:80%;"> MacWhinney</span><span id="S2.p6.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2014</span></a><span id="S2.p6.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S2.p6.1.5" class="ltx_text" style="font-size:80%;">, which allows us to evaluate competing models on two different dimensions. First, we evaluate models in their ability to reproduce the specific words recovered by annotators.</span><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We cannot know whether the word recovered by an annotator was the word intended by the child speaker.</span></span></span><span id="S2.p6.1.6" class="ltx_text" style="font-size:80%;">
This analysis focuses specifically on what we term </span><span id="S2.p6.1.7" class="ltx_text ltx_font_italic" style="font-size:80%;">communicative successes</span><span id="S2.p6.1.8" class="ltx_text" style="font-size:80%;"> (Table </span><a href="#S2.T1" title="Table 1 ‣ 2 Task and Modeling Setup ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.p6.1.9" class="ltx_text" style="font-size:80%;">A) — instances where a phoneme sequence was not only phonemically transcribed (PhonBank </span><span id="S2.p6.1.10" class="ltx_text ltx_font_typewriter" style="font-size:80%;">%phon</span><span id="S2.p6.1.11" class="ltx_text" style="font-size:80%;"> tier), but also received a </span><span id="S2.p6.1.12" class="ltx_text ltx_font_italic" style="font-size:80%;">gloss</span><span id="S2.p6.1.13" class="ltx_text" style="font-size:80%;">, or orthographic transcription.
This allows us to assess the probability that each model assigns to the annotator-recovered word, with the best model being the one that assigns the highest average probability (alternatively, the lowest </span><span id="S2.p6.1.14" class="ltx_text ltx_font_italic" style="font-size:80%;">surprisal</span><span id="S2.p6.1.15" class="ltx_text" style="font-size:80%;">, or negative log probability</span><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>For statistics-oriented readers, this is the per-instance log-likelihood of the data under the model.</span></span></span><span id="S2.p6.1.16" class="ltx_text" style="font-size:80%;">) to the glosses.</span></p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p"><span id="S2.p7.1.1" class="ltx_text" style="font-size:80%;">Second, we test whether models can predict when a child's production will </span><span id="S2.p7.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">not</span><span id="S2.p7.1.3" class="ltx_text" style="font-size:80%;"> receive a gloss (reflecting the annotator's uncertainty as to the child's intended word).
This analysis relies on the communicative successes described above, as well as so-called </span><span id="S2.p7.1.4" class="ltx_text ltx_font_italic" style="font-size:80%;">communicative failures</span><span id="S2.p7.1.5" class="ltx_text" style="font-size:80%;"> — instances where phoneme sequences are transcribed, but lack a gloss, due to difficulty in identifying the child's intended word (Table </span><a href="#S2.T1" title="Table 1 ‣ 2 Task and Modeling Setup ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.p7.1.6" class="ltx_text" style="font-size:80%;">B).
In the absence of an annotator-recovered word, surprisal cannot be calculated.
Instead, we measure the ``peakedness'' of the guesses regarding word identity by calculating the </span><span id="S2.p7.1.7" class="ltx_text ltx_font_italic" style="font-size:80%;">information entropy</span><span id="S2.p7.1.8" class="ltx_text" style="font-size:80%;"> of the posterior distribution,</span></p>
</div>
<div id="S2.p8" class="ltx_para">
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1.2" class="ltx_Math" alttext="\displaystyle H(X)=-\sum_{i=1}^{n}{P(x_{i})\log P(x_{i})}," display="inline"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.2.1" xref="S2.E2.m1.2.2.1.1.cmml"><mrow id="S2.E2.m1.2.2.1.1" xref="S2.E2.m1.2.2.1.1.cmml"><mrow id="S2.E2.m1.2.2.1.1.4" xref="S2.E2.m1.2.2.1.1.4.cmml"><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.4.2" xref="S2.E2.m1.2.2.1.1.4.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.4.1" xref="S2.E2.m1.2.2.1.1.4.1.cmml">​</mo><mrow id="S2.E2.m1.2.2.1.1.4.3.2" xref="S2.E2.m1.2.2.1.1.4.cmml"><mo maxsize="80%" minsize="80%" id="S2.E2.m1.2.2.1.1.4.3.2.1" xref="S2.E2.m1.2.2.1.1.4.cmml">(</mo><mi mathsize="80%" id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">X</mi><mo maxsize="80%" minsize="80%" id="S2.E2.m1.2.2.1.1.4.3.2.2" xref="S2.E2.m1.2.2.1.1.4.cmml">)</mo></mrow></mrow><mo mathsize="80%" id="S2.E2.m1.2.2.1.1.3" xref="S2.E2.m1.2.2.1.1.3.cmml">=</mo><mrow id="S2.E2.m1.2.2.1.1.2" xref="S2.E2.m1.2.2.1.1.2.cmml"><mo mathsize="80%" id="S2.E2.m1.2.2.1.1.2a" xref="S2.E2.m1.2.2.1.1.2.cmml">−</mo><mrow id="S2.E2.m1.2.2.1.1.2.2" xref="S2.E2.m1.2.2.1.1.2.2.cmml"><mstyle displaystyle="true" id="S2.E2.m1.2.2.1.1.2.2.3" xref="S2.E2.m1.2.2.1.1.2.2.3.cmml"><munderover id="S2.E2.m1.2.2.1.1.2.2.3a" xref="S2.E2.m1.2.2.1.1.2.2.3.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" id="S2.E2.m1.2.2.1.1.2.2.3.2.2" xref="S2.E2.m1.2.2.1.1.2.2.3.2.2.cmml">∑</mo><mrow id="S2.E2.m1.2.2.1.1.2.2.3.2.3" xref="S2.E2.m1.2.2.1.1.2.2.3.2.3.cmml"><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.3.2.3.2" xref="S2.E2.m1.2.2.1.1.2.2.3.2.3.2.cmml">i</mi><mo mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.3.2.3.1" xref="S2.E2.m1.2.2.1.1.2.2.3.2.3.1.cmml">=</mo><mn mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.3.2.3.3" xref="S2.E2.m1.2.2.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.3.3" xref="S2.E2.m1.2.2.1.1.2.2.3.3.cmml">n</mi></munderover></mstyle><mrow id="S2.E2.m1.2.2.1.1.2.2.2" xref="S2.E2.m1.2.2.1.1.2.2.2.cmml"><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.2.4" xref="S2.E2.m1.2.2.1.1.2.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.2.2.2.3" xref="S2.E2.m1.2.2.1.1.2.2.2.3.cmml">​</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo maxsize="80%" minsize="80%" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S2.E2.m1.2.2.1.1.2.2.2.3a" xref="S2.E2.m1.2.2.1.1.2.2.2.3.cmml">​</mo><mrow id="S2.E2.m1.2.2.1.1.2.2.2.5" xref="S2.E2.m1.2.2.1.1.2.2.2.5.cmml"><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.2.5.1" xref="S2.E2.m1.2.2.1.1.2.2.2.5.1.cmml">log</mi><mo lspace="0.167em" id="S2.E2.m1.2.2.1.1.2.2.2.5a" xref="S2.E2.m1.2.2.1.1.2.2.2.5.cmml">⁡</mo><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.2.5.2" xref="S2.E2.m1.2.2.1.1.2.2.2.5.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.2.2.2.3b" xref="S2.E2.m1.2.2.1.1.2.2.2.3.cmml">​</mo><mrow id="S2.E2.m1.2.2.1.1.2.2.2.2.1" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.E2.m1.2.2.1.1.2.2.2.2.1.2" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.cmml">(</mo><msub id="S2.E2.m1.2.2.1.1.2.2.2.2.1.1" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.cmml"><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.2" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.2.cmml">x</mi><mi mathsize="80%" id="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.3" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.3.cmml">i</mi></msub><mo maxsize="80%" minsize="80%" id="S2.E2.m1.2.2.1.1.2.2.2.2.1.3" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo mathsize="80%" id="S2.E2.m1.2.2.1.2" xref="S2.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.2.1.1.cmml" xref="S2.E2.m1.2.2.1"><eq id="S2.E2.m1.2.2.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3"></eq><apply id="S2.E2.m1.2.2.1.1.4.cmml" xref="S2.E2.m1.2.2.1.1.4"><times id="S2.E2.m1.2.2.1.1.4.1.cmml" xref="S2.E2.m1.2.2.1.1.4.1"></times><ci id="S2.E2.m1.2.2.1.1.4.2.cmml" xref="S2.E2.m1.2.2.1.1.4.2">𝐻</ci><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑋</ci></apply><apply id="S2.E2.m1.2.2.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.2"><minus id="S2.E2.m1.2.2.1.1.2.3.cmml" xref="S2.E2.m1.2.2.1.1.2"></minus><apply id="S2.E2.m1.2.2.1.1.2.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2"><apply id="S2.E2.m1.2.2.1.1.2.2.3.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.2.2.3.1.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3">superscript</csymbol><apply id="S2.E2.m1.2.2.1.1.2.2.3.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.2.2.3.2.1.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3">subscript</csymbol><sum id="S2.E2.m1.2.2.1.1.2.2.3.2.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3.2.2"></sum><apply id="S2.E2.m1.2.2.1.1.2.2.3.2.3.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3.2.3"><eq id="S2.E2.m1.2.2.1.1.2.2.3.2.3.1.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3.2.3.1"></eq><ci id="S2.E2.m1.2.2.1.1.2.2.3.2.3.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S2.E2.m1.2.2.1.1.2.2.3.2.3.3.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.2.2.1.1.2.2.3.3.cmml" xref="S2.E2.m1.2.2.1.1.2.2.3.3">𝑛</ci></apply><apply id="S2.E2.m1.2.2.1.1.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2"><times id="S2.E2.m1.2.2.1.1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.3"></times><ci id="S2.E2.m1.2.2.1.1.2.2.2.4.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.4">𝑃</ci><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E2.m1.2.2.1.1.2.2.2.5.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.5"><log id="S2.E2.m1.2.2.1.1.2.2.2.5.1.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.5.1"></log><ci id="S2.E2.m1.2.2.1.1.2.2.2.5.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.5.2">𝑃</ci></apply><apply id="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.2">𝑥</ci><ci id="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.2.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\displaystyle H(X)=-\sum_{i=1}^{n}{P(x_{i})\log P(x_{i})},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p9" class="ltx_para ltx_noindent">
<p id="S2.p9.2" class="ltx_p"><span id="S2.p9.2.1" class="ltx_text" style="font-size:80%;">where </span><math id="S2.p9.1.m1.1" class="ltx_Math" alttext="P(x_{i})" display="inline"><semantics id="S2.p9.1.m1.1a"><mrow id="S2.p9.1.m1.1.1" xref="S2.p9.1.m1.1.1.cmml"><mi mathsize="80%" id="S2.p9.1.m1.1.1.3" xref="S2.p9.1.m1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.p9.1.m1.1.1.2" xref="S2.p9.1.m1.1.1.2.cmml">​</mo><mrow id="S2.p9.1.m1.1.1.1.1" xref="S2.p9.1.m1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S2.p9.1.m1.1.1.1.1.2" xref="S2.p9.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S2.p9.1.m1.1.1.1.1.1" xref="S2.p9.1.m1.1.1.1.1.1.cmml"><mi mathsize="80%" id="S2.p9.1.m1.1.1.1.1.1.2" xref="S2.p9.1.m1.1.1.1.1.1.2.cmml">x</mi><mi mathsize="80%" id="S2.p9.1.m1.1.1.1.1.1.3" xref="S2.p9.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo maxsize="80%" minsize="80%" id="S2.p9.1.m1.1.1.1.1.3" xref="S2.p9.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p9.1.m1.1b"><apply id="S2.p9.1.m1.1.1.cmml" xref="S2.p9.1.m1.1.1"><times id="S2.p9.1.m1.1.1.2.cmml" xref="S2.p9.1.m1.1.1.2"></times><ci id="S2.p9.1.m1.1.1.3.cmml" xref="S2.p9.1.m1.1.1.3">𝑃</ci><apply id="S2.p9.1.m1.1.1.1.1.1.cmml" xref="S2.p9.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p9.1.m1.1.1.1.1.1.1.cmml" xref="S2.p9.1.m1.1.1.1.1">subscript</csymbol><ci id="S2.p9.1.m1.1.1.1.1.1.2.cmml" xref="S2.p9.1.m1.1.1.1.1.1.2">𝑥</ci><ci id="S2.p9.1.m1.1.1.1.1.1.3.cmml" xref="S2.p9.1.m1.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.1.m1.1c">P(x_{i})</annotation></semantics></math><span id="S2.p9.2.2" class="ltx_text" style="font-size:80%;"> is the probability of the </span><math id="S2.p9.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p9.2.m2.1a"><mi mathsize="80%" id="S2.p9.2.m2.1.1" xref="S2.p9.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p9.2.m2.1b"><ci id="S2.p9.2.m2.1.1.cmml" xref="S2.p9.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.2.m2.1c">i</annotation></semantics></math><span id="S2.p9.2.3" class="ltx_text" style="font-size:80%;">th candidate word.
This provides us with a concise index of uncertainty: if posterior probability mass is centered on one or a few guesses for a given phoneme sequence, then entropy will be low; if the posterior is split across many candidate guesses, then entropy will be high.
The best model under this analysis will be the one most able to discriminate failures from successes on the basis of entropy.
We measure this with the </span><span id="S2.p9.2.4" class="ltx_text ltx_font_italic" style="font-size:80%;">receiver operating characteristic</span><span id="S2.p9.2.5" class="ltx_text" style="font-size:80%;">, or ROC, which measures the diagnostic ability of a classifier over the range of possible thresholds.</span></p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p"><span id="S2.p10.1.1" class="ltx_text" style="font-size:80%;">In the third analysis, we quantify how much the estimate of word identity changes as a function of 1) conditioning on context (using a fitted prior) 2) conditioning on data (the posterior when using a uniform prior), or 3) conditioning on </span><span id="S2.p10.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">both</span><span id="S2.p10.1.3" class="ltx_text" style="font-size:80%;"> context </span><span id="S2.p10.1.4" class="ltx_text ltx_font_italic" style="font-size:80%;">and</span><span id="S2.p10.1.5" class="ltx_text" style="font-size:80%;"> data (the posteriors reflecting the fitted priors).
As a baseline for comparison, we start with a uniform prior, where all words in the vocabulary are equiprobable.
We then measure the per-word average </span><span id="S2.p10.1.6" class="ltx_text ltx_font_italic" style="font-size:80%;">information gain</span><span id="S2.p10.1.7" class="ltx_text" style="font-size:80%;">, or Kullback-Liebler divergence, between that uniform prior distribution and each of the distributions identified above.
Information gain can be interpreted as a measure of </span><span id="S2.p10.1.8" class="ltx_text ltx_font_italic" style="font-size:80%;">entropy reduction</span><span id="S2.p10.1.9" class="ltx_text" style="font-size:80%;">, corresponding to the difference between the uniform prior and the somewhat more peaked estimates of word identity under the fitted priors, and the (usually) yet more peaked estimates under the posteriors.
If the models are using the perceptual signal to identify words, then the prior information gain will be small in comparison to the posterior information gain.
If, by contrast, caregivers are relying heavily on their prior expectations, then the prior information gain will be larger with respect to the posterior information gain.</span></p>
</div>
<div id="S2.p11" class="ltx_para">
<p id="S2.p11.1" class="ltx_p"><span id="S2.p11.1.1" class="ltx_text" style="font-size:80%;">A further question is how these measures of information gain will track with developmental time.
We expect prior information gain to </span><span id="S2.p11.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">increase</span><span id="S2.p11.1.3" class="ltx_text" style="font-size:80%;"> over developmental time: as the child says more words in the surrounding context, the priors can better constrain guesses for the masked words (placing more mass on a smaller set of words, reflected in lower entropy).
At the same time, as children's productions approximate conventional pronunciations, we expect to see an increase in posterior information gain.
It remains to be seen how these two quantities will interact.</span></p>
</div>
<figure id="S2.T1" class="ltx_table">

<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.15.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S2.T1.16.2" class="ltx_text" style="font-size:113%;">Examples of communicative success and failure, with samples from highest-ranked prior and posterior candidates. </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S2.T1.11" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2" class="ltx_tr">
<td id="S2.T1.2.2.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-bottom:0.8611pt;" rowspan="3"><span id="S2.T1.2.2.3.1" class="ltx_text" style="font-size:80%;">
<span id="S2.T1.2.2.3.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.1pt;height:87pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:87.0pt;transform:translate(-39.93pt,-39.15pt) rotate(-90deg) ;">
<span id="S2.T1.2.2.3.1.1.1" class="ltx_p">PhonBank transcript</span>
</span></span></span></td>
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-bottom:0.8611pt;" colspan="2">
<span id="S2.T1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A. Communicative Success<sup id="S2.T1.1.1.1.1.1" class="ltx_sup"><sup id="S2.T1.1.1.1.1.1.1" class="ltx_sup"><span id="S2.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></sup></span><span id="S2.T1.1.1.1.2" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-bottom:0.8611pt;" colspan="2"><span id="S2.T1.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">B. Communicative Failure<sup id="S2.T1.2.2.2.1.1" class="ltx_sup"><sup id="S2.T1.2.2.2.1.1.1" class="ltx_sup"><span id="S2.T1.2.2.2.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></sup></span></td>
</tr>
<tr id="S2.T1.11.12.1" class="ltx_tr">
<td id="S2.T1.11.12.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.11.12.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;color:#808080;">mot</span></td>
<td id="S2.T1.11.12.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.11.12.1.2.1" class="ltx_text" style="font-size:80%;color:#808080;">this is</span></td>
<td id="S2.T1.11.12.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.11.12.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;color:#808080;">mot</span></td>
<td id="S2.T1.11.12.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.11.12.1.4.1" class="ltx_text" style="font-size:80%;color:#808080;">do you want ta put some beans in your eggs?</span></td>
</tr>
<tr id="S2.T1.11.13.2" class="ltx_tr">
<td id="S2.T1.11.13.2.1" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.13.2.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;color:#808080;">mot</span></td>
<td id="S2.T1.11.13.2.2" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.13.2.2.1" class="ltx_text" style="font-size:80%;color:#808080;">you want mamma let's see</span></td>
<td id="S2.T1.11.13.2.3" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.13.2.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;color:#808080;">chi</span></td>
<td id="S2.T1.11.13.2.4" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.13.2.4.1" class="ltx_text" style="font-size:80%;color:#808080;">no</span></td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<td id="S2.T1.4.4.3" class="ltx_td ltx_align_right"><span id="S2.T1.4.4.3.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">%phon</span></td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_left"><span id="S2.T1.4.4.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">chi</span></td>
<td id="S2.T1.3.3.1" class="ltx_td ltx_align_left">
<span id="S2.T1.3.3.1.2" class="ltx_text" style="font-size:80%;">/ A@  wAn  d@  </span><span id="S2.T1.3.3.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="font-size:80%;border-color: #000000;">wid<sup id="S2.T1.3.3.1.1.1" class="ltx_sup"><sup id="S2.T1.3.3.1.1.1.1" class="ltx_sup">∗</sup></sup></span><span id="S2.T1.3.3.1.3" class="ltx_text" style="font-size:80%;"> /</span>
</td>
<td id="S2.T1.4.4.5" class="ltx_td ltx_align_left"><span id="S2.T1.4.4.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">chi</span></td>
<td id="S2.T1.4.4.2" class="ltx_td ltx_align_left">
<span id="S2.T1.4.4.2.2" class="ltx_text" style="font-size:80%;">/ ju    mEIk  yo@⁢r   </span><span id="S2.T1.4.4.2.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="font-size:80%;border-color: #000000;">fEt<sup id="S2.T1.4.4.2.1.1" class="ltx_sup"><sup id="S2.T1.4.4.2.1.1.1" class="ltx_sup">∗</sup></sup></span><span id="S2.T1.4.4.2.3" class="ltx_text" style="font-size:80%;"> /</span>
</td>
</tr>
<tr id="S2.T1.7.7" class="ltx_tr">
<td id="S2.T1.7.7.4" class="ltx_td ltx_align_right" style="padding-bottom:1.72221pt;"><span id="S2.T1.7.7.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;">gloss</span></td>
<td id="S2.T1.7.7.5" class="ltx_td" style="padding-bottom:1.72221pt;"></td>
<td id="S2.T1.7.7.6" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.7.7.6.1" class="ltx_text ltx_font_italic" style="font-size:80%;">I    want  to  &lt;read&gt;</span></td>
<td id="S2.T1.7.7.7" class="ltx_td" style="padding-bottom:1.72221pt;"></td>
<td id="S2.T1.7.7.3" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;">
<span id="S2.T1.7.7.3.2" class="ltx_text ltx_font_italic" style="font-size:80%;">you  make  your</span><span id="S2.T1.7.7.3.3" class="ltx_text" style="font-size:80%;"> </span><math id="S2.T1.5.5.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T1.5.5.1.m1.1a"><mo mathsize="80%" id="S2.T1.5.5.1.m1.1.1" xref="S2.T1.5.5.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.m1.1b"><lt id="S2.T1.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.m1.1c">&lt;</annotation></semantics></math><span id="S2.T1.6.6.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">unintelligible<math id="S2.T1.6.6.2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T1.6.6.2.1.m1.1a"><mo id="S2.T1.6.6.2.1.m1.1.1" xref="S2.T1.6.6.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.2.1.m1.1b"><gt id="S2.T1.6.6.2.1.m1.1.1.cmml" xref="S2.T1.6.6.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.2.1.m1.1c">&gt;</annotation></semantics></math></span><span id="S2.T1.7.7.3.4" class="ltx_text" style="font-size:80%;"> </span><math id="S2.T1.7.7.3.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S2.T1.7.7.3.m2.1a"><mo mathsize="80%" stretchy="false" id="S2.T1.7.7.3.m2.1.1" xref="S2.T1.7.7.3.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.3.m2.1b"><ci id="S2.T1.7.7.3.m2.1.1.cmml" xref="S2.T1.7.7.3.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.3.m2.1c">\rightarrow</annotation></semantics></math><span id="S2.T1.7.7.3.5" class="ltx_text" style="font-size:80%;"> </span><span id="S2.T1.7.7.3.6" class="ltx_text ltx_font_typewriter" style="font-size:80%;">yyy</span>
</td>
</tr>
<tr id="S2.T1.11.14.3" class="ltx_tr">
<td id="S2.T1.11.14.3.1" class="ltx_td"></td>
<td id="S2.T1.11.14.3.2" class="ltx_td ltx_align_left"><span id="S2.T1.11.14.3.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;color:#808080;">mot</span></td>
<td id="S2.T1.11.14.3.3" class="ltx_td ltx_align_left"><span id="S2.T1.11.14.3.3.1" class="ltx_text" style="font-size:80%;color:#808080;">okay that's fine</span></td>
<td id="S2.T1.11.14.3.4" class="ltx_td ltx_align_left"><span id="S2.T1.11.14.3.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;color:#808080;">mot</span></td>
<td id="S2.T1.11.14.3.5" class="ltx_td ltx_align_left"><span id="S2.T1.11.14.3.5.1" class="ltx_text" style="font-size:80%;color:#808080;">can I make one?</span></td>
</tr>
<tr id="S2.T1.11.15.4" class="ltx_tr">
<td id="S2.T1.11.15.4.1" class="ltx_td" style="padding-bottom:1.72221pt;"></td>
<td id="S2.T1.11.15.4.2" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.15.4.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;color:#808080;">mot</span></td>
<td id="S2.T1.11.15.4.3" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.15.4.3.1" class="ltx_text" style="font-size:80%;color:#808080;">okay mommy's gonna pick out a book</span></td>
<td id="S2.T1.11.15.4.4" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.15.4.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;color:#808080;">mot</span></td>
<td id="S2.T1.11.15.4.5" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.15.4.5.1" class="ltx_text" style="font-size:80%;color:#808080;">no</span></td>
</tr>
<tr id="S2.T1.11.16.5" class="ltx_tr">
<td id="S2.T1.11.16.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.16.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Language Model</span></td>
<td id="S2.T1.11.16.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:1.72221pt;" colspan="4">
<span id="S2.T1.11.16.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">C. Best <span id="S2.T1.11.16.5.2.1.1" class="ltx_text ltx_font_smallcaps">Prior</span> Guesses for</span><span id="S2.T1.11.16.5.2.2" class="ltx_text" style="font-size:80%;"> </span><span id="S2.T1.11.16.5.2.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="font-size:80%;border-color: #000000;">wid / fEt</span>
</td>
</tr>
<tr id="S2.T1.8.8" class="ltx_tr">
<td id="S2.T1.8.8.1" class="ltx_td ltx_align_left"><span id="S2.T1.8.8.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">cdl+context<sup id="S2.T1.8.8.1.1.1" class="ltx_sup"><sup id="S2.T1.8.8.1.1.1.1" class="ltx_sup"><span id="S2.T1.8.8.1.1.1.1.1" class="ltx_text ltx_font_upright">‡</span></sup></sup></span></td>
<td id="S2.T1.8.8.2" class="ltx_td ltx_align_left ltx_border_t" colspan="2"><span id="S2.T1.8.8.2.1" class="ltx_text" style="font-size:80%;">see (.86)   look (.03)   go (.02) play (.01)</span></td>
<td id="S2.T1.8.8.3" class="ltx_td ltx_align_left ltx_border_t" colspan="2"><span id="S2.T1.8.8.3.1" class="ltx_text" style="font-size:80%;">own (.74)    house (.01)   shapes (.01)    friends (.01)</span></td>
</tr>
<tr id="S2.T1.9.9" class="ltx_tr">
<td id="S2.T1.9.9.1" class="ltx_td ltx_align_left"><span id="S2.T1.9.9.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">bert+context<sup id="S2.T1.9.9.1.1.1" class="ltx_sup"><sup id="S2.T1.9.9.1.1.1.1" class="ltx_sup"><span id="S2.T1.9.9.1.1.1.1.1" class="ltx_text ltx_font_upright">‡</span></sup></sup></span></td>
<td id="S2.T1.9.9.2" class="ltx_td ltx_align_left" colspan="2">
<span id="S2.T1.9.9.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">read</span><span id="S2.T1.9.9.2.2" class="ltx_text" style="font-size:80%;"> (.49) see (.28)  play (.04) know (.04)</span>
</td>
<td id="S2.T1.9.9.3" class="ltx_td ltx_align_left" colspan="2"><span id="S2.T1.9.9.3.1" class="ltx_text" style="font-size:80%;">own (.25) choice (.24) point (.04) bed (.03) call (.03)</span></td>
</tr>
<tr id="S2.T1.11.17.6" class="ltx_tr">
<td id="S2.T1.11.17.6.1" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.17.6.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">childes-1gram</span></td>
<td id="S2.T1.11.17.6.2" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;" colspan="2"><span id="S2.T1.11.17.6.2.1" class="ltx_text" style="font-size:80%;">I    (.04)    a (.03)     the (.03)    yeah (.03)</span></td>
<td id="S2.T1.11.17.6.3" class="ltx_td ltx_align_left" style="padding-bottom:1.72221pt;" colspan="2"><span id="S2.T1.11.17.6.3.1" class="ltx_text" style="font-size:80%;">I (.04)      a (.03)     the (.03)     yeah (.03)      it (.02)</span></td>
</tr>
<tr id="S2.T1.11.18.7" class="ltx_tr">
<td id="S2.T1.11.18.7.1" class="ltx_td ltx_border_t" style="padding-bottom:1.72221pt;"></td>
<td id="S2.T1.11.18.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:1.72221pt;" colspan="4">
<span id="S2.T1.11.18.7.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">D. Best <span id="S2.T1.11.18.7.2.1.1" class="ltx_text ltx_font_smallcaps">Posterior</span> Guesses for</span><span id="S2.T1.11.18.7.2.2" class="ltx_text" style="font-size:80%;"> </span><span id="S2.T1.11.18.7.2.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="font-size:80%;border-color: #000000;">wid / fEt</span>
</td>
</tr>
<tr id="S2.T1.10.10" class="ltx_tr">
<td id="S2.T1.10.10.1" class="ltx_td ltx_align_left"><span id="S2.T1.10.10.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">cdl+context<sup id="S2.T1.10.10.1.1.1" class="ltx_sup"><sup id="S2.T1.10.10.1.1.1.1" class="ltx_sup"><span id="S2.T1.10.10.1.1.1.1.1" class="ltx_text ltx_font_upright">‡</span></sup></sup></span></td>
<td id="S2.T1.10.10.2" class="ltx_td ltx_align_left ltx_border_t" colspan="2">
<span id="S2.T1.10.10.2.1" class="ltx_text" style="font-size:80%;">see (.967) watch (.012) </span><span id="S2.T1.10.10.2.2" class="ltx_text ltx_font_italic" style="font-size:80%;">read</span><span id="S2.T1.10.10.2.3" class="ltx_text" style="font-size:80%;"> (.005) look (.001)</span>
</td>
<td id="S2.T1.10.10.3" class="ltx_td ltx_align_left ltx_border_t" colspan="2"><span id="S2.T1.10.10.3.1" class="ltx_text" style="font-size:80%;">own (.59)  feet (.27)  foot (.02) food (.01)  hat (0.01)</span></td>
</tr>
<tr id="S2.T1.11.11" class="ltx_tr">
<td id="S2.T1.11.11.1" class="ltx_td ltx_align_left"><span id="S2.T1.11.11.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">bert+context<sup id="S2.T1.11.11.1.1.1" class="ltx_sup"><sup id="S2.T1.11.11.1.1.1.1" class="ltx_sup"><span id="S2.T1.11.11.1.1.1.1.1" class="ltx_text ltx_font_upright">‡</span></sup></sup></span></td>
<td id="S2.T1.11.11.2" class="ltx_td ltx_align_left" colspan="2">
<span id="S2.T1.11.11.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">read</span><span id="S2.T1.11.11.2.2" class="ltx_text" style="font-size:80%;"> (.61)    see (.35)    watch (.01)    hear (.01)</span>
</td>
<td id="S2.T1.11.11.3" class="ltx_td ltx_align_left" colspan="2"><span id="S2.T1.11.11.3.1" class="ltx_text" style="font-size:80%;">bet (.31)   own (.24)  cut (.06)   shot (.04)   bed (.03)</span></td>
</tr>
<tr id="S2.T1.11.19.8" class="ltx_tr">
<td id="S2.T1.11.19.8.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-bottom:1.72221pt;"><span id="S2.T1.11.19.8.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">childes-1gram</span></td>
<td id="S2.T1.11.19.8.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-bottom:1.72221pt;" colspan="2"><span id="S2.T1.11.19.8.2.1" class="ltx_text" style="font-size:80%;">we (.34)    need (.11)    and (.06)    would (.04)</span></td>
<td id="S2.T1.11.19.8.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-bottom:1.72221pt;" colspan="2"><span id="S2.T1.11.19.8.3.1" class="ltx_text" style="font-size:80%;">it (.15)    that (.11)   fit (.06)    what (.06)    feet (.05)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S2.I2" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S2.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><sup id="S2.I2.ix1.3.3.1" class="ltx_sup">∗</sup></span> 
<div id="S2.I2.ix1.p1" class="ltx_para">
<p id="S2.I2.ix1.p1.3" class="ltx_p"><span id="S2.I2.ix1.p1.3.1" class="ltx_text" style="font-size:80%;">masked phoneme sequence     
</span><sup id="S2.I2.ix1.p1.3.2" class="ltx_sup"><span id="S2.I2.ix1.p1.3.2.1" class="ltx_text" style="font-size:80%;">†</span></sup><span id="S2.I2.ix1.p1.3.3" class="ltx_text" style="font-size:80%;"> </span><span id="S2.I2.ix1.p1.3.4" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">mot</span><span id="S2.I2.ix1.p1.3.5" class="ltx_text" style="font-size:80%;">= Mother, </span><span id="S2.I2.ix1.p1.3.6" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">chi</span><span id="S2.I2.ix1.p1.3.7" class="ltx_text" style="font-size:80%;">= Child     
</span><sup id="S2.I2.ix1.p1.3.8" class="ltx_sup"><span id="S2.I2.ix1.p1.3.8.1" class="ltx_text" style="font-size:80%;">‡</span></sup><span id="S2.I2.ix1.p1.3.9" class="ltx_text" style="font-size:80%;"> Model considers </span><math id="S2.I2.ix1.p1.3.m3.3" class="ltx_math_unparsed" alttext="+/-20" display="inline"><semantics id="S2.I2.ix1.p1.3.m3.3a"><mrow id="S2.I2.ix1.p1.3.m3.3b"><mo mathsize="80%" rspace="0em" id="S2.I2.ix1.p1.3.m3.1.1">+</mo><mo lspace="0em" maxsize="80%" minsize="80%" rspace="0em" stretchy="true" symmetric="true" id="S2.I2.ix1.p1.3.m3.2.2">/</mo><mo lspace="0em" mathsize="80%" id="S2.I2.ix1.p1.3.m3.3.3">−</mo><mn mathsize="80%" id="S2.I2.ix1.p1.3.m3.3.4">20</mn></mrow><annotation encoding="application/x-tex" id="S2.I2.ix1.p1.3.m3.3c">+/-20</annotation></semantics></math><span id="S2.I2.ix1.p1.3.10" class="ltx_text" style="font-size:80%;"> utterances of surrounding context.</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
</div>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">3 </span>Methods and Model Details</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="font-size:80%;">We test several language models in their ability to predict adult caregivers' interpretations of children's linguistic and proto-linguistic vocalizations in the Providence corpus </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.p1.1.2.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Demuth </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S3.p1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2006</span></a><span id="S3.p1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S3.p1.1.5" class="ltx_text" style="font-size:80%;">.
Utterances and phonological transcripts with both phonemic and orthographic transcription were retrieved through childes-db 2020.1. </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.p1.1.6.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Sanchez </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S3.p1.1.7.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2019</span></a><span id="S3.p1.1.8.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S3.p1.1.9" class="ltx_text" style="font-size:80%;">.</span></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Selecting Communicative Successes and Failures</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="font-size:80%;">We selected as communicative successes all tokens produced by children in the intersection of four criteria:
</span>
<span id="S3.I1" class="ltx_inline-enumerate">
<span id="S3.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(1)</span> <span id="S3.I1.i1.1" class="ltx_text" style="font-size:80%;">possessing monosyllabic IPA forms (motivated below)
</span></span>
<span id="S3.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(2)</span> <span id="S3.I1.i2.1" class="ltx_text" style="font-size:80%;">possessing <span id="S3.I1.i2.1.1" class="ltx_text ltx_font_italic">no</span> unintelligible (CHILDES code <span id="S3.I1.i2.1.2" class="ltx_text ltx_font_typewriter">xxx</span>) or phonology-only (<span id="S3.I1.i2.1.3" class="ltx_text ltx_font_typewriter">yyy</span>) tokens in the same utterance
</span></span>
<span id="S3.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(3)</span> <span id="S3.I1.i3.1" class="ltx_text" style="font-size:80%;">whose gloss is extant as a token in BERT (motivated below) </span></span>
<span id="S3.I1.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(4)</span> <span id="S3.I1.i4.1" class="ltx_text" style="font-size:80%;">whose gloss is included in the Carnegie Mellon Pronunciation Dictionary (henceforth CMU dictionary).
</span></span>
</span><span id="S3.SS1.p1.1.2" class="ltx_text" style="font-size:80%;">
Communicative failures had to meet the first criterion, but must have received the gloss of </span><span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_typewriter" style="font-size:80%;">yyy</span><span id="S3.SS1.p1.1.4" class="ltx_text" style="font-size:80%;"> (with no other </span><span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_typewriter" style="font-size:80%;">yyy</span><span id="S3.SS1.p1.1.6" class="ltx_text" style="font-size:80%;"> or </span><span id="S3.SS1.p1.1.7" class="ltx_text ltx_font_typewriter" style="font-size:80%;">xxx</span><span id="S3.SS1.p1.1.8" class="ltx_text" style="font-size:80%;"> in the same utterance).
Under these definitions, an utterance could contain several communicative successes, but at most one failure.</span></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Candidate Vocabulary</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="font-size:80%;">The inventory of candidate words considered by each model was the intersection of
</span>
<span id="S3.I2" class="ltx_inline-enumerate">
<span id="S3.I2.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(1)</span> <span id="S3.I2.i1.1" class="ltx_text" style="font-size:80%;">words in the CMU dictionary with one or two syllables and
</span></span>
<span id="S3.I2.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(2)</span> <span id="S3.I2.i2.1" class="ltx_text" style="font-size:80%;">tokens present in BERT (motivated below) </span></span>
<span id="S3.I2.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(3)</span> <span id="S3.I2.i3.1" class="ltx_text" style="font-size:80%;">tokens that appeared 3 or more times in CHILDES (to limit to words that might reasonably be said in this context).
</span></span>
</span><span id="S3.SS2.p1.1.2" class="ltx_text" style="font-size:80%;">
This means that while only one-syllable phoneme sequences were analyzed, two-syllable words were also considered as possible candidate interpretations.
The final inventory of candidates, </span><math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi mathsize="80%" id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">V</annotation></semantics></math><span id="S3.SS2.p1.1.3" class="ltx_text" style="font-size:80%;">, included 7,904 words.
We reconcile IPA formats following a procedure detailed in our code.</span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Priors: Language Models</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text" style="font-size:80%;">For each communicative success and failure, we retrieve prior probabilities over candidate words using a suite of probabilistic language models.
As a ``best'' prior architecture, we use BERT </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.SS3.p1.1.2.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Devlin </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S3.SS3.p1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2019</span></a><span id="S3.SS3.p1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S3.SS3.p1.1.5" class="ltx_text" style="font-size:80%;">, which has demonstrated extremely competitive performance for single-word completion tasks, including spoken word recognition </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.SS3.p1.1.6.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Salazar </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S3.SS3.p1.1.7.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2020</span></a><span id="S3.SS3.p1.1.8.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S3.SS3.p1.1.9" class="ltx_text" style="font-size:80%;">.
By virtue of its attentional mechanisms, BERT is able to effectively model long distance dependencies </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.SS3.p1.1.10.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Jawahar </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S3.SS3.p1.1.11.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2019</span></a><span id="S3.SS3.p1.1.12.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S3.SS3.p1.1.13" class="ltx_text" style="font-size:80%;">, and capture speech register and discourse-level information.
We compute the probabilities for the masked word </span><math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="P(w)" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.2" xref="S3.SS3.p1.1.m1.1.2.cmml"><mi mathsize="80%" id="S3.SS3.p1.1.m1.1.2.2" xref="S3.SS3.p1.1.m1.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.2.1" xref="S3.SS3.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS3.p1.1.m1.1.2.3.2" xref="S3.SS3.p1.1.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS3.p1.1.m1.1.2.3.2.1" xref="S3.SS3.p1.1.m1.1.2.cmml">(</mo><mi mathsize="80%" id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">w</mi><mo maxsize="80%" minsize="80%" id="S3.SS3.p1.1.m1.1.2.3.2.2" xref="S3.SS3.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.2"><times id="S3.SS3.p1.1.m1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.2.1"></times><ci id="S3.SS3.p1.1.m1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.2.2">𝑃</ci><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">P(w)</annotation></semantics></math><span id="S3.SS3.p1.1.14" class="ltx_text" style="font-size:80%;"> from BERT, using a language modeling head with the </span><span id="S3.SS3.p1.1.15" class="ltx_text ltx_font_typewriter" style="font-size:80%;">transformers</span><span id="S3.SS3.p1.1.16" class="ltx_text" style="font-size:80%;"> library </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.SS3.p1.1.17.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Wolf </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S3.SS3.p1.1.18.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2020</span></a><span id="S3.SS3.p1.1.19.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S3.SS3.p1.1.20" class="ltx_text" style="font-size:80%;">.
For each masked phoneme sequence, we take the real-valued vector of predictions corresponding to the model's vocabulary, extract the activations corresponding to the candidate words, and compute the softmax to yield a vector of probabilities over the candidate words (Table </span><a href="#S2.T1" title="Table 1 ‣ 2 Task and Modeling Setup ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS3.p1.1.21" class="ltx_text" style="font-size:80%;">).</span></p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text" style="font-size:80%;">We test an ``off-the-shelf'' model of BERT trained on large quantities of (principally adult-directed) language scraped from the internet, predicting the word from the immediate utterance only (</span><span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">BERT+OneUtt</span><span id="S3.SS3.p2.1.3" class="ltx_text" style="font-size:80%;">).
We additionally test the predictions of a BERT model meant to best capture adult expectations about children's utterances.
To do this we ``fine-tune'' the above model on adult and child CHILDES utterance glosses, excluding PhonBank.
In fine tuning, a new model is initialized with an ``off-the-shelf'' model, then the weights in the model are updated to best predict masks inserted into a new training set — in this case, the lines of 80% of CHILDES transcripts (20% were held out for model validation).
This fine-tuned model (</span><span id="S3.SS3.p2.1.4" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL+OneUtt</span><span id="S3.SS3.p2.1.5" class="ltx_text" style="font-size:80%;">) should be expected to be more representative of adult linguistic expectations in understanding child speech than the off-the-shelf model for three reasons.
First, it should assign higher probability to words that are common in speech to and from children.
Second, it should assign higher probability to non-sentence fragments, which are ubiquitous in conversational speech but somewhat less prevalent in adult-directed written language. Third, it may prove capable of developing an expectation for the dyadic, back-and-forth structure of scenes typically captured in transcripts.</span></p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text" style="font-size:80%;">In addition to fine-tuning the model, we manipulate whether prior estimates reflect access to the larger discource context as captured by the transcript before and after a phoneme sequence.
In that these models are meant to be representative of </span><span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">caregiver</span><span id="S3.SS3.p3.1.3" class="ltx_text" style="font-size:80%;"> expectations, these models condition the prediction of the masked token on what the caregiver and child </span><span id="S3.SS3.p3.1.4" class="ltx_text ltx_font_italic" style="font-size:80%;">both</span><span id="S3.SS3.p3.1.5" class="ltx_text" style="font-size:80%;"> say, both before and after the masked token.
We create priors parallel to those above by feeding the models 20 utterances preceding and following each mask (</span><span id="S3.SS3.p3.1.6" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL+Context</span><span id="S3.SS3.p3.1.7" class="ltx_text" style="font-size:80%;"> and </span><span id="S3.SS3.p3.1.8" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">BERT+Context</span><span id="S3.SS3.p3.1.9" class="ltx_text" style="font-size:80%;">).</span></p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text" style="font-size:80%;">BERT has its own vocabulary, which imposes limitations on the vocabulary in the analysis.
Standard implementations of BERT split longer words into ``word pieces'', or most common repeated sub-sequences.
In English, this often yields morphological segmentation (</span><span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">e.g., fishing</span><span id="S3.SS3.p4.1.3" class="ltx_text" style="font-size:80%;"> </span><math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S3.SS3.p4.1.4" class="ltx_text" style="font-size:80%;"> </span><span id="S3.SS3.p4.1.5" class="ltx_text ltx_font_typewriter" style="font-size:80%;">fish</span><span id="S3.SS3.p4.1.6" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS3.p4.1.7" class="ltx_text ltx_font_typewriter" style="font-size:80%;">##ing</span><span id="S3.SS3.p4.1.8" class="ltx_text" style="font-size:80%;">), but the process is highly noisy.
For the purposes of predicting a masked word, BERT predicts only one word or word piece. We limit the vocabulary to word-initial word pieces like </span><span id="S3.SS3.p4.1.9" class="ltx_text ltx_font_italic" style="font-size:80%;">fish</span><span id="S3.SS3.p4.1.10" class="ltx_text" style="font-size:80%;">, and exclude continuations like </span><span id="S3.SS3.p4.1.11" class="ltx_text ltx_font_typewriter" style="font-size:80%;">##ing</span><span id="S3.SS3.p4.1.12" class="ltx_text" style="font-size:80%;"> from consideration.
This also motivates the choice to predict monosyllabic phoneme sequences, in that the model does not allow us to predict multiple words (which might be contained in </span><span id="S3.SS3.p4.1.13" class="ltx_text ltx_font_typewriter" style="font-size:80%;">yyy</span><span id="S3.SS3.p4.1.14" class="ltx_text" style="font-size:80%;">).</span></p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.2" class="ltx_p"><span id="S3.SS3.p5.2.1" class="ltx_text" style="font-size:80%;">In addition to the BERT models, we also test two simpler priors.
The first is a simple smoothed unigram model estimated from counts in CHILDES.
This model, </span><span id="S3.SS3.p5.2.2" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CHILDES 1-gram</span><span id="S3.SS3.p5.2.3" class="ltx_text" style="font-size:80%;">, assigns probability to all word types proportional to their counts in the same CHILDES dataset used in the </span><span id="S3.SS3.p5.2.4" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL</span><span id="S3.SS3.p5.2.5" class="ltx_text" style="font-size:80%;"> models, above.
To account for unseen data, we add a small pseudocount (.001) smoothing to all counts before computing probabilities.
The second is the </span><span id="S3.SS3.p5.2.6" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">UniformPrior</span><span id="S3.SS3.p5.2.7" class="ltx_text" style="font-size:80%;"> model, which assigns equal probability to all words (</span><math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="1/|V|" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mrow id="S3.SS3.p5.1.m1.1.2" xref="S3.SS3.p5.1.m1.1.2.cmml"><mn mathsize="80%" id="S3.SS3.p5.1.m1.1.2.2" xref="S3.SS3.p5.1.m1.1.2.2.cmml">1</mn><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S3.SS3.p5.1.m1.1.2.1" xref="S3.SS3.p5.1.m1.1.2.1.cmml">/</mo><mrow id="S3.SS3.p5.1.m1.1.2.3.2" xref="S3.SS3.p5.1.m1.1.2.3.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS3.p5.1.m1.1.2.3.2.1" xref="S3.SS3.p5.1.m1.1.2.3.1.1.cmml">|</mo><mi mathsize="80%" id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">V</mi><mo maxsize="80%" minsize="80%" id="S3.SS3.p5.1.m1.1.2.3.2.2" xref="S3.SS3.p5.1.m1.1.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><apply id="S3.SS3.p5.1.m1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.2"><divide id="S3.SS3.p5.1.m1.1.2.1.cmml" xref="S3.SS3.p5.1.m1.1.2.1"></divide><cn type="integer" id="S3.SS3.p5.1.m1.1.2.2.cmml" xref="S3.SS3.p5.1.m1.1.2.2">1</cn><apply id="S3.SS3.p5.1.m1.1.2.3.1.cmml" xref="S3.SS3.p5.1.m1.1.2.3.2"><abs id="S3.SS3.p5.1.m1.1.2.3.1.1.cmml" xref="S3.SS3.p5.1.m1.1.2.3.2.1"></abs><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">1/|V|</annotation></semantics></math><span id="S3.SS3.p5.2.8" class="ltx_text" style="font-size:80%;">, where </span><math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="|V|" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mrow id="S3.SS3.p5.2.m2.1.2.2" xref="S3.SS3.p5.2.m2.1.2.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS3.p5.2.m2.1.2.2.1" xref="S3.SS3.p5.2.m2.1.2.1.1.cmml">|</mo><mi mathsize="80%" id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">V</mi><mo maxsize="80%" minsize="80%" id="S3.SS3.p5.2.m2.1.2.2.2" xref="S3.SS3.p5.2.m2.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><apply id="S3.SS3.p5.2.m2.1.2.1.cmml" xref="S3.SS3.p5.2.m2.1.2.2"><abs id="S3.SS3.p5.2.m2.1.2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.2.2.1"></abs><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">|V|</annotation></semantics></math><span id="S3.SS3.p5.2.9" class="ltx_text" style="font-size:80%;">is the number of candidates).
This provides the comparison case of a maximally uninformative prior.</span></p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Likelihood</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text" style="font-size:80%;">For the likelihood, </span><math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="P(d|w)" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi mathsize="80%" id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS4.p1.1.m1.1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS4.p1.1.m1.1.1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p1.1.m1.1.1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.1.cmml"><mi mathsize="80%" id="S3.SS4.p1.1.m1.1.1.1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.1.1.1.2.cmml">d</mi><mo fence="false" mathsize="80%" id="S3.SS4.p1.1.m1.1.1.1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi mathsize="80%" id="S3.SS4.p1.1.m1.1.1.1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.1.1.1.3.cmml">w</mi></mrow><mo maxsize="80%" minsize="80%" id="S3.SS4.p1.1.m1.1.1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><times id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2"></times><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">𝑃</ci><apply id="S3.SS4.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS4.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.2">𝑑</ci><ci id="S3.SS4.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.3">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">P(d|w)</annotation></semantics></math><span id="S3.SS4.p1.1.2" class="ltx_text" style="font-size:80%;">, we use a transformation of string edit distance between the phoneme sequence produced by the child and all candidate words.
Specifically, we use exponentiated negative edit distance </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S3.SS4.p1.1.3.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Levy</span><span id="S3.SS4.p1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2008</span></a><span id="S3.SS4.p1.1.5.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S3.SS4.p1.1.6" class="ltx_text" style="font-size:80%;">:</span></p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S6.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.4" class="ltx_Math" alttext="\displaystyle P(d|w)\propto e^{-\beta~{}\times~{}\text{dist}(d^{\prime}:w^{\prime},~{}d)}" display="inline"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.cmml"><mi mathsize="80%" id="S3.E3.m1.4.4.1.3" xref="S3.E3.m1.4.4.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.2.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E3.m1.4.4.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.4.4.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.cmml"><mi mathsize="80%" id="S3.E3.m1.4.4.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml">d</mi><mo fence="false" mathsize="80%" id="S3.E3.m1.4.4.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.cmml">|</mo><mi mathsize="80%" id="S3.E3.m1.4.4.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.3.cmml">w</mi></mrow><mo maxsize="80%" minsize="80%" id="S3.E3.m1.4.4.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="80%" id="S3.E3.m1.4.4.2" xref="S3.E3.m1.4.4.2.cmml">∝</mo><msup id="S3.E3.m1.4.4.3" xref="S3.E3.m1.4.4.3.cmml"><mi mathsize="80%" id="S3.E3.m1.4.4.3.2" xref="S3.E3.m1.4.4.3.2.cmml">e</mi><mrow id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.4.cmml"><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.1.cmml"><mo mathsize="80%" id="S3.E3.m1.2.2.2.2.1a" xref="S3.E3.m1.2.2.2.2.1.cmml">−</mo><mrow id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.1.2.cmml"><mi mathsize="80%" id="S3.E3.m1.2.2.2.2.1.2.2" xref="S3.E3.m1.2.2.2.2.1.2.2.cmml">β</mi><mo lspace="0.412em" mathsize="80%" rspace="0.412em" id="S3.E3.m1.2.2.2.2.1.2.1" xref="S3.E3.m1.2.2.2.2.1.2.1.cmml">×</mo><mtext mathsize="80%" id="S3.E3.m1.2.2.2.2.1.2.3" xref="S3.E3.m1.2.2.2.2.1.2.3a.cmml">dist</mtext></mrow></mrow><mo id="S3.E3.m1.3.3.3.3.3" xref="S3.E3.m1.3.3.3.4.cmml">⁣</mo><mrow id="S3.E3.m1.3.3.3.3.2.1" xref="S3.E3.m1.3.3.3.3.2.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E3.m1.3.3.3.3.2.1.2" xref="S3.E3.m1.3.3.3.3.2.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.3.3.2.1.1" xref="S3.E3.m1.3.3.3.3.2.1.1.cmml"><msup id="S3.E3.m1.3.3.3.3.2.1.1.3" xref="S3.E3.m1.3.3.3.3.2.1.1.3.cmml"><mi mathsize="80%" id="S3.E3.m1.3.3.3.3.2.1.1.3.2" xref="S3.E3.m1.3.3.3.3.2.1.1.3.2.cmml">d</mi><mo mathsize="80%" id="S3.E3.m1.3.3.3.3.2.1.1.3.3" xref="S3.E3.m1.3.3.3.3.2.1.1.3.3.cmml">′</mo></msup><mo lspace="0.278em" mathsize="80%" rspace="0.278em" id="S3.E3.m1.3.3.3.3.2.1.1.2" xref="S3.E3.m1.3.3.3.3.2.1.1.2.cmml">:</mo><mrow id="S3.E3.m1.3.3.3.3.2.1.1.1.1" xref="S3.E3.m1.3.3.3.3.2.1.1.1.2.cmml"><msup id="S3.E3.m1.3.3.3.3.2.1.1.1.1.1" xref="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.cmml"><mi mathsize="80%" id="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.2" xref="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.2.cmml">w</mi><mo mathsize="80%" id="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.3" xref="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.3.cmml">′</mo></msup><mo mathsize="80%" rspace="0.357em" id="S3.E3.m1.3.3.3.3.2.1.1.1.1.2" xref="S3.E3.m1.3.3.3.3.2.1.1.1.2.cmml">,</mo><mi mathsize="80%" id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">d</mi></mrow></mrow><mo maxsize="80%" minsize="80%" id="S3.E3.m1.3.3.3.3.2.1.3" xref="S3.E3.m1.3.3.3.3.2.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><csymbol cd="latexml" id="S3.E3.m1.4.4.2.cmml" xref="S3.E3.m1.4.4.2">proportional-to</csymbol><apply id="S3.E3.m1.4.4.1.cmml" xref="S3.E3.m1.4.4.1"><times id="S3.E3.m1.4.4.1.2.cmml" xref="S3.E3.m1.4.4.1.2"></times><ci id="S3.E3.m1.4.4.1.3.cmml" xref="S3.E3.m1.4.4.1.3">𝑃</ci><apply id="S3.E3.m1.4.4.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.4.4.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1">conditional</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.2">𝑑</ci><ci id="S3.E3.m1.4.4.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3">𝑤</ci></apply></apply><apply id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.3.1.cmml" xref="S3.E3.m1.4.4.3">superscript</csymbol><ci id="S3.E3.m1.4.4.3.2.cmml" xref="S3.E3.m1.4.4.3.2">𝑒</ci><list id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.3"><apply id="S3.E3.m1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1"><minus id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1"></minus><apply id="S3.E3.m1.2.2.2.2.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2"><times id="S3.E3.m1.2.2.2.2.1.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1.2.1"></times><ci id="S3.E3.m1.2.2.2.2.1.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2.2">𝛽</ci><ci id="S3.E3.m1.2.2.2.2.1.2.3a.cmml" xref="S3.E3.m1.2.2.2.2.1.2.3"><mtext mathsize="56%" id="S3.E3.m1.2.2.2.2.1.2.3.cmml" xref="S3.E3.m1.2.2.2.2.1.2.3">dist</mtext></ci></apply></apply><apply id="S3.E3.m1.3.3.3.3.2.1.1.cmml" xref="S3.E3.m1.3.3.3.3.2.1"><ci id="S3.E3.m1.3.3.3.3.2.1.1.2.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.2">:</ci><apply id="S3.E3.m1.3.3.3.3.2.1.1.3.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.2.1.1.3.1.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.3">superscript</csymbol><ci id="S3.E3.m1.3.3.3.3.2.1.1.3.2.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.3.2">𝑑</ci><ci id="S3.E3.m1.3.3.3.3.2.1.1.3.3.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.3.3">′</ci></apply><list id="S3.E3.m1.3.3.3.3.2.1.1.1.2.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.1.1"><apply id="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.2">𝑤</ci><ci id="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.3.3.2.1.1.1.1.1.3">′</ci></apply><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝑑</ci></list></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\displaystyle P(d|w)\propto e^{-\beta~{}\times~{}\text{dist}(d^{\prime}:w^{\prime},~{}d)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.8" class="ltx_p"><span id="S3.SS4.p3.8.1" class="ltx_text" style="font-size:80%;">where </span><math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="dist" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mrow id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi mathsize="80%" id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.1a" xref="S3.SS4.p3.1.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.SS4.p3.1.m1.1.1.4" xref="S3.SS4.p3.1.m1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.1b" xref="S3.SS4.p3.1.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.SS4.p3.1.m1.1.1.5" xref="S3.SS4.p3.1.m1.1.1.5.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><times id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1"></times><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑑</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">𝑖</ci><ci id="S3.SS4.p3.1.m1.1.1.4.cmml" xref="S3.SS4.p3.1.m1.1.1.4">𝑠</ci><ci id="S3.SS4.p3.1.m1.1.1.5.cmml" xref="S3.SS4.p3.1.m1.1.1.5">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">dist</annotation></semantics></math><span id="S3.SS4.p3.8.2" class="ltx_text" style="font-size:80%;"> is the Levenshtein distance (minimal number of deletions, insertions and substitutions) between citation form </span><math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="d^{\prime}" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><msup id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi mathsize="80%" id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">d</mi><mo mathsize="80%" id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">𝑑</ci><ci id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">d^{\prime}</annotation></semantics></math><span id="S3.SS4.p3.8.3" class="ltx_text" style="font-size:80%;"> for candidate word </span><math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="w^{\prime}" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><msup id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mi mathsize="80%" id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml">w</mi><mo mathsize="80%" id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">superscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2">𝑤</ci><ci id="S3.SS4.p3.3.m3.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">w^{\prime}</annotation></semantics></math><span id="S3.SS4.p3.8.4" class="ltx_text" style="font-size:80%;">, designated here </span><math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="(d^{\prime}:w^{\prime})" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mrow id="S3.SS4.p3.4.m4.1.1.1" xref="S3.SS4.p3.4.m4.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS4.p3.4.m4.1.1.1.2" xref="S3.SS4.p3.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p3.4.m4.1.1.1.1" xref="S3.SS4.p3.4.m4.1.1.1.1.cmml"><msup id="S3.SS4.p3.4.m4.1.1.1.1.2" xref="S3.SS4.p3.4.m4.1.1.1.1.2.cmml"><mi mathsize="80%" id="S3.SS4.p3.4.m4.1.1.1.1.2.2" xref="S3.SS4.p3.4.m4.1.1.1.1.2.2.cmml">d</mi><mo mathsize="80%" id="S3.SS4.p3.4.m4.1.1.1.1.2.3" xref="S3.SS4.p3.4.m4.1.1.1.1.2.3.cmml">′</mo></msup><mo lspace="0.278em" mathsize="80%" rspace="0.278em" id="S3.SS4.p3.4.m4.1.1.1.1.1" xref="S3.SS4.p3.4.m4.1.1.1.1.1.cmml">:</mo><msup id="S3.SS4.p3.4.m4.1.1.1.1.3" xref="S3.SS4.p3.4.m4.1.1.1.1.3.cmml"><mi mathsize="80%" id="S3.SS4.p3.4.m4.1.1.1.1.3.2" xref="S3.SS4.p3.4.m4.1.1.1.1.3.2.cmml">w</mi><mo mathsize="80%" id="S3.SS4.p3.4.m4.1.1.1.1.3.3" xref="S3.SS4.p3.4.m4.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo maxsize="80%" minsize="80%" id="S3.SS4.p3.4.m4.1.1.1.3" xref="S3.SS4.p3.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><apply id="S3.SS4.p3.4.m4.1.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1.1"><ci id="S3.SS4.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.1">:</ci><apply id="S3.SS4.p3.4.m4.1.1.1.1.2.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.1.1.1.1.2.1.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.2">superscript</csymbol><ci id="S3.SS4.p3.4.m4.1.1.1.1.2.2.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.2.2">𝑑</ci><ci id="S3.SS4.p3.4.m4.1.1.1.1.2.3.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.2.3">′</ci></apply><apply id="S3.SS4.p3.4.m4.1.1.1.1.3.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m4.1.1.1.1.3.1.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.3">superscript</csymbol><ci id="S3.SS4.p3.4.m4.1.1.1.1.3.2.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.3.2">𝑤</ci><ci id="S3.SS4.p3.4.m4.1.1.1.1.3.3.cmml" xref="S3.SS4.p3.4.m4.1.1.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">(d^{\prime}:w^{\prime})</annotation></semantics></math><span id="S3.SS4.p3.8.5" class="ltx_text" style="font-size:80%;">, and the observed transcription </span><math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="(d)" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mrow id="S3.SS4.p3.5.m5.1.2.2"><mo maxsize="80%" minsize="80%" id="S3.SS4.p3.5.m5.1.2.2.1">(</mo><mi mathsize="80%" id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">d</mi><mo maxsize="80%" minsize="80%" id="S3.SS4.p3.5.m5.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">(d)</annotation></semantics></math><span id="S3.SS4.p3.8.6" class="ltx_text" style="font-size:80%;">.
For the results presented here, we grid sample </span><math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><mi mathsize="80%" id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><ci id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">\beta</annotation></semantics></math><span id="S3.SS4.p3.8.7" class="ltx_text" style="font-size:80%;"> values between 1 and 6 by </span><math id="S3.SS4.p3.7.m7.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S3.SS4.p3.7.m7.1a"><mn mathsize="80%" id="S3.SS4.p3.7.m7.1.1" xref="S3.SS4.p3.7.m7.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m7.1b"><cn type="float" id="S3.SS4.p3.7.m7.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m7.1c">0.1</annotation></semantics></math><span id="S3.SS4.p3.8.8" class="ltx_text" style="font-size:80%;"> increments, and take the value that assigns the highest posterior probability to a sample of 1000 communicative successes across models (</span><math id="S3.SS4.p3.8.m8.1" class="ltx_Math" alttext="\beta=3.2" display="inline"><semantics id="S3.SS4.p3.8.m8.1a"><mrow id="S3.SS4.p3.8.m8.1.1" xref="S3.SS4.p3.8.m8.1.1.cmml"><mi mathsize="80%" id="S3.SS4.p3.8.m8.1.1.2" xref="S3.SS4.p3.8.m8.1.1.2.cmml">β</mi><mo mathsize="80%" id="S3.SS4.p3.8.m8.1.1.1" xref="S3.SS4.p3.8.m8.1.1.1.cmml">=</mo><mn mathsize="80%" id="S3.SS4.p3.8.m8.1.1.3" xref="S3.SS4.p3.8.m8.1.1.3.cmml">3.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.8.m8.1b"><apply id="S3.SS4.p3.8.m8.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1"><eq id="S3.SS4.p3.8.m8.1.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1.1"></eq><ci id="S3.SS4.p3.8.m8.1.1.2.cmml" xref="S3.SS4.p3.8.m8.1.1.2">𝛽</ci><cn type="float" id="S3.SS4.p3.8.m8.1.1.3.cmml" xref="S3.SS4.p3.8.m8.1.1.3">3.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.8.m8.1c">\beta=3.2</annotation></semantics></math><span id="S3.SS4.p3.8.9" class="ltx_text" style="font-size:80%;">).
This treatment of edit distance does not take into account phoneme similarity, </span><span id="S3.SS4.p3.8.10" class="ltx_text ltx_font_italic" style="font-size:80%;">i.e., </span><span id="S3.SS4.p3.8.11" class="ltx_text" style="font-size:80%;">that certain phonemes are much more perceptually similar.
We propose another more sophisticated likelihood function that captures this in the Discussion.</span></p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text" style="font-size:80%;">All model training and analysis code, as well as the fine-tuned model can be accessed at </span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:80%;">https://osf.io/v7c3e/?view_only=176bb0f538af424da59007c53eff7e05</span><span id="S3.SS4.p4.1.2" class="ltx_text" style="font-size:80%;">.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Predicting Adult Recoveries</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:80%;">A comparison of Bayesian speech recognition models reflecting different priors reveals that the </span><span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">cdl+context</span><span id="S4.SS1.p1.1.3" class="ltx_text" style="font-size:80%;"> prior assigns the lowest average surprisal (highest average probability) to the recovered word gloss in the transcript.
As Table </span><a href="#S4.T2" title="Table 2 ‣ 4.1 Predicting Adult Recoveries ‣ 4 Results ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS1.p1.1.4" class="ltx_text" style="font-size:80%;"> reveals, BERT models making use of context perform better than those that do not.
CHILDES-tuned BERT models outperform the respective off-the-shelf BERT models.
All BERT models outperform the </span><span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CHILDES 1gram</span><span id="S4.SS1.p1.1.6" class="ltx_text" style="font-size:80%;"> model, and all models with fitted priors assign significantly higher probability to the recovered glosses than the </span><span id="S4.SS1.p1.1.7" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">UniformPrior</span><span id="S4.SS1.p1.1.8" class="ltx_text" style="font-size:80%;"> model.
These results mean that the model that is
</span>
<span id="S4.I1" class="ltx_inline-enumerate">
<span id="S4.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(1)</span> <span id="S4.I1.i1.1" class="ltx_text" style="font-size:80%;">fine-tuned to the child environment and
</span></span>
<span id="S4.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(2)</span> <span id="S4.I1.i2.1" class="ltx_text" style="font-size:80%;">uses the surrounding utterance context
</span></span>
</span><span id="S4.SS1.p1.1.9" class="ltx_text" style="font-size:80%;">
is best able to predict the recoveries made by adults.</span></p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.22.6.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S4.T2.10.5" class="ltx_text" style="font-size:113%;">Average prior surprisal on communicative successes from the Providence corpus (lower is better).
The difference in average probability assigned to the actual gloss is <math id="S4.T2.6.1.m1.1" class="ltx_Math" alttext="2^{\text{diff}}" display="inline"><semantics id="S4.T2.6.1.m1.1b"><msup id="S4.T2.6.1.m1.1.1" xref="S4.T2.6.1.m1.1.1.cmml"><mn id="S4.T2.6.1.m1.1.1.2" xref="S4.T2.6.1.m1.1.1.2.cmml">2</mn><mtext id="S4.T2.6.1.m1.1.1.3" xref="S4.T2.6.1.m1.1.1.3a.cmml">diff</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T2.6.1.m1.1c"><apply id="S4.T2.6.1.m1.1.1.cmml" xref="S4.T2.6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.6.1.m1.1.1.1.cmml" xref="S4.T2.6.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.6.1.m1.1.1.2.cmml" xref="S4.T2.6.1.m1.1.1.2">2</cn><ci id="S4.T2.6.1.m1.1.1.3a.cmml" xref="S4.T2.6.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T2.6.1.m1.1.1.3.cmml" xref="S4.T2.6.1.m1.1.1.3">diff</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.1.m1.1d">2^{\text{diff}}</annotation></semantics></math>, where <span id="S4.T2.10.5.1" class="ltx_text ltx_markedasmath">diff</span> is the difference between two model scores.<sup id="S4.T2.10.5.2" class="ltx_sup">∗</sup>Paired <math id="S4.T2.9.4.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.T2.9.4.m4.1b"><mi id="S4.T2.9.4.m4.1.1" xref="S4.T2.9.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.T2.9.4.m4.1c"><ci id="S4.T2.9.4.m4.1.1.cmml" xref="S4.T2.9.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.4.m4.1d">t</annotation></semantics></math>-tests confirm sig. differences between models, <math id="S4.T2.10.5.m5.1" class="ltx_Math" alttext="p&lt;10^{-5}" display="inline"><semantics id="S4.T2.10.5.m5.1b"><mrow id="S4.T2.10.5.m5.1.1" xref="S4.T2.10.5.m5.1.1.cmml"><mi id="S4.T2.10.5.m5.1.1.2" xref="S4.T2.10.5.m5.1.1.2.cmml">p</mi><mo id="S4.T2.10.5.m5.1.1.1" xref="S4.T2.10.5.m5.1.1.1.cmml">&lt;</mo><msup id="S4.T2.10.5.m5.1.1.3" xref="S4.T2.10.5.m5.1.1.3.cmml"><mn id="S4.T2.10.5.m5.1.1.3.2" xref="S4.T2.10.5.m5.1.1.3.2.cmml">10</mn><mrow id="S4.T2.10.5.m5.1.1.3.3" xref="S4.T2.10.5.m5.1.1.3.3.cmml"><mo id="S4.T2.10.5.m5.1.1.3.3b" xref="S4.T2.10.5.m5.1.1.3.3.cmml">−</mo><mn id="S4.T2.10.5.m5.1.1.3.3.2" xref="S4.T2.10.5.m5.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.5.m5.1c"><apply id="S4.T2.10.5.m5.1.1.cmml" xref="S4.T2.10.5.m5.1.1"><lt id="S4.T2.10.5.m5.1.1.1.cmml" xref="S4.T2.10.5.m5.1.1.1"></lt><ci id="S4.T2.10.5.m5.1.1.2.cmml" xref="S4.T2.10.5.m5.1.1.2">𝑝</ci><apply id="S4.T2.10.5.m5.1.1.3.cmml" xref="S4.T2.10.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.T2.10.5.m5.1.1.3.1.cmml" xref="S4.T2.10.5.m5.1.1.3">superscript</csymbol><cn type="integer" id="S4.T2.10.5.m5.1.1.3.2.cmml" xref="S4.T2.10.5.m5.1.1.3.2">10</cn><apply id="S4.T2.10.5.m5.1.1.3.3.cmml" xref="S4.T2.10.5.m5.1.1.3.3"><minus id="S4.T2.10.5.m5.1.1.3.3.1.cmml" xref="S4.T2.10.5.m5.1.1.3.3"></minus><cn type="integer" id="S4.T2.10.5.m5.1.1.3.3.2.cmml" xref="S4.T2.10.5.m5.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.5.m5.1d">p&lt;10^{-5}</annotation></semantics></math>.</span></figcaption>
<table id="S4.T2.11" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.11.1" class="ltx_tr">
<th id="S4.T2.11.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S4.T2.11.1.2.1" class="ltx_text" style="font-size:80%;">Model</span></th>
<th id="S4.T2.11.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<span id="S4.T2.11.1.1.1" class="ltx_text" style="font-size:80%;">Avg. Prior Surprisal</span><sup id="S4.T2.11.1.1.2" class="ltx_sup"><span id="S4.T2.11.1.1.2.1" class="ltx_text" style="font-size:80%;">∗</span></sup><span id="S4.T2.11.1.1.3" class="ltx_text" style="font-size:80%;"> (bits)</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.11.2.1" class="ltx_tr">
<th id="S4.T2.11.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.11.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL+Context</span></th>
<td id="S4.T2.11.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.11.2.1.2.1" class="ltx_text" style="font-size:80%;">3.17</span></td>
</tr>
<tr id="S4.T2.11.3.2" class="ltx_tr">
<th id="S4.T2.11.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.11.3.2.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">BERT+Context</span></th>
<td id="S4.T2.11.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.11.3.2.2.1" class="ltx_text" style="font-size:80%;">4.59</span></td>
</tr>
<tr id="S4.T2.11.4.3" class="ltx_tr">
<th id="S4.T2.11.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.11.4.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL+OneUtt</span></th>
<td id="S4.T2.11.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T2.11.4.3.2.1" class="ltx_text" style="font-size:80%;">5.28</span></td>
</tr>
<tr id="S4.T2.11.5.4" class="ltx_tr">
<th id="S4.T2.11.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.11.5.4.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">BERT+OneUtt</span></th>
<td id="S4.T2.11.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.11.5.4.2.1" class="ltx_text" style="font-size:80%;">7.09</span></td>
</tr>
<tr id="S4.T2.11.6.5" class="ltx_tr">
<th id="S4.T2.11.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.11.6.5.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CHILDES 1gram</span></th>
<td id="S4.T2.11.6.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.11.6.5.2.1" class="ltx_text" style="font-size:80%;">8.80</span></td>
</tr>
<tr id="S4.T2.11.7.6" class="ltx_tr">
<th id="S4.T2.11.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.11.7.6.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">UniformPrior</span></th>
<td id="S4.T2.11.7.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.11.7.6.2.1" class="ltx_text" style="font-size:80%;">12.95</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p"><span id="S4.SS1.p2.2.1" class="ltx_text" style="font-size:80%;">We next investigate how the prior probabilities in the previous analysis combine with likelihoods to predict word identity.
That is, how do the adults' prior expectations support inference when children's productions are more or less adult-like?
Comparing average surprisal across edit distances (Figure </span><a href="#S4.F1" title="Figure 1 ‣ 4.1 Predicting Adult Recoveries ‣ 4 Results ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p2.2.2" class="ltx_text" style="font-size:80%;">) reveals that models using BERT-based priors assign massively higher probability to word identities posited by annotators.
For child productions that are 2 phonemes away from the citation form (</span><math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="x=2" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">x</mi><mo mathsize="80%" id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn mathsize="80%" id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝑥</ci><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">x=2</annotation></semantics></math><span id="S4.SS1.p2.2.3" class="ltx_text" style="font-size:80%;"> in Fig. </span><a href="#S4.F1" title="Figure 1 ‣ 4.1 Predicting Adult Recoveries ‣ 4 Results ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p2.2.4" class="ltx_text" style="font-size:80%;">), </span><span id="S4.SS1.p2.2.5" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL+Context</span><span id="S4.SS1.p2.2.6" class="ltx_text" style="font-size:80%;"> assigns on average a probability of .24 (</span><math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="2^{-1~{}\times~{}surprisal}" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><msup id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn mathsize="80%" id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">2</mn><mrow id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml"><mo mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3a" xref="S4.SS1.p2.2.m2.1.1.3.cmml">−</mo><mrow id="S4.SS1.p2.2.m2.1.1.3.2" xref="S4.SS1.p2.2.m2.1.1.3.2.cmml"><mrow id="S4.SS1.p2.2.m2.1.1.3.2.2" xref="S4.SS1.p2.2.m2.1.1.3.2.2.cmml"><mn mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.2.2" xref="S4.SS1.p2.2.m2.1.1.3.2.2.2.cmml">1</mn><mo lspace="0.412em" mathsize="80%" rspace="0.412em" id="S4.SS1.p2.2.m2.1.1.3.2.2.1" xref="S4.SS1.p2.2.m2.1.1.3.2.2.1.cmml">×</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.2.3" xref="S4.SS1.p2.2.m2.1.1.3.2.2.3.cmml">s</mi></mrow><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.3.2.1" xref="S4.SS1.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.3" xref="S4.SS1.p2.2.m2.1.1.3.2.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.3.2.1a" xref="S4.SS1.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.4" xref="S4.SS1.p2.2.m2.1.1.3.2.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.3.2.1b" xref="S4.SS1.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.5" xref="S4.SS1.p2.2.m2.1.1.3.2.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.3.2.1c" xref="S4.SS1.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.6" xref="S4.SS1.p2.2.m2.1.1.3.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.3.2.1d" xref="S4.SS1.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.7" xref="S4.SS1.p2.2.m2.1.1.3.2.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.3.2.1e" xref="S4.SS1.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.8" xref="S4.SS1.p2.2.m2.1.1.3.2.8.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.3.2.1f" xref="S4.SS1.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.9" xref="S4.SS1.p2.2.m2.1.1.3.2.9.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.3.2.1g" xref="S4.SS1.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathsize="80%" id="S4.SS1.p2.2.m2.1.1.3.2.10" xref="S4.SS1.p2.2.m2.1.1.3.2.10.cmml">l</mi></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">2</cn><apply id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><minus id="S4.SS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3"></minus><apply id="S4.SS1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2"><times id="S4.SS1.p2.2.m2.1.1.3.2.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.1"></times><apply id="S4.SS1.p2.2.m2.1.1.3.2.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.2"><times id="S4.SS1.p2.2.m2.1.1.3.2.2.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.2.1"></times><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.2.2.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.2.2">1</cn><ci id="S4.SS1.p2.2.m2.1.1.3.2.2.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.2.3">𝑠</ci></apply><ci id="S4.SS1.p2.2.m2.1.1.3.2.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.3">𝑢</ci><ci id="S4.SS1.p2.2.m2.1.1.3.2.4.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.4">𝑟</ci><ci id="S4.SS1.p2.2.m2.1.1.3.2.5.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.5">𝑝</ci><ci id="S4.SS1.p2.2.m2.1.1.3.2.6.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.6">𝑟</ci><ci id="S4.SS1.p2.2.m2.1.1.3.2.7.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.7">𝑖</ci><ci id="S4.SS1.p2.2.m2.1.1.3.2.8.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.8">𝑠</ci><ci id="S4.SS1.p2.2.m2.1.1.3.2.9.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.9">𝑎</ci><ci id="S4.SS1.p2.2.m2.1.1.3.2.10.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2.10">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">2^{-1~{}\times~{}surprisal}</annotation></semantics></math><span id="S4.SS1.p2.2.7" class="ltx_text" style="font-size:80%;">) to the correct gloss.
This compares favorably to .12 under </span><span id="S4.SS1.p2.2.8" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">BERT+Context</span><span id="S4.SS1.p2.2.9" class="ltx_text" style="font-size:80%;">, .08 under </span><span id="S4.SS1.p2.2.10" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL+OneUtt</span><span id="S4.SS1.p2.2.11" class="ltx_text" style="font-size:80%;">, .03 under </span><span id="S4.SS1.p2.2.12" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">BERT+OneUtt</span><span id="S4.SS1.p2.2.13" class="ltx_text" style="font-size:80%;">, .006 under the </span><span id="S4.SS1.p2.2.14" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CHILDES 1gram</span><span id="S4.SS1.p2.2.15" class="ltx_text" style="font-size:80%;">, and .002 under </span><span id="S4.SS1.p2.2.16" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">UniformPrior</span><span id="S4.SS1.p2.2.17" class="ltx_text" style="font-size:80%;">.
</span><span id="S4.SS1.p2.2.18" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL+Context</span><span id="S4.SS1.p2.2.19" class="ltx_text" style="font-size:80%;"> assigns uniformly higher probability (lower surprisal) to the correct word identity, particularly when the phonetic form is more dissimilar (3 or more edits).
This means that priors support recognition more when the perceptual input is noisier.</span></p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2102.03462/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F1.4.1.1" class="ltx_text" style="font-size:113%;">Figure 1</span>: </span><span id="S4.F1.5.2" class="ltx_text" style="font-size:113%;">
Posterior surprisal (negative log probability) of the recovered meaning for communicative successes.
Error bars indicate standard error of the mean.
</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Predicting communicative failures</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="font-size:80%;">A separate question is which model best predicts whether a particular phoneme sequence will be a communicative success or failure.
We address this by testing how well posterior entropy under the models can predict communicative failures.
As with the first analysis, the </span><span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">cdl+context</span><span id="S4.SS2.p1.1.3" class="ltx_text" style="font-size:80%;"> model provides the best trade-off between the prevalence of true positives and false positives (Figure </span><a href="#S4.F2" title="Figure 2 ‣ 4.2 Predicting communicative failures ‣ 4 Results ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS2.p1.1.4" class="ltx_text" style="font-size:80%;">).
As both </span><span id="S4.SS2.p1.1.5" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">UniformPrior</span><span id="S4.SS2.p1.1.6" class="ltx_text" style="font-size:80%;"> and </span><span id="S4.SS2.p1.1.7" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CHILDES 1gram</span><span id="S4.SS2.p1.1.8" class="ltx_text" style="font-size:80%;"> models assign constant entropy to phoneme sequences (prior probabilities of candidates do not change as a function of context), their posterior entropy </span><span id="S4.SS2.p1.1.9" class="ltx_text ltx_font_italic" style="font-size:80%;">only</span><span id="S4.SS2.p1.1.10" class="ltx_text" style="font-size:80%;"> reflects the contribution of the perceptual data.
This analysis provides converging evidence that a model that is tuned specifically to child language and uses the surrounding utterance context — the one that best instantiates child-directed listening — is best able to replicate adult inferences.</span></p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2102.03462/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="322" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.4.1.1" class="ltx_text" style="font-size:113%;">Figure 2</span>: </span><span id="S4.F2.5.2" class="ltx_text" style="font-size:113%;">
Classification performance in predicting communicative failures, as measured by the ROC of posterior entropy.
The solid line with slope = 1 indicates chance.
The area above this line indicates better classification performance.
</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Quantifying prior vs. posterior information</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="font-size:80%;">Finally, we quantify the information gain over time in conditioning on context (the fitted priors), conditioning on data (the posterior under the </span><span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">UniformPrior</span><span id="S4.SS3.p1.1.3" class="ltx_text" style="font-size:80%;"> model), and conditioning on both (the posteriors corresponding to the fitted priors).
This analysis shows a larger shift in the probability distribution over candidates (greater information gain) going from the uniform prior to the </span><span id="S4.SS3.p1.1.4" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">cdl+context</span><span id="S4.SS3.p1.1.5" class="ltx_text" style="font-size:80%;"> prior compared to going from the uniform prior to its corresponding posterior (red line vs. green line in panel 1 of Figure </span><a href="#S4.F3" title="Figure 3 ‣ 4.3 Quantifying prior vs. posterior information ‣ 4 Results ‣ Child-directed Listening: How Caregiver Inference Enables Children's Early Verbal Communication" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS3.p1.1.6" class="ltx_text" style="font-size:80%;">).
That is, the prior under the </span><span id="S4.SS3.p1.1.7" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">CDL+Context</span><span id="S4.SS3.p1.1.8" class="ltx_text" style="font-size:80%;"> model contributes </span><span id="S4.SS3.p1.1.9" class="ltx_text ltx_font_italic" style="font-size:80%;">more</span><span id="S4.SS3.p1.1.10" class="ltx_text" style="font-size:80%;"> information (better constrains guesses to word identity) than perceptual information alone.
Contrary to our predictions, we find that the information gain for the prior is relatively constant over time for the CHILDES-fitted models.
This suggests that child-directed listening can helpfully constrain adult listeners' interpretations of children's earliest verbal productions.
As expected, children's improving articulatory abilities result in an increase of all models' posteriors over developmental time, as the likelihood function shared across models is able to contribute more and more to the task of interpretation.</span></p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2102.03462/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.1.1" class="ltx_text" style="font-size:113%;">Figure 3</span>: </span><span id="S4.F3.5.2" class="ltx_text" style="font-size:113%;">
Average information gain from conditioning word prediction on context only (red, corresponding to the prior), perceptual data only (green), and context and perceptual data (blue, corresponding to the posterior) relative to a uniform prior.

</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:80%;">Language development is often characterized in terms of an increasing facility with processes on the side of the learner: developing motor planning, recognizing regularities of linguistic structure at different levels, and relating structure to entities and communicative contexts in the world.
The current work suggests that early verbal communication depends not only on these well-studied developmental processes, but also on cognitive processes in the minds of adult caregivers.</span></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text" style="font-size:80%;">We note two limitations with the current work before discussing its implications.
First, the simple measure of edit distance does not capture the perceptual confusability of phonemes: </span><span id="S5.p2.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">bug</span><span id="S5.p2.1.3" class="ltx_text" style="font-size:80%;"> and </span><span id="S5.p2.1.4" class="ltx_text ltx_font_italic" style="font-size:80%;">rug</span><span id="S5.p2.1.5" class="ltx_text" style="font-size:80%;"> are equally good candidates for </span><span id="S5.p2.1.6" class="ltx_text ltx_font_italic" style="font-size:80%;">pug</span><span id="S5.p2.1.7" class="ltx_text" style="font-size:80%;">. One potential elaboration would be to use a </span><span id="S5.p2.1.8" class="ltx_text ltx_font_italic" style="font-size:80%;">weighted</span><span id="S5.p2.1.9" class="ltx_text" style="font-size:80%;"> edit distance measure that takes into account the perceptual confusability of the phonemes.
For example using a </span><span id="S5.p2.1.10" class="ltx_text ltx_font_italic" style="font-size:80%;">probabilistic finite state string transducer</span><span id="S5.p2.1.11" class="ltx_text" style="font-size:80%;"> would allow assigning edits different ``costs'' according to experimentally-obtained confusion probabilities, </span><span id="S5.p2.1.12" class="ltx_text ltx_font_italic" style="font-size:80%;">e.g., <cite class="ltx_cite ltx_citemacro_citep"><span id="S5.p2.1.12.1.1" class="ltx_text ltx_font_upright">(</span><span class="ltx_text" style="font-size:80%;">Cutler </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S5.p2.1.12.2.2.1.1" class="ltx_text ltx_font_upright">, </span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2004</span></a><span id="S5.p2.1.12.3.3" class="ltx_text ltx_font_upright">)</span></cite></span><span id="S5.p2.1.13" class="ltx_text" style="font-size:80%;">.</span></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text" style="font-size:80%;">Second, we make the simplifying assumption that inferences made by adult annotators in the lab are representative of the inferences made by adult caregivers in the moment, communicating in real time with children.
While the inferential capacities of annotators are likely substantially </span><span id="S5.p3.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">less</span><span id="S5.p3.1.3" class="ltx_text" style="font-size:80%;"> than those of adult caregivers (who have access to the non-linguistic context, as well as significantly more shared history with the child), research assistants may well be a decent proxy for adult listeners, due to their training as transcribers and exposure to child language.
Potential differences in the inferential capacities of caregivers relative to other adult ``listeners'' should be tested experimentally.
</span></p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text" style="font-size:80%;">These results additionally call attention to the interpretation of common methods in child language research.
For example, vocabulary production measures on the Communicative Development Inventories </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.p4.1.2.1" class="ltx_text" style="font-size:80%;">(</span><span class="ltx_text" style="font-size:80%;">Fenson </span><span class="ltx_ERROR undefined">\BOthers</span><span class="ltx_text" style="font-size:80%;">.</span><span id="S5.p4.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span><span class="ltx_text" style="font-size:80%;">2007</span></a><span id="S5.p4.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S5.p4.1.5" class="ltx_text" style="font-size:80%;">, have been historically interpreted as an index of children's vocabulary and articulatory maturity.
However, the current work suggests that successful communication – adult recognition of a word as a conventional form — relies additionally on adult inferential processes.
Indeed the measure of a word's ``babiness,''
a significant predictor of the order of children's reported vocabulary production, may reflect the degree to which a word is more likely in child-directed speech compared to adult-directed speech.</span></p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text" style="font-size:80%;">Furthermore, our data invite a reconstrual of the nature of feedback in early language development.
For example, if we assume that successful communication is itself reinforcing, child-directed </span><span id="S5.p5.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">listening</span><span id="S5.p5.1.3" class="ltx_text" style="font-size:80%;"> might provide feedback to the child learner even in the absence of child-directed </span><span id="S5.p5.1.4" class="ltx_text ltx_font_italic" style="font-size:80%;">speech</span><span id="S5.p5.1.5" class="ltx_text" style="font-size:80%;">: a caregiver who interprets a child's production of ``uh'' to mean ``up'' may not </span><span id="S5.p5.1.6" class="ltx_text ltx_font_italic" style="font-size:80%;">say</span><span id="S5.p5.1.7" class="ltx_text" style="font-size:80%;"> anything in response to the child's production, but provides feedback by effecting change on the part of the child when they pick the child up.
This, in turn, leads to new puzzles: if adult caregivers can help many deficient communicative acts succeed, what presses children to get better?</span></p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text" style="font-size:80%;">Finally, we speculate regarding the role that child-directed listening might contribute to the emergence of language, both on evolutionary timescales and cases of rapid language emergence like Nicaraguan Sign Language.
The current work suggests that successful recovery of meaning from child speech acts reflect not only the inductive biases, linguistic knowledge, and articulatory maturity of speakers, but also the inferential biases of listeners.</span></p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:80%;">We present a suite of Bayesian models of spoken word recognition to characterize the process of </span><span id="S6.p1.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">child-directed listening</span><span id="S6.p1.1.3" class="ltx_text" style="font-size:80%;">, or how adult caregivers find meaning in the noisy and often non-conventional speech productions of young children.
We find that priors capitalizing on recent neural architectures — when trained specifically on child speech samples, and taking advantage of the greater linguistic context to make predictions — are best able to simulate adult inferential processes when interpreting noisy child speech. This research paves the way for understanding how children learn to employ language as goal-seeking agents in the presence of others.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.12.12.1" class="ltx_text" style="font-size:80%;">Cutler </span><span id="bib.bib1.13.13.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib1.14.14.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib1.15.15.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib1.16.16.5" class="ltx_text" style="font-size:80%;">2004)</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.18.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib1.19.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib1.20.3" class="ltx_text" style="font-size:80%;">cutlerEtAl2004</span><span id="bib.bib1.21.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib1.22.5" class="ltx_text" style="font-size:80%;">Cutler, A., Weber, A., Smits, R.</span><span id="bib.bib1.23.6" class="ltx_ERROR undefined">\BCBL</span><span id="bib.bib1.24.7" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib1.25.8" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib1.26.9" class="ltx_text" style="font-size:80%;"> Cooper, N. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.27.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib1.28.2" class="ltx_text" style="font-size:80%;">2004Dec.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.29.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib1.30.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib1.31.3" class="ltx_text" style="font-size:80%;">Patterns of English phoneme confusions by native
and non-native listeners Patterns of English phoneme confusions by
native and non-native listeners.</span><span id="bib.bib1.32.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib1.33.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.34.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span><span id="bib.bib1.35.2" class="ltx_text" style="font-size:80%;">J Acoust Soc Am11663668–3678.
</span><span id="bib.bib1.36.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib1.37.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib1.38.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.12.12.1" class="ltx_text" style="font-size:80%;">Demuth </span><span id="bib.bib2.13.13.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib2.14.14.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib2.15.15.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib2.16.16.5" class="ltx_text" style="font-size:80%;">2006)</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.18.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib2.19.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib2.20.3" class="ltx_text" style="font-size:80%;">demuthEtAl2006</span><span id="bib.bib2.21.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib2.22.5" class="ltx_text" style="font-size:80%;">Demuth, K., Culbertson, J.</span><span id="bib.bib2.23.6" class="ltx_ERROR undefined">\BCBL</span><span id="bib.bib2.24.7" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib2.25.8" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib2.26.9" class="ltx_text" style="font-size:80%;"> Alter, J. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.27.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib2.28.2" class="ltx_text" style="font-size:80%;">2006.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.29.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib2.30.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib2.31.3" class="ltx_text" style="font-size:80%;">Word-minimality, epenthesis and coda licensing in the
early acquisition of English Word-minimality, epenthesis and coda
licensing in the early acquisition of English.</span><span id="bib.bib2.32.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib2.33.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.34.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span><span id="bib.bib2.35.2" class="ltx_text" style="font-size:80%;">Lang Speech492137–174.
</span><span id="bib.bib2.36.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib2.37.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib2.38.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.12.12.1" class="ltx_text" style="font-size:80%;">Devlin </span><span id="bib.bib3.13.13.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib3.14.14.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib3.15.15.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib3.16.16.5" class="ltx_text" style="font-size:80%;">2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.18.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib3.19.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib3.20.3" class="ltx_text" style="font-size:80%;">devlin2018</span><span id="bib.bib3.21.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib3.22.5" class="ltx_text" style="font-size:80%;">Devlin, J., Chang, M</span><span id="bib.bib3.23.6" class="ltx_ERROR undefined">\BHBI</span><span id="bib.bib3.24.7" class="ltx_text" style="font-size:80%;">W., Lee, K.</span><span id="bib.bib3.25.8" class="ltx_ERROR undefined">\BCBL</span><span id="bib.bib3.26.9" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib3.27.10" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib3.28.11" class="ltx_text" style="font-size:80%;"> Toutanova, K. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.29.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib3.30.2" class="ltx_text" style="font-size:80%;">2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.31.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib3.32.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib3.33.3" class="ltx_text" style="font-size:80%;">BERT: Pre-training of Deep Bidirectional Transformers
for Language Understanding BERT: Pre-training of deep bidirectional
transformers for language understanding.</span><span id="bib.bib3.34.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib3.35.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.36.1" class="ltx_ERROR undefined">\BIn</span><span id="bib.bib3.37.2" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib3.38.3" class="ltx_ERROR undefined">\APACrefbtitle</span><span id="bib.bib3.39.4" class="ltx_text" style="font-size:80%;">Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (</span><span id="bib.bib3.40.5" class="ltx_ERROR undefined">\BPGS</span><span id="bib.bib3.41.6" class="ltx_text" style="font-size:80%;"> 4171–4186).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.42.1" class="ltx_ERROR undefined">\APACaddressPublisher</span><span id="bib.bib3.43.2" class="ltx_text" style="font-size:80%;">Association for Computational Linguistics.
</span><span id="bib.bib3.44.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib3.45.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib3.46.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.10.10.1" class="ltx_text" style="font-size:80%;">Fenson </span><span id="bib.bib4.11.11.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib4.12.12.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib4.13.13.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib4.14.14.5" class="ltx_text" style="font-size:80%;">2007)</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.16.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib4.17.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib4.18.3" class="ltx_text" style="font-size:80%;">fenson2007macarthur</span><span id="bib.bib4.19.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib4.20.5" class="ltx_text" style="font-size:80%;">Fenson, L.</span><span id="bib.bib4.21.6" class="ltx_ERROR undefined">\BCBT</span><span id="bib.bib4.22.7" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib4.23.8" class="ltx_ERROR undefined">\BOthersPeriod</span><span id="bib.bib4.24.9" class="ltx_text" style="font-size:80%;">.
 </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.25.1" class="ltx_ERROR undefined">\APACrefYear</span><span id="bib.bib4.26.2" class="ltx_text" style="font-size:80%;">2007.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.27.1" class="ltx_ERROR undefined">\APACrefbtitle</span><span id="bib.bib4.28.2" class="ltx_text" style="font-size:80%;">MacArthur-Bates communicative development inventories
Macarthur-Bates communicative development inventories.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.29.1" class="ltx_ERROR undefined">\APACaddressPublisher</span><span id="bib.bib4.30.2" class="ltx_text" style="font-size:80%;">Paul H. Brookes Publishing Company Baltimore, MD.
</span><span id="bib.bib4.31.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib4.32.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib4.33.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.12.12.1" class="ltx_text" style="font-size:80%;">Gibson </span><span id="bib.bib5.13.13.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib5.14.14.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib5.15.15.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib5.16.16.5" class="ltx_text" style="font-size:80%;">2013)</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.18.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib5.19.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib5.20.3" class="ltx_text" style="font-size:80%;">gibsonBergenPiantadosi2013</span><span id="bib.bib5.21.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib5.22.5" class="ltx_text" style="font-size:80%;">Gibson, E., Bergen, L.</span><span id="bib.bib5.23.6" class="ltx_ERROR undefined">\BCBL</span><span id="bib.bib5.24.7" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib5.25.8" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib5.26.9" class="ltx_text" style="font-size:80%;"> Piantadosi, S. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.27.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib5.28.2" class="ltx_text" style="font-size:80%;">2013.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.29.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib5.30.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib5.31.3" class="ltx_text" style="font-size:80%;">Rational integration of noisy evidence and prior
semantic expectations in sentence interpretation Rational integration
of noisy evidence and prior semantic expectations in sentence
interpretation.</span><span id="bib.bib5.32.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib5.33.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.34.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span><span id="bib.bib5.35.2" class="ltx_text" style="font-size:80%;">Proceedings Natl. Acad. Sci.
U.S.A.110208051–8056.
</span><span id="bib.bib5.36.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib5.37.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib5.38.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.12.12.1" class="ltx_text" style="font-size:80%;">Golinkoff </span><span id="bib.bib6.13.13.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib6.14.14.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib6.15.15.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib6.16.16.5" class="ltx_text" style="font-size:80%;">2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.18.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib6.19.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib6.20.3" class="ltx_text" style="font-size:80%;">golinkoff2015baby</span><span id="bib.bib6.21.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib6.22.5" class="ltx_text" style="font-size:80%;">Golinkoff, R</span><span id="bib.bib6.23.6" class="ltx_ERROR undefined">\BPBI</span><span id="bib.bib6.24.7" class="ltx_text" style="font-size:80%;">M., Can, D</span><span id="bib.bib6.25.8" class="ltx_ERROR undefined">\BPBI</span><span id="bib.bib6.26.9" class="ltx_text" style="font-size:80%;">D., Soderstrom, M.</span><span id="bib.bib6.27.10" class="ltx_ERROR undefined">\BCBL</span><span id="bib.bib6.28.11" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib6.29.12" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib6.30.13" class="ltx_text" style="font-size:80%;"> Hirsh-Pasek, K. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.31.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib6.32.2" class="ltx_text" style="font-size:80%;">2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.33.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib6.34.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib6.35.3" class="ltx_text" style="font-size:80%;">(Baby)Talk to Me: The Social Context of Infant-Directed
Speech and Its Effects on Early Language Acquisition (Baby)Talk to Me:
The Social Context of Infant-Directed Speech and Its Effects on Early
Language Acquisition.</span><span id="bib.bib6.36.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib6.37.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.38.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span><span id="bib.bib6.39.2" class="ltx_text" style="font-size:80%;">Current Directions in Psychological
Science245339–344.
</span><span id="bib.bib6.40.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib6.41.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib6.42.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.12.12.1" class="ltx_text" style="font-size:80%;">Jawahar </span><span id="bib.bib7.13.13.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib7.14.14.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib7.15.15.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib7.16.16.5" class="ltx_text" style="font-size:80%;">2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.18.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib7.19.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib7.20.3" class="ltx_text" style="font-size:80%;">jawahar2019</span><span id="bib.bib7.21.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib7.22.5" class="ltx_text" style="font-size:80%;">Jawahar, G., Sagot, B.</span><span id="bib.bib7.23.6" class="ltx_ERROR undefined">\BCBL</span><span id="bib.bib7.24.7" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib7.25.8" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib7.26.9" class="ltx_text" style="font-size:80%;"> Seddah, D. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.27.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib7.28.2" class="ltx_text" style="font-size:80%;">2019</span><span id="bib.bib7.29.3" class="ltx_ERROR undefined">\APACmonth</span><span id="bib.bib7.30.4" class="ltx_text" style="font-size:80%;">07.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.31.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib7.32.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib7.33.3" class="ltx_text" style="font-size:80%;">What Does BERT Learn about the Structure of Language?
What does BERT learn about the structure of language?</span><span id="bib.bib7.34.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib7.35.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.36.1" class="ltx_ERROR undefined">\BIn</span><span id="bib.bib7.37.2" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib7.38.3" class="ltx_ERROR undefined">\APACrefbtitle</span><span id="bib.bib7.39.4" class="ltx_text" style="font-size:80%;">Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics (</span><span id="bib.bib7.40.5" class="ltx_ERROR undefined">\BPGS</span><span id="bib.bib7.41.6" class="ltx_text" style="font-size:80%;"> 3651–3657).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.42.1" class="ltx_ERROR undefined">\APACaddressPublisher</span><span id="bib.bib7.43.2" class="ltx_text" style="font-size:80%;">Association for Computational Linguistics.
</span><span id="bib.bib7.44.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib7.45.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib7.46.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.6.6.1" class="ltx_text" style="font-size:80%;">Levy (</span><span id="bib.bib8.7.7.2" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib8.8.8.3" class="ltx_text" style="font-size:80%;">2008)</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.10.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib8.11.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib8.12.3" class="ltx_text" style="font-size:80%;">levy2008noisy</span><span id="bib.bib8.13.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib8.14.5" class="ltx_text" style="font-size:80%;">Levy, R. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.15.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib8.16.2" class="ltx_text" style="font-size:80%;">2008.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.17.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib8.18.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib8.19.3" class="ltx_text" style="font-size:80%;">A noisy-channel model of human sentence comprehension
under uncertain input A noisy-channel model of human sentence comprehension
under uncertain input.</span><span id="bib.bib8.20.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib8.21.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.22.1" class="ltx_ERROR undefined">\BIn</span><span id="bib.bib8.23.2" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib8.24.3" class="ltx_ERROR undefined">\APACrefbtitle</span><span id="bib.bib8.25.4" class="ltx_text" style="font-size:80%;">Proceedings of the 2008 conference on Empirical Methods
in Natural Language Processing Proceedings of the 2008 conference on
Empirical Methods in Natural Language Processing (</span><span id="bib.bib8.26.5" class="ltx_ERROR undefined">\BPGS</span><span id="bib.bib8.27.6" class="ltx_text" style="font-size:80%;"> 234–243).
</span><span id="bib.bib8.28.7" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib8.29.8" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib8.30.9" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.10.10.1" class="ltx_text" style="font-size:80%;">Norris </span><span id="bib.bib9.11.11.2" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib9.12.12.3" class="ltx_text" style="font-size:80%;"> McQueen (</span><span id="bib.bib9.13.13.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib9.14.14.5" class="ltx_text" style="font-size:80%;">2008)</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.16.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib9.17.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib9.18.3" class="ltx_text" style="font-size:80%;">norrisMcQueen2008</span><span id="bib.bib9.19.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib9.20.5" class="ltx_text" style="font-size:80%;">Norris, D.</span><span id="bib.bib9.21.6" class="ltx_ERROR undefined">\BCBT</span><span id="bib.bib9.22.7" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib9.23.8" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib9.24.9" class="ltx_text" style="font-size:80%;"> McQueen, J</span><span id="bib.bib9.25.10" class="ltx_ERROR undefined">\BPBI</span><span id="bib.bib9.26.11" class="ltx_text" style="font-size:80%;">M. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.27.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib9.28.2" class="ltx_text" style="font-size:80%;">2008Apr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.29.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib9.30.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib9.31.3" class="ltx_text" style="font-size:80%;">Shortlist B: a Bayesian model of continuous
speech recognition Shortlist B: a Bayesian model of continuous
speech recognition.</span><span id="bib.bib9.32.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib9.33.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.34.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span><span id="bib.bib9.35.2" class="ltx_text" style="font-size:80%;">Psychol Rev1152357–395.
</span><span id="bib.bib9.36.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib9.37.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib9.38.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.10.10.1" class="ltx_text" style="font-size:80%;">Rose </span><span id="bib.bib10.11.11.2" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib10.12.12.3" class="ltx_text" style="font-size:80%;"> MacWhinney (</span><span id="bib.bib10.13.13.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib10.14.14.5" class="ltx_text" style="font-size:80%;">2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.16.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib10.17.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib10.18.3" class="ltx_text" style="font-size:80%;">rose2014phonBank</span><span id="bib.bib10.19.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib10.20.5" class="ltx_text" style="font-size:80%;">Rose, Y.</span><span id="bib.bib10.21.6" class="ltx_ERROR undefined">\BCBT</span><span id="bib.bib10.22.7" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib10.23.8" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib10.24.9" class="ltx_text" style="font-size:80%;"> MacWhinney, B. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.25.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib10.26.2" class="ltx_text" style="font-size:80%;">2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.27.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib10.28.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib10.29.3" class="ltx_text" style="font-size:80%;">The PhonBank project The PhonBank
project.</span><span id="bib.bib10.30.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib10.31.5" class="ltx_text" style="font-size:80%;">.
</span><span id="bib.bib10.32.6" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib10.33.7" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib10.34.8" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.12.12.1" class="ltx_text" style="font-size:80%;">Salazar </span><span id="bib.bib11.13.13.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib11.14.14.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib11.15.15.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib11.16.16.5" class="ltx_text" style="font-size:80%;">2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.18.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib11.19.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib11.20.3" class="ltx_text" style="font-size:80%;">salazarEtAl2020</span><span id="bib.bib11.21.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib11.22.5" class="ltx_text" style="font-size:80%;">Salazar, J., Liang, D., Nguyen, T</span><span id="bib.bib11.23.6" class="ltx_ERROR undefined">\BPBI</span><span id="bib.bib11.24.7" class="ltx_text" style="font-size:80%;">Q.</span><span id="bib.bib11.25.8" class="ltx_ERROR undefined">\BCBL</span><span id="bib.bib11.26.9" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib11.27.10" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib11.28.11" class="ltx_text" style="font-size:80%;"> Kirchhoff, K. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.29.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib11.30.2" class="ltx_text" style="font-size:80%;">2020</span><span id="bib.bib11.31.3" class="ltx_ERROR undefined">\APACmonth</span><span id="bib.bib11.32.4" class="ltx_text" style="font-size:80%;">07.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.33.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib11.34.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib11.35.3" class="ltx_text" style="font-size:80%;">Masked Language Model Scoring Masked language model
scoring.</span><span id="bib.bib11.36.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib11.37.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.38.1" class="ltx_ERROR undefined">\BIn</span><span id="bib.bib11.39.2" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib11.40.3" class="ltx_ERROR undefined">\APACrefbtitle</span><span id="bib.bib11.41.4" class="ltx_text" style="font-size:80%;">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics (</span><span id="bib.bib11.42.5" class="ltx_ERROR undefined">\BPGS</span><span id="bib.bib11.43.6" class="ltx_text" style="font-size:80%;"> 2699–2712).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.44.1" class="ltx_ERROR undefined">\APACaddressPublisher</span><span id="bib.bib11.45.2" class="ltx_text" style="font-size:80%;">OnlineAssociation for Computational Linguistics.
</span><span id="bib.bib11.46.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib11.47.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib11.48.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.10.10.1" class="ltx_text" style="font-size:80%;">Sanchez </span><span id="bib.bib12.11.11.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib12.12.12.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib12.13.13.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib12.14.14.5" class="ltx_text" style="font-size:80%;">2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.16.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib12.17.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib12.18.3" class="ltx_text" style="font-size:80%;">sanchez2019</span><span id="bib.bib12.19.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib12.20.5" class="ltx_text" style="font-size:80%;">Sanchez, A., Meylan, S., Braginsky, M., MacDonald, K., Yurovsky, D.</span><span id="bib.bib12.21.6" class="ltx_ERROR undefined">\BCBL</span><span id="bib.bib12.22.7" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib12.23.8" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib12.24.9" class="ltx_text" style="font-size:80%;"> Frank, M. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.25.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib12.26.2" class="ltx_text" style="font-size:80%;">2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.27.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib12.28.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib12.29.3" class="ltx_text" style="font-size:80%;">childes-db: A flexible and reproducible interface to the
child language data exchange system childes-db: A flexible and reproducible
interface to the child language data exchange system.</span><span id="bib.bib12.30.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib12.31.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.32.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span><span id="bib.bib12.33.2" class="ltx_text" style="font-size:80%;">Behavior Research Methods5141928–1941.
</span><span id="bib.bib12.34.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib12.35.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib12.36.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.6.6.1" class="ltx_text" style="font-size:80%;">Shannon (</span><span id="bib.bib13.7.7.2" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib13.8.8.3" class="ltx_text" style="font-size:80%;">1951)</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.10.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib13.11.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib13.12.3" class="ltx_text" style="font-size:80%;">shannon1951</span><span id="bib.bib13.13.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib13.14.5" class="ltx_text" style="font-size:80%;">Shannon, C. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.15.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib13.16.2" class="ltx_text" style="font-size:80%;">1951.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.17.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib13.18.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib13.19.3" class="ltx_text" style="font-size:80%;">Prediction and Entropy of Printed English
Prediction and Entropy of Printed English.</span><span id="bib.bib13.20.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib13.21.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.22.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span><span id="bib.bib13.23.2" class="ltx_text" style="font-size:80%;">Bell Systems Technical Journal3050–64.
</span><span id="bib.bib13.24.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib13.25.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib13.26.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.10.10.1" class="ltx_text" style="font-size:80%;">Snow </span><span id="bib.bib14.11.11.2" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib14.12.12.3" class="ltx_text" style="font-size:80%;"> Ferguson (</span><span id="bib.bib14.13.13.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib14.14.14.5" class="ltx_text" style="font-size:80%;">1977)</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.16.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib14.17.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib14.18.3" class="ltx_text" style="font-size:80%;">snow1977talking</span><span id="bib.bib14.19.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib14.20.5" class="ltx_text" style="font-size:80%;">Snow, C</span><span id="bib.bib14.21.6" class="ltx_ERROR undefined">\BPBI</span><span id="bib.bib14.22.7" class="ltx_text" style="font-size:80%;">E.</span><span id="bib.bib14.23.8" class="ltx_ERROR undefined">\BCBT</span><span id="bib.bib14.24.9" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib14.25.10" class="ltx_ERROR undefined">\BBA</span><span id="bib.bib14.26.11" class="ltx_text" style="font-size:80%;"> Ferguson, C</span><span id="bib.bib14.27.12" class="ltx_ERROR undefined">\BPBI</span><span id="bib.bib14.28.13" class="ltx_text" style="font-size:80%;">A. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.29.1" class="ltx_ERROR undefined">\APACrefYear</span><span id="bib.bib14.30.2" class="ltx_text" style="font-size:80%;">1977.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.31.1" class="ltx_ERROR undefined">\APACrefbtitle</span><span id="bib.bib14.32.2" class="ltx_text" style="font-size:80%;">Talking to Children Talking to children.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.33.1" class="ltx_ERROR undefined">\APACaddressPublisher</span><span id="bib.bib14.34.2" class="ltx_text" style="font-size:80%;">Cambridge University Press.
</span><span id="bib.bib14.35.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib14.36.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib14.37.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.6.6.1" class="ltx_text" style="font-size:80%;">Soderstrom (</span><span id="bib.bib15.7.7.2" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib15.8.8.3" class="ltx_text" style="font-size:80%;">2007)</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.10.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib15.11.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib15.12.3" class="ltx_text" style="font-size:80%;">soderstrom2007beyond</span><span id="bib.bib15.13.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib15.14.5" class="ltx_text" style="font-size:80%;">Soderstrom, M. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.15.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib15.16.2" class="ltx_text" style="font-size:80%;">2007.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.17.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib15.18.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib15.19.3" class="ltx_text" style="font-size:80%;">Beyond babytalk: Re-evaluating the nature and content
of speech input to preverbal infants Beyond babytalk: Re-evaluating the
nature and content of speech input to preverbal infants.</span><span id="bib.bib15.20.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib15.21.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.22.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span><span id="bib.bib15.23.2" class="ltx_text" style="font-size:80%;">Developmental Review274501–532.
</span><span id="bib.bib15.24.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib15.25.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib15.26.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.10.10.1" class="ltx_text" style="font-size:80%;">Wolf </span><span id="bib.bib16.11.11.2" class="ltx_ERROR undefined">\BOthers</span><span id="bib.bib16.12.12.3" class="ltx_text" style="font-size:80%;">. (</span><span id="bib.bib16.13.13.4" class="ltx_ERROR undefined">\APACyear</span><span id="bib.bib16.14.14.5" class="ltx_text" style="font-size:80%;">2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.16.1" class="ltx_text" style="font-size:80%;">
</span><span id="bib.bib16.17.2" class="ltx_ERROR undefined">\APACinsertmetastar</span><span id="bib.bib16.18.3" class="ltx_text" style="font-size:80%;">wolfEtAl2020</span><span id="bib.bib16.19.4" class="ltx_ERROR undefined">{APACrefauthors}</span><span id="bib.bib16.20.5" class="ltx_text" style="font-size:80%;">Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A.</span><span id="bib.bib16.21.6" class="ltx_ERROR undefined">\BDBL</span><span id="bib.bib16.22.7" class="ltx_text" style="font-size:80%;">Rush, A</span><span id="bib.bib16.23.8" class="ltx_ERROR undefined">\BPBI</span><span id="bib.bib16.24.9" class="ltx_text" style="font-size:80%;">M. </span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.25.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span><span id="bib.bib16.26.2" class="ltx_text" style="font-size:80%;">2020</span><span id="bib.bib16.27.3" class="ltx_ERROR undefined">\APACmonth</span><span id="bib.bib16.28.4" class="ltx_text" style="font-size:80%;">10.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.29.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib16.30.2" class="ltx_ERROR undefined">\APACrefatitle</span><span id="bib.bib16.31.3" class="ltx_text" style="font-size:80%;">Transformers: State-of-the-Art Natural Language
Processing Transformers: State-of-the-art natural language
processing.</span><span id="bib.bib16.32.4" class="ltx_ERROR undefined">\BBCQ</span><span id="bib.bib16.33.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.34.1" class="ltx_ERROR undefined">\BIn</span><span id="bib.bib16.35.2" class="ltx_text" style="font-size:80%;"> </span><span id="bib.bib16.36.3" class="ltx_ERROR undefined">\APACrefbtitle</span><span id="bib.bib16.37.4" class="ltx_text" style="font-size:80%;">Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations (</span><span id="bib.bib16.38.5" class="ltx_ERROR undefined">\BPGS</span><span id="bib.bib16.39.6" class="ltx_text" style="font-size:80%;"> 38–45).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.40.1" class="ltx_ERROR undefined">\APACaddressPublisher</span><span id="bib.bib16.41.2" class="ltx_text" style="font-size:80%;">OnlineAssociation for Computational Linguistics.
</span><span id="bib.bib16.42.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib16.43.4" class="ltx_ERROR undefined">\CurrentBib</span><span id="bib.bib16.44.5" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2102.03461" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2102.03462" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2102.03462">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2102.03462" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2102.03463" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 05:49:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
