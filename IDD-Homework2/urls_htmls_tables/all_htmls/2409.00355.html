<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion</title>
<!--Generated on Sat Aug 31 05:25:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.00355v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S1" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S1.SS0.SSS0.Px1" title="In 1 Introduction ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Instructor-side personalization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S1.SS0.SSS0.Px2" title="In 1 Introduction ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Student-side personalization.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>YA-TA</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS1" title="In 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS1.SSS0.Px1" title="In 2.1 Data Setup ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Instructor-side data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS1.SSS0.Px2" title="In 2.1 Data Setup ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Student-side data.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS2" title="In 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span><span class="ltx_text ltx_font_smallcaps">DRaKe</span> Framework</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS2.SSS0.Px1" title="In 2.2 DRaKe Framework ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Dual retrieval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS2.SSS0.Px2" title="In 2.2 DRaKe Framework ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Knowledge fusion.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS3" title="In 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>User Interface: Video Referencing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS1" title="In 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>G-Eval</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS1.SSS1" title="In 3.1 G-Eval ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS1.SSS1.Px1" title="In 3.1.1 Experiment Setup ‣ 3.1 G-Eval ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Test set construction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS1.SSS1.Px2" title="In 3.1.1 Experiment Setup ‣ 3.1 G-Eval ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Baselines.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS1.SSS2" title="In 3.1 G-Eval ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Evaluation Criteria</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS1.SSS3" title="In 3.1 G-Eval ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>G-Eval Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS2" title="In 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Case Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS2.SSS1" title="In 3.2 Case Study ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS2.SSS1.Px1" title="In 3.2.1 Experiment Setup ‣ 3.2 Case Study ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Course setting.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS2.SSS1.Px2" title="In 3.2.1 Experiment Setup ‣ 3.2 Case Study ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Student setting.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS2.SSS1.Px3" title="In 3.2.1 Experiment Setup ‣ 3.2 Case Study ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Example of personalized response.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.SS2.SSS1.Px4" title="In 3.2.1 Experiment Setup ‣ 3.2 Case Study ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title">Failure cases and limitation.</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S4" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Extension of YA-TA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S5" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S6" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S7" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S8" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#A1" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>YA-TA Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#A2" title="In YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>G-Eval Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#A2.SS1" title="In Appendix B G-Eval Details ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Student Information</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#A2.SS2" title="In Appendix B G-Eval Details ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Questions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#A2.SS3" title="In Appendix B G-Eval Details ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>G-Eval Criteria</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Dongil Yang</span>   <span class="ltx_text ltx_font_bold" id="id2.2.id2">Suyeon Lee<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex1.1.1.1">1</span></span></span></span></span></span>   <span class="ltx_text ltx_font_bold" id="id3.3.id3">Minjin Kim</span>
<br class="ltx_break"/>  <span class="ltx_text ltx_font_bold" id="id4.4.id4">Jungsoo Won</span>  <span class="ltx_text ltx_font_bold" id="id5.5.id5">Namyoung Kim</span>  <span class="ltx_text ltx_font_bold" id="id6.6.id6">Dongha Lee</span>  <span class="ltx_text ltx_font_bold" id="id7.7.id7">Jinyoung Yeo<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">2</span></span></span></span></span>
<br class="ltx_break"/></span>Yonsei University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id8.8.id8">{wingu, isuy.groot, donalee, jinyeo}@yonsei.ac.kr
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes">  Equal contribution  Co-corresponding authors</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Engagement between instructors and students plays a crucial role in enhancing students’ academic performance. However, instructors often struggle to provide timely and personalized support in large classes. To address this challenge, we propose a novel Virtual Teaching Assistant (VTA) named YA-TA, designed to offer responses to students that are grounded in lectures and are easy to understand. To facilitate YA-TA, we introduce the Dual Retrieval-augmented Knowledge Fusion (<span class="ltx_text ltx_font_smallcaps" id="id9.id1.1">DRaKe</span>) framework, which incorporates dual retrieval of instructor and student knowledge and knowledge fusion for tailored response generation. Experiments conducted in real-world classroom settings demonstrate that the <span class="ltx_text ltx_font_smallcaps" id="id9.id1.2">DRaKe</span> framework excels in aligning responses with knowledge retrieved from both instructor and student sides. Furthermore, we offer additional extensions of YA-TA, such as a Q&amp;A board and self-practice tools to enhance the overall learning experience. Our video is publicly available.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
Video: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://youtu.be/y2EucPEUgZc" title="">https://youtu.be/y2EucPEUgZc</a></span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tr" id="p1.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1">
Dongil Yang<span class="ltx_note ltx_role_thanks" id="p1.1.2.1.1.1.1.1.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>  Equal contribution</span></span></span>   Suyeon Lee<span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex3.1.1.1">1</span></span></span></span></span>   Minjin Kim</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.1">  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.1">Jungsoo Won</span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.2">Namyoung Kim</span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.3">Dongha Lee<span class="ltx_note ltx_role_thanks" id="p1.1.2.1.1.2.1.3.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.2.1.3.1.1">  Co-corresponding authors</span></span></span></span></span>  <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.4">Jinyoung Yeo<span class="ltx_note ltx_role_footnotemark" id="footnotex4"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex4.1.1.1">2</span></span></span></span></span></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.1">Yonsei University</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.4.1.1">{wingu, isuy.groot, donalee, jinyeo}@yonsei.ac.kr</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="577" id="S1.F1.g1" src="extracted/5824914/images/motivation.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The motivating example of YA-TA. A typical LLM faces challenges in providing responses that consider both instructor and student sides. YA-TA addresses these issues by employing a <span class="ltx_text ltx_font_smallcaps" id="S1.F1.2.1">DRaKe</span> framework.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Active interaction between instructors and students, including tailored feedback to student questions, significantly enhances academic performance <cite class="ltx_cite ltx_citemacro_citep">(Agwu and Nmadu, <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib1" title="">2023</a>)</cite>. However, when an instructor is responsible for a larger number of students, providing personalized responses to every query becomes challenging. Although Teaching Assistants (TAs) are often employed to address this issue, they frequently struggle to offer timely and personalized responses, while consuming significant manpower and resources <cite class="ltx_cite ltx_citemacro_citep">(Hicke et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib7" title="">2023a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">This situation underscores the pressing demand for Virtual Teaching Assistants (VTAs) capable of providing personalized tutoring unrestricted by time or location. Large Language Models (LLMs) have demonstrated remarkable conversational capabilities, making LLM-powered TAs well-suited to serve as effective VTAs <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib3" title="">2023</a>)</cite>. VTAs have two key objectives to enhance educational effectiveness: 1) enhancing instructor convenience by responding to students’ questions in a manner that aligns with the instructor’s teaching style (<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">i.e.</span>, instructor-side personalization) and 2) assisting student learning by offering tailored support (<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">i.e.</span>, student-side personalization) <cite class="ltx_cite ltx_citemacro_citep">(College, <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib5" title="">2024</a>)</cite>.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Instructor-side personalization.</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">To enhance instructor convenience, instructors must find the TA reliable and satisfactory, allowing them to delegate Q&amp;A tasks confidently. To meet this end, the TA’s responses should align with the instructor’s lecture, ensuring no conflict between the instructor’s explanations and the TA’s response.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Student-side personalization.</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p1.1">To support student learning effectively, the TA must provide answers that match students’ comprehension levels, helping them deepen their understanding of the course. Given the various academic backgrounds among students, the TA should assess each student’s knowledge based on their information and tailor responses accordingly.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p2.1">There have been several efforts to construct VTAs. <cite class="ltx_cite ltx_citemacro_citet">Dong (<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib6" title="">2023</a>); Matsuda and Frank (<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib13" title="">2024</a>)</cite> aim to build instructor-personalized VTAs by utilizing Retrieval Augmented Generation (RAG) to generate answers based on external course materials. However, these works do not consider that students’ understanding level of the course varies due to different academic backgrounds. On the other hand,  <cite class="ltx_cite ltx_citemacro_citet">Park et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib14" title="">2024</a>)</cite> construct student-personalized VTAs by providing responses that consider students’ learning styles. However, they do not base their responses on the instructor’s teaching style.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p3.1">To consider both sides of personalization, we introduce YA-TA (Yonsei Academic Teaching Assistant), which, to the best of our knowledge, is the first multi-turn question-answering (QA) agent that incorporates personalization for both instructors and students. However, achieving personalization for both sides is challenging, as it requires integrating information from multiple sources.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p4">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p4.1">To tackle this challenge, we propose <span class="ltx_text ltx_font_smallcaps" id="S1.SS0.SSS0.Px2.p4.1.1">DRaKe</span> (<span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.SS0.SSS0.Px2.p4.1.2">D</span>ual <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.SS0.SSS0.Px2.p4.1.3">R</span>etrieval-<span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.SS0.SSS0.Px2.p4.1.4">a</span>ugmented <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.SS0.SSS0.Px2.p4.1.5">K</span>nowledg<span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S1.SS0.SSS0.Px2.p4.1.6">e</span> Fusion) framework, which consists of two steps before response generation: 1) Dual Retrieval and 2) Knowledge Fusion by integrating retrieved knowledge. On the instructor side, we retrieve the instructor’s statements related to the student’s query. On the student side, we retrieve the academic information about the student, such as the courses the student has previously taken and their grades. Subsequently, we leverage LLMs’ Chain-of-Thought (CoT) abilities <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib18" title="">2022</a>)</cite> to reason over the retrieved knowledge from both sides and generate responses by blending this knowledge.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p5">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p5.1">This approach ensures that the responses align with the instructor’s philosophy and are adapted to the student’s background. To demonstrate the effectiveness of our method, we conduct experiments on real-world classes. Evaluation results and case study demonstrate that our <span class="ltx_text ltx_font_smallcaps" id="S1.SS0.SSS0.Px2.p5.1.1">DRaKe</span> framework significantly enhances personalization for both instructors and students. Additionally, we offer extensions like a Q&amp;A Board and Self-Practice, which further enrich the student’s learning experience.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>YA-TA</h2>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="410" id="S2.F2.g1" src="extracted/5824914/images/overview-2.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overview architecture of YA-TA. The image of the response is a screenshot of the YA-TA user interface. In YA-TA’s final response, the part highlighted in <span class="ltx_text" id="S2.F2.3.1" style="color:#1E90FF;">blue</span> indicates where instructor-side personalization is evident, while the part highlighted in <span class="ltx_text" id="S2.F2.4.2" style="color:#FF8C00;">orange</span> indicates areas where student-side personalization is evident.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.4">YA-TA is a multi-turn QA system that aims to generate a reliable and comprehensible response to a student’s query. Formally, given the instructor-side knowledge <math alttext="K_{I}" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">K</mi><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">𝐾</ci><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">K_{I}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">italic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math>, the student-side knowledge <math alttext="K_{S}" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">K</mi><mi id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">𝐾</ci><ci id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">K_{S}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>, and the dialogue context <math alttext="D_{t}=\{q_{1},r_{1},...,r_{t-1},q_{t}\}" class="ltx_Math" display="inline" id="S2.p1.3.m3.5"><semantics id="S2.p1.3.m3.5a"><mrow id="S2.p1.3.m3.5.5" xref="S2.p1.3.m3.5.5.cmml"><msub id="S2.p1.3.m3.5.5.6" xref="S2.p1.3.m3.5.5.6.cmml"><mi id="S2.p1.3.m3.5.5.6.2" xref="S2.p1.3.m3.5.5.6.2.cmml">D</mi><mi id="S2.p1.3.m3.5.5.6.3" xref="S2.p1.3.m3.5.5.6.3.cmml">t</mi></msub><mo id="S2.p1.3.m3.5.5.5" xref="S2.p1.3.m3.5.5.5.cmml">=</mo><mrow id="S2.p1.3.m3.5.5.4.4" xref="S2.p1.3.m3.5.5.4.5.cmml"><mo id="S2.p1.3.m3.5.5.4.4.5" stretchy="false" xref="S2.p1.3.m3.5.5.4.5.cmml">{</mo><msub id="S2.p1.3.m3.2.2.1.1.1" xref="S2.p1.3.m3.2.2.1.1.1.cmml"><mi id="S2.p1.3.m3.2.2.1.1.1.2" xref="S2.p1.3.m3.2.2.1.1.1.2.cmml">q</mi><mn id="S2.p1.3.m3.2.2.1.1.1.3" xref="S2.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p1.3.m3.5.5.4.4.6" xref="S2.p1.3.m3.5.5.4.5.cmml">,</mo><msub id="S2.p1.3.m3.3.3.2.2.2" xref="S2.p1.3.m3.3.3.2.2.2.cmml"><mi id="S2.p1.3.m3.3.3.2.2.2.2" xref="S2.p1.3.m3.3.3.2.2.2.2.cmml">r</mi><mn id="S2.p1.3.m3.3.3.2.2.2.3" xref="S2.p1.3.m3.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.p1.3.m3.5.5.4.4.7" xref="S2.p1.3.m3.5.5.4.5.cmml">,</mo><mi id="S2.p1.3.m3.1.1" mathvariant="normal" xref="S2.p1.3.m3.1.1.cmml">…</mi><mo id="S2.p1.3.m3.5.5.4.4.8" xref="S2.p1.3.m3.5.5.4.5.cmml">,</mo><msub id="S2.p1.3.m3.4.4.3.3.3" xref="S2.p1.3.m3.4.4.3.3.3.cmml"><mi id="S2.p1.3.m3.4.4.3.3.3.2" xref="S2.p1.3.m3.4.4.3.3.3.2.cmml">r</mi><mrow id="S2.p1.3.m3.4.4.3.3.3.3" xref="S2.p1.3.m3.4.4.3.3.3.3.cmml"><mi id="S2.p1.3.m3.4.4.3.3.3.3.2" xref="S2.p1.3.m3.4.4.3.3.3.3.2.cmml">t</mi><mo id="S2.p1.3.m3.4.4.3.3.3.3.1" xref="S2.p1.3.m3.4.4.3.3.3.3.1.cmml">−</mo><mn id="S2.p1.3.m3.4.4.3.3.3.3.3" xref="S2.p1.3.m3.4.4.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.p1.3.m3.5.5.4.4.9" xref="S2.p1.3.m3.5.5.4.5.cmml">,</mo><msub id="S2.p1.3.m3.5.5.4.4.4" xref="S2.p1.3.m3.5.5.4.4.4.cmml"><mi id="S2.p1.3.m3.5.5.4.4.4.2" xref="S2.p1.3.m3.5.5.4.4.4.2.cmml">q</mi><mi id="S2.p1.3.m3.5.5.4.4.4.3" xref="S2.p1.3.m3.5.5.4.4.4.3.cmml">t</mi></msub><mo id="S2.p1.3.m3.5.5.4.4.10" stretchy="false" xref="S2.p1.3.m3.5.5.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.5b"><apply id="S2.p1.3.m3.5.5.cmml" xref="S2.p1.3.m3.5.5"><eq id="S2.p1.3.m3.5.5.5.cmml" xref="S2.p1.3.m3.5.5.5"></eq><apply id="S2.p1.3.m3.5.5.6.cmml" xref="S2.p1.3.m3.5.5.6"><csymbol cd="ambiguous" id="S2.p1.3.m3.5.5.6.1.cmml" xref="S2.p1.3.m3.5.5.6">subscript</csymbol><ci id="S2.p1.3.m3.5.5.6.2.cmml" xref="S2.p1.3.m3.5.5.6.2">𝐷</ci><ci id="S2.p1.3.m3.5.5.6.3.cmml" xref="S2.p1.3.m3.5.5.6.3">𝑡</ci></apply><set id="S2.p1.3.m3.5.5.4.5.cmml" xref="S2.p1.3.m3.5.5.4.4"><apply id="S2.p1.3.m3.2.2.1.1.1.cmml" xref="S2.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p1.3.m3.2.2.1.1.1.1.cmml" xref="S2.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.p1.3.m3.2.2.1.1.1.2.cmml" xref="S2.p1.3.m3.2.2.1.1.1.2">𝑞</ci><cn id="S2.p1.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S2.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S2.p1.3.m3.3.3.2.2.2.cmml" xref="S2.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p1.3.m3.3.3.2.2.2.1.cmml" xref="S2.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.p1.3.m3.3.3.2.2.2.2.cmml" xref="S2.p1.3.m3.3.3.2.2.2.2">𝑟</ci><cn id="S2.p1.3.m3.3.3.2.2.2.3.cmml" type="integer" xref="S2.p1.3.m3.3.3.2.2.2.3">1</cn></apply><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">…</ci><apply id="S2.p1.3.m3.4.4.3.3.3.cmml" xref="S2.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p1.3.m3.4.4.3.3.3.1.cmml" xref="S2.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S2.p1.3.m3.4.4.3.3.3.2.cmml" xref="S2.p1.3.m3.4.4.3.3.3.2">𝑟</ci><apply id="S2.p1.3.m3.4.4.3.3.3.3.cmml" xref="S2.p1.3.m3.4.4.3.3.3.3"><minus id="S2.p1.3.m3.4.4.3.3.3.3.1.cmml" xref="S2.p1.3.m3.4.4.3.3.3.3.1"></minus><ci id="S2.p1.3.m3.4.4.3.3.3.3.2.cmml" xref="S2.p1.3.m3.4.4.3.3.3.3.2">𝑡</ci><cn id="S2.p1.3.m3.4.4.3.3.3.3.3.cmml" type="integer" xref="S2.p1.3.m3.4.4.3.3.3.3.3">1</cn></apply></apply><apply id="S2.p1.3.m3.5.5.4.4.4.cmml" xref="S2.p1.3.m3.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.p1.3.m3.5.5.4.4.4.1.cmml" xref="S2.p1.3.m3.5.5.4.4.4">subscript</csymbol><ci id="S2.p1.3.m3.5.5.4.4.4.2.cmml" xref="S2.p1.3.m3.5.5.4.4.4.2">𝑞</ci><ci id="S2.p1.3.m3.5.5.4.4.4.3.cmml" xref="S2.p1.3.m3.5.5.4.4.4.3">𝑡</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.5c">D_{t}=\{q_{1},r_{1},...,r_{t-1},q_{t}\}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.5d">italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = { italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }</annotation></semantics></math> which ends with the student’s query, YA-TA’s goal is to generate a response <math alttext="r_{t}" class="ltx_Math" display="inline" id="S2.p1.4.m4.1"><semantics id="S2.p1.4.m4.1a"><msub id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">r</mi><mi id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">𝑟</ci><ci id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m4.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="S2.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{t}=f(D_{t},K_{I},K_{S})" class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><msub id="S2.E1.m1.3.3.5" xref="S2.E1.m1.3.3.5.cmml"><mi id="S2.E1.m1.3.3.5.2" xref="S2.E1.m1.3.3.5.2.cmml">r</mi><mi id="S2.E1.m1.3.3.5.3" xref="S2.E1.m1.3.3.5.3.cmml">t</mi></msub><mo id="S2.E1.m1.3.3.4" xref="S2.E1.m1.3.3.4.cmml">=</mo><mrow id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml"><mi id="S2.E1.m1.3.3.3.5" xref="S2.E1.m1.3.3.3.5.cmml">f</mi><mo id="S2.E1.m1.3.3.3.4" xref="S2.E1.m1.3.3.3.4.cmml">⁢</mo><mrow id="S2.E1.m1.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.4.cmml"><mo id="S2.E1.m1.3.3.3.3.3.4" stretchy="false" xref="S2.E1.m1.3.3.3.3.4.cmml">(</mo><msub id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">D</mi><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S2.E1.m1.3.3.3.3.3.5" xref="S2.E1.m1.3.3.3.3.4.cmml">,</mo><msub id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.2.cmml">K</mi><mi id="S2.E1.m1.2.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.2.2.3.cmml">I</mi></msub><mo id="S2.E1.m1.3.3.3.3.3.6" xref="S2.E1.m1.3.3.3.3.4.cmml">,</mo><msub id="S2.E1.m1.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.cmml"><mi id="S2.E1.m1.3.3.3.3.3.3.2" xref="S2.E1.m1.3.3.3.3.3.3.2.cmml">K</mi><mi id="S2.E1.m1.3.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.3.cmml">S</mi></msub><mo id="S2.E1.m1.3.3.3.3.3.7" stretchy="false" xref="S2.E1.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><eq id="S2.E1.m1.3.3.4.cmml" xref="S2.E1.m1.3.3.4"></eq><apply id="S2.E1.m1.3.3.5.cmml" xref="S2.E1.m1.3.3.5"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.5.1.cmml" xref="S2.E1.m1.3.3.5">subscript</csymbol><ci id="S2.E1.m1.3.3.5.2.cmml" xref="S2.E1.m1.3.3.5.2">𝑟</ci><ci id="S2.E1.m1.3.3.5.3.cmml" xref="S2.E1.m1.3.3.5.3">𝑡</ci></apply><apply id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"><times id="S2.E1.m1.3.3.3.4.cmml" xref="S2.E1.m1.3.3.3.4"></times><ci id="S2.E1.m1.3.3.3.5.cmml" xref="S2.E1.m1.3.3.3.5">𝑓</ci><vector id="S2.E1.m1.3.3.3.3.4.cmml" xref="S2.E1.m1.3.3.3.3.3"><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">𝐷</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2.2">𝐾</ci><ci id="S2.E1.m1.2.2.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3">𝐼</ci></apply><apply id="S2.E1.m1.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.2">𝐾</ci><ci id="S2.E1.m1.3.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.3">𝑆</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">r_{t}=f(D_{t},K_{I},K_{S})</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_f ( italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.3">To achieve this, we propose Dual Retrieval-augmented Knowledge Fusion (<span class="ltx_text ltx_font_smallcaps" id="S2.p3.3.1">DRaKe</span>), which 1) concurrently retrieves <math alttext="K_{I}" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><msub id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml"><mi id="S2.p3.1.m1.1.1.2" xref="S2.p3.1.m1.1.1.2.cmml">K</mi><mi id="S2.p3.1.m1.1.1.3" xref="S2.p3.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><apply id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p3.1.m1.1.1.1.cmml" xref="S2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.p3.1.m1.1.1.2.cmml" xref="S2.p3.1.m1.1.1.2">𝐾</ci><ci id="S2.p3.1.m1.1.1.3.cmml" xref="S2.p3.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">K_{I}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="K_{S}" class="ltx_Math" display="inline" id="S2.p3.2.m2.1"><semantics id="S2.p3.2.m2.1a"><msub id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mi id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml">K</mi><mi id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1">subscript</csymbol><ci id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2">𝐾</ci><ci id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">K_{S}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.1d">italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> and 2) integrates them in the response via knowledge fusion module <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S2.p3.3.m3.1"><semantics id="S2.p3.3.m3.1a"><mrow id="S2.p3.3.m3.1.2" xref="S2.p3.3.m3.1.2.cmml"><mi id="S2.p3.3.m3.1.2.2" xref="S2.p3.3.m3.1.2.2.cmml">f</mi><mo id="S2.p3.3.m3.1.2.1" xref="S2.p3.3.m3.1.2.1.cmml">⁢</mo><mrow id="S2.p3.3.m3.1.2.3.2" xref="S2.p3.3.m3.1.2.cmml"><mo id="S2.p3.3.m3.1.2.3.2.1" stretchy="false" xref="S2.p3.3.m3.1.2.cmml">(</mo><mo id="S2.p3.3.m3.1.1" lspace="0em" rspace="0em" xref="S2.p3.3.m3.1.1.cmml">⋅</mo><mo id="S2.p3.3.m3.1.2.3.2.2" stretchy="false" xref="S2.p3.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><apply id="S2.p3.3.m3.1.2.cmml" xref="S2.p3.3.m3.1.2"><times id="S2.p3.3.m3.1.2.1.cmml" xref="S2.p3.3.m3.1.2.1"></times><ci id="S2.p3.3.m3.1.2.2.cmml" xref="S2.p3.3.m3.1.2.2">𝑓</ci><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">f(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.p3.3.m3.1d">italic_f ( ⋅ )</annotation></semantics></math>. We explain the data setup process (<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS1" title="2.1 Data Setup ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>), <span class="ltx_text ltx_font_smallcaps" id="S2.p3.3.2">DRaKe</span> framework (<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS2" title="2.2 DRaKe Framework ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>), and the user interface (<a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.SS3" title="2.3 User Interface: Video Referencing ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>) in this section. The overview of YA-TA is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.F2" title="Figure 2 ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Setup</h3>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Instructor-side data.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">As YA-TA is based on LLMs, we must handle all data in textual form. When the instructor uploads lecture videos of a course to our system, we extract the audio and run an off-the-shelf automatic speech recognition model to transcribe it into textual segments.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/openai/whisper-large-v3" title="">https://huggingface.co/openai/whisper-large-v3</a></span></span></span> Each segment contains transcribed text along with its corresponding timestamp indicating the start and end times of the audio or the video. We store the video and text segments for each lecture in the <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px1.p1.1.1">instructor course database</span>, organized by course ID.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Student-side data.</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.2">For the student-side data, we utilize an <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.2.1">academic information system</span> that contains students’ transcripts and a <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.2.2">student query database</span> that stores past queries. A transcript includes the student’s name, major, semester, and grades (<span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.2.3">e.g.</span> <math alttext="A^{+}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px2.p1.1.m1.1a"><msup id="S2.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">A</mi><mo id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2">𝐴</ci><plus id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.1.m1.1c">A^{+}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.1.m1.1d">italic_A start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="B^{-}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS1.SSS0.Px2.p1.2.m2.1a"><msup id="S2.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">B</mi><mo id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.2">𝐵</ci><minus id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.2.m2.1c">B^{-}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.2.m2.1d">italic_B start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT</annotation></semantics></math>, <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.2.4">etc.</span>) of all courses enrolled in the past and represents his or her overall academic performance. While we may use a system from an actual institution, we manually construct a number of transcripts for demonstration purposes. <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.2.5">Student query database</span> contains a collection of query records of a student. Each record contains queries submitted by the student about a specific course across multiple sessions. Such record indicates the student’s comprehension level within the scope of the course.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span class="ltx_text ltx_font_smallcaps" id="S2.SS2.1.1">DRaKe</span> Framework</h3>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Dual retrieval.</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.3">We retrieve knowledge from both instructor and student sides to equip YA-TA with resources to generate reliable and comprehensive responses.
In the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.3.1">instructor knowledge</span> retrieval step, we first fetch segments corresponding to a particular course from the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.3.2">instructor course database</span>. Among them, we select top <math alttext="k" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> segments by ensembling a sparse and a dense retriever to account for both lexical and semantic similarities when forming <math alttext="K_{I}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS2.SSS0.Px1.p1.2.m2.1a"><msub id="S2.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">K</mi><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝐾</ci><ci id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.2.m2.1c">K_{I}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.2.m2.1d">italic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Implementation details are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#A1" title="Appendix A YA-TA Details ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span> In the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.3.3">student knowledge</span> retrieval step, we fetch a student’s transcript from the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.3.4">academic information system</span> by the student ID and query record from the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.3.5">student query database</span> by the course ID. We combine them to form <math alttext="K_{S}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S2.SS2.SSS0.Px1.p1.3.m3.1a"><msub id="S2.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">K</mi><mi id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝐾</ci><ci id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.3.m3.1c">K_{S}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.3.m3.1d">italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> to encompass both the overall and course-specific performances of the student. For example, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.F2" title="Figure 2 ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">2</span></a>, when a student named Kelly asks a question about course CS50, YA-TA retrieves a number of relevant segments from CS50 as the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.3.6">instructor knowledge</span>, along with Kelly’s transcript and query record as the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.3.7">student knowledge</span>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.1" style="width:424.9pt;height:88.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-93.1pt,19.2pt) scale(0.69539634622599,0.69539634622599) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.1">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.1.2" rowspan="2"><span class="ltx_text" id="S2.T1.1.1.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S2.T1.1.1.1.3">Criteria for Instructor Pers.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S2.T1.1.1.1.4">Criteria for Student Pers.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.1.5">Criterion for Both Pers.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.2">Groundedness</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.3">Helpfulness</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.4">Comprehensiveness</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.5">Overall</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.3.1">GPT-3.5-Turbo</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.3.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.3">3.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.4">4.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.5">4.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.6">4.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.7">3.48</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.1">GPT-4o</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.3">4.12</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4">4.04</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.5">4.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.6">4.16</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.7">3.5</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.5.1" rowspan="4">
<span class="ltx_ERROR undefined" id="S2.T1.1.1.5.1.1">\hdashline</span><span class="ltx_text" id="S2.T1.1.1.5.1.2">GPT-3.5-Turbo</span>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.5.2">+ Instructor Knowledge</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.3.1">4.56</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.4.1">4.82</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5">4.68</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.6">4.36</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.7">3.76</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.6.1">+ Student Knowledge</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.2">3.92</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.3">4.18</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.6.4.1">4.8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.5"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.6.5.1">4.46</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S2.T1.1.1.6.6.1">3.94</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.7.1">+ <span class="ltx_text ltx_font_smallcaps" id="S2.T1.1.1.7.1.1">DRaKe</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S2.T1.1.1.7.2.1">4.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S2.T1.1.1.7.3.1">4.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S2.T1.1.1.7.4.1">4.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S2.T1.1.1.7.5.1">4.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.6"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.7.6.1">4.06</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>G-Eval result between YATA and other models. The best results for each base model are <span class="ltx_text ltx_font_bold" id="S2.T1.4.1">bolded</span> and the second-best result is <span class="ltx_text ltx_framed ltx_framed_underline" id="S2.T1.5.2">underlined</span>.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Knowledge fusion.</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">The main goal of the knowledge fusion module <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p1.1.m1.1a"><mrow id="S2.SS2.SSS0.Px2.p1.1.m1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2.cmml"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.1.2.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2.2.cmml">f</mi><mo id="S2.SS2.SSS0.Px2.p1.1.m1.1.2.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px2.p1.1.m1.1.2.3.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2.cmml"><mo id="S2.SS2.SSS0.Px2.p1.1.m1.1.2.3.2.1" stretchy="false" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2.cmml">(</mo><mo id="S2.SS2.SSS0.Px2.p1.1.m1.1.1" lspace="0em" rspace="0em" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">⋅</mo><mo id="S2.SS2.SSS0.Px2.p1.1.m1.1.2.3.2.2" stretchy="false" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2"><times id="S2.SS2.SSS0.Px2.p1.1.m1.1.2.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2.1"></times><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.2.2">𝑓</ci><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.1.m1.1c">f(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.1.m1.1d">italic_f ( ⋅ )</annotation></semantics></math> is to generate a reliable and comprehensible response to the query by integrating <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.1">instructor knowledge</span> and <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.2">student knowledge</span>. However, simply injecting them into the response generator may not produce the best response as each knowledge is composed of raw data. Therefore, we abstract each knowledge to a higher level utilizing the reasoning ability of an LLM, before passing it to the response generator. For the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.3">instructor knowledge</span>, we use an LLM to extract useful segments as <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.4">evidence</span> that provide necessary information to answer the query. We extract rather than interpret to minimize any deviation from the instructor’s exact words, which reflect their principles regarding the course topic. As the <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.5">student knowledge</span> contains raw information such as a transcript or a query record (<span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.6">i.e.</span>, list of past queries), we use an LLM to transform it into a <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.7">plan</span>. This <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.8">plan</span> serves as a helpful guide for the response generator, enabling it to personalize the response for the student. Finally, we feed <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.9">evidence</span> and <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.10">plan</span> into the response generator, which then effectively integrates them to produce a response that is both grounded in the lecture and tailored to the student. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.F2" title="Figure 2 ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">2</span></a>, YA-TA’s response, processed through the <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.SSS0.Px2.p1.1.11">DRaKe</span> framework, demonstrates a seamless fusion of <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.12">instructor knowledge</span> and <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.13">student knowledge</span>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>User Interface: Video Referencing</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">As a system designed to assist students learn effectively, we not only offer personalized responses but also enhance the overall learning experience through the user interface. Below the response, we embed the lecture video paused at the exact timestamp of the referenced segment. By replaying the video, students can grasp the full context of the instructor’s lecture regarding the response. We control this interface using the title of the lecture video and the timestamp of the referenced segment, both of which are seamlessly generated during the decoding process of the response generator.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To evaluate the efficacy of the <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">DRaKe</span> framework in achieving personalization for both instructors and students, we employ two complementary methods: 1) G-Eval <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib11" title="">2023</a>)</cite> to quantitatively assess YA-TA’s responses across multiple criteria, and 2) case studies to qualitatively analyze the <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">DRaKe</span> framework.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>G-Eval</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Experiment Setup</h4>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Test set construction.</h5>
<div class="ltx_para" id="S3.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px1.p1.1">We generate the test set for evaluation by simulating a scenario where students with various academic backgrounds ask different questions about the lectures, and YA-TA provides answers to each question. We select an English course for computer science (CS50 from Harvard University<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We obtained permission to use the lecture videos for this paper from the lecturer. The videos can be accessed at the following link: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/watch?v=8mAITcNt710" title="">https://www.youtube.com/watch?v=8mAITcNt710</a></span></span></span>) as the testbed. Then, we extract potential questions from the lectures using <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.Px1.p1.1.1">GPT-3.5-Turbo</span>. One of the authors, who is a CS expert, filters 10 high-quality questions from the questions extracted by <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.Px1.p1.1.2">GPT-3.5-Turbo</span>. Additionally, we create profiles for 5 students with diverse majors and academic backgrounds. As each question is matched with multiple student profiles, we generate 50 test sets, each comprising a query, student knowledge, and instructor knowledge.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Baselines.</h5>
<div class="ltx_para" id="S3.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS1.Px2.p1.1">We set two baselines using GPT-3.5-Turbo and GPT-4o, where both are provided only the dialogue context without any retrieved knowledge.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>In this section, we utilize <span class="ltx_text ltx_font_typewriter" id="footnote5.1">gpt-3.5-turbo-0125</span> and <span class="ltx_text ltx_font_typewriter" id="footnote5.2">gpt-4o-2024-05-13</span>.</span></span></span> Additionally, we conduct an ablation study to investigate the effect of each type of knowledge. These models are then instructed to generate responses to queries from the test set.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Evaluation Criteria</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">We employ G-Eval <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib11" title="">2023</a>)</cite> to assess performance across various criteria, scoring from 0 to 5. Instructor-side metrics are: (1) <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p1.1.1">Precision</span>: Does the answer provide necessary information without redundancy?; (2) <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p1.1.2">Groundedness</span>: Is the answer aligned with the instructor’s statements and teaching philosophy? Student-side metrics are: (1) <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p1.1.3">Helpfulness</span>: How satisfied is the student likely to be?; (2) <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p1.1.4">Comprehensiveness</span>: Does the answer appropriately consider the student’s academic ability? Lastly, for both sides: (1) <span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p1.1.5">Overall</span>: Does the response align with the instructor’s statements and reflect the student’s information?</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>G-Eval Results</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S2.T1" title="Table 1 ‣ Dual retrieval. ‣ 2.2 DRaKe Framework ‣ 2 YA-TA ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">1</span></a> shows that retrieving information from just one side outperforms dual retrieval, which highlights the challenge of achieving personalization on both sides. Additionally, the highest performance achieved by the <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.SSS3.p1.1.1">DRaKe</span> framework when both sides are considered together demonstrates that our framework excels at integrating knowledge from both perspectives.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.5">
<tr class="ltx_tr" id="S3.T2.5.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T2.5.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.6.1.1">
<span class="ltx_p" id="S3.T2.5.6.1.1.1" style="width:433.6pt;"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.T2.5.6.1.1.1.1">Social Science<span class="ltx_text ltx_font_medium" id="S3.T2.5.6.1.1.1.1.1"> (Yonsei GPP6003)</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1" style="width:433.6pt;"><span class="ltx_text" id="S3.T2.1.1.1.1.1.1" style="position:relative; bottom:-2.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S3.T2.1.1.1.1.1.1.g1" src="extracted/5824914/images/icon/student-info.png" width="14"/></span> <span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1.2">Student Information</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.7.1.1">
<span class="ltx_p" id="S3.T2.5.7.1.1.1" style="width:433.6pt;">Major: Artificial Intelligence</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.8.1.1">
<span class="ltx_p" id="S3.T2.5.8.1.1.1" style="width:433.6pt;">Degree: Graduate Student</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.9.1.1">
<span class="ltx_p" id="S3.T2.5.9.1.1.1" style="width:433.6pt;">Semester: 3</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.10.1.1">
<span class="ltx_p" id="S3.T2.5.10.1.1.1" style="width:433.6pt;">Grade: (Data Structures, A), (Algorithms, A+), …</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.2.1.1">
<span class="ltx_p" id="S3.T2.2.2.1.1.1" style="width:433.6pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="S3.T2.2.2.1.1.1.g1" src="extracted/5824914/images/icon/question.png" width="12"/> <span class="ltx_text ltx_font_bold" id="S3.T2.2.2.1.1.1.1">Student’s Query</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.11.1.1">
<span class="ltx_p" id="S3.T2.5.11.1.1.1" style="width:433.6pt;">Explain the relationship between capitalism and democracy.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.3.1.1">
<span class="ltx_p" id="S3.T2.3.3.1.1.1" style="width:433.6pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="10" id="S3.T2.3.3.1.1.1.g1" src="extracted/5824914/images/icon/evidence.png" width="10"/> <span class="ltx_text ltx_font_bold" id="S3.T2.3.3.1.1.1.1">Evidence</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.12.1.1">
<span class="ltx_p" id="S3.T2.5.12.1.1.1" style="width:433.6pt;">However, <span class="ltx_text ltx_font_bold" id="S3.T2.5.12.1.1.1.1">it is generally considered difficult to maintain democracy in the absence of capitalism.</span> timestamp: (start: 819.96, end: 904.66) …</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.4.4.1.1">
<span class="ltx_p" id="S3.T2.4.4.1.1.1" style="width:433.6pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="11" id="S3.T2.4.4.1.1.1.g1" src="extracted/5824914/images/icon/yata.png" width="14"/> <span class="ltx_text ltx_font_bold" id="S3.T2.4.4.1.1.1.1">YA-TA Response</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.13.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.13.1.1">
<span class="ltx_p" id="S3.T2.5.13.1.1.1" style="width:433.6pt;">Capitalism and democracy interact with each other.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.14.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.14.1.1">
<span class="ltx_p" id="S3.T2.5.14.1.1.1" style="width:433.6pt;">…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.15.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.15.1.1">
<span class="ltx_p" id="S3.T2.5.15.1.1.1" style="width:433.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.5.15.1.1.1.1">capitalism plays an important role in the continuous maintenance and development of democracy.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.16.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.16.1.1">
<span class="ltx_p" id="S3.T2.5.16.1.1.1" style="width:433.6pt;">…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.17.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.17.1.1">
<span class="ltx_p" id="S3.T2.5.17.1.1.1" style="width:433.6pt;">In the field of artificial intelligence, this relationship is particularly evident. For example, in a capitalist market, <span class="ltx_text ltx_font_bold" id="S3.T2.5.17.1.1.1.1">the advancement of AI technology is driven by competition, leading to innovation and economic growth</span>.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.5.1.1">
<span class="ltx_p" id="S3.T2.5.5.1.1.1" style="width:433.6pt;"><span class="ltx_text" id="S3.T2.5.5.1.1.1.1" style="position:relative; bottom:-2.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S3.T2.5.5.1.1.1.1.g1" src="extracted/5824914/images/icon/ChatGPT.png" width="14"/></span> <span class="ltx_text ltx_font_bold" id="S3.T2.5.5.1.1.1.2">GPT-4o Response</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.18.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.18.1.1">
<span class="ltx_p" id="S3.T2.5.18.1.1.1" style="width:433.6pt;">Capitalism and democracy are two important concepts in modern society, representing economic and political systems, respectively.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.19">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.5.19.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.19.1.1">
<span class="ltx_p" id="S3.T2.5.19.1.1.1" style="width:433.6pt;">…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.20">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.5.20.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.5.20.1.1">
<span class="ltx_p" id="S3.T2.5.20.1.1.1" style="width:433.6pt;">The way capitalism and democracy are combined can vary widely depending on each country’s historical and cultural context.</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>A sample translated response from YA-TA and GPT-4o.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.6">
<tr class="ltx_tr" id="S3.T3.6.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T3.6.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.7.1.1">
<span class="ltx_p" id="S3.T3.6.7.1.1.1" style="width:433.6pt;"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S3.T3.6.7.1.1.1.1">Computer Science<span class="ltx_text ltx_font_medium" id="S3.T3.6.7.1.1.1.1.1"> (Harvard CS50)</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.1">
<span class="ltx_p" id="S3.T3.1.1.1.1.1" style="width:433.6pt;"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1" style="position:relative; bottom:-2.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S3.T3.1.1.1.1.1.1.g1" src="extracted/5824914/images/icon/student-info.png" width="14"/></span> <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.2">Student Information</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.8.1.1">
<span class="ltx_p" id="S3.T3.6.8.1.1.1" style="width:433.6pt;">Major: Artificial Intelligence</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.9.1.1">
<span class="ltx_p" id="S3.T3.6.9.1.1.1" style="width:433.6pt;">Degree: Graduate Student</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.10.1.1">
<span class="ltx_p" id="S3.T3.6.10.1.1.1" style="width:433.6pt;">Semester: 3</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.11.1.1">
<span class="ltx_p" id="S3.T3.6.11.1.1.1" style="width:433.6pt;">Grade: (Data Structures, A), (Algorithms, A+), …</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.2.2.1.1">
<span class="ltx_p" id="S3.T3.2.2.1.1.1" style="width:433.6pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="S3.T3.2.2.1.1.1.g1" src="extracted/5824914/images/icon/question.png" width="12"/> <span class="ltx_text ltx_font_bold" id="S3.T3.2.2.1.1.1.1">Student Query</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.12.1.1">
<span class="ltx_p" id="S3.T3.6.12.1.1.1" style="width:433.6pt;">Which sorting algorithm is described as having a time complexity of n log n.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.3.3.1.1">
<span class="ltx_p" id="S3.T3.3.3.1.1.1" style="width:433.6pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="10" id="S3.T3.3.3.1.1.1.g1" src="extracted/5824914/images/icon/evidence.png" width="10"/> <span class="ltx_text ltx_font_bold" id="S3.T3.3.3.1.1.1.1">Evidence</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.13.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.13.1.1">
<span class="ltx_p" id="S3.T3.6.13.1.1.1" style="width:433.6pt;">And then this one got a little mathy: log base 2 of n. And <span class="ltx_text ltx_font_bold" id="S3.T3.6.13.1.1.1.1">log base 2 of n just means what is the value when you take n and divide it by two again and again.</span> timestamp:(start:1778.84, end:1790.4) …</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.4.4.1.1">
<span class="ltx_p" id="S3.T3.4.4.1.1.1" style="width:433.6pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="11" id="S3.T3.4.4.1.1.1.g1" src="extracted/5824914/images/icon/yata.png" width="14"/> <span class="ltx_text ltx_font_bold" id="S3.T3.4.4.1.1.1.1">YA-TA Response</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.14.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.14.1.1">
<span class="ltx_p" id="S3.T3.6.14.1.1.1" style="width:433.6pt;">The sorting algorithm described as having a time complexity of n log n is merge sort.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.15.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.15.1.1">
<span class="ltx_p" id="S3.T3.6.15.1.1.1" style="width:433.6pt;">…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.16.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.16.1.1">
<span class="ltx_p" id="S3.T3.6.16.1.1.1" style="width:433.6pt;">The process of dividing the array into halves and then merging them requires <span class="ltx_text ltx_font_bold" id="S3.T3.6.16.1.1.1.1">log base 2 of n steps, as you divide the array size by two repeatedly.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.17.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.17.1.1">
<span class="ltx_p" id="S3.T3.6.17.1.1.1" style="width:433.6pt;">…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.18.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.18.1.1">
<span class="ltx_p" id="S3.T3.6.18.1.1.1" style="width:433.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.6.18.1.1.1.1">Given your background in Data Structures, Algorithms, and Machine Learning</span>, you would appreciate that merge sort is particularly useful for sorting large datasets efficiently.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T3.5.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.5.5.1.1">
<span class="ltx_p" id="S3.T3.5.5.1.1.1" style="width:433.6pt;"><span class="ltx_text" id="S3.T3.5.5.1.1.1.1" style="position:relative; bottom:-2.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S3.T3.5.5.1.1.1.1.g1" src="extracted/5824914/images/icon/ChatGPT.png" width="14"/></span> <span class="ltx_text ltx_font_bold" id="S3.T3.5.5.1.1.1.2">GPT-4o Response</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.6.1.1">
<span class="ltx_p" id="S3.T3.6.6.1.1.1" style="width:433.6pt;">Several sorting algorithms have a time complexity of <math alttext="O(nlogn)" class="ltx_Math" display="inline" id="S3.T3.6.6.1.1.1.m1.1"><semantics id="S3.T3.6.6.1.1.1.m1.1a"><mrow id="S3.T3.6.6.1.1.1.m1.1.1" xref="S3.T3.6.6.1.1.1.m1.1.1.cmml"><mi id="S3.T3.6.6.1.1.1.m1.1.1.3" xref="S3.T3.6.6.1.1.1.m1.1.1.3.cmml">O</mi><mo id="S3.T3.6.6.1.1.1.m1.1.1.2" xref="S3.T3.6.6.1.1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.T3.6.6.1.1.1.m1.1.1.1.1" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.cmml"><mo id="S3.T3.6.6.1.1.1.m1.1.1.1.1.2" stretchy="false" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.2" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.3" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.3.cmml">l</mi><mo id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1a" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.4" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.4.cmml">o</mi><mo id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1b" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.5" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.5.cmml">g</mi><mo id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1c" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.6" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.6.cmml">n</mi></mrow><mo id="S3.T3.6.6.1.1.1.m1.1.1.1.1.3" stretchy="false" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.1.1.1.m1.1b"><apply id="S3.T3.6.6.1.1.1.m1.1.1.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1"><times id="S3.T3.6.6.1.1.1.m1.1.1.2.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.2"></times><ci id="S3.T3.6.6.1.1.1.m1.1.1.3.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.3">𝑂</ci><apply id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1"><times id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.1"></times><ci id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.2">𝑛</ci><ci id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.3">𝑙</ci><ci id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.4.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.4">𝑜</ci><ci id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.5.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.5">𝑔</ci><ci id="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.6.cmml" xref="S3.T3.6.6.1.1.1.m1.1.1.1.1.1.6">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.1.1.1.m1.1c">O(nlogn)</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.1.1.1.m1.1d">italic_O ( italic_n italic_l italic_o italic_g italic_n )</annotation></semantics></math>. The most notable ones include:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.19">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.19.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.19.1.1">
<span class="ltx_p" id="S3.T3.6.19.1.1.1" style="width:433.6pt;">1. Merge Sort: …</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.20">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.20.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.20.1.1">
<span class="ltx_p" id="S3.T3.6.20.1.1.1" style="width:433.6pt;">2. Heapsort: …</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.21">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T3.6.21.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.21.1.1">
<span class="ltx_p" id="S3.T3.6.21.1.1.1" style="width:433.6pt;">3. Quicksort: …</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.22">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T3.6.22.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.6.22.1.1">
<span class="ltx_p" id="S3.T3.6.22.1.1.1" style="width:433.6pt;">These algorithms are widely used due to their efficiency and effectiveness in sorting large datasets.</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>A sample response from YA-TA and GPT-4o.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Case Study</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section, we present a qualitative analysis of our <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.1">DRaKe</span> framework through case studies of YA-TA interacting with a designed student.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Experiment Setup</h4>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Course setting.</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.1">We select the CS50 lecture used in G-Eval, along with a Korean course for social science (GPP6003 from Yonsei University)<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>For convenience, we named the course identifier GPP6003. The videos can be accessed at the following link: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.learnus.org/local/ubonline/view.php?id=216527" title="">https://www.learnus.org/local/ubonline/view.php?id=216527</a></span></span></span>, as our test bed.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Student setting.</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p1.1">We set the virtual student as a third-semester graduate student majoring in Artificial Intelligence. Additionally, we assume this student has taken computer-related courses and achieved very high grades but has not taken any social science courses. Based on the student knowledge, we assume a high understanding level of computer science and a lower proficiency in social sciences.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S3.F3.g1" src="extracted/5824914/images/extension.jpg" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Extension of YA-TA</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Example of personalized response.</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px3.p1.1">As illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.T2" title="Table 2 ‣ 3.1.3 G-Eval Results ‣ 3.1 G-Eval ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#S3.T3" title="Table 3 ‣ 3.1.3 G-Eval Results ‣ 3.1 G-Eval ‣ 3 Experiments ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">3</span></a>, YA-TA produces responses that are personalized for both the instructor and the student. The similarity between the evidence and YA-TA’s responses shows that YA-TA bases its answers on the lecture. Furthermore, YA-TA uses examples relevant to the student’s background, which demonstrates its ability to tailor responses based on the student’s academic background.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">Failure cases and limitation.</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px4.p1.1">Insufficient responses may result from the limitations of YA-TA, as it is designed to respond based on the instructor knowledge. When a student’s question falls outside or only slightly overlaps with the lecture’s scope, there may not be enough evidence to generate a helpful answer, leading to insufficient responses.</p>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Extension of YA-TA</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To enhance learning effectiveness, we provide two additional educational tools: Q&amp;A Board and Self-Practice. The Q&amp;A Board strengthens interaction between instructors and students, supporting in-depth learning. The Self-Practice tool enables students to test and review what they have learned.</p>
</div>
<div class="ltx_para" id="S4.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Q&amp;A Board:</span>
The Q&amp;A board allows students to ask questions beyond the lecture and seek additional help. When a student posts a question, YA-TA drafts a response based on the instructor’s knowledge. The instructor then reviews and refines this draft, ensuring it aligns with their teaching philosophy with minimal effort. This process facilitates direct student-instructor engagement while enabling instructors to efficiently provide thoughtful and personalized responses.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Self-Practice:</span> Our system also provides quizzes to allow students to self-assess their understanding of the instructor’s knowledge. Quizzes are generated based on the instructor knowledge by focusing on the highlighted key points. Using a similar prompt as in the instructor-side retrieval process of YA-TA, quizzes help students evaluate their grasp of the material that the instructor considers important, thereby enhancing their learning experience.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Personalized LLMs have been extensively studied in NLP for educational purposes by implementing methods such as training datasets with instructor knowledge and assessing student academic levels <cite class="ltx_cite ltx_citemacro_citep">(Porsdam Mann et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib15" title="">2023</a>; Woźniak et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib19" title="">2024</a>)</cite>. Previous studies on instructor-side personalization involve fine-tuning models with specific datasets to generate customized responses <cite class="ltx_cite ltx_citemacro_citep">(Hicke et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib8" title="">2023b</a>; Chevalier et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib4" title="">2024</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib10" title="">2023</a>; Macina et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib12" title="">2023</a>; Chae et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib2" title="">2023</a>)</cite>, and using Retrieval-augmented Generation <cite class="ltx_cite ltx_citemacro_citep">(Levonian et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib9" title="">2023</a>)</cite>. Student-side personalization tailors learning experiences to individual academic levels, with systems that dynamically adjust to student needs <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib3" title="">2023</a>)</cite> and offer personalized learning paths <cite class="ltx_cite ltx_citemacro_citep">(Sajja et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib17" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we propose YA-TA, a multi-turn QA agent that provides personalized responses for both instructors and students. YA-TA can be set up with just a lecture video and a single click, without the need for additional model training, making it highly versatile. Additionally, our platform enhances the educational experience by offering extensions such as Q&amp;A Board and Self-Practice.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitation</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">While YA-TA offers extensive services to enhance the learning experience, several limitations must be acknowledged.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Firstly, when extracting knowledge from both instructors and students, there is a potential risk of patent and privacy issues. It is crucial to obtain explicit permission from instructors before using their videos and to secure consent from students when using their personal information. Additionally, the reliance on OpenAI’s API may lead to cost-related challenges.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Moreover, since we build memory specific to each lecture and student, our framework cannot integrate content from multiple courses into a single response. Our approach also does not account for the diverse features a student may exhibit across different courses, as it analyzes queries posed by the student within a single course rather than across multiple courses.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgments</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">This work was supported by Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korean government (MSIT)(No.RS-2020-II201361, Artificial Intelligence Graduate School Program (Yonsei University)) and (No.RS-2021-II212068, Artificial Intelligence Innovation Hub) and (No.RS-2022-II220077,AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data). Jinyoung Yeo and Dongha Lee are the co-corresponding authors
(jinyeo@yonsei.ac.kr, donalee@yonsei.ac.kr).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agwu and Nmadu (2023)</span>
<span class="ltx_bibblock">
Udu David Agwu and John Nmadu. 2023.

</span>
<span class="ltx_bibblock">Students’ interactive engagement, academic achievement and self concept in chemistry: an evaluation of cooperative learning pedagogy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Chemistry Education Research and Practice</em>, 24(2):688–705.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chae et al. (2023)</span>
<span class="ltx_bibblock">
Hyungjoo Chae, Minjin Kim, Chaehyeong Kim, Wonseok Jeong, Hyejoong Kim, Junmyung Lee, and Jinyoung Yeo. 2023.

</span>
<span class="ltx_bibblock">Tutoring: instruction-grounded conversational agent for language learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 37, pages 16413–16415.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023.

</span>
<span class="ltx_bibblock">Empowering private tutoring by chaining large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2309.08112</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chevalier et al. (2024)</span>
<span class="ltx_bibblock">
Alexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Jameson Aragon, Arturo Rodríguez Fanlo, Simon Frieder, Simon Machado, et al. 2024.

</span>
<span class="ltx_bibblock">Language models as science tutors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2402.11111</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">College (2024)</span>
<span class="ltx_bibblock">
National College. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://nationalcollege.com/news/effective-goals-for-teaching-assistants" title="">Effective goals for teaching assistants</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-07-12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong (2023)</span>
<span class="ltx_bibblock">
Chenxi Dong. 2023.

</span>
<span class="ltx_bibblock">How to build an ai tutor that can adapt to any course and provide accurate answers using large language model and retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2311.17696</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hicke et al. (2023a)</span>
<span class="ltx_bibblock">
Yann Hicke, Anmol Agarwal, Qianou Ma, and Paul Denny. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.02775" title="">Ai-ta: Towards an intelligent question-answer teaching assistant using open-source llms</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hicke et al. (2023b)</span>
<span class="ltx_bibblock">
Yann Hicke, Anmol Agarwal, Qianou Ma, and Paul Denny. 2023b.

</span>
<span class="ltx_bibblock">Chata: Towards an intelligent question-answer teaching assistant using open-source llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2311.02775</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levonian et al. (2023)</span>
<span class="ltx_bibblock">
Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Millie-Ellen Postle, and Wanli Xing. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation to improve math question-answering: Trade-offs between groundedness and human preference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2310.03184</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Yu Li, Shang Qu, Jili Shen, Shangchao Min, and Zhou Yu. 2023.

</span>
<span class="ltx_bibblock">Curriculum-driven edubot: A framework for developing language learning chatbots through synthesizing conversational data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2309.16804</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023.

</span>
<span class="ltx_bibblock">Gpteval: Nlg evaluation using gpt-4 with better human alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2303.16634</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Macina et al. (2023)</span>
<span class="ltx_bibblock">
Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu Kapur, Iryna Gurevych, and Mrinmaya Sachan. 2023.

</span>
<span class="ltx_bibblock">Mathdial: A dialogue tutoring dataset with rich pedagogical properties grounded in math reasoning problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2305.14536</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matsuda and Frank (2024)</span>
<span class="ltx_bibblock">
Koh Matsuda and Ian Frank. 2024.

</span>
<span class="ltx_bibblock">Langchain unleashed: Advancing education beyond chatgpt’s limits.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2024)</span>
<span class="ltx_bibblock">
Minju Park, Sojung Kim, Seunghyun Lee, Soonwoo Kwon, and Kyuseok Kim. 2024.

</span>
<span class="ltx_bibblock">Empowering personalized learning through a conversation-based tutoring system with student modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, pages 1–10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Porsdam Mann et al. (2023)</span>
<span class="ltx_bibblock">
Sebastian Porsdam Mann, Brian D Earp, Nikolaj Møller, Suren Vynn, and Julian Savulescu. 2023.

</span>
<span class="ltx_bibblock">Autogen: A personalized large language model for academic enhancement—ethics and proof of principle.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">The American Journal of Bioethics</em>, 23(10):28–41.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et al. (2009)</span>
<span class="ltx_bibblock">
Stephen Robertson, Hugo Zaragoza, et al. 2009.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Foundations and Trends® in Information Retrieval</em>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sajja et al. (2023)</span>
<span class="ltx_bibblock">
Ramteja Sajja, Yusuf Sermet, Muhammed Cikmaz, David Cwiertny, and Ibrahim Demir. 2023.

</span>
<span class="ltx_bibblock">Artificial intelligence-enabled intelligent assistant for personalized and adaptive learning in higher education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2309.10892</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Advances in neural information processing systems</em>, 35:24824–24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woźniak et al. (2024)</span>
<span class="ltx_bibblock">
Stanisław Woźniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, and Jan Kocoń. 2024.

</span>
<span class="ltx_bibblock">Personalized large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2402.09269</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>YA-TA Details</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We use BM25  <cite class="ltx_cite ltx_citemacro_citep">(Robertson et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#bib.bib16" title="">2009</a>)</cite> for the sparse retriever and <span class="ltx_text ltx_font_typewriter" id="A1.p1.1.1">text-embedding-ada-002</span> for the dense retriever in the instructor knowledge retrieval phase. Also, we set the number of retrieved segments <math alttext="k" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mi id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><ci id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">italic_k</annotation></semantics></math> as 10. For both LLM reasonser and response generator, we use <span class="ltx_text ltx_font_typewriter" id="A1.p1.1.2">gpt-3.5-turbo-0125</span>.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>G-Eval Details</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Student Information</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">We aim to evaluate YA-TA’s ability to achieve student-side personalization for various students. To meet this end, we create virtual student profiles with diverse academic backgrounds.</p>
</div>
<figure class="ltx_figure" id="A2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="785" id="A2.F4.g1" src="extracted/5824914/images/geval/student_info.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Virtual student profiles that we set</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Questions</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">First, we generate advocate questions likely to arise from the CS50 course using the prompt employed to create the quiz. Then, a CS expert selects 10 high-quality questions that students would ask.</p>
</div>
<figure class="ltx_table" id="A2.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A2.T4.1">
<tr class="ltx_tr" id="A2.T4.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.T4.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.1.1">
<span class="ltx_p" id="A2.T4.1.1.1.1.1" style="width:421.6pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.1.1.1">Question</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T4.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.2.1.1">
<span class="ltx_p" id="A2.T4.1.2.1.1.1" style="width:421.6pt;">1. How does AI improve the experience of using recommendation systems in services like Netflix?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.3.1.1">
<span class="ltx_p" id="A2.T4.1.3.1.1.1" style="width:421.6pt;">2. What is the primary purpose of "rubber duck debugging" in programming?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.4.1.1">
<span class="ltx_p" id="A2.T4.1.4.1.1.1" style="width:421.6pt;">3. Which sorting algorithm is described as having a time complexity of n log n in the lecture?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.5.1.1">
<span class="ltx_p" id="A2.T4.1.5.1.1.1" style="width:421.6pt;">4. What is the purpose of the base case in a recursive function?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.6.1.1">
<span class="ltx_p" id="A2.T4.1.6.1.1.1" style="width:421.6pt;">5. What is the difference between a user prompt and a system prompt in the context of AI-based educational tools?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.7.1.1">
<span class="ltx_p" id="A2.T4.1.7.1.1.1" style="width:421.6pt;">6. What is the primary difference between traditional AI approaches and machine learning in the context of solving games?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.8.1.1">
<span class="ltx_p" id="A2.T4.1.8.1.1.1" style="width:421.6pt;">7. Describe the basic algorithm for playing the game Breakout as explained in the lecture.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.9.1.1">
<span class="ltx_p" id="A2.T4.1.9.1.1.1" style="width:421.6pt;">8. What real-world example is used to explain the divide and conquer algorithm in the lecture?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.10.1.1">
<span class="ltx_p" id="A2.T4.1.10.1.1.1" style="width:421.6pt;">9. What is the primary advantage of using a binary search algorithm over a linear search algorithm?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="A2.T4.1.11.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.11.1.1">
<span class="ltx_p" id="A2.T4.1.11.1.1.1" style="width:421.6pt;">10. What is the main reason for using pseudocode before writing actual code?</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>10 high-quality questions from the questions extracted by GPT-3.5-Turbo.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>G-Eval Criteria</h3>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">We utilize G-Eval prompts to assess 5 criteria. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.00355v1#A2.F5" title="Figure 5 ‣ B.3 G-Eval Criteria ‣ Appendix B G-Eval Details ‣ YA-TA: Towards Personalized Question-Answering Teaching Assistants using Instructor-Student Dual Retrieval-augmented Knowledge Fusion"><span class="ltx_text ltx_ref_tag">5</span></a> shows the example prompt we used for G-Eval.</p>
</div>
<figure class="ltx_figure" id="A2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1129" id="A2.F5.g1" src="extracted/5824914/images/geval/precision.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>G-Eval prompt used to assess precision</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Aug 31 05:25:17 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
