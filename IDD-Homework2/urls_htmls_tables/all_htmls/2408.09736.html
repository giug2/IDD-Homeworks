<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.09736] Coarse-Fine View Attention Alignment-Based GAN for CT Reconstruction from Biplanar X-Rays</title><meta property="og:description" content="For surgical planning and intra-operation imaging, CT reconstruction using X-ray images can potentially be an important alternative when CT imaging is not available or not feasible. In this paper, we aim to use biplana…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coarse-Fine View Attention Alignment-Based GAN for CT Reconstruction from Biplanar X-Rays">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Coarse-Fine View Attention Alignment-Based GAN for CT Reconstruction from Biplanar X-Rays">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.09736">

<!--Generated on Thu Sep  5 17:40:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Coarse-Fine View Attention Alignment-Based
GAN for CT Reconstruction from Biplanar X-Rays</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhi Qiao1,
Dongheng Chu6,
Hanqiang Ouyang345,
Huishu Yuan3,
Xiantong Zhen1,
Pei Dong2,
Zhen Qian1
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1 Beijing United Imaging Research Institute of Intelligent Imaging, Beijing, China
</span>
<span class="ltx_contact ltx_role_affiliation">2 United Imaging Intelligence
Beijing, China
</span>
<span class="ltx_contact ltx_role_affiliation">3 Department of Radiology, Peking University Third Hospital, Beijing, China
</span>
<span class="ltx_contact ltx_role_affiliation">4 Engineering Research Center of Bone and Joint Precision Medicine
</span>
<span class="ltx_contact ltx_role_affiliation">5 Beijing Key Laboratory of Spinal Disease Research
</span>
<span class="ltx_contact ltx_role_affiliation">6 Beijing University of Posts and Telecommunications, Beijing, China
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">For surgical planning and intra-operation imaging, CT reconstruction using X-ray images can potentially be an important alternative when CT imaging is not available or not feasible. In this paper, we aim to use biplanar X-rays to reconstruct a 3D CT image, because biplanar X-rays convey richer information than single-view X-rays and are more commonly used by surgeons. Different from previous studies in which the two X-ray views were treated indifferently when fusing the cross-view data, we propose a novel attention-informed coarse-to-fine cross-view fusion method to combine the features extracted from the orthogonal biplanar views. This method consists of a view attention alignment sub-module and a fine-distillation sub-module that are designed to work together to highlight the unique or complementary information from each of the views. Experiments have demonstrated the superiority of our proposed method over the SOTA methods.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">X-ray imaging has been extensively used in orthopedic surgeries. X-rays provide excellent contrast for bones and have the advantages of lower radiation doses, lower expenses, and faster imaging speed over the other imaging technologies. However, a major limitation of X-rays is that they could not provide a 3D view of the internal structure. CT reconstruction using X-ray images can potentially be an important alternative when CT imaging is not available or not feasible. The 3D images generated through CT reconstruction from X-rays can provide valuable information for diagnosis and treatment planning. Additionally, the use of biplanar X-ray inputs, which are taken from two perpendicular angles, can further enhance the accuracy of the reconstruction, providing a more complete and detailed representation of the anatomy and structures of interest.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Some existing works have attempted 3D reconstruction from biplanar 2D X-ray images. In these biplanar X-ray-based CT reconstruction methods, a crucial step is the cross-view fusion, which is used to fuse the deep features derived from the two orthogonal biplanar views. The aforementioned methods use a matrix permutation process to keep the biplanar features geometrically consistent in orientation, followed by a direct addition or a concatenation operator for further fusion. The addition or concatenation process in the above cross-view fusion method assumes that each view provides an equivalent contribution to CT reconstruction. Nevertheless, the different views can demonstrate different morphological characteristics of the organs of interest. Intuitively, it is more beneficial to extract specific information from each view when performing cross-view fusion, because it helps to highlight unique or complementary information. Therefore, it is desirable to explore more efficient and effective fusion methods such that the relevant information from each view can be combined without unnecessary noise or redundancy.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose a <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">C</span>oarse-Fine <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">V</span>iew <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">A</span>ttention <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">A</span>lignment-based GAN for CT reconstruction from biplanar X-rays (abbreviated as <span id="S1.p3.1.5" class="ltx_text ltx_font_bold">CVAA-GAN</span>). Inspired by the advantages of GANs for cross-modality image transfer, especially in the medical domain, we adopt GAN as the main framework, which is composed of a Generator network and a Discriminator network. In the Generator, We propose the Coarse-Fine View Attention Alignment (CVAA) module to fuse the features extracted from the orthogonal biplanar views. More specifically, we first propose a view attention alignment sub-module to learn the fusion weights for cross-view fusion. Then, we introduce a fine-distillation sub-module to extract finer features from the coarse features. For the Discriminator, we opt to use a typical Patch-3D Discriminator.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.09736/assets/bibm_generator.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Network architecture of the CVAA-GAN Generator</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">CVAA-GAN</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we introduce our proposed method. Similar to other 3D GAN architectures, our method involves a 3D Generator and a 3D Discriminator. These two models are alternatively trained.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Generator</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The proposed 3D generator, as illustrated in Fig.<a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Coarse-Fine View Attention Alignment-Based GAN for CT Reconstruction from Biplanar X-Rays" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, consists of three individual components: two encoder networks to extract view features with the same architecture working in parallel for the two orthogonal views respectively, and a fusion decoder network for 3D image reconstruction.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.4.1.1" class="ltx_text">II-A</span>1 </span>Encoder</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">The encoder network aims to extract features from the input 2D X-ray images and map them into 3D feature space.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p"><span id="S2.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Feat2D</span>. To optimally utilize information from 2D X-ray images, we introduce the Feat2D modules in the generator’s encoding path where a typical densenet component is used.</p>
</div>
<div id="S2.SS1.SSS1.p3" class="ltx_para">
<p id="S2.SS1.SSS1.p3.1" class="ltx_p"><span id="S2.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">2D<math id="S2.SS1.SSS1.p3.1.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S2.SS1.SSS1.p3.1.1.m1.1a"><mo stretchy="false" id="S2.SS1.SSS1.p3.1.1.m1.1.1" xref="S2.SS1.SSS1.p3.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p3.1.1.m1.1b"><ci id="S2.SS1.SSS1.p3.1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p3.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p3.1.1.m1.1c">\Rightarrow</annotation></semantics></math>3D</span>.
The 2D to 3D mapping procedure bridges the information from the 2D features to the 3D volumetric data. Motivated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, we expand the 2D features to 3D by repeating the 2D feature maps along the X-ray’s projection directions in 3D, which is based on an assumption commonly used by the back-projection-based CT reconstruction methods that the 3D features often repeat themselves along the projection direction. Then, we feed the initial 3D features into a basic 3D convolution block. In order to align the two 3D features in a consistent orientation, we transform them into a unified coordinate space.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.4.1.1" class="ltx_text">II-A</span>2 </span>Decoder</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">The decoder network is designed to fuse the biplanar view features, and project the deep features (lower resolution) learned by the encoder onto the voxel space (higher resolution) for final CT image reconstruction. We assume the biplanar X-ray images are captured within a negligible time interval, meaning no data shift caused by patient motions. In the deepest layer of the encoder, for initial low-resolution reconstruction, without any auxiliary information, we follow the existing view alignment method via the addition operator to combine these two view features to reconstruct the initial CT deep features (lowest resolution) which have the coarsest anatomical structure information, as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Coarse-Fine View Attention Alignment-Based GAN for CT Reconstruction from Biplanar X-Rays" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In the following, a basic 3D convolution block is used to enhance the feature correlation for the 3D structure, and then an up-convolution module is used to map the features into the higher-resolution feature space.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p"><span id="S2.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">CVAA Module</span>. In the following layers of the decoder, the inputs consist of two X-ray-related features from the encoders and the up-convolved features from the last decoder layer. The obtained X-ray-related features contain more high-frequency texture information, which can be seen as shallow features in the current resolution. The up-convolved features contain more low-frequency structural features which can be seen as deep features. The deep features can help identify which voxels are more useful and can guide a more reasonable fusion. In order to efficiently fuse the two views, we introduce a View Attention Alignment (VAA) sub-module to get the coarse features. Motivated by the coarse-fine scheme, we furthermore introduce a Fine Distillation (FD) sub-module to extract finer features from the coarse features. Besides the coarse and fine features, we also keep using the traditionally used view alignment features via the addition operator to enrich feature diversity, and the up-convolved features from the last decoder layer to take the advantage of the residual network. Finally, we ensemble all of the above features via a concatenation operator, followed by a basic 3D convolution layer and an up-convolution layer. After the up-convolution, the decoder outputs of the current layer are generated.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">View Attention Alignment. We first utilize the deep features to correlate two separate shallow features via a concatenating process and a basic 3D convolution block (Conv3d+Relu) to get the structure-aware view features. Two structure-aware view features are further concatenated and then fed into another basic Conv3D block (kernel size = 7, padding = 3, out_channel = 2) to get the mixture features. Finally, a Softmax activation function is used to get attention weights. The weights are used to combine the two shallow features via operations of dot product and addition to derive the fusion features which are also considered the coarse features.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Fine Distillation. After the coarse features are obtained, we further filter the noises and distill the coarse features. Firstly, we combine coarse features and deep features via concatenating to correlate coarse features with the structural information. Then, the combined features are fed into a basic Conv3D layer (kernel size = 3, padding = 1, out_channel = 1), followed by a Sigmoid function to generate the distillation weights. The distillation weights are directly applied to the coarse features via dot product to derive the filtered features which are also considered the fine features.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.09736/assets/bibm_qualitative.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="521" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Representative CT reconstruction results shown in the mid-axial (1st row), mid-sagittal (2nd row), and mid-coronal (3rd row) views. Our method is compared with baseline methods (PSR, 3DCNN, X2CT-GAN) and ground truth (GT).</figcaption>
</figure>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Objective Functions</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In this section, we introduce the loss functions used in model learning.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Adversarial Loss</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.10" class="ltx_p">In image style translation, in contrast to Vanilla GAN and Wasserstein GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, least squares generative adversarial networks (LSGANs) has been shown to generate higher quality images and perform more stable during the learning process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In our framework, we adapt the loss function of LSGANs as the loss function for the discriminator. The objective functions can be defined as follows:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.66" class="ltx_math_unparsed" alttext="\begin{split}&amp;L(G)=E_{X}(||D(X,G(X))-\mathbb{I}||^{2})\\
&amp;L(D)=E_{X}(||D(X,Y)-\mathbb{I}||^{2})+E_{X}(||D(X,G(X)||^{2})\end{split}" display="block"><semantics id="S2.E1.m1.66a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S2.E1.m1.66.66.1"><mtr id="S2.E1.m1.66.66.1a"><mtd id="S2.E1.m1.66.66.1b"></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.66.66.1c"><mrow id="S2.E1.m1.66.66.1.66.26.26"><mrow id="S2.E1.m1.66.66.1.66.26.26.27"><mi id="S2.E1.m1.1.1.1.1.1.1">L</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.66.66.1.66.26.26.27.1">​</mo><mrow id="S2.E1.m1.66.66.1.66.26.26.27.2"><mo stretchy="false" id="S2.E1.m1.2.2.2.2.2.2">(</mo><mi id="S2.E1.m1.3.3.3.3.3.3">G</mi><mo stretchy="false" id="S2.E1.m1.4.4.4.4.4.4">)</mo></mrow></mrow><mo id="S2.E1.m1.5.5.5.5.5.5">=</mo><mrow id="S2.E1.m1.66.66.1.66.26.26.26"><msub id="S2.E1.m1.66.66.1.66.26.26.26.3"><mi id="S2.E1.m1.6.6.6.6.6.6">E</mi><mi id="S2.E1.m1.7.7.7.7.7.7.1">X</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.66.66.1.66.26.26.26.2">​</mo><mrow id="S2.E1.m1.66.66.1.66.26.26.26.1.1"><mo stretchy="false" id="S2.E1.m1.8.8.8.8.8.8">(</mo><msup id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1"><mrow id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1.1.1"><mo stretchy="false" id="S2.E1.m1.9.9.9.9.9.9a">‖</mo><mrow id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1.1.1.1"><mrow id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1.1.1.1.1"><mi id="S2.E1.m1.11.11.11.11.11.11">D</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1.1.1.1.1.2">​</mo><mrow id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1.1.1.1.1.1.1"><mo stretchy="false" id="S2.E1.m1.12.12.12.12.12.12">(</mo><mi id="S2.E1.m1.13.13.13.13.13.13">X</mi><mo id="S2.E1.m1.14.14.14.14.14.14">,</mo><mrow id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1.1.1.1.1.1.1.1"><mi id="S2.E1.m1.15.15.15.15.15.15">G</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1.1.1.1.1.1.1.1.1">​</mo><mrow id="S2.E1.m1.66.66.1.66.26.26.26.1.1.1.1.1.1.1.1.1.1.2"><mo stretchy="false" id="S2.E1.m1.16.16.16.16.16.16">(</mo><mi id="S2.E1.m1.17.17.17.17.17.17">X</mi><mo stretchy="false" id="S2.E1.m1.18.18.18.18.18.18">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.19.19.19.19.19.19">)</mo></mrow></mrow><mo id="S2.E1.m1.20.20.20.20.20.20">−</mo><mi id="S2.E1.m1.21.21.21.21.21.21">𝕀</mi></mrow><mo stretchy="false" id="S2.E1.m1.22.22.22.22.22.22a">‖</mo></mrow><mn id="S2.E1.m1.24.24.24.24.24.24.1">2</mn></msup><mo stretchy="false" id="S2.E1.m1.25.25.25.25.25.25">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S2.E1.m1.66.66.1d"><mtd id="S2.E1.m1.66.66.1e"></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.66.66.1f"><mrow id="S2.E1.m1.65.65.65.40.40"><mi id="S2.E1.m1.26.26.26.1.1.1">L</mi><mrow id="S2.E1.m1.65.65.65.40.40.41"><mo stretchy="false" id="S2.E1.m1.27.27.27.2.2.2">(</mo><mi id="S2.E1.m1.28.28.28.3.3.3">D</mi><mo stretchy="false" id="S2.E1.m1.29.29.29.4.4.4">)</mo></mrow><mo id="S2.E1.m1.30.30.30.5.5.5">=</mo><msub id="S2.E1.m1.65.65.65.40.40.42"><mi id="S2.E1.m1.31.31.31.6.6.6">E</mi><mi id="S2.E1.m1.32.32.32.7.7.7.1">X</mi></msub><mrow id="S2.E1.m1.65.65.65.40.40.43"><mo stretchy="false" id="S2.E1.m1.33.33.33.8.8.8">(</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E1.m1.34.34.34.9.9.9">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E1.m1.35.35.35.10.10.10">|</mo><mi id="S2.E1.m1.36.36.36.11.11.11">D</mi><mrow id="S2.E1.m1.65.65.65.40.40.43.1"><mo stretchy="false" id="S2.E1.m1.37.37.37.12.12.12">(</mo><mi id="S2.E1.m1.38.38.38.13.13.13">X</mi><mo id="S2.E1.m1.39.39.39.14.14.14">,</mo><mi id="S2.E1.m1.40.40.40.15.15.15">Y</mi><mo stretchy="false" id="S2.E1.m1.41.41.41.16.16.16">)</mo></mrow><mo id="S2.E1.m1.42.42.42.17.17.17">−</mo><mi id="S2.E1.m1.43.43.43.18.18.18">𝕀</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E1.m1.44.44.44.19.19.19">|</mo><msup id="S2.E1.m1.65.65.65.40.40.43.2"><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E1.m1.45.45.45.20.20.20">|</mo><mn id="S2.E1.m1.46.46.46.21.21.21.1">2</mn></msup><mo stretchy="false" id="S2.E1.m1.47.47.47.22.22.22">)</mo></mrow><mo id="S2.E1.m1.48.48.48.23.23.23">+</mo><msub id="S2.E1.m1.65.65.65.40.40.44"><mi id="S2.E1.m1.49.49.49.24.24.24">E</mi><mi id="S2.E1.m1.50.50.50.25.25.25.1">X</mi></msub><mrow id="S2.E1.m1.65.65.65.40.40.45"><mo stretchy="false" id="S2.E1.m1.51.51.51.26.26.26">(</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E1.m1.52.52.52.27.27.27">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E1.m1.53.53.53.28.28.28">|</mo><mi id="S2.E1.m1.54.54.54.29.29.29">D</mi><mrow id="S2.E1.m1.65.65.65.40.40.45.1"><mo stretchy="false" id="S2.E1.m1.55.55.55.30.30.30">(</mo><mi id="S2.E1.m1.56.56.56.31.31.31">X</mi><mo id="S2.E1.m1.57.57.57.32.32.32">,</mo><mi id="S2.E1.m1.58.58.58.33.33.33">G</mi><mrow id="S2.E1.m1.65.65.65.40.40.45.1.1"><mo stretchy="false" id="S2.E1.m1.59.59.59.34.34.34">(</mo><mi id="S2.E1.m1.60.60.60.35.35.35">X</mi><mo stretchy="false" id="S2.E1.m1.61.61.61.36.36.36">)</mo></mrow><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E1.m1.62.62.62.37.37.37">|</mo><msup id="S2.E1.m1.65.65.65.40.40.45.1.2"><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E1.m1.63.63.63.38.38.38">|</mo><mn id="S2.E1.m1.64.64.64.39.39.39.1">2</mn></msup><mo stretchy="false" id="S2.E1.m1.65.65.65.40.40.40">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S2.E1.m1.66b">\begin{split}&amp;L(G)=E_{X}(||D(X,G(X))-\mathbb{I}||^{2})\\
&amp;L(D)=E_{X}(||D(X,Y)-\mathbb{I}||^{2})+E_{X}(||D(X,G(X)||^{2})\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS1.p1.9" class="ltx_p">where <math id="S2.SS2.SSS1.p1.1.m1.2" class="ltx_Math" alttext="X=\{X_{1},X_{2}\}" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.2a"><mrow id="S2.SS2.SSS1.p1.1.m1.2.2" xref="S2.SS2.SSS1.p1.1.m1.2.2.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.2.2.4" xref="S2.SS2.SSS1.p1.1.m1.2.2.4.cmml">X</mi><mo id="S2.SS2.SSS1.p1.1.m1.2.2.3" xref="S2.SS2.SSS1.p1.1.m1.2.2.3.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.1.m1.2.2.2.2" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.3" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.3.cmml">{</mo><msub id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.2" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.2.cmml">X</mi><mn id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.3" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.4" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.3.cmml">,</mo><msub id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.2" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.2.cmml">X</mi><mn id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.3" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.5" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.2b"><apply id="S2.SS2.SSS1.p1.1.m1.2.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2"><eq id="S2.SS2.SSS1.p1.1.m1.2.2.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.3"></eq><ci id="S2.SS2.SSS1.p1.1.m1.2.2.4.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.4">𝑋</ci><set id="S2.SS2.SSS1.p1.1.m1.2.2.2.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.2"><apply id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.2">𝑋</ci><cn type="integer" id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.3">1</cn></apply><apply id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.2">𝑋</ci><cn type="integer" id="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.2.2.2.3">2</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.2c">X=\{X_{1},X_{2}\}</annotation></semantics></math> are the input 2D Xray images, <math id="S2.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mi id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">Y</annotation></semantics></math> is objective 3D CT scan, and <math id="S2.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="G(X)" display="inline"><semantics id="S2.SS2.SSS1.p1.3.m3.1a"><mrow id="S2.SS2.SSS1.p1.3.m3.1.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.cmml"><mi id="S2.SS2.SSS1.p1.3.m3.1.2.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.p1.3.m3.1.2.1" xref="S2.SS2.SSS1.p1.3.m3.1.2.1.cmml">​</mo><mrow id="S2.SS2.SSS1.p1.3.m3.1.2.3.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.3.m3.1.2.3.2.1" xref="S2.SS2.SSS1.p1.3.m3.1.2.cmml">(</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.1" xref="S2.SS2.SSS1.p1.3.m3.1.1.cmml">X</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.3.m3.1.2.3.2.2" xref="S2.SS2.SSS1.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m3.1b"><apply id="S2.SS2.SSS1.p1.3.m3.1.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2"><times id="S2.SS2.SSS1.p1.3.m3.1.2.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.1"></times><ci id="S2.SS2.SSS1.p1.3.m3.1.2.2.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.2.2">𝐺</ci><ci id="S2.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.1c">G(X)</annotation></semantics></math> is the reconstructed 3D CT scan via the CVAA-GAN Generator. <math id="S2.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="E_{X}(\cdot)" display="inline"><semantics id="S2.SS2.SSS1.p1.4.m4.1a"><mrow id="S2.SS2.SSS1.p1.4.m4.1.2" xref="S2.SS2.SSS1.p1.4.m4.1.2.cmml"><msub id="S2.SS2.SSS1.p1.4.m4.1.2.2" xref="S2.SS2.SSS1.p1.4.m4.1.2.2.cmml"><mi id="S2.SS2.SSS1.p1.4.m4.1.2.2.2" xref="S2.SS2.SSS1.p1.4.m4.1.2.2.2.cmml">E</mi><mi id="S2.SS2.SSS1.p1.4.m4.1.2.2.3" xref="S2.SS2.SSS1.p1.4.m4.1.2.2.3.cmml">X</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.p1.4.m4.1.2.1" xref="S2.SS2.SSS1.p1.4.m4.1.2.1.cmml">​</mo><mrow id="S2.SS2.SSS1.p1.4.m4.1.2.3.2" xref="S2.SS2.SSS1.p1.4.m4.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.4.m4.1.2.3.2.1" xref="S2.SS2.SSS1.p1.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.p1.4.m4.1.1" xref="S2.SS2.SSS1.p1.4.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S2.SS2.SSS1.p1.4.m4.1.2.3.2.2" xref="S2.SS2.SSS1.p1.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.4.m4.1b"><apply id="S2.SS2.SSS1.p1.4.m4.1.2.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.2"><times id="S2.SS2.SSS1.p1.4.m4.1.2.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.2.1"></times><apply id="S2.SS2.SSS1.p1.4.m4.1.2.2.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.4.m4.1.2.2.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.4.m4.1.2.2.2.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.2.2.2">𝐸</ci><ci id="S2.SS2.SSS1.p1.4.m4.1.2.2.3.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.2.2.3">𝑋</ci></apply><ci id="S2.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.4.m4.1c">E_{X}(\cdot)</annotation></semantics></math> represents the expectation function, and here we use mean loss assumed to be independent and identically distributed. <math id="S2.SS2.SSS1.p1.5.m5.2" class="ltx_Math" alttext="D(X,Y)" display="inline"><semantics id="S2.SS2.SSS1.p1.5.m5.2a"><mrow id="S2.SS2.SSS1.p1.5.m5.2.3" xref="S2.SS2.SSS1.p1.5.m5.2.3.cmml"><mi id="S2.SS2.SSS1.p1.5.m5.2.3.2" xref="S2.SS2.SSS1.p1.5.m5.2.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.p1.5.m5.2.3.1" xref="S2.SS2.SSS1.p1.5.m5.2.3.1.cmml">​</mo><mrow id="S2.SS2.SSS1.p1.5.m5.2.3.3.2" xref="S2.SS2.SSS1.p1.5.m5.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.5.m5.2.3.3.2.1" xref="S2.SS2.SSS1.p1.5.m5.2.3.3.1.cmml">(</mo><mi id="S2.SS2.SSS1.p1.5.m5.1.1" xref="S2.SS2.SSS1.p1.5.m5.1.1.cmml">X</mi><mo id="S2.SS2.SSS1.p1.5.m5.2.3.3.2.2" xref="S2.SS2.SSS1.p1.5.m5.2.3.3.1.cmml">,</mo><mi id="S2.SS2.SSS1.p1.5.m5.2.2" xref="S2.SS2.SSS1.p1.5.m5.2.2.cmml">Y</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.5.m5.2.3.3.2.3" xref="S2.SS2.SSS1.p1.5.m5.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.5.m5.2b"><apply id="S2.SS2.SSS1.p1.5.m5.2.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.2.3"><times id="S2.SS2.SSS1.p1.5.m5.2.3.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.2.3.1"></times><ci id="S2.SS2.SSS1.p1.5.m5.2.3.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.2.3.2">𝐷</ci><interval closure="open" id="S2.SS2.SSS1.p1.5.m5.2.3.3.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.2.3.3.2"><ci id="S2.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.1">𝑋</ci><ci id="S2.SS2.SSS1.p1.5.m5.2.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.2.2">𝑌</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.5.m5.2c">D(X,Y)</annotation></semantics></math> is used for conditional discrimination, where <math id="S2.SS2.SSS1.p1.6.m6.1" class="ltx_Math" alttext="X_{1}" display="inline"><semantics id="S2.SS2.SSS1.p1.6.m6.1a"><msub id="S2.SS2.SSS1.p1.6.m6.1.1" xref="S2.SS2.SSS1.p1.6.m6.1.1.cmml"><mi id="S2.SS2.SSS1.p1.6.m6.1.1.2" xref="S2.SS2.SSS1.p1.6.m6.1.1.2.cmml">X</mi><mn id="S2.SS2.SSS1.p1.6.m6.1.1.3" xref="S2.SS2.SSS1.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.6.m6.1b"><apply id="S2.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.6.m6.1.1.1.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.6.m6.1.1.2.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.1.2">𝑋</ci><cn type="integer" id="S2.SS2.SSS1.p1.6.m6.1.1.3.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.6.m6.1c">X_{1}</annotation></semantics></math> and <math id="S2.SS2.SSS1.p1.7.m7.1" class="ltx_Math" alttext="X_{2}" display="inline"><semantics id="S2.SS2.SSS1.p1.7.m7.1a"><msub id="S2.SS2.SSS1.p1.7.m7.1.1" xref="S2.SS2.SSS1.p1.7.m7.1.1.cmml"><mi id="S2.SS2.SSS1.p1.7.m7.1.1.2" xref="S2.SS2.SSS1.p1.7.m7.1.1.2.cmml">X</mi><mn id="S2.SS2.SSS1.p1.7.m7.1.1.3" xref="S2.SS2.SSS1.p1.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.7.m7.1b"><apply id="S2.SS2.SSS1.p1.7.m7.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.1.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.7.m7.1.1.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1.2">𝑋</ci><cn type="integer" id="S2.SS2.SSS1.p1.7.m7.1.1.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.7.m7.1c">X_{2}</annotation></semantics></math> are first extended from 2D to 3D and then followed by a basic 3D convolution layer (as the 2D<math id="S2.SS2.SSS1.p1.8.m8.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S2.SS2.SSS1.p1.8.m8.1a"><mo stretchy="false" id="S2.SS2.SSS1.p1.8.m8.1.1" xref="S2.SS2.SSS1.p1.8.m8.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.8.m8.1b"><ci id="S2.SS2.SSS1.p1.8.m8.1.1.cmml" xref="S2.SS2.SSS1.p1.8.m8.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.8.m8.1c">\Rightarrow</annotation></semantics></math>3D sub-module in the encoder pathway). Then, the 3D features combined with <math id="S2.SS2.SSS1.p1.9.m9.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.SS2.SSS1.p1.9.m9.1a"><mi id="S2.SS2.SSS1.p1.9.m9.1.1" xref="S2.SS2.SSS1.p1.9.m9.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.9.m9.1b"><ci id="S2.SS2.SSS1.p1.9.m9.1.1.cmml" xref="S2.SS2.SSS1.p1.9.m9.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.9.m9.1c">Y</annotation></semantics></math> via concatenation on the channel dimension are put into the discriminator.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Quantitative results of baseline methods and CVAA-GAN(The values in parentheses are the standard deviations.)</figcaption>
<table id="S2.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.3.1.1" class="ltx_tr">
<th id="S2.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.3.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<td id="S2.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.1.1.2.1" class="ltx_text" style="font-size:70%;">MAE</span></td>
<td id="S2.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.1.1.3.1" class="ltx_text" style="font-size:70%;">MSE</span></td>
<td id="S2.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.1.1.4.1" class="ltx_text" style="font-size:70%;">Cosine Similarity</span></td>
<td id="S2.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.1.1.5.1" class="ltx_text" style="font-size:70%;">PSNR</span></td>
<td id="S2.T1.3.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.1.1.6.1" class="ltx_text" style="font-size:70%;">SSIM</span></td>
</tr>
<tr id="S2.T1.3.2.2" class="ltx_tr">
<th id="S2.T1.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.3.2.2.1.1" class="ltx_text" style="font-size:70%;">PSR</span></th>
<td id="S2.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.2.2.2.1" class="ltx_text" style="font-size:70%;">0.03258(8.9e-5)</span></td>
<td id="S2.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.2.2.3.1" class="ltx_text" style="font-size:70%;">0.003204(2e-6)</span></td>
<td id="S2.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.2.2.4.1" class="ltx_text" style="font-size:70%;">0.9360(5e-4)</span></td>
<td id="S2.T1.3.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.2.2.5.1" class="ltx_text" style="font-size:70%;">25.1421(2.246)</span></td>
<td id="S2.T1.3.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.2.2.6.1" class="ltx_text" style="font-size:70%;">0.6025(0.002)</span></td>
</tr>
<tr id="S2.T1.3.3.3" class="ltx_tr">
<th id="S2.T1.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.3.3.3.1.1" class="ltx_text" style="font-size:70%;">3DCNN</span></th>
<td id="S2.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.3.3.2.1" class="ltx_text" style="font-size:70%;">0.02982(9.1e-5)</span></td>
<td id="S2.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.3.3.3.1" class="ltx_text" style="font-size:70%;">0.003093(2e-6)</span></td>
<td id="S2.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.3.3.4.1" class="ltx_text" style="font-size:70%;">0.9389(3e-4)</span></td>
<td id="S2.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.3.3.5.1" class="ltx_text" style="font-size:70%;">25.4031(2.208)</span></td>
<td id="S2.T1.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.3.3.6.1" class="ltx_text" style="font-size:70%;">0.6328(0.001)</span></td>
</tr>
<tr id="S2.T1.3.4.4" class="ltx_tr">
<th id="S2.T1.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.3.4.4.1.1" class="ltx_text" style="font-size:70%;">X2CT-GAN</span></th>
<td id="S2.T1.3.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.4.4.2.1" class="ltx_text" style="font-size:70%;">0.01975(4.6e-5)</span></td>
<td id="S2.T1.3.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.4.4.3.1" class="ltx_text" style="font-size:70%;">0.001765(8e-7)</span></td>
<td id="S2.T1.3.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.4.4.4.1" class="ltx_text" style="font-size:70%;">0.9664(1e-4)</span></td>
<td id="S2.T1.3.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.4.4.5.1" class="ltx_text" style="font-size:70%;">27.8361(2.947)</span></td>
<td id="S2.T1.3.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.4.4.6.1" class="ltx_text" style="font-size:70%;">0.7673(0.006)</span></td>
</tr>
<tr id="S2.T1.3.5.5" class="ltx_tr">
<th id="S2.T1.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.3.5.5.1.1" class="ltx_text" style="font-size:70%;">CVAA-GAN</span></th>
<td id="S2.T1.3.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.3.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.01635(3.1e-5)</span></td>
<td id="S2.T1.3.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.3.5.5.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.001253(5e-7)</span></td>
<td id="S2.T1.3.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.3.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.9757(7e-5)</span></td>
<td id="S2.T1.3.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.3.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">29.7508(3.612)</span></td>
<td id="S2.T1.3.5.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.3.5.5.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.8113(0.004)</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Reconstruction Loss</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The conditional adversarial loss tries to make the prediction look similar to real data. However, it alone is not sufficient to ensure that the generated data is similar to the ground truth. Therefore, an additional constraint, such as the reconstruction loss, is required to enforce the reconstructed CT to be voxel-wise close to the ground truth. We define the reconstruction loss,
<math id="S2.SS2.SSS2.p1.1.m1.2" class="ltx_Math" alttext="L=E_{X}(||Y-G(X)||^{2})" display="inline"><semantics id="S2.SS2.SSS2.p1.1.m1.2a"><mrow id="S2.SS2.SSS2.p1.1.m1.2.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.2.2.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.3.cmml">L</mi><mo id="S2.SS2.SSS2.p1.1.m1.2.2.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.2.cmml">=</mo><mrow id="S2.SS2.SSS2.p1.1.m1.2.2.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.cmml"><msub id="S2.SS2.SSS2.p1.1.m1.2.2.1.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.3.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.2.2.1.3.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.3.2.cmml">E</mi><mi id="S2.SS2.SSS2.p1.1.m1.2.2.1.3.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.3.3.cmml">X</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.1.m1.2.2.1.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.2.cmml">​</mo><mrow id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.cmml">(</mo><msup id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.cmml"><mrow id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml">Y</mi><mo id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml">X</mi><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.2b"><apply id="S2.SS2.SSS2.p1.1.m1.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2"><eq id="S2.SS2.SSS2.p1.1.m1.2.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.2"></eq><ci id="S2.SS2.SSS2.p1.1.m1.2.2.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.3">𝐿</ci><apply id="S2.SS2.SSS2.p1.1.m1.2.2.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1"><times id="S2.SS2.SSS2.p1.1.m1.2.2.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.2"></times><apply id="S2.SS2.SSS2.p1.1.m1.2.2.1.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.2.2.1.3.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.3">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.2.2.1.3.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.3.2">𝐸</ci><ci id="S2.SS2.SSS2.p1.1.m1.2.2.1.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.3.3">𝑋</ci></apply><apply id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1">superscript</csymbol><apply id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1"><minus id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.1"></minus><ci id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.2">𝑌</ci><apply id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3"><times id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.1"></times><ci id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2">𝐺</ci><ci id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">𝑋</ci></apply></apply></apply><cn type="integer" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.2c">L=E_{X}(||Y-G(X)||^{2})</annotation></semantics></math>
where, inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, we apply 2D projections to the predicted volume and compare the resulting projection views to the X-ray images from the corresponding ground truth. Orthogonal projections, instead of perspective projections, are carried out to simplify the process as this auxiliary loss focuses only on the general shape consistency, but not the X-ray veracity. We choose to use three orthogonal projection planes, i.e., the axial, the coronal, and the sagittal plane, respectively, following the convention in the medical imaging community.
Some previous works have combined the reconstruction loss with the adversarial loss to improve the reconstruction performance. We also adopt this strategy. The adversarial loss plays an important role in encouraging local realism of the synthesized output. However, for surgical planning and intra-surgical observation, higher priority should be given to global and local shape consistencies.
</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first introduce a real-world lumbar vertebra dataset. We evaluate the proposed CVAA-GAN model with several commonly used metrics. To demonstrate the effectiveness of our method, we select several state-of-the-art methods as baselines. Fair comparisons and comprehensive analysis are given to demonstrate the improvement of our proposed method over the baselines. Finally, we show the CT reconstruction results from real-world biplanar X-rays using CVAA-GAN.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">In the experiments, we use a privacy dataset to verify proposed method. A set of real-world lumbar vertebra postoperative CT data which is collected from a hospital. It contains 268 3D CT scans of 268 different patients. We randomly select 212 CT scans for training and the rest 56 CT scans are used for testing. We first resample the CT scans to the same isotropic voxel resolution (<math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="2\times 2\times 2~{}mm^{3}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.2.1" xref="S3.SS1.p1.1.m1.1.1.2.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.2.3.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.2.1a" xref="S3.SS1.p1.1.m1.1.1.2.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.2.4" xref="S3.SS1.p1.1.m1.1.1.2.4.cmml">2</mn></mrow><mo lspace="0.330em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1a" xref="S3.SS1.p1.1.m1.1.1.1.cmml">​</mo><msup id="S3.SS1.p1.1.m1.1.1.4" xref="S3.SS1.p1.1.m1.1.1.4.cmml"><mi id="S3.SS1.p1.1.m1.1.1.4.2" xref="S3.SS1.p1.1.m1.1.1.4.2.cmml">m</mi><mn id="S3.SS1.p1.1.m1.1.1.4.3" xref="S3.SS1.p1.1.m1.1.1.4.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><apply id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2"><times id="S3.SS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.2.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2.2">2</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.2.3">2</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.4.cmml" xref="S3.SS1.p1.1.m1.1.1.2.4">2</cn></apply><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑚</ci><apply id="S3.SS1.p1.1.m1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.4.1.cmml" xref="S3.SS1.p1.1.m1.1.1.4">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.4.2.cmml" xref="S3.SS1.p1.1.m1.1.1.4.2">𝑚</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.4.3.cmml" xref="S3.SS1.p1.1.m1.1.1.4.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">2\times 2\times 2~{}mm^{3}</annotation></semantics></math>), followed by center-cropping with a fixed crop size (<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="128\times 128\times 128" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.1a" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.4" xref="S3.SS1.p1.2.m2.1.1.4.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">128</cn><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">128</cn><cn type="integer" id="S3.SS1.p1.2.m2.1.1.4.cmml" xref="S3.SS1.p1.2.m2.1.1.4">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">128\times 128\times 128</annotation></semantics></math>). We then clip data with CT value span [-1000, 4096], and use min-max normalization to scale data into 0-1 range. Ideally, paired CT and biplanar X-ray data are needed to train and test our proposed model. However, such a paired dataset is difficult to collect and currently not available. Therefore, we opt to use the digitally reconstructed radiographs (DRR) technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to synthesize the corresponding X-rays from volumetric CT images and generate the paired CT and X-ray images.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Settings</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">The generator and discriminator are trained alternatively following the standard GAN process. Meanwhile, we use instance normalization to regularize intermediate feature maps of our generator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We use the Adam solver <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to train our networks. The learning rate of Adam is 2e-4, momentum parameters <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.5" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">β</mi><mn id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><eq id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></eq><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3">1</cn></apply><cn type="float" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\beta_{1}=0.5</annotation></semantics></math> and <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\beta_{2}=0.99" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msub id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.cmml">β</mi><mn id="S3.SS2.p1.2.m2.1.1.2.3" xref="S3.SS2.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">0.99</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><eq id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></eq><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2">𝛽</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3">2</cn></apply><cn type="float" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">0.99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\beta_{2}=0.99</annotation></semantics></math>. We train our model for a total of 200 epochs. All of methods are conducted in NVIDIA A40. Constrained by GPU memory limit, the batch size is set to 4 in all our experiments. Three state-of-the-art methods are selected as baselines for CT reconstruction task, PSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, 3DCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, X2CT-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. For the baselines, we reproduce PSR, 3DCNN, and use the open-source codes of X2CT-GAN. The parameters of models are set as in publications.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Qualitative Results</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We first qualitatively evaluate CT reconstruction results as
shown in Fig.<a href="#S2.F2" title="Figure 2 ‣ II-A2 Decoder ‣ II-A Generator ‣ II CVAA-GAN ‣ Coarse-Fine View Attention Alignment-Based GAN for CT Reconstruction from Biplanar X-Rays" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For each method, the mid-axial, mid-sagittal, and mid-coronal slices of the reconstruction CT are shown in the first, second, and third rows, respectively. PSR just using a single-view X-ray generates blurrier volumes compared to the other biplanar X-ray-based models. Compared with 3DCNN, X2CT-GAN, and ours generate sharper boundaries of the organs and implants, suggesting the cross-view fusion in the decoder is effective in preserving more fine-grained information from the input X-ray data. Last, our proposed CVAA-GAN visually performs the best, generating the closest morphological characteristics of the vertebrae and the implants to the ground truth, and the sharpest boundaries, suggesting our method achieves less reconstruction distortion and less detailed texture loss.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Quantitative Results</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Quantitative results are summarized in Table <a href="#S2.T1" title="TABLE I ‣ II-B1 Adversarial Loss ‣ II-B Objective Functions ‣ II CVAA-GAN ‣ Coarse-Fine View Attention Alignment-Based GAN for CT Reconstruction from Biplanar X-Rays" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
We can find that 1) PSR with single X-ray input has the worst performance compared to other biplanar models. 2) X2CT-GAN and our model have better performance than 3DCNN. The cross-view fusion in the decoder pathway is essential in recovering the underlying 3D anatomy. 3) Compared with the best baseline, CVAA-GAN shows apparent performance improvement, suggesting the view attention alignment sub-module can leverage different strengths of different view features in the cross-view fusion step and the fine distillation sub-module can direct the model to focus on more valuable information that is complementary to each of the views without unnecessary noise or redundancy.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we propose a novel view attention alignment to fuse biplanar X-ray features which can leverage different characteristics from different views. Furthermore, we introduce a coarse-to-fine scheme to extract finer features from coarse features to further improve algorithm performance. Experimental results demonstrated the superior performance of our approach compared with the SOTA baselines.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The work is supported by the following projects, National Natural Science Foundation of China (82171927), Peking University Third Hospital Clinical Key Project (BYSY2018003), National Natural Science Foundation of China (82102638) and Peking University Third Hospital Clinical Key Project- (BYSYZD2021040).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Kasten, D. Doktofsky, and I. Kovler, “End-to-end convolutional neural network for 3d reconstruction of knee bones from bi-planar x-ray images,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Machine Learning for Medical Image Reconstruction: Third International Workshop, MLMIR 2020</em>.   Berlin, Heidelberg: Springer-Verlag, 2020, p. 123–133.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, ser. NIPS’14.   Cambridge, MA, USA: MIT Press, 2014, p. 2672–2680.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley, “Least squares generative adversarial networks,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on Computer Vision (ICCV)</em>, 2017, pp. 2813–2821.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Jiang, S. Shi, X. Qi, and J. Jia, “Gal: Geometric adversarial loss for single-view 3d-object reconstruction,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2018</em>, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss, Eds.   Cham: Springer International Publishing, 2018, pp. 820–834.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. M. Galvin, C. Sims, G. Dominiak, and J. S. Cooper, “The use of digitally reconstructed radiographs for three-dimensional treatment planning and ct-simulation,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International Journal of Radiation Oncology*Biology*Physics</em>, vol. 31, no. 4, pp. 935–942, 1995.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Ulyanov, A. Vedaldi, and V. S. Lempitsky, “Instance normalization: The missing ingredient for fast stylization.” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 12 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. Henzler, V. Rasche, T. Ropinski, and T. Ritschel, “Single-image tomography: 3d volumes from 2d cranial x-rays,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Computer Graphics Forum</em>, vol. 37, no. 2, pp. 377–388, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
X. Ying, H. Guo, K. Ma, J. Wu, Z. Weng, and Y. Zheng, “X2ct-gan: Reconstructing ct from biplanar x-rays with generative adversarial networks,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019, pp. 10 611–10 620.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.09735" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.09736" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.09736">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.09736" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.09737" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 17:40:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
