<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.08361] Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks</title><meta property="og:description" content="Federated learning (FL) has been promoted as a popular technique for training machine learning (ML) models over edge/fog networks.
Traditional implementations of FL have largely neglected the potential for inter-networ…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.08361">

<!--Generated on Thu Feb 29 20:08:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Cooperative Federated Learning 
<br class="ltx_break">over Heterogeneous Edge/Fog Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Su Wang, , Seyyedali Hosseinalipour, , Vaneet Aggarwal, , Christopher G. Brinton, , David J. Love, , Weifeng Su, , and Mung Chiang
</span><span class="ltx_author_notes">S. Wang, V. Aggarwal, C. Brinton, D. Love, and M. Chiang are with Purdue University, IN, USA e-mail: {wang2506, vaneet, cgb, djlove, chiang}@purdue.edu.S. Hosseinalipour and W. Su are with the University at Buffalo–SUNY, NY, USA e-mail: {alipour, weifeng}@buffalo.edu.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning (FL) has been promoted as a popular technique for training machine learning (ML) models over edge/fog networks.
Traditional implementations of FL have largely neglected the potential for inter-network cooperation, treating edge/fog devices and other infrastructure participating in ML as separate processing elements. Consequently, FL has been vulnerable to several dimensions of network heterogeneity, such as varying computation capabilities, communication resources, data qualities, and privacy demands.
<span id="id1.id1.1" class="ltx_text" style="color:#000000;">We advocate for cooperative federated learning (CFL), a cooperative edge/fog ML paradigm built on device-to-device (D2D) and device-to-server (D2S) interactions. Through D2D and D2S cooperation, CFL counteracts network heterogeneity in edge/fog networks through enabling a model/data/resource pooling mechanism, which will yield substantial improvements in ML model training quality and network resource consumption.</span>
We propose a set of core methodologies that form the foundation of D2D and D2S cooperation and present preliminary experiments that demonstrate their benefits. We also discuss new FL functionalities enabled by this cooperative framework such as the integration of unlabeled data and heterogeneous device privacy into ML model training. Finally, we describe some open research directions at the intersection of cooperative edge/fog and FL.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="color:#000000;">Recently, much attention has been given to the implementation of data analytics and machine learning (ML) techniques at the network edge to handle the complexity of emerging Internet of Things (IoT) services, ranging from user-oriented (e.g., object recognition) to network-oriented (e.g., signal classification) applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</span>
IoT devices are now capable of gathering data from various sources, connecting to the internet, and performing computation tasks.
Collectively, they form edge/fog networks capable of producing machine intelligence insights. <span id="S1.p1.1.2" class="ltx_text" style="color:#000000;">Traditionally, data insights were produced via centralized computing, where network devices send all of their local measurements to a single central server for ML training. Such methods led to system-wide latency and resource inefficiencies as a result of data transmissions from edge devices to the server, for centralized ML tasks specifically. These limitations have led to the emergence of distributed ML techniques and in particular federated learning (FL).</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="color:#000000;">Standard FL shifts the processing portion of ML training from the server to the edge/fog devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. As shown in the upper left corner of Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, it involves a “star” server-to-device communication topology, inside of a three-part cyclical process: (i) edge devices independently and locally train an ML model, (ii) ML models are sent to the central server for global aggregation, and (iii) the server synchronizes devices’ ML models into an aggregated ML model called the global model.</span>
While standard FL features global aggregations, this is the only form of cooperation among network elements.
Inter-device and inter-network communications - key features of IoT networks - can also facilitate cooperation and are missed opportunities in FL. For example, direct device-to-device (D2D) communication links that are otherwise underutilized could be employed for faster and communication-efficient ML model training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Traditional FL therefore does not exploit the full potential of cooperation in large-scale edge/fog networks. </p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.08361/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="452" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text" style="color:#000000;"> The progression from standard FL towards network-aware cooperative FL. Starting from standard FL and its associated trade-offs, we develop the pillars of cooperative FL frameworks, namely device-to-device and device-to-server cooperation.
We envision that future work towards integrating collaboration into FL involves ideas such as multi-hop cooperation, integration of unlabeled data, and heterogeneous privacy.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="color:#000000;">We propose cooperative federated learning (CFL), a cooperative edge/fog ML paradigm that jointly orchestrates device, server, and network infrastructure resources to enhance FL while considering its core trade-offs, as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</span>
<span id="S1.p3.1.2" class="ltx_text" style="color:#000000;">CFL extends the notion of cooperation to address the key missed opportunities in standard FL, which are summarized below:</span></p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Edge devices with powerful local processors or small local datasets idly wait for network stragglers to finish training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Powerful network infrastructure elements such as edge/fog servers are underutilized in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Edge devices without direct connectivity to the central server are neglected during ML model training and synchronization processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">IoT devices, which may have diverse privacy requirements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, are all discouraged from sharing data/models over the network.</p>
</div>
</li>
</ol>
<p id="S1.p3.2" class="ltx_p">These shortcomings are the result of prohibiting powerful devices, idle edge servers, and network infrastructure from helping computationally weak and/or overburdened devices in FL.
<span id="S1.p3.2.1" class="ltx_text" style="color:#000000;">Simultaneously, D2D and device-to-server (D2S) cooperation over such edge/fog networks has been shown to be feasible and beneficial to learning processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</span>
Well-designed cooperation mechanisms can thus unlock the full potential of edge/fog networks for FL, leading to (i) improved ML performance, (ii) energy efficiency, (iii) temporal efficiency (e.g., faster ML training), and (iv) diverse data/model privacy.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Cooperative Federated Learning</span>
</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="color:#000000;">We propose cooperative federated learning (CFL), a novel paradigm that expands the dimensions of cooperation in FL beyond global aggregations. CFL develops inter-element cooperation mechanisms including selective data sharing, computation resource sharing, ML model sharing, and data distribution comparisons.
FL driven by such multi-faceted cooperation better exploits the availability of links among network devices, servers, and infrastructure in contemporary edge/fog systems. Through such links, CFL unlocks the potential of cooperative edge/fog networks for ML.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text" style="color:#000000;">Whereas the state-of-the-art in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> has mostly considered data offloading as a mechanism for improving local statistical properties in FL, our vision for CFL involves a broader look at cooperation in FL, including D2D- and D2S-driven data, model, and resource cooperation.</span>
<span id="S2.p2.1.2" class="ltx_text" style="color:#000000;">
These proposed cooperation technologies aim to improve the balance among the trade-offs in FL shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For example, intelligent cooperation can lead to better ML model performance with less energy usage and system delay.</span></p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Specifically CFL (i) leverages D2D links for data and model sharing, which we term D2D cooperation, and (ii) leverages D2S links to incorporate edge servers, routers, and other network infrastructure into the FL ecosystem through data processing and ML model transmission tasks.
<span id="S2.p3.1.1" class="ltx_text" style="color:#000000;"> We consider data transfers in CFL noting that while some applications of FL (e.g., healthcare analytics) discourage data sharing, other applications have milder data privacy restrictions, especially when the data is generated with ML as the primary purpose (e.g., FL for self-driving vehicles with sensor measurements).</span></p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Combined, D2D and D2S cooperation form the two pillars of CFL, which together enable many complementary technologies, we only examine a few of which for brevity. As depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we examine (i) multi-hop cooperation due to its key influence on improved resource efficiency (i.e., energy efficiency and temporal sensitivity), (ii) integration of unlabeled data as it enhances ML performance for devices, and (iii) devices heterogeneous privacy demands because it focuses on the data/model privacy aspects of CFL.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Overarching Technologies for CFL</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the following, we explain how D2D and D2S cooperation exploit the network characteristics inherent in edge/fog networks, and fulfill the missed opportunities of FL.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.4.1.1" class="ltx_text">II-A</span>1 </span>Device-to-device (D2D) cooperation</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">In edge/fog networks, devices are heterogeneous statistically (i.e., different dataset characteristics) and structurally (i.e., varying computation/communication capabilities). In standard FL, such heterogeneity leads to isolated resource-abundant and resource-scarce devices, some of which may introduce straggler effects and delay ML model aggregations. In the worst-case, resource-scarce devices may be unable to complete ML model training iterations, possibly due to insufficient battery, or result in the existence of unused local data, as model training in straggler devices use smaller batches of data. To cope with device heterogeneity, we exploit D2D cooperation as discussed below.
</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p"><span id="S2.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Cooperation as resource pooling.</span> Cooperative edge/fog networks can reallocate the intensity of local ML training by leveraging D2D links for data transfers from resource-scarce to resource-abundant devices. Through this process, the impact of stragglers (i.e., resource-scarce devices) on ML model training is then reduced.
Simultaneously, D2D cooperation shifts the burden of ML model training to devices with energy-efficient processors, and thus can lead to network energy savings. </p>
</div>
<div id="S2.SS1.SSS1.p3" class="ltx_para">
<p id="S2.SS1.SSS1.p3.1" class="ltx_p"><span id="S2.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">D2D driven model offloading.</span>
Similar to data and subsequent ML training offloading, D2D cooperation can mitigate some of the model aggregation overhead. Rather than only transmitting local ML models to the aggregation server, devices can transfer different parts/chunks of their model to their neighboring devices. In this way, those devices that are far away from the server or have limited communication resources (e.g., limited bandwidth) can communicate with their neighboring devices.
Devices that receive models from their neighbors then combine received models with their local one and then transmit these partially combined ML parameters to the server.
Thus, D2D model offloading can reduce the number of devices engaging in resource intensive uplink transmissions.
</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.4.1.1" class="ltx_text">II-A</span>2 </span>Device-to-server (D2S) cooperation</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">Current FL research presumes conducting ML model training solely on the devices and neglects the underutilized network infrastructure elements such as edge servers.
While edge servers may not gather data themselves, they can add value to the FL process owing to their powerful local processors and dedicated communications equipment which can be exploited through network collaboration. We will refer to all cooperation, aside from global model aggregations, between devices and edge servers as D2S cooperation.
Two potential use cases of edge servers are provided below.
</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p"><span id="S2.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Computational resources.</span>
D2S cooperation allows resource-scarce devices to transfer their local data to a physically stable and computationally powerful edge server. These edge servers then function similarly as a resource-abundant device, enabling the network to process more training data and lessening straggler effects.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p"><span id="S2.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Flexible data caches.</span>
Using the data they receive from nearby devices, edge servers can act as local data caches, which can carry globally representative distributions of high quality data, to mitigate the impact of non-i.i.d. data across the network.
Additionally, this functionality enables better tracking of the distribution shifts in the data via comparing old data with newly arriving data at the edge servers, which enables more informative decisions on ML model training.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Enhancing the Core Properties of CFL</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">While many applications/extensions are possible from the foundation of D2D/D2S cooperation, we focus on a subset of techniques that can enhance the core trade-offs in CFL. We present high-level explanations of (i) multi-hop cooperation due to its benefits for resource efficiency (energy efficiency and time sensitivity), (ii) integration of unlabeled data as it extends FL to benefit a wider range of devices (e.g., devices with unlabeled data), and (iii) heterogeneous privacy for its enhancements to data/model privacy.
</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Multi-hop D2D and D2S cooperation.</span>
<span id="S2.SS2.p2.1.2" class="ltx_text" style="color:#000000;">Multi-hop D2D and D2S cooperation refers to extending the above concepts developed for single-hop D2D and D2S cooperation to multiple, sequential links in between devices.</span>
This envisioned technology can greatly improve the resource efficiency (i.e., less energy consumption and/or faster model training) of FL. In particular, multi-hop cooperation enables greater connectivity/reach from resource-scarce to resource-abundant edge devices or servers.
</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Integration of unlabeled datasets.</span> D2D and D2S cooperation can better represent and facilitate the contribution of devices with partially labeled or unlabeled datasets in FL. <span id="S2.SS2.p3.1.2" class="ltx_text" style="color:#000000;">Unlabeled data refers to data samples that have not been tagged with a ground-truth, e.g., images taken by cameras mounted on smart cars without pre-assigned or pre-identified types of objects within the image.</span>
In standard FL, only devices with labeled data are engaged in ML model training. Consequently, edge devices with unlabeled datasets are unlikely to have their data properly represented at the global ML model and so are likely to suffer from poor ML performance.
Roughly speaking, D2D and D2S cooperation can enable approximations of the local data distribution of each device at its neighboring devices/network elements, even if the device has fully or mostly unlabeled data.
Subsequently, the type of distributed learning method being applied can be tuned to improve ML performance across the network.
</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Heterogeneous privacy.</span>
<span id="S2.SS2.p4.1.2" class="ltx_text" style="color:#000000;">Similar to statistical (i.e., data-level) and structural (i.e., computation/communication resources) differences, edge/fog devices also exhibit heterogeneity with respect to their privacy needs.
Heterogeneous privacy needs will motivate selective D2D and D2S cooperation.
For example, D2D cooperation can involve sharing sensitive data only among mutually trusted devices (e.g., edge devices belonging to the same user or family), while among untrusted neighbors, this sharing can be limited to sharing insensitive data or even prohibited completely.
This is one of the future complementary technologies depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. With such methods in place, D2D/D2S cooperative technologies can improve the resource efficiency and ML performance while meeting data/model privacy requirements of edge/fog network elements.</span></p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Towards Network-Aware CFL</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">As depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our vision for CFL relies on cooperative edge/fog networks to enhance standard FL on (i) ML performance - the effectiveness of the ML model trained by the network, (ii) energy efficiency - the network-wide accumulated energy expenditure on data processing, and data/model communication, (iii) temporal efficiency - the total time including idle time consumed by the FL process, and (iv) data and model privacy - the heterogeneous privacy needs in large-scale edge/fog networks.
Methodologies that develop network-aware CFL must carefully balance their contributions to these four coupled elements, which contain design trade-offs.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">The first layer of network-aware CFL consists of the pillars of D2D and D2S cooperation and their core mechanisms. D2D and D2S cooperation can leverage data and model transfers to cope with device heterogeneity. <span id="S2.SS3.p2.1.1" class="ltx_text" style="color:#000000;">For example, data offloading through D2D links can yield energy savings, and ML model routing through D2S links can mitigate straggler effects.</span></p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">With our established frameworks of D2D and D2S cooperation, we can then develop complementary technologies to enhance CFL along the four design considerations of (i) ML performance, (ii) energy efficiency, (iii) temporal sensitivity, and (iv) data/model privacy.
In Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we depict three sample complementary techniques, with each technique primarily improving one aspect of CFL. For instance, multi-hop collaborations such as those seen in industrial IoT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> enhance resource (energy and delay) efficiency, integration of unlabeled data such as those in autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> can improve ML performance, and heterogeneous privacy as seen in social trust <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> offers an alternative approach to data/model privacy.
We next develop D2D and D2S cooperation as the two pillars of network-aware CFL, and, as future work, explain how complementary technologies can enhance them.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Network-aware D2D Cooperation</span>
</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2303.08361/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="221" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Data and local model offloading form the basis of effective D2D cooperation. We envision smart data offloading by edge devices through dataset stratification.
Additionally, we propose local model offloading, where devices offload segments of their local ML models to other devices. </figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">The first step towards network-aware CFL is to develop and maximize the benefits arising from D2D cooperation.
<span id="S3.p1.1.1" class="ltx_text" style="color:#000000;">Well-designed D2D cooperation can enable efficient orchestration of limited network resources, leading to improvements in the trade-offs of FL from Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</span> To this end, we first introduce a set of core, overarching technologies to enable effective D2D cooperation, and thereafter propose future work on complementary technologies to further enhance D2D cooperation.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2303.08361/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="107" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.2.1" class="ltx_text" style="color:#000000;">Our D2D cooperation-driven method incurs both less compute time and energy consumption relative to the state-of-the-art <math id="S3.F3.2.1.m1.1" class="ltx_Math" alttext="\mathsf{NOVA}" display="inline"><semantics id="S3.F3.2.1.m1.1b"><mi mathcolor="#000000" id="S3.F3.2.1.m1.1.1" xref="S3.F3.2.1.m1.1.1.cmml">𝖭𝖮𝖵𝖠</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.m1.1c"><ci id="S3.F3.2.1.m1.1.1.cmml" xref="S3.F3.2.1.m1.1.1">𝖭𝖮𝖵𝖠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.m1.1d">\mathsf{NOVA}</annotation></semantics></math> methodology on two commonly used machine learning datasets (MNIST for numbers and Fashion-MNIST for clothes).</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Core Techniques for Effective D2D Cooperation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Effective D2D cooperation improves network resource efficiency and ML model training through innovations in data and model offloading, which we propose in Fig. <a href="#S3.F2" title="Figure 2 ‣ III Network-aware D2D Cooperation ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. At the data level, we propose dataset stratification, a method which clusters local datasets for higher quality data offloading. At the ML model level, we propose local model offloading, a technique that involves partial local ML model offloading to streamline efficient ML model aggregations. </p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Dataset Stratification</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">After offloading data, the sending device (sender) continues local ML model training on a smaller local dataset, as keeping a copy of the transferred data leads to bias at ML model aggregations from counting the same data multiple times. On the other end, the receiving device (receiver) may receive data that is unrepresentative of the data gathered at the sender.
As a result, random/naive data offloading may hinder rather than help the ML model training, motivating dataset stratification.
</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">As depicted on the left subplot of Fig. <a href="#S3.F2" title="Figure 2 ‣ III Network-aware D2D Cooperation ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, dataset stratification clusters datasets into strata (i.e., categories) based on task-dependent criteria.
<span id="S3.SS1.SSS1.p2.1.1" class="ltx_text" style="color:#000000;">
Using the example of clothing recognition (i.e., the Fashion-MNIST dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>) for data collected by smartphones’ cameras, each stratum may contain data belonging to a unique type of clothing (e.g., T-shirts or coats).
Then, through sequential selection of the most representative data samples (i.e., those that are closest to the strata average) from the most populous strata, devices can offload datapoints which well-capture the distribution of their local dataset.</span>
In doing so, dataset stratification (i) keeps the distribution of the dataset at senders relatively intact, and (ii) enables receivers to receive a representative sample of data from senders.
</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">As an example, consider a smart car communicating with a drone. From its operation, the car has images of mostly stop signs and traffic lights, which the drone may not. Dataset stratification enables the car to transmit a small set of representative stop sign and traffic light pictures, without significantly distorting its local dataset distribution, to the drone.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>Local Model Offloading</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">We propose segmenting and sequentially transmitting the devices’ local ML models at global ML model aggregations. <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">Each segment of an ML model is a subset of model parameters. For example, in the case of neural networks, each segment may contain the parameters associated with a layer of the neural network. Local model offloading, depicted in Fig. <a href="#S3.F2" title="Figure 2 ‣ III Network-aware D2D Cooperation ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, enables devices with limited access to the server (e.g., due to unsatisfactory channel conditions) to offload different segments of their local ML models to intermediary devices with a better accessibility to the server.</span>
These intermediary devices will combine their local ML models with their received partial ML models, leading to a set of partially aggregated parameters, which are then sent to the central server.
In this way, the central server is able to perform the global aggregation using all ML model parameters while saving communication resources.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p"><span id="S3.SS1.SSS2.p2.1.1" class="ltx_text" style="color:#000000;">We have taken initial steps toward formalizing this D2D cooperation methodology in our prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, including optimization formulation, theoretical results, and convergence proofs.
To evaluate its potential benefits, we compare it against the algorithm Nova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> on two common datasets used to evaluate FL methods: MNIST (numbers) and Fashion-MNIST (clothes). Simulation results are shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III Network-aware D2D Cooperation ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where both methods train a two layer convolutional neural network (CNN) across a network of 10 devices. A detailed description of the computational infrastructure, wireless channel models, and models of energy consumption (including energy from both the communication and computation processes) can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</span>
<span id="S3.SS1.SSS2.p2.1.2" class="ltx_text" style="color:#000000;">We compare against Nova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, a recent and well-established method for aggregations involving heterogeneous training epochs across edge/fog devices.</span>
Our proposed CFL technology is seen to yield (i) consistent energy savings, and (ii) faster ML training times.
</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2303.08361/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_img_landscape" width="221" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Device-to-Server (D2S) cooperation via data/model transfers can improve the resource efficiency and performance of FL. Additionally, intelligent selection of the aggregation server can further reduce aggregation delay.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Future Development of Complementary Technologies</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Further extension of D2D cooperation can further enhance the trade-offs in FL and subsequently CFL depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. <span id="S3.SS2.p1.1.1" class="ltx_text" style="color:#000000;"> A few open research directions are summarized below:</span></p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>D2D cooperation in non-stationary networks</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">In non-stationary networks, devices will enter and exit the network, leading to varying network size, changing compute resource availability, and time-varying D2D links.
Here effective cooperation should consider the physical stability of edge/fog devices to determine effective time-varying anchor devices.
<span id="S3.SS2.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">These anchors will receive nearby ML models from devices as they leave the network and transmit the latest global ML model to new devices as they join the network. In doing so, anchor devices improve ML training by enabling more devices to contribute to the training process in-between global model aggregations.</span></p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.5.1.1" class="ltx_text">III-B</span>2 </span><math id="S3.SS2.SSS2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS2.1.m1.1b"><mi id="S3.SS2.SSS2.1.m1.1.1" xref="S3.SS2.SSS2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.1.m1.1c"><ci id="S3.SS2.SSS2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.1.m1.1d">n</annotation></semantics></math>-hop cooperation</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.2" class="ltx_p"><math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mi mathcolor="#000000" id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">n</annotation></semantics></math><span id="S3.SS2.SSS2.p1.2.1" class="ltx_text" style="color:#000000;">-hop cooperation aims to broaden the scope of D2D cooperation, providing resource-scarce devices with greater access to resource-abundant ones through intermediary devices. Consequently, this technology can further enhance the resource and time savings introduced by single-hop D2D cooperation. This calls for novel optimization methodologies to characterize the benefits and trade-offs of <math id="S3.SS2.SSS2.p1.2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS2.p1.2.1.m1.1a"><mi mathcolor="#000000" id="S3.SS2.SSS2.p1.2.1.m1.1.1" xref="S3.SS2.SSS2.p1.2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.1.m1.1b"><ci id="S3.SS2.SSS2.p1.2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.1.m1.1c">n</annotation></semantics></math>-hop cooperation.</span></p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.4.1.1" class="ltx_text">III-B</span>3 </span>Heterogeneous privacy needs in D2D cooperation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Edge/fog devices may have heterogeneous privacy needs. For example, D2D connections may be allowed based on trust or familiarity.
In such cases, devices often band together into cliques, which are private groups with a certain level of mutual trust.
<span id="S3.SS2.SSS3.p1.1.1" class="ltx_text" style="color:#000000;">Data transferring can be restricted to links between mutually trusted devices (e.g., smart devices such as a smartphone, laptop, and tablet of the same owner).
In intra-clique cooperation, then, devices can share data without any restrictions, while, in inter-clique cooperation, devices may only be willing to share model parameters or insensitive data.
Furthermore, dataset stratification can be designed to separate data based on sensitivity, with restricted offloading of sensitive strata (e.g., personal health data or biometrics) among intra-clique devices and unrestricted offloading of insensitive strata (e.g., weather information).</span></p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS4.4.1.1" class="ltx_text">III-B</span>4 </span>Inclusion of devices with unlabeled data</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">In practical edge/fog networks, some devices may have mostly or fully unlabeled datasets.
Standard FL neglects all these devices and obtains a global ML model by only engaging the devices with labeled datasets.
<span id="S3.SS2.SSS4.p1.1.1" class="ltx_text" style="color:#000000;">Through D2D cooperation, edge devices can share small quantities of data, labeled or unlabeled, to develop estimates of data distributions at devices with unlabeled datasets.
This technique, termed unlabeled distribution estimation, will then involve determining unique combinations of ML models trained by devices with labeled datasets for use at devices with unlabeled datasets.</span></p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Network-aware D2S Cooperation</span>
</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Edge servers, especially in large-scale edge/fog networks, offer an untapped resource in standard FL.
<span id="S4.p1.1.1" class="ltx_text" style="color:#000000;">D2S cooperation aims to facilitate efficient utilization of these resources, e.g., by enabling devices with limited computation capabilities to transfer local training data to edge servers. In doing so, edge servers can leverage their powerful and efficient processors to improve energy efficiency and training delay of ML model training, thus enhancing the trade-off between energy consumption and ML performance.</span>
In the following, we introduce a set of core technologies that enable effective D2S cooperation, and thereafter explain complementary future technologies to further enhance it. </p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2303.08361/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_img_landscape" width="221" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S4.F5.2.1" class="ltx_text" style="color:#000000;"> On both Fashion-MNIST and CIFAR-10, our method based on D2S collaborations enable both energy and time savings during the ML model training process.</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Core Techniques for Effective D2S Cooperation</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Edge servers can have a diverse set of functionalities when assisting ML. They can act as computational resources to train ML models, as communication gateways to reroute ML models during global aggregations, and as the model aggregation points.
Starting at the computational level, we propose a novel technology called load balancing of data processing tasks, that relies on data offloading from edge devices to edge servers. Then, we propose efficient ML model parameter and data routing from edge devices to edge servers using base stations and network routers. Finally, we develop a concept called floating aggregation point, a method to optimize the selection of the aggregation server to save communication resources and minimize communication delay.
We present a visual summary of these new technologies in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-A2 Local Model Offloading ‣ III-A Core Techniques for Effective D2D Cooperation ‣ III Network-aware D2D Cooperation ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and explain them in detail below.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.4.1.1" class="ltx_text">IV-A</span>1 </span>Load Balancing of Data Processing</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">In standard FL, edge devices perform all of the computationally intensive ML model training tasks. We propose a novel technology of load balancing for data processing tasks in order to make use of the computational power at edge servers, similar to mobile edge computing frameworks in large-scale edge/fog networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
As part of load balancing, resource-constrained edge devices have the option to transfer a subset or all of their local data to nearby base stations (or road-side units), which through efficient data routing (another innovation which we explain next) relay the data to one or many edge servers.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.4.1.1" class="ltx_text">IV-A</span>2 </span>Efficient Data and Parameter Routing</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">To enable D2S cooperation, we propose efficient data and model parameter routing through the use of routers/switches as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-A2 Local Model Offloading ‣ III-A Core Techniques for Effective D2D Cooperation ‣ III Network-aware D2D Cooperation ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Through a combination of base stations and routers/switches, we can finely control the routing of data and ML models based on communication factors, such as channel congestion (a base station may be serving many users), and systems factors, such as computation power availability (an edge server may be running intensive data backups).
This fine-grained control of data and ML model routing enables energy efficient and fast ML training.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS3.4.1.1" class="ltx_text">IV-A</span>3 </span>Floating Aggregation Point</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">In scenarios with many edge servers, we can improve resource efficiency by dynamically selecting the aggregation server. Specifically, we propose floating aggregation point, a novel technology that adjusts the aggregation server based on changing edge/fog network properties.
As shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-A2 Local Model Offloading ‣ III-A Core Techniques for Effective D2D Cooperation ‣ III Network-aware D2D Cooperation ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the choice of global aggregation server changes in response to devices’ positions and dataset sizes, which influence the total communication/computation resource consumption for ML model training and parameter aggregation (uplink) and broadcasting (downlink).</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p"><span id="S4.SS1.SSS3.p2.1.1" class="ltx_text" style="color:#000000;">We have taken initial steps towards formalizing such a D2S cooperation methodology in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, including corresponding mathematical formulations and a proof-of-concept testbed implementation.
To demonstrate the potential benefits, we compare our method to FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and Nova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> on two common benchmark datasets in FL literature: Fashion-MNIST (clothes) and CIFAR-10 (common objects) in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Network-aware D2S Cooperation ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In this experiment, we consider a network of 20 edge devices and 10 edge servers training a two layer CNN. Other system parameters, such as the wireless channels and edge server links, can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</span>
This shows that CFL obtains substantial improvements over both baselines in terms of energy and time savings for the same target ML performance.
</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Future Development of Complementary Technologies</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">D2S cooperation can be further extended to enhance the existing trade-offs in FL and CFL depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Cooperative Federated Learning over Heterogeneous Edge/Fog Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
<span id="S4.SS2.p1.1.1" class="ltx_text" style="color:#000000;">The following outlines a few open research directions:</span></p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>Non-stationary servers</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Given current trends leveraging unmanned aerial vehicles (UAVs) as mobile communication servers (e.g., at sporting events), a natural next step for D2S collaboration involves non-stationary edge/fog servers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
<span id="S4.SS2.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">Since mobile servers’ locations can be controlled, efficient server placement methods can be pursued to improve data/model routing and offloading.
These methods should carefully investigate the trade-offs involved in server positioning.
For example, placing a server near a dense neighborhood of devices may ease aggregation delay, but placing a server near a few resource-scarce devices may save more computation energy.</span></p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>Joint D2D and D2S collaboration</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Frameworks combining D2D and D2S can yield further benefits to ML performance, resource consumption, and time efficiency.
<span id="S4.SS2.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">Such frameworks can enable simultaneous D2D and D2S data/model offloading. For example, as an edge device offloads data to another device, this secondary device could simultaneously can offload data to an edge server. This combined D2D and D2S cooperation ensures that (i) resource-abundant edge devices do not become overburdened, and (ii) resource consumption of data offloading (i.e., energy and delay) is optimized.</span></p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS3.4.1.1" class="ltx_text">IV-B</span>3 </span>Inclusion of unlabeled data</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">D2S cooperation can also be used to extend FL to edge/fog networks with fully unlabeled data, such as autonomous driving where camera-equipped cars take images without labels. One possible approach is to extended the well-known concept of contrastive learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to FL. Standard contrastive learning differentiates among datapoints in centralized settings via determining their similarities and differences.
However, in federated settings, edge devices’ datasets may simply be too small or lack sufficient data for standard contrastive learning to be effective. Through D2S collaboration, contrastive learning can be enabled in FL by leveraging edge servers as caches of data. As caches, edge servers can then supplement edge devices’ local datasets with their cached data so that the compare and contrast steps of contrastive learning are feasible/effective in FL.
</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We proposed cooperative federated learning (CFL), a paradigm that extends the notion of cooperation in federated learning (FL) and unlocks the potential of edge/fog networks in the execution of distributed machine learning tasks.
Through device-to-device (D2D) and device-to-server (D2S) cooperation, CFL counteracts the heterogeneity of edge/fog networks to improve ML model performance, energy efficiency, temporal sensitivity, and data/model privacy. We proposed novel technologies that enable efficient D2D and D2S cooperation in CFL.
Finally, we illustrated how CFL can extend the frontiers of research in FL. </p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Salau, A. Rawal, and D. B. Rawat, “Recent advances in artificial
intelligence for wireless internet of things and cyber-physical systems: A
comprehensive survey,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Internet Things J.</em>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advances
and open problems in federated learning,” <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">Found. Trends Machine
Learn.</em>, vol. 14, no. 1–2, pp. 1–210, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Su, Y. Wang, T. H. Luan, N. Zhang, F. Li, T. Chen, and H. Cao, “Secure and
efficient federated learning for smart grid with edge-cloud collaboration,”
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Industrial Inform.</em>, vol. 18, no. 2, pp. 1333–1344, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Reisizadeh, I. Tziotis, H. Hassani, A. Mokhtari, and R. Pedarsani,
“Straggler-resilient federated learning: Leveraging the interplay between
statistical accuracy and system heterogeneity,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas
Inf. Theory</em>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
V. S. Mai, R. J. La, and T. Zhang, “Federated learning with server learning:
Enhancing performance for non-iid data,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv:2210.02614</em>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. Lin, J. Wu, J. Li, X. Zheng, and G. Li, “Friend-as-learner: Socially-driven
trustworthy and efficient wireless federated edge learning,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Mobile Comput.</em>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B. Kar, W. Yahya, Y.-D. Lin, and A. Ali, “Offloading using traditional
optimization and machine learning in federated cloud-edge-fog systems: A
survey,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Commun. Surveys &amp; Tuts.</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Z. Li, Y. He, H. Yu, J. Kang, X. Li, Z. Xu, and D. Niyato, “Data
heterogeneity-robust federated learning via group client selection in
industrial iot,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Internet Things J.</em>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C. Luo, X. Yang, and A. Yuille, “Self-supervised pillar motion learning for
autonomous driving,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of CVPR</em>, 2021, pp. 3183–3192.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Hosseinalipour, S. Wang, N. Michelusi, V. Aggarwal, C. G. Brinton, D. J.
Love, and M. Chiang, “Parallel successive learning for dynamic distributed
model training over heterogeneous wireless networks,”
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv:2202.02947</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “A novel framework for
the analysis and design of heterogeneous federated learning,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Signal Process.</em>, vol. 69, pp. 5234–5249, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
R. Kaewpuang, D. Niyato, P. Wang, and E. Hossain, “A framework for cooperative
resource management in mobile cloud computing,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE J. Select. Areas
Commun.</em>, vol. 31, no. 12, pp. 2685–2700, 2013.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
B. Ganguly, S. Hosseinalipour, K. T. Kim, C. G. Brinton, V. Aggarwal, D. J.
Love, and M. Chiang, “Multi-edge server-assisted dynamic federated learning
with an optimized floating aggregation point,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv:2203.13950</em>,
2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Savazzi, M. Nicoli, M. Bennis, S. Kianoush, and L. Barbieri, “Opportunities
of federated learning in connected, cooperative, and automated industrial
systems,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Commun. Mag.</em>, vol. 59, no. 2, pp. 16–21, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for
contrastive learning of visual representations,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020, pp.
1597–1607.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td">
<span id="tab1.1.1.1.1" class="ltx_inline-block">
<span id="tab1.1.1.1.1.1" class="ltx_p"><span id="tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Su Wang (S’19)</span>  is a Ph.D. candidate at Purdue University.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td">
<span id="tab2.1.1.1.1" class="ltx_inline-block">
<span id="tab2.1.1.1.1.1" class="ltx_p"><span id="tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Seyyedali Hosseinalipour (M’20)</span>  is an Assistant Professor at University at Buffalo (SUNY).</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td">
<span id="tab3.1.1.1.1" class="ltx_inline-block">
<span id="tab3.1.1.1.1.1" class="ltx_p"><span id="tab3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Vaneet Aggarwal (SM’15)</span>  is a Professor at Purdue University.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab4" class="ltx_float biography">
<table id="tab4.1" class="ltx_tabular">
<tr id="tab4.1.1" class="ltx_tr">
<td id="tab4.1.1.1" class="ltx_td">
<span id="tab4.1.1.1.1" class="ltx_inline-block">
<span id="tab4.1.1.1.1.1" class="ltx_p"><span id="tab4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Christopher G. Brinton (SM’20)</span>  is an Assistant Professor at Purdue University.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab5" class="ltx_float biography">
<table id="tab5.1" class="ltx_tabular">
<tr id="tab5.1.1" class="ltx_tr">
<td id="tab5.1.1.1" class="ltx_td">
<span id="tab5.1.1.1.1" class="ltx_inline-block">
<span id="tab5.1.1.1.1.1" class="ltx_p"><span id="tab5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">David Love (F’15)</span>  is the Nick Trbovich Professor at Purdue University.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab6" class="ltx_float biography">
<table id="tab6.1" class="ltx_tabular">
<tr id="tab6.1.1" class="ltx_tr">
<td id="tab6.1.1.1" class="ltx_td">
<span id="tab6.1.1.1.1" class="ltx_inline-block">
<span id="tab6.1.1.1.1.1" class="ltx_p"><span id="tab6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Weifeng Su (F’18)</span>  is a Professor at University at Buffalo (SUNY).</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab7" class="ltx_float biography">
<table id="tab7.1" class="ltx_tabular">
<tr id="tab7.1.1" class="ltx_tr">
<td id="tab7.1.1.1" class="ltx_td">
<span id="tab7.1.1.1.1" class="ltx_inline-block">
<span id="tab7.1.1.1.1.1" class="ltx_p"><span id="tab7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Mung Chiang (F’12)</span>  is the President of Purdue University.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.08360" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.08361" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.08361">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.08361" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.08363" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 20:08:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
